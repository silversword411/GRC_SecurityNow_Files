GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#655

DATE:		March 20, 2018

TITLE:		Pwn2Own 2018

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-655.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the aftermath of CTS Labs' abrupt disclosure of flaws in AMD's outsourced chipsets; Intel's plans for the future and their recent microcode update news; several of Microsoft's recent announcements and actions; the importance of testing, in this case VPNs; the first self-driving automobile pedestrian death; a SQRL update; a bit of closing-the-loop feedback with our listeners; and a look at the outcome of last week's annual Pwn2Own hacking competition.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  An update on the AMD flaws, some more details.  Turns out this is probably the real deal.  We'll also talk about the big Pwn2Own conference, just went on last week.  All the machines that are hacked, all the machines that couldn't be hacked, and a sterling play-by-play, all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 655, recorded Tuesday, March 20, 2018:  Pwn2Own 2018.



It's time for Security Now!, the show where we get together with the smartest guy I know when it comes to this stuff and figure out what's going on in the world around us, Mr. Steven Gibson of the GRC, Gibson Research Corporation, dot com.  Hi, Steve.



STEVE GIBSON:  Leo, it's great to be with you once again.



LEO:  Live long and prosper.



STEVE:  I'm looking at our Security Now! Episode 655 and thinking, ooh, in 11 weeks we're going to be at 666.



LEO:  Devilish.  The devilish episode.



STEVE:  It is.



LEO:  We've never done a 666, have we.



STEVE:  We never, no.  And that also puts us at two thirds of our way through the history of this podcast because of course that's going to end at 999 because I only have three digits.



LEO:  You're really serious about that.  I was hoping you were just joking.



STEVE:  I've got everybody worried about that right now.  They're like, no, no.  Someone suggested use negative numbers.  It's like, no, that doesn't work.



LEO:  It's about six more years.  But you know how fast this first 13 went.  It could go like that.



STEVE:  Yes, yes, yes.  And you still have your hair.  So we talked last week about the fact that the day after last Tuesday's podcast, which would have been, what, the 14th, Wednesday the 14th.  And the 15th and the 16th were the Pwn2Own conference, which has been since 2007 an annual event up at Canada, hosted by Trend Micro's Zero Day Initiative, and also in this case sponsored by Microsoft and VMware.  We have the results of that competition, for better or for worse, and sort of a little bit of both.  So that's going to be our - we're going to wrap that up.



And we're also going to require from you, Leo, a dramatic reading of the two-day event because, with a little bit of massaging of what they provided in a couple of their blog postings, I pulled it together and pumped it up a little bit so that it's a little bit more like an Olympic judging contest.



LEO:  You're not going to make me read this whole thing, though, are you?



STEVE:  Well, if you don't, I will, because I think people will find it interesting.  But I think it'll be pretty good.



LEO:  I don't know if I can do the whole thing in a funny voice, but I'll do my best.



STEVE:  You don't have to do it - if you need to fall out of character, that's fine.  As I was reading it, I was thinking, okay, this would be good, the way this thing was put together.



LEO:  Cool.



STEVE:  But we're also going to talk about the aftermath of last week's surprise announcement by CTS Labs that I know you've been talking about on other shows.



LEO:  With some suspicion, I might add.



STEVE:  Yeah, well, about motives and intent and so forth.  And in reaction to that, their CTO has formally produced a statement that I want to share because there's something to empathize with also.  And of course we just now 90 minutes ago have a reaction, the first official reaction beyond the first little sentence from AMD themselves.  So we want to talk about that.



Also Intel now has announced plans for their own future.  And we have some recent microcode update news that was actually, I should have looked last week to see what was happening with that KB4090007 page because it's much expanded.  So that's good news.  We also have several Microsoft recent announcements and actions.  Also another lesson, and they just keep on coming, about the importance of testing, in this case three popular VPNs, all of which were found wanting when tested.



Also I do want to touch on, just because it's an interesting bit of technology, I know you were just talking about it over on MacBreak Weekly, this last evening's first death caused by a self-driving automobile.  Also I have a bit of an update on the progress with SQRL.  And as I was waiting for the podcast to start, I was thinking how anxious I am to get back to it because we're right down to the last few little things to be resolved.  It's getting very exciting.



Also we'll have a bit of closing-the-loop feedback with our listeners.  And then we're going to talk about the mixed results from this year's Pwn2Own, which significantly had no Chinese participants.  And that's a little bit of a tease for why it was a bit of a lackluster result.



So our Picture of the Week I got a kick out of.  It's the result of last week's Pwn2Own.  We talked about how there would be the award given, the Master of Pwn.  And this is the guy, Richard Zhu, or I guess maybe it's Zhu, I don't know...



LEO:  Zhu.  Zhu.



STEVE:  Richard Zhu, who won the contest.  And this is him wearing his Master of Pwn sweatshirt, giving himself some props, which he has earned and which we will be talking about later in the show, when we talk about the result of the Pwn2Own 2018.



LEO:  He got more than a nice jacket, though, I must say.



STEVE:  Yes, he did.



LEO:  Yes.



STEVE:  Yes, he did.  So as you commented last week, one of the best-named exploits to come along, the Ryzenfall, which was of course one of the four classes.  The biggest one was the hardware problem in the Chimera vulnerability.  These were announced as we were literally going to press.  As I was putting the podcast together Monday evening, as I was assembling everything, the topic was different.  And Tuesday morning it was, oh, crap, this is important.  We need to change the title of the podcast.  And so I quickly rearranged things.



So in the week that has ensued, the controversy of course, which we discussed last podcast, was that the CTS Labs, a relatively new, unknown security firm, about a year old, did not provide AMD with what has now become customary, well, customary is 90 days.  As we know, Google's Project Zero gave Intel 200 days because they understood the nature of the severity and knew that Intel would take this seriously.  Anyway, this was not the case with CTS Labs.  There's been a bunch of fur flying over the last week, some people saying that there was no proof of this.



The good news is that one independent researcher, Dan Guido, whom we referred to also last week, who's the CEO of Trail of Bits, did independently verify the report and tweeted to that effect on Twitter.  He said:  "Regardless of the hype around the release, the bugs are real, accurately described in their technical report, which is not public as far as I can tell, and their exploits work."  So at this point we're beyond wondering whether there's something here, and we are now at the point of, like, okay, so what's the impact of this to end users?  How does the world feel about the way CTS Labs acted, and what is AMD's position?



So first of all, the consensus is, much like Spectre and Meltdown, these are difficult to exploit.  They do require - and this is still a little unclear because we don't have details.  But what we're being told is that administrative access is required for these to be exploited, which is not true of Spectre and Meltdown.  Which caused much more concern, that is, any unprivileged process was able to leverage, if it was clever enough, the Spectre and Meltdown vulnerabilities in order to get cross-process information leakage, which was the big concern.  What we're being told is that this requires administrative access.  So you have to first have some privilege elevation in order to get that access and then exploit it.



On the other hand, later this podcast we'll be talking about elevation of privilege attacks, all of which the attackers at least week's Pwn2Own were able to achieve.  So we shouldn't take too much, I mean, AMD is trying to play up the fact that, oh, you have to be at admin.  Well, that's not looking like that's that difficult for an unprivileged process to obtain for itself.  So AMD, as I mentioned before, is on record now acknowledging the four problems, that is, basically saying we're not happy with CTS Labs for not giving us any time; but, yes, we acknowledge ourselves that these are problems.



Okay.  But there was a lot of pushback about, like, against the way CTS Labs handled this.  So I want to share the letter than the CTO just published on the AMDflaws.com site addressing this  because, I mean, this is an issue for the industry moving forward.  And I think he raises some interesting and worth discussing points.  He starts by saying, by way of a disclaimer:  "I am a technical guy," he writes.  "I love building things and researching things.  I do not like the whole world of PR."  He says:  "It is too messy for me.  I have written this letter in my own language, without PR proofing.  Please forgive me if there are any grammatical errors, or not written according to correct writing standards."



On the history of their publication of these exploits he said:  "We have started researching ASMedia chips about a year ago."  And that's about when this group pulled themselves together and declared themselves an entity.  "After researching for some time, we have found manufacturer backdoors inside the chip which give you full control over the chips."  And what's interesting, too, is that a lot of us have these chips on our motherboards.  So the focus of their disclosure was AMD because AMD had formally used a descendant of these in their own chipset.  But this may still have some farther reaching consequences which have yet to have been articulated.



Anyway, he says:  "...hardware manufacturer backdoors giving you full control over the chips."  And that's the ASM1042, ASM1142, and ASM1143.  And if someone's interested, although this hasn't been leveraged as far as we know yet into exploits, you can probably determine if those are the ASMedia chips on your motherboard.



He said:  "We wanted to go public with the findings, but then saw that AMD have outsourced their chipset to ASMedia.  So we decided to check the state of AMD.  We bought a Ryzen computer and whimsically," he writes, "ran our exploit PoC [proof of concept], and it just worked out of the box.  Full Read/Write/Execute on the AMD Chipset, as is, no modifications.  To be honest," he says, "we were a bit shocked by it.  How they have not removed the backdoors when integrating ASMedia IP [intellectual property] into their chipset," he writes, "is beyond me.  So then we said, okay, what on earth is going on in AMD, and started researching AMD."



He says:  "It took time to set up the working environment to start communication with the AMD Secure processor.  But after reaching a full working setup and understanding of the architecture, we started finding vulnerabilities, one and another and another.  And not complex, scary, logical bugs, but basic mistakes, like screwing up the digital signatures mechanism.  At that point, about once a week we found a new vulnerability, not in one specific section, but across different sections and regions of the chips.  It's just filled with so many vulnerabilities that you just have to point, research, and you'll find something."  He says, parens:  "(Obviously a personal opinion).  After that," he writes, "we decided we have to go public with this.  I honestly think it's hard to believe we're the only group in the world who has these vulnerabilities, considering who are the actors in the world today, and us being a small group of six researchers."



So he then starts a next section titled "Responsible Disclosure."  He writes:  "I know this is an extremely heated topic for debate, where everyone has a strong opinion.  Unfortunately, I also have a strong opinion on this topic.  I think," he writes, "that the current structure of responsible disclosure has a very serious problem.  If a researcher finds a vulnerability, this model suggests that the researcher and the vendor work together to build mitigations, with some time limit - 30, 45, 90 days, whatever - at the end of which the researcher will go out with the vulnerabilities.  The time limit is meant to hasten the vendor to fix the issues.



"The main problem in my eyes with this model," he writes, "is that during these 30, 45, 90 days, it's up to the vendor if it wants to alert the customers that there is a problem.  And as far as I've seen, it is extremely rare that the vendor will come out ahead of time notifying the customers, 'We have problems that put you at risk.  We're working on it,'" he writes.  "Almost always it's post-factum:  'We had problems.  Here's the patch.  No need to worry.'  The second problem is, if the vendor doesn't fix it in time, then what?  The researcher goes public with the technical details and exploits, putting customers at risk?  How we have accepted this mode of operation," he writes, "is beyond me, that researchers advertise at the end of the time limit the technical details of the vulnerabilities because the vendor didn't respond.  Why should the customers pay for the vendor's lack of actions?"



He says:  "I understand this is the model today, and people follow suit.  But I think we can do better.  I think that a better way would be to notify the public on day zero that there are vulnerabilities and what is the impact, to notify the public and the vendor together, and not to disclose the actual technical details ever unless it's already fixed; to put the full public pressure on the vendor from the get-go, but to never put customers at risk.  This model has a huge problem.  How can you convince the public you are telling the truth without the technical details?  And we have been paying that price of disbelief in the past 24 hours."  Oh, boohoo, but still.



He says:  "The solution we came up with is a third-party validation, like the one we did with Dan of Trail of Bits.  In retrospect, we would have done this with five third-party validators to remove any doubts, a lesson," he writes, "for next time.  I know there are many questions, and a whole lot of confusion.  We are trying our best to answer reporters, update our site with Q&A, and clarify what's going on.  So far the media focus was on CTS, and I think I understand this.  But very soon we will have to deal with the fact that a huge company with products spread throughout millions of computers in the world is riddled with so many problems that it's unclear how to even address this.  If you have any technical questions, please contact me at [his email address].  I'll try to answer as many as I can."



So that's his position.  One of the attacks that they have come under is this issue of the AMD stock price, this sort of being the equivalent almost of insider trading, where if they were convinced that the public would react negatively to this news, then they could go short on AMD stock and then make a hyperbolic announcement which actually, I mean, this follow-on statement is a little stronger and worrisome than their original AMDflaws website and technical disclosure that was a little more sober than what they came out with.  And, I mean, I would argue, I guess my position is I can sort of see what he's doing.



Certainly AMD is under massive pressure, although it's difficult to believe that they would have been any less motivated to fix this in a timely fashion had this disclosure been done privately.  That is, he's right about the size of the problem, how widespread it is, how bad it is.  He had proof-of-concept code.  He had one person he could point AMD to.  AMD could certainly validate it just as quickly as Dan at Trail of Bits could.  And especially on the heels of Spectre and Meltdown they would understand the severity of this.  So my take is this was not them behaving correctly.  It's not as if AMD wouldn't have immediately taken action.  I think it's clear that they would have for something like this.



So it looks like they were wanting to be glory hounds in an industry where common practice has been to give the company some opportunity to fix this because in fact their behavior has demonstrably put users at risk.  I think that's clear.  Given the nature of the problems, the fact that, if it's the case that the chips are riddled with security vulnerabilities of the kind they say, and having so clearly pointed us to where they are, there is now a window which is wide open which would not have been if firmware updates were available.  And almost all, there's the Chimera, remember, had two aspects, a firmware problem and a hardware problem.  They claim the hardware problem cannot be fixed, that it's in hardware.  AMD has in their own disclosure some talk of mitigation, although they say that they're working with the vendor, so they're also not saying that this isn't the case that there is this kind of problem.  But all of the rest is firmware fixable, meaning that patches can be created.



And in fact in AMD's statement from about two hours ago they are saying firmware patches are underway and will happen absolutely as fast as possible.  Given that this is a third-party chipset, you know that AMD is putting pressure on ASMedia, and ASMedia is going to be moving as quickly as they can to preserve their relationship with AMD.  So looking at this whole thing, I have to say that, if we say that the goal is to truly protect end users, to protect the people who have the systems, then as long as there are measures in place to patch firmware - and we now know that both Linux is able to and Microsoft are able to, as part of the boot process, to apply a dynamic firmware update.  As long as that's in place, that should have a chance to happen before the discoverer of the flaw is able to or does make a public disclosure.  I think that's really where we have to come down.



LEO:  Should we be worried about other ASMedia chips in Intel machines, a USB controller or anything like that?  I mean...



STEVE:  I think so.  I think that one of the takeaways is that they've already found what they call "manufacturer backdoors" inside three other chips - ASM1042, 1142, and 1143.  And, I mean, I know, I'm not sure what the vintage of those chips are, but I'm sure I've seen when I'm looking at device drivers...



LEO:  Oh, yeah, they're all over the place, yeah.



STEVE:  Yeah.  And so this could be much farther reaching.  So they already had backdoors in these other multiple ASMedia chips.  And it was when they discovered that AMD had subcontracted this chipset family to ASMedia, that these guys said, whoa.  And they got a Ryzen machine, and they ran their proof of concept, and it took over the computer.  And so that's when they said, whoopsie, this is bigger than one audio chipset vendor.  This is AMD and everybody who's using those chipsets.  So I don't think we've heard the end of this ASMedia problem.  And what we may be seeing is driver updates from ASMedia that fixes the firmware in their own chips.



LEO:  It's just those four ASM chips that we know of so far.



STEVE:  Right, right, right.  And we will of course on this podcast be covering that as we get more news and information.



LEO:  Yeah, and AMD indicated that they thought they could have a firmware fix pretty quickly, which was interesting.



STEVE:  Yes, yes.  Which is to say that they verified this themselves.  I'm sure they stomped on the Taiwanese ASMedia hard and said, okay, look, fix this tonight.  And these things should not take that long.  I would argue also, this is much more of, like, a software feeling fix than the Intel microcode stuff.  I mean, it's firmware.  But this is a higher level software-y feeling fix than Intel's.  So it makes sense that it would be, like, whoops, you guys aren't checking a digital signature properly.  That's a higher level, easier to fix thing than microcode in the Intel architecture.



And speaking of which, an editorial was just posted from the Intel CEO, and it looks like his name is Brian Krzanich?



LEO:  Krzanich.



STEVE:  Krzanich, thank you.  Brian Krzanich.  And he's talking about, okay, where Intel goes from here.  "Advancing Security at the Silicon Level" was his editorial.  And basically what he wrote comes down to, in the second half of this year, the next silicon coming from Intel, which is the Intel Xeon scalable processors which are codenamed Cascade Lake, as well as the eighth-generation Intel core processors, both expected in the second half of this year, will be incorporating next-generation hardware fixes for the Spectre and Meltdown problems.



So as we know, right now they're scurrying around with microcode patches, essentially, to the existing, what is it, fifth-, sixth-, and seventh-generation architectures.  They'll then be doing essentially everything for the last five years.  But naturally anything next they do will incorporate, from manufacturing, improvements.  And because they have some time now, there's no articulation yet.  And this has caused some annoyance in the industry.  He's talking about security, like, walls, protective walls.



In his editorial he says:  "While Variant 1 will continue to be addressed via software mitigations, we are making changes to our hardware design to further address the other two," meaning the much more concerning Spectre problems.  He said:  "We have redesigned parts of the processor to introduce new levels of protection through partitioning that will protect against both variants 2 and 3," which are the Spectre variants.  "Think of this," he writes, "this partitioning as additional protective walls between applications and user privilege levels to create an obstacle for bad actors."  So no more details than that at this point.



And the processor watchers in the industry who are really wanting to know what Intel is doing are expressing their annoyance at not having any more detail yet.  I'm sure that'll be forthcoming.  And he's at the CEO level, so he's just reading what engineering told him to read.  But what this does say is that, as would be expected, whereas what they've having to do is essentially to emergency patch the existing architectures of the existing chips, next generation and all chips moving forward will get probably better mitigation, not only built in, but have the opportunity to be better.



And as I mentioned at the top of the show also, I forgot to have us take a look at where we stood with 4090007.  That's that KB page.  If you google KB4090007, that's the Intel microcode updates which Microsoft is publishing in their own updates.  And whereas the first time we looked there were only two, and that's what then motivated me to update my InSpectre app so that it clearly displayed the CPU ID because you know whether there's something for you based upon those IDs.  Now there's a ton.  It's actually eight different CPU IDs spanning Skylake, Kaby Lake, and Coffee Lake.  And one of the other things that Brian said in his editorial was that they now have firmware updates available and published for all processors released in the last five years.  So Skylake, Kaby Lake, Coffee Lake, as well as Haswell and one other that I can't remember.  But everything in the last five years.



So they've jumped on this.  And at this point it's still a "go get it" from Microsoft.  But Intel has made those available.  Microsoft has incorporated those into an evolving update that you can find by googling KB4090007.  And then you can use InSpectre to get your CPU ID and see whether it is now among that growing list, in which case installing that update will give you, essentially will modify Windows so that, every time it boots, your processor is freshly patched with the firmware changes that allow the worst of the Spectre vulnerabilities to be mitigated in hand with changes to Windows.



And speaking of changes to Windows, this month's Patch Tuesday had a bunch, I think it was 15 different vulnerabilities that were fixed.  But among the things that were fixed was that Microsoft released updates for, finally, and I was hoping for this, I'm going to have to change the language once I confirm.  In fact, I'm just turning around.  I have a Windows 10 x86 machine...



LEO:  I was wondering what that was behind you.



STEVE:  Yes.



LEO:  I've never seen a laptop behind you, yeah.



STEVE:  You can see it back there.  It's been updating for about five hours.  These updates are not - when you do a major move, it can take a while, and I haven't had this machine on for a while.  So it looks like it's done.  And I wanted to have - my hope was that it wouldn't be able to take that long, and I'd be able to verify.  But I assume the news is correct, which is that Windows 10 x86 now got updates in March, that is, it already has them, for microcode running the Win 10 Fall Creators Update, as well as Intel's sixth-gen Skylake processors.  And now we know - so that would have been built into the updates, which is news.  That means that the automatically delivered updates included Skylake, instead of having to go get it yourself.  Not all the other ones yet.



So it sounds like Microsoft is staging the release of this.  Right now, if you really want to go get it, you can, for example, to get also Kaby Lake and Coffee Lake coverage, which is what's in the KB4090007.  But given the fact that just this month, a couple weeks ago, they did incorporate the microcode updates for Skylake, which were the first ones to appear at that patch-it-yourself page.  This suggests that, as soon as Microsoft is absolutely sure that the firmware isn't going to cause any other problems, that this will get rolled into future - probably April's Patch Tuesday.  So that means that ultimately everybody's going to get themselves fixed.  But now, so far, the Windows 10 Fall Creators updates had been alone.  They moved it to x86.  But now we also, as a consequence of Patch Tuesday, we have the x86 also versions of 7 and 8.1, as well as Server 2008 and 2012.



So Microsoft's doing the right thing.  They are getting everything covered.  And that means I'm going to be changing the language in InSpectre here in a second, after the podcast is done.  And I've verified it myself to confirm that the x86 variants of these Windows OSes have been fixed because in InSpectre at the moment it just says I hope Microsoft does this.  The good news is they are doing it.  So that's great.  And in which case InSpectre will be able to show you some better news.



Okay.  So Microsoft has followed Intel in beefing up their bounties.  Remember we talked about this in February, so last month.  Intel beefed up their own bug bounty program.  And I don't quite understand why both of them, Intel and Microsoft, have a year-end deadline or expiration.  But both Intel and Microsoft's bug bounty programs are only valid through December 31st.  Which I don't get because it's not like bugs are going to stop happening.



LEO:  Bugs expire in December.  Didn't you hear?  No more.



STEVE:  Yeah, we wish.



LEO:  We fixed them all.



STEVE:  So Intel has announced last month that they will pay out up to a quarter million dollars - yes, $250,000 - for reports of new, similarly serious to Spectre and Meltdown side-channel bugs.  In other words, they want to know about these.  If the severity ratings are 9 or 10 on the CVSS, the Common Vulnerability Scoring System that is, like, top-level severity, then you get a quarter million dollars if you bring that news to Intel.  If the severity is between 7.0 and 8.9, meaning just shy of 9, then you can get as much as $100,000.  And below 7.0 you're down at $20,000.  But still nothing to sneeze at, I mean, if it's some sort of a side-channel problem.



Well, so Intel announced that last month.  Now Microsoft has virtually duplicated that program.  Same sort of story applies.  And I'm not sure what happens if you co-release the news to both of them.  Do you get two paydays?  I mean, I hope.  So if anyone else is able to find anything wrong like Spectre or Meltdown, Microsoft says we're going to pay you for that information.  And similarly it could be worth a quarter million dollars.



Their announcement said:  "Today, Microsoft is announcing the launch of a limited-time [same expiration] bounty program for speculative execution side-channel vulnerabilities.  This new class of vulnerabilities was disclosed in January 2018 and represented a major," writes Microsoft, "advancement in the research in this field.  In recognition of that threat environment change, we are launching a bounty program to encourage research into the new class of vulnerability" - which is cool - "and the mitigations Microsoft has put in place to help mitigate this class of issues."



So they're saying a quarter million dollar jackpot is sitting here until the end of the year for anyone who can find anything else like this, or something that we missed in these vulnerabilities.  So they have a four-tier.  They said new categories of speculative execution attacks get you a quarter million dollars.  Azure speculative execution mitigation bypass is worth 200,000.  Windows speculative execution mitigation bypass also up to 200,000.  And then instance of a known speculative execution vulnerability in Windows 10 or Microsoft Edge.



And they say:  "This vulnerability must enable the disclosure of sensitive information across a trust boundary."  In other words, some way of using a known vulnerability that they missed, you can get $25,000.  So this is all good. I mean, we've talked about this.  Google, what, is there like at $1,600 or something.  I mean, and we've talked in the past about these bug bounties offering such little money that it wasn't clearly going to incent a lot of research.  But a quarter million dollars, that's going to get you maybe thinking about other clever ways of leveraging our current processor technology.



So that's what we need.  We've seen, for example, with cryptojacking, where the ability to turn virtual cyber currency into money has created a huge amount of pressure to install crypto mining unwittingly into people's browsers or servers.  In other words, the fact that there's money at the end of the trail makes a difference.  And so now we've got money at the end of the speculative execution trail, both from Microsoft and Intel.  Maybe, if you can get both, half a million dollars for finding some new problem.  And that's what it takes.  It takes somebody wanting to find a problem to do so.  And these companies have the money, so I think this is good.  And I just don't know why it expires on New Year's Eve.



LEO:  Well, maybe they, you know, they don't want to be liable forever.



STEVE:  I think that's probably it, yes, is that otherwise it's an open door, and they could be held to it maybe forever.  So, yeah, I think you're probably right.  The attorneys probably got involved and said, eh, you need to not let this thing be open forever.



So, okay.  Speaking of my laptop that took five hours to update this morning, I did want to inform our listeners that Microsoft is changing the way Windows does its twice annual major updates.  By Microsoft's estimate, the Creators Update, which was released almost a year ago, took on the average of about 82 minutes, during which time those rollercoaster dots - is there any formal name for those things, Leo?



LEO:  I like that.  I nominate that for the name.



STEVE:  It always reminds me of a rollercoaster where you kind of go over the top, yeah.  It's too bad they didn't use the infinity symbol, though, instead, for the rollercoaster, because that would be a little more representative of how long you're waiting.  But anyway, so the rollercoaster dots, while you are offline, was 82 minutes.  Now, in the Fall Creators Update that some of us have just suffered through, they improved that so that the rollercoaster dots were spinning, by their computation or their estimate, about 51 minutes.



So we're coming into, and we referred to this in the last couple weeks, it hasn't been officially named, but it's provisionally being called the Spring Creators Update.  I can't remember what the context was for me talking about it last week or the week before, but there was something that they were going to be doing with this thing called the Spring Creators Update.  I don't remember now what it was.  But they're now saying that they're planning for what I would call the "system unusable time" to be nothing more than about half an hour.  So it's not gone, but they're trying to squeeze down to 30 minutes.



And it's not that they've produced some miracle.  What they've done is they have been progressively moving more of the work of performing one of these major updates, which essentially it's a hidden reinstall of Windows.  I mean, it is a whole new Windows that is coming in and then moving everything over from the old Windows into the new Windows and saying, okay, here we go.  So what they've been doing is they've been moving more and more of that into the so-called online phase, where you can still use your computer.  And they're saying that it's in the background with low priority, so it won't interfere, it won't make things sluggish while it's doing it.  It's not like running cryptocurrency mining in your machine.  But as a consequence, the total time required has actually inflated significantly because they can't commandeer, they just don't want to commandeer your machine in this offline phase for as long as it would take.



LEO:  They do something sort of like this right now.  When you install Microsoft Office, it continues for a while in the background.  But it might let you run Word partially.



STEVE:  Well, and remember, even booting.  You know, famously, they were under - it took so long to boot [crosstalk].



LEO:  No, you can't do anything.  But look, you're seeing the desktop.



STEVE:  Look, oh, here's your desktop, exactly.  It's not quite here yet.  Things are still popping in, you know, populating.  And it's like, uh, wait a minute, what?



LEO:  It's ironic because the initial installs are actually much faster now because they use images, and they blast them [crosstalk].



STEVE:  Right, right.



LEO:  It's the updates.  And if you think about it, an update is a nontrivial thing to do.



STEVE:  Oh, I don't even want to - like I'm glad that's somebody else's problem because that's not a problem anybody wants to have.



LEO:  I wonder, though, if you could just do a diff, you know, just take the fully - but you can't because every system's different, I guess. 



STEVE:  Right.



LEO:  That's the problem.  If you could say, oh, just take - if you had a known system in the diff, and you just applied the vectors, the changes, even if it was nonsensical, it'd be just a little bit here, a little bit there, you could do it.  But the problem is that doesn't work, yeah, because you don't know the starting point.



STEVE:  Right.



LEO:  Oh, man, I don't know how they do it.  That must be some genius's full-time job.



STEVE:  Oh, and when you look at how many files there are, I mean, the whole Windows Update thing, the idea that you can go in and remove one that you don't like?  What?



LEO:  I always think of it as - the analogy I use on the radio show is, you know, you've seen the magician who whips the tablecloth out, and all the silverware and the plates are the same.  Well, this is doing that, and then putting a new tablecloth in.  Right?



STEVE:  That's good.



LEO:  You're whipping it out, and then you're putting a new one in without disturbing anything.  Good luck.  



STEVE:  And in the process you change the forks to chopsticks.



LEO:  Just 'cause.  Just 'cause you can.



STEVE:  Yeah.



LEO:  It's spring, let's put in a yellow tablecloth today.



STEVE:  That's right.  So we know well the lesson that you know nothing about security until it's been tested.  I've used the example for years about Ballmer prancing around the stage before the release of Windows XP, declaring before its release that this was the most secure operating system Microsoft had ever produced.  And I was here then.  And it's like, uh...



LEO:  No.



STEVE:  No.  That's not something you can declare.  You can want it to be true, but history is the judge of these things.  So that is to say, only when something has been tested can you affirmatively say, I mean, and even then you can't say we know it's true.  But you can say smart people tried to break it and couldn't.  So that's a lot better than if that never happened, if no one tried to break it, and everyone's just saying, oh, yeah, Ballmer's entertaining, so let's all use his operating system.



So we had an independent organization, vpnMentor.com, that contracted with three external ethical hackers, one who wished to remain completely anonymous; one, well, he might as well be anonymous because his handle is "File Descriptor," and I don't know who that is, so he's anonymous; but Paulos Yibelo is the third guy, who was the manager of this VPN testing team.  The File Descriptor guy is a reputable ethical hacker working for Cure53, who is a company we've referred to in the past.  We've covered some of their work.  And that was the company hired by TunnelBear to identify and fix issues with their VPN applications.  And Cure53 is a leading security research firm.  Paulos has previously found vulnerabilities in popular VPNs and published them in the past, and his work has been covered in ZDNet, Slashdot, and other places.  So these guys are real.



Well, they took a look at three VPNs - Hotspot Shield, PureVPN, and ZenMate VPN - and found significant, readily exploitable IP leaks, that is, IP address leaks in all three.



LEO:  The user's IP address?



STEVE:  Yes, yes.



LEO:  That's not good.



STEVE:  Readily obtainable.  Now, we know that you don't use a VPN necessarily for complete anonymity.



LEO:  As you shouldn't.  That's not what a VPN's supposed to be, yeah.



STEVE:  Right.  That's what TOR promises more than a VPN.  It is the case, though, a VPN gives you encryption.  And you get some anonymity because your traffic is going to emerge from an endpoint at your VPN host server out onto the Internet.  And hopefully, you know, and so the point is that at that point the packets are being rewritten with - your source IP is rewritten with that server endpoint source IP so that, when they go out to a server, the IP address that the website that you're visiting sees is the VPN server IP.  The packet then goes back.  The VPN server that's been keeping track of this rewrites the destination from itself back to you, and then the packet goes through the tunnel and so forth.



So it's sort of wrong to expect really strong anonymity from a VPN.  But neither should it have really bad, readily exploitable problems.  And all three did.  Of the three organizations, only Hotspot Shield responded promptly and the way we would want them to.  They had three problems, but they just - immediately they stepped up, worked with the security researchers, recognized, acknowledged, and immediately fixed the problems.



At this point, today, that has not happened with PureVPN and ZenMate VPN, both of whom have outstanding problems they've been informed about and haven't resolved.  So as we know, mistakes happen.  Anybody can make them.  I think it's significant that Hotspot Shield said, okay, we're on this.  It would be nice if Hotspot Shield had themselves contracted for a security audit, the way for example TunnelBear did, get themselves audited and be able to say, hey, you know, we had people check us out, and we passed.  But the mistakes are interesting.



So there were three problems in Hotspot Shield, all now fixed.  One of the things that Hotspot Shield, a feature, is a Chrome extension to make it just sort of like, oh, look, it works, easy drop-in.  Well, no, that's not the way I'm going to run a VPN.  I'm going to run a VPN by installing a client, like OpenVPN, and then using it to establish a tunnel, and then everything in my system goes through the tunnel.  But I get it.



LEO:  I'm surprised these companies don't use OpenVPN, actually.



STEVE:  I know.  I know.



LEO:  Boy, that's weird, isn't it.  You think they use a custom-built VPN?



STEVE:  They might well use an OpenVPN protocol.  But in this case it was a Chrome extension that had a problem.  There was a proxy autoconfig script.  And it turns out that the proxy autoconfig script was looking for a query parameter in the URL that was act=afProxyServerPing.  And if that parameter was in the URL, it routed all traffic to the proxy hostname provided by that parameter.  Which means any site could give you a URL, like that you click on, and your traffic is hijacked, and all of your VPN traffic runs through this hijacking site.  So, whoops, not what you want from a VPN.



Also they noted that the DNS lookup was using the system's DNS, which that's an often-made mistake.  I've referred to this for years.  Not all VPNs capture all of your Internet traffic, that is, some of them just capture your TCP connections or your web surfing connections, saying, oh, yeah, real easy VPN, whatever.  But the DNS lookups don't go through the tunnel.  Well, that was also the case.  And the problem with that is that DNS leaks your IP.  I of course have the DNS Spoofability page where GRC pretends to be a DNS server for the user, and I end up being able to check the action of the DNS server.  But it's possible to leak all the way back to your IP.



There's a site, www.dnsleaktest.com, which operates very similar to mine.  In fact, I was wondering if they were using mine, like proxying my page because it looks - they copied it, essentially, sort of a subset of its behavior.  But it demonstrates that it is very easy to determine who anybody's DNS servers are.  I mention this to our listeners because this is a test you can easily perform.  If you're a VPN user, fire up your VPN and then go to www.dnsleaktest.com and see whether it shows the same DNS servers, that is probably your ISP's DNS servers, or whether it's showing you the DNS servers belonging to your VPN provider, which is what you would want.  You would want for the DNS queries generated by your system to be tunneled through the VPN also and not to just ignore it and make a standard system-level DNS query, which can easily and readily leak your IP.



And then the last thing they found was, again, in this proxy autoconfig, a very broad whitelist.  And just because I thought some of our listeners might get a kick out of it, I have it in the show notes here.  But, I mean, it's amazing what is on the whitelist that bypasses the proxy, so much so that, for example,  localhost dot anything, if localhost appears in the domain name, it bypasses the VPN.  So, like, localhost.evilsite.com, and you're not VPNed.



So it's like, whoops, again, all these have been fixed.  It's worrisome that there were these problems.  But this is a perfect lesson of you really just can't assume something is secure until somebody has looked at it, I mean, somebody who knows what they're doing has tried to find problems with it.  And this is why, going back to these bounties, why I'm really happy to see Microsoft and Intel creating some significant cash windfall bounties for hackers, to motivate hackers to try to find more problems.  When we look, we find problems.  And it's not until you have really, really tried hard and found nothing that you can begin to say, okay, this seems good.  So anyway, people using these other two VPNs, I would look for updates hopefully coming from them of their fixes.  And Hotspot Shield has immediately stepped up and fixed the problems that they were having.  So hats off to those guys.



And I did want to just touch on, although this is not a Security Now! topic, just on last night's news of a pedestrian death, just because it's a novel event.  This is the first pedestrian death involving a self-driving car.  It occurred in Tempe, Arizona at 10:00 p.m. last evening.  There was a driver, a so-called "backup driver," in the auto.  It was an SUV owned by Uber as part of their self-driving autonomous vehicle project.  There was, remember, what was it, in 2016 Tesla had a problem.  Their autopilot software, it was considered, contributed to the death of a Tesla driver, his name was Joshua Brown, although he had ignored repeated warnings to put his hands on the wheel and didn't.  So he certainly had some responsibility there.



In this case it's not looking like, I mean, this just happened.  So camera footage is being checked, the Uber's self-driving software I'm sure is black boxes and green boxes and red boxes, all the boxes are being probed.  But the video footage that's been seen, the driver's report, nobody was impaired.  The driver wasn't, you know, the backup driver was not impaired.  Neither was the pedestrian.  But a 49-year-old woman stepped out with her bicycle in the middle of the block, directly in front of the car.  The feeling was from the police reports that anybody, like a human driver driving along, it was a 35-mile-an-hour zone, the car was going 40 miles an hour, so about as much over as we all typically drive.  It'll be interesting to see who decides where fault is.



But I really liked what Andy had to say at the end of MacBreak Weekly, too, Leo.  I agree with him that downstream I wouldn't be surprised - it's going to take a couple generations, maybe a generation, 20 or 30 years.  But I wouldn't be surprised if ultimately we get this stuff nailed.  And it doesn't look like it was a technology fault.  It looks like something...



LEO:  I don't think you can blame the car at all.



STEVE:  No.



LEO:  I mean, if somebody, I mean, I don't know what happened.  We're just going by the police report.  And the police were the ones who said - the Tempe Police said this couldn't have been avoided even with a human driver.  But first of all, I'm surprised it wasn't going the speed limit.  That, they should be dinged for that.  Autonomous vehicles should travel exactly the speed limit, and maybe even a little less.  But even going 35 miles an hour, if somebody jumps out in front of you, there's physics, you know.



STEVE:  Yes.



LEO:  Autonomy does not overrule physics.  It's not even reaction time.  The reaction time of an autonomous vehicle should be instant.  It's physics.  You can't stop.  So I don't think you can blame - but of course this will go against the account of autonomous vehicles, which is in a way a shame because I don't think it should.



STEVE:  Well, maybe if it introduced another note of caution, that might not be a bad thing.



LEO:  Sure.



STEVE:  Just, you know, it's like, let's slow ourselves down a little bit.



LEO:  What are you going to do?  What would you do in the software except go down, go slow down.



STEVE:  You're right, that's a very good point.  That's a very good point.



LEO:  What would you do to fix it?  You can't fix that.



STEVE:  Yup.



LEO:  Can't fix humans, that's the problem.  You can fix a machine.  You can't fix a human.  And you know what, it's going to happen.  Sometime a kid's going to - I was telling Lisa this when we were talking about the story yesterday.  Some kid's going to chase a ball out from behind a car and get clobbered by an autonomous vehicle.  Now, it happens all the time by human-driven vehicles.  But when it's a robot, that's going to be a problem.



STEVE:  Well, it's very much like how upset we get when an airliner crashes.



LEO:  Right.



STEVE:  But we're not as upset by the distributed deaths occurring on the highways all over the country every day.  I mean, the famous line is that statistically it's far safer to travel by air.  Nobody really likes that because it takes...



LEO:  But humans are not rational.  That's the real problem.



STEVE:  Right.



LEO:  We're not.  And this is why I'm kind of stepping up to defend this robot vehicle.



STEVE:  I think you're right, Leo.



LEO:  Because it'll be a shame if we don't get autonomy because of an irrational fear of autonomy.



STEVE:  Right.  Right.  And we won't talk about Terminator and Skynet because...



LEO:  Well, it's built into the human psyche.  We're afraid of these things, yeah.



STEVE:  That's another problem, yeah.  Okay.  So I just wanted to give our listeners an update on where I'm spending still full time, and that is on SQRL.  Something I've been meaning to talk about since I mentioned it maybe a month ago, I called it the "sensitive data monitor."  I did a number of things - there are a number of things now looking back I wish I had done differently.  Not many, but like internal architectural stuff that turned out to not evolve as well as I wanted.



But one of the things I love is that every single string of text is in a separate file with a numerical number, meaning that it's super simple to change the wording of anything, and of course to make it language-specific.  That, I nailed that.  Also because it needs to be language - I wanted to give it the opportunity of being multilingual.  Different languages require different amounts of space.  So I always built in a scaling to the UI.  Well, that turned out to be a win because of this DPI awareness that we need now because our screens have gotten such high resolution that, if the app didn't scale itself to the resolution of the monitor, it would be too small on high-resolution monitors.



And in fact the InSpectre app doesn't have this technology, and I've had complaints from people saying, "I can't read it," because they've got 4K monitors, and the thing's the size of a postage stamp on their monitor.  It's like, well, I'm sorry.  Get a magnifying glass.  But anyway, so I got that right.  The other thing I got right from day one, and I'm only saying this because I want other people writing secure code to hear this, mostly what I'm about to say next, but the other thing I got right was creating containment for everything that was sensitive.  That is, there is one single location where everything that is sensitive ever resides.  And I created it upfront so everything I ever did was in that container.  And that has turned out to be the biggest win, of course, because it is easy to lock it in memory and to wipe it.  And so that ended up being another really - I'm very pleased with that design decision.



But something I did about a month ago I wish I had done at the beginning.  And so anybody who is going to write SQRL components, we've got clients and server-side stuff in the works.  There's a Java side.  There's a JavaScript.  We've got browser plugin.  Android and iOS clients are on the way.  So a lot's happening.



About a month ago - and I mentioned this on the podcast, but I never followed up - I created a window which I called the "sensitive data monitor."  And it is a window that dynamically shows the content of that sensitive data area, and labels it, and formats it so I can see it.  And as a consequence of that - I had always intended to do this at the end of the project.  Now it's ended up being so useful, I wish I had always had it.  And that is that, while you're - and this is only, of course, for R&D mode and the developer edition, where  it logs all of the guts of its crypto in order to help other developers confirm their internal operation, and it logs all kinds of stuff.  It also shows you the contents of the sensitive data area.



And as a consequence of that, not only of my seeing it, but also letting the people who are testing the client have that, we found some edge cases where, if you did this, and then you did that, and then you did this, then, oh, look, a key that should have been proactively zeroed didn't get zeroed in that particular case.  And as a result, I then have been subsequently far more aggressive in proactively wiping things.



But then I caused myself some problems because I was overwiping.  I was like, I broke some things by removing information that was necessary, which is why I wish I had done it from day one because then, as I was moving through the development of this, I would have always had my eye on that, making sure that when I was done with some data, it was immediately eliminated.  And again, I'm being over cautious.  Nobody else, I mean, evidence demonstrates that people are leaving stuff in RAM all the time.  So I'm wanting not to make that kind of mistake.  But I just wanted to say to anyone else developing security-aware software, doing this, just having it there onscreen while you're working with it is incredibly useful.



And I'm almost done.  I fixed a problem with SQRL running in Sandboxie, which now it works just perfectly.  Some people didn't want to use the screen darkening.  SQRL has a UAC-style screen darkening to demonstrate that you're using SQRL to keep web browsers from being able to spoof SQRL.  And when we turned that off because some people didn't want it because it also breaks copy and paste, which is good because you don't want that for your password, but some people are, you know, they're wanting that.  And so I allowed screen darkening to be disabled, but that created a keyboard focus problem which I'll be fixing probably tomorrow.



And then it's, like I hesitate to say because something that's this big and involved doesn't - it never ends instantly.  But I will then turn my attention to bringing up the web forums which I'm going to have online before the formal announce because there's no way I can handle the consequence of all of our listeners grabbing it and starting to play with it.  We need someplace for people to go to self-help and answer questions and so forth. But it's very close to being time for me to come up to Petaluma, Leo, and sit down with you and maybe Father Robert, and go through this.



LEO:  Just remember, I'm going out of the country April 19th through May 5th.  So either before then or after then.  Well, Robert could do it.  You don't have to have me here.



STEVE:  Oh, but you're a perfect foil.  You're wonderful.  [Crosstalk].  What does that do?  What does that button do again?



LEO:  What the hell's that for?  Hey, this just in.  Expedia is announcing that 880,000 payment cards have been exfiltrated by bad guys from their site.



STEVE:  Darn.



LEO:  Yeah.



STEVE:  Well, what was that sponsor you were just telling us about, Leo?  Capital One?



LEO:  Yeah, everybody's got to use Capital One.  Between January 1st of 2016 and December 22nd of 2017 the breach for its partner platform, and then between January and June for its consumer platform.  Phone numbers, names, email, billing addresses.  Orbitz was not affected, but Expedia was.



STEVE:  Boy.



LEO:  Anyway, just, you know...



STEVE:  Yup, it happens.



LEO:  It happens again, and again, and again.



STEVE:  The good news is, if the world chooses to adopt SQRL, we will never again have a theft of passwords.  That's just gone because SQRL gives websites no secrets to keep.



LEO:  Yup. 



STEVE:  Which is nice.  I got an interesting note from an anonymous sender.  He used our web form for sending feedback.  And the subject was "SpinRite Story in 652" - so, what, three weeks ago - "could have been social engineering."  And his submission was dated March 7, so a couple weeks ago.  He said:  "Hi, Steve.  I just caught SN-652, listened to the SpinRite story."  And so then he quotes a dialogue, or he makes one up.



"Customer?:  I lost my copy of SpinRite.  I'm incapable of recordkeeping.  I have no backups.  I'm an unorganized person.  By the way, the email I bought SpinRite with no longer works <wink>.  Here's my new email address.  Can I have a new copy, please?"



And so he quotes GRC Support saying:  "No problem.  We don't need to verify anything.  We trust you.  Here's a link to the download."



LEO:  Isn't that nice.



STEVE:  "Feel free to change your contact information.  We'll just email you the secret PIN in plaintext."



LEO:  You shouldn't announce this on the air, Steve.



STEVE:  The customer responds:  "Really?  Yoink!  If anyone asks, I was never here.  And, no, I am not wanted in five states for fraud."  So then this guy says:  "How was this different from someone calling your cell carrier or your bank and claiming to be you?"  He says:  "This could have been social engineering.  It is often said the customer is always right.  But what if they aren't a customer?"  Anyway, and then he says:  "Looking forward to the end of SQRL" - which as I just said is approaching rapidly - "and the beginning of SpinRite 6.1," which is also approaching rapidly.



So I just wanted to say we have a database dating back, well, actually it's in FoxPro.



LEO:  That tells you something.  Almost dBase II.



STEVE:  Actually, it was in dBase II, and we updated to FoxPro, running in a DOS box.



LEO:  Wow.



STEVE:  And it's got SpinRite 1 customers in it.  So I'm sure that Sue did look up this person's name.  And she does some due diligence.  But at the same time, let's remember who we are.  I mean, the company's policies are an extension of mine.  And we often share stories of somebody who let their friend run their copy of SpinRite.  Last week we said some guy fixed his dentist's RAID array and asked his dentist to buy a copy, and he believes they did, as a way of thanks and sort of payment for the services that the software rendered.



LEO:  Nice, yeah.  That's the right thing to do.



STEVE:  That's the way to be.  I remember back when there was competition, there were other hard disk utilities.  Every single license said "can only be run on a single hard drive."  And that just never seemed reasonable to me.



LEO:  You're just not greedy enough, Steve.



STEVE:  It's just not practical.  Well, and that, too.  When I do something like InSpectre, everyone says, oh, just charge a dollar, charge a dollar.



LEO:  Yeah.



STEVE:  Well, yes, I would have made $547,000 or something.  But on the other hand, many fewer people would have actually used it.  So I think my policy is right.  Greg and Sue are gainfully employed.  I get to have cabernet.  And I've had time, thank you everybody, to develop SQRL and offer it to the world.  SpinRite 6.1 will follow, and we'll go from there.



LEO:  And this show, yeah.



STEVE:  And the show, exactly.



LEO:  We'll send you some - we have a new wine sponsor, and we have a wine from them that I - because I know you love cabs.



STEVE:  I do.



LEO:  I think you might really like - we're going to send a couple of bottles down, I think.



STEVE:  Cool.



LEO:  That was Lisa's idea.  So that'll make up for that lost revenue.



STEVE:  I don't feel it's lost.



LEO:  No, I agree, I agree.



STEVE:  I mean, our customers, so many of our listeners have said they bought a copy of SpinRite to support the podcast, and then to support my efforts.  And hopefully they're getting some value in return.  It's not a charity, it's like here's some software.  And the other thing I'm not doing, you notice, I mean, people complain it hasn't been updated in a long time.  That's true.  On the other hand, I'm not dinging people for upgrade fees every year, trying to milk their loyalty to SpinRite.



LEO:  That's a lot of the reason things get upgraded.  Not for real value...



STEVE:  Exactly.



LEO:  Just because then they can charge you.



STEVE:  Yes, exactly.



LEO:  It's an annuity.  No, that's what we love about you, Steve.



STEVE:  Well, anyway, I got a kick out of him saying, "That guy could have been making it up."  And it's like, yes, he could have.  And, you know, fine.  I'm not going to lose any sleep over it.  We're doing fine, and I appreciate all the listeners who are honest.



Closing the loop, two things.  A Kari Mattsson said:  "Any comments on this?" and then quoted:  "EV certificates under DigiCert's root CA without human intervention."  And he or she provided a link to TrustCubes.com/ev-ssl and asked the question, "Shouldn't EV certs be more scrutinized?"  And that's interesting because it's a great question.  The answer is no, if the scrutiny is placed where it should be.  For example, I have loved the fact, and I've mentioned this on the air, that I've been able and can issue my own EV certs with DigiCert.  I've done it in the middle of the night when I've needed something.  And I love that fact.  Where the scrutiny needs to be applied is on me and my account are GRC.  That's the identity binding that  needs to be absolutely verified.



And what's cool is, independent of my certificate recycling and renewal, I will get contacted by someone from DigiCert saying, hey, we need to reverify you for EV status.  Which I think is brilliant.  So what they've done is they've decoupled the timing of the identity binding to the account from the certificate issuance, allowing the account holder to issue EV certificates whenever they need to, and then while at the same time not degrading the integrity of the process because they are asynchronously making sure you're still you.  So I loved that question, and I just wanted to note that there is a way that you can get the best of both worlds, and that's the service that DigiCert in my experience alone has been offering me, and I really appreciate it.



And then I love this one other tweet from Itinerant.  Itinerant?  Yeah.  Wait, is that the way you pronounce it?



LEO:  Yeah, itinerant, yeah.



STEVE:  Itinerant.  For some reason that just seemed wrong to me.  Itinerant Engineer.  Anyway, I love this.  He tweeted both to me and to SwiftOnSecurity the observation:  "Perfect Forward Secrecy" is the "I guess you just had to be there" of encryption.  And of course that's exactly what it is.



LEO:  And somebody just bought a copy of SpinRite.



STEVE:  And somebody just bought a copy of SpinRite.  Thank you.



LEO:  Thank you.  All right, Steve.  On we go with Pwn2Own.  I'm going to get my sportscaster voice ready here.



STEVE:  Yeah, we've got to - the way they wrote up the blow-by-blow, I just thought you'd have - our listeners would get a kick out of you running through...



LEO:  I don't know if I have a sportscaster.



STEVE:  You've got a million voices, Leo.



LEO:  I'll find one.



STEVE:  Bottom line is that China was sorely missed.  China's hackers routinely win at Pwn2Own, typically sweeping the board.  And notably, we've talked in previous years, the Tencent and the Keen teams have been the big winners in the last couple years' competition.  In last year's competition of Pwn2Own the top five winners were from China, and three of them were from the Tencent group.  And as I mentioned when we were on the front side of Pwn2Own, the Chinese government has decided that they're going to keep their hackers at home.  And so there were no Chinese entrants this year, and it showed.



There are potentially many bugs and cash prizes to be had.  Last year, which was the event's 10-year anniversary, since the first one was 2007, the Zero Day Initiative awarded a total of $833,000 to white hat hackers, 833, 833,000 to white hat hackers, exposing 51 different zero-day bugs.  And most of those were found by Chinese researchers.  This year, by comparison, despite pulling together a purse of two million available dollars, this year's hackers took home only $267,000 out of that total of two million, despite the fact that it was a target-rich environment.



Oracle's VirtualBox, VMware Workstation, Microsoft Hyper-V Client, Chrome, Safari, Edge, Firefox, Adobe Reader, Microsoft Office 365, Microsoft Outlook, NGINX, Apache Web Server, Microsoft Windows SMB, and OpenSSL, all of those were valid targets for exploitation.  And despite the fact, very few of them were exploited.  Mostly the browsers took a hit.  Actually, it was browsers and in one case VirtualBox.  And $267,000 in total this year, despite the fact that the largest prize was a quarter million dollars for a sandbox escape from Hypervisor to the kernel under Hyper-V.



LEO:  They had one of those last year.



STEVE:  No takers.  Yeah.



LEO:  Isn't it possible, though, that this software was more secure, like these things had been patched?  Or do you think the Chinese team just has all these exploits and no one else does?



STEVE:  I think that they're good.  I think that...



LEO:  I think this is bad because now we don't know what they can do.



STEVE:  Oh, it's definitely bad, Leo.  It's bad. 



LEO:  I mean, they're not exactly friendly adversaries.



STEVE:  Unh-unh.  No.  And also the timing was a little unfortunate because several non-Chinese competitors dropped out at the last minute after the immediately previous day's Patch Tuesday, which closed some holes they'd been planning to exploit.



LEO:  Oh, man.



STEVE:  So it was like, whoops.  Yeah.



LEO:  So I have some misgivings about this whole thing because people hold onto exploits for a whole year instead of...



STEVE:  Yes, yes, yes, for the competition.



LEO:  ...responsible disclosure.



STEVE:  Yes.



LEO:  They're trying to make money.  Now, it says here that the Pwn2Own, the CanSecWest people buy these exploits.  What does that mean?



STEVE:  Yeah, and it's weird, too.  And that's the language that they use is that they purchase...



LEO:  Does that mean they hold onto them?



STEVE:  Yes.  Well, no, it means that they purchase them.  The Zero Day Initiative is run by Trend Micro.  So Trend Micro protects their users immediately as a benefit of using Trend Micro's antimalware products.  And then they do then responsibly disclose them to the vendors of the products.



LEO:  Right away.



STEVE:  So they're not - yes, yes, right away.  So this year's winner succeeded in two instances.  And you'll be reading his name in a second, Richard, and you pronounced it correctly.



LEO:  Zhu.



STEVE:  Zhu won the contest with 12 points for hacking Edge and Firefox and took home, out of that 267,000, 120 of that.  But anyway, I thought our listeners would get a kick out of you sort of taking us through the competition.



LEO:  This is a lot.  I'm only going to do a few paragraphs.  I'll let you do the rest.



STEVE:  Okay.



LEO:  "Day One:  The first day of Pwn2Own 2018 has come to a close, and so far we've awarded $162,000 USD and 16 points toward Master of Pwn."  Oh, I love this.  "Today saw two successful attempts, one partial success, and one failure.  In total, we purchased three Apple bugs, two Oracle bugs, three Microsoft bugs," and a partridge in a pear tree.  No, I added that.  "The day" - oh, I love this.  This play-by-play is pretty funny.



STEVE:  Yes.



LEO:  "The day began with Richard Zhu (@fluorescence) targeting Apple Safari with a sandbox escape.  Unfortunately, he couldn't get his exploit chain working within the time allotted due to a failure in the heap spray technique."  Oh, no.  "Despite this, the bugs he brought to the contest were certainly interesting and were purchased through the regular ZDI program."  I guess that's what we were talking about.  They gave him money even though he didn't do it right, or it didn't work.



STEVE:  Correct.



LEO:  "Undaunted, Richard later returned to target Microsoft Edge with a Windows kernel elevation of privilege exploit (EoP), bringing a flair for the dramatic.  After his first attempt failed, he proceeded to debug his exploit in front of the crowd while still on the clock.  His second attempt nearly succeeded, but the target blue-screened just as his shell started.  Oh, Richard!  But his third attempt succeeded with one minute and 37 seconds left on the clock."  How long do they give them?



STEVE:  Ten minutes.



LEO:  Ten minutes?  Whoa.  "In the end, he used two use-after-free bugs in the browser linked to an integer overflow in the kernel to successfully run his code with elevated privileges.  Nice going, Richard!"  That's you; right?  You wrote that.



STEVE:  Uh-huh.  I did.



LEO:  "His dramatic last-ditch effort earned him $70,000 and seven points toward Master of Pwn."  All right.  That's quite enough of that.  Quite enough of that.  But that is, it is dramatic.



STEVE:  Oh, it is.



LEO:  Have you ever watched one of these?



STEVE:  No, I have not.



LEO:  Be kind of fun.



STEVE:  Yes.



LEO:  I take it they have an audience.



STEVE:  Yup, they have a bunch of people watching.  So it's done in front of a group.  Actually, I misspoke.  It's 10 minutes average, so it's three attempts in 30 minutes.



LEO:  Oh, half an hour, I mean, you know.



STEVE:  Yeah, yeah, yeah.



LEO:  Not a lot of time.



STEVE:  Yes.  And so essentially...



LEO:  I love him debugging it on the fly.



STEVE:  Yes.  So, like, you know, the clock is ticking, and he's like, oh, crap, why didn't that work?  And so these guys know their stuff.



LEO:  That's pretty good.  Pretty good.



STEVE:  Yeah.  So anyway, essentially there were several Apple Safari escapes.  All of them were elevation of privilege exploits.  There was also one found in Edge that we just talked about, and one in Firefox.



LEO:  I'm going to read - I'll read you the next one because it's too good.



STEVE:  Oh, okay.



LEO:  I'm sorry, I'm just looking at it.



STEVE:  It really is, Leo.



LEO:  "Up next, Niklas Baumstark, or @_niklasb, from the phoenhex team targeted the Oracle VirtualBox.  Apparently not one for added intrigue, his exploit immediately popped not one, not two, but three different calculator apps to rub his success into Oracle's face.  His demonstration qualified as a partial success as he used an out-of-bounds read and a Time of Check / Time of Use to still earn him $27,000 and three Master of Pwn points.  It was a great demonstration, and we look forward to more of Niklas' research in the future."  What is he, 12?  Probably; right?  This kid's going somewhere, folks.



"The final attempt on Day One saw Samuel Gross, or @5aelo of phoenhex, targeting Apple Safari with a macOS kernel elevation of privilege.  Last year his exploit involved a Touch Bar component, and this year proved to be no different, folks.  Sam deftly leveraged a combination of a Just-In-Time optimization bug in the browser (whoops, Apple!); and then a macOS logic bug to escape the sandbox; and, finally, a kernel overwrite to execute code with a kernel extension and successfully exploit Apple Safari.  This lovely exploit chain earned him $65,000 and six points toward Master of Pwn.  And as he did last year, he left a message for us on the Touch Bar once he was complete."  What was the message?  They don't say.



STEVE:  I know.



LEO:  Something like FU, Apple; right?  Take that.  Holy cow.  He leveraged the Touch Bar.



STEVE:  Yes, yes.



LEO:  Unbelievable.  Unbelievable.



STEVE:  Yes.



LEO:  If you're one of these big companies, you've got to look at this day and go, oh, boy.  Is there video?  Wait a minute.  There seems like there might be video.



STEVE:  Yeah, there are videos of this happening.



LEO:  Of them doing this.



STEVE:  Yeah.  On the second day, it says:  "The day began with the return of Richard, this time taking aim at Mozilla Firefox with a browser kernel elevation of privilege.  He eschewed all drama today and successfully popped Mozilla Firefox on his first attempt with his clever use of an out-of-bounds write in the browser, followed by an integer overflow in the Windows kernel, to earn himself another $50,000 and five more Master of Pwn points..."



LEO:  I want to go next year.  I want to see this.



STEVE:  "...to bring his event total to" - yeah, yeah.



LEO:  Look, and he gets like a drone trophy.  No, it's a bug.



STEVE:  And he's very happy.



LEO:  That's funny.  Oh, that is so funny.  Good for Richard Zhu.  Yeah.  Well, you know what, I'll extend this offer to CanSecWest.  If they want me to do play-by-play next year, I'm available.



STEVE:  Well, that was great.  So the follow-up news is, and I don't know what this means:  DEF CON China is May 11, 12, and 13 coming up.  And so it's going to be held in Beijing.  And I don't know what this means in terms of the Chinese government policy relative to having a security show of that sort.  Maybe if it's within China's borders they'll allow their hackers to attend.  Because it's a standard-looking DEF CON layout and format and presentation.



LEO:  Well, I think that's the issue.  No hackers want to come to the U.S. anymore.  You can get arrested.



STEVE:  I know.



LEO:  Isn't it ironic that people will go to China?  It's safer?



STEVE:  Yup.



LEO:  Here's the message, by the way.  This is a picture of the Touch Bar.  It says "Pwned by 5aelo," and a happy face.



STEVE:  Nice.



LEO:  That's cool.  And all the codes above it.  No fake code there.



STEVE:  So, my friend, that's our podcast.  



LEO:  Our time is up for this fabulous episode of Security Now!.  Thank you, Steve Gibson.  We do this show every Tuesday, round about 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC.  If you want to watch live, you can, at TWiT.tv/live, by the way.  And if you're going to do that, you might as well get in that chatroom there, where there are some fabulous people chatting along and providing us with images and all of that stuff:  irc.twit.tv.



Now, Steve has the on-demand audio of the show, and he has something unique at GRC.com:  transcripts.  Elaine Farris will be listening to this and trying to write it down.  And then those handmade transcripts are great, though, for searching. So if you hear something, or you want to go back through one of the previous episodes - do you have transcripts for every episode?



STEVE:  Every single one.  She didn't start at the beginning, but after a while it was proving so useful that I said, okay, let's go back and start from number one.



LEO:  So you could search for "honey monkeys," it would take you right to number one.



STEVE:  Yup.  Got it.



LEO:  Yeah, I don't think we've mentioned them since.  We have audio and video, oddly enough, of the show.  You can get that at TWiT.tv/sn.  While you're at Gibson Research Corporation, Steve's site, GRC.com, don't forget to pick up SpinRite, right, the world's best hard drive recovery and maintenance utility, and check out all the other stuff going on over there.  It's a beehive of activity.  Steve wraps up SQRL, gets back to SpinRite 6 and all sorts of other fun projects.  InSpectre is still there.  Actually, you probably do want to get InSpectre these days, right, see what CPU ID you have.



STEVE:  Yeah, exactly, in order to see whether you've got new microcode available.



LEO:  That's GRC.com/InSpectre, spelled R-E.  Or, you know, if you Google "InSpectre" you'll find it right away.  It's number one.  



STEVE:  Yup.



LEO:  What else?  I guess that's about it except thank you, Steve.  And we'll be back next week, I hope you will, too, for another gripping edition of Security Now!.  Bye-bye.



STEVE:  Thank you, Leo.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#656

DATE:		March 27, 2018

TITLE:		TLS v1.3

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-656.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  We discuss the mess with U.S. voting machines, technology's inherent security versus convenience tradeoff, the evolving 2018 global threat landscape, and welcome news on the bug bounty front from Netflix and Dropbox.  We have the interesting results of Stack Overflow's eighth annual survey of 101,592 developers, worrisome news on the U.S. government data overreach front, some useful and important new web browser features, messenger app troubles, a critical Drupal update coming tomorrow, some welcome news for DNS security and privacy, a bit of miscellany, and a look at the just-ratified TLS v1.3.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  It's time to talk all the latest security news and, when we get to the end, a review of, finally, the arrival of TLS 1.3.  Oh, and if you're a Drupal user, you're going to want to stay tuned.  There's some very important news about Drupal security coming up.  Lots more, too.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 656, recorded Tuesday, March 27th, 2018:  TLS Happens.



It's time for Security Now!, the show where we cover your privacy and security online with the guy in charge at GRC.com, the Gibson Research Corporation's head honcho, Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  One might say the only honcho.  There's not a lot of honcho competition over here.



LEO:  I don't even know what a "honcho" is.



STEVE:  I don't, either.  Sounds like someone that rides on a horse, maybe.  I don't know.



LEO:  Yeah, Honcho meet Pancho.  Hello.  Happy Birthday, Steve.  I understand you had a celebration yesterday.



STEVE:  It was, yeah, March 26th is the annual anniversary of my existence on the planet.  And so far I'm still here.



LEO:  Yay.  Yay.  We're all relieved.



STEVE:  And speaking of "still here," so is TLS.  Today's title is TLS v1.3 Happens.  And when I mentioned that to you, you got all excited.



LEO:  Woohoo!



STEVE:  And I said, yeah, I guess that's exciting for a certain demographic.



LEO:  You know, the only reason I said that is it seems like it's been about to happen for ages.



STEVE:  Has been about to happen.  And in fact at the end of the show we'll talk about what it means.  But one of the things that I did was I plotted the length of time between TLS, or SSL as it was formerly named, sort of like Prince, the number of years between.  And the fact that it's been increasingly slow suggests that we're beginning to get it right.  And in fact one of the adoption protocols that we'll talk about for TLS reminded me of how right the way this was done is, and how wrong the way the Wi-Fi Alliance does theirs.  I mean, you couldn't find a more polar opposite approach where - well, anyway, we'll talk about that.



We had a lot of other news.  There's the mess with U.S. voting machines and the continuous concerns and politics surrounding this upcoming midterm election and then the next presidential election and what that means.  We've got an item that put me in mind of technology's inherent security versus convenience tradeoff, how we want both, but it's a tradeoff no matter what we do.



Symantec's annual global Threat Landscape Report is out and looking back at 2017 with some interesting numbers that we need to talk about.  There's some welcome news on the bug bounty front with Netflix and Dropbox in different ways.  The interesting results from Stack Overflow, which is a major developer site, you know, many times when I'm just googling something that I want to quickly understand or get some details of, Stack Overflow will pop up in Google search results.  So it's a big deal.



They have their Eighth Annual Survey, the biggest one they've ever had.  101,592 developers were asked stuff, which is interesting, and we'll talk about that.  We've got, and I heard you talking about this at least on MacBreak Weekly because I just had the show on as I was wrapping things up and getting things ready.  Worrisome news on the U.S. government data overreach front.  And I wanted to mention that I'm really liking the new Leo.  I don't know what happened to the old...



LEO:  What new Leo?



STEVE:  Well, the new "I'm concerned about privacy" Leo.  



LEO:  Oh, yeah, I have changed a little.



STEVE:  Rather than, "Ah, I don't care about this.  It doesn't matter, blah blah blah."  It's like, now there's a new Leo in town, and I think it's wonderful.



LEO:  Yeah, well...



STEVE:  A whole new Leo.  It's like, where did this come from?



LEO:  I don't think I'm the only one.  I think the Facebook debacle has really kind of widened people's eyes.



STEVE:  Yeah.  And so we'll also talk about that because we've got some messenger app troubles, Facebook Messenger among them.  Some important new browser features.  And you guys are Drupal-based; right?



LEO:  Yeah, we have a Drupal backend. 



STEVE:  So there is a critical, CRITICAL, all caps, preannounced Drupal update coming tomorrow which was announced last Wednesday deliberately with a week's advance.  Of course we didn't know about it last Tuesday for the last podcast.  It happened the day after.  So I want to make sure that you guys and our listeners all know, I mean, they're saying - they've declared, I think it's between 18:00 and 19:30 UTC.  I don't know what that is for normal people.  But, I mean, like tomorrow is the announcement with it is urgent that everybody update.  Apparently they expect there to be exploits happening within an hour of their announcement.



LEO:  That's at noon Pacific, I think, 9:00 a.m. Eastern, yeah.



STEVE:  Okay, good.  So we'll talk about that.  We also have some welcome news on the DNS security and privacy front, a tiny bit of miscellany, and then we'll wrap up with a look at the just-ratified, finally arrived, much-discussed, and somewhat controversial due to perfect forward secrecy, TLS v1.3, which has actually now landed.



LEO:  Wow.  Amazing.



STEVE:  Yes.  And a pretty fun Picture of the Week, too, while we're at it, which sort of ties into one of the things I want to talk about in our miscellany.  So a good podcast.



LEO:  All coming up.



STEVE:  So our Picture of the Week some clever person produced shortly after Elon Musk's stunning launch of the Tesla and the dummy in the spacesuit, the Starman.  I just got a big kick out of it.  I'm going to mention the Star Trek franchise in our miscellany section here in an hour and a half or so.  But anyway, the Picture of the Week is just - it's a kick.  It's the Enterprise Voyager crew, who apparently opened their shuttle doors and pulled in the red Tesla roadster.  One of them is aiming, I can't remember his name now, aiming a tricorder at it, and the Vulcan guy has got his hand on it, and they're sort of standing around thinking, what the heck is this thing doing?



LEO:  It's V'ger.  I love it.  I love it.



STEVE:  Well, anyway, just a fun little bit of just, yeah, love the creativity of the Internet.  So for the sake of just not leaving something out that probably doesn't really matter, or I guess it matters because it just demonstrates sort of how nothing is happening that should, what made the news is that the U.S. Department of Homeland Security has designated election systems as a subsector of the country, the U.S.'s critical infrastructure after the intelligence community broadly concluded last year that Russia had tried to interfere with the 2016 U.S. presidential election.  As a consequence, it turns out there's something that I was really never very aware of.  They're called ISACs, I-S-A-C, which is the abbreviation for Information Sharing and Analysis Center, which is our tax dollars here in the U.S. apparently spinning around in circles and getting nothing done.



LEO:  Oh, man.



STEVE:  Look at this website.  I have a link here in the show notes:  NationalISACs.org/member-isacs.  And so there's already a bunch of them.  Who knew?  And I'll just give you - I'll read down to the I's in alphabetical order:  automotive, aviation, communications, defense industrial base, downstream natural gas - as opposed to upstream natural gas, I guess - electricity, emergency management and response, financial services, healthcare, information technology.  Okay, that gives you an idea.



So who knows what this is.  I mean, it just - it's bureaucracy at its finest.  And the problem of course is that - so the idea is that states and localities are supposed to become members of the election system's ISAC, and I don't know what happens then.  I mean, the problem is, and we've been discussing voting machines for years, which arguably are, based on last year's Black Hat and Defcon, horrifically insecure.  I mean, we've talked about unauthenticated remote WiFi access, Evil Maid access where there's like an active USB port on the side, and you can just kind of sally up next to the machine and slip in a thumb drive and take it over remotely and make it reboot and dump its contents out.



And we were talking earlier this year about the problem that the Black Hat and Defcon conference organizers are having for this coming summer's events because now the people who have traditionally been reselling these machines are being illegally threatened by their machines' manufacturers against reselling, with attorneys writing them fraudulent letters claiming that it's illegal to resell them.  And certainly after, what, we're approaching the end of year 12 now, and nothing could be more clear to us and our audience than that this whole process is completely bass-ackwards.  I mean, completely broken.



What there should be is the kind of process that we'll be discussing, that adopted and ratified TLS v1.3, which is a standard that industry and academia together worked on, polished, refined, argued over, fought over, wrestled to the ground, tested, implemented, and just absolutely nailed.  And now it is being adopted.  That's the process that we should have for something in many ways sort of similar to, that shows many similar characteristics to voting automation.  The idea that Diebold - pronouncing the name correctly this time - could offer for sale a black box that people adopt and use for something as important as voting, where it's, oh, this is our proprietary technology, and you can't see inside, but trust us.  No.  No.



I mean, and it's just - it's insane that, unfortunately, this aspect of capitalism has been allowed to take hold and be employed in the U.S. or any election system.  It ought to be an open standard, developed in full view.  And then, yes, I have no problem if then companies want to compete on implementing the standard which is created, which is what we do now, where we have other standards.  The standard is open.  You implement the standard.  It is verified that you have done so correctly.  And then you're welcome to sell your hardware that runs the system that has been universally agreed upon and pounded on.  You're welcome to sell that.  Fine.  But this notion that, oh, we've got the greatest security experts; and, after all, we're us.  Who cares?



Anyway, so nothing is happening.  And I don't know how, like how long it's going to take for anything real to happen.  But as long as we allow closed architecture voting, which is what this has become, I mean, and this is why now they're saying, okay, well, let's get a paper trail.  Okay, well, okay, I guess that's better.  But it still seems problematical.  So this is a problem we know how to solve, but unfortunately commercial interests in this country are preventing its proper solution.  So who knows when that'll change.  However, creating a new bureaucracy to talk about it doesn't seem to me like we're moving in the right direction.



The Intercept had an interesting article last week where they probably wouldn't surprise anyone to note that the NSA had a system using some of their crazy project names, in this case OAKSTAR and MONEYROCKET, which tied into the XKeyScore search system.  It turns out that, since 2003 at least, the NSA has been actively tracking bitcoin use.  And certainly who would this surprise?  Maybe what is surprising is that there was a deliberate project designed to do this.



So it is the case that, when we first started talking about these nodes on the Internet that were installed in high-throughput datacenters where the NSA was basically monitoring all traffic that passed through, we spent a lot of time talking about what that means and the consequences of it to privacy and HTTP versus HTTPS and all of that.  It turns out that one of the things that was already being done in the auspices of these OAKSTAR and MONEYROCKET projects was that traffic was being analyzed for bitcoin addresses, and the NSA was already - they'd already rolled up their sleeves some time ago and were essentially way ahead of the rest of pretty much the academic community, which came later, and said, you know, this isn't really as anonymous as everyone seems to think it is.  And this blockchain that is creating an audit trail can itself be audited, and we can figure out where the money is going.



Well, the NSA was doing that quite some time ago.  So this is what put me in mind of noting that we really cannot have it both ways.  We want convenience.  And I would argue that we can have the opportunity for security.  That is, with limits, security can be had on the Internet.  And we'll be talking a little bit later about some of the revelations regarding Facebook and their Messenger that again demonstrate that it really is illusory beyond a certain point; that it is really, if you're relying on technology which you didn't create, if you're relying on any magic, any hocus-pocus, any black box, then you actually can hope, and maybe you have the opportunity for security, but there's no guarantee.



And we've talked about how, if you really want to have a secure communication with someone, you've got to meet them naked under a thick blanket in the middle of a football field and whisper into their ear.  Otherwise, if you're using any technology, it's just not secure.  And of course this has been known for a long time.  Phone tapping and tracking people's cellular phones, even before we realized it was actually happening, it was the fodder for fictional stories, and at one point it was seemingly fiction.  Now it turns out, no, it's something that's just going on all the time.



So anyway, I thought it was interesting that people are still seeming surprised to some degree that this is going on.  And I think our takeaway is that the Internet provides, again, sort of opportunistic security.  But the moment you rely on something you didn't create, the moment you rely on something you don't have absolute control over, then you're making an assumption that it's safe, which may or may not be true.



LEO:  But, I mean, this is modern life.  We're all interdependent in modern life.  You drive down the freeway at 60 miles an hour in a two-ton hunk of metal, and you just hope that the parachute's been properly packed.  I mean, there's no - you know?



STEVE:  Right.



LEO:  And that the guy coming in the other direction didn't have one too many.  I mean, it's society.  Really, the truth is it's a problem of civilization as much as anything else.  We're interdependent.



STEVE:  Well, yes.  And look at the convenience that we get.  I mean, look at the leverage this provides.  I'm not suggesting - we all know I'm no Luddite.  I'm not suggesting we roll things back and give up electrons.  I love electrons.  But I think that's the point is that there's this, you know, in hearing people talk, there's this notion that security can be an absolute.  And clearly one of the prevailing lessons of the many years of this podcast is that, no, there really isn't an absolute.  The more interest there is in penetrating security, the less secure it is.



LEO:  And I think also it's important, part of the equation that we don't really get too much into is the hazards of the risks, what the risks are.  For instance, there's only four days' food supply in the United States.  You want a real problem...



STEVE:  Yeah.



LEO:  You know, just interrupt the food supply and watch what happens.  So that's a serious risk, but a difficult thing to accomplish.  Hacking the grid, a little bit easier, apparently.  In fact, apparently it's already happened.



STEVE:  A little too easy, yes.



LEO:  With some pretty severe consequences.  Somebody knowing where I get my hair cut isn't the end of the world.  The hazards are lower or lighter.  I mean, they're there.  And as you point out, I've recognized recently that they're significant.  But I don't think it's like somebody's going to haul me away in chains, either.



STEVE:  Well, no.  And I guess I would argue that this is, I mean, not like any of us are contemplating being criminals.  But it seems like a really bad idea these days because...



LEO:  Yeah.  You would think.



STEVE:  Yeah.



LEO:  But don't the criminals get enabled, too?  I mean, that's kind of why I am against the idea the Department of Justice has been floating of the hidden keys in the phones that - it's something you talked about.  It would be one good solution to this is a unique key that is unlockable with a second lock by the manufacturer should law enforcement need it.  But at the same time, I don't know.  It's kind of...



STEVE:  Well, in fact, yes.  Russia has given Telegram 15 days from last Tuesday.



LEO:  Right.  Right.



STEVE:  So it was last Tuesday that the high court in Russia said to Telegram, you turn over your keys, or we're shutting you down.  And Telegram intends to fight and appeal.  But they got shut down by China because...



LEO:  We knew Putin passed this.  This was passed a year ago.



STEVE:  Yes.  Yes.



LEO:  I'm not willing to give up.  You've got to just - what it is, is knowledge.  You've got to know what the tradeoffs are, what the risks are.



STEVE:  Yes.  Yes.



LEO:  And we have a benefit by using all this stuff.  Is it worth...



STEVE:  I would argue a massive benefit.



LEO:  Massive.  And this is what Jeff Jarvis is always saying, too, is that is it worth the tradeoff?  You just need - it's about knowledge, not about walking it into ignorantly.



STEVE:  Yeah.  So, okay.  I got a kick out of Symantec's number for the explosion in cryptojacking.  Because, you know, when the denominator is very small, even a modest numerator results in a very large quotient.



LEO:  Oh, Steve, math is hard.  Why are you bringing up math again?



STEVE:  So Symantec, and we talk about this every year because it's always interesting stuff, they published for 2018 their Annual Internet Security Threat Report, looking retrospectively back on what happened in 2017.  Well, now, of course we've been talking about cryptojacking.  And in fact, yes, it is the new thing.  It is the latest and greatest means for bad guys to try to make money.  But the headline is "Cryptojacking attacks explode by 8,500%."



LEO:  Wow.



STEVE:  And it's like, whoa, that must just be - how are we even operating?  And of course I'm put in mind of the fact that there was probably one cryptojacking attempt at the end of 2016.  And so, yes, by comparison...



LEO:  There's 8,500...



STEVE:  ...that is a massive [crosstalk].



LEO:  What is cryptojacking, just so we know what's increasing by 8,500%?  Ransomware?



STEVE:  No, no, crypto mining.  It's running miners on other people's stuff.



LEO:  Oh, that thing.



STEVE:  Yes.  And of course, if you count all of the browsers...



LEO:  It's through the roof, yeah.



STEVE:  ...that visited Coinhive recently, I'm sure that's a bunch.  So it's like, okay, fine.  So, yes, no surprise here that cryptojacking is the big deal.  What was interesting in their report is that they're hypothesizing that the next target will be IoT.  And, okay, maybe.  So we know that end user browsers were the first target, a la Coinhive, because you could just run a Monero miner on somebody's computer, and they'd wonder why their page was so slow until it turns out that an ad or a compromised website had injected crypto mining code in their browser page, and the computer was like, oh, my god, really?  This is the worst JavaScript I've ever had to interpret.



So next we covered that higher power servers have since been hit, and that, oh, there's money in those servers.  If you can get some serious metal cranking hashes, and a bunch of them all aimed at the same cryptocurrency account, it adds up.  So there's money there.  Now, of course the next question would be, okay, what about IoT?  And you would think just based on the number of light bulbs and Nest thermostats and door locks and alarm systems and DVRs and routers, certainly lots of routers, that, wow, okay, there is a target-rich environment.



The problem is that this is a little more like browsers, where the value proposition from a browser was difficult to make, which was why there was the switch to high-powered servers because this really is all about how fast can you hash?  And any consumer device is super cost-sensitive.  And so the processing power is scaled down in your average light bulb to just the bare minimum to decide what color it should be and to get it onto your network and hope.  So it remains to be seen whether, I mean, now, maybe routers - routers have some beef behind them.  So I could see consumer routers being targeted.



But I would be surprised if this penetrates much into these devices which are just barely online.  I mean, they're online enough to receive a packet.  And I would argue they make - and I'm not suggesting this - that they make better attack platforms for DoS because it's easy, takes very little processing power to emit a packet.  And they can do a lot of damage at the receiving end, especially if it's being leveraged off a memcached server.  But there's no way to short-circuit the proof-of-work required for crypto currency.  And, I mean, that says it, "proof of work."  It's hard to get much work out of a light bulb.  It's just, you know.  So I think we'll have to see whether that's ever going to happen.



LEO:  We make it up in volume.



STEVE:  Exactly.  Now, they did say, Symantec said that they logged 1.7 million cryptocurrency-driven intrusions during the month of December 2017 alone.  But again, that can't even be servers.  That's got to be browsers.  So they're casting a wide net for their definition.  The report went on to say that lower tech targeted phishing remains the number one way the majority of attacks begin.  That's still the way people get in is they send somebody a seductive email that says - and my own email system sees those constantly, just people sending you some nonsense with a zip file saying that you've won something, or DHL needs your signature release to deliver the package, I mean, just random stuff.



LEO:  Or the IRS; right?  I should play you this voicemail that Lisa got from the "IRS" saying the sheriffs are on their way.  They're coming for you.  It's all fake, yeah, but it works.



STEVE:  And I think it might have been on mainstream media, like on TV, that someone I saw, it was either that or it was on one of your weekend shows, someone saying this is not the way the IRS works.



LEO:  No, they don't do this, no.



STEVE:  Exactly.



LEO:  No, no.  But it's this time of year it happens.  It's very sad, really.



STEVE:  Right.



LEO:  It doesn't get you and me, it gets elderly people who are kind of a little clueless about technology or about how things work, and they get scared. 



STEVE:  Yeah.  So the number is interesting:  71% of attacks are traceable back to spearphishing, 71%.



LEO:  Wow, that's interesting.



STEVE:  Yes, nearly three out of four attacks begin that way.  So it is still, I mean we know that that's the way that the APT, the Advanced Persistent Threat got into Sony, was that we tracked it back to an administrator that opened a piece of email and clicked on something that he or she shouldn't have, and that allowed somebody in.  And the concern is that, as I've often commented, it's amazing to me that all these servers are being compromised just for their CPU power for cryptocurrency mining.  Nobody much cares, it seems, that they just did establish a beachhead in someone's network.  I mean, if you're in their server, you can, like, look around and see what else delicious might be there.



LEO:  Yeah, that's much more concerning.  



STEVE:  Yes, yes.  And instead it's just like, oh, how strong is this processor?  Let's squeeze it for something.  Let's squeeze some coin out of it.



LEO:  Coin out of it.



STEVE:  So the concern, though, is in a targeted attack, an enterprise's network is the target, that is, they went in for a reason, and then they tend to set up shop.  And one of the things that Symantec finds when they do forensic after the fact, after the discovery that, ooh, bad guys are in and have been rummaging around, is they're often shocked to see how far back the trail of evidence goes.  So it's like, yikes, they've been here for three years, and no one's spotted them until now.  And so they get in, and then they set up shop, and they sit there watching what's going on.



LEO:  That's why you need a canary.



STEVE:  That's right.  That's a very good point.  Another aspect that Symantec put their finger on that I was glad for, because we've looked at this, and that is that we are seeing an almost doubling in the instances of so-called software update supply chain attacks.  Remember that CCleaner got hit last year where a compromised version of CCleaner managed to get onto their download server.  And so even though you were actually getting it from the legitimate source, you were downloading a compromised binary.  And so that's the so-called "software update supply chain," where we have all of our stuff is now updating itself all the time, and we're hoping that the servers from which we are obtaining that code are strong and have not been compromised.



But, boy, that's a powerful attack.  If you can get something malicious into a popular supply chain, well, in fact, that's the way Petya managed to get into the Ukraine was through compromising a legitimate accounting software app that then updated itself and allowed Petya to get in and then spread laterally through corporate networks across the globe.  So that's something we have covered and that we've been seeing.



And then, of course, mobile malware continues to be a challenge.  Again, Symantec had some interesting stats, and that was that about 20%, only in the Android world because, as we know, the Android supply chain ecosystem is, by design and for the sake of some increased freedom, much less tightly controlled than iOS's chain, which Apple has a stranglehold over.  In the Android world, only one in five devices are running the latest major release of Android, and only 2.3% Symantec says are up to date with the latest minor release.



So the other lesson we know is worrisome as the software update supply chain compromise is still the best advice is to keep your devices current and updated as much as possible.  And that's just more challenging to do if you're an Android user.  You may want to be doing it and just not have it be easy.  It's a little bit like the position those of us with older laptops are in now with BIOSes not being updated for the Meltdown and Spectre vulnerabilities because they're just not going to get an update.  Their manufacturers are like, eh, no, that was then.  And so we can hope that Microsoft will be dealing with that.



So anyway, I thought that they had - Symantec always has some interesting numbers from their annual report, and this was no different.



So speaking of the wild future...



LEO:  Yes?



STEVE:  There's something interesting happening, and to me it demonstrates an evolution in the industry which is good, and that is that Netflix has launched a public bug bounty program.



LEO:  Oh, good, good.



STEVE:  Of course we were talk - yes.  We were talking last week about the big and oddly time-limited, but okay, because it's big I guess they had to, Microsoft and Intel bug bounties.  Remember those were for up to a quarter million dollars each if you found some previously unknown side-channel attacks.  Last Wednesday Netflix announced it would be expanding its own bug bounty program, opening it to any white hat hacker and increasing their top reward to $15,000.  Now, what's interesting is that there's a company, a venture-financed company, Bugcrowd, which allows any registered hackers, so I guess you register as a hacker, to scour Netflix mobile, cloud, and software platform for minor and/or critical bugs; and they'll have a value, depending upon how big they are, between $100 and $15,000.



And so Netflix is doing this through Bugcrowd, and the bounties can be used with the Netflix.com website experience, as they put it, as well as their Android and iOS mobile apps, which of course are super popular, used by 117 million Netflix users.  According to Bugcrowd, the typical Netflix bug bounty payoff historically - because Netflix was using them, but it was a closed program.  I remember we talked about it, where a hundred developers, like a hundred known hackers were invited into the program, and there was a low ceiling on how much they could make.  And I remember kind of being, like, oh, okay.



Well, anyway, now it's wide open.  Anybody who is a registered hacker with Bugcrowd can do this, and the limit is set at $15,000.  And I think Bugcrowd as an entity is going to be worth us keeping an eye on.  They describe themselves on their site as a "Radical Cybersecurity Advantage:  Managed Bug Bounties for the Enterprise."  And last month they took in a 26 million round of funding and expanded into London and Sydney.



LEO:  You know what this is, this means that there are a lot of people who make this their living.  This is how they make a living.



STEVE:  Yes.  And in fact, this is exactly to my point, Leo.  This seems like a good thing.



LEO:  If you're good enough to do this, there are people who, you know, this is their business.  They find bugs.  Like Dan Kaminsky; right?



STEVE:  Yeah.  And the whole idea of bug bounties, I think, is worth thinking about like just as a thing.



LEO:  It's like hiring a hired gun.  Find our flaws, yeah.



STEVE:  Yeah.  Well, and as sort of a meme in the industry, what this suggests is that, as we have been saying, a company is inherently unable to police itself.  It just - it's got management that's pushing its developers to ship stuff before the developers want to because they engaged all of their other corporate infrastructure and got committed to a release date.  Or it's the problem of coders being unable to, I mean, just coders' egos being unable to see their own mistakes.



I mean, and I've talked about this often.  The reason you use a debugging tool is you can stare at your own code.  You know there's a bug right in front of you, and you're looking right at it, and you cannot see it.  You just - you can't, until you finally single-step the debugger onto the error, and it goes [buzzer sound].  And it's like, ooh.  Then finally there, when your face is just rubbed in it, and it's like, oh, I mean, I love that.  I just love the experience because it just sort of brings you up short and says, okay, yeah.  You did forget to zero that register.  Or, yes, that just did overflow the size that you were sure would be plenty large enough or didn't even think about, or the compiler caught you out and said, oh, we're going to optimize this because who needs this extra thing.  And it's like, no, no, I meant that on purpose.



So, you know, bugs come from all kinds of different places.  But so what this is, is we're beginning to see the formalization of third-party external paid challenging of code.  And I think this represents an important maturation of this whole cyber ecosystem that we're living in.  Too often what we see are things like we've seen in the past with the Wi-Fi Alliance, where nobody challenges anything.  They just proclaim something as, oh, here you go.  Everybody gets a sticker on their router if they followed our specs.  And then it turns out one after the other they're just a disaster.



So instead we have something like the TLS effort that has been painfully and carefully developed over time.  But that's expensive.  I mean, I would argue no commercial entity can afford that kind of process for something that they're trying to get to market quickly, as everybody is always trying to do.  So if you're not going to do the 10-year next version, the point release of TLS, then you're just going to ship something.  So it's a perfect solution to arrange for somebody paid and coordinated in a proper fashion, which is what Bugcrowd does.



Many mature enterprises would be happy, much as Netflix has said fortunately they are, to pay somewhere between $100 and $15,000, averaging at a little over 1,000, for someone finding a bug.  Thank you very much.  I mean, I would love it if someone found a bug and said, "Hey, Steve, here's something you missed."  Yes, thank you.  So anyway, I just think - so I'll bet you this is something we're going to see.  I mean, we are seeing this notion of bounties more and more.  I just think they make sense.



And to that end, also last Wednesday, Dropbox updated its vulnerability disclosure policy to clarify its relationship with cybersecurity researchers and also to offer, to propose, essentially, a standard for the rest of the tech industry to hopefully follow.  Among other things, Dropbox has pledged that they would not bring a DMCA claim against good faith participants in their bug bounty program, which in retrospect seems obvious, but I think it's good to make it clear.



So what they're proposing, they had a series of standards:  a clear statement that external security research is welcomed; a pledge not to initiate legal action for security research conducted pursuant to the policy, including good faith accidental violations.  Yes.  A clear statement that we, they say, Dropbox, consider actions consistent with the policy as constituting "authorized" conduct under the Computer Fraud and Abuse Act (CFAA).



Also a pledge that we, Dropbox, won't bring a DMCA (Digital Millennium Copyright Act) action against a researcher for research consistent with the policy; a pledge that, if a third party initiates legal action, Dropbox will make it clear when a researcher was acting in compliance with the policy and was therefore authorized by us.  A specific note that we, Dropbox in this case, won't negotiate bounties under duress, meaning, for example, if you find something, tell us immediately with no conditions attached.



Also specific instructions on what a researcher should do if they inadvertently encounter data not belonging to themselves, so like a hold harmless policy.  And a request to give us - and this is significant relative to the AMD disclosure - a question to give us reasonable time to fix an issue before making it public.  They say:  "We do not, and should not, reserve the right to take forever to fix a security issue."  But they would like to have time to do so.



So again, I think this again moves us in the right direction.  This says that, in addition to a bounty program to reasonably incentivize people who like to do this kind of reverse-engineering and want to find problems, I mean, I think this should be a career.  And as we begin to see moves like this by Dropbox where they say, yes, we're going to make it clear what our policies are about reverse-engineering our stuff and attaching a reward to it; and, exactly as you said, Leo, this could become something that someone could support themselves doing if they could become good enough at it.  And lord knows it's a target-rich environment out there.  I wouldn't be at all surprised if you could support yourself.



LEO:  Yeah.  I think this is really interesting.



STEVE:  So Stack Overflow, as I mentioned at the top of the show, a very popular and well-known developer-centric site, basically my interaction with them is they're just a big online forum, well indexed by Google.  And so it's often the case that when I'm searching for some random, like why doesn't this seem to be working, has anybody else ever encountered this before kind of thing, I'll put in a query, and up comes Stack Overflow with a voting and ranking system to sort of separate the wheat from the chaff and allow me to see what's going on.  They have for eight years been publishing a developer survey where they just put out a survey to the people visiting the site saying we want to know what you guys are doing.  What's important to you?  What do you like?  What do you wish for?  What stuff are you using?



So what did they learn this year?  At almost 70%, 69.8%, JavaScript has remained the most popular programming / scripting / markup language for the sixth year in a row.  They've been doing this for eight years.  So JavaScript, number one.  Then in second, and almost in third place, at 38.8 and 34.4, Python has just bypassed C# after it passed up PHP last year.  So that makes Python the number one fastest growing language.  As for frameworks, at just shy of 50% (49.6%), Node.js has remained the most popular framework.  And barely, almost neck and neck, 27.8% vs. 27.2, React has surpassed .NET core to enter the top three frameworks.



58.7% has MySQL as the most popular database technology, where it has been for all eight years of the survey.  And SQLite dropped from number three to number five, now just shy of 20%.  Rust ranked as the most loved programming language at 78.9%, with Kotlin taking second place at 75.1 and Python in third place at 68%.



I got a kick out of this:  89.9%, so just shy of 90%, has Visual Basic 6 as the most dreaded language for its third year with that dubious honor.  And what was interesting was number two, okay, VB6 is number one.  That puts it above Cobol in dread ranking, probably because people actually encounter it.  I can't imagine anybody seeing Cobol anymore these days.  But anyway, yes, Cobol would be dreaded, too.  But not quite as much as Visual Basic 6.



For the second year in a row, developers chose Python as the language they most wanted to learn, but unfortunately their employers have them using JavaScript at the moment.  So maybe they'll get to Python if they get a different job.  Visual Studio Code and Visual Studio ranked first and second as the most popular IDEs, although looking over that category they seemed more like editors to me.  For example, Notepad++, Sublime Text, and Vim were the third, fourth, and fifth places.  So it's like, okay.



Android Studio was the most popular IDE among mobile developers, and Vim the most popular among sysadmins and devops.  And, not surprisingly, as with years past, Windows remained the developers' primary operating system, followed by macOS and Linux, respectively.  And, let's see, one, two - I have five.  More than half of all developers use two monitors.  Some of us  use five.  I just love screen real estate.  It's funny, I have Lorrie set up with a large monitor now that's perched over her laptop, which she uses.  And she says, "I can't imagine how I lived without this before."  It's like, yeah, I know, it spoils you to have a bunch of screen.  And at 16%, the number one ranked focus for developers is web.



LEO:  Yeah.



STEVE:  So, wow.



LEO:  It's not surprising, yeah.



STEVE:  Yeah, yeah.  So there is the full survey at Stack Overflow, although Bleeping Computer, I have a link in the show notes, has a very nice breakdown from which I was able to quickly pull these numbers.  So a tip of the hat to Bleeping Computer for their coverage of this.



And as I alluded to at the beginning of the show, and Leo, as I heard you talking about, no doubt over the weekend, but also just on MacBreak Weekly, the CLOUD Act, which was essentially snuck into the big $1.3 trillion omnibus spending bill which both houses of Congress passed last Thursday.  I saw a picture of this thing sitting on a table next to our President, who was announcing - it was at least 18 inches high of printed paper.  And I don't know why, I mean, it's horrific to imagine that that's the legislation.  I think it was 2,200 pages.



Anyway, this was first introduced, this notion of the CLOUD Act was first introduced in the middle of February.  And it sort of languished because, well, because we needed more time to think about it.  It's regarded as the government's response to Microsoft's refusal five years ago to turn over U.S. citizens' data which was residing in their cloud servers in Ireland.  So CLOUD, it's a cleverly named acronym.  CLOUD actually stands for Clarifying Lawful Overseas Use of Data.  And in this case we can imagine what "clarification" amounts to.  The EFF has been blowing a gasket over this, as you can imagine, ever since it surfaced.  They consider, and their page on this calls this a backdoor to the U.S. Constitution's Fourth Amendment, which was our protection against unwarranted search and seizure.



So essentially, as you've been saying, Leo, and I just wanted to bring it to our listeners' attention, this is now law.  This CLOUD Act was slipped into this must-pass budget spending bill in order to make this law.  And this effectively eliminates the need for search warrants or probable cause for obtaining U.S. citizens' data stored online overseas.  And as a consequence - oh, and also it supports international cross-border treaties so that we're able to make foreign citizens' data available to other governments.  And of course in a treaty they're able to make U.S. citizens' data available.



So anyway, this has got a lot of privacy watchers and security people very much up in arms and upset, although we already had this, as I understand it, this international treaty mechanism in place.  Obviously it wasn't clear, thus the clarification, and Microsoft's ability five years ago to argue against agreeing to produce this data that was stored offshore.  I think that in the future the law we have now says, sorry, we're not going to be able to provide that protection to our customers.  Did you have anything more, Leo, about this? 



LEO:  No.  Yeah, I mean, we've talked about it a lot, but don't know what to do.  [Crosstalk] right through.



STEVE:  Yeah, it's unfortunate, yup.  Chrome and Opera already support, as we have been discussing, built-in ad filtering.  Firefox will be joining them.  It's slated for the third quarter of 2018.  And, boy, any Firefox enthusiasts, I would recommend checking out their Roadmap Wiki.  Firefox's Roadmap is quite exciting.  I mean, I guess any browser's roadmap is going to be exciting, just like looking at the things that they're working on and that they're planning to do.  As I scanned through it, I thought, ooh, good.  This is going to be a good year for Firefox.



Of course, many people are taking a look at it again as a consequence of their quantum technology, which really gave it a big boost in speed.  What the Roadmap Wiki announces is a built-in, default-enabled abusive ad blocker, which will be appearing, as it has with Chrome, later this year, and enabled by default.  Those of us who already have uBlock Origin or Adblock Plus or one of those add-ons, already have a lot of this sort of protection.



And I have to say, as I have mentioned before, I'm always amazed and a little bit chagrined when I use any browser that has zero content filtering.  Depending upon where you go, some sites are virtually unusable just with all of the nonsense visually that you're assaulted by, not to mention how much slower they seem to operate as a consequence of them having to pull all this stuff down into your browser.  So for the typical user who isn't already adding some sort of content filtering technology to their system, it'll be nice to have Firefox keeping pace.  And do take a look at the Mozilla Roadmap if you're a Firefox enthusiast.



The concern over homograph attacks is growing because the use of the attacks is growing.  Homograph attacks we've talked about before.  They leverage the use of Unicode in domain names.  Now, the intent is to support large character set Unicode to be able to allow the display of non-ASCII-based domain names.  The problem is that, as soon as you allow Unicode, you open yourselves and users to spoofing and fraudulent domain names.  There are all kinds of examples.



But basically Unicode, due to the fact that there are so many lookalike characters, it's possible to create, I mean, we're always giving PayPal as an example.  There are, for example, characters with, like, little dots below them that you might not even notice that is a different domain name.  Well, if the browser shows you the Unicode version, what you see looks like PayPal and isn't.  So browsers have typically solved this problem by showing you, and the name is sort of a play off of Unicode, they have Punycode because there is an ASCII representation of Unicode using standard ASCII.  It uses hyphens and numbers in a sort of a weird ASCII construction.



The point is that, if you show the Punycode rather than Unicode, there's no way anyone is going to be confused.  PayPal with some Unicode rendered as Punycode looks nothing like PayPal.  So Edge, for example, Microsoft's Edge browser, does not render Unicode in the address bar.  It shows you the ASCII Punycode.  Chrome has kind of a compromise.  They show the ASCII, that is, the Punycode, in the title bar, but I guess to be a little more friendly they show the Unicode in the URL address bar, which is a little worrisome to me with Chrome being the majority browser and so many non-savvy users.  I mean, if you didn't look in the title bar for the page and see something crazy, the Punycode, then you could easily click okay before realizing you are not at the proper domain.



The good news is there is an add-on for Chrome produced by Phish [P-H-I-S-H] dot AI, called IDN Protect.  And IDN is International Domain Names.  So if you google "Phish.AI IDN Protect," you'll get a link to their Chrome add-on in the Chrome Store, and also you'll see their GitHub link because the GitHub page for this add-on is also there.  Firefox does show, unfortunately, and I must have talked about this before on the podcast because when I went to check it I had flipped the default.  By default, Firefox will show Unicode, but you can change that.



So as always, go to in Firefox about:config and just search for "puny," P-U-N-Y, and you'll get one item will survive the search weed-out.  And mine was already dark, meaning that it was the non-default setting.  It had been set to dark for IDN_show_punycode.  And that's what you want.  I can't imagine, I mean, unless you're a very careful international web user who really wants to see things in Unicode, the danger is big.  Edge won't show it to you at all.  Chrome won't show it to you in the title bar.  It will in the address bar, unfortunately.  You can disable it in Firefox.



And I would argue, if you want to protect your friends and family, consider this IDN Protect.  What it does is it will detect if a domain name has Unicode and just put up an intercept, just to bring your attention to the fact that the page you are at has a Unicode domain name.  Inspect it carefully.  Make sure you're where you want to go.  We are seeing more instances of this being used for spoofing.  And anybody can register a domain name.  Anybody can now easily get an HTTPS certificate for it.  You'd have a hard, well, you wouldn't be able, probably, to get an EV cert; but you could certainly get a TLS connection showing a secure connection, so all of the other window dressing that makes someone feel secure, yet they really shouldn't be.  So I would argue that, if you're a Firefox user, turn off showing Punycode for Firefox and consider this little Chrome Store add-on just to make sure you notice if you're going somewhere you don't expect to be.



And Leo, I know you talked about this before.  I just did want to make a note for our podcast that Facebook was found - of course they've been much in the news lately over the whole Cambridge Analytica mess with the election.  But it was discovered by some users who downloaded their Facebook history that, to their absolute surprise, a complete log of their use of SMS texting and telephone log was part of what Facebook was collecting.  And really it looks like Facebook has not been forthcoming about this.  Facebook argues that they always asked for permission.  But Android did not require apps to get permission initially.  They then have successively tightened that up.



I mean, all the evidence suggests that Facebook was acquiring this data without their users' permission.  A number of very security and privacy-conscious users are absolutely sure they never gave permission, and in some cases installed the Facebook Messenger app several times, always being careful about what permissions they gave, yet Facebook was collecting this data without permission.  So anyway, just a heads-up to users of Facebook on Android, Facebook Messenger on Android, that this had been going on.  And I don't know if it makes sense to, if you can, purge it?  I know that, Leo, you have famously just proactively deleted your Facebook account.



LEO:  I undid it, though.  I had till the end of the month.  And you know why I undid it?  My wife guilted me into it.  She said, "Because you've deleted your Facebook account, it now says Lisa Laporte is married, but not to whom."  And she wants it to say to me.



STEVE:  Well, that's sweet, I guess.  



LEO:  Yeah, how can I say no?



STEVE:  But she would be married to someone Laporte.



LEO:  No, no.  The name disappears.



STEVE:  Well, but Lisa Laporte.



LEO:  Well, yeah, I know, but I don't - it's really interesting.  So this is how normal people react.  Your whole life, her whole life history she feels has disappeared; right?



STEVE:  Wow.  And were you able to, like, expunge all of this other stuff?  I mean, so you did this to make a statement.



LEO:  Yeah, because they already - I know they have everything they want, and they've had it for a long time.



STEVE:  And Leo, you're not a big mystery to the world.



LEO:  No, I'm not anyway.  But I feel like, yeah, I want to send a signal to Facebook that this isn't okay, and then by extension to everybody else that there is a price to be paid, that consumers aren't pushovers here.  And so I really did want to do this.  But you know what they say:  "Happy wife, happy life."  I don't care that much.  I care more about Lisa.  But there's no way of verifying that they deleted everything.  I mean, who knows what they do?  We know they lie.



STEVE:  Yeah.  Yeah.



LEO:  I don't know.  You know, it didn't matter that much to me, but I did want to send that signal.  On the other hand it's - obviously more people join Facebook every day than they'd ever possibly lose.  So it's kind of a hopeless thing.



STEVE:  Well, and there's all this talk now about regulation, about social media.



LEO:  Yeah, we're glad to see that.



STEVE:  And so we don't know what's going to come of it.  But it really, I mean, it is a big deal.  And even Marcus being quoted saying, "Yes, I think we should be regulated," it's like, oh, okay.



LEO:  Yeah, well, what he's really saying is just a little bit.  A little bit.  It's prudent to ask for some regulation.  Please sir, hit me. 



STEVE:  So last Wednesday they put out a deliberate one-week notice of what they called a "highly critical release" to go public on March 28th, which is tomorrow.  And so they wrote for a description:  "There will be a security release of Drupal 7.x, 8.3.x, 8.4.x, and 8.5.x on March 28th, 2018 between 18:00 and 19:30 UTC, one week from the publication of this document, that will fix a highly critical security vulnerability.  The Drupal Security Team urges you to reserve time for core updates at that time because exploits might be developed within hours or days."



LEO:  Because Drupal culls its PHP code.  As soon as there's an update, they start looking at it; right? 



STEVE:  Yup.  Security release announcements will appear on the Drupal.org security advisory page.  They said:  "While Drupal 8.3.x and 8.4.x are no longer supported, and we don't normally provide security updates for unsupported minor releases, given the potential severity of this issue we are providing 8.3.x and 8.4.x releases that include the fix for sites which have not yet had a chance to update to 8.5.0.  The Drupal security team strongly recommends the following."



They said:  "Sites on 8.3.x should immediately update to the 8.3.x release that will be provided in the advisory, and then plan to update to the latest 8.5 security release in the next month.  Sites on 8.4.x should immediately update to the 8.4.x release that will be provided in the advisory, and then plan to update to the latest 8.5 security release in the next month.  Sites on 7.x or 8.5.x can immediately update when the advisory is released using the normal procedure."



Oh, and they did note that this will not require a database update.  They said:  "The security advisory will list the appropriate version numbers for all three Drupal 8 branches.  Your site's update report page will recommend the 8.5.X release, even if you are on 8.3.x or 8.4.x.  But temporarily updating to the provided back port for your site's current version will ensure you can update quickly without the possible side effects of a minor version update."



So they said:  "The Security Team or any other party is not able to release any additional information about this vulnerability until the announcement is made.  The announcement will be made public at," and then they give the URL www.drupal.org/security, "over Twitter and in email to those who have subscribed to our list."  So no further details, but they couldn't be making anything more clear.  I mean, there's nothing they could do to drive home the point more clearly that this has to, you know, that anyone using Drupal tomorrow - what, you said between 18:00 and 19:30 is around noon in Pacific time?



LEO:  Yeah, I think so.  Noon or 11:00.  Noon or 1:00, I mean.  But, yeah, I talked to Patrick.  He's all over it.  He knew it all the time.  



STEVE:  Ah, he's cool.



LEO:  He's going to be sitting there, his sandwich in hand, ready to push the button.



STEVE:  Finger on the button, finger on the button; right.



LEO:  It doesn't give you a lot of time, does it.  I mean, it's like...



STEVE:  No, no. 



LEO:  ...wow, it's a race between you and the bad guys.



STEVE:  And the fact that there has been a week means that the bad guys are also - they're sitting here clicking refresh on their browser, waiting to get the information.



LEO:  They're staring at the code.  They're going, what is in there?  What's in there?



STEVE:  Yes.  And what we absolutely know, in fact they probably have their diff engines all set and primed to, like, to immediately process.  Now, what we absolutely know is many sites will not do this.  Our listeners are.  You guys are ready.  Responsible sites.  But, yes, it is the number two CMS on the 'Net, and there are many people who had somebody else set their Drupal up some time ago, and then they wandered off.  They were subcontractors.  It's not being maintained.  So I'll just bet you next week or two weeks or three weeks, if not all of those, we'll be talking about exploits because this sounds like they're going to be - they have no choice but to change their system in a way that bad guys could leverage.  It's clear that that's what they're worried about.



LEO:  We'll do live coverage, live streaming coverage of Patrick patching the server.



STEVE:  So some good news on the DNS front.  We've been talking about it a lot because, as we know, DNS is sort of the laggard within the security domain of the Internet.  It is by default over UDP, so it is spoofable.  It is interceptable.  There's no encryption available, unlike with TCP connections where we can use TLS tunneling.  There is DNSSEC, but DNSSEC is signing.  It's not privacy.  So even when we had DNSSEC to secure the DNS, as it's called, the Domain Name System, we still don't have privacy.  That's not part of it.



So it turns out that there is an effort underway which has sort of been done, I mean, it's not private because it's an IETF draft process which has been moving slowly towards ratification as IETF draft processes do.  But I'm bullish about this.  To me, this makes sense.  The bad news is the acronym.  Technically it's called Trusted Recursive Resolver via DNS over HTTPS.  Unfortunately, they only kept the last three, DNS over HTTPS, so it's DoH.  Yes, it's D-O-H is the acronym.  The TRR, the Trusted Recursive Resolver, they decided that was to heavy for them to drag along.  That would be TRRDOH.  Instead we just get DoH.  So this is the DoH protocol.



The good news is this is an experiment for, well, which is moving through draft, for essentially allowing a client, like a web browser, to establish a semi-static TLS connection to a DNS over HTTPS.  In other words, you bring up a secure tunnel.  You maintain the tunnel for the duration of your use of the client.  So when you launch Chrome or you launch Firefox, the browser would establish a tunnel, so it would perform the authentication and the handshaking and establish security with a provider once and leave that connection up.



Remember that TLS and TCP do not require any continuous packet traffic in order to maintain a static connection.  There will probably be plenty.  If you're using your browser, you're always doing DNS lookups.  But the idea is that this would solve, very much like a VPN, but without any of the overhead.  When you think about it, we've pretty much got HTTP now moved to HTTPS so that we've got authentication so we have security, and we've got encryption so we have privacy for our individual web browsing actions.  But unless we're using a VPN, as we mentioned last week, even some VPNs it turns out are not tunneling DNS for the system.  If they're only a VPN for your browser, then DNS lookups still go out the old-fashioned way, so you don't have browser-based security.  And essentially just understand this represents a huge privacy problem.  Anybody observing, passively observing TCP traffic, I'm sorry, UDP traffic, Internet traffic, is able to see by default everywhere you go because your browser is asking for the IP for a domain name, and that lookup process is occurring.



So where we stand is we know that Google has announced and is running an API that's supporting DNS over HTTPS, with Google's public DNS service.  Mozilla has teamed up with Cloudflare, and they're conducting a pilot test which will begin, I believe, with the next version of Firefox, which would be v60.  Matthew Prince, who's the cofounder and CEO of Cloudflare, was interviewed by Threatpost over all of this, and he was quoted saying:  "DNS is a 45-year-old protocol.  It was never built to have encryption in it or to be secure.  Yet, DNS acts as the Internet's vital directory and is vulnerable to many different types of abuses.  This is a privacy nightmare, this is a security nightmare, and this is a performance nightmare; and yet it's the foundation of the Internet."



So what Firefox will be doing is with v60 bringing up optionally this Trusted Recursive Resolver, TRR.  And in fact that's the way to find it in the about:config for Firefox as soon as you have 60.  It'll be in the nightly builds, and at this point not enabled by default.  So you'll need to go to about:config and then search for "trr."  That'll bring up all of the options that allow you to turn that on.  And Ghacks.net site has a very nice write-up of all of the Firefox settings.  There's a whole bunch of different options.



My feeling is this is where we're going to end up.  There are privacy worrywarts who are concerned that all this does is move the privacy problem, like it relocates it to the endpoint, to the supplier.  I would disagree with that.  I think that this is enough of a problem not to have DNS protected to allow it to be so spoofable and so visible by default that it is a win to incorporate a DNS tunnel into DNS lookup for our browsers.  And, for example, the connection with Mozilla and Cloudflare is not meant to be the way Mozilla's always going to have it.  I would imagine it will be an option.



But the presumption is there will be many different providers of DNS over HTTPS, and that individual users will be able to decide, certainly in the case of Mozilla and, who knows, I would imagine even in the case of Chrome.  I would imagine, since Google offers DNS over HTTPS by default, they will be the default provider for their own browser.  But certainly users will probably be able to have that as a configurable setting.



So anyway, I just wanted to put this on everyone's radar.  This feels like the solution that's going to win.  Certainly we know about OpenDNS and their own solution for using DNSCurve in order to encrypt and provide both privacy and security.  That's always had the feeling of a closed standard.  And they may very well, I wouldn't be at all surprised, as an example, OpenDNS was another provider of the IETF coming standard DNS over HTTPS, or DoH, as I guess we're going to have to get accustomed to calling it.  So it looks like there will be a way before long for us to have secure DNS, as well.



A bit of miscellany:  Last Saturday I decided to take a look at "Star Trek:  Discovery."  I had deliberately eschewed it until now because it was only available for pay on CBS All Access, which really means paid access.  Lorrie and I watched five episodes in a single sitting.  



LEO:  Because you only have a week; right?  So you're going to try to...



STEVE:  Because you have a week, exactly.



LEO:  ...watch the whole thing in one week.



STEVE:  Oh, we will do another chunk tonight and another.  Because she's been out of town for the last two nights.  Otherwise it would already be done.  I just wanted to say it really seems good.  This is not Jean Luc's prime directive-driven universe.  It's set 10 years ahead of Kirk and Spock when the Klingons have not yet been brought into the Federation, and they're not happy.  The captain of the Discovery is sort of a get-the-job-done renegade.  And so far I can only vouch for the first five episodes, but we loved it.



LEO:  Hmm.  I haven't watched it for the same reasons.  I didn't want to - and I saw mixed reviews on the first few episodes.  Does it get better, do you think?



STEVE:  I think it does.  I really - I'm excited.  And of course everyone knows I'm really excited about the April 13th release of "Lost in Space," which looks great.  But I think this had, I think it was 13, maybe 15, I don't remember now how many episodes total.  But it seemed to have a nice first season.  And so, yes, we'll be watching it during our seven-day free trial and then saying, uh, thank you very much.



LEO:  I love it that they got the original theme in there at the end.  That's awesome.



STEVE:  Yeah.  Yeah.  And again, Leo, I'm giving it a full double Vulcan salute, you know, thumbs up because...



LEO:  All right.  Well, you've got me, then.  You think you'll keep paying for it?  How many episodes do you have to watch in seven days?



STEVE:  I think 13.  I think it's 13, might be 15.  There were a bunch.  But believe me, that's not a problem.  I mean, it's really good.  We will be watching it tonight and tomorrow night.



LEO:  How are you watching it?  Is there a Roku app?



STEVE:  I'm a Fire TV person because I can't stand the Amazon controller.  So I'm sure there is a - oh, and if you have Amazon Prime, it is available through Amazon Prime through a connection with CBS All Access.



LEO:  Oh, but you have to sign up for CBS All Access, and then you can do it.  



STEVE:  Well, I just pressed a button on Amazon, and it sort of did it through Amazon.  So it was all - it was very transparent.



LEO:  Oh, yeah, I could do that, yeah.  Yeah, that makes it too easy.



STEVE:  It is very easy.  And then you've just go to go into Amazon Prime and...



LEO:  Just forget you're paying for it, yeah.



STEVE:  Yeah, exactly, and then say no thank you because it's $10 a month after that.  And there's nothing else that I want to watch.  Maybe there'll be other things that people will want to watch.  I mean, remember we were all fans of "The Good Wife," which was then extended by CBS as "The Good Fight," and that's also under paid access.  But, eh, it's not that good.  So anyway.



And lastly, before we talk about TLS v1.3, I wanted to mention the Disk Cleanup app in all of our Windows versions.  Disk Cleanup has been there forever.  And I have recently been running Disk Cleanup on my 7 and 8 and 9 - well, not 9, there's no 9 - 7 and 8 and 10.  And it's finding 15GB of stuff to clean up.



LEO:  Wow.



STEVE:  Yeah.



LEO:  Now, do you have to do the cleanup system files to get that stuff?  Or is that just in the...



STEVE:  Yes, yes, you do.



LEO:  Oh, okay.  That's the key is press that button.



STEVE:  Exactly.  So everybody, put Disk Cleanup into your search bar or your Cortana or whatever, find it.  And then, when it comes up, down in the lower left is a Cleanup System Files.  Press that and wait a while, and then go and turn all the checkboxes on.  Now, again, of course, this is going to remove your ability to back out of where you currently are with Windows Update and Windows versions.



LEO:  Because you're getting rid of restore points and stuff like that.



STEVE:  Yes, yes.  But oh, my god, I mean, it's huge amounts, well, not only of space, but of time.  So don't do this if you intend to use your system again that week.  Well, depending upon what day it is.  No, seriously, it's an overnight thing.  So at the end of the day, start that process.  You can see how much space you're going to get back.  My biggest was 15GB of space.  So that's not nothing.



I just wanted to kind of put it on people's radar that you don't have to use a third-party tool.  You don't have to do anything sketchy.  It's Microsoft's own thing that's always been there.  You do need to click up Cleanup System Files to get all the biggies.  And don't do it unless you're sure that your system is stable and you won't be rolling back to anything previous.  But there's a lot of space being taken up unless you deliberately capture it, recover it yourself.



And finally, TLS v1.3.  We've talked about it a number of times.  We were talking about it just a couple weeks ago because we were getting close to ratification.  That did happen a couple days ago.  It is final.  And this of course is the next point release of Transport Layer Security, which is the successor to Secure Socket Layer, SSL, which we have had until TLS came along, which provides authentication with the whole much controversial CA-issued secure server certificates, which are signed by certificate authorities that allow our browsers to verify that we are connecting to the domain, the owner of the domain we believe we are, and through that process negotiate encryption between the endpoints so that we get privacy.



Just the tip of the specification says, under Document Quality, and everyone will understand why I'm reading this in a second, they said:  "There are over 10 interoperable implementations of the protocol from different sources, written in different languages.  The major web browser vendors and TLS library vendors have draft implementations or have indicated they will support the protocol in the future.  In addition to having extensive review in the TLS working group, the protocol has received unprecedented security review by the academic community.



"Several TRON" - which is the abbreviation for TLS Ready or Not - "conferences were held with academic community to give them a chance to present their findings for TLS.  This has resulted in improvements to the protocol.  There was also much consideration and discussion around any contentious points" - and we know what those were - "resolved through polls and working group last calls.  Please note that ID-nits complains about the obsoleted/updated RFCs not being listed in the abstract.  This is intentional because the abstract is now a concise and comprehensive overview and is free from citations."



So again, as I said earlier, this is the way this should be done.  And thank goodness for something as critical as this it's the way it was done.  We didn't start this way.  There really never was an SSL 1.0 because Netscape created the spec, and it was so troubled that it was never released.  2.0, despite their best efforts, it was basically a single-shop operation, and it had lots of problems.  So for a core protocol like this, this is the way it needs to be done.  And as unfortunately the Wi-Fi Alliance's recent announcement demonstrates, they still don't have a clue.  They're still not getting it right.  So I imagine we'll be talking about future WiFi security problems in the future.  Probably not any more TLS problems for some time.



So TLS 1.3 brings us a bunch of things.  We get improved cryptography by completely dropping support for earlier and now formally obsoleted hashing algorithms like MD5 that still had a presence in cipher suites that might have been acceptable.  No more.  It also adds support for much stronger modern alternatives such as the ChaCha20, Poly1305, the Edwards Ed25519 Curve, the x25519 Curve - those are the elliptic curves, actually those are the ones which I adopted for SQRL's use - and x448.  So much stronger crypto.



1.3 also supports quicker initial handshake connection negotiation between the client and the server, cutting down on the required round trips, as I have referred to before.  So HTTPS over TLS 1.3 will no longer be slower than HTTP for that reason, and that removes one of the arguments against going to HTTPS, saying, oh, there's more overhead and it's slower.  No, not any longer.



1.3 also supports new features to reduce the time needed to establish encryption handshakes with hosts to which the client has recently connected.  So that technology has been nailed down, meaning that, if you tend to be going back to Google a lot, you're able to just essentially pick up where you left off, even if you had dropped a connection for a while.  1.3 also brings stronger protection against downgrade attacks, which is one of the things we've most spoken about in previous versions of SSL.  It was often possible, because the client offered the server a list of protocols, suites, cipher suites that it understood, and the server was able to choose among those.



There was no provision in the earlier versions of these protocols to prevent intercepting the client's list and only using the weakest of the things it was offering, which would cause the server to sort of shrug and go, uh, okay.  Anyway, can't do that anymore with TLS 1.3.  We've solved this problem of downgrade attacks finally.  So additional security as a consequence.



And the one big remaining question was what about perfect forward secrecy?  Despite the efforts of the financial business sector that seemed to be the most well-organized group fighting against the adoption in 1.3 of mandatory support for perfect forward secrecy which, as we know, would protect us against the future disclosure of a server certificate which we're not protected from at the moment.  The idea would be, if you captured TLS traffic that you could not decrypt today, but you then in the future managed to obtain a certificate, even an expired retired certificate, from a server that had been involved in that initial negotiation, you were able to then come back and decrypt traffic from the past that you had captured.  That's lack of perfect forward secrecy.  That is not possible to do under TLS v1.3.



So this is going to mean that there will still be laggards who are forcing TLS 1.2 because they don't want to update their technology in their IT infrastructure to support 1.3.  But time is going to move forward; and at some point we're going to see, as we have seen in the deprecation of 1.0, we are now using 1.1 and 1.2 and 1.3.  At some point 1.1 is going to go away, and then at some point 1.2 is going to go away.  And the world will then all be using 1.3 with perfect forward secrecy as a requirement.  So browsers like Chrome, Edge, Firefox, and Pale Moon have already rolled out support for earlier versions of the TLS 1.3 draft; and they will be updating if necessary, if any update is necessary, updating their support to the final draft standard now that we have that.



And as I mentioned also at the top of the show, I did a little bit of a timeline.  We got SSL 2.0 in 1995 because 1.0 never really happened.  A year later we went to SSL 3.0  Three years later we went to TLS 1.0, that is, 1999.  That's how long TLS 1.0 was ratified, 1999.  Then TLS 1.1 came in 2006 after seven years.  So we lived a long time with TLS 1.0.  TLS 1.2 relatively quickly fixed some problems because we were getting better with security with 1.1.  So it was only two years later, in 2008.  Yet that was a decade ago.  2008, 10 years ago, is when we got 1.2.  And so now 10 years later we are moving to 1.3.



So my feeling is this is probably going to, I mean, it'll be very surprising if there are any showstopping problems in 1.3.  This thing, first of all, it is a mature evolution of 1.2 that we've lived with for 10 years.  We've learned a lot from 1.2.  We have new crypto.  We've got new ciphers.  We've got improvements in the protocol.  So basically 1.3 wraps up a decade of intense understanding and learning and evolution about how to solve this problem, and makes it a standard.  We're not all using it today.  There will be places that we cannot connect to over 1.3.  But it is the future, and at least we now have a standard against which to implement all future technology.  So, yay, that's how the process works.



LEO:  Nice, very nice.  And if you don't mind, I'd like to share, as we close out the show, a tweet from Edward Snowden that he just recently put up that talks about a 2009 interview, actually it's a little clip from a 2009, what would that be, almost 10 years ago, interview with Mark Zuckerberg in the BBC.  And it's only a 30-second clip, but I think this will - I think you'll enjoy this.



[Clip]



BBC:  So who is going to own the Facebook content, the person who puts it there, or you?



ZUCKERBERG:  The person who's putting the content on Facebook always owns the information.  And that's why this is such an important thing and why Facebook is such a special service that people feel a lot of ownership over; right?  This is their information.  They own it.  They often want to...



BBC:  And you won't sell it?



ZUCKERBERG:  No, of course not.  I mean, they want to share it with only a few people.



BBC:  So just to be clear, you're not going to sell or share any of the information on Facebook.



ZUCKERBERG:  What the terms say is just we're not going to share people's information except for with the people that they've asked for it to be shared.



[End clip]



LEO:  Okay.  Just, you know, so you know what the truth is about Facebook.  Ahem.



STEVE:  Ah, interesting.  Interesting.



LEO:  That was nine years ago.



STEVE:  Interesting.



LEO:  No, no, we won't sell it.  We won't sell it.  Never sell it.



STEVE:  Interesting.



LEO:  No.  Why would we sell it?  What possible reason would we have to sell it?  We just give it to you.



STEVE:  Yeah, that's a little blast from the past.  How things change.



LEO:  How things change.  Steve Gibson, nothing seems to change with Steve.  He's still here at 13 years, and we hope he goes on another 13 years.  A lot of people worry about this 999 thing.  Just keep it in mind, we can give you four digits.  If needed, we can give you a digitectomy.  Or what's the opposite of a digitectomy?  We'll give you an added digit, and you can go on as long as you want because we sure want you to keep going on.



Steve Gibson's website is GRC.com.  That's where you go to find his SpinRite, the world's best hard drive maintenance and recovery utility.  If you have a hard drive of any kind, you need SpinRite.  While you're there, check out all sorts of good stuff he's got up there, SQRL and Perfect Passwords and Password Haystacks and ShieldsUP!, which is probably in the billions by now, billions served.  I don't know, the number goes up all the time.  And of course this show.  You'll find it there, as well, including handwritten transcripts by the fine Elaine Farris - she does a great job of translating geek into English - and MP3 audio of the show.  If you want video of the show - and people do want to see you, Steve.  They want to see the 62-year-old moustache.



STEVE:  Yeah, until they have one.  



LEO:  You can come to our site, TWiT.tv/sn.  You can watch live.  We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  That would be at live.twit.tv or TWiT.tv/live.  We have two pages depending on which player you want to use.  You can also watch on other services.  We're all over the place.  If you do watch live, though, join us in the chatroom, irc.twit.tv.  You can be with the kids in the back of the class throwing spitballs, stuff like that.



We have on-demand versions of audio and video.  Steve's got his; we've got ours.  And the best thing to do, because it all comes from the same place, ultimately, would be subscribe in your favorite podcatcher.  That way you get every episode.  You can start building your collection of Security Now! shows.  Steve, have a great Tuesday afternoon.  I know where you'll be, seated on the couch this evening.



STEVE:  Yes, sir.



LEO:  Enjoy your trip through space, and I'll see you next week.



STEVE:  I will do that, and I'll have a full report next week on what finally happened because there's no doubt that we will be powering through.



LEO:  I've got to watch it.  I'm going home tonight.



STEVE:  I recommend it.  It is really - it is gritty.  It's wonderful.



LEO:  Thank you, Steve.  We'll see you next time.



STEVE:  Okay, my friend.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#657

DATE:		April 3, 2018

TITLE:		ProtonMail

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-657.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss "Drupalgeddon2,"  Cloudflare's new DNS offering, a reminder about GRC's DNS Benchmark, Microsoft's Meltdown meltdown, the persistent iOS QR code flaw and its long-awaited v11.3 update, another VPN user IP leak, more bug bounty news, an ill-fated-seeming new email initiative, free electricity, a policy change at Google's Chrome Store, another "please change your passwords" after another website breach, a bit of miscellany, a heartwarming SpinRite report, some closing-the-loop feedback from our terrific listeners, and a closer look at the Swiss encrypted ProtonMail service.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  We're going to talk about 1.1.1.1, Steve's DNS Benchmark, and which DNS servers are the fastest to use.  He's also got a review of the encrypted email system from Switzerland, ProtonMail.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 657, recorded April 3rd, 2018:  ProtonMail.



It's time for Security Now!, the show where we cover your security and privacy online as we have done for more than a decade.  Wow.



STEVE GIBSON:  My god.



LEO:  My god.



STEVE:  We're in our 12th year, in fact.



LEO:  It's all thanks to this guy right here, Mr. Steve Gibson of GRC.com.  Hi, Steve.



STEVE:  You know, for the first time ever I'm able to do my wave salute without worrying about bumping into the microphone.



LEO:  You switched sides.



STEVE:  Well, so to speak.  The landscapers decided now would be a great time to grind up some trees.



LEO:  Oh, man.



STEVE:  So I listened to it.  They went away for a while, and I thought, oh, maybe they're done.  But I think that was just lunch break, and they're back with a vengeance.  So the microphone is so directional that I thought, okay, fine.  It's a pain in the butt to move it over; but, on the other hand, I can now do my salute without having to bump the microphone.



LEO:  And I don't hear a thing, so good job.



STEVE:  Yeah, great.  So ProtonMail came up a couple times.  First of all, a lot of our listeners are wondering about secure email.  Now, I've always regarded that phrase as an inherent oxymoron, and so I've never really just focused on it much.  But I know it's what people want.  And a question from a listener sort of reminded me of that question.  It was about, if I were to leave Google, where would I go, you know, leave Gmail.  And then, coincidentally, ProtonMail announced an upgrade to their mobile clients.  And I thought, okay, fine.  Let's talk about what they've done because they've arguably done the best job they can, given the inherent limitations of email.  And they're a thousand meters below solid rock in Switzerland, so that's a story we have to tell.



LEO:  That's the kind of thing, though, that you often would mock; right?



STEVE:  Yeah.



LEO:  Like that doesn't have anything to do with security.



STEVE:  The backhoe can still cut the fibers.



LEO:  Yeah, exactly.



STEVE:  Exactly.  But we're going to talk about a bunch of things before we get to talk about ProtonMail.  We have now the information that was pending last week on what has been named Drupalgeddon2, as in Armageddon, Drupalgeddon, which arrived the day after last week's podcast.  So we have the information about that.  We have a very interesting and I think significant new DNS offering from the oft-mentioned Cloudflare people, who are just doing a bang-up job with all kinds of stuff.  I mean, they're really making a name for themselves.  And GRC's own DNS Benchmark confirms their claims.



So later down in the show notes is a screenshot of my just having run GRC's Benchmark, and I'm going to show people - well, actually it's pretty simple, but there's a button you press if you want to add DNS servers to the Benchmark.  So people are going to want to add 1.1.1.1, and not so much 1.0.0.1.  We'll talk about that.  Then we've got to talk about Microsoft's Meltdown meltdown, the persistent iOS QR code scanner flaw that persisted even across the much-anticipated version 11.3 update to iOS, which fixed a bunch of things.



There's another VPN user IP leak, some more bug bounty news which is kind of interesting in a different direction, a very ill-fated-seeming new email initiative that we need to talk about just because it may be coming to a client near you, the consequences of free electricity, a policy change at Google's Chrome Store, another "please change your passwords" after another website breach, a bit of miscellany, a heartwarming SpinRite report, some closing-the-loop feedback from our terrific listeners, and then we're going to talk about, brought to you from Switzerland, an encrypted email service that may be worth looking at if Gmail is chafing for one reason or another.  Although I made a comment last week about them looking and reading everyone's email, and on a different podcast I realized, oh, that's old news.



LEO:  Yeah, I heard you say that on Know How, and I meant to tell you that, yeah.



STEVE:  Ah, that's where it was.  Right, right, right.



LEO:  Yeah, they stopped doing that.



STEVE:  Well, good.



LEO:  Not that they're not spying on you.



STEVE:  Not that there still might not be a reason to increase your security.



LEO:  And your ISP's spying on you, for sure, and et cetera, et cetera, et cetera.



STEVE:  Yup.



LEO:  All right.  I can't wait.  Actually, I've been looking at ProtonMail.  My preference, my strong preference is just, if I want secure email, to use PGP or GNU Privacy Guard and encrypt the mail, and that way I don't have to worry, but metadata leaks.  And as we know, metadata is valuable, too.



STEVE:  Yup.



LEO:  So, good.  I look forward to this.  I have a picture.



STEVE:  So our Picture of the Week, having just jumped up and down and congratulated Cloudflare, I don't know where they get these numbers.



LEO:  Fred Flintstone, obviously [referring to yabba-dabba-do in the background].



STEVE:  Yeah.  So they're showing 1.1.1.1.



LEO:  That's their new service, which they launched on April Fool's Day.



STEVE:  Their new service.



LEO:  Which confused everybody.



STEVE:  It did, especially because it's 4/1, and it's four ones.



LEO:  That's what CEO Matthew Prince said.  This is exactly the perfect time to launch it, on 4/1; right?  Even though...



STEVE:  Yeah.  So the good news is it's not a joke.  However, okay, so they're showing 14.8ms for their DNS lookup.  They're showing OpenDNS at 20.6, Google's Public DNS at 34.7, and the average ISP - and I don't know what ISP is at 68.23.  We'll get to this in a minute.  But I have to say I'm really sort of tickled that, if you google for the term "DNS Benchmark" and look at images, it's just a screen full of screenshots of GRC's Benchmark.  So I ran it a few minutes ago in order to make a screenshot for the show notes.  And my ISP, right, it makes sense, is the closest DNS server to me.  I mean, any packets going to any other DNS server have to go by my ISP first, sort of by definition.



Well, it was 8ms for four of my Cox servers.  Cox is my ISP here in Southern California.  1.1.1.1, to their credit, and I fully intend to give them credit, was 1ms longer.  It was the fastest non-Cox DNS server of all.  I mean, and I've got OpenDNS, and I've got Google Public DNS, and a whole bunch of other ones are in there.  So it is absolutely the fastest alternative to any DNS server that's not your ISP's.  But I don't know whose ISP takes 68ms to respond.  I just can't explain that.  But not mine.



So the good news is, for Windows users - and it runs under Wine, I think, too, so non-Windows users who have access to Wine should be able to run GRC's DNS Benchmark and test this themselves.  And I would suggest everyone does because, in my case, the performance penalty is, well, it is measurable, but it is arguably not worth sacrificing the benefits which we will be talking about when we talk about Cloudflare's offering.  So anyway, the Picture of the Week was what is being plastered everywhere.  And it's like, eh, okay.



The good news is no one has to believe this bar chart with numbers that are strange.  I mean, Google's Public DNS wasn't that much slower, nor was OpenDNS.  They were slower, but not, like, ouch.  So anyway, people can find out for themselves, and everyone should because your mileage will vary.  And, boy, if your ISP is taking 68ms to respond to a single UDP packet, if you have any choice, you might consider what else they're slow at.



So, okay.  Last Wednesday the big news that had had a week's notice given landed, which was there is a very bad remote code execution vulnerability in Drupal that dates all the way back to version 6.x branch, which was discontinued more than two years ago.  So this is so bad, and apparently there are people still using something on the 6.x branch, that they offered updates even back there.  So that suggests that this has been a very longstanding problem.  And the other thing this suggests, unfortunately, is that there are going to be a lot of problems with Drupal sites that are not getting their email, that are not patching themselves, that have just been set up by some techie who wandered off, and people are using the site and don't realize that they're now a target because they can be found.



Last Wednesday, after the patches were announced, Drupal took their own site down after the release of the updates so that it, too, could be updated.  They didn't want to do it beforehand, much as they may have wished to, because it may have been possible for people to figure out what was going on.  And of course this is the problem is that this is a big target.  As we know, they are the number two content management platform on the Internet, with a ton of installations.



So Drupalgeddon1 was a super severe bug.  I think it was rated 25 out of 25.  It was an SQL injection bug in 2014.  So nothing really that bad has happened until now, thus giving this one Drupalgeddon2's name.  It's an input validation issue where invalid query parameters can be passed into Drupal web pages.  No proof-of-concept code was released because of course this isn't some third party that found it.  The bug was found by Drupal's own security people.  A guy named Greg Knaddison, who's a security team member, found it and explained that it has to do with the way Drupal interprets a value that begins with a hash as having a special meaning.  And so they're trying to promote the idea that - they're working, of course, to downplay the severity of this, saying that hundreds of thousands of sites immediately patched within the first 12 hours of the release.



On the other hand, an independent firm, SiteLock, did some measurements of their own and determined that only 18% of Drupal websites were found to be running the latest core updates.  So this suggests that, as we keep seeing with these kinds of problems, the vast majority of websites running Drupal are likely to be vulnerable to compromise because they're not being updated with the latest security patches.  So I'm glad that everybody listening to this podcast knows; and, if you have any influence over local Drupal sites, that you immediately update it.  We know, Leo, that your TWiT engineers were on top of it from the get-go.



LEO:  Yup.  Both sites patched like at noon, right after the patch came out, yeah.



STEVE:  Nice, nice.  So I'm afraid that we will be covering exploitation of servers.  And we know what's going to happen.  They're not going to dig into anyone's network.  They're just going to set up crypto miners because that's the thing now.  Everyone wants to run cryptocurrency mining on servers, and I wouldn't be surprised if the hackers are rubbing their hands together because I think there are about a million Drupal sites.  So they're probably rubbing their hands together saying, hey, there may be some three quarters of a million servers available for exploitation, if we can find them.  And unfortunately, as a company like SiteLock demonstrates, it's easy to determine if a site is Drupal-based, based on the response to queries from the web.  So I'm afraid we're going to be covering the downside of this.



Okay.  So Cloudflare.  The IP is wonderful, of course, 1.1.1.1.  Hard to forget that one.  They also offer 1.0.0.1, but everyone should take note that that is significantly slower.  It's meant to be a backup, and so you want to make sure 1.0.0.1 is your secondary DNS IP.  And at the moment you might even consider using your own ISP's DNS servers as backup, with 1.1.1.1 as your primary.  And the logic that IP resolvers use is to first try the primary resolver that you've registered.  In this case it'd be 1.1.1.1.  And if it fails, typically then the rest of them that are set up as backups are simultaneously queried if the primary doesn't respond.  GRC's DNS Benchmark also verifies reliability, and there wasn't a single query missed in the - I've forgotten now how many I do.  Probably a hundred.  So reliability was right up there.



So the news is, and we've sort of been stepping all over it already, is that Cloudflare has decided that they want to jump into sort of the growing DNS provider space.  We've talked about others that have.  Of course Google is, OpenDNS has been there, and Cisco purchased OpenDNS.  And we actually talked about this before the Cloudflare announcement in the context of Mozilla last week because remember that I was talking about how the next version of Firefox - we're at 59 right now.  When we get to 60, six zero, Firefox will be beta testing their connection with Cloudflare as the de facto DoH, D-O-H, DNS over HTTPS.  And this is exciting because this gives us security and privacy for DNS, and probably no more speed than we get with UDP because you just can't get any faster than a single packet being sent and a packet coming back.  TCP has to do the same thing.



However, DNS is not encrypted and is not authenticated.  And so if we run DNS with the same single-packet query inside of a TLS encrypted and authenticated tunnel to a provider, then we get all of the benefits and no reduction in performance.  So Cloudflare at this IP 1.1.1.1 - and anybody who's interested can actually go to that domain with a web browser, https://1.1.1.1.  Put that IP in, and that will resolve to their announcement and configuration and setup instructions.  And so it's meant to be user friendly.  So you can actually put https://1.1.1.1 into your web browser and go take a look at the service. 



So they have, as I mentioned, two resolver IPs - that one, the quad one, and also 1.0.0.1 - which when you put both of those in, add those to GRC's DNS Benchmark, it automatically gets the IPs that are configured for your system, presumably from your ISP or OpenDNS or whatever provider.  So it adds those to its large list of DNS servers.  You can then manually add these.  And I'm thinking it's probably time for me to rev the software.  I haven't touched it in eight years because it works.  There's no bugs.  So maybe it's time to add 1.1.1.1 and 1.0.0.1, just to have them built in.



But anyway, Olafur Gudmundsson, who's the Director of Engineering at Cloudflare, said of this effort:  "Our goals with the public resolver are simple.  Cloudflare wants to operate the fastest public resolver on the planet, while raising the standard of privacy protections for users."  And I can assert that, yes, theirs is faster than any other non-ISP resolver in my own testing.  And I did this several times over the course of the last several days.



He said:  "We began talking with browser manufacturers about what they would want from a DNS resolver.  One word kept coming up:  privacy.  Beyond just a commitment not to use browser data to help target ads, they wanted to make sure we would wipe all transaction logs within a week.  That was an easy request.  In fact, we knew we could go much further.  We committed to never writing the query IP address to disk and to wiping all logs within 24 hours."



He says:  "The DNS resolver 1.1.1.1 is also supporting privacy-enabled TLS queries on port 853" - the service is DNS over TLS - "so we can keep queries hidden from snooping networks.  Furthermore, by offering the experimental DoH (DNS over HTTPS) protocol, we improve upon privacy and a number of future speedups for end users as browsers and other applications can now mix DNS and HTTPS traffic into one single connection."  Okay, now, I have no idea what that means.  It's like, what?  That needs to be clarified because I don't see how establishing a single connection to port 853, which is DNS over TLS, allows you to pipeline non-DNS traffic.  So maybe they've got something more up their sleeve that isn't apparent.



He says also:  "With aggressive negative caching" - and I'll explain that in a second - "as described in RFC 8198, we can further decrease the load on the global DNS system."  Negative caching is the process of remembering DNS query failures in addition to DNS query successes.  So it's obvious that DNS caching says, oh, look, here's something that people are asking for.  Let's keep it locally in our cache after having resolved it once through what may have been a recursive DNS lookup.  And as we know, DNS queries have an expiration as part of them, so that permits someone who receives it fresh from the authoritative server to hold it in their cache until its own self-stated expiration times out, in which case they need to go get it again.



Well, negative caching says, well, someone asked us for a domain name that doesn't exist.  We've looked for it.  Couldn't find it.  So let's also remember that.  That's negative caching, where you cache a negative outcome of a search - again, just to speed up the return of an NXDOMAIN, which says, sorry, that doesn't exist.  And that's, you know, I'm sure that Cloudflare would never consider taking us to some advertising page, but they do properly return an NXDOMAIN.  We'll note that some ISP DNS doesn't, that is, ISPs often use a failed lookup to take you to their own search page, where they do a little marketing, which really breaks the definition of DNS and has been controversial in the past.



So he says:  "This technique first tries to use the existing resolver's negative cache, which keeps negative, non-existent information around for a period of time."  He says:  "For zones signed with DNSSEC" - and that's another thing that they are supporting which is very cool, I'll get to it in a second - "and from the NSEC records in cache, the resolver can figure out if the requested name does not exist without doing any further querying."



He also says:  "We use DNSSEC validation when possible, as that allows us to be sure the answers are accurate and untampered with.  The cost of signature verifications is low, and the potential savings we get from aggressive negative caching more than make up for that.  We want our users to trust the answers we give out, and thus perform all possible checks to avoid giving bad answers to the clients."



He says:  "However, DNSSEC is very unforgiving.  Errors in DNSSEC configuration by authoritative DNS operators can make such misconfigured domains unresolvable.  To work around this problem, Cloudflare will configure negative trust anchors on domains with detected and vetted DNSSEC errors and remove them once the configuration is rectified by the authoritative operators.  This limits the impact of broken DNSSEC domains by temporarily disabling DNSSEC validation for a specific misconfigured domain, restoring access to end consumers."



So essentially what this means is they're doing all of the work and heavy lifting for us.  Remember that, for the time being, until browsers start doing DNS over TLS, we're using UDP.  So we have an unsecured, non-authenticated query that we're making, if you were to configure it to 1.1.1.1, to Cloudflare.  But with Firefox 60 - and I meant to go look at Chrome to see whether Chrome can already have this turned on because certainly Google is offering this service, as I mentioned last week.



So I want to make it clear that, at the moment, some configuration is required in order to switch from UDP to TLS.  When you do that, then you're establishing an authenticated TLS connection to the DNS provider, in this case Cloudflare.  So your queries are encrypted.  You get privacy.  You get the authentication of TLS.  And Cloudflare is a DNSSEC-aware resolver so, to the degree that domains are signed, then they'll verify the signatures, making sure that what they're getting is not spoofed.  And because you've got TLS for the so-called "last mile," your query to them, that's both private and unspoofable.  So this is sort of a nice and unsuspected compromise on requiring all endpoints like us to do DNSSEC ourselves, at least for our browsers, and not yet for our operating systems, but that may be coming.



But our browsers can be smart enough to do their own resolution through an encrypted tunnel and really tighten up the DNS system which, as we know, as I said last week, is still sort of the one thing hanging out there.  In the show notes here I have a screenshot of my having run the DNS Benchmark around 8:30 this morning.  And the first four entries are for Cox Communications.  And sure enough, they are the four fastest resolvers.  The fifth entry says MegaPath Networks Inc., and it shows the IP 1.1.1.1.



So if anyone is interested, if you just google "DNS Benchmark," the first handful of links are to GRC.  When you run the Benchmark, since it isn't yet - I haven't touched the code in eight years, since 2010 is when I finished the work on that.  When you run the Benchmark, you'll need to add 1.1.1.1 and 1.0.0.1 to the existing list of DNS resolvers.  Well, on the Nameservers tab on the upper left is an Add/Remove button for adding and removing nameservers.  And so it's as simple as clicking that.  Up pops a dialog that prompts you for the IPs of resolvers you want to add or remove, and you can do so right there.  So add 1.1.1.1, 1.0.0.1, and then just click Run Benchmark.  It'll start spinning and showing you results.



And what was interesting is, first of all, it's not clear, but there's an insider tip.  You can left-click in the bar chart, and it will show you actual numerical results.  I don't remember whether you can do it while it's running.  You probably can.  Everything works.  I put a lot of time into this Benchmark.  But certainly when it's done you can left-click on the bars and then cruise around, and a little floater that tracks where you left-click will show you the actual timings of all of those.  That's how I know that the first four, the Cox Communications resolvers, were all responding in 8ms, and the next fastest one was 9ms.



What's interesting is that one, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13, 14, 15 entries down, below Quad 9, below both of Google's resolvers, below the OpenDNS resolvers, below DSL Extreme and a couple stray Cox Communications resolvers, like way further down the list is 1.0.0.1.  So that one is, I mean, it's still quick.  It's nothing like 68ms.  I don't know where they got that.  But it is a few milliseconds slower, and definitely slower than a whole bunch of others that we're familiar with, the OpenDNS resolvers, Google's, and so forth.  So 1.1.1.1 is absolutely, for me, next fastest from all of my ISP's resolvers.  But 1.0.0.1 is well down the list, which I imagine you're going to want to see for yourself, and the DNS Benchmark allows you to get those results.  And if you poke around in the UI, there's all kinds of other little goodies in the DNS Benchmark.



Okay.  One more story, and then we'll take our second break:  the Windows Meltdown meltdown patch patch.  A Swedish security researcher, Ulf Frisk (U-L-F Frisk), who's also the developer of a tool known as PCILeech, which is a kernel memory hacking tool that you can find on GitHub, discovered that Microsoft's January, that is, beginning of the year 2018 patch for the Meltdown fix for Windows 7 64-bit - that's all, the 64-bit version of Windows 7, not 8.1, not Windows 10, just that one - mistakenly flipped a permission bit in the page table management.  It was supposed to be set to Supervisor Mode Only.  Somehow it got flipped to User Mode.  As a consequence of the work he was doing on his PCILeech tool, he quickly discovered that, until last Friday, so end of March, so January-February-March, for three months Windows 7 64-bit memory, all of it, was completely readable and writable to all user-mode applications.  Whoops.



So Microsoft quickly added, pushed out an out-of-cycle emergency update late last week to flip this wayward kernel memory access control permission bit back where it was supposed to be.  The flaw was not remotely exploitable.  It was just about internal OS memory management.  So any attacker would have needed, first of all, to know about it, and then to have physical access and/or some malware installed.  But it was devastating because, I mean, it completely blew away all of the OS protection, essentially making absolutely all of the system's virtual memory readable and writable to any user mode program running on that system.



So you want to make sure, now that I've said this, and now that everybody knows about this, I mean, this is, like, instantaneously exploitable.  So if you're running Windows 7/64, and that's of course Service Pack 1, and Server 2008 R2 - which is the same OS basically, the server stuff is enabled in it - you want to absolutely make sure that you're all patched up to date, which will then include KB4100480.  If you're curious, go into Updates and just make sure that you're seeing KB4100480 has been installed, and then you know you're okay.



Now, you can also test it, if you're interested.  If you just Google PCILeech (L-E-E-C-H), you'll get taken to Ulf's page over on GitHub, where you can play with this tool he's got.  He warns that some instability may be created in the system, which could bluescreen, in the case of your system being vulnerable.  So save everything first.  And if you're interested, you can run PCILeech just to verify that that system is in fact not vulnerable.  And I would not recommend anyone do that on a live server because you could take it down by mistake.  So anyway, for three months all of Windows 7 64-bit OS memory was readable/writable by all applications running on the system, and that got fixed late last week.  Whew.



So speaking of bugs and patches, there is a much-covered iOS flaw in the QR code scanner which was added to the normal built-in iOS camera app.  It's kind of a neat thing that we got with the jump to iOS 11, such that just using the camera app rather than needing a third-party QR code scanning app, if the camera sees a chunk of real estate which parses out to be a valid QR code, it says, "Oh, there's a QR code," and drops a little message down on the screen saying, you know, "Tap to go to this site."



Well, just before Christmas, on December 23rd of 2017, so just over three months ago, it came to a German security researcher's attention, a guy by the name of Roman Mueller, that there was a way to spoof these URLs.  That is, you could create a QR code which showed one domain name on the screen, but took you to a different domain name, which is not good.  And 90 days later this problem is still unpatched.



So Roman published his findings.  He gave Apple three months to fix this.  And I tried it yesterday on my newly updated version 11.3, and it still works, that is, it's still broken.  Using a URL that is present in a lot of the coverage of this online, so anybody can find it, and I have the URL here in the show notes, when you scan this sample QR code, it says you're going to be going to Facebook.com, clear as it could be.  And when you tap it, it takes you to infosec.rmit.de, which is Roman's demo page, saying nope.



Now, the problem, of course, is this could be a malicious page, spoofing Facebook, and no one would be the wiser.  So I'm at a complete loss to understand how Apple could have let this be out there for 90 days.  Roman kept quiet.  He let everybody know after 90 days.  And if you google "iOS QR code bug" there's a huge amount of coverage in the press.  So let's hope that this shines a spotlight on it that apparently -  I can't imagine what Apple is thinking that they haven't fixed this.  This is a bad problem, and essentially it means that something is parsing the QR code to get this full URL, but the code that displays where you're going to go is different than the code which takes you there.



So I can't understand how Apple has not fixed this, but they haven't, and now the world knows about it.  So now it can start getting exploited because there's no mystery anymore.  This is trivial to do, and I wouldn't be surprised if bad guys are not thinking, how can we fool iOS users with this?  So let's hope that Apple gets this thing fixed quickly now that they seem to have not given it the attention that I would argue it needs.



And speaking of 11.3, I've spoken of it, and I've been waiting for it for months because, as we've talked about, I was very disappointed when I think it was the very last version of 10 and then all of the 11s just completely brought my 6s to its knees, my iPhone 6s, because they adopted this ridiculous, oh, we're going to slow your phone down because we're worried that the battery's not strong enough.  And of course I had alleged that I was taking really good care of my battery, and I'm sure it was in good shape.



Sure enough, what we have now in 11.3 is that the throttling has been removed until the phone actually experiences a problem with the battery, which seems like the right policy.  But there's also a battery health meter.  And as I expected, my very old iPhone 6s is 100% health of its battery because I take really good care of my battery.  So it was always wrong that Apple, I mean, basically I got an iPhone 10 out of frustration because they had rendered my iPhone 6s unusable from a software update, which I think is unconscionable.  So they got my money, and now I've got an iPhone 6s which runs just like it used to, just perfectly wonderful, with an even bigger screen than my 10.



So anyway, they did fix, with 11.3, 43 known security vulnerabilities, about half of them in WebKit.  So it's definitely worth updating.  I have maybe 10 iOS devices.  I've got two phones, and I've got pads everywhere.  Not a single one of them updated themselves or notified me of an update.  It wasn't until I read that 11.3 was available that I then went into the control panel and, like, oh, yeah, there's my little red "1" saying you've got a notification.  So anyway, now everything's up to date.  But for what it's worth, for those who, like me, don't seem to have their devices automatically updating in any timely fashion - because this was pushed out, I think, the middle of last week - you may want to go check.  You'll probably find that, if you're not at 11.3, your device is willing to go to 11.3.  You've just got to give it a little bit of a nudge in order to get it unstuck.



We have another VPN client IP leak.  There's a well-known leakage problem of IP addresses, and we've talked about this for years.  The WebRTC function inherently leaks the IP of the client as part of the WebRTC functionality, which is normally not that big a problem because, after all, any site that you're going to knows your IP anyway.  But in the case of a VPN user, you are often, as we were discussing last week, wanting your IP to be kept to yourself.  That is, you want any site you're visiting to see the IP servers, or the IP endpoint IP, as a proxy for yours, but not yours.  Well-designed VPNs have filtered and do filter the WebRTC IP leakage so that that doesn't happen.



But a researcher, Paolo Stagno, who goes by the handle "VoidSec," audited 83 VPN apps - although, having looked them over, the definition is a little loose because he's got various non-VPN proxies and things in there, too - but 83 things that can put themselves between your browser and you, and most of them are VPNs, 83 of them, to see whether they were properly anonymizing their users by filtering this longstanding WebRTC IP leak.  He found that, of those 83, 17 VPNs were still leaking their users' IP addresses.  And again, the definition here is a little loose for VPN.  But I checked, I scanned through the list to see if there were any important ones.  Hotspot Shield, which we have spoken of often, does not leak.  And I remember you chuckling over this one, Leo, when I mentioned it before, HideMyAss VPN does leak.



LEO:  Oh, well, doesn't hide it that well, yeah.



STEVE:  So your ass is not as well hidden as the name of the VPN service would suggest.  The cool thing is, because he doesn't have the budget or the logistics for checking all VPNs, what he did instead is to create a free service that allows all VPN users to figure out whether their particular VPN is blocking this WebRTC leakage.  The site is https://ip.voidsec.com.  So ip.voidsec.com.  That will just present a nice little page showing you the IP address that you're connecting from.  And this, as I was saying before, any website you go to has an IP from which you are connecting.



And, by the way, you can do this right now.  You don't need - if you're just curious, don't use a VPN and so you can see what it looks like.  You can see if your web browser supports WebRTC.  When you go to ip.voidsec.com, you'll probably see a listing of several IPs which are being presented through the WebRTC interface.  And then he does have, is maintaining - and Leo, you've got it on the screen right now - a Google doc showing all of the spreadsheets that he has tested, and many that other people are reporting yea and nay for, which is how I know that HideMyAss doesn't.  So ip.voidsec.com, if you're a VPN user, absolutely go there.  Make sure that the WebRTC interface is not leaking; and, if it is, go screaming to your VPN provider because that's, well, if you're using HideMyAss then we already know that you've got some leakage problem which really is not what you want.



Anyway, so more on the bug bounty front, this time the federal government.  As I've said, as we've been talking about this recently, I think this is a new career opportunity for people who want to work on their own.  And if you're good enough, these bounties are large enough that you can probably support yourself.  So recently, as of Sunday, Easter Sunday, April 1st, the DoD, the U.S. Department of Defense has bellied up to the bar with their own bug bounty program.



Well, actually, they're offering a bug bounty program through HackerOne, which is an organization which is sort of the clearinghouse for enterprises that want to offer, but have somebody else manage, a bug bounty.  And I would imagine they probably get a piece of the action.  HackerOne is able to count among their customers General Motors, Lufthansa, Starbucks coffee, the EU, Spotify, Airbnb, Lending Club, Nintendo, WordPress, Twitter, Shopify, Ghostery, Google Play, and Cylance.  So some big players.  It's HackerOne.com.



Jack Messer, who's the project lead at Defense Manpower Data Center with the DoD, he said:  "The Department of Defense enterprise system" - which I guess is their travel, I saw in some other coverage, it was like their own travel system, their travel agency.  So I guess the DoD is naturally shipping people all over the place, and so they wanted to make sure that their publicly facing travel management system is not leaking too much.



So he said:  "...is relied on by millions of employees for global operations.  The DoD has seen tremendous success to date working with hackers to secure our vital systems," he wrote, "and we're looking forward to taking a page from their playbook.  We're excited to be working with the global ethical hacker community and the diverse perspectives they bring to the table" - boy, this guy's a bureaucrat - "to continue to secure our critical systems."  Okay.  "To be eligible to participate in the DoD's bug bounty challenge, individuals from the public must be United States taxpayers or a citizen of or eligible to work in the U.K., Canada, Australia, or New Zealand.  U.S. government active military members and contractor personnel are also eligible to participate, but not eligible for financial rewards."



Okay.  So I dug in a little bit, looking into the history of this, because I was curious.  And it turns out that the first instance was two years ago with Hack the Pentagon, where over - and this is a little breathtaking - 3,000 vulnerabilities have been resolved since then.  The first Hack the Air Force bug bounty challenge found 207 valid reports, and hackers earned more than $130,000 for their contributions.  The second Hack the Air Force resulted in 106 valid vulnerabilities, and $103,883 paid to hackers.  Hack the Army in December of 2016 found 118 valid vulnerabilities and paid out $100,000.  Hack the Pentagon in May of 2016 resulted in 138 valid vulnerabilities, which were resolved, and they just wrote "tens of thousands" paid to ethical hackers for their efforts.  And then Hack the Air Force 2.0, which was another round later, demonstrated continued momentum of the Hack the Pentagon program, which is sort of the overall umbrella, beyond just its first year, and further hardened the attack surface.



So if there are any listeners who have hacking skills, who like to dig into systems, you want to make sure that you do it ethically, that you're not crossing any lines.  But it's very clear to me that this concept of bug bounties, where high-profile, well-moneyed interests really have to have their products secured, and there's this dawning awareness that their own engineers are not doing the job for whatever reason.  And as I've said, I mean, I understand.  It takes other people to find bugs in things.  That's sort of the nature of the beast.  There's a career opportunity here to make some good money.  So something worth considering.



Okay.  This just seems like a bad idea.  There's something called BIMI, and I don't - I think you would pronounce it "beam me," as in "beam me up," BIMI, Brand Indicators for Message Identification.  And there's a lot of smoke around this.  There's a site, www.brandindicators.org.  There's even something called the Authindicators Working Group.  So the members of the Authindicators Working Group include an outfit called Agari that sort of seems to be spearheading this, Comcast, Google, LinkedIn, Microsoft, Oath as in the Verizon subsidiary that operates AOL and Yahoo, PayPal, Return Path, and ValiMail, those last two being email providers.



And so what this group is trying to do I'm worried about.  They're trying to come up with a way to attach verified logos to email.  The site asks the question rhetorically:  "What if you could put your brand on every email, and users could trust it's you?"  From the site they say:  "Brand Indicators for Message Identification (BIMI) is an industry-wide standards effort that will use brand logos as indicators to help people avoid fraudulent email..."



LEO:  Oh, this is absurd.  God, this just frustrates me.



STEVE:  I know, "...while giving marketers a huge new opportunity to put their brands in front of consumers for free."



LEO:  Because no phishing scam would ever use your logo.



STEVE:  I know, I know.



LEO:  I mean, they wouldn't dare.



STEVE:  I know.  And so they say:  "If you use Yahoo Mail, you will see brand indicator logos from Groupon, Agari, and other large brands in the financial services, aviation, and technology industries."  And so just to explain what they're...



LEO:  Does it show up somewhere beside the email?  I mean, like on the side?



STEVE:  Yeah.  Yeah, the idea would be that - and if you go to the site, Leo, if you go to BrandIndicators.org, you can see the way they're proposing.  It would be like, in the same way that our messages show, like, the people's faces, like in iMessage, the idea would be there would be a circle with a company logo next to their email in our inbox.  And so they say, under Brand Impression:  "Every time you send an email, you are guaranteed your logo will be displayed in the preview pane and near the From line."



Under Trusted Communications, they say:  "When users see your logo, they'll trust the email and be more likely to respond to your email and do business with you."  And then, under Stop Phishing, they said:  "Brand indicators will authenticate the sender of the email using an existing standard, DMARC, before displaying your logo."  Except, as you immediately picked up on, Leo, none of that is true.



LEO:  Okay.



STEVE:  So I went over to GitHub to...



LEO:  So, wait, it's a win-win situation.



STEVE:  It's a win-win.  Authindicators.github.io.  And I'm thinking, oh, there's an RFC.  There's like a specification.  There's all this thing going on.  What's it about?  So I scrolled down, after all getting through the boilerplate of how wonderful this is going to be.  And under Section 3.2 Security, it says:  "Brand indicators are a potential vector for abuse."



LEO:  Oh.



STEVE:  Yeah, no kidding.  "BIMI creates a relationship between sending organization and email receiver so that the receiver can display appropriately designated indicators if the sending domain is verified and has meaningful reputation with the receiver.  Without verification and reputation, there is no way to prevent a bad actor [and then gives] exxample.com from using example.com's brand indicators and behaving in a malicious manner."  Yeah, exactly the problem.



LEO:  Well, good.  This really worked.



STEVE:  And then, get this.



LEO:  So really this isn't about verification.  This is about putting your logo on the email is what this is.



STEVE:  Yes.  And the last line:  "This document does not cover these verification and reputation mechanisms, but BIMI requires them to control abuse."  In other words, we're going to - won't this be fabulous to have logos in email that cannot be spoofed.  We don't know how to do that.  But, boy, we've got everybody all revved up.



LEO:  By the way, if you get an email from me, it will be signed by me, either using an S/MIME cert or a PGP signature that is verifiably mine, and you will know it's me.  And that exists and has existed for more than a decade.  Oh, well.



STEVE:  Yeah.



LEO:  It doesn't have the logo in it, though.  That's nice.



STEVE:  I know.  I saw some reference to them having approached the CAB Forum, the CA Browser Forum.  And I thought, oh, that's kind of interesting.  Imagine if the certificate authorities could sign a logo.  In that case you could use the same certificate authority system to assert that this logo is associated with this domain.  So maybe.  But anyway, none of it's there yet.  And unfortunately, let's hope none of this happens.  I mean, they're already launching this.  They're like, oh, they've got specs and protocols, and Yahoo email is doing it now, even though it's completely - none of the antispoofing protection is there.  And of course the moment that PoPal shows the PayPal logo in email, it's like, okay, no one can trust that anymore.  So, bad idea.



LEO:  Bad dog.  Bad dog.



STEVE:  I don't know what they're thinking.  Anyway, free electricity.  What could possibly go wrong if you have free electricity?  Well, cryptocurrency mining might be expected to pop up anywhere you have tech-savvy, well, university students  who are not paying their own electric bills.  And sure enough, it turns out it's becoming a big problem.  I got a kick out of an online posting which was titled "University Found Out I'm Mining Bitcoin in My Dorm."  And the posting reads:



"I have free electricity here at ASU, and I decided to use Nice Hash to mine some bitcoin.  I've been mining for a month with no problems with a pair of GTX 1080 Ti's," but, you know, some big GPUs.  "But I just received an email today that I need to uninstall my mining hardware."  And this person writes:  "Now, I'm pretty sure this is not based off my electric bill, as I mined with my laptop for a bit at the same time, and they said I had two devices mining."  Meaning they probably saw two connections from his dorm room, two MAC addresses with mining protocol.  He says:  "Now I'm wondering how I can get around this problem.  Should I use a VPN, or is there another way to disguise my mining?"



So this was part of a larger story that I got a kick out of that was basically saying that, unfortunately, universities' security is generally not tiptop, and that university servers are being besieged with cryptocurrency miners, as we might expect.  But there's that problem, and then there's a problem that students, who do not have to pay their own electric bill because it's just part of their tuition or whatever, I mean, your individual room is not typically charged for electricity.  They're saying, hey,  free electricity, let's fire up some GPUs, and we'll be able to heat the room and get some cryptocurrency at the same time.  So, yeah, of course.  I imagine we will see universities become a little savvy, as apparently this ASU has, and start detecting and probably blocking cryptocurrency mining where possible.



LEO:  Does it use a known port?  Would you have to do deep packet inspection?



STEVE:  Yeah, you would need to look at the packets in order to see what was going on.  But I imagine universities are doing some good firewalling anyway, just in order to deal with all of the antics.



LEO:  Yeah.  Can you imagine?



STEVE:  Tor, oh, my god.  Torrents and who knows what.  Yeah, it's like, yikes.  So meanwhile, Bleeping Computer reported that Google has announced that, effective yesterday, April 2nd, the Chrome Web Store review staff has stopped accepting new extensions, new Chrome browser extensions, that perform cryptocurrency mining, and that existing Chrome extensions that perform cryptocurrency mining will be delisted sometime in late June.  So it turns out that, until yesterday, Google was explicitly allowing deliberately clear mining with Chrome, if mining was the extension's sole purpose, so it was registered in the Chrome Store as a currency miner, and if the user was informed in advance that their computer's resources were going to be used for this CPU, basically, hardware-intensive computer-draining task.



James Wagner, who's the Extensions Platform Product Manager for Google, said that, unfortunately, about 90% of all extensions which do mining are attempting to slip that by their user.  And they're not complying in one way or another with those policies.  So they're not doing user-mediated mining, they're actually doing cryptojacking on the sly.  So Google is going to start tracking those things down, and they're not going to accept any more that they know of.  And that would allow them, the moment they see that the one got by, to just yank it without - by formally changing their policy.



And if anyone's wondering, remember that Chrome includes its own Task Manager under the More Tools menu item that will show the percentage of CPU being consumed per tab.  So Chrome breaks it out by tab.  And if something is using up a lot of your system, if Chrome itself is consuming 90% of your system's reserves, you can determine which tab that you've got running is actually doing that.  And it may be that you've visited a site that is doing some mining on your browser without your permission.



The Under Armour folks who publish MyFitnessPal, a very popular fitness app that has about 150 million users, lost control of their login information.  All of their 150 million users' username, email, and hashed passwords were stolen.  Everyone affected has been notified by email.  There's no information, however, on how the passwords were hashed.  They did say that no credit card information or other information appears to have been accessed, since that's stored in a nonbreached database.



So the protocol is what we've become accustomed to after such breeches.  We don't know how strongly protected the hashes were, so we don't know how difficult it will be to reverse the hashes to determine what the passwords were.  But certainly, if you are a MyFitnessPal user, and you did not get email from them, change your password.  And if it happens that you were not using a unique password at MyFitnessPal, you will want to make sure that, because a bad guy could reverse the password potentially and determine what your plaintext password was and would have your username and email, they might go poking around trying to see where else they can log in with the same information.  You want to make sure that, if you've reused that information anywhere else, you get that changed.



So Leo?  "Star Trek:  Discovery."



LEO:  Oh, yeah, you've binged it.



STEVE:  I did.  Lorrie and I...



LEO:  Trying to save a buck.



STEVE:  Lorrie and I, we loved it. 



LEO:  Oh, good.



STEVE:  So I just wanted to follow up, for what it's worth.  It's on CBS All Access.  You can get it for seven days free trial.  It's 15 hours, 15 episodes, and absolutely worthwhile.  It is, as I mentioned when I talked about it before, it is not Jean-Luc's Prime Directive Federation.  It's gritty and sort of adult and really, really good.  So I just wanted to follow up and say that we're really happy that we watched the first season, and we'll wait for another season to get done and probably, I would say, this time pay for a month in order to watch it over the course of a few nights.  It was really good.



LEO:  Good.  That's good to know.



STEVE:  I got really one of those classic perfect notes from a SpinRite user, Dan Martins in Fairbanks.  And I don't know if there's any Fairbanks other than Alaska.  I was just assuming Alaska.  But the subject was "SpinRite did it again."  And he said:  "Dear Steve, I'm writing to pass along my mother's ecstatic joy and thanks for what SpinRite did for her.  I've been listening to you and Leo for years, and I purchased SpinRite mostly to support you and everything you do for us.  I've used it from time to time to check on the drives in my systems, and it may have [I think he meant] been helping to keep everything working without any trouble, as we know it can.



"However, my mother's computer did not have SpinRite's maintenance help, and it died without warning.  Well, really there was warning, but she did not know to read the signs.  She explained that it had been taking longer and longer to start up in the morning, but she heard that computers slowed down, so she figured that was normal.  And naturally she had not backed up about three months' worth of work on the sequel to her first novel.  The copy she had on a thumb drive would not read back, and it looked like a complete loss.



"Since I'm my family's go-to computer tech, she called in a panic and explained that she and her husband had tried everything, and they feared the worst.  But I knew one thing they had not tried.  Her computer has a DVD, so I created and sent her a bootable SpinRite CD.  After it arrived, I helped them start SpinRite running and told them to call me back when it was finished.  Since I've been listening to you and Leo for years, we all know how this story ends.  When she rebooted her computer, everything was back to full speed like it was new; and, more importantly, her nearly finished second novel was completely readable."



LEO:  Ooh.  Oh, wow.  That's huge.



STEVE:  Yes.  "She couldn't believe it, but I knew to expect it.  Having had a 'near death' experience, she will be backing up to multiple drives from now on.  So a happy Easter was had, and 'thank you' hardly seems like enough.  Your avid listener and supporter, Dan."  So Dan, wow, thank you so much.  And I'm glad SpinRite was able to come to the rescue.



Okay.  A little bit of closing the loop.  I got a kick out of this one Doug J sent with the subject "COBOL" because remember, what was that, that was the second most dreaded language or something?  First was JavaScript, believe it or not, and COBOL was second worst.  And so he said:  "Hello, Steve.  Unfortunately, COBOL is still the old and crumbling foundation for PeopleSoft.  I dread it because," he says, "once a year I have to look at it and hope I do not have to make some change, fearing unintended consequences."



And I'd heard of PeopleSoft before, but I forgot what it was.  Well, it turns out that it's one of those things Oracle purchased.  PeopleSoft, Inc. was a company that provided human resource management systems, financial management solutions, supply chain management, customer relationship management, you know, all those HRMS, FMS, SCM, and CRM junk that you hear about.  Oh, and enterprise performance management software.  Perfect.  I mean, that's what you want to write in COBOL are  those sorts of things.  "As well as software for manufacturing and student administration to large corporations, governments, and organizations.  It was an independent corporation until Oracle purchased it in 2005."  So I guess they're going to milk it for whatever revenue it can still generate with its COBOL code, and then maybe just quietly put it out to pasture, who knows.



Anyway, I got a kick out of the fact that my mention last week of COBOL stirred up one of our listeners.  And Greg V said, his subject was "Cloudflare's Quad 1s and Privacy."  And he asked:  "If DNS UDP traffic passes through your ISP unencrypted, how can it be private?  Isn't it a simple task for the ISP to track the queries?"  Then he signs off, "SpinRite Owner Greg."  And of course, as we know, yes, as long as you're using UDP, no privacy, no authentication, man-in-the-middleable, and subject to all kinds of exploitation.



Where we are headed, however, at least initially with our browsers, will be to establish, I imagine, when we launch the browser, the browser will come up and reach out to its preconfigured DNS over TLS provider like Cloudflare or like Google, bring up a tunnel, and then all of the browser's DNS resolution will then go through that tunnel in order to give us all the benefits of security and authentication and the benefit, for example, in the case of Cloudflare, of extremely good performance.  So for now we don't have that yet.  Quad 1 from Cloudflare over UDP gives us very good speed.  And of course, as I covered before, you can use GRC's Benchmark to find out how it compares.



And, by the way, I didn't say, I would love to hear back from our listeners what other people find using the Benchmark.  I'll be happy to share that with our broader listener base if people want to send me or tweet their results from running the GRC Benchmark, having added 1.1.1.1 and 1.0.0.1, and then do the Benchmark.



And, finally, Scott T on the East Coast.  His subject was "Secure Email / Transition Away from Gmail."  He says:  "I'm looking to move away from Google Mail and was wondering what options you recommend besides spinning up one's own server.  Or, if you do recommend spinning up your own server, what's the best way to make it as maintenance-free as possible?"



LEO:  If you have to add, do not.



STEVE:  Yes, Leo.  Yeah, thank you, perfect.



LEO:  Can I recommend FastMail? 



STEVE:  Please.



LEO:  FastMail.com is awesome.



STEVE:  Yes.



LEO:  They are very active in the IMAP community.  It's the best IMAP implementation I've ever run into.  Lots of nice features.  It's not free, but I took my email off of Gmail, and I use FastMail entirely.  I used to use Gmail for spam filtering, but I don't even feel I need that anymore.  They have very good spam filtering.  So highly recommend FastMail.com.  They're really good.  But I'm going to listen to what you say about ProtonMail.



STEVE:  Well, so, yes.  So FastMail.com, it's absolutely something to consider.  And I was hoping you were going to chime in and tell us who your provider was.



LEO:  Yeah, I love them.  Love them.



STEVE:  So ProtonMail has been sort of floating around.  It was, first of all, I get the feeling across the board that the founders' hearts are in the right place.  It is, I would say, to every degree possible, encrypted email service.  It was founded four years ago, in 2014, by some guys at CERN, the well-known research facility in Switzerland.  On the desktop, ProtonMail offers web browser, JavaScript-driven, client-side encryption.  So it's browser based on the desktop.  And there's JavaScript running there which performs encryption of the contents in a way that they don't get the key.  So this is essentially TNO email service.  Trust No One email.  But of course doing this is not simple because you still need to be able to transact email with other people, with other ProtonMail users and so forth.



So let me sort of get into some of these details.  It's also worth noting that they offer iOS and Android apps.  And it was a recent, like last week, update in their mobile offerings that brought them back onto my radar.  On the web side, since November of last year they've been encrypting the user's contacts.  So the user's key that they don't have is used to encrypt and store the user's contacts on their server.  Last week, that feature was added to ProtonMail's iOS and Android apps so that that's available for the first time there, as well.



So ProtonMail is operated by Proton Technologies AG, which is a company based in Geneva with servers in two locations in Switzerland, which of course places both sets of servers outside of U.S. and EU jurisdiction, which may be important for some users.  The default account setup is free, and the service is sustained by optional paid or just voluntarily.  You can pay for services, or you can just send them some money to support them.  And it's been a success.  As of a year ago, January 2017, they had over two million users.  It was initially a crowdfunded startup.  I think it was Indiegogo that launched them four years ago, and they were looking for $100,000, and they instantly got $500,000 and some odd dollars.  So it was clearly something that was interesting a lot of users.



So they explain on their site, under Anonymous Email and protecting their users' privacy, they say:  "No personal information is required to create your secure email account.  By default," they write, "we do not keep any IP logs which can be linked to your anonymous email account.  Your privacy comes first."  Under Open Source / Free Secure Email they say:  "We believe email privacy should be available to all.  That's why our code is open source, and basic ProtonMail accounts are always free.  You can support the project by donating or upgrading to a paid account."  And Easy to Use / Security without the hassle:  "ProtonMail can be used on any device without software installed, meaning web-based.  ProtonMail's secure email accounts are fully compatible with other email providers.  You can send and receive emails normally."



Now, here's where it gets interesting because they've done a lot.  And I did get a kick out of, as I mentioned at the top of the show, under Physical Security they mentioned:  "ProtonMail's infrastructure resides in Europe's most secure datacenter underneath 1,000 meters of solid rock."  So, okay.  But what matters to us, of course, is the technology.  So ProtonMail uses a combination of public key crypto and symmetric encryption protocols to offer end-to-end encryption.  When a user creates a ProtonMail account, their browser generates a public key pair.  So of course that's the public key and its matching private key, and those are RSA keys.



It'd be nice, maybe at some point they will evolve to elliptic curve keys, since that would make them much smaller and handier.  But for now, RSA keys.  And actually they're 4096-bit RSA keys, so they're not messing around.  The public key is used to encrypt the user's emails and other user data.  The private key capable of decrypting the user's data is symmetrically encrypted with the user's mailbox password.  This symmetric encryption happens in the user's browser using AES-256, so as good an encryption algorithm as there is.



Upon account registration, the user is asked to provide a login password for their account, and ProtonMail also offers users to log in with two-password mode that requires a login password and a mailbox password.  The login password in that case is used for authentication.  The mailbox password encrypts the user's mailbox that contains received emails, contacts, and user information, as well as that private encryption key.  So that allows them to store the key for you.  But then, for example, if you went to a browser where you hadn't been before, and you wanted to have full access, you would use both of these authentications, the second one doing a local decryption of your matching private key.  So they never have it, but you're able to essentially retrieve it in encrypted form from them.



Then they say:  "Upon logging in, the user has to provide both passwords.  This is to access the account and the encrypted mailbox and its private encryption key.  The decryption takes place client-side, either in a web browser or in one of the mobile apps.  The public key and the encrypted private key are both stored on ProtonMail servers.  Thus ProtonMail stores decryption keys only in their encrypted form, so ProtonMail developers" - that is, the ProtonMail people - "are unable to retrieve user emails nor reset user mailbox passwords."



So again, with true security comes responsibility.  It is TNO.  It is Trust No One.  So they can't perform recovery for you.  You've got to make sure you store these things safely.  And so this system, as they wrote it, absolves ProtonMail from storing either the unencrypted data or the mailbox password, divulging the contents of past emails, and decrypting the mailbox if requested or compelled by a court order.  In other words, they can't do any of that.  They don't have the keys.



ProtonMail exclusively supports HTTPS and uses TLS, and they've got a great TLS.  Ephemeral key exchange to encrypt all Internet traffic between users and the ProtonMail servers.  4096-bit RSA SSL certificate was signed by QuoVadis Trust/Link and supports Extended Validation, Certificate Transparency, Public Key Pinning, and Strict Transport Security.  So they did all of the right things, checked all the boxes.  In fact, they hold an A+ rating from Ivan Ristic's Qualys SSL Labs for their servers.  Then a couple years ago they added native support to the web interface and their mobile apps for PGP.  So they support Pretty Good Privacy.  This allows a user to export their ProtonMail PGP-encoded public key to others outside of ProtonMail, enabling them to use the key for encryption, for mail coming back to those users.  And they said that the ProtonMail team plans to support PGP encryption from ProtonMail to outside users.



And then the last bit of coolness that I got a kick out of - because, again, you've got security between your client and them.  What about outbound from them?  And that's of course the bugaboo with email, unless you know that your intended destination is a PGP user.  So they said:  "An email sent from one ProtonMail account to another is automatically encrypted with the public key of the recipient."



Remember, they have the public keys.  They don't have the private keys.  And that's cool because that allows them - so any email incoming is immediately encrypted with the recipient's public key, and they can't decrypt it from that point.  And inter-ProtonMail traffic never leaves their servers, and essentially it comes from the user, and it's immediately encrypted with using the public key of its ProtonMail recipient and then waits there for that recipient to pick it up.  So, for example, it would be easy if you wanted to set up a little network among a group to all use a free ProtonMail account, and all of your email stays on their servers and is always kept encrypted, and only the recipient is able to decrypt it after it's been transferred into their inbox.



They say:  "Once encrypted, only the private key of the recipient can decrypt the email.  When the recipient logs in, their mailbox password decrypts their private key to unlock their inbox.  And now email sent from ProtonMail to non-ProtonMail email addresses may optionally be sent in plaintext or with end-to-end encryption.  With encryption, the email is encrypted with AES under a user-supplied password.  The recipient receives a link to the ProtonMail website on which they can enter the password and read the decrypted email.  ProtonMail assumes that the sender and the recipient will have previously exchanged this password through a back channel.  Such emails can be set to self-destruct after a period of time."



So that's a really cool solution for wanting to encrypt something to somebody who doesn't themselves - who is not tech savvy, doesn't support, doesn't know how to do PGP or perform their own decryption.  So the idea would be you would come up with a password, get it to them through some other channel, fax it to them, carefully dictate it over the phone, whatever, so that they have that.  You then are able to send them email which you encrypt with that password.  What they get is a web link, a clickable link that takes them back to ProtonMail over HTTPS with TLS, and so complete security.  And they enter the password, which on their browser performs symmetric decryption so that even ProtonMail never sees the decrypted email.  It's decrypted on the recipient's browser so that they're then able to receive the mail.



So as I said, they thought through essentially every possible scenario to extend the email encryption envelope as far as possible.  And I just, you know, given the technology is open source, is auditable, clearly these guys have their hearts in the right places.  They don't want to be able to respond to court orders, and they've designed a system whereby they can't.  You as the user need to make sure you don't lose your passwords because inherently they are unable to provide them to you.



But in return for that, you get a very nice, Trust No One, free or optionally paid or donation-supported email system based in Switzerland that does allow you to get encrypted content to people who otherwise don't know how to do anything with encryption, with the obligation being to somehow get the decryption password to them so that you're then able to exchange mail.  So a very cool solution, and one worth looking at if somebody wants to move away from Gmail.  And of course, Leo, your FastMail is just sort of a straightforward email system.



LEO:  It's, yeah, not encrypted.



STEVE:  If somebody doesn't want all of that rigmarole.



LEO:  Encryption's up to you.  It's sad that, and I understand why, but that GNU Privacy Guard or PGP never really caught on because it solves the problem.  It doesn't solve the metadata problem, but it solves the problem of encrypted email, Trust No One and all of that stuff.



STEVE:  Yup, it does.



LEO:  But it's too hard for people to use, and just nobody uses it.  S/MIME certificates are easier, but nobody uses that, either.



STEVE:  Yeah.  In fact, I was talking with the Padre on Know How last week, and we talked about that.  The way I look at it is if the only reason we have moved from that land where Firesheep was able to acquire people's browser sessions and impersonate them, is that it was all done for the end user.  That is, users don't have to do anything with encryption.  All they do is, like, oh, look, I have a padlock now, or my key's not broken any longer.  Or, oh, look, the URL is in green.  I guess that's good.  Green means good.



The point is that it was servers that did this, and browsers that did this.  End users had to do nothing.  And even getting websites to do it was like pulling teeth.  We had to make it free so that it didn't cost anything, and it was all automatic, and you just, like, hook up.  And, oh, look, your server automatically does Let's Encrypt and off you go.  I mean, what we've seen is that we have to drop the bar so low in order to get people to do anything more, that finally we've got that with HTTPS.  So that suggests that there's just no way that that was going to happen with email unless, again, unless it was just automatically somehow magically done, and no one had to lift a finger.  So what we ended up with is, where it was important, the tools are there to do it and to make it pretty easy.



LEO:  It's just it's a measure of how people don't really - as long as people understand, they don't really care.  This is a postcard, they don't seem to really care.  So the real issue is the key exchange is a difficult thing, and that's something, by the way, PGP has solved beautifully.  There are these public key servers.  Any PGP or GNU Privacy Guard implementation will have a lookup feature that looks it up on the servers, an ability to post yourself on the servers so there's a directory.  If people want to have my PGP key, for instance, they could search for my email address or my name on the servers, download it, and add me.  And it's very simple, I mean, it's a system that really works.  And I really think that's the problem is just nobody really cares.



STEVE:  Yeah.



LEO:  Yeah.  The nice thing about that is, I mean, ProtonMail is not impervious to a warrant from the Swiss authorities.  And with the CLOUD Act, it's probably not impervious to U.S. authorities, either, in the long run.  But PGP, only you have the key.  They'd have to come to you and say unlock it.  And at least you'd know.



STEVE:  True, true.



LEO:  At least you'd know.  I just don't, you know, I'm glad ProtonMail exists, and I've had an account since 2014.  I mean, I signed up early on.



STEVE:  Well, and for what it's worth, remember, as with between PGP users, between ProtonMail users it's the same.  I mean, it's encrypting before it leaves the browser and decrypting only when it is received by the other end.  So ProtonMail does offer the same end-to-end encryption, and no attack on their servers legally or physically can reveal the contents.



LEO:  A Gmail to Gmail email is encrypted in place and in flight.  Only Gmail to Gmail.  And it's unclear whether Google has access to the mail.  I think they do because they have antispam tools.



STEVE:  Well, and Gmail is only encrypted over HTTPS in flight.



LEO:  Yes, that's right.



STEVE:  So instead of using a local private key.



LEO:  Right.  I use a 4096-bit key in PGP.  But, I mean, it's not that - there's no point.  I don't have, I mean, you know. 



STEVE:  Yeah.  In fact, even Padre, who like you has been a PGP user from the beginning, he said, "I think I have maybe three people.  I've got, like, three."



LEO:  I have a long list of names in my key list.  But the entire conversation I've ever had with any of these is, "Uh, is this set up correctly?"  "Yes, it is."  And that's it.



STEVE:  Hello, can you hear me?



LEO:  It works.



STEVE:  Yes, I hear you.



LEO:  And I do, every month I get a couple of emails from people saying, well, you're the only one I know who uses it, so is it working?  And I'll respond, encrypted if I can, back to them, and that's about it.  I had one guy, and he stopped, used to send me news articles encrypted, like suggestions for stories to cover, which was fine.  I mean...



STEVE:  Yeah.  And I said when I was talking to Padre, it's like, I mean, I just have no need for it.  I've never bothered.  If I ever needed to send something encrypted, I would write it in Word.  I would encrypt it myself.



LEO:  Right.  You'd use miniLock or something like that.



STEVE:  Exactly, and then attach it to an email and say, here.  Here's a blob.  Decrypt it using our prearranged solution.



LEO:  I've had my miniLock key on my website for several years, and I've never received anything.  But what are you going to send me?  I mean, so anyway, there is a very clever password manager that uses PGP.  It actually is, I think, extremely - if I were going to design a personal - but you have to maintain it.  It does a tree, a folder tree of all your passwords in PGP encrypted.  And then I guess it must have an in-the-clear name, so that leaks a little bit, but only locally.  It's called Pass, if anybody wants to look at it.  It's the only application I know of.  And then there's Keybase, which I use, and that's another way people can keep track of your key.



I do sign my mail.  And I think that that's important.  I think that that is, back to this BIMI story, if people might impersonate me, I do think it's good that, if you get an email - people impersonate me all the time - if you get an email from me that's not signed, not to trust it.



STEVE:  Yup.



LEO:  Steve Gibson's at GRC.com.  That's the Gibson Research Corporation.  And you know what you'll find there?  Many, many things.  First and foremost, of course, SpinRite, the world's best hard drive maintenance and recovery utility.



STEVE:  Yabba-dabba-doo.



LEO:  We hear another great story about SpinRite.  You'll also find SQRL, Perfect Paper Passwords, ShieldsUP!, the DNS...



STEVE:  The DNS Benchmark.



LEO:  ...Benchmark, wow, must have.  InSpectre, still relevant.  Lots of stuff.  That's all free because Steve does it all out of the goodness of his heart, including this show, which you could find there.  He has audio and very nicely crafted, handwritten, well, I'm sure she types, but human-written transcripts.  That's using her hands.  Elaine Farris does those each week.  It takes a few days to get them out, but that's a great way to search the site for stuff, and it's a really nice thing to have.



We also have audio and video on our website, TWiT.tv/sn.  In fact, if you're there, subscribe.  You might as well get every episode.  You want to have the collection, get all 657.  It's a wonderful thing to have.  And you can watch it, too, if you have a hankering to see how Steve does his Vulcan signoff there.  And what else?  Oh, we do it every Tuesday at 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to watch live at TWiT.tv/live and join us in the chatroom at irc.twit.tv.



STEVE:  Yay.



LEO:  Yay.  Thank you, Steve.  We'll see you next time on Security Now!.



STEVE:  Okay, buddy.  Thanks.



LEO:  Bye-bye.



STEVE:  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#658

DATE:		April 10, 2018

TITLE:		Deprecating TLS 1.0 & 1.1

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-658.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss Intel's big Spectre microcode announcement, Telegram not being long for Russia, U.S. law enforcement's continuing push for "lawful decryption," more state-level Net Neutrality news, Win10's replacement for Disk Cleanup, a bug bounty policy update, some follow-up to last week's Quad-1 DNS conversation, why clocks had been running slow throughout Europe, and then a look at the deprecation of earlier versions of TLS and a big Cisco mistake.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of security news, including the end of the line for TLS 1.0 and 1.1, and a response to the haters who are mad about 1.1.1.1.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 658, recorded Tuesday, April 10th, 2018:  Deprecating TLS 1.0 and 1.1.



It's time for Security Now!.  Oh, yeah.  Oh, yeah.  I know, I imagine thousands of people all over the world saying, "Oh, yeah."  Yeah.  Of course, it's not really a surprise to them since they pushed the Play button and all that.  But still, you're playing the right show.  This is Security Now!.  Steve Gibson, he's our...



STEVE GIBSON:  The technology has not let you down.



LEO:  It has not let you down.  The deterministic...



STEVE:  Unlike the technology we discuss every week here, which pretty much does let you down.



LEO:  Yeah, it's letting you down all the time.  That's Steve Gibson, GRC.com.  He's the creator of SpinRite, the world's finest hard drive maintenance/recovery utility, but also ShieldsUP! and, gosh, so many useful tools and utilities.  And he's been spending the last 13 years every Tuesday with us, just waving his hand like that.



STEVE:  Look how my hand blurs when I move it really fast, Leo.  You can hardly even see it.



LEO:  Steve's on the spectrum.  Just don't mind him.  No, just teasing.



STEVE:  I'm joining Mark.



LEO:  He's joining Mark Zuckerberg in the land of the...



STEVE:  Woohoo.



LEO:  ...perpetually perplexed.



STEVE:  Ah, yes.



LEO:  No, Steve is a lifesaver for those of us who follow technology and want to understand it better and understand the perils of it, and that's what we do each and every week.



STEVE:  And Leo, this Picture of the Week I have been saving.  I've been wanting to find an opportunity to deploy it for quite some time.



LEO:  I don't think I can fit it on the screen.



STEVE:  I had to squeeze it down.  I wasn't happy to have to squeeze it down.  But you can still see what it is, I mean, very clearly.  But we'll get to that in a minute.



LEO:  Okay.



STEVE:  Today Episode 658, for those of you who are counting along, is titled "Deprecating TLS 1.0 & 1.1," which follows on from our recent discussion of the final ratification and the adoption of 1.3 and all the goodies it offers.  And this was triggered by a blog posting of DigiCert's, where they were talking about their own discontinuation on April 1st.  And they said, no, no, no, this is not a joke.  We are discontinuing support for 1.0 and 1.1 of TLS.  I don't know whether they yet support 1.3, but they certainly do 1.2.  That's where everybody is.



So I want to talk about that because there was one just perfect quote from their blog that I really enjoyed; and I thought, okay, it makes sense now to talk about how we get out of where we have been.  And as we know, it's never easy to stop using something which is working and isn't obviously a problem.  Even when it is obviously a problem, we often still just say, oh, well, like v4 of IP, IPv4.  We're all still using it.  We're not supposed to be because it ran out.



LEO:  It ran out.



STEVE:  It ran dry.  There is no more.  But, oh, look, I'm still using it, and so are you.  So what the heck.  Anyway, we're going to talk about Intel's big Spectre microcode announcement;  Telegram inching out the door of Russia, or actually being pushed out the door; U.S. law enforcement's continuing push for, in air quotes, "lawful decryption," which actually unfortunately amounts to unlawful encryption, which I just - the way that bill was worded is annoying.



Also we have more state level, local U.S. state level Net Neutrality news.  Lawrence Abrams at Bleeping Computer is a fan of a coming Windows 10 replacement for Disk Cleanup I wanted to share, since you and I were just talking about Disk Cleanup and how we recommend it to people.  We've got a bug bounty policy update which is sort of interesting, I think very interesting, actually.  And something kind of icky occurred to me as I was pulling this all together that I want to share.  Some follow-up to last week's Quad 1 DNS conversation.  I upset a lot of our listeners who were all Quad9 happy.  It's like, wait a minute.  Anyway - you changed your quad.  Also why clocks had been running slow throughout Europe all year until just recently.  Really.  That's in our Miscellany category.



And I forgot to put it in the show notes until I'd already published them, so I'll put it right here, right now.  Friday the 13th is in, what, three days.  That's when "Lost in Space" becomes available on Netflix.



LEO:  Yay.  Yay.  Can't wait.



STEVE:  So I haven't seen it.  I'm not making any recommendations.  Those will come next Tuesday, more than likely.  But I just wanted to give everybody a reminder of that.  And then we'll wrap up by talking about the process, like where we stand as an industry with the various versions of TLS.  All of SSL, that's all gone now.  So now we're on TLS, and we've got four of them.  We've got 1.0, 1.1, 1.2, and 1.3.  And we're saying goodbye to the first two.  So we'll talk about that interesting update and news for our listeners, oh, and the Picture of the Week when we come back from our first sponsor.



LEO:  Can't wait.  I'm not sure what the point of it is, but I'm sure we'll find out.  I'm sure we'll find out.



STEVE:  Oh, yeah.  Well, I was going to say also, before I forget, this just in from a fan and friend of the show Simon Zerafa.  And that is that today's Patch Tuesday updates for Windows 7, which is the same as Windows Server 2008 R2, have known crashes in them.



LEO:  Oh, isn't that nice.



STEVE:  So, whoops.  After you install the update, your Windows 7 or Server 2008 R2 is known to leak memory out of its SMB server - the Server Message Blocks, file and printer sharing - and a blue screen.  Whoops.  A stop error occurs on computers that don't support the Streaming Single Instruction Multiple Data, SIMD Extensions 2, that's SSE2.  So maybe not crucial.



First of all, if your chip is SSE2-compliant, then you don't have to worry about blue screening.  And if you're not doing file and printer sharing and/or a memory leak, I'm not sure how severe it is, that is, how quickly the memory gets depleted.  But I guess this must have happened to Microsoft just as they were getting ready, committed already to roll it out because today is the second Tuesday of April, and we've got patches with known problems.  So if you're a little more conservative, you may choose - they have said they're right on this, that they're working on a resolution and will provide an update in an upcoming release, presumably without waiting till next month.  So you might want to wait a bit longer for them to get that fixed.



And our Picture of the Week, Leo.



LEO:  Yes.



STEVE:  Someone sent this to me, and I had the same feeling you did.  It's like, what, so?  Then I realized, wait a minute.  This looks like your Ace hardware store, whatever.  They've decided that these bolt cutters are high-end expensive.  And unlike the rubber mallet to the right, which is just hanging on the J hook, these they're going to lock down with - looks like a plastic-sheathed wire rope.



LEO:  Oh, I see what you're...



STEVE:  It's got padlocks at each end.



LEO:  Yeah, yeah.  Oh, good idea.



STEVE:  And so there's no way that a thief could arrange to steal those bolt cutters which are secured by exactly what it is they are designed to cut.



LEO:  This is a subtle one, Steve.  I did not get it right away.  But if you look at the middle one...



STEVE:  Yes, that's the perfect one, yes.



LEO:  The lock wire is poised to be cut by the bolt cutter.  I think I'll take this one, it works nice.



STEVE:  And you can test it on your way out the door.



LEO:  Maybe that's the point, it's a tester.



STEVE:  That's right.  Ah, love it.  It's just wonderful.  This is a variation on the fox guarding the henhouse.  It's like, okay, wait a minute.



LEO:  I didn't get it right away, and now I do.  That's hysterical.  I love it.



STEVE:  It's just wonderful.  So thank you, whoever it was who sent that to me.  It was months ago.  I capture these, and I save them.  I have a place to keep them.  And when there's nothing that is salient to the topic of the week, I pull one out of the archive.  And this one, I've just been waiting to share this because this is just - this is one for the ages.



LEO:  Nice.  Very funny.



STEVE:  Yeah.  Okay.  So on a not-so-funny note, we have Intel having given up.  That's the announcement in the April 2nd, which is apparently their final, revision to their so-called "Microcode Revision Guidance" PDF.  I've got the link in the show notes for anyone who's interested.  And this will drive an update to my InSpectre app.  I imagine I'll just - it's so quick that I'll do it quickly, maybe this evening.  And this shouldn't come as a surprise to our listeners, who have heard me every single time talk about how I've been surprised Intel could even fix this problem with microcode.



Well, it turns out they can't fix all their chips, is what this comes down to.  Up until this April 2nd chart, they had listed all of the various processors by CPU ID and architecture and family and lineage and everything, all these wacky names they come up with.  They had them under categories of pre-beta, beta, production candidate, and production, which were like, okay.  Or pending, I think.  There was an earlier one for a while, too.  It's like, okay, we haven't gotten to that one yet.  Or we have it, we're working on it, we're almost going to release it, and we have.



Well, they've added another category:  Stopped.  And so for their description on "Stopped" they said:  "After a comprehensive investigation of the microarchitectures and microcode capabilities of these products, Intel has determined to not release microcode updates for these products for one or more reasons including, but not limited to the following."  And they have three bullet points:  micro-architectural characteristics that preclude a practical implementation of features mitigating Variant 2 of Spectre.  Second bullet point:  limited commercially available system support software, which I guess means there's nobody who's willing to push any patch we came up with out into an actual platform.  And the third bullet point:  based on customer inputs, most of these products are implemented as "closed systems" and therefore are expected to have a lower likelihood of exposure to these vulnerabilities.  Which I think that's reasonable.



So while this will be disappointing to those people who do have those processors, first of all, they are the older chips.  They're generally older than five years old.  So it may well have been actually the case that, even had Intel produced microcode, there were no OEMs around who were interested in bothering to push out the fix.  So what we have now with this PDF is a final comprehensive listing of the state of the chips that Intel has.  Most of them are in production.  There are just a few, I mean, with pages and pages of this PDF, that are in "production candidate," so they're in just the final prerelease state.  And then there is a handful that are "stopped."



So what I'll do, now that we have this, what I'll do is to add that knowledge into InSpectre and incorporate that so that it will affirmatively let people who run InSpectre know whether there is microcode available for their chip somewhere, which presumably Microsoft will eventually incorporate, if they haven't already, into their own microcode, on-the-fly boot-time microcode patch; but also if Intel has affirmatively now said microcode is never going to happen for this chip.  So, sorry about that.



Anyway, so we do have sort of the final position.  And again, while yes we're unhappy that we've got chips that have this problem, as I've said repeatedly, I'm impressed that the microcode had the ability to be patched at all.  I mean, the idea of microcode is not to just make up your own instructions after you've shipped the chip and changed things around.  The idea is to compromise on a fully hard-wired design which would require a wafer of silicon probably three inches across, just because the Intel chipset, the instruction set has become so unwieldy over the years, to sort of compromise and have a microcode step which runs faster than the instruction rate so that some of the more complex instructions - and, boy, there are some hairy instructions in Intel land - you would actually implement those as sort of a processor inside a processor, rather than just all brute force with electronics and gates.



So it makes sense.  And so I've been impressed that they could do as much as they have, and relieved that for a problem this bad it's even been possible to have an after-sale update to the chips.  And I am thankful also that Microsoft has stepped up to the task of dynamically installing that microcode provided by Intel because, as we know, our current supply chain for systems is lacking.  It's maybe better, well, maybe it's about the same as it is for mobile devices.  I was going to say historically we've always talked about how sad the situation is with old Android phones that are just never going to see a fix.  This is, you know, we're in a similar situation with our desktop and laptop systems.  I have several that I would love to get patched.  I mean, and they're Lenovo.  But it's like, eh, they're too old.  So no.



But my newer ones, they already got patched.  And a couple Dell laptops got patched, too.  So I've been impressed with Dell's performance, actually.  I think they're one of the most impressive OEMs in terms of following up on this and dealing with even their older devices which they're still keeping in their support loop.  So anyway, probably tomorrow InSpectre will get updated.  And I did, as I said I might last week, update the DNS Benchmark for the first time.  When was it I finished that?  In '08, I think, so 10 years.  Yeah, it was '08.  So I did it, and we'll talk about that in a minute.



I wanted to mention that, as we have been discussing, Russia doesn't like people having conversations inside their borders that they cannot decrypt.  So four days ago Russia's telecommunications watchdog agency filed a lawsuit against Telegram, which we've been discussing this was happening, which is within Russia a very popular encrypted messaging app, of course, that we've talked about often because Telegram uses a controversial rolled, I mean, it's bizarre.  It's more than controversial.  It's just wacky, their own encryption system where just all kinds of stuff is flying in every direction, and no actual cryptographer, you know, cryptographers just scratch their heads and say, what?  But Telegram has responded by saying, okay, so break it, and we'll give you some money.  But no one has bothered because it's just wacky.  Still, it's wacky and no one has broken it, as far as we know.



So this telecommunications watchdog in Russia has asked a Moscow court to rule in favor of restricting access to the service inside Russia's borders - in other words, blacklisting Telegram.  So as we know, the FSB, which is Russia's main intelligence service, had requested access, formally going through the steps.  And I doubt that they ever expected to get it.  But they formally requested access to Telegram's encryption keys so that the FSB could access encrypted messages sent through Telegram.  Telegram, of course, based in the U.K., refused to help.  So the FSB filed a lawsuit to compel access to those encryption keys, claiming their need to have access for national security and the agency's fight against terrorism.



So thus far every court that has been asked to rule on this as they go through the machinations has ruled, not surprisingly, in favor of the Russian state.  And the final decision came last month when Russia's Supreme Court ruled that Telegram must hand over users' encryption keys to FSB agents without a court order whenever agents requested access.  So a very broad request.  So Telegram responded through their attorneys that they have no such plans to do that.  They are just making, I guess it's a face-saving attempt to fight back, filing a lawsuit against the Russian government at the European Court of Human Rights, saying  this is a breach of Russian citizen human rights.  That'll go nowhere because ultimately Russia is going to do what it wants to do.



So what will happen is, with the next ruling from the lawsuit filed four days ago, when that happens - and everyone's expecting this Moscow court to rule favorably again as it surely will - then Telegram's Internet domains will be added to Russia's official national block list, which all telcos and ISPs are required to abide by, which will in turn shut down Telegram throughout Russia.  So certainly at this point no Telegram user within Russia, as a consequence of these gears grinding away as they have been for quite some time, will be taken by surprise when Telegram suddenly stops working for them.  They'll have to do something else.  So anyway, that's where that is, and I imagine that maybe by next week or the week after we'll just have a short blurb saying Telegram, as is the case in China, no longer functions within the borders of Russia.



The U.S. Senate, meanwhile, is getting set to take yet another run at mandatory lawful decryption of communications.  It was just after the San Bernardino terrorist attack where, of course, which we covered at the time, Farook's work iPhone was not easily decrypted, that our senators Dianne Feinstein and Richard Burr, those were the two at the time who pushed some legislation which they authored that never got to a vote because it generated a lot of blowback and critique as a consequence of the fact that it was seeking, essentially, mandated backdoors in popular consumer products.



Well, they're back again.  Actually not with Richard Burr, but Dianne Feinstein is, this time with the current Senate Judiciary Chairman Chuck Grassley.  So they're getting ready, along with the Deputy Attorney General of the U.S., Rod Rosenstein, to make another attempt at essentially, the way they're putting it, outlawing the use of encryption which the government cannot have access to.  So I would imagine it would be under warrant.  But as we know, it would follow the traditional U.S. search warrant model.



I don't know how this is going to end.  My feeling is that law enforcement and our intelligence services are going to continue to push on our legislators to produce a solution, and that our legislators are not going to agree with academia and cryptographers and industry that there is no way to do this without severely - essentially, as we know what the argument is, there's no way to put in a backdoor that the bad guys cannot also get access to.  So we'll see.  My sense is we're going to go around and around and around until finally the legislation occurs that will compel consumer products to have a means for providing lawful decryption under a court order.  I'll be surprised if that doesn't happen.



And on the topic of legislation, a little better news maybe, the state of Oregon yesterday signed into law their own state Net Neutrality legislation.  It makes them the second state to have done so in the U.S.  The state of Washington had previously done so.  And Washington's legislation was aggressive and may in fact fail under challenge because of course all of the ISPs are loudly declaring that they're going to fight to overturn this, or to have some sort of federal legislation enacted to prevent states from going their own way.



What Washington state did was they flatly stated that no ISPs or broadband carriers can provide any favorable treatment based on the source of the content that they're carrying.  What the state of Oregon did was deliberately designed to withstand a challenge in court, which is to use, again, the power of their purse to say that no state government entity can purchase that bandwidth from an ISP that is not abiding by the same Net Neutrality principles and laws that the FCC is in the process of ending.



And as I was looking through this to sort of get a sense for what was different and what it said, there was one glitch that was unfortunate, and that is that the legislation does acknowledge the possibility that even state agencies might be forced to choose an ISP that violates Net Neutrality when there are no other options.  In other words, the purchasing requirements will not apply when an ISP is, in the words of the legislation, the sole provider of fixed broadband Internet access service to the geographic location subject to the contract.  And as I've often stated, in my own location here, right in the middle of Southern California, I have no alternative other than Cox and cable.  There's no fiber.  There's no high-speed copper.  I have one broadband provider.  And as we know, unfortunately there are many areas in the U.S. where that's the case.



So anyway, certainly what we're seeing is the intention and execution of some pushback from the end of FCC's previous U.S. administration protection for the concept of Net Neutrality.  And here I sort of feel like the reverse of this constant go-around with encryption.  My sense is Net Neutrality's probably - the principle is probably going to win out over the long term, where I feel less bullish about encryption winding up that way.



Windows 10 has - we've spoken of the upcoming next major feature update, which is still, I guess it hasn't been officially named because it's still being referred to as the "so-called Spring Creators Update."  But it's getting ready to happen.  That will be adding a feature which Bleeping Computer's founder Larry Abrams is bullish about, so much so that he intends to stop recommending, as he has been, the always-there-since-the-early-days Disk Cleanup utility in Windows.  Disk Cleanup will still be present, but Windows 10's Spring Creators Update adds the not quite as catchily named Free Up Space Now option.



So if anyone is on the inside track, or the early adopter track, whatever it is called, you may have already seen it.  And certainly, when it is released, the Spring Creators Update, you put the word "storage," you just search for "storage."  That'll bring up the panel in Windows 10, and there's a new option there:  Free Up Space Now.  And what Larry likes is that it, unlike Disk Cleanup, which may sometimes show options for which there is nothing to be cleaned up - that is, it'll show you like zero megabytes or kilobytes savings are available.  Unlike that, Free Up Space Now only enumerates those things that can be saved, and you don't have to go through that second step you do with Disk Cleanup that, Leo, you were aware of and mentioned when I brought it up last time, where you have to explicitly say "Clean up system files," click that, and then it goes away again for a long time and then finally comes back and says, oh, look, I found 10 more gig.  It's like, yes.  So anyway, it's all in one, Free Up Space Now, coming to Windows 10 Spring Creators Update.



LEO:  Have you done your taxes, Steve?



STEVE:  Yes.  They're all handed into my CPA.  Mine is one of those kind of complex business and...



LEO:  Oh, I bet it is, that's right.  Anybody who has a business, it's crazy, yeah.



STEVE:  Yeah.  So more than I can handle.



LEO:  So I'm just checking here, let me just do "winver" real quickly because I just got an update, but I don't think it - yeah, I'm still at 1709. 



STEVE:  Yup, and I just checked mine while you were talking, and same thing here.  I got cumulative update for Windows 10 v1709 for x64-based systems.  And in the Update History it shows the feature update.  I think I applied that in November.



LEO:  Yeah.



STEVE:  So that's the thing that we're looking for is that maybe it'll be called the Spring Creators Update.  Did they actually officially call the previous one the Fall Creators Update ever?



LEO:  Yeah.  It was the FCU, Fall Creators Update.  And they're refused to say a name.  I think they don't want to name them.  But you'll know if you hit the Windows key, type "winver."  It will tell you what version.  And if it's 1709, that's the previous version.  The new one will be 1803.  So you will know that, anyway.  But they stagger these rollouts.  It starts today, but it won't get to everybody today.



STEVE:  Can you ask for it tomorrow?  If you, like, say, "Hey, I want an update?" 



LEO:  That's a good question.  I mean, if you were an insider you'd get it.  But you'd also get the next one, which you don't want.  They usually do an ISO version that you can download from Microsoft, but that takes a while.  So I don't think there's any way to force it.  Not that I know of.  Let me look in Advanced Options.  No, I don't think so.  I'll tell you what, I'll ask Paul and Mary Jo that tomorrow.



STEVE:  Cool.  I'll be listening.  So, okay.  It was an interesting article that CyberScoop picked up suggesting sort of a different side to the bug bounties issue.  And in pulling the pieces together, I came up with my own, maybe even kind of more worrisome problem.  So since they're sort of tempering my own recent bullishness about bug bounties, I wanted to share it for some perspective.  So last Wednesday CyberScoop published an article titled "The bug bounty market has some flaws of its own."  It got its inspiration for the article primarily from a woman who founded Microsoft's own bug bounty program almost five years ago, Katie Moussouris.  She was subsequently the Chief Policy Officer at HackerOne, that's the commercial bug bounty-managing service, and has now founded her own firm, Luta (L-U-T-A) Security.



Well, it happens that today, Tuesday, April 10th, is the first day of the three-day CYBERUK 2018 Conference which is being hosted by the U.K.'s national Cyber Security Centre, which is part of GCHQ.  And Katie was there this morning presenting, and somebody was tweeting some of her presentation.  And she was also then subsequently tweeting up a storm afterwards.



Her home page, that is, the Luta Security home page has a banner that says, "#1 Solutions Architects for Vulnerability Disclosure or Bug Bounty Programs."  And in fact the homepage currently announces that the U.K. government announced at the CYBERUK Conference - the one she's at and was presenting to, she was in the first track - that its new National Cyber Security Centre, the NCSC, is partnering with her firm to invite a select group of security practitioners in the community to participate in what they call the historic first U.K. government pilot for vulnerability coordination.  So the U.K. is beginning to sort of move in this direction also.



And, finally, her page concludes with sort of the footer, "Bounty Smarter, Not Harder."  I looked through the slides that were presented, a bunch of her tweets.  And the upshot of it all is that there is some concern that more of the industry than just Katie share that there could be some downside to these big monster quarter million dollar bug bounties, which major players in the commercial industry have recently been announcing.  And essentially it creates a distortion where an award for finding a problem is often larger than the annual salary of someone whose job it is for there not to be any problems.



And Katie's position, after years of her involvement in the industry, is that it is a mistake for major companies to simply solve their problem by offering a massive reward for finding problems in the wild; that there's a danger of sort of just focusing less on prevention in the first place rather than curing them after the fact.  As we know, many of these massive companies are cash rich.  And so it's easy to - and you can also sort of see there's a PR upside to it.  You know, that's been part of our motivation for covering this, is it's like, oh, look, Microsoft is really serious about wanting their bugs to be stomped on.  They're offering - and of course Intel, too, in the case of these side channel attacks - a quarter million dollars through the end of the year for anyone who finds any more of these.



So that makes headlines.  It makes a big splash.  The concern is that it's easy to throw money at that, and it also tends to sort of maybe create some perverse incentives.  So in an interview with CyberScoop, Katie said of bug bounty seeking that, first of all, it's not cut and dry, that motivations vary among hackers, but most are driven by a combination of three factors:  financial compensation, which is most obvious, I guess; peer recognition, which is certainly the case and has been probably a major motivator prior to the big cash awards; and then also just the pursuit of intellectual happiness, loving what they do.



There is a book that was just published by MIT Press in academia where the editors of a series of articles to which Katie was a contributor, they did an analysis of the market for software flaws and revealed, I guess somewhat unsurprisingly, that this so-called "defensive bug bounty market" is very stratified with a small number of highly skilled individuals essentially winning the lion's share of the rewards.  So it is not uniform.  In the dataset they analyzed, the authors found that a small number of key sellers, as they call them, of discovered bugs are finding the overwhelming majority of all the bugs.  So not uniform distribution, highly spiked.



In one dataset provided by the guys at HackerOne, that as I mentioned we've talked about them a couple times now, the site and service that manages bug bounty programs for other corporations, just 5% of bug hunters were responsible for finding 23% of the flaws.  Which actually it's not like 5% found 95% of the flaws.  So 5% found almost a quarter of them.  So, yes, it's not uniform.  It's also, however, not massively skewed incorrectly.



So I guess perhaps the best advice is not to give up your day job; that it's probably going to take many, many hours of poring through disassembled code or maybe writing your own custom fuzzing pentesting code.  So it ought to first and foremost be something you enjoy doing.  You ought to think of it more as a hobby than as a career, some way to fulfill any spare time that you may have, which could have a big payoff if you find something significant.



So with all of that said, while I was thinking about this, it occurred to me that there was another dark side that I don't think anybody - certainly we haven't talked about it.  I've never read anything about it, and it hadn't occurred to me until now.  But it's a very worrisome possibility that, again, could be created by these quarter million dollar bounties.  Okay, so we have a few givens.  We know that it is extremely difficult to create flawless code, so much so that, despite the fact that everybody tries, nobody does.



We also know that it's extremely difficult to find flaws by inspection.  I've often talked about how, as a developer myself, when faced with a bug, there have been times when I have stared at the code, at a page which apparently has a problem, looking at it and not seeing it.  And it's not until single-stepping with a debugger onto the problem that you finally go, oh.  I mean, you can't not see it at that point.  And it's like, oh.  Things like type conversions that are happening by the compiler where somewhere, in some include file somewhere, something was typed to be the size of a byte, and your code is assuming it's the size of a word or a long or something.  I mean, those sorts of things where you just...



LEO:  That's why you need a compiler.  You keep doing assembly language, of course you're going to get bit by that.  I'm curious.  Do you ever do modern things like test-driven design?  Do you write tests for your code, you know, for your function?  I don't even know.  Do you do functions?



STEVE:  Yeah, yeah.



LEO:  You write little macro functions; right?  I mean, I presume that's how you work.



STEVE:  True.  And arguably, Leo, where SpinRite is 14 years old, and the DNS Benchmark is 10 years old...



LEO:  There's not many bugs in that thing, yeah.



STEVE:  There are no bugs.  I mean, I have an approach.  My approach is incremental creation of the code where I'm testing as I go.  I do get tripped up when I think of something later that I wish I had done earlier.



LEO:  And add it, yeah, that's how you can screw stuff up badly.



STEVE:  Yeah, because then you get side-effects.  But anyway, so we have difficult to create flawless code, difficult to find problems by inspection.  They're just hard to see.  And we know that backdoors, okay, so "backdoors" is the term we use typically for deliberately inserted loopholes in a system's security.  But as we've previously seen, not all backdoors take the form of a retrospectively obvious thing like a hard-coded password in firmware.  We often find that as we have discussed often in routers, where someone discovers, oh, look, password 1234 backwards was left in the firmware of some D-Link or Linksys or who knows what router.  I don't mean to pick on any one particular brand because they've all had problems that way.



So the point is there are situations where you discover what is obviously a backdoor that somebody put in there.  But we also know that not all backdoors are retrospectively obvious.  And we've talked about some.  For example, you could have what you might consider a gray door occupying a gray area, for example, as we saw with that dual elliptic curve DRBG, the Deterministic Random Bit Generator, which may have been a deliberate design flaw, designed and implanted into that spec by the NSA.  We don't know, but it looked worrisome.  And it was there.  And they did come up with it.  And they then paid RSA a large sum of money at the time for RSA to make that the default of the four, even though it was the slowest of all of them.  So, like, it's suspicious, but not proof.



So my point is that you can also have gray doors, which sort of occupy this gray zone, this gray area.  So, okay.  Offering an award of a quarter million dollars to the "discoverer," and I'm going to put "discoverer" in quotes now, air quotes, of a critical flaw in a mainstream commercial system can also create a powerful incentive for such a flaw to first be deliberately implanted by a confederate accomplice who has code writing access.  In other words, whereas the NSA - and remember that we've wondered that the U.S. intelligence services might not have agents installed in major corporations with this kind of access, doing things on behalf of national security.  Well, a quarter million dollars is a lot of incentive for some people, and not all coders in all corporations are happy with their jobs.



So my point is it's so extremely difficult to create flawless code when you want to that no one expects it any longer.  This tends to remove suspicion of flaws which we've now all come to accept, which in turn means that it's absolutely possible for a very subtle, yet still critical flaw to be deliberately added to existing code in such a fashion - now, you can't go back and edit that code to put the flaw in because, again, we have version control systems, and someone's going to come along, and your name is going to be on the change that was made, and you're going to have to explain why you did that.  But when writing a bunch of new code that all appears to be hunky-dory, it's entirely feasible that somebody could deliberately put in a flaw that cannot be seen by inspection, doesn't cause anything to fail, cannot be found casually, and for which a compelling subsequent discovery miracle story could be fabricated with a substantial award at the other end.



And I'm not suggesting that the employee who wrote the code could win the prize for finding it.  But this is why you need the confederate operating outside of the company as an independent researcher to, quote, "discover," unquote, this problem and win himself a quarter million dollars in cash.  So I was thinking about, as I was running through the nature of these incentives, with projects as big as they are, as many coders as there are, as well meaning as most programmers are, as these bounties become massive, it does create various sorts of perverse incentives.  And it just occurred to me that, wow, that might make a great subject for a believable work of fiction.  Let's hope it stays that way.  Who knows?  And we probably would never know because, if it's done well, it would just look like it was a mistake.



I wanted to follow up on last week's discussion of Quad 1 and Cloudflare.  I got a bunch of what I would have to describe as "blowback" from our listeners, who were all happy with Quad9, and happy with my previous happiness of it, and basically saying, hey, what about Quad9?  Now you're all happy and jumping around about 1.1.1.1, but I'm all set up on 9.9.9.9.



LEO:  Oh, come on.  How hard is it to switch?  Geez Louise.  Oh, I have to go in and change my DNS?  Oh, man.



STEVE:  I know.  So...



LEO:  Steve, make up your mind.  You can't change your mind.



STEVE:  So I did want to follow up.  As I mentioned...



LEO:  I don't think you changed your mind anyway.  It's just another thing you could use, a different thing to use.



STEVE:  Exactly.  It offers different benefits.  And it turns out that Quad9 also offers DNS over TLS.  Apparently not DNS over HTTPS.  I looked around for that.  I couldn't find any sign of that.  But DNS over TLS, and I'm going to clarify the distinction in a second.  I did want to mention, as I already did at the top of the show, but to embellish a little bit, that as promised, GRC's DNS Benchmark received its first update in a decade because I added 1.1.1.1, 1.0.0.1, and 9.9.9.9.  And in looking at the feedback that I had asked from our listeners, most people confirmed that 1.1.1.1 was fast.  But my experience with 1.0.0.1 being a lot slower was not generally experienced by others.  In fact, I would have to say I was an outlier in that regard, so you ought to discount what I said.



Again, nobody can be sure about the access to their own DNS without testing it.  That's the only way to know.  So thus GRC's DNS Benchmark, the only way to know.  Many people reported that, to my surprise, that 1.1.1.1 was faster than their own ISP's DNS, not to the degree that the chart which was last week's Picture of the Week showed, which was bizarre, suggesting that ISPs were 68ms.  I mean, 1.1.1.1 was 7ms, and the ISPs were 8ms, which is reasonable.  But for what it's worth, 1.0.0.1 was right there in the running.  And there were several reports of the older Level 3 DNS - remember 4.2.2.1, 4.2.2.2, 4.2.2.3, and 4.2.2.4 - that those guys were faster than 1.1.1.1.  Again, the only thing that is consistent about DNS is that it is inconsistent, massively, from one user to the next.  So the only way to know is to test it yourself.  But I wanted to correct the record about 1.0.0.1 being so much slower.  I appear to be an outlier there.



And, yes, I had forgotten about Quad9.  And remember that this was the service, they're using the 9-dot network, which is IBM's space.  And so this was put together by people who wanted to create a secure DNS to filter out bad guys so that, for example, phishing emails, when you clicked on the link in the phishing email, nothing would happen because that domain name would have already been taken out of service by your DNS provider if you were using 9.9.9.9.  So if you want that, that's still a good  deal.  It didn't seem to have the performance, frankly, that Cloudflare's services did.  But again, as I said, it's one thing for the Benchmark to say that this is faster than that.  Hover the mouse over and left-click on the chart, and it'll show you numbers.  And in fact the printout of the summary also shows you numbers.  You get a complete numerical printout with much more resolution to the numbers.  So you can see if it's a difference that matters.  But at least you have the actual numbers.



Okay.  So DNS over TLS versus DNS over HTTPS.  DNS over HTTPS is what I spoke of the last two weeks, the so-called "DoH," DoH protocol, which is the one that Mozilla has incorporated into v60 of Firefox, which is now the nightly Firefox and will be released on May 8th.  And even then I think you still need to go into about:config and turn it on.  But what that means is that the typical Firefox user, with a little bit of configuration - and maybe at some point I'm sure they'll surface it at the UI so you just click a checkbox and say yes, I want to use secure private DNS.  That's over HTTP.



As far as I know, Cloudflare is the only provider of that version of secure DNS.  I could not find an indication of it from Quad9 or Google DNS.  But all three of them - Cloudflare, Quad9, and Google DNS - do support sort of the more standards-based original secure DNS over TLS.  That's the one where it runs over port 853.  And it's different enough that I think I'm going to give it a podcast soon.  It's specified fully, that is, DNS over TLS is fully specified in an RFC 7858.  And you have to play some games with the certificate validation because you don't have the same infrastructure in place that you automatically have with HTTPS and web browsers and domain names and so forth.  So I'm probably going to do a podcast talking about DNS over TLS because it's clearly a solution that makes sense.



But the HTTPS version supported by Cloudflare is something people could play with now.  Cloudflare has open source DNS over HTTPS clients for Linux, Mac, and Windows 32- and 64-bit.  There's a page, if you just google DNS over HTTPS, that'll take you to - or maybe say "developers," add "developers" in there.  That'll take you to the page where you can download a client for any of those platforms - Linux, Mac, or Windows.  You install the client on your system.  It sets up essentially a DNS proxy running on the localhost IP, so 127.0.0.1, and defaults to port 53, which is the DNS port.  Or you can set it to a different port outside of the lower 1024 because those are reserved for root or privileged access, and so you would need to be able to install it as a privileged service in order to use that.



But the point is you then change your DNS to 127.0.0.1, and you're now using DNS over HTTPS to Cloudflare.  So you don't have to wait, my point is, for Firefox to only be making that available for its browser, and not for other browsers, and not for your whole system.  If you change your system, or even your router, to 127.0.0.1, then all of the processes in your system will look at the localhost proxy to get DNS.  And speaking of routers, it is already built into the pfSense router from v2.3 on, and it will be surfaced at the GUI in 2.4.4.  So it turns out even now it is a simple process to add this to pfSense, although I should mention pfSense is supporting DNS over TLS, which is the RFC standards-based solution.



So anyway, I wanted to spend a little more time, update where the Benchmark is, the fact that, yes, Quad9 supports it; Google DNS supports it.  If you're a pfSense user,  you can use DNS over TLS for your whole network in order to get security and privacy.  And I'm sure we'll be talking about this in the future.  And I didn't mean to abandon all of the Quad9 people.  As you said, Leo, I'm just looking at something new and shiny.  And actually it is faster.  So if speed really is more important to you than the security that 9.9.9.9 offers, well, 1.1.1.1 does shave a few milliseconds off the queries, again, depending upon your actual performance.



LEO:  Yeah.  I already have malware protection in other ways.  So I just like the idea of having encrypted DNS, that's all.



STEVE:  Yup, exactly.



LEO:  If you don't need the other features of Quad9.  Plus kind of I trust - I'll be honest with you.  I don't know who Quad9 is run by.  I mean, I know who it's run by, but I don't know who they are.



STEVE:  We know Cloudflare.



LEO:  Yeah, know Cloudflare.



STEVE:  We know their heart's in the right place.



LEO:  Nothing against the Quad9 guys, I just don't know who they are.  I literally know the Cloudflare guys.  I mean, they're sponsors, but John Graham-Cumming's an old friend and all that.



STEVE:  So.



LEO:  Yes, sir.



STEVE:  This week Europe's electric transmission lobby announced that oven, microwave, and alarm clocks across the continent were finally no longer six minutes behind.  What?



LEO:  What?



STEVE:  So, yes.  By resolving a grid dispute between Serbia and Kosovo...



LEO:  Oh, my.



STEVE:  ...and running the continental grid at a slightly higher frequency than normal.



LEO:  So it is, it is the frequency.



STEVE:  Yes.  Now, it is the case that many systems still rely upon AC for their timing.



LEO:  No.



STEVE:  Because it's more accurate than crystals.



LEO:  If it's exactly 60Hz or whatever.



STEVE:  Well, and see, that's just it is you can count the cycles over a long period of time and adjust.  And so what actually happens is that AC line frequency is not constant because, for example, in the summer during peak hours where there's a heavy load, the electric load actually slows down the generators and lowers the AC frequency because there's just too much demand.



LEO:  So the frequency's directly tied to how fast the generator is spinning.



STEVE:  Yes, yes.



LEO:  Wow, wow.



STEVE:  And so what happens is there's, like, really, really accurate time bases, like cesium beam because the decay of cesium atoms is well known.  So there are absolutely really good time references.  And so the grid, the cycles of current on the grid are counted.  And at night the generators are sped up in order to catch up and make up for the slowdown that can occur during the day.



LEO:  I didn't know that.  So they know that clocks and other timing devices are doing this, so they actually compensate.



STEVE:  Yes, yes.



LEO:  I'll be danged.  I'll be danged.



STEVE:  And so I have a picture in the show notes of a quartz crystal's frequency variation as a function of temperature.  And it's pretty good.  But because a quartz crystal is itself mechanical, and it's using the natural harmonic oscillation in order to set the frequency, if you heat it up, it expands, gets bigger, and so it slows down.  And when you make it colder, it shrinks, gets smaller, and speeds up.  So quartz crystals, which are used for timing, they require temperature compensation if you want them to be accurate over the long term.  The beauty of using AC is that, while there might be a short-term drift, for example, in the middle of a hot summer day, that gets made up for.  So over the longer term, nothing, nothing is more accurate than counting zero crossings of AC.



LEO:  So it's actually not an old-fashioned technology.



STEVE:  Right.



LEO:  It's a legit one.  I'll be darned.  I'll be darned.



STEVE:  Yes.  So get this.  Well, when it all goes correctly.



LEO:  Well, that's one of the things in Japan.  I'm going to Japan, and some parts of the country are 50Hz, and some are 60.  So you have to buy a clock for the right region, I guess.



STEVE:  Right, right.



LEO:  Wow, wow.



STEVE:  So get this, Leo.  Between mid-January and early March of this year, a grid dispute between Serbia and Kosovo resulted in 113 gigawatt hours of unmet demand from Kosovo, meaning more demand than there was power.  Since Kosovo is part of the Continental Europe Power System, the unmet demand on this 25-country system pulled more power than was available, resulting in a grid-wide slowdown of the spinning generators, which were unable to keep up with demand.  This in turn led to a, again, system-wide, a Europe-wide decline in frequency to an average, because Europe is on the 50Hz system, to an average of 49.996Hz.  And this in turn caused AC time-based clocks, which were dutifully counting the zero crossings of the current and then dividing by 50 to get seconds, as a consequence of the over demand from Kosovo, time was passing too slowly.  And over the span of three months, clocks throughout all of continental Europe lost six minutes.  So they were running six minutes behind.



LEO:  But, now, these are clocks in a closet somewhere.  Most people would have said, oh, the clock's behind, and set it.



STEVE:  Well, yes.  And in fact that's a very good point, Leo.  So what happened is, last month the European Network of Transmission System Operators, ENTSO, publicly admonished Serbia and Kosovo for not properly balancing their grids according to previous agreements.  The group wrote:  "This average frequency deviation, that has never happened in any similar way throughout the Continental Europe Power System, must cease.  ENTSO is urging European and national governments and policymakers to take swift action," you know, after three months.



LEO:  Swift.



STEVE:  Yeah, swift.  Two days later, on March 8th, the TSO, the Transmission System Operators from Serbia and Kosovo confirmed that they were back to balancing their grids appropriately.  And, finally, last week, just last week, so it took another month, ENTSO announced that it had restored the lost six minutes to clocks around the continent by maintaining a slightly higher than normal average frequency of 50.01Hz deliberately for a month.  In other words, somebody was counting and, oh, my god, it's been three months, and they knew how many cycles behind they were.  So for a month they went fast and put the lost cycles back into the power grid, and all the clocks came back.



LEO:  Unless you'd set your clock.



STEVE:  Ah, exactly.



LEO:  In which case you're fast now.



STEVE:  In my show notes I have here, ENTSO wrote:  "One of the effects is notably that the digital clocks geared by electric frequency are now back on time - that is, as long as you oven-clock owners within the Continental Europe Power System did not change your slow clocks to the correct time a month ago.  If you did, now you could be six minutes fast. But at least you're less likely to be late now."  So I thank - Ars Technica picked up on that little fun tidbit, which I got a kick out of.



LEO:  What a great story.  I'm guessing that the reason they did that is because there are clocks, I'm thinking of radio stations and other facilities where they have, you know, they don't manually set the clocks.



STEVE:  Untended, yes.



LEO:  Yeah.  And those are probably more mission-critical than your oven clock.  So your oven clock's probably not that accurate anyway, so you set it.



STEVE:  Well, and I'm sure you must have taken things apart the way I did as a kid.



LEO:  Oh, yeah.



STEVE:  The old-style clock motor had this spinning disk on the back of it.  And it was synchronized to AC.  I mean, it was following AC.  And if AC slowed down, so would it.  And if AC sped up, so would it.



LEO:  The cheap ones weren't smart enough to do averaging or anything.  They just figured, well, we'll trust the power grid.



STEVE:  Well, and again, remember, I mean, over the long term, nothing is more accurate because there actually are people out there...



LEO:  That's what surprises me.  That's amazing.



STEVE:  ...counting cycles.  They're saying, oh, we're 13 cycles behind after this grueling afternoon.  We'll put those lost cycles in tonight.  Isn't that cool?  I just love that.



LEO:  That's really interesting, yeah.



STEVE:  So I did get a nice note I wanted to share.  It's in the form of a question from J.D. Green on the 9th, which was yesterday.  Actually, I guess it was a tweet because I have an @Cybts1.  He said:  "Hi Steve.  Love hearing you on Security Now! each week.  On last show you spoke about an email about SpinRite.  I was wondering if SpinRite works on thumb drives.  I have one that seems to be corrupted, and I would like to recover its contents.  Thanks for all your work."



And of course he was talking about that neat note about the guy's mom who had her sequel to her book, her novel that was endangered because her computer died, even though it had been slowing down for a long time, and she probably should have taken a hint.  And then remember that she had also a thumb drive that was non-readable when it came to reading it.  And I don't know whether the guy who sent the email ever tried to recover the thumb drive.



But for J.D. Green and anyone else, yes, SpinRite works on thumb drives.  We have in the past covered a number of listener stories and feedback where their thumb drives that had critical data on it, they just assumed it was solid-state, and it was ber-reliable, and it would not, you know, like they could count on it.  And when it came time to read something from it, they couldn't.  So the trick with thumb drives is that they need to be recognized by the BIOS when you boot the system into SpinRite.  So the only thing you have to do, since BIOSes don't have plug-and-play capability to recognize that a thumb - typically they don't.  There are some that do, but typically they don't.  You need to have the thumb drive installed when you turn the system on.  Then the BIOS will see it, SpinRite will see it, and you can run SpinRite on it.



And as we've seen, we don't know for sure, obviously because we don't know what the nature of the damage was or is to any specific drive, but these drives have the ability to have SpinRite recover their data, and we've covered many stories of that happening.  In fact, it's sort of related, but Father Robert had an Android phone that he recovered this way, too, because the phone was able to look like a drive.  He was able to put it in that mode, run SpinRite on it...



LEO:  Oh, my god, that's awesome.  I didn't know you could do that. 



STEVE:  ...and recovered it.



LEO:  That's great.



STEVE:  Yeah, he shared that story with us.  So, yeah, very cool.  When all else fails, as your last resort.  Okay.  So, finally, I wanted to talk about winding down the earliest versions of TLS.  I talked about this, this was a blog post from DigiCert titled "Deprecating TLS 1.0 & 1.1."  And I loved the line from the blog.  The author, who's a communications guy who simplifies technology, said:  "...but on the Internet there's a big difference between 'nearly dead' and 'dead.'"  Which is exactly what we've seen.  Time and time again we've seen that old technologies that are not proactively terminated, they're zombies.  They just live on forever.



And so, for example, we finally, of course, got rid of MD4, then MD5, then SHA-1.  But, I mean, it took deliberate acts of, okay, we're absolutely no longer going to support this any longer, and then made that decision.  So I thought his point was exactly on, which is there's a big difference between nearly dead and dead.  It will live on forever unless the decision is made to terminate it.  And that's getting ready to happen.



So where are we in the world right now?  Well, first of all, we know that the majority version of TLS is 1.2.  That is, it was finished in '08, remember, 10 years ago.  So everyone's had plenty of time to get up to speed.  Even so, it took Internet Explorer five years to get it supported from the time it was released.  So there are some instances where people have been slow to adopt.



But as we talked about a couple weeks ago, 1.3 is finalized, so it's ready to go.  It turns out that there's very low use of 1.1.  1.1 was just sort of - it was a fix for 1.0.  But 1.2, which is an improvement over 1.1, has been around for a decade.  So for various historical reasons, because SSL v3.0 and TLS v1.0 are virtually the same protocol, just sort of had a renaming, there's been some hangover of TLS 1.0, more so even than 1.1 because, if you were going to change, you would go to 1.2, which as I said was finished a decade ago.  So there's a site, it's SSL Pulse.  It's actually a service of SSL Labs.  And Leo, you will find it fascinating.  It's SSLLabs.com/ssl-pulse.  And the guys at SSL Labs maintain a whole bunch of really interesting statistics about the state of SSL, well, actually it's now TLS, servers around the Internet.



Okay.  So put some numbers on this, of the 150,000, so 150K HTTPS-enabled sites which this SSL Pulse service at SSL Labs monitors, 88% support TLS 1.0.  Okay, 88% still support 1.0; 85% support 1.1.  So again, slightly stronger adoption or support for 1.0 than even the newer 1.1.  Whereas most of the Internet is actually using, that is, now remember, it's one thing for the servers to support all four versions - 1.0, 1.1, 1.2, and maybe 1.3.  What actually gets used is what the client chooses, well, actually the client and server together.



LEO:  There's a handshake; right?  I mean, there's some sort of conversation.



STEVE:  Yes.  Well, yeah.  So support, we need to separate "support for" versus "what's being used."  What's being used is 1.2 because the preference is to use the best, and servers are configured to use 1.2 if it's available from the client, 1.1 if it isn't, 1.0 if 1.1 isn't.  So there is, you know, you're always trying to negotiate the most recent protocol that both endpoints support.  So as a consequence, the fact that browsers have now, for at least five years, every major browser supported 1.2.  That's now what everybody is using.



But as I mentioned, IE did not support, even though TLS 1.2 was released in '08, IE did not support 1.2 until five years later with the 2013 release of IE11, which was the first version to support 1.2.  But that's five years ago, so now everybody's got that, too.  Also, Android versions prior to 5, which was released in 2014, only supported TLS 1.0, which interestingly represents nearly 18% of Android devices still in use today.  So a substantial portion of Android in use today is at prior to 5.0 and only supports TLS 1.0.



So Cloudflare, back in our discussion again, which as we know is one of the world's largest CDNs, Content Delivery Networks, has good visibility into what's going on at Internet scale.  And they recently shared that about 11% of traffic on their network still uses TLS 1.0, and a much smaller percentage, 0.38 using TLS 1.1.  And of course the rest are up already at v1.2.  But still, 11% using TLS 1.0 is a substantial bit.



However, deadlines are approaching.  A major change being brought on by the PCI standard, the Payment Card Industry, is happening at the end of June.  So exactly at the middle of this year, June 30th, 2018.  The Payment Card Industry standard requires the deprecation of TLS 1.0.  So any website that wants to maintain PCI compliance must stop supporting 1.0 at the middle of the year, June 30th, 2018.  And as we know, most websites already support 1.2.  So browsers, for the last five years, all major browsers have supported TLS 1.0, many for even longer than that.



So what's finally happening is that other companies have announced their plans to deprecate 1.0.  DigiCert already did on April Fools' Day, April 1st, Sunday before last.  They disabled support for TLS 1.0 and 1.1 for all their services including their website and API.  Another CDN, the KeyCDN, will end support for TLS 1.0 and 1.1, oh, on March 30th.  So they already did, too, two weeks ago, as will Cloud.gov.  There's a service, Fastly, will stop support of 1.0 and 1.1 on May 8th.  Cloudflare has announced its intentions to disable 1.0 and 1.1 on June 4th, so earlier in that same month that the PCI compliance requires that it be ended at the end of the month.  And Microsoft Office 365 will support only TLS 1.2 starting next Halloween.  So on October 31st support for the earlier versions of TLS ends.  So we are seeing a formal winding down.



Now, one consequence, remember, there are still some 1.0 gotchas out there.  The DigiCert blog posting noted that there are older, or just not yet updated, development tools which don't yet support 1.2, such as cURL, which are in wide use.  And since GitHub was one of the first major services to turn off TLS 1.0 and 1.1, some things that had been working suddenly were broken by this.  So GitHub did this in February two months ago, and that revealed some breakage in a number of developer tools which have since been updated.  So as we know, sometimes it takes breaking it in order to force something to get fixed.  It does look like now that we've got a good, strong, very secure standard, TLS 1.2, that was finished 10 years ago, that we're finally getting ready to say goodbye, forceably as is necessary, to 1.0 and 1.1.



LEO:  Tada.



STEVE:  The deprecation of the earlier two, yup.  And then eventually we will be moving to 1.3.  Probably not fast, since 1.2 doesn't have any known problems.



LEO:  It's pretty good; right?  We don't really need - yeah.



STEVE:  Mostly it's the perfect forward secrecy and some improvement in handshaking, and so connection startup speed.  But no known oh-my-god reason to abandon it.  Remember that 1.0 was subject to the POODLE attacks and a few of the other problems, which have since been mitigated by browsers jumping through hoops in order to avoid that.  So it'll just be good.  It's necessary to finally just say no to these older protocols so we can take support for their workarounds out. 



LEO:  Can you take it out of your browser somehow, like disable it?



STEVE:  Yes.  You can certainly do it at the server side, and there are options in the browser to have your browser no longer offer...



LEO:  Say I don't want to accept 1.0 or 1.1.



STEVE:  Yup.  That's the right thing to do.



LEO:  So look in your browser config.



STEVE:  Yeah, just google "disable TLS 1.0 Firefox" or Chrome or whatever your browser of choice is.



LEO:  Right.



STEVE:  And I know, for example, on IE, in the Internet Options there's a set of checkboxes for the various versions.  You just turn them off.



LEO:  It is conceivable that would keep you from getting on some ancient site.



STEVE:  It is conceivable, yes.



LEO:  But highly unlikely, I think; right?



STEVE:  Yes.  And I would argue, you know, maybe wait a few more months, like wait till there becomes a bit of a groundswell, and then any other consequences would end up surfacing.  It's sort of the way I was able to, thanks to DigiCert's amazing service, to get them to make a special cert for me for SHA-1, which didn't expire until midnight, like the very last day I could possibly do it, in order to keep GRC available for people with really old browsers until I had no choice but to move to SHA-256, which I then did a day or two before.  So it's the sort of thing where, unless there's a clear problem, you know, we've mitigated the known problems with TLS 1.0 and 1.1.  But still, you just finally have to say, okay, no more of this nonsense.  We're just going to stop supporting you.  Everybody's had a decade to catch up.  It's time.



LEO:  And by the way, you probably should turn off SSL 3.0, if that's in your settings, as well. 



STEVE:  Definitely.  Even I don't have that one.



LEO:  Even in your Windows XP?  You do not use IE.



STEVE:  Even I don't have it.



LEO:  You don't use IE on Windows XP, I'm sure, because I doubt that that has that as an option.  I mean, and most websites, it's not the website, it's the server that they're using.



STEVE:  Correct.



LEO:  And also most all websites are going to be updating their servers fairly regularly.  I mean, that's not a hard thing.  You don't have to redesign the site or anything, it's just built into the server.



STEVE:  Nope, nothing.  It's all the low-level protocol interaction.



LEO:  Very good, Steve.  A couple of quick updates.  One I've just been checking.  According to Paul Thurrott, Microsoft decided not to push out the Spring Update or whatever the hell they're calling it today.



STEVE:  Spring's sprung.



LEO:  Instead they pushed out an update to insiders.  So I guess there were some more betas they want to do before they push it out.  So it was supposed to be today, the Patch Tuesday for the month of April, but no.  Also, for those of you who subscribe to the Security Now! feed, and we love it, and use Overcast.fm as your podcast app, for reasons unknown, earlier today you might have heard a little house music stuck in the Security Now! feed.  As far as we could tell, this has nothing to do with us because it only happened if you're using Overcast.  But we're working with Marco Arment, its author, to figure out what the heck?  And I apologize if you were awoken during your morning commute by a little house music.  That is, it's not on our feed.  I checked the feed.  It's not in the feed.  So something happened, maybe an interaction between the feed and Marco's great software.  We love it.  We love Overcast.



STEVE:  Ah, those computers, Leo.



LEO:  You know?  You were talking about bugs earlier?  The most difficult thing, of course, is an interaction with the outside world.  And you never know what you're going to get from the outside world.  You never know.



STEVE:  Hate that, when you want the computers to actually do real work for you and get something done.



LEO:  I don't know what's going on.  Poor Marco.  He's debugging as we speak.



STEVE:  Cool.



LEO:  Yup.  Thank you, Steve Gibson.  You'll find Steve and all of the goodness of the GRC Corporation at the website GRC.com.  That includes, of course, SpinRite, the world's best hard drive recovery and maintenance utility.  Not just hard drives.  Android phones.  SSD cards.  All sorts of stuff.  GRC.com.  Buy SpinRite because that's his bread and butter.  But you'll find plenty of free stuff there, including the world-famous ShieldsUP!.  And SQRL is going to be there.  And, oh, man, I can go on and on.  Perfect Paper Passwords, Password Haystacks, all sorts of great security research information.



Oh, and maybe this show once in a while.  You might want to check that out, too.  You definitely won't get any house music if you go to GRC.com and search for a Security Now! episode.  He offers 64Kb MP3 audio plus nice transcripts by Elaine Farris, so you can read along as you listen.  Or search, and that's really the most useful thing.  You can search through all the previous 658 episodes and find anything you want and jump right to it.  GRC.com.



We have audio and video - video, who'd a thunk it - at our website, TWiT.tv/sn.  If you want to watch the show created live, all you have to do is tune in on Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  The stream is at TWiT.tv/live.  There's also an audio stream there, if you want to listen.  And if you do either of those live things, please join us in the chatroom:  irc.twit.tv.  And of course it's always a good idea to subscribe.  Use Overcast or Pocket Casts or iTunes, whatever it is you listen to podcasts with, and just make sure you get Security Now! each and every week, the minute it's available.  Thank you, Steve.  We'll see you next time, and have fun with "Lost in Space." 



STEVE:  Oh, yes.  "Lost in Space," Friday the 13th, everybody.  We'll talk about it next week, I'm sure.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#659

DATE:		April 17, 2018

TITLE:		Never a Dull Moment

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-659.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss AMD's release of their long-awaited Spectre variant 2 microcode patches, the end of Telegram Messenger in Russia, the on-time arrival of Drupalgeddon2, Firefox and TLS v1.3, the new and widespread UPnProxy attacks, Microsoft's reversal on no longer providing Windows security updates without AV installed, Google Chrome's decision to prematurely remove HTTP cookies, the Android "patch gap," renewed worries over old and insecure Bitcoin crypto, new attacks on old IIS, a WhatsApp photo used for police forensics, and an IoT vulnerability from our You Can't Make This Stuff Up department.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with the latest from Drupalgeddon2, the Telegram fiasco in Russia, TLS 1.3, and why you've got to turn off UPnP on your router.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 659, recorded Tuesday, April 17th, 2018:  Never a Dull Moment.



It's time for Security Now!, the show where we protect you and your loved ones online through the good auspices, the good offices of Mr. Steven "Tiberius" Gibson, although I might now want to call you "Dr. Smith."



STEVE GIBSON:  Oh.  We have that in the show notes.



LEO:  We'll be talking about that, I'm sure.



STEVE:  That's right.



LEO:  Steve is the guy in charge of GRC.com, the Gibson Research Corporation.  I guess that's the eponymous Gibson Research Corporation.  He's also the master in charge of SpinRite, the world's best hard drive recovery and maintenance utility.  And for the last 10, 11, 12 years he's been filling us in on what's going on in computing.  Hi, Steve.



STEVE:  Hey, Leo.  Great to be with you again.  Elaine had a question from last week's audio because I guess I made a comment that SpinRite was - we were talking about bugs and updating, and I said SpinRite was...



LEO:  Bug-free.



STEVE:  I said, like, 14 years old.  She immediately put her brakes on because she's listening to all this, she says, wait a minute.  Isn't it, like, 30 years old?  And so, well, what I meant to say was SpinRite 6.0 was completed in 2004, and so here we are in '18, and we're getting ready, I'm getting ready, of course every one knows, to start returning to and moving SpinRite 6 forward as soon as I get SQRL put to bed.  But so it was SpinRite 6.0 that is 14 years ago it was published.  And much like the DNS Benchmark, really I was talking about the DNS Benchmark and how I had updated it for the first time in 10 years because I finished it in - its last update was in '08 when I finished it.  So anyway...



LEO:  Yeah.  You're saying the current version of SpinRite is that old.



STEVE:  Yes, exactly.



LEO:  Which is remarkable for any software.



STEVE:  Exactly.  And I didn't have any - there was, like, lots of news.  I mean, like lots of news.  So it's almost a good thing, a blessing that there was no one huge, dominant, take up half the podcast story because then we wouldn't have been able to get to everything.  And there was just lots of interesting things.  So I called this episode 659, I just labeled it "Never a Dull Moment."



LEO:  That's the truth.



STEVE:  And, boy.  So we do have AMD's release of their much-anticipated and somewhat long-awaited, although hopefully they won't stumble out of the gate the way Intel did, their microcode updates for the Spectre variant 2 problem, which cannot be fixed without hardware fixes in the processor by the operating system working in conjunction with the processor, so that just happened.  We have some details about the very messy end of Telegram Messenger in Russia that we've been tracking for the last few podcasts.



The on-time arrival of Drupalgeddon2, I mean, it happened exactly as we anticipated it was going to.  And so we've got that.  Also Firefox and TLS v1.3.  A new and widespread UPnProxy is what Akamai is calling these attacks.  Microsoft's reversal on its no longer providing Windows security updates without AV installed policy, which they announced at the beginning of the year, I mean, I remember, we were all like, what?  What?  So the idea, well, we'll talk about that in a minute.  Google Chrome has made a decision which they announced to start as of October, which would be Chrome 70, to prematurely remove HTTP cookies, which is...



LEO:  What?



STEVE:  ...understandable, but, I know, kind of controversial.



LEO:  How do you do that, yeah.



STEVE:  Yeah.  So we've also got the so-called "Android patch gap," where Android OEMs have been discovered to be lying about how well patched their phones are.  A renewal of worries over old and insecure bitcoin crypto.  New attacks on old IIS.  A WhatsApp photo that was cleverly used by police for forensics.  An IoT vulnerability straight from our You Can't Make This Stuff Up department, and more.  So, yes.  As I said, never a dull moment.



LEO:  Yes.  So, yes, he says.



STEVE:  And some miscellany and a fun note, not a testimonial about SpinRite this week, but someone who's been trying, he was waiting to give one, and finally he got tired of waiting.  So anyway, lots of fun stuff.



LEO:  You know, sometimes the best testimonial is "I don't really ever have to use it.  I just - everything works."



STEVE:  That's actually kind of what he said.  It's like, okay.



LEO:  Yeah, haven't needed it.  I just SpinRite ahead of time, and everything's good.



STEVE:  That's right.



LEO:  All right, Steve Gibson.  I'm going to get the remote control out and turn your brightness up a little bit because you need every bit of it.



STEVE:  Yeah, I'm a little dim today.



LEO:  You need every bit of brightness you've got today.



STEVE:  A little dim.



LEO:  For those not watching the video, Steve's on a big screen, I feel like it's a Star Trek view screen behind me.  We've got to...



STEVE:  Yes, over your left shoulder.



LEO:  Yeah.  We've got to give him some brightness.  Go ahead.  You can talk while I brighten you up.



STEVE:  Ah.  Well, so anyway, yes, thank you.  Our Picture of the Week is actually about a story we will get to in a little bit, this UPnProxy.  But I liked it, this was part of the report that described this particular exploit.  And what's happened is that this is fiction meets reality.  How many times have those of us who are tech savvy sort of rolled our eyes when the plot of the cyber whatever it is involves the bad guy bouncing around the world and looping around and bouncing off of 19 servers, unquote, whatever those are supposed to be, before making the final connection, and consequently that's a big plot glitch is, oh, there's no way we can track him down because they routed through 19 other locations all over the world.  Well, that got a lot easier.



And this picture from this report that we'll be covering in a second shows an attacker, location unknown, connecting to a virtual private server that then hops through three so-called "proxy routers," which anyone looking at the picture will sort of recognize as, oh, I have one of those on my shelf.  And that's exactly the point is it turns out that something that we have long warned of, it was our Episode 358 or something.  I have it in the show notes, and we'll get to it a little bit later.  We warned our listeners to turn this thing off.  Well, it's now being abused widely to turn individual SOHO, you know, Small Office Home Office routers, regular residential consumer routers, into basically sort of the grand central of bad guy traffic.



So I thought this was a great picture because it demonstrates that, yes, what once was sort of fanciful and it's like, okay, well, maybe super advanced hackers could pull this off, but it's not really going to happen in the real world, no.  Now, I mean, you, too, can do this from the comfort of, I guess, your couch, or your mother's basement, or wherever you happen to be.  So anyway, sort of a fun picture we will get to.



And Leo, for the first - this actually, I was going to number this zero, except we don't have zero - our show notes items are not numbered.  But it actually, believe it or not, is the case today, April 17th, I guess as a consequence of the fact that April 15th fell on a Sunday, that must be why...



LEO:  Yeah, and Monday was like Emancipation Day in Washington D.C.  So it'll be Tuesday, yeah.



STEVE:  Ah, okay.  So in the U.S., for those who are not in the U.S., our so-called Internal Revenue Service, the IRS, this is our tax filing deadline date for the 2017 calendar year.  And one of the headlines I got a kick out of said that the IRS's e-filing system is - wait for it - "overtaxed."



LEO:  Ha ha.  That just sounds like a clever headline writer because what I saw was that the IRS didn't know why it wasn't working.



STEVE:  They don't, no.



LEO:  So it may not be overtaxed.  It may be something else.



STEVE:  Under attack, yes.  Yeah.  So basically our U.S. e-filing system, which one would expect to be heavily used, has not been working well all day.  And as we know, it is extremely difficult nowadays to keep a system online for which there's any reason for bad guys to want to have not be online.  There are so many ways now with DDoS amplification attacks and the ready availability of things that can attack in order to launch these attacks, that, I mean, as we've been talking about...



LEO:  Could be some 15 year old in Columbus, Ohio; right?



STEVE:  Yes, yes.  It doesn't have to be a nation-state mounting an attack against the IRS in the U.S., and probably isn't.  It's just somebody who says, hey, you know, this is the time that lots of people are going to be filing their taxes at the e-filing website.  So let's just take those guys offline.  And if that's the case, I mean, we don't know.  But I guess some IRS person was giving testimony in front of Congress, not a big high-profile issue, just some meeting somewhere, and one of the congresspeople said, "So, do you know that the e-filing system is offline right now?"  And he's like, "Oh, well, yes, we're looking into that, and we expect it to be back up later today."  Which apparently they have no idea what's wrong.  Oh, and the other...  



LEO:  It's been eight hours, at least last time I checked.



STEVE:  Yes.  And it says that it's a "planned outage." 



LEO:  No.  That's not true.



STEVE:  No, no, I saw a screenshot.



LEO:  That seems stupid, if you plan an outage on tax filing day.



STEVE:  Wouldn't that be bad planning?



LEO:  Oh, man.  I wouldn't put it past them.



STEVE:  It literally said "April 17, 2018 Planned Outage."  Which basically means it wasn't planned, but the message was planned to appear and declare that it was planned.



LEO:  Unbelievable.



STEVE:  So, yeah.



LEO:  Unbelievable.



STEVE:  Clearly not planned.



LEO:  We didn't plan this.



STEVE:  Okay.  So we have AMD having last week published the updates for microcode for all of their chips since 2011.  So that's dating back to - you know how everybody's got these code names for their chips.  These are the first Bulldozer core products introduced in 2011.  So from Bulldozer on there are now somewhere - which isn't to say that it's actually in anyone's ADM chips yet, but supposedly.  So those patches are available.  They were supposed to be released last Tuesday for the Windows 10 v1709, the Fall Creators Update, which is where we still are, the most recent official version of Windows 10.  And do you have news, Leo?  Because I know you and Paul and Mary Jo were talking last Wednesday about when the Spring Update, whatever it was going to be called, Spring Creators Update?



LEO:  Well, so one thing, first of all, it doesn't have a name.  It's 1803.  The other thing is that Paul and Mary Jo said, you know, no one at Microsoft said it was going to be Patch Tuesday.  Everybody just assumed it was going to be Patch Tuesday.  So Microsoft never said it's coming out then.  So they're not late, in other words.



STEVE:  Right.



LEO:  Although they did say there is...



STEVE:  There was no promise made.



LEO:  There was a delay.  Its internal code name is Redstone 4.  And according to Paul, the last time I saw, they've put out a new RTM.  They don't like to call it RTM, but they put out a new RTM for it.



STEVE:  A hopeful RTM. 



LEO:  Yeah, hopeful RTM.  And so it'll be soon.  It'll be soon.



STEVE:  Okay.  Well, so aside from that, it was supposed to be the case that last Tuesday's Patch Tuesday existing version of Windows 10, which is 1709, that that added the mitigations and the Spectre variant 2 for the AMD chips.  Which is to say they had provided those to Microsoft.  And just as Microsoft has incorporated the Intel microcode patches into an earlier release of Windows, that was supposed to have been done for AMD.  Now, I have a laptop with - and I forgot what it was now.  It's an AMD 64, something 5,000, M45000, I don't remember now what it was.  But I think it was only, like, 2013.  So I would have expected it to be swept up in that.



I did the update.  I checked, and it's not showing this as fixed.  However, there are developer guidelines now that have also been published which enumerate the changes in the chips, the CPU ID bits, which can be queried to determine whether it's got the updated functions.  So the point of all this is that probably tomorrow I will, as I have been recently, return to InSpectre and add support for these new functions which are now documented in the developer docs, which will allow my InSpectre app to tell users whether their chip has been updated.  And presumably Microsoft won't update the firmware unless they're also updating the Windows OS to support it.



So anyway, I guess it was last week that we talked about the final, for Intel, on the Intel side, the final list of CPU IDs which had Intel's updates and, controversially, those that did not.  And I did then rev InSpectre to Release 8, which it is now, with knowledge of that.  So there's another line in the little summary screen at the top of InSpectre telling you whether microcode updates are available, which is different from whether they are actually in effect for your chip.  So that allows you to see whether there's hope if you're on the Intel side.  And I haven't yet found the same thing from AMD.  I'll see whether they've got, by CPU ID, a list of which ones do and don't have those available.



What I do have now is the ability to determine whether it's actually in place or not in the hardware.  So I plan to be updating InSpectre, it would probably be Release 9, to incorporate that additional information.  So for what it's worth, AMD is finished that.  They did it all at once. It's supposed to be from Bulldozer on, which was 2011.  So Microsoft will probably be incorporating those.  Oh, I'm sorry, Microsoft has said they did, although I haven't seen it.  So hopefully I'll get some tweets from people saying, yeah, ever since last Tuesday, InSpectre has been saying that my system is all patched against the Spectre variant, although I haven't seen that.  I haven't looked actually since.  So I will.



But remember, separate from this, we still have the Ryzenfall, Masterkey, Fallout, and Chimera vulnerabilities which came to light last month.  Those are considered less dangerous and not as big a problem, but those are yet to be dealt with by AMD, who has at least acknowledged them and said that they would be getting to it.  So, progress.  And hopefully on the Windows side those using systems that are no older than about seven years and who are using a recent enough version of Windows that Microsoft will patch them, we'll be able to get those microcode updates.  So, yay.



We've been tracking, because it's sort of interesting, for the last few podcasts the fate of Telegram in Russia.  As we know, Russia said we're not going to allow any encryption that we cannot decrypt.  Telegram said, sorry, we're not going you the keys to the kingdom.  Russia went through several rounds of various court actions in order to make this compulsory.  Telegram stood their ground, said sorry, nope.  And so Russia then late last week got the final machinations finished to require that Telegram get blocked.  That is, Telegram's domains.  Apparently in an effort to - and I guess it's sort of they were playing cutesy because, I mean, this is a losing battle on Telegram's part.  They moved, Telegram moved a bunch of their IPv4 IP addresses deliberately into net blocks being used by Amazon's AWS and Google's cloud platforms.  As a consequence, Russia, using a rather blunt instrument, blocked 15.8 million IPs...



LEO:  Oh, man.



STEVE:  ...on those Amazon and Google cloud platforms.



LEO:  Oh, lord.



STEVE:  Yes.  And I have them in the show notes.  I mean, it is a massive block.  So in network terminology, if you have a network with a /16, that says that the high two bytes is the network, and then the low two bytes are typically .0.0, meaning that all addresses with any of the lower two bytes are part of this block.  Well, this isn't quite that bad.  It's a /15.  But there's three /15s and a /14, meaning that basically four different networks, each one being specified by two bits plus the highest order bit of the third byte, have all been blacklisted.  That is, just blacked out.



So that ends up being 15.8 million IPs.  Which, unfortunately, because many, many, many other Amazon and Google services were occupying this huge region of IPv4 space, all kinds of Russian retailers and non-Telegram services, credit card processing, it just created a huge, I mean, and is still a huge upheaval.  There's been a tremendous - this happened yesterday.  And so it's been a tremendous problem in Russia because they were just going to say no, we're not fooling around here.  We're blocking all of this.



So in addition, Russia's telecommunications regulator asked Apple and Google to pull Telegram from their app stores, you know, iOS and Android.  They said pull the apps.  They also requested the side-loading site APK Mirror to cease serving Telegram, which would probably be the first alternative for Android users should Google comply, as I would imagine they would, and pull Telegram from the Play Store, if they can do so for Russia's view of the Play Store.  And also the telecommunications regulator for Russia also urged VPN providers to prevent Telegram messages from getting through.



So right now there's a mess over there.  I don't know what Telegram's position will be.  I mean, they ought to just give up.  I mean, they should have seen the writing on the wall.  There was some comment that I encountered where Telegram said, well, they're considering doing a peer-to-peer messaging system, but no time has been set for that happening.  And The New York Times in their reporting noted that the ban on Telegram would put the Kremlin in a slightly awkward position because many inside the government, including those in President Vladimir Putin's press office, use Telegram themselves.  So Russia's Foreign Ministries announced its intention to move over to the Viber messaging app.  Viber claims that it is completely encrypted, as well, but presumably not so much.  Or I don't know, maybe we'll go through the same dance again.  But it's been interesting to watch this.



As we know, this happened with Telegram in China, where Telegram is now blocked in China.  Now it's happened in Russia.  And the problem is that someone trying to run a service in direct opposition to the government that controls telecommunications within that space, they're going to lose that battle.  So I guess I'm somewhat at a loss to understand what Telegram thought they were doing except just it's not expensive for them to fight Russia, so I guess they've decided, well, we'd rather fight than just go quietly.  So, fine.  Still, Telegram is gone from Russia, essentially.  Unsurprisingly.



And we've also been following from the start what has been called "Drupalgeddon2."  Remember that Drupalgeddon1 was a very bad SQL server exposure that Drupal suffered years ago.  Now we've got essentially what is regarded as a 25 out of 25 in the NIST's ranking, so the highest level of critical rating.  Remember that they gave a one-week notification, the Drupal team did, one-week notification that on a Wednesday they were going to be making public the details of patches for both the 7 and the 8 series, as well as the 6 series of Drupal, even though it had long since stopped being maintained because there were still some version 6's on line, and this was that bad.



So the problem with patching is, especially with something like Drupal, where it's possible to analyze it and figure out what was done, is that the next day, the day after, on Thursday, Check Point Research came out with just, I mean - and it wasn't like any great explosion of surprise.  They said, okay, here's what happened.  Here's what was done.  Since it was patched, it was in the public.  It was available to everyone.  And so that produced full details.  Immediately afterwards GitHub began hosting proof-of-concept demonstrations.  And now many security outlets are reporting that Drupal sites are 1000% predictably under attack.



So we have a remote code execution, an RCE for unpatched Drupal 6, 7, and 8.  So essentially, if you've got a server running Drupal, and you haven't patched it, it's going to get found, and it's going to get taken over.  Probably you'll notice that it starts running really slowly because what people want to do these days is run crypto mining, cryptocurrency mining on a strong Internet-connected machine.  So if you start noticing things start going slow, you need to deal with it.



What we know now is that Drupal had insufficient input sanitation on Form API, so-called FAPI, AJAX requests.  And as a result, an attacker was able to inject a malicious payload into the internal forms structure, which would have caused Drupal to execute it without any sort of user authentication.  And so by exploiting this vulnerability an attacker would have been able, and can, is able, if you haven't patched 6, 7, or 8 in the last couple weeks, to carry out a full-site takeover of anyone using Drupal.



So on last Friday, the day after this Thursday announcement that this is happening, in what they considered a public service announcement, and this of course was Friday the 13th, which really was for these guys, Drupal published:  "This Public Service Announcement is a follow-up to" - and then they've got the number of a previous release - "Drupal core RCE (Remote Code Execution).  This is not an announcement of a new vulnerability.  If you have not updated your site as described in" - and then again the original announcement - "you should assume" - get this.  This is Drupal saying:  "...you should assume your site has been targeted and follow directions for remediation as described below.



"The security team is now aware," Drupal writes, "of automated attacks attempting to compromise Drupal 7 and 8 websites using the vulnerability reported in" - and again, this is SA-CORE-2018-002.  "Due to this, the security team is increasing the security risk score of that issue to 24/25."  I don't remember what it was, but obviously a little lower than that.  Still, like right at the ceiling.  "Sites not patched by Wednesday, 2018-04-11" - so that was the Wednesday a week before - "may be compromised.  This is the date when evidence emerged of automated attack attempts."  That is, the same day the patch was announced.



"It is possible targeted attacks occurred before.  Simply" - and then here.  "Simply updating Drupal will not remove backdoors or fix compromised sites.  If you find," they write, "that your site is already patched" - I had to do a double-take on this one.  "If you find that your site is already patched, but you didn't do it, that can be a symptom that the site has been compromised."  Meaning that the attackers are closing the back door behind them after they already get in to keep anybody else from coming in and pushing them out.



"Some attacks in the past have applied the patch as a way to guarantee," Drupal writes, "that only that attacker is in control of the site."  So it's hard for me to imagine that somebody listening to this wouldn't have also been listening to both of our previous two podcasts where the urgency of this was made very clear, even when it was still a week away from being known.  But somebody would have been very ready.



So what they are reporting is that it looks like these automated attacks are closing the backdoor behind them, patching this so that they alone are there.  Therefore, if your site appears to be patched, and you didn't do it, that's additional reason to worry.  And I don't know for a fact that they're installing cryptocurrency mining.  But that's what people are doing these days.  So if you suddenly see that your CPU utilization has jumped up, that's a good reason to suspect also that this has happened.



So this unfortunately is now the model for vulnerabilities being discovered on the Internet.  It was discovered.  They got the patches ready.  They preannounced to everyone, get ready for a patch for which we expect to have exploits immediately follow.  That happened.  The exploits happened.  Now they're underway, and sites are being taken over.  So I hope, if by any chance you are a Security Now! listener and you skipped a couple weeks of announcements, and you somehow didn't already know that your Drupal site needs to get patched, don't wait any longer.  



LEO:  And assume you've been hacked.



STEVE:  And, yes, you really do.  At this point it's not difficult to find you, to check your version and to insert an exploit, if the backdoor hasn't been closed by somebody already beating somebody else in.  Yikes.



So I performed the following test just now, this morning, in preparing these notes, and it all works.  Firefox has begun bringing up support for TLS v1.3, but not enabling it by default.  This is a very cool thing for any Firefox user.  SSLLabs.com is Ivan Ristic's very cool SSL, now TLS testing facility.  We normally talk of it in terms of testing sites, that is, testing like GRC.com, Google.com, Apple.com, whatever, Amazon.com.



But the second item on Ivan's testing menu is check your browser.  You can also use SSLLabs.com as an individual to check the strength of the crypto that your browser is using.  When you do it with Chrome, you will find it's already supporting TLS v1.3.  When you do it with Firefox, you will find TLS v1.2.  So again, that's SSLLabs.com, and then you choose Test Your Browser over on the upper right, the second item down. 



So with Firefox - and I updated my Firefox, I'm at 59.0.2, which is the latest, and that was on a Windows 10 machine - you'll see that your browser supports v1.2.  Go to about:config, put "about:config" in the URL bar of Firefox and hit Enter.  That will give you this huge list of things you can tweak.  In the Search area, you can just put in "version.max."  That'll weed it right down.  I think I had two items in there when I had version.max.  And you will see that it is likely set to 3.  Set it to 4.  That won't hurt anything.



Set it to 4.  You don't need to restart Firefox.  And if you had opened another tab, like if you just opened another tab to do about:config, go back to the SSL Labs tab, refresh that.  And you will now be greeted with the fact that your browser is running TLS v1.3.  So just changing, under about:config, and then search for version.max.  The whole thing is security.tls.version.max.  Change the 3 to a 4, and your Firefox will now be persistently and from then on operating with the very latest version of the Transport Layer Security, TLS v1.3.



So Akamai is calling this UPnProxy, and of course we recognize the first four letters, UPnP.  I'm speechless.  Years ago we talked about this, and we were stunned that UPnP, that Universal Plug and Play, aka Universal Plug and Pray, protocol which was only ever intended to be a function or a feature on the inside, on the LAN side of consumer routers.  And even then it was a worrisome idea.  This was Microsoft's solution, essentially for Xbox to be able to make itself available behind a NAT router, the idea being that it was a zero configuration solution.  Any time you hear the words "zero configuration," that's when you should start worrying because, if nobody has to configure anything, then the bad guys don't either.



And so the idea was that, with no configuration, for example, an Xbox on your home network would be able to find the router, talk to the router, doesn't need any router admin login passwords or anything.  It's just able to say, hi there.  I'd like an outside line, please.  And the router says, oh, that's why I'm here, and opens up a port or multiple ports, however many the Xbox wants.  I mean, essentially, Universal Plug and Play is an invisible, unprotected, insecure, zero authentication protocol that completely defeats the router's NAT firewall, the beauty of NAT being that only outbound connections are allowed, and incoming traffic is only allowed back in if it matches something that first went out.  It's just it's a perfect firewalling solution, which Universal Plug and Play completely subverts.



Okay.  So it was bad enough, right, when the Universal Plug and Play service was exposed on the LAN.  Bad enough.  As we covered years ago on Security Now! Episode 389, it came to our attention that many routers had not bothered to restrict it to the LAN, and also had it available on the WAN.  Which, I mean, words - there are no words.  It's unbelievably insane.  So it was so bad that I dropped everything I was doing at the time and added a new test to GRC's ShieldsUP!, which, Leo, you'll remember at the time.  I called it "GRC's Instant UPnP Exposure Test."  And I want to remind everyone about it so that everyone can make sure that anything that they've done since then hasn't caused this to be reopened for them because there are many routers that currently are still doing this today.



So just go to GRC.com under Services, I think it's the second item in our top-level menu.  Under Services you'll find ShieldsUP!, the first thing on that submenu.  Select that, say okay, and there's a big orange box that I haven't touched in all these years.  Click that, and in a few seconds you will be able to confirm, with everything coming up hopefully green, that your router, wherever you are, is not making Universal Plug and Play service available to the Internet.



LEO:  I can't believe that after all these years we still have to tell people that.



STEVE:  Leo, when you do the test - and you're doing it right now.  When you do it, it will show you the count.  This morning it was 53,922 open UPnP ports found.



LEO:  Wow.  Wow.  Where is that number?  Oh, here it is.



STEVE:  So click that big orange thing.



LEO:  The big orange button, yeah, yeah, yeah.



STEVE:  Yeah.  Click that.  And now it's running the test for you.  And now it ran the test for you.



LEO:  Good news.



STEVE:  And you're green, doot de doo.



LEO:  We're green, look at that.  Wow.



STEVE:  And let's see.  So what is it, five...



LEO:  That's a lot of open routers:  55,922.



STEVE:  Okay, same number I had this morning.



LEO:  That's good news.



STEVE:  That's good news.  Let's hope that it doesn't increment in the next 10 minutes while the listeners to this live podcast are checking theirs to see what's going on.  And I'm noticing an increase in traffic at GRC, so I know people are listening to it and running the test right now.  Okay.  So here's what's happening.  Because there are so many, an unbelievable number of routers exposed, Akamai did a test using their network resources.  They have detected more than 4.8 million routers that expose various UPnP services through their WAN interface.  That number should be zero.  Zero.  I mean, it's an absolute mistake.  There's no - it's just amazing to me that this is still happening.



And Leo, their whitepaper that I've linked to in the show notes has toward the end a list of routers which are today, as a consumer benefit, they posted the list of routers they have identified which are still doing this.  There are a bunch of ASUS routers, unfortunately.  And there are some of my favorite Ubiquiti routers, not the one that we've chosen, but some of the wireless routers still have this available.  So if you page down through that PDF, Leo, that you've got onscreen right now, you should find, I think, toward the end is their list.  They fully document this for anybody who's interested, and then they show the various routers which they have identified which are exposed.



So the point is that, when routers do this, you can normally disable it.  You can normally turn off, and it should never be on - it shouldn't even be an option.  But hopefully you can turn off Universal Plug and Play on the WAN interface.  There is no valid reason for having that exposed.  So for some reason a lot of ASUS routers do offer it, frighteningly.



LEO:  Yeah, sometimes you need it for outbound traffic.  Famously, this was created for the Xbox by Microsoft.  That's who made this so that the Xbox could create an Xbox server, and you could play games with people, and they could have inbound access.  But that's not WAN access.  That's a very different thing.



STEVE:  Correct, correct.  So anyway, I just wanted to make sure.  So what's happened is, because there are millions of SOHO routers with UPnP services exposed, and because Universal Plug and Play can be used to create a NAT mapping, not only are you able to map from an inside IP and port to an outside IP and port, you can arbitrarily - basically it's known as packet rewriting.  You rewrite the destination IP of incoming packets.  Normally they would be rewritten to an IP destination inside the LAN.  So, for example, that's where the Xbox would be.  And so it would say, ah, packets coming into this port on this router get rewritten to 192.168.1 dot whatever the Xbox is.



You can, however, establish packet destination rerouting to any IP you want.  What's happening is these routers are being used to bounce incoming traffic back out.  In other words, with enough of these routers, it's possible for a bad guy to do exactly what we see in all of the fictional cyberattack scenarios where traffic is routed around the globe five or six times, bouncing off of proxy servers.  Well, this turns SOHO routers into proxies which are able to be used trivially to route public traffic from one UPnP router to another to another to another to another in order to hide the origin of the traffic.



And there's nothing to prevent multiple bad guys from all doing this simultaneously on any given user's router.  Meaning that your router with UPnP exposed becomes a little Grand Central Station of malicious traffic, bouncing off of your WAN interface in order to obscure and hide whatever these bad guys are doing.  And of course the problem is, when this gets traced back, it gets traced back to you, to your IP or the one you had at the time.  So it's better not to have to explain to the authorities that you weren't the one who was originating this traffic.  Just better to have this shut down and not have an open router.



So use GRC's quick Universal Plug and Play test to verify that no response is coming from the UPnP service port on your WAN interface.  It takes five seconds to do it and know that you're okay.  And if you're not okay, then absolutely, I mean, change routers.  Update your firmware.  Turn off Universal Plug and Play on the WAN interface if that's an option.  And, boy, you certainly have my endorsement in scolding any router manufacturer.  I don't know what ASUS is thinking.  But as I recall from the Akamai document, there was like a huge block of ASUS router models that have Universal Plug and Play exposed, so that can't be a mistake.  That's a policy decision, which I just - it makes me scratch my head.



In January of this year, when the Meltdown and Spectre attacks became known, Microsoft gave us the first mitigations to those attacks that they were able to in the January updates for Windows.  And somewhat puzzling at the time, and certainly controversial, was their decision to announce, well, the announcement of their decision and the decision that they would no longer update Windows unless a specific registry key was present, which their AV solutions and other AV solutions could create in order to assert that they were not going to crash people's machines, cause blue screens, and so forth.  And I remember you and I, Leo, were like, what?  Just kind of scratching our heads, and I heard you talking to...  



LEO:  It was kind of a funky way to do it.



STEVE:  It's like weird, yeah.  And normally, because you had Windows Defender or Security Essentials or something, you were kind of probably going to be okay.  But there were instances where Windows 7 might not have had something enabled, and suddenly Windows Updates would stop, and you wouldn't get them anymore ever, or so we believed.  Well, turns out that there was a lot of back pressure on this policy, and Microsoft learned that that had not been such a good idea.  So, quietly, the March update for Windows 10 happened even if that key was missing.  And last week this April's updates happened, even if that key was not present for Windows 7 and 8.1.  In other words, that's no longer true now.  Windows 10 and 7 and 8.1 are all now ignoring the key, whether it's present or not, and going ahead and doing updates.



Microsoft, of course, now has to kind of backpedal and explain what, what, what?  And so now they're now saying, well, we've now been able to verify that no one's AV is actually causing a problem.  Or everyone's AV is now no longer causing a problem or some nonsense.  Anyway, basically this is gone, and I just sort of wanted to correct, or I wanted to update everybody with where we stood and the fact that this was now happily a non-issue; that everybody, no matter what version of Windows you're using, whether you've got a registered AV or not, you are getting Windows security updates.  Oh, and Microsoft said in their notice that this change, in other words, undoing what we did in January, this change has been made to protect user data.  Okay, right, yeah.



Okay.  So this is a little bit controversial in my mind.  Google has announced that, starting with Chrome 70 - which is slated for still a ways away, toward the end of October 2018 - that their Chrome browser is going to begin deliberately ignoring the requested expiration time on cookies delivered to the browser over HTTP.  Now, as opposed to HTTPS.  In other words, this is another pushback by Chrome using their market dominance and their position to make things more troublesome for non-HTTPS, non-TLS-connected websites.  So starting with Chrome 70, any website trying to present a persistent cookie, like a cookie with an expiration date set way in the future, will have it capped at one year.  And their longer term plan is over time to proactively reduce HTTP cookie life down to as little as a few days.



So I guess I have mixed feelings.  This is clever because it's going to create some inconvenience for sites trying to create statically persistent session cookies so that you can stay logged on without having to reauthenticate and re-log on.  Typically that's done with a cookie.  And most sites just say, oh, you know, you're logged on until you log off, and leave it at that.  They do so by setting an expiration far in the future.



Now, the reason I have sort of mixed feelings is that Chrome is also deliberately breaking the HTTP standard.  I mean, they're just saying they're going to deliberately ignore and violate a longstanding standard.  So I guess I understand it, but it makes me feel a little queasy.  We all remember the danger of HTTP cookies because that's what Firesheep made very famous back in the day when it was easy to - like when Facebook and Snapchat and all the various services were not yet using HTTPS all the time.  You would typically use that briefly to log in, to protect your username and password.  But then, somewhat paradoxically, they would drop you back down to HTTP, but your cookie was there so that anybody sniffing the traffic could grab the cookie and immediately jump online impersonating you, essentially acquire your logged-in session for their own.



So anyway, I think it's - I wanted to explain this to everybody, to tell our audience that this is something that Chrome has decided to do, probably for the best.  But still annoying that they're going to be ignoring this standard.  I don't know what kind of timeframe they expect for shortening this to a few days.  It certainly will upset people who for whatever reason want to stay with HTTP, want to be able to have persistent sessions with HTTP-issued cookies.  They just won't be able to do that in the future for more than a couple days, eventually, apparently.  So it'll be interesting to see how this evolves.



A German security firm, SRL (Security Research Labs), did some careful looking at a large number of Android mobile devices, primarily smartphones, and discovered that OEMs of Android phones were deliberately misrepresenting the completeness of patches for their devices.  In Android you're able to look at the so-called "security patch level" as a certain date.  It's presented as a date.  And the clear statement that is making is that your phone patched up to this date incorporates all of the known problems earlier than that.  Well, it turns out it's not even close to being true.  So in an interview with Wired magazine, the researchers at SRL, at Security Research Labs, said:  "Sometimes these guys just change the date without installing any patches."  They said probably for - I know, I know.



LEO:  [Indiscernible anguish].



STEVE:  Probably for marketing reasons, they just set the patch level to an almost arbitrary date, whatever looks best.



LEO:  Oh, that's terrible.  Yesterday.  Yeah, we fixed it yesterday.



STEVE:  Yeah, oh, we're all - don't worry.



LEO:  Don't worry about that.



STEVE:  You've got, oh, yesterday.  We're updated.



LEO:  That's got to be some crappy phones, though.  I can't imagine.



STEVE:  Okay.  So here it is.  They enumerate them.  And if you follow the link, Leo, in the notes you can get the detail because they have a nice little red-colored chart.  But 0-1 missing patches was Google, Sony, Samsung, and Wiko Mobile.



LEO:  Well, I wish they'd say whether it's 0 or 1.  That's kind of annoying.



STEVE:  I know.



LEO:  There's a big difference between 0 and 1.



STEVE:  Yeah.  On the other hand...



LEO:  I've got to say Google is zero; right?



STEVE:  Yes.  On the other hand, HTC, Huawei, LG, and Motorola are 3-4.



LEO:  Yeah, that's not good.



STEVE:  No.  That is to say, they're claiming they are current, yet they are not.  Three to four missing security patches later than the date they said they are current to.  And then the worst at 4-plus were people I'd never heard of:  TCL and ZTE.  



LEO:  Chinese companies both.  In fact, all of them, I don't know about ALPS, but TCL, OPPO and ZTE are all three Chinese companies.



STEVE:  Yeah, yeah.  And I don't know how to...



LEO:  Huh?



STEVE:  What were you going to say?



LEO:  I just said wow.



STEVE:  Oh, yeah.  And also OnePlus, Nokia, and is it pronounced Xiaomi?



LEO:  Xiaomi.  Another Chinese company.  Another Chinese company.



STEVE:  Xiaomi.  I've heard of them, sure.



LEO:  Big, big Chinese - maybe the number one phone maker in China, yeah.



STEVE:  So Xiaomi, OnePlus, and Nokia, those are 1-3 missing patches.  Meaning none of them have zero.  They were all missing at least one.



LEO:  Yeah.  That's ridiculous.  That's ridiculous.



STEVE:  Yes.  And they said:  "Specifically, the above" - that is, that table I was reading from - "focused on security patches for Critical and High severity vulnerabilities that were released in 2017," still missing from their phones.



LEO:  You know it's sad, Motorola's on here.  Used to be a Google company, but now it's owned by Lenovo.  And I have to think that maybe because it's owned by a Chinese company it, too, is not getting updated.



STEVE:  Yeah, yeah.  Now, there is some good news.  We've discussed previously that Google is aware of this problem, that is, because they're the main driver of this.  They have a project underway called Treble, which is rearchitecting Android to disentangle the OEM-level customizations from the OS code.  Because right now it's necessary, essentially, all of this stuff has to pass through the OEMs and be sort of backported into the stuff, into the OEMs' own code blob, which then needs to be pushed out for their phones.  And it's unclear if it's just lack of concern.



Essentially, what we believe is that OEMs are unhappy because all they want to do is sell hardware.  They want to sell DVD players, just consumer electronics, and just like sell them and forget them.  And so they're complaining about the burden of this constant updating patching of the devices they've sold, which consumes too much of their resources because, as it is structured today, it's involved to get these things patched.



So Google's Treble project is, in recognition of this serious significant overhead, is restructuring Android to separate the OEM junk from more of the OS stuff, which will reduce the burden on the OEM for these kinds of ongoing fixes.  So Google's responding, and we can hope that, as we move forward, we're going to get this whole process sped up some.



Two bits of miscellany, then I'm going to continue.  The first is, Leo, "Lost in Space."



LEO:  Okay.



STEVE:  Did you see it?



LEO:  I only watched the first episode. 



STEVE:  Okay.



LEO:  And I have mixed feelings about it.  I downloaded the whole thing for our trip, so it's on my iPad.  They spent a long time getting that girl out of that ice in the first episode.  That just kind of went on and on and on.  But I liked it.



STEVE:  Yeah.



LEO:  I didn't hate it.  What did you say?  I feel like it's a mixed bag.  It's not great.



STEVE:  Yeah.  Lorrie and I - yes, I agree.  Lorrie and I watched all 10.



LEO:  Wow.



STEVE:  So we did Friday, Saturday, and Sunday.



LEO:  Holy cow.



STEVE:  So I've seen the whole first 10 episodes.  I hope there's more.  I hope that...



LEO:  You liked it enough that you want to see more, okay.



STEVE:  Yes.  I would rather watch it than "House of Cards," and that got, what?



LEO:  Well, that's pretty good, yeah.



STEVE:  Three seasons?



LEO:  Yeah, yeah.



STEVE:  No, it was great.  I will say, and this is not a spoiler, I'm never going to spoil anything.  But, boy, Dr. Smith is annoying.



LEO:  Yeah, yeah.  Well, he was kind of annoying in the original.



STEVE:  Yes.  He was like a fly in the ointment.



LEO:  But at least he had something going for him in that one.  This one they've really made Smith kind of not so good.



STEVE:  Yeah.  Parker Posey we've talked about.  And, yikes.  So be prepared to be annoyed.  On the other hand, I mean, I was thinking about it.  It's engaging that she, Dr. Smith, is so annoying.



LEO:  Right.



STEVE:  I mean, you just keep wanting something to fall on her, but nothing does.  But really cool robot.  Neat robot.  And an interesting back story there.  We learn about, you know, anyway, I'll just say I like this.



LEO:  I think people who dislike it the most are people who were real fans of the original because it's nothing, nothing like the original.



STEVE:  No.  And I think that's probably good.



LEO:  Yeah.  It was a cartoon.



STEVE:  Waving your arms around, "Danger Will Robinson, danger."  And being attacked by salad monsters, it just doesn't really - that's not going to do it.  But anyway, I just wanted to follow up on that.  We liked it.



LEO:  Yeah, it was entertaining.



STEVE:  I hope we get another season of it, yeah.



LEO:  Yeah.  Mixed bag.



STEVE:  Yes.  And it won't surprise anybody that there will probably be another season.  So without giving away any spoilers, I'm sure Netflix is hoping for more, and I am.



I did get, as I mentioned at the top of the show, a nice note that I wanted to share.  And I didn't know what "Overcoming SSE" was.  This is Mike in Dundas, Ontario, on the 15th of April, so that's two days ago, on Sunday.  He said:  "Hi, Steve and Leo," addressing it to both of us.  He said:  "I've been waiting a long time to drop you guys a note.  I decided the trigger for my correspondence was to be a miraculous save using SpinRite. That's why I titled this document 'Overcoming SpinRite Story Envy.'"



LEO:  Ah, SSE.



STEVE:  So that's SSE, SpinRite Story Envy.  He says:  "I, like many, many others whom I have heard on this podcast, I purchased," he wrote, "a copy of SpinRite as a thank-you for the invaluable service you both provide."  He says:  "I've been listening to Leo for quite some time, from very early when he spent a lot of time in Toronto."  And of course you and I were there.



LEO:  And you did, too, that's right, yeah.



STEVE:  Yes.  It was in Toronto that you proposed this podcast to me, in fact.



LEO:  I got on my hands and knees and proposed to Steve.



STEVE:  So he says:  "Thought I would give this new Security Now! podcast a listen."  He says:  "You had me from the very beginning.  I guess I am a 'SN0dder,'" he says, S-N-0-D-D-E-R.



LEO:  He likes his acronyms.



STEVE:  Yeah, a "Security Now zero-day-er."



LEO:  Oh, I like it.  Oh, a SN0dder, yes.



STEVE:  S-N-0-D, a SN0dder.



LEO:  I'm using that.  Hello to all you SN0dders.



STEVE:  That's right.  He says:  "However, I've finally given up waiting for a drive to have trouble because I don't think it will ever happen to me personally."  He says:  "I practice Steve's Hard Drive Hygiene..."



LEO:  Exactly.



STEVE:  Okay, he arranged not to do SHDH, which was Steve's Hard Drive Hygiene protocol, "...to the point where I too have a dedicated SpinRite machine, and my daughter uses 'SpinRite' as a verb."  So he says:  "Just wanted to say thanks, and that you guys are the best."



LEO:  Wow.



STEVE:  So, Mike, thanks for sharing.



Okay.  We've known for about five years that the original SecureRandom function, which was provided by JavaScript, was not random, or not as random as we hoped.  We have known for five years, since about 2013, maybe even 2012, that it actually only provided about 48 bits of entropy.  So there was a scramble around, we covered this at the time on the podcast, a scramble around to improve the JavaScript library.  There was a...



LEO:  It is Java or JavaScript?  I'm sorry.



STEVE:  It was JavaScript.



LEO:  Script, okay.



STEVE:  Yes.  And so the concern is that there were at the time browser-based bitcoin wallets, and also some wallet apps that were using some early JavaScript libraries, even if they were standalone non-browser based.  So essentially what has been found by researchers is that those early web browsers' JavaScript, which offered the so-called SecureRandom function, were only providing about 48 bits of true entropy.  We want to have, like, 256; right?  Forty-eight, I mean, that's still a lot because 32, after all, is 4.3 billion.



But several things have happened in the meantime that have sort of created a trifecta.  We've got first an inherently low-entropy private key because the SecureRandom generates both the bitcoin address and the private key used to protect your access to it, so an inherently low-entropy private key.  Second, in the last five years a huge increase in brute-force processing power, which is to say brute-force cracking power driven by all of the GPU and ASIC design for hashing fast.  And then the significant increase in bitcoin valuation, which has moved bitcoin from an, eh, I really don't care if I even know where my wallet is, to oh, shoot, where did I put that?  Or what was my password?  I know you can relate to that, Leo.



LEO:  Absolutely.  I've had the "oh, shoot" moment, yes.



STEVE:  So what we have is means, motive, and opportunity for there to be a renewed interest in attacking bitcoin resources, which are not as secure as they really should have been then.  But since then it's gotten easier to attack them, and there's much more motivation to attack them.  And as we've learned, the protection that they have is much weaker than was believed.



So for what it's worth, the researchers who are looking at this have said, if you have had your bitcoin for a long time, if you may have generated it back in the early days when it sort of just didn't matter, and you were screwing around, and you thought, oh, what the heck, and you used a web browser, like a browser-based wallet, even if you subsequently moved to something much more secure, the point is the original entropy being low means that there are now, and researchers know this, there are people working on cracking older bitcoin addresses and their private keys.



And then, finally, if you have a substantial sum in a bitcoin wallet which you have had for a long time, and whose entropy, as I've just been explaining, you have reason to distrust, then it is probably worth creating a new bitcoin address and transferring your balance from the old one to the new one and move into one with guaranteed known high entropy.  So I just sort of wanted to pass that on as a public announcement.



I know exactly where mine was synthesized when I first talked about it.  It was a Windows-based wallet on a version of Windows with a known cryptographically secure source of entropy.  So I have every reason to believe I'm okay.  But if those three, if those multiple things fit you - you had it for a long time, it may have come from a JavaScript source, and you've got more money in there than you would like to lose - worth probably moving it to a new bitcoin address, freshly created with a full set of entropy bits rather than a lower number.



LEO:  Or if you forgot your password, here's a chance to try to brute-force it; right?



STEVE:  And there you go, exactly.



LEO:  It's a silver lining.



STEVE:  That's a very - I hadn't flipped it over like that, Leo.  That's a very good point, indeed.  And along the lines of things that people didn't bother attacking once, that they're now coming back to attack with new motivation?  It turns out that, as we know, malicious cryptocurrency miners are looking around everywhere for any still-exploitable places where they can install and run their mining software.  And the devices must be connected to the Internet.  And optimally they will be as fast as possible.  And as we have been discussing, this of course places public Internet servers among the ideal set of targets.



So now another old and long since known bug.  It hasn't been patched because it's too old.  But that doesn't keep it from being a problem.  It's a bug in Microsoft's IIS v6.  IIS v6 was the one that shipped with Windows XP and Server 2003.  So, old.  But the WebDAV service was known and has been known to have a problem.  A pair of Chinese researchers a little over a year ago discovered that an attacker can craft and send a malicious PROPFIND request to an IIS v6 server that has WebDAV enabled.  And that PROPFIND request contains an oversized IF header, which is one of the fields in that query.  When the IIS WebDAV controller reads the request, guess what?  Because it's oversized, it won't surprise you that a buffer overflow occurs, allowing attackers to deliver and execute code on the targeted server.



Now, due to the server's age - this was 2003, right, and Windows XP?  Even though today, get this, even though today IIS v6 accounts for 11.3% of all IIS server installations, so one in nine of IIS servers are still IIS v6.  The code is well past end of life and will no longer, obviously is not going to be updated.  So Shodan, a search under Shodan shows that there are a little over 600,000 - 600,000 - publicly accessible IIS v6 servers on the Internet.  The good news is WebDAV was not enabled by default.  So most of them, most of those 600,000 publicly accessible IIS v6 servers do not have WebDAV enabled.



But any servers that were using Share Point portal do have it enabled, so there are still plenty of them.  And as a consequence of all of this, again, and bad guys now having a new motivation to find servers, there is a campaign running at scale, meaning Internet scale, trying to find those servers and install Electroneum cryptocurrency mining on those machines.  There are security firms tracking a campaign that is apparently being very effective.  And so Electroneum is the cryptocurrency being mined by this campaign, installing itself into IIS v6 servers that are unfortunate enough to have WebDAV enabled.  And there's potentially, who knows how many, I think I saw something like 65,000 somewhere.  So a substantial percentage of that 600,000 potential publicly accessible IIS servers.  Yikes.



And I was initially a little confused by a story about how a photo on WhatsApp of somebody's hand which did not show their fingertips was used as a new source of crime forensics.  It was covered by the BBC and picked up by Bleeping Computer.  The South Wales Police Chief Dave Thomas said that, although the scale and the quality of the photograph proved a challenge, the small bits were enough to prove that the person whose hand was shown was the dealer of the drugs that were depicted in the picture.



So over WhatsApp somebody was offering some illegal drugs with the palm of their hand showing the various samples of these various pills.  And although the picture wasn't enough to identify the person because even in the photo you could not see the fingerprints, what somebody cleverly realized was that they could see a lot more.  If you scroll down, I think there is a picture in their story.  Or maybe, oh, I think it was the BBC's coverage.  So if you can find the link to the BBC story, they show the picture, where you could only see a little bit of the person's hand.  But because they had suspects for the crime and knew who it probably was, what they were able to do was use the photo as a further source of confirmation.



And what I was reminded of was our discussion about Tor, where as we have noted several times in the past, when traffic goes into the Tor network, you never know where it's going to come out.  And Tor is so popular now, and there are so many nodes that can be weakly linked, that if you didn't know where someone's traffic was connecting to, you'd be hard pressed to figure it out.  But if you suspect what the two endpoints are, it is very easy to confirm the communication once you know what it is, even though you've got Tor in the middle stirring things up and confusing things.  So in other words, confirming a suspicion is not something that Tor protects.



And similarly, using a photo which is insufficient to identify someone can still be used to confirm with sufficient exactitude in order to close the case.  And in this case, there were 11 convictions that resulted essentially from the fact that the photo that itself didn't identify someone was enough to confirm law enforcement's suspicion.  So I thought that was sort of an interesting twist.



And, finally, we will wrap up this week's podcast with one from our You Cannot Make This Up department.  So get this, Leo.  An unnamed casino located somewhere in North America - the casino is unnamed because this is a report from the casino's security forensics company that of course wants to leave their employer, their contractor, unknown.  This unnamed casino located in North America had 10GB of its internal network data, including all of the casino's high-roller database, exfiltrated to a location in Finland.  That occurred through a newly installed, Internet-connected, aquarium thermometer.



LEO:  At least the fish were comfortable.



STEVE:  Oh, my lord.  Nicole Eagan, the CEO of cybersecurity company Darktrace, told attendees at an event in London last Thursday how cybercriminals hacked this casino through its Internet-connected thermometer installed into an aquarium in the lobby of the casino.  According to Eagan, the hackers exploited a vulnerability in the thermostat to gain a foothold into the network.  Once there, they accessed the high-roller database of gamblers and additional information, then pulled it back across the network, out the thermostat, and up to the cloud.  So that was a very busy thermostat for a while.



LEO:  It got really hot.



STEVE:  And of course - yes.  And this, of course, is a perfect and classic demonstration of the crucial need that we have often talked about on this podcast for network isolation and segmentation; that you really want to keep the security of things you can't vouch for or that may be suspicious, like your fish tank aquarium thermometer, which you just have to have on the Internet, after all, to make sure while you're traveling that your fish are comfortable.  The good news is we are beginning to see WiFi routers with multiple isolatable guest networks.  And I think that's a great thing in this evolution.



I was configuring a network the other day, it was actually an ASUS router, and I'm going to go back and make sure that it doesn't have its WAN port exposed after today's podcast.  And I must have looked at it when I was scrutinizing it, but I'm going to make sure.  But it offered four isolated guest networks with options like "allow guests to interact."  And so you would turn that off if you don't need the various IoT things within your home to talk to each other, yet you want them to talk to the Internet.



And so I would, moving forward, look for routers for your home that support multiple isolatable guest networks because I think this is going to be a trend.  And think in terms of, not visitors, I mean, yes, also visitors, but give a couple of your router's guest networks to your IoT devices.  Let them have their own place to play, not on your main network.  And you can imagine that this casino really wishes that they had done exactly that.  So very cool stuff.  And as I said at the top of the podcast...



LEO:  Such a great story.



STEVE:  ...never a dull moment.



LEO:  Never a dull moment.  It's the perfect name for this show.  We're going to close the book on Episode 659.  You can watch this show, we do it live in front of a live studio audience every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch at TWiT.tv/live, and join us if you do in the chatroom at irc.twit.tv.  After the fact you can get your own copy of the podcast for your collection.  Steve's got them at his website, GRC.com, the Gibson Research Corporation.  If you go there you can download it.  He's got transcriptions, a giant mug - good lord, that's huge - and a whole lot more at GRC.com.  While you're there, don't forget to pick up a copy of SpinRite, his bread and butter and the world's best hard drive recovery and maintenance utility.  If you use it right, you never have to use it, which is kind of a paradox.



STEVE:  Ooh.  R-I-T-E.  If you use it "rite," yeah.



LEO:  Never have to use it.  You can also get a lot of freebies there, and all sorts of information.  Steve, that site is just jam packed.  It's like the back of a Dr. Bronner's Peppermint Soap bottle.  You could just read it for days.  GRC.com.  We also have copies of audio and video, so you can see Steve's giant mug in space.  All you have to do is go to TWiT.tv/sn, or get your on-demand versions by subscribing in your favorite podcatcher.  That way you'll get every show.



STEVE:  And Leo, we should note to our listeners that you are off to Japan.



LEO:  I am, on Thursday.  So either Jason Howell or Father Robert Ballecer, I actually don't know, one or the other will fill in for me for the next couple of weeks.  I'll be back May 7th.  So we can resume this ongoing colloquy. 



STEVE:  Cool.



LEO:  If you have questions or comments or suggestions for Steve, he's on the Twitter, @SGgrc.  Or you can go to GRC.com/feedback.  Thank you, Steve.  See you next week.



STEVE:  Thank you, my friend.  Have a great vacation, and we'll see you in three weeks.



LEO:  Three weeks.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#660

DATE:		April 24, 2018

TITLE:		Azure Sphere

HOSTS:	Steve Gibson & Fr. Robert Ballecer

SOURCE:	https://media.GRC.com/sn/SN-660.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss Drupalgeddon2 continuing to unfold right on plan.  The Orangeworm takes aim at medical equipment and companies.  The FDA moves forward on requiring device updates.  Microsoft leads a new Cybersecurity Tech Accord.  We talk about another instance of loud noises and hard drives not mixing, considerations for naming your WiFi network, the unappreciated needs of consumer routers, Google's new unencrypted messaging app push, Amazon pulling the trigger on "in-car" package delivery, the first puzzle recommendation in a long time, and Microsoft's move to secure the IoT space.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  Drupalgeddon hits the big screen.  Your medical equipment needs deworming.  A group of major vendors have had their own Council of Elrond to destroy the One Ring, the "ring" being security exploits.  Amazon brings back the trunk money.  And might Steve have something nice to say about Microsoft?  Find out next on Security Now!.



FR. ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode #660, recorded April 24th, 2018:  Azure Sphere.



It's time for Security Now!.  This is the show with Steve Gibson.  He's our very own master of security, the guru of trusting no one.  He's like our Rodney McKay, but without the starting gate and without destroying five eighths of a solar system.  I'm Fr. Robert Ballecer, in for Leo Laporte, who's currently touring Japan, looking for plutons, I think.  Pluton  magma rock, Steve?  Is that the thing we're doing?



STEVE GIBSON:  Well, it's great to be with you today and next week, Padre.  Yes, this is Episode 660, and our topic is Azure Sphere.  Which I know that Leo and Paul and Mary Jo had a lot of fun with last week because they were like, wait a minute, Microsoft is doing Linux?  What?  Really?  And so we will get to that after we deal with the rest of the week's news.  But it is an interesting project which I have to say I'm kind of bullish about.  I think that Microsoft has done something good.  This came out of the Microsoft Research Group.  It is 100% focused on something that we on this podcast have been wishing for, hoping for, not knowing how we could get from here to there for.  And that is security for IoT devices.



FR. ROBERT:  Yes.



STEVE:  And the best news is it's all license-free.  Microsoft is essentially donating a bunch of really solid work, all the intellectual property, all the licensing, everything, just for the benefit of what this architecture would mean for IoT.  So we'll talk about all of that at the end of the podcast.



But we need to talk about the ongoing drama surrounding Drupalgeddon2, which is continuing to unfold right on schedule as we had expected.  Symantec had a report about an interesting multiyear campaign that they called "Orangeworm," which didn't really push me a lot except that it got so much press coverage, and so many people tweeted to me about it, I thought, well, okay, we need to talk about it a little bit.  The FDA is indicating they're going to move forward on requiring device updates.  Microsoft at last week's RSA conference finally closed the deal on something they had initiated at the previous year's RSA conference, which is the so-called Cybersecurity Tech Accord.  And I thought it would be fun to look at who's in and who's not, sort of an interesting who's who on both sides.



Also we've run across another instance of loud noises not being good for hard drives that is just kind of a kick.  Also along the same lines, some considerations for naming your WiFi network.  Also the unappreciated needs of consumer routers.  Actually, we've all been talking about this for a long time.  But a study showed just how unappreciated those needs are and came out with a little bit of concern there.  We also have, oddly enough, a brand new messaging app from Google which lacks only one arguably very important characteristic, which we'll talk about.  And of course Google keeps trying to do messaging for Android and just seems unable to somehow get here from - or, wait, there from here, wherever they are.  Also Amazon pulled the trigger today on something that we talked about maybe six months ago or more, which is in-car package delivery.



Also I have the first puzzle recommendation in a long time.  All of our listeners know that I love puzzles.  But I am extremely picky about the puzzles that I love, and consequently I'm constantly getting recommendations from our listeners.  I kind of look at it and think, eh, that didn't quite hook me.  I have one, finally, first puzzle in many, many, many months that just has all the right things.  And then we'll do a little bit of miscellany and talk about the Azure Sphere, as Microsoft has called it.  So I think a really interesting, fun couple of hours for our listeners.  And for you and me.



FR. ROBERT:  Now, Steve, of course there are weeks where we'll do this show, and there's some news, but not a lot of news.  This is a lot of news, and it's a lot of varied news.  I have no idea if we're actually going to make it all the way through that because a lot of that is juicy.  Way, way juicy.



STEVE:  Well, we'll just do the best we can.



FR. ROBERT:  Indeed, indeed.  All right, Steve, guide us through it.  Drupalgeddon.  I love the name.



STEVE:  Yeah, well, okay.  We've been talking about it for the last several weeks.  The gang at Drupal discovered a very, I mean, like the worst possible problem in the Drupal CMS - Content Management System for websites - imaginable, where anyone who is able to access the site remotely is able to run code on that server with no authentication.  I mean, just a remote code exploit that is trivial to do.



So they gave the whole world, the industry, one week's notice of the fact that this was going to happen.  So this was three Wednesdays ago.  Three weeks ago tomorrow they gave a one-week notice that on the following Wednesday they would be releasing an absolutely over-the-top, critical, drop everything in the middle of the day.  They told us at I think it was noon Pacific time that there's going to be a patch.  And it's so bad that they even did a fix for the no longer maintained v6 series of Drupal because 7 and 8 are where most of the world is.  But if anyone was still on 6, they needed to fix that, too.



So one week after the announcement, on schedule, they released the patch.  So that was two weeks ago tomorrow, 13 days ago.  One day after that, the first proof-of-concept exploit code appeared on GitHub, which was, again, predictable, expected.  It's like, okay.  If they're patching something, it's easy to look at the old code and the new code and go, oh, look what they changed, and then back that out and figure out what it was that they were trying to prevent.



So last week we were one week downstream; and, yes, right on schedule there were problems.  First of all, security companies shortly after the release, like within hours, began reporting massive scans of the Internet for vulnerable Drupal candidate servers.  And Drupal is, I think, number two in the CMS world.  I think that WordPress is number one, and Drupal's number two.  I knew a couple weeks ago when we talked about it.  I think that's what the numbers are.



FR. ROBERT:  Yeah.



STEVE:  And in fact TWiT is all Drupal based, also.  So I made sure that Leo and Leo's team were all up to speed.  And when he checked, it's like, oh, yeah, yeah, we know, we know.  Because of course the Drupal guys sent out email and blog posts, I mean, they really wanted to not have the community hurt by this, although what we also know is that there will be, as is always the case, a dispiriting percentage of sites that don't update, that never update.  They're just going to be victims of this.



And so this is called Drupalgeddon2 because in 2014 there was a very bad SQL problem that was similarly exploitable, and that was Drupalgeddon with a one implied because it was the first one.  So here we are now, almost two weeks downstream of the release of the patch.  We have lots of malicious activity.  Coin miners are being installed.  PHP backdoors are being installed.  Perlbots are being installed.  There are several instances of a very large botnet called Tsunami which has added Drupalgeddon2 awareness so that it's able to jump to and exploit those servers in addition to everything else it was already doing.



I got a kick out of, and I don't mean to be critical of Ars Technica at all, they do great reporting and provide a lot of great coverage.  And oftentimes the person who writes the article doesn't do the headlines, I know from my own having written a column for eight years in InfoWorld.  Sometimes I would write something very carefully, and the headline would mischaracterize what I had said because it was more catchy, which used to annoy me.  Anyway, the subhead on Ars coverage said "Bug patched in March is still being exploited to take full control of servers."  And I thought, still?



FR. ROBERT:  Wait, wait, yeah.



STEVE:  It's never not going to be exploited.  I mean, unfortunately, we still have Code Red and Nimda worms that are out there roaming around.  I coined the term many, many years ago, IBR, Internet Background Radiation.  That's what all this is.  It's just background radiation on the Internet, just agents out there that are just looking around for old things, systems that have never been rebooted, that have installed themselves and so forth.  And as if this wasn't bad enough, one week after the patch, another different problem was found.  And it's funny that this got very little coverage.  I ran across it, and then I was looking for it again, and I had to go to the Drupal.org site and check the security announcements because I found very little coverage in the press.



I'm not sure how this got by because it's also a critical error.  There's a JavaScript library in the Drupal core called CKEditor, which is a rich text editor that is very popular.  It's built into Drupal.  It's often used.  There's some question about whether the default instance of it is vulnerable.  But anyway, it has a cross-site scripting vulnerability which Drupal called "moderately critical," although it also allows you to run HTML and JavaScript code in the victim's browser to gain access to sensitive information.



FR. ROBERT:  Yeah, and I didn't get that because, I mean, anything that allows for cross-site scripting, that's not moderately critical, that's critical.  That's about as bad as it can get because I can now run arbitrary code.  So I don't understand that classification.



STEVE:  Right.  The only thing I could think was that it was just fatigue from Drupalgeddon2.



FR. ROBERT:  Compared to what just happened, this is moderate.



STEVE:  Exactly.  It's like, oh, yeah, we really, really don't want to do this again, do we?  So let's just, like, fix this please.  So I did want to let our listeners know that you may need to be updating yourself again, even if you just did, if you're a Drupal site.  And CKEditor, if that's part of what you're using, there's a plugin with an image tag, enhanced image plugin in that particular version.  It does not affect Drupal 7, only the Drupal 8 line.



So anyway, I just sort of wanted to check in on Drupalgeddon2.  As I said, everything unfortunately is proceeding according to plan.  This having come out, unfortunately, the nature of it, of Drupal, is that it was very simple for this to be reverse-engineered.  It's not like Microsoft burying some changes inside of binary DLLs that are all mixed around and recompiled into a blob where you'd have to really do reverse-engineering.  Drupal just isn't delivered that way.  So it was easy for bad guys to find this.  And unfortunately it's exploitable by anyone who accesses the server.  And as we've been talking about, due to this crazy popularity of coin miners, malicious cryptocurrency mining is now the big get for servers because they tend to have good connectivity and lots of CPU power behind them.  So, yeah, unfortunately it's on track.



FR. ROBERT:  Now, Steve, it's interesting because Drupal actually seemed to have handled this quite well.  They gave people advance warning.



STEVE:  Yes.



FR. ROBERT:  Because they understood - this didn't always used to be true.  But they understood that the minute they released the patches, that every black and white hat would be going through the code, as you mentioned, to find what had changed.  And then they would be able to backwards engineer a proof-of-concept exploit within hours, if not minutes.



STEVE:  Yeah, unfortunately this is the world we're in now, I mean, it's almost a malware ecosystem of bad guys really interested in taking advantage of any vulnerabilities.  And it's weird, too, when you step back a little bit, we now just sort of take this for granted.  I mean, it's like, okay, this is what's going to happen.  I mean, we knew it beforehand.  And even in covering it it's like, okay, yeah.  Unfortunately, this is going according to plan.



And I think part of the reason is that now with cryptocurrency creating a new pressure to get mining software onto hardware, there's a new reason to do this.  I mean, for years we've had viruses, and I used to comment often on this podcast to Leo that isn't it interesting that the virus just seems to want to propagate and not do anything really dastardly.  And it's a good thing, but it was sort of like a hobby, hobbyists or hackers who just wanted to see what would happen.  Like the original Morris worm, for example.  No malicious intent, it's just like, ooh, this would be cool.  Can I write something that can propagate itself?  Wham.  Yes.  Unfortunately...



FR. ROBERT:  I was going to say that malware grows up.  I mean, when malware was started, as you mentioned, it was mostly an oddity.  It was a curiosity to see if it would actually work.  From there you moved into, okay, well, maybe you infect a couple of systems.  You've got an advanced persistent threat inside of a network.  Maybe that might pay off financially down the line.  Then it moved to, well, we can create botnets, and we can do crazy things and horrible things with botnets, and we can shake people down that way.  I think, yeah, this cryptocurrency mining malware is a new generation because it's no longer waiting for a payoff.  It's the minute I start infecting machines, I get paid.



STEVE:  Yes.  And we could argue that there was sort of another stage which we went through the last couple years which was the encryption of people's data in order to try to then extort money from them.  But there was something wrong with that model.  Many people couldn't afford to pay.  Many people had backups.  Many people just sort of shrug and go, oh, I'll just get another computer.  Like there was just not anything there of tremendous value.  So that was a problem, but that's largely been supplanted now, exactly as you say, by this notion that, oh, I can actually create something that is going to start dribbling cash into my purse.



FR. ROBERT:  Right.  Well, I mean, all those previous generations you did something that would affect the performance of the machines that you were infecting, and therefore it caused an annoyance that had people pushing back.  The cryptocurrency  malware, not necessarily so.



STEVE:  Yes, especially if it's designed not to take up too much resource.



FR. ROBERT:  Precisely.



STEVE:  It can sit there purring away in the background.



FR. ROBERT:  Yeah.  And this just adds to that background noise radiation, the background radiation that you talked about, which I would see every time we brought up the big network for Interop.  There would always be a steady stream of attacks from really, really old malware, but they were on devices that have been long forgotten.  We've gone from having datacenters full of servers and devices that we've forgotten about to now having an entire world filled with IoT devices and virtualized machines that have been forgotten about.  As you mentioned, we know for a fact there's a huge component of Drupal machines that will never be patched.



STEVE:  Never.



FR. ROBERT:  Because they've been abandoned.



STEVE:  Yup.



FR. ROBERT:  Okay, well, luckily whenever I do this show you always bring me good news.  It uplifts me, Steve.



STEVE:  Okay, well, we will look for some of that.



FR. ROBERT:  Oh, you know what I just - someone pointed out that I actually made you skip the Pic of the Day.



STEVE:  Well, we did, but I have the picture again in the show notes later where we'll be talking about it.  So I thought, okay, well, yeah.



FR. ROBERT:  Yeah, we'll do that.  



STEVE:  So Symantec released a report where, as they do, they've got threat intelligence groups that are looking around, tracking what's going on.  In early 2015, in January, they began seeing evidence of some group which was mostly and quietly targeting medical organizations.  And the press coverage that I mentioned sort of took an interesting spin.  All of the headlines talked about high-tech imaging devices like X-rays and MRI machines being infected.  And so that's like, oh, my god.



But it turns out, again, that's not really what was going on.  In reading into this more carefully, Symantec has dubbed this thing "Orangeworm," stating that an unknown hacking group had been found installing a wormable trojan on machines hosting software used for controlling X-rays and MRI machines, as well as machines used to assist patients in completing consent forms.  And in some of the little more carefully assembled coverage, the presumption was that maybe this group was targeting medical organizations because you got much richer personal information content from collecting medical records and reselling them than you did somebody's shopping records, if you managed to compromise some retailer.



So what Symantec has seen is that the preponderance of attacks is in the U.S.  I think it's 17% in the U.S.  Then they had a 10% pie slice of where they were unable to determine where the attacks were.  Then I think 7% was, or maybe it was 10% was in Israel.  And then it just sort of ran around through the pie, with each slice getting successively smaller.  But global.  So this appears to be a focus on global attacks.  I would imagine perhaps that there's a preponderance in the U.S., just because of the systems in place and probably, unfortunately, lack of security that lets them get in.  It's not just medical organizations.  It's also suppliers to them.  So it might be that, for example, if a medical organization has an IT relationship with some of their higher end suppliers, that that's a way into the medical organization.



And we've seen this famously in several places.  I think it was the Target credit card breach that was famous a few years ago.  It was actually a contractor of Target's, an HVAC manufacturer that had the vulnerability.  And so the bad guys were able to get into Target through their HVAC supplier as a consequence.  So that sort of thing.  So there's a trojan called the Kwampirs, K-W-A-M-P-I-R-S, which is largely in the healthcare sector in the U.S. and Europe and Asia, now in its third year.



So this is one of the so-called, I don't know if it would be an APT, but it's an advanced persistent intent, so an API, I guess.  And they're calling it a "supply chain attack" because it is also sort of upstream of the victims.  But definitely seems focused on medical suppliers, pharmaceuticals, the healthcare industry.  And they don't think it's a nation-state actor.  They say that it has the fingerprints of somebody who probably wants maybe rich patient data from healthcare providers.  So that may just be something that they're in the resale business of that kind of detailed personal information for who knows what purpose.  So, interesting.



FR. ROBERT:  Right.  Now, what I found fascinating about the story is they're not going after the MRI machines themselves.  I mean, that's what gets the headlines.  That makes it sound spectacular. 



STEVE:  Right, exactly, exactly.



FR. ROBERT:  Because an MRI machine is still sort of a fanciful device in hospitals that miraculously gives you a diagnosis.  But instead, they're going for the control systems.  They're not going after the embedded systems, but they're going for the control systems.  And I think it's because, if you look at most hospital networks, since I've worked with a couple of our hospitals, you can pretty much guess what they're going to be connected to because MRI machines are very specific types of devices.  They will always be connected to one or two networks that you'd really want access to.



So I see this as an attempt to pivot.  They want to be in on a machine that they know is going to be connected to networks that they want.  And then they can just turn that and say, okay, now I'm going to attack the patient network database.  Or I'm going to go after their billing database.  That's, I mean, that just shows a level of forethought that I guess it's now common for malware, but didn't used to be.



STEVE:  Yeah.  And I think it also suggests, again, a focused intent.  They're not out there spraying, looking for arbitrary ways in.  They're saying we're going to go after this particular profile of company and, as you said, identifying some typical ways in.  And probably, once they've succeeded a few times, they start thinking, hey, going after the large control system for these large machines seems to be a successful strategy.  And so they're just sort of refining a strategy over the course of three years because we're now - it was all of 2015, all of 2016 and 2017, and now we're in 2018.  So it was first seen in January of 2015.  So some long-term focus for whatever reason.



FR. ROBERT:  And you thought that hackers were just nerds in the basement.



STEVE:  Now, you were looking for some good news.



FR. ROBERT:  Yes.



STEVE:  We do have some good news.  There have been, again, very much the headline grabbing are things like insulin pump can be taken over remotely, and your pacemaker can be reprogrammed, and this whole concern over hacking medical devices is headline grabbing.



The good news is that the FDA is getting ready, they're gearing up to take some action on medical devices, including eventually - bureaucracies don't do anything quickly.  But they understand, someone understands there, based on the most recent medical device safety action plan - I have a PDF in our show notes if anyone is interested - they recognize that cybersecurity is no longer optional and is exactly the kind of thing that FDA regulation can require and enforce as part of the oversight and verification of the safety of devices.  So the idea being that device safety, the definition is being extended to require after-market, after-sale, long-term security maintenance, not just battery maintenance and making sure the leads don't get corroded.  But there needs to be an ongoing management of device safety.  And I think that's just all for the best.



In this document, after you get way down into it, point number four - I've skipped all the other ones.  Under "Advance Medical Device Cybersecurity" it reads:  "The FDA plans to consider potential new premarket authorities to require firms on the front end, to, one, build in capability to update and patch device security into a product's design and to provide appropriate data regarding this capability to the FDA as part of the device's premarket submission; and, two, to develop" - what they're calling, and I don't know what this means - "a 'Software Bill of Materials' that must be provided to the FDA as part of a premarket submission and made available to medical device customers and users so that they can better manage their networked assets and be aware of which devices in their inventory or use may be subject to vulnerabilities.  In addition, availability of a [again] 'Software Bill of Materials' will enable streamlining of timely post-market mitigations."



So that sounds like they're saying the FDA is going to require that these things no longer be allowed to remain black boxes, where there's no visibility into them.  In the future, you're going to have to disclose what the moving parts are in there, in this so-called "Software Bill of Materials," so that if a vulnerability is discovered, it's possible to check that against your bill of materials and say, oh, you're using this version of SSL or this build or distro of Linux in your embedded thing, and the world just found a problem with that.



FR. ROBERT:  Right.



STEVE:  So essentially creating accountability where there's been absolutely none, and where it's been feasible for this sort of black box concept to reign.  And of course we know one other place where it would be fabulous to see the same sort of management, and that is voting machines because the model is very much the same as it is for medical devices.  It's a black box where Diebold is just allowed to say, oh, yeah, we have the best security.



FR. ROBERT:  And that bill of materials is actually a big part of this because it means, if you find something wrong with a component, it's not just that you know, okay, it's that component.  It means you can now search your database and say, what other products are using that component?  That's huge.  That has not existed before.  They used to have to go device by device by device.  Now it's, oh, we found this software component and this hardware component combine to make a vulnerability.  Any devices use those same two components?  And you can do a quick search.



The other part about this, Steve, is the FDA actually got a lot of pushback.  I don't think some people understand how big this actually is of a change.  Manufacturers came back to the FDA, when the FDA was first starting to look at this.



STEVE:  Oh, you can imagine, yes.



FR. ROBERT:  They were pushing hard, and they were basically saying, look, the FDA, you only care about the medical efficacy of a treatment, a device, a medication.  You shouldn't be telling us how to build the devices and how to provide security.



STEVE:  So the argument was that the FDA was overstepping their boundaries.



FR. ROBERT:  Right, exactly.  This is not your area of regulation.  And the FDA, they must have consulted some tech-savvy people, came back and said, wait a minute.  



STEVE:  If not us, then who?



FR. ROBERT:  If the tech is broken, that affects the medical efficacy.  That's exactly your charter.  



STEVE:  Yup.



FR. ROBERT:  But I think you're right. I think federal agencies need to start doing this, this whole "security forward" view, where security is part of the first discussion, not something that you tack onto the end, which we both know that that's sort of been the M.O., not just for medical or voting machines, but from pretty much everything.  Security is, after you get the thing working, then you can secure it.



STEVE:  Well, IoT.  It's like, oh, yes, everything's connected to everything.  And it's like, wait a minute, you know...



FR. ROBERT:  Oh, yeah, we'll throw a password on it somewhere.  Don't worry about it.  It's okay.  It's okay.



STEVE:  Yeah, yeah, [crosstalk].



FR. ROBERT:  So that is good news.  So when I do eventually have to have a pacemaker installed because I'm still eating way too much high-fat foods, at least I know that, A, it won't be as easy to hack me; and, B, I will have the right to actually look at the data coming off my pacemaker.



STEVE:  Yes.  And you alone will know how to do that.



FR. ROBERT:  No, I'm going to put it on the Internet.  It'll be on my Twitter feed.



STEVE:  Okay.  So at last year's RSA conference, Microsoft's Chief Legal Officer, Brad Smith, started talking about what he called the Cybersecurity Tech Accord.  And apparently he's been doing - he/they, Microsoft seems to be the leader here - has been doing a year's worth of behind-the-scenes meetings and conversations and committees and the stuff that big companies do that just puts me to sleep.  But it was successful such that, at last week's RSA conference, a little over a year later - the previous one was in February as opposed to early April - the Cyber Tech Accord was announced.  It's got a domain,  CyberTechAccord.org.  And essentially this is a voluntary membership of companies who want to say they're going to team up and agree to profile their behavior in a certain way.  So there's, like, four primary tenets to this accord.



Strong defense:  Tech companies should do their best to protect users from any type of cyberattack, regardless of source or the user's native country.  No offensive development:  Tech companies should never provide material support to government-backed cyberattacks.  Third is capacity building:  Companies should build and provide customers with the necessary tools to protect their data and themselves from state-sponsored attacks.  And, finally, collective action:  Companies will collaborate with each other to share data on attacks and disclose attacks to affected users.



So this isn't a big deal.  I mean, it's sort of nice to see.  It feels very bureaucratic to me.  But certainly it demonstrates, for companies that are wanting to be part of the accord, that they've got their heart in the right place, I guess, if nothing else.  In the announcement a couple people who are members were quoted.  Kevin Simzer, who's the COO of Trend Micro, who of course we all know, said:  "The real world consequences of cyber threats have been repeatedly proven.  As an industry we must band together to fight cybercriminals and stop future attacks from causing even more damage."



And then Carolyn Herzog, who is the general counsel for ARM, said:  "The Tech Accord will help to protect the integrity of the one trillion connected devices we expect to see deployed within the next 20 years.  It aligns the resources, expertise, and thinking of some of the world's most important technology companies to help to build a trusted foundation for technology users who will benefit immensely from a more secure connected world."  So again, yeah, yeah, yeah.  I mean, I guess it's not like it's forcing anyone to do anything.  But it does feel like a good thing to have.  And maybe within the umbrella this creates communications channels.  It allows companies that wouldn't otherwise be grouped together to share things.



Okay.  So 34 tech companies that are very familiar to us are in the Accord as of the announcement last week:  Arm, Avast, Bitdefender, BT, CA Technologies, Cisco, Cloudflare, DataStax, Dell, DocuSign, Facebook, Fastly, FireEye, F-Secure, GitHub, Guardtime, HP, Intuit, Juniper Networks, LinkedIn, Microsoft, Nokia, Oracle, RSA, SAP, Stripe, Symantec, Telefonica, Tenable, Trend Micro, and VMware.  I don't know why Amazon, Apple, Google, and Intel are not on that list.  They had to have been approached.  It seems odd that any company would say we don't want to do this, or we don't want to have our name listed.  Maybe they're not joiners.  I don't know.



FR. ROBERT:  It's probably the "collective action" bullet point because those companies do make part of their living off of having unique security and marketable security.



STEVE:  And unshared security.



FR. ROBERT:  Correct, correct.



STEVE:  Ah, okay.



FR. ROBERT:  The other thing about this is I like this.  I mean, I love the fact that companies are willing to throw their hat into the ring.  So I don't want to take any of that away.  And I've learned to not say "but."  So instead I'll say "and."  And there is a problem.  And the problem is that second bullet point.  I mean, yes, I could do strong defense.  Yes, I can do capacity building so I'll continue to have strong defense.  Yes.  Maybe I'm even up for collective action.



But the no offensive development clause is problematic because if the - I'm going to use my country because that's where we live.  If the United States government comes to you and says we want this, it's very hard to say no, especially now since we've just had a law passed that essentially says they can do it anyways.  Because what is material support?  Material support might just be credentials that they're looking for.  And they now have the ability to go into any server that a U.S. company controls and take what they want. 



STEVE:  Well, and I wonder if material support means involuntary support.  I mean, because in the U.S. we have court orders; and  companies need to be able, I mean, they can fight them.  But ultimately our courts decide ultimately, I mean, even if it goes all the way up to the Supreme Court, whether a company is allowed to say no or not.



FR. ROBERT:  Yeah.  And I get it.  Look.  There's going to be people who look at this, and they're going to say, this is just security theater.  These companies, there's nothing in this accord that actually tells them what they have to do.  But if you join enough of these, it tells you what people actually want out of your company.  Remember when Microsoft first started releasing some of their products into open source, and people thought, oh, this is all just a big show.  This is just Microsoft wanting to cozy up with those that they later intend to backstab.  It turned out that, no, that was not a one-shot deal.  Microsoft has actually been pushing more and more of their innovation into open source, and they've been [crosstalk].



STEVE:  Oh, we're about to talk about that at the end, yes.



FR. ROBERT:  So, yeah, say what you want about it, I still think this is a net positive.



STEVE:  No, I agree, yeah.



FR. ROBERT:  All right, Steve.  So we've got two good pieces of news.  My pacemaker won't be hacked quite as easily, and we've got vendors who are jumping on the security bandwagon.  But what I really want to know is if I should stop screaming at my hard drives because I do it all night.  I mean, they're in my NAS, and I'm just looking at them, and I'm saying, you know, you're just dirty.  You're all dirty.



STEVE:  So we've covered this a couple times, and there's a famous - it's become famous - video on YouTube of some guy literally at the top of his lungs screaming at a large RAID array of drives, or a large drive array, and causing trouble.  Which initially surprised people until we realized that of course sound is acoustics, which is mechanical.  And today's state-of-the-art hard drives are still the storage medium of choice when you need ultra high-density in datacenters because, yes, solid state is coming online.  But still, in terms of dollars per byte, where you absolutely have to have the highest densities and the lowest cost because you need so much storage, spinning media is still the way to do it.



In order to get the level of storage density that today's drives offer, the track spacing, I mean, the term "spacing," or the term "track spacing" is an oxymoron.  There is no space between tracks.  I mean, it's just - it's ridiculous what the TPI, the Tracks Per Inch is now.  And so what that suggests is, since drives are still mechanical, that suggests that external vibration applied to the outside of the drive will translate into some motion that the heads, which are tightly controlled but are on a pivot, need to deal with.  So all of the drives have a servo control system which maintains the heads on track.  But it's already fighting that job, the job of maintaining the heads with essentially absolutely no tolerance.  I mean, if there was any tolerance, the engineers would have removed it in order to get higher density.  So you can't have any higher density, or they already would.



So against that existing challenge, you then essentially twist the drive.  Not a lot, just a little.  And that causes the heads to go off track.  So that's why vibration is a modern hard drive's worst enemy, and sound is vibration.  So in this famous YouTube video there was some - I think they were performing reads from the drive and intermittently screaming at these drives.  And sure enough, the read rate dropped because the heads were driven off track by the screaming vibration.  And the drive knew that it was off track.  It's like, whoops, where did that sector go that I was looking for?  And so it would have to go around again, another revolution of the drive, in order to retry that sector read.  Consequently, the reading rate dropped.



And of course we've talked about SpinRite often where people talk about how their drive, their system is still working, but it's slower.  And so what's happening is the drives are having to - the problems are beginning to be uncorrectable by the software algorithm.  So the drive sort of shrugs internally and says, well, let's just try again.  And so it waits for the sector to circle back around, and then it reads it, and it's able to correct it.  SpinRite, of course, is able to work with the drive in order to fix the problem, in order to make the sector readable in a single revolution rather than taking multiple one or more retries.  And as a consequence of that, people often find that, after running SpinRite, their computer boots again fast.  It's like it used to be.



And in fact a couple weeks ago we talked about a story where someone's mom had lost her sequel to her first novel, couldn't read it any longer.  And she'd been noticing that her computer had been slowing down, but she figured, well, that's what happens.  Run SpinRite, and she got her data back, and it was fast again.  Oh, good, and now you're showing the video, yes.



FR. ROBERT:  So he's got a big array, a RAID array of drives in a datacenter, and he's just screaming at it.  And then they go to the status screen.  And on the status screen you can see what's happening to the throughput because the error correction kicks up.  And on so many levels this totally makes sense.  It's a mechanical device.  Vibrations are going to affect a mechanical device.



STEVE:  Yup.



FR. ROBERT:  And by the way, thank you, Aneroid, for giving us that link.



STEVE:  Yes, thank you.  And so what happened last week, last Wednesday in the early hours of April 18th at a Swedish datacenter, was actually something that we had talked about maybe a year ago.  I think a year ago it was a test of the fire suppression system, as I recall.  This time it was just an anomalous misfiring of the fire suppression system.  These big datacenters, of course, have to protect their systems from any sort of fire.  It's uncommon, but you could have some power supply blows up in a server and catches on fire.  Okay, you need to put that out.  So they use very high-pressure inert gas, which they need to get out of the tanks and into the atmosphere as quickly as possible.  Sometimes, if these systems are not tuned, they are incredibly loud.



And so in the news coverage of this, it turns out that on April 18th the loud sound emitted by the high-pressure release of inert gas used in this datacenter's fire suppression system destroyed the hard drives of a Swedish datacenter, taking down the NASDAQ Stock Exchange operations - it was Nordic NASDAQ - across Northern Europe.  And you might say, wait a minute, how could it destroy the drive?  Well, if you are writing when this happens, that will destroy the drive because essentially writing occurs in bursts.



After you encounter the beginning of the sector you say, okay, we're on track.  We're good to go.  Basically you turn on a big electromagnet, which is very powerful now because it needs to write very small bits very fast.  And it's assuming that, between now and when it gets done writing the sector, everything is going to be okay.  But if something happens to that drive mechanically, you then cross-write, you're writing on a diagonal across adjacent tracks, and you can destroy the drive to the point where it's unrecoverable.



FR. ROBERT:  Now, Steve, I'm thinking it didn't actually destroy the drive because our audience would pick up on this.  The mechanical drive would be okay because the heads didn't crash.



STEVE:  Actually, it would have destroyed the low-level formatting, which is no longer something that you're able to correct.



FR. ROBERT:  Right, you can't do that anymore.



STEVE:  Right.



FR. ROBERT:  Because as the head moves it would bounce across tracks, and you're just writing a swath of destruction across a rapidly spinning drive, and there's no way to correct that anymore because you can't do that low level.



STEVE:  No.



FR. ROBERT:  Wow, okay.  Wow, I guess you do destroy the hardware.  



STEVE:  So for what it's worth, anyone who's working with hard drives, pay attention to vibration.  Vibration is not a hard drive's friend, especially these days, where the track density is just ridiculous. 



FR. ROBERT:  And just so you know, anytime here at TWiT we start having glitches with our servers, we're just going to blame it on Patrick Delahanty in the datacenter yelling at the hard drives.



STEVE:  Oh, yes, you want to get somebody who walks softly. 



FR. ROBERT:  Nobody here.  Literally nobody here.



STEVE:  So, okay.  So this is a weird story, but I thought it was worth discussing because we all know, especially this audience, that the SSID of a WiFi network, the name we give our WiFi network is, unless you make it private, that is, unless you tell it not to respond to beacon requests and to broadcast itself, it's public.  And anyone these days who opens up their wireless monitor is sometimes entertained by the names that their neighbors have given their WiFi network.  I have a buddy who his is like NORAD West Probe Station or something like that.  It's like, oh, okay.  I tend to be a little less creative, but it's often entertaining to see what people have called their WiFi networks.



Recently, Michigan police were called to a Planet Fitness Gym, and that was earlier this month, to investigate what was considered a bomb threat because somebody who was at the gym had named their WiFi network Remote Detonator.  Which is probably not in this day and age a good idea.



FR. ROBERT:  Not smart, no.



STEVE:  A different gym patron at Planet Fitness gym spotted what they considered to be a worrisome WiFi network name and called the police.  And apparently that was in accord to the gym's normal procedures.  The gym was closed down and reopened three hours later after bomb-sniffing dogs swept the building without finding any trace of explosive devices.  And so I would argue that, while somebody might have thought it was a cute prank to name their WiFi Remote Detonator, even they were regretting the fact that their gym that they were trying to work out in was closed for three hours.



Saginaw Township Police Chief Donald Pussehl told a local paper that everything was legal from the police's standpoint.  There was no crime, and no actual threat was made.  No one called saying there was a bomb.  It was just somebody who passively noted that one of the SSIDs, one of the available WiFi networks was named Remote Detonator.  And apparently this is considered protected speech under the First Amendment.  Now, there have been instances of this causing problems in the past.  In 2016 a passenger on a Qantas Airline flight had named their WiFi hotspot Mobile Detonation Device.  Again...



FR. ROBERT:  Yeah, maybe just don't do that.



STEVE:  Not a good idea when you're getting on a plane.  That grounded the flight for hours before it was cleared to take off.  And again, I'm sure that passenger wasn't happy that the flight was grounded.  Who knows what connections were missed.  And sitting on the tarmac or being unable to leave the gate while people figure out what Mobile Detonation Device is on a plane, that's going to be an inconvenience.  And a year later, in 2017,  a Turkish Airlines airplane made an emergency landing in Sudan after somebody onboard looked at the WiFi networks available and saw Bomb On Board was actually what someone had set their SSID to.



So anyway, I guess this takes the form of a public service announcement.  That's probably, I mean, it's not technically illegal.  Maybe it's a function of where you are in the U.S.  First Amendment apparently covers this.  You're not actively screaming "Fire" in a theater and doing something where you could argue that you have breached your First Amendment rights.  But really, think twice before naming your SSID something inflammatory which, depending upon where you are, really could cause a lot of discomfiture to people around you.



FR. ROBERT:  And here's the thing.  I mean, we can go back and forth.  This is not TWiL.  This is not This Week in Law.



STEVE:  You are broadcasting something, after all.



FR. ROBERT:  You are broadcasting.  And remember that, even under the constitutional protections given to us, fighting words and words that are clear and present danger to the public safety are not protected speech.  That's why you can't yell "Fire" in a crowded theater, and that's why me telling you that I'm going to kill you and your family is not protected either because those are fighting words.  You could go back and forth on this because this is more passive.  This wasn't someone threatening to blow something up or challenging someone.  It was just someone being very stupid about what they call a publicly broadcasting device.  And so let's not get into that.  Let's just say, how about this, don't be that guy.  Just don't be that guy.  I think we can all be like that; right?  Unless you want to be that guy, and you're okay with being that guy, don't be that guy.



STEVE:  Well, yes.  Okay.  How about be creative?  Certainly there are a whole bunch of other really fun things that you could get up to.



FR. ROBERT:  I've got one.  So I've been doing a lot of time in planes over the last couple of months.  In fact, I added it up the other day.  Since the first of this year, I've spent a total of, is it 72 hours over the Atlantic?  That's just over the Atlantic.  So that's fun.  That was, like, nine trips.



STEVE:  Wow, yeah.



FR. ROBERT:  But on one of the trips, just for giggles because I had my gear with me, I set up a hotspot on my laptop, and it was Starbucks Free WiFi.  And I used the exact same SSID that they use in every single Starbucks coffee store.  I had 40 connections to that.  So people obviously had not put their phones into airplane mode.  But I'm thinking there were at least a few who were thinking, oh, Starbucks on a plane?  Yeah, that's probably true.  That's probably right.



STEVE:  Or if their phone had previously joined to that...



FR. ROBERT:  Precisely, it would auto join.



STEVE:  Yes, then it's just going to auto join.  It's going to go, oh, hey, I can...



FR. ROBERT:  Which it shouldn't because they're supposed to be in airplane mode.  But, eh, well, whatever.



STEVE:  That's very true.



FR. ROBERT:  Try that, folks.  And actually that's harmless.  Try that the next time.  Even if you only have an Android phone with a hotspot function, turn it on and just watch the client list and see how many will connect.  So Starbucks WiFi is one that I use.  The other is Free Airport WiFi.  There's a bunch that use that.  Just see what auto connects.



STEVE:  That's a very good point.  It does demonstrate that we're not in airplane mode.



FR. ROBERT:  Yes, yes.  All right.



STEVE:  Okay.  So the trouble with our consumer routers.  We've spent a lot of time over the last few months especially because the typical consumer router is a purchase-and-drop-in, sort of a plug-and-go.  The router comes out of the box able to function.  You plug the WAN end into your cable modem or to your DSL modem.  It uses DHCP to acquire the IP that your ISP has given it.  You plug your wired devices in over on the LAN side.  It has a DHCP server that does the same thing, giving everybody an IP.  You obviously have to give it a password in order to set up your LAN, and give it an SSID, give it a name.  And unfortunately, at that point, that is the last time most people ever think about their router.  It's basically plug it in, make it work, walk away.



A publication in the U.K., Broadband Genie, out of curiosity, conducted a survey of a little over 2,200 of their readers of age greater than 18.  So, okay, I guess you'd expect a certain level of maturity and responsibility and so forth.  What we know is that routers can have security problems, have historically had problems, have had manufacturer-implanted backdoors with fixed passwords.  There are worms on the 'Net that are checking routers for vulnerabilities.  There's Universal Plug and Play that is enabled by default.  My own ASUS, a recent ASUS router that I purchased, I was very impressed.  It had "Respond to ICMP ping request" disabled on the LAN, but UPnP enabled on the WAN.



FR. ROBERT:  Oh, so close.



STEVE:  And it's like, guys, why? 



FR. ROBERT:  Why?  Why?



STEVE:  Why UPnP?  What possible reason to have Universal Plug and Play enabled on the WAN interface?



FR. ROBERT:  And ASUS is actually one of the better ones.



STEVE:  Yes, yes.  It's an impressive router.  It's got four guest networks and all bells and whistles and dynamic free DDNS and everything you could ask for.  And Universal Plug and Play enabled.  I just - I can't understand it.



And I guess it's because they also have remote - oh, it even has Let's Encrypt.  It builds and binds a Let's Encrypt SSL cert to your router for you, so you're able to do an HTTPS external connection to it.  So it's amazing.  But Universal Plug and Play enabled.  And maybe it's so that you're able to open ports and get to it, unfortunately.  Or at least as a mixed blessing.



Anyway, the point is this was the Picture of the Week at the top of the show notes.  I have it here again in our show notes.  This Broadband Genie survey asked 2,200 people about their router history.  How many have ever updated their router's firmware?  Answer:  14%.



FR. ROBERT:  Wow.



STEVE:  Yup.  How many ever changed their network name?  I mean, okay, now, I'm like, how do you not do that?  I guess it must come with some - in fact, I guess I know because I remember looking at a list of available WiFi just a couple days ago, and there were, like, four Linksyses.  That's all it said, "Linksys."



FR. ROBERT:  Or Comcast.



STEVE:  How do you know it's...



FR. ROBERT:  They will give you the SSID and the password on a piece of paper, as well as on the modem itself.  So most people don't even change it.  They'll go, oh, okay, guess that's what I'm going to use.  They don't even know how to get into the management page.



STEVE:  And 18%, only 18% had ever changed their WiFi network name.  That same 18% had ever changed the admin password.  Meaning that the default admin password for, what, 82% of all WiFi routers among this population has never been changed.



FR. ROBERT:  And if the default was also to allow remote login, that's a problem.



STEVE:  Yup, exactly.  If WAN admin is enabled, as it sometimes is on routers, you're hosed.  You will have somebody, even if there's no known vulnerability that lets a worm get into your router, if somebody has any interest in you, they can get in and start browsing around your network.  So, wow.  And we know, for example, that Universal Plug and Play can be used, we were talking about it last week, to turn your router into a public proxy in order to use you as a waypoint for malicious traffic bouncing around the Internet.  Which is not how you want your bandwidth used.



FR. ROBERT:  No, not really, not really.



STEVE:  Only 30% of router owners ever check to see what devices are connected to the network.  So they're just like, oh, you know.  I mean, famously in the old days, of course, everyone's neighbors were using their WiFi.  Now everybody's neighbors has their own WiFi.  Once upon a time that was, you know, it was like, oh, yeah, I'm just using my neighbor's WiFi because he doesn't care.  It's like, okay.



And only 31% had ever changed their WiFi password.  So it's exactly as you said, Father.  It's like, okay, 69% just took what was written down and typed it in.  Hopefully it was secure.  Wow.  And half the people, none of the above.  Half the people none.



FR. ROBERT:  Of course.  Well, half of the people don't even know that they should do that.  That's the thing.  That's the real issue.  It's not malicious.  It's not, oh, yeah, I should do that, but I'm not going to.  It's a device, and they figure that they bought the device from the provider - Comcast, Cox, whatever it's going to be.  And the provider's going to take care of it, which of course we know, of course they're not going to do that.  But they don't know that.  They don't know better.



STEVE:  These things are sold as appliances that you plug in and you do nothing with.  And so we're going to talk about here at the end of this podcast today Microsoft's initiative for potentially changing this.  And it's the reason, as I said at the top of the show, I'm very bullish about this.  We have to as a, well, more than as a society, as a global community, we need to change the way we do this.  It has to be that moving forward this device takes care of itself.



This device, I mean, we don't need to change anyone's behavior.  It can be fine that they buy something and plug it in and set it up.  But it has to be able to take responsibility for managing itself.  And I'm somewhat dumbstruck that that hasn't already happened, that it's still - like, for example, if I log into my ASUS router's admin page, there's like a flashing yellow exclamation point telling me that, oh, look, you need to check to see if you've got updated firmware.  And often there is new firmware.  It's like, okay.  Why doesn't it just do this?  Why doesn't it wait until 2:00 a.m. and download and reinstall firmware for me?



FR. ROBERT:  I actually asked that.  So my router of choice right now is the Synology series.  They make the 1900 and the 2600.  I like them because they're Linux based, and they are very good about firmware because they also do NASes, and they're always updating the firmware for the NASes, and they share components between the operating systems.  But the one thing that's always bothered me is the router will, by default, look for updated firmware and download it so it's ready to go.  But it won't pull the trigger.



And I asked my Synology guys, why  not pull the trigger?  And they said it's actually a legal thing because, if something happens, we're in trouble.  It's like, okay, then that's a piece of law that we need to fix because, like you said, most people are not the audience of Security Now!.  Most people...



STEVE:  Never, never, never going to.



FR. ROBERT:  Never going to.  They think of it as an appliance, so it needs to work like an appliance.  I don't think about updating my refrigerator's firmware, so I shouldn't have to think about updating my router's firmware.  And we need to make it okay for companies like ASUS and Synology and even Linksys to turn that switch and say, you know what, we're just going to take care of this for you.  When it's a weird time and no one's using it, I'm going to go down for 10 minutes so I can update my firmware.  And then of course when that happens, and that's going to be wonderful, there'll be people like me who are screaming, "Why did you choose 3:00 a.m. to update?  Of course I'm working at 3:00 a.m.  You're horrible."



STEVE:  Right, right.



FR. ROBERT:  It's a no-win.



STEVE:  So there was, again, a lot of interesting news about Google's announcement of a new messaging system for their Android platform.  It was funny, I think it was on Saturday Megan and Jason covered this.  They didn't cover the angle that we take on this podcast.  But they were, I mean - and of course Jason is Mr. Android.  They did talk about all the attempts that Google has made through the years at messaging.  And nothing they do gets traction.  iMessage is still sort of the industry standard for it just works.  It does everything people want.  If you're an iOS user you get the blue balloons, and the two friends that you have that are on Android, they get green because they have to use SMS.



Well, what the press picked up on is an interesting flaw that exists and has a lot of the press scratching their head.  First of all, Google calls it "Chat."  And what it is, is a standards-based extension of the existing SMS and MMS, using something called RCS, which is Rich Communication Services.  And within RCS there's this notion of a universal profile which is sort of the set of detailed interactions and specifications within RCS to allow everything to interact so that, if all the carriers supported the universal profile of RCS, then everybody would be able to interact.



And to Google's immense credit, what they have somehow managed to do, and it's phenomenal, actually, is they've gotten, I think it's 34 carriers, 11 somethings, and two OEMs, maybe it's 11 OEMs and two manufacturers, to all agree and all support the universal profile for RCS.  What this means is that the forthcoming Google Chat for Android will be universally interoperable globally.



And it was Verizon and one other, I can't remember, two final major carriers, even they went with it.  They dropped this notion of trying to differentiate themselves and go their own way.  It's like, you know, no.  We're going to do this new chat app which is going to use RCS.  And sure enough, on the surface, it delivers an experience like iMessage.  You know, multimedia, return receipts, group messaging, a lot of the things that have not been available before.  However, it misses one arguably important feature.  There's no encryption.



FR. ROBERT:  Wah-wah.



STEVE:  Now, thinking about this, I guess I can kind of understand that maybe Google can't solve this problem.  Apple can because they manage the keys.  They're the closed ecosystem, and you're tied into Apple and iOS, and they're pushing versions and everything.  And again, stated simply, they manage the keys.  We've just been talking about how Telegram has been recently and incrementally, but now completely, pushed out of Russia, and previously was pushed out of China.  Well, Google doesn't want their chat app to be outlawed by major segments of the globe.  You know, China is big.  And Russia is huge.



So I guess they probably decided, okay, we're going to finally come up with a way to give our Android users the features they want and not battle, not do this encryption battle which is brewing, just not have that be our problem.  If individual users want encryption, they can use Signal.  They can use Telegram where it's legal.  They can solve that problem one way or the other.  But for most people, and it turns out that most people do - all the studies show most people use whatever is the default chat app in their Android device.  Very few people bother with a third-party chat app unless they have some need or reason for it.



So I think what Google is doing is just saying, okay, we're going to, by pushing the standard to universal profile for Rich Communication Services, even though there's no encryption, we're going to give people the experience they want.  And lots of people probably don't care.



FR. ROBERT:  Yeah, that's true.  I mean, encryption to a lot of people still sounds like hocus pocus.  And as long as it does what they want it to do, they'll use it.  Although, Steve, I will say this.  So I just came back from Rome.  I had an international meeting with my counterparts from across the world.  So I had a representative from the African nations, a representative from the Asian nations, a representative from the European nations, the Latin American nations, United States, and India.



STEVE:  And they're all like other versions of you?



FR. ROBERT:  Basically, yeah.



STEVE:  Okay, cool.



FR. ROBERT:  I'll be the Curia counterpart, so I'm at the headquarters, and we all do the same work.  But it was interesting because we were looking for a way to communicate with all of us.  And we went through a couple different solutions.  They didn't want to use Slack, and Hangouts was a nonstarter for them.  But every single one of them except for the U.S. delegate used WhatsApp.  That's the big one.  And they won't go anywhere without it.  So I'm thinking, unless you can offer more than what WhatsApp offers, you're not going to crack outside of the U.S. market that much.  That's just a thing.  So Google can make yet another client because they're really good at doing that.  They've done a couple since Hangouts, and Hangouts is still around.



STEVE:  And Allo they also are giving up on, by the way.  It's just like, okay, fine, that never happened.  It just didn't get traction.



FR. ROBERT:  That's why it's so hard for me to get excited about another Google Chat release because I'm thinking, well, in three years they'll come out with one, it'll be better, and maybe it'll have the things I want this time, like encryption.



STEVE:  Well, it is nice, and I thank them for moving the cellular industry forward beyond SMS and MMS, that had been the lowest common denominator.  Essentially it was the lowest common denominator.  And so, if nothing else, this has forever moved the cell industry because everyone has agreed to support it, has upped the ante now to what is arguably a more useful functional chat system as a new lowest common denominator.  But as you said, at the cost of you guys, your team can't use it because you need to have some control.



FR. ROBERT:  Now, Steve, next up, you know what I'd love to do?  I would love nothing more than to allow Amazon to get into my car whenever they'd like to.  So I want to talk about that.  Want to talk about that?



STEVE:  What could possibly go wrong?



FR. ROBERT:  What could possibly be wrong with that?  So, Steve, I was really, really not comfortable with allowing Amazon into my home just by pressing a button.  But I think I would totally be okay with them having a special button that just pops my trunk anytime they want.



STEVE:  Yeah.  So they've been circling around this for about a year.  There have been some pilot tests.  And today, on April 24th, they announce the availability of delivery to the trunk of your car.  So this is a new delivery service which they're rolling out for General Motors and Volvo vehicles initially.  It's available in 37 cities starting today.  So essentially what this uses, it uses, in the case of GM, the OnStar system, which is a communications system between your car and the cloud, literally, above you in the cloud.  And Volvo has a similar system for a connected vehicle system.  This allows them to know where your car is parked.



You use their application, the Amazon Key app, which you add your car to the Amazon Key app, and you take a picture of it so that the Amazon courier is able to identify it.  They need your license plate also in order to make sure they're using the right one.  And essentially the delivery person queries Amazon to get the current whereabouts of your car.  It does need to be within a reasonable distance of your physical delivery address.  So, for example, it can't be the other side of the country, or in a different country, or maybe even in a different state.  So the idea being home or work sort of location.



And this of course is trying to solve the problem of secure delivery.  Many people who are, for example, apartment dwellers don't have a convenient place where Amazon can leave packages where they have sufficient security.  Or even if you are in a condo or in a community, or in a home where there's a lot of foot traffic, it can be problematical to leave boxes somewhere.  So Amazon has done things like having the Amazon Lockers, where they put something, and then you go and get it.  And so this is an attempt essentially to turn the trunk of your car into a semi-private locker.



The technology of OnStar and Volvo's technology allows them to unlock your car.  So this is all basically - well, to locate your car and to unlock it.  So it's easy to put the pieces together.  The Amazon courier determines where the car is, goes there, identifies it visually, verifies the license, is standing next to it, presses a button on their app.  And with the knowledge that they have a delivery, they probably have to scan the package, verify that they have it.  Everything makes sense.  Amazon verifies that you are expecting a package; you're expecting a trunk delivery.



And so your car spontaneously unlocks for this Amazon person, who then puts the package in the trunk, closes the trunk, relocks the car, and all of these things have to happen in order for the delivery person to have successfully delivered the package and then get their next assignment.  Packages are limited to 50 pounds.  They cannot weigh more than that.  They cannot be larger than 26x21x16.  They cannot require a signature.  They cannot have a value over $1,300, nor can they be from a third-party seller.  They can only be from Amazon directly.



FR. ROBERT:  That's to stop scams; right.  So that's to stop vendors from trying to scam through this.



STEVE:  Exactly.  So anyway, I get the problem they're trying to solve.  Maybe it's better than a drone flying overhead and dropping it on your head.  Not clear on that one.  And again, we immediately saw problems where, as you commented, Padre, that people were aiming a webcam at the front door, and it took, what, was it a week before all kinds of...



FR. ROBERT:  It might not even have been that.  I think it was, like, three days.



STEVE:  I think it was, before that system got hacked.  So I guess, I mean, no one is going to not understand that you're giving Amazon permission, with lots of caveats, to have access to the interior of your car.  Hopefully people don't have super valuable things already because we know that cars are not that secure.  Locks can be jimmied.  Windows can be broken and so forth.  So I guess within - and I don't think it unlocks only your trunk.  I think the car unlocks.  All the doors pop open.  I don't think it's an exclusive trunk unlock.  I'm not sure.  Maybe if the system can provide that, then it's just the trunk.  For example, I think my car's trunk is able to unlock.  I don't have any connectivity.  I'm driving something old.



FR. ROBERT:  I don't have any connectivity, but I can unlock my trunk without unlocking my doors.  And also I have the ability to disable the lever on the inside so you can't actually pop my trunk from the cabin.  You need both my key and my fob to make that work.  So I have to put it in the keyhole, and I have to press the fob in order for it to pop it.



STEVE:  Nice.



FR. ROBERT:  I know most people don't turn that on because I'm kind of paranoid.  But I'm with you, Steve.  I understand why they made this.  And I will make fun of it mercilessly because I think it's a horrible, horrible idea.  But there is an edge case or two where this would be more convenient than, say, having it delivered to an Amazon Locker that you then go to to get.  So, okay, sure.



STEVE:  Yeah.  If your car is sitting in an accessible parking structure all day, and you're at work, and you want something delivered to your trunk, and you have not left anything valuable inside, it's not as if any random Amazon employee is able to walk up to your car anytime they want to and open it.  I mean, so it is as locked down as it can conceivably be, given that it has to be - you are allowing under certain requirements your car to be opened.  So, yeah, okay.  



FR. ROBERT:  Aneroid in the chatroom brings up a good point, which is, if they could do this service while your car was in motion, then it would be worth it for the entertainment value.  You're just going down the highway.  Suddenly your trunk pops open and there's a guy leaning out the front of a car, just trying to drop the box in.



STEVE:  That's right.



FR. ROBERT:  Okay, okay.  That's it for the Amazon - what are they calling that, the Trunk Club?



STEVE:  I don't think - I didn't see a name, actually, associated with it.  In-Car Delivery.



FR. ROBERT:  In-Car Delivery.  Do you remember those commercials, 10, 15 years ago of the trunk monkey?  This kind of reminds me of that. It's just something that maybe 10 years ago people would have thought of, yeah, I'm going to allow a retailer access to my trunk?  This has the same sort of feel.  I'm going to allow someone to put a monkey in my trunk?  I'm sorry.  I might be dating myself with that.



STEVE:  And of course, if people don't have connected cars, it's not an option.



FR. ROBERT:  Right, yeah.  And also I would say, if you have one of those cars that doesn't have a separate trunk from the rest of the car, I probably wouldn't do that, either.



STEVE:  Yes.  Then your interior is accessible to somebody at the trunk.



FR. ROBERT:  Also it means that someone could just watch a delivery car and then know which cars they should break into because there's a brand new box in there.  There's the trunk monkey.  Thank you.  John Slanina, always ready with - ooh.  Ooh.  Actually, that might work better than the Amazon Delivery Service.



STEVE:  So our listeners know that I enjoy puzzles.  I have an affection for clever, well-designed puzzles.  And as I mentioned at the top of the show, I feel I'm very picky because our listeners who know this are sending me links to discoveries of theirs all the time, and few of them make the cut.  Infinite Loop was a previous favorite of ours.  The Sequence is probably the all-time blockbuster puzzle, like fan favorite or podcast favorite.  The Sequence was multiplatform, and essentially it was visual programming.  Your goal was to design a little visual machine which would move pucks from a source to a sink, and do it repetitively.  And, oh, my goodness, fabulous.  Blockwick was another, which was just a terrific take on the traditional sliding block puzzle.



The things I like have a number of features in common.  There's no timer.  There's no time limit.  There's no [buzzer sound] bothering you.  There's no rush.  There's no hurry.  It's meant to be enjoyable and relaxing.  So you also don't need to worry about running out of anything - time or turns or attempts or choice or anything.  That's not what it's about.  It's about getting from the start to the finish at your own speed, making mistakes, being forgiven infinitely, and also having a sense of progress, a sense of, okay, I can see that I'm getting somewhere.  It's not a sudden, like, aha sort of thing.



So with all of that preamble, I found another one, and it's on the screen.  And I like it.  It's not free.  I paid $3 for it, I think $2.99, through iTunes.  It's called Dissembler, D-I-S-S-E-M-B-L-E-R.  It appeared at the end of February, and I just happened to see it by a coincidence, at a moment when I was receptive.  Brand new concept, simple, original, very clean.  No timer, no ads, no annoyances.  You can undo things without limit.  Anyway, it's got nice background music which, yes, you could turn off.  There are people who don't like anything making noises or background music.



I've only spent a few hours with it, so I can't vouch for it in the long term.  But it's really a nice concept.  And for those people who have liked...



FR. ROBERT:  But what am I trying to do here?



STEVE:  Oh, okay.



FR. ROBERT:  Oh, I see.  I get it.  All right.



STEVE:  The idea is that you...



FR. ROBERT:  Oh, it's cute, yeah.



STEVE:  With the Pad or the - it is available on Steam, so if you wanted to you're able to use it on any Steam-compatible platform.  But essentially you have an array of colored squares in a certain pattern, and you're able to flip any pair of them.  And when you do so, if you get three or more contiguously connected of the same color, then it's removed from the board.



FR. ROBERT:  Oh, I love these.  This is a sequence game.  It's all about the sequence.



STEVE:  Yes.



FR. ROBERT:  I love those. 



STEVE:  Yes.  It's really well done.  So again, I haven't - it's been months since I found something that I could recommend.  I can recommend this.  I don't know where it goes in the long term.  I did read some reviews before I wanted to recommend it.  And there's a concern that it gets too difficult.  There is, however, an infinite levels mode where you can just say, just make them up.  I just enjoy solving these puzzles.  I think the first 120-plus are deliberately designed and really very nice.  So anyway, Dissembler, my first puzzle recommendation in a long time.



FR. ROBERT:  That might be my next airplane hit because...



STEVE:  It would be perfect for an airplane, yup.



FR. ROBERT:  Jason Howell got me onto Monument Valley, which I love.



STEVE:  Oh, yes, yes, yes, yes, yes.



FR. ROBERT:  But it's so limited.  I mean, I went through that game in 20 minutes.



STEVE:  Yes.  You end up solving it, it's like, oh, crap.



FR. ROBERT:  I need more.  Give me more.  So, yeah, if there's an unlimited, that could get me over the Atlantic the next time I have to go.  All right.



STEVE:  Okay.  So time to talk about Azure Sphere.



FR. ROBERT:  Yes, I like.



STEVE:  And I'm impressed.  Everybody knows I don't carry Microsoft's water ever.  I'm the person who wrote Never10, so that gives you a sense for my position on Microsoft.  They've done something great.  There is a group known as Microsoft Research which is sufficiently disconnected from Microsoft Profit - and that's not P-R-O-P-H-E-T, that's P-R-O-F-I-T - that Microsoft Research Group are able to do good things.



What they have done with Azure Sphere is design a complete front-to-back, soup-to-nuts, open, free ecosystem for securing IoT devices.  It starts with an IoT-friendly processor, a custom chip design, which is a variation of an existing MediaTek chip.  Remember that ASMedia were the bad people who put the backdoor in the AMD chips.  That's not these people.  MediaTek are good people.  One thing you can do, Padre, is google MediaTek, M-E-D-I-A-T-E-K space MT3620.  In my show notes I didn't have a link.  But MediaTek, it's the MT3620.  That's the processor which Microsoft designed in collaboration with MediaTek.  It is what we need for IoT.  It has the equivalent of the Apple Secure Enclave in hardware.  And that's this wacky thing they call Pluton.  It's the hardware security side.



About a year ago Microsoft Research produced a document titled "Seven Properties of Highly Secure Devices."  And I do have a link to the PDF in the show notes, or you can probably just google "seven properties of highly secure devices."  The first property is a hardware-based root of trust, meaning that you have unforgeable cryptographic keys that are generated in and protected by hardware.  Which means that they are physically resistant to side-channel attacks, and it gives the device a unique unforgeable identity that is inseparable from the hardware.  This has that.  Also, a small trusted computing base.



So you have private keys stored in a hardware-protected vault, inaccessible to software.  Remember that that's possible.  That is, you can have your private key sign something without the key itself ever being accessible.  You can only ask it to do the work of signing.  So this creates a division of software into self-protecting layers.



Then you have defense in depth, multiple mitigations applied against each threat.  Then compartmentalization, certificate-based authentication, renewable security, and failure reporting.  Those are the seven properties which Microsoft incorporated into the system.  So we first have an amazing little chip that will be available for developers to play with around the middle of the year and available for MediaTek toward the end of the year.  However, the intellectual property, the IP of the chip, 100% free.  It is license-free.  Anybody else who wants to make this chip is able to do so.  This chip is a multi ARM...



FR. ROBERT:  Cortex, yeah, A7.



STEVE:  Exactly.  It has an application-level processor which is a single-core ARM Cortex-A7 running at 500MHz.  Then it's got sub-processors.  It has a dual core ARM Cortex-M4 with a floating point.  It's got A-to-D converters, general purpose I/O, I2C, I2S serial interfaces, pulse with modulation, an SPI interface and a UART built in, both 2.4 and 5GHz WiFi ABGN, all of this on chip.  And a custom Linux that is a secure Linux distro which Microsoft named Azure Sphere OS, also part of this.  And, finally, a cloud-based service which is, I was stunned, not tied to Microsoft.



FR. ROBERT:  Right.



STEVE:  It's an open cloud-based service that allows - so essentially Microsoft has designed and done all the work of creating an affordable, feasible, truly secure, cryptographically secure and enforced in hardware based chip.  It has both RSA and elliptic curve technology built in, and all the hardware and other support you could want in order for making a low-cost, like consumer-cost, WiFi-connected device that knows how to identify itself to whatever cloud services somebody wants to tie it to so that it's able to essentially do exactly what we were talking about with a router.



In fact, this would be a perfect chip to use for a little router.  It's got all the I/O and capabilities that a router would want and the ability to check in with a cloud for the cloud service to verify on an ongoing basis the true security and update in a secure fashion, as well.  So that, for example, it's able to receive signed update packages that cannot be forged because they are signed by someone whose keys are built into this thing and that are only accessible to it.  I couldn't be more stunned and delighted that Microsoft has done this.



FR. ROBERT:  Just looking at the spec sheet, I mean, it is impressive.  There is a question to me about how low power it is, in other words, how much does it consume, because that will determine what types of applications this will be used for.  But with that many GPIO - so you've got 72 GPIO.  You've got 12 PWM counters.  It does I2C, SPI.  Basically everything that you would want out of, say, like an Arduino training kit, all the way up to industrial automation, this looks like it could handle, which is phenomenal.



STEVE:  Five UART I2C and SPI interfaces, eight analog-to-digital converter inputs.



FR. ROBERT:  Dual-band WiFi.  That's impressive on such a small package.



STEVE:  Yeah.



FR. ROBERT:  But, Steve, how is that key stored?  Does this have some sort of equivalent of like a Secure Enclave?



STEVE:  Yes, yes.



FR. ROBERT:  It does.



STEVE:  Yes, yes, yes.  So that is absolutely key to it.  So they call this thing Pluton (P-L-U-T-O-N) security system.  And they said outside of these three end user-accessible cores - so we talked about the application core, and there's the sub-processors.  There's also the WiFi has its own core that runs the WiFi.  So outside of these three end user-accessible cores, they say the MT3620 contains an isolated security subsystem with its own ARM Cortex-M4F core that handles secure boot and secure system operation.



In addition, a 1x1 dual-band 802.11 a/b/g/n WiFi radio subsystem is controlled by a dedicated Andes N8 32-bit RISC core.  This subsystem contains radio baseband and MAC that is designed to allow high-throughput applications with greater power efficiency.  They said operation of the MT3620 security features and WiFi networking are isolated from and run independently of end-user applications.  Only hardware features supported by the Azure Sphere secure IoT platform are available to 3620 end users.  As such, security features and WiFi are only accessible via defined APIs and are robust to programming errors in end-user applications regardless of whether these applications run on the Cortex-A7 or the user-accessible Cortex-M4F cores.



So essentially it is hardware secure, presenting an API which allows you, for example, to ask the Secure Enclave, which is what this is, to sign a hash of something for you, but never expose the hardware, the private key, and only perform the operation, not give you access to the raw material.  I mean, I'm just, it's like, yay.



FR. ROBERT:  They're giving away the chip.  But that part I'm gathering will work best if you're using it with Azure services because that whole idea of being able to store the key and have  updates securely delivered to the chip is probably going to be a bit smoother if you're using Microsoft's web services.



STEVE:  Well, it's all open.  Okay.  So they said:  "Azure Sphere Open Cloud."  They said:  "The Azure Sphere Security Service guards every Azure Sphere device.  It renews security, identifies emerging threats, and brokers trust between device, cloud, and other endpoints."  So I agree with you, Padre, that certainly they could be providing those services, and I would imagine many manufacturers, as long as Microsoft makes it affordable, would just rather turn that over to Microsoft.  However, Azure Sphere, they say, gives you choice.  You can connect data from any cloud, proprietary or public, or even your on-prem infrastructure, to the Azure Sphere Security Service.  And I read also elsewhere that it worked with Google or AWS or any other cloud provider.



So again, they're not trying to make this about Microsoft.  I mean, they look, from everything I've seen, their heart's in the right place.  And it looks to me like they've done a beautiful job.  Anyway, tons of links in the show notes.  All of the information is available online for anyone who's interested, and a dev kit available in a couple months.  And I don't know if you ran across a picture of it, but it's a cute little Arduino-looking sort of thing.



FR. ROBERT:  Yeah, I couldn't find a picture.



STEVE:  It probably has the same - it's got the two opposing rows of headers.  So I would bet that it's got Arduino-compatible pins.



FR. ROBERT:  Well, Mukti in the chatroom is saying, well, what's Microsoft getting out of this?  Well, that's actually really easy.  Gartner came up with their magic quadrants, and by the end of 2018 they expect just the security component of the IoT to hit $1.8 billion.  That's just one component of a multibillion dollar industry.  So Microsoft is hoping that in doing this you will associate Microsoft with secure IoT.  And if you associate Microsoft with secure IoT, it's much more likely that you're going to choose them as the vendor for your IoT solution.  That's what they get out of it.  There's nothing nefarious about it.  There definitely a profit in store for Microsoft, if this works well.



STEVE:  Yeah.



FR. ROBERT:  I'm excited, Steve.  I mean, this has been something that we've been looking at for a while, which is a security-centric approach to the Internet of Things because no one, no one is going to be able to argue that the Internet of Things doesn't have problems.  We've been seeing it since it first became a thing.  And actually the Internet of Things really first started coming around when I first came onto the network.  That was one of the very first things that we reported on for This Week in Enterprise Tech.  And it became very rapidly, rapidly obvious that even though the devices were interesting, and they reported back interesting data, that security just was not there.  And there have been so many talks over the years about how we're going to improve that, we're going to put security first.  To see a big player actually do that, I like that.



STEVE:  Well, and the problem is everyone has traditionally approached this from here's what I want to do.  Okay, here's a chip that will allow my coders to code the app that the thing does.  Instead, we start with an architecture that is all and only about security, which is open, and which you then lay your software on top of.  So, I mean, it turns it around where it puts security first.  And I'm just like, okay, yes.  I have no problem if there's competition for it, but at least now there is a solution.  And I can't see a downside.



FR. ROBERT:  No, I really can't.  I mean, if Microsoft can give me an open piece of hardware that I can thoroughly scrutinize, I can go forward and back and look at all the different solutions, all the different components that they've included and say, okay, the hardware is secure.  The communication for the firmware is secure.  That gives me the confidence to then design a device that might have access to, say, oh, I don't know, medical information, or personally identifiable information.  It means that when I'm designing my IoT device, I don't have to worry that it's going to be the malware vector for my network.  And that changes what I think is possible with IoT because, if I'm assuming, as I am right now, that every IoT device I put on the Internet is going to get owned, it changes what I think is possible. 



STEVE:  Well, and imagine if this acquires a reputation, if you're looking around for solutions, that is, as a consumer, and you have heard that Azure Sphere is the way to do secure IoT, then that's what you choose.  So it becomes a competitive advantage for manufacturers to embed the Azure Sphere platform into their solution.  We'll cross our fingers.



FR. ROBERT:  Right.  Well, Steve, I've gotten to the bottom of this doc here, and that's wonderful, except for the fact that I haven't heard about SpinRite at all.  Don't we normally talk about SpinRite during these episodes?



STEVE:  Well, I did talk about shouting at drives and SpinRite.  And all of our listeners know about SpinRite.  And so thank you for bringing it up.  I will remember to bring a story to our listeners with you next week.



FR. ROBERT:  Actually, I do have a story because I knew I would be hosting this show with you.



STEVE:  Yay.



FR. ROBERT:  So I had a little tidbit.



STEVE:  Cool.



FR. ROBERT:  One of the things that I did for our IT at what we call the Curia - it's our worldwide headquarters, you know, and that's where I'm going to be working next year.  But the last visit I made I had a meeting with their IT guy.  And we always have an issue with wireless in the Curia.  It's an old building.  Wireless signals don't go through really well.  So I told him that I would bring some of my enterprise gear, some of my enterprise wireless APs.  I brought some Zero stuff, some Ubiquiti stuff, some Rocket stuff.  And I told them, here, play and see what works best.



And then he said, "Well, do you have anything else in your toolkit?" because he loved the toys that I brought.  And I said, "Well, there's one thing that I always have with me, and you could use it while I'm not here."  And I had my copy of SpinRite.  So I put it in my little folder, and I said, "Just take care of this for me."  He's had it for a total of three days, and he just wrote me, and he said, "I've used it six times."  And he said, "It's amazing because we had some systems that we thought were dead, and they were in the corner, and I was going to work on them when I had it.  And on a whim I tried SpinRite on one of the rotating drives, and it worked perfectly, and it brought it back to life, and I've been able to copy stuff off."  And he goes, "I'm now going to go through our entire inventory and see where this might help."  So SpinRite is officially in the Vatican.



STEVE:  Yay.



FR. ROBERT:  You can put that on your site. 



STEVE:  Nice.  Well, it made it to the International Space Station.



FR. ROBERT:  There you go.



STEVE:  And so now it's in the Vatican.  Very nice.



FR. ROBERT:  So it made it into the hereafter and made it into the here yonder.



STEVE:  That's perfect.



FR. ROBERT:  There you go.



STEVE:  Thank you for telling us, Padre.  I appreciate that.



FR. ROBERT:  Folks, we thank you for joining us.  Of course I'm Father Robert Ballecer.  That's Steve Gibson.  We do this show every Friday.  Now, normally it's with Leo Laporte, but I have the honor, the privilege of doing it for the next two weeks, including this week.  I'm sorry, every...



STEVE:  Every Tuesday.



FR. ROBERT:  Every Tuesday.  Did I say Wednesday?  I mean Tuesday at 13:30 Pacific time.  I'm not going to do UTC because that's a Leo thing he's really good at.  Now, Steve is always here to inject you with a healthy sense of paranoia.  Folks, it is still paranoia if they're out to get you.  But, you know, sometimes it makes you look better.  You can find all of our shows at TWiT.tv/securitynow, including back episodes, and a place where you could subscribe if you want to get your security goodness into your device of choice every week.



We've got an audio version, a video version, and a high-definition video version, as well as versions in iTunes, Stitcher, and wherever fine podcasts are found.  Don't forget that you can also get high-quality audio recordings at GRC.com, which is the home of Steve Gibson, where you'll also find his outstanding products like SpinRite, ShieldsUP!, and coming soon, SQRL.



STEVE:  And SpinRite is now Vatican-approved.



FR. ROBERT:  And it is now Vatican-approved.  You actually can put that there.



STEVE:  That's very nice.



FR. ROBERT:  We are extraterritorial.  It's been an absolute pleasure, Steve.  And next week can I present a theme?  Because a big fan of your show, I don't know if you know this, but David Hewlett, who played Dr. Rodney McKay in the Stargate franchise, he has watched or listened to every episode of Security Now! ever.



STEVE:  No kidding.  Wow.



FR. ROBERT:  He is a huge fan.  So I'm thinking...



STEVE:  Well, and I'm a fan of his.



FR. ROBERT:  I'm going to see if I can send you a Stargate Atlantis patch, and I'll wear one, as well.  And I'll see if I can bring in my - hopefully my ZPM will be done by then.  We'll go through the Stargate of Security, the Security Gate.



STEVE:  Nice.



FR. ROBERT:  Does that sound doable?



STEVE:  I look forward to talk to you again next week, my friend.



FR. ROBERT:  All right.  Steve Gibson, Fr. Robert Ballecer, and remember that if you want to keep your data safe going into the future, you're going to need some Security Now!.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#661

DATE:		May 1, 2018

TITLE:		Securing Connected Things

HOSTS:	Steve Gibson & Fr. Robert Ballecer

SOURCE:	https://media.GRC.com/sn/SN-661.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss Win10 getting a new spring in its step, Microsoft further patching Intel microcode, the U.K.'s NHS planning to update, another hack of modern connected autos, Oracle's botched WebLogic patch, an interesting BSOD-on-demand Windows hack, a PDF credentials theft hack (which Adobe won't fix), your Echo may be listening to you, a powerful hotel keycard hack, a bit of errata and feedback, and a discussion of another Microsoft-driven security initiative.



SHOW TEASE:  Your hotel room is owned.  Your connected car needs help.  Your Echo has turned into an eavesdropper.  And does Microsoft have the security answer for everything from your light bulb to your nuclear plant?  It's all next on Security Now!.



FR. ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode #661, recorded on May 1st, 2018:  Securing Connected Things.



It's time for Security Now!.  This is the show with Steve Gibson, our very own security guru.  He's on a mission of singular focus to make the universe safer and simpler.  He's kind of like Thanos but with less purple.  I'm Father Robert Ballecer, the Digital Jesuit, in for Leo Laporte, who's currently in Japan learning how to pilot giant robots.  Steve, how are you, my friend?



STEVE GIBSON:  I'm great.  And fun to be with you for our second of two of your hosting of the podcast while Leo, as you said, is off roaming around Japan somewhere.  Last week we talked primarily about an initiative that Microsoft had put together, the Azure Sphere.  So I misstated something that we'll be correcting in the errata section of today's podcast, which makes more sense because I was giving them too much credit for something.  And so now it's like, okay, now I understand.  This does make more sense.  But they've been busy.



And there's another Microsoft initiative I want to wrap up today's podcast discussing with you about their sort of conceptual display of how to, like, what's necessary to truly solve, and not just say we did and hope no one challenges us, but what's necessary to truly solve the IoT security problem where it's not just light bulbs, but it's also nuclear power plants and SCADA managed systems.  I mean, how do you really, really do it?  And they haven't - they're not, like, selling it.  You can't buy it off the shelf anywhere.  But they produced a very comprehensive sort of executive overview of, like, saying - and they also, in the beginning, say this is not like anything we invented, or rocket science.  This is applying understood cryptographic principles all the way.  So I think we'll have a neat discussion about that for this 661st podcast, as we start off May.



But we also need to talk about the fact that Windows 10 just got a new spring in its step - not called the Spring Creators Update, but now officially named and released, apparently a week before it was expected, and causing some trouble.  Microsoft also has further patched Intel microcode.  This was actually happening while you and I were talking last week.  But it's important, so we need to catch up on that.  Also we know that the NHS in the U.K., their National Health System, got really hurt by WannaCry.  They've decided how to fix that in the future.



We also have another update on the hacking of modern autos with a really interesting look inside and a frightening port scan of a VW.  We've got Oracle's botched WebLogic patch which, even if you give a company responsible disclosure, if they then screw up the patch, well, then everybody knows about it.  And that's the situation we're in right now with very vulnerable WebLogic servers that are right now being exploited after Oracle said, okay, we got this fixed.  We also have an interesting Blue Screen of Death-on-demand Windows hack, which even Microsoft said, eh, okay.  A new PDF credential leak, which Adobe has said they won't fix.  Also a little bit of a problem with the Amazon device - I'm trying to remember, can I say "Echo"?



FR. ROBERT:  You can, you can.



STEVE:  I can't say the "A" word.  But the Echo is safe.  It may be listening to you.  Which of course has everyone freaked out all the time.  I can't remember if it's Lisa, Leo's wife and CFO of TWiT, or if it's other guests.  I think there are some anti-IoT listening device people within the TWiT network.  Anyway, they're like, no, no, no, I'm not having any of those speaker things because they might be listening.  Well, it turns out yes.



Also, tying in to the Picture of the Week, you and I are going to have fun talking about a very powerful hotel key card hack which, after a short bit of time, produces a master key for that entire facility, allowing any door to be opened.  We have a bit of errata, as I mentioned, and some feedback.  And then we're going to talk about this very sweeping proposal that Microsoft has put forward about how to actually make sure bad guys don't go in and turn off the coolant on the reactor, which you really do want to avoid having happen.



FR. ROBERT:  Cortana, please open the pod bay doors.  Now, Steve, this is a packed, packed episode.  And not only is it a packed episode, but you've got some stories in here that I absolutely want to get your feedback on.



STEVE:  We're going to have fun.



FR. ROBERT:  Especially the key card hack and some of the Echo on-demand stuff.  So let's get to it.  All right.  I skipped the Picture of the Week last week, so I'm not going to do it again this time.



STEVE:  You're a quick study, Padre.



FR. ROBERT:  Especially since it's cool.  It's hardware.  I'm a hardware guy.  Steve, what is this?



STEVE:  So it ties into the story we'll be talking about, about the hotel key card hacking, which the F-Secure guys apparently spent a decade on.  Now, it's not clear that they actually did nothing else for 10 years because that's a long time.  But the story is that one of them 10 years ago had a laptop stolen from a hotel room.  And that's, of course, 10 years ago laptops were more expensive and more cherished and prized.  And it was a security guy's laptop, which might have had crown jewels and stuff on it.  So that got them thinking about the security of hotel rooms and the movement from physical keys to the arguably more convenient for everybody, if they were equally secure, new key card technology that you stick into the slot or you swipe or whatever you do.



So here we are now, 10 years downstream.  And as we will be talking about when we get to it a little bit later in the podcast, they completely cracked the system so that, after sniffing a facility's card, they're able to synthesize a master key for that facility that opens every door there.  So, interesting.  This is just a picture, a cool picture of the hardware that they used, showing an RFID-style power and signal loop antenna hooked to some cool little - these things are all over the place now - small little battery-powered processor I/O device.



FR. ROBERT:  Yeah, it looks like an embedded processor with some sort of software-defined radio shield on it.



STEVE:  Yeah.  And it's not clear, I guess, I mean, you need to power the RFID, so you've got to have enough juice to get it going.  But you're right.  We can see an antenna connection, and it looks like it loops around the back and then comes back forward where it plugs into the loop antenna.  But anyway, it's got a couple little blue LEDs because you've got to have those.  And it looks neat.  So we'll be talking about the details of the hack that they pulled off.  But I thought that would make sort of a perfect little Picture of the Week for us.



FR. ROBERT:  And it's a tease.  We like teases.



STEVE:  And we do.



FR. ROBERT:  All right.  Now, you did mention that you gave Microsoft too much credit last week, so let's go ahead and take some of that back. 



STEVE:  Yeah.  Okay.  So first of all - okay.  We've been talking about the continually unnamed, upcoming, next feature update to Windows.  We have, as we know, there are periodic security updates.  And then there are the bigger, what, are they supposed to be semi-annual, I think twice a year officially, feature updates.



FR. ROBERT:  Right.



STEVE:  The previous one was the Fall Creators Update for 2017.  And we didn't know if this was going to be the Spring Creators Update.  It turns out they're just calling it the April 2018 update.  And most people were expecting it to land on the 8th, which would be the second Tuesday of the month, this first Tuesday of the month being May Day, May 1st.  So essentially this is all happening as early in the month as it could.  But people got surprised.  It actually hit yesterday, making it on the last day of April rather than the first day of May.  So this is Windows 10 v1803.  Until now we were at 1709.  So we're now at 1803 with the most recent feature update.  And there's, like, a whole bunch of crap, oh, I mean, I guess I could say that.



FR. ROBERT:  Stuff.



STEVE:  Stuff.



FR. ROBERT:  Yeah, stuff.



STEVE:  New features.  New features in the April 2018 update's timeline, and it will walk your dog for you and things you may or may not have an interest in.  There's a lot of criticism of it.  There are problems already that people are having.  Lots of buzz about how not to have it stuck in your machine.  Apparently, well, and I did it on purpose because I have to, to see what's going to happen to me.  And it took maybe an hour and a half.  And afterwards I used the new storage whatever it is feature to clean up because there was always the disk cleanup that we've talked about.  And Microsoft decided to give us, like to take more responsibility for that.  And so I ran that afterwards, after making sure that it didn't seem to have broken anything, and I got 8GB of hard drive back on a system that was previously already cleaned.  So once you're sure that you want to stay with this version 1803, there's 8GB waiting for you to recoup, if you're interested.



The thing that, I mean, I'm not a big Win10 fan.  I have Win10 machines because I need to make sure that the little software gizmos that I produce work there.  And in fact it turns out that in the case, for example, of InSpectre, they screwed up the, what was it, the 64-bit version of one of their updates so that you couldn't, from a 32-bit code, you got different results as a 32-bit app compared to a 64-bit app in querying the new API.  And so I was forced to write my first-ever 64-bit MASM as a little stub which is built into InSpectre so that it runs that to do its probing, if that's where it finds itself.  So, yeah, I have Win10 systems, although they're not my main world yet.



Two interesting things.  Strangely enough, and we've already - we've been seeing Microsoft hating Linux less recently.  They've made some interesting moves in that direction.  This version, this 1803, has built-in the TAR and CURL commands.



FR. ROBERT:  Yup.  Yup.



STEVE:  And so that's like, oh, that's nice.  The thing that I liked, because it's annoying to me when I get these little slide-out, what do they call them, toasts or something, the new messaging API that kind of stuff slides out down the lower right?



FR. ROBERT:  Yeah, that's one of the first things I kill.



STEVE:  Yeah, new version of Candy Crush Saga.  It's like, okay, what?



FR. ROBERT:  I can't stand them, the little notification applets, yeah.



STEVE:  So there's something called "Focus Assist" that you can get to through the Control Panel under System.  Focus Assist basically allows you to keep your focus rather than having it assist you.



FR. ROBERT:  And we're talking about human focus, by the way, which is, I mean, seriously, there's nothing that will break your concentration, like you're in the zone, programming, writing, whatever it may be, and then you get a pop-up for mail, or there's a new tweet.  I mean, it just breaks you out of that.  It's nice that finally one of the manufacturers said, hey, you know what, what if we gave them an option to remove the distractions?



STEVE:  Yes.  So you can, without any fancy stuff, you're able to say only show me dire warnings where my computer or I am near death.  And what I do is, because you can do a time of day where you want to keep your focus, I say, okay, turn that on at 2:01 a.m.  Turn it off at 2:00 a.m.  So I have a one-minute vulnerability where my focus may be disturbed, but otherwise it's on all the time.  Anyway, there's lots more.  I would say you probably know, if you have Windows 10 Pro and Enterprise and one of the other 12 other than Home, you are able to delay feature updates for up to 365 days.  I know that because I once dialed it all the way up to maximum, and that's where it stopped was at 365.  And there are other things you can do if you have a version of Win10 Home, and you really don't want this.  The point is, if you do a check for updates, and presumably on Tuesday of next week, on May 8th, I mean, this has moved into the update channel rather quickly.



FR. ROBERT:  Yeah, they just push it.



STEVE:  And it's, yes, they're expected to push it.  And the biggest takeaway is people are having problems.  There are all kinds of problems after 24 hours.  So make sure this is what you want.  You do have 10 days, I think it's 10 days, to back out of it.  As long as you don't make lots of other changes and things, you can back out safely, if it ends up installing itself on you, and you regret that.



FR. ROBERT:  Steve, I updated this laptop here to the Pro, the Enterprise version of Windows, specifically so I could tell the automatic updates to go stuff themselves.  And it's amazing.  They understand that it's annoying, so they give the higher end customers the ability to turn that off.  But everyone else just gets it downloaded automatically.  Yay.  Yay.



STEVE:  Okay.  More on updates.  Also, as I mentioned while you and I were talking last week, Microsoft released a round of updates to their microcode update packages.  We have talked extensively about KB4090007.  That was a memorable number.  That's 4090007 which was the Windows manual install microcode update only for the latest version of Windows 10, which at the time until now was 1709.  This was first released on the 13th of March, and then the next day related releases for 1703 and 1607, the successively earlier builds, were released.  But there was no release for 1507, which was the still earlier one.  So as of last week, for our listeners who don't already know, and you're not being updated or notified or anything so there's no reason you would.  And following what we discussed a couple weeks ago, which was Intel's final statement that, okay, here's all of the processors by CPUID that we are going to and have patched for the Spectre v2 vulnerability.  Oh, and a couple that we've decided we're not going to.



So as we know, when that happened, I incorporated that list into an updated version of InSpectre so that it would be able to tell people whether there was a microcode update available somewhere, whether it was going to be from your BIOS supplier through your OEM that you bought your machine from or, hopefully, from Microsoft.  So as of last week, that is, 4/24, we have updates for the 1507, 1607, 1703, and 1709 - much more comprehensive than what we got in March, but still not complete.  So I have a little processor, it's a 406c4, and it's on Intel's list of "has been patched," but it is not yet in Microsoft's patch update.  So presumably they will get this done, that is, they've already demonstrated a willingness to rev these KB articles.



I've got all the Knowledge Base numbers in the show notes for anyone who's interested.  And 4090007 is still current, I mean is still the proper Knowledge Base article, if you are at the previous Fall Creators Update version of Windows 10, 1709.  So presuming that most of our listeners are there, you can still just google "KB4090007" and then scroll down to the bottom to get the link to the catalog, the update catalog.  And then you'll see four entries.  You'll see the older ones that were released on March 13th for x86 and x64, and then the newer ones released last week on the 24th, also for x86 and x64.  So hopefully we will eventually get full coverage from Microsoft for all the processors that Intel has done the microcode update engineering for.  Certainly it wasn't easy for them to do that.  And in fact it turns out they probably couldn't for some.



Oh, and there is something notably missing.  I forgot to mention that, even though there are microcode updates for 1507, there isn't one for the 1511 version because then it jumps to 1607.  And I'm sure Paul and Mary Jo, who follow this more closely than I do, would know what 1511 is and why Microsoft just said, eh, no, sorry.  But at some point Microsoft's going to make you go up to something newer anyway, where you will be able to use the updates.



FR. ROBERT:  My guess is that it's one of those niche versions for some sort of embedded device.  Steve, actually I had a question about this.  Now that these are starting to come out in earnest, and we think we have a version of the patches that's actually going to work, that will actually be usable, is there going to be a cat-and-mouse game?  I know they're updating the microcode.  But as soon as these hit, people are going to be disassembling the binaries to figure out exactly what they patched and what they did, and they're going to be looking for ways around this.  Is this something that's going to have to be repatched and repatched and repatched?  Or will the firmware fix, is it disabling something that it's just going to not allow exploits in the future?



STEVE:  So what it's giving us is it's giving software more control over some features which were believed to be safe.  So we discovered, unfortunately, that it was possible due to all of the optimizations in the processor to deliberately bias the branch prediction logic in one direction, and then use that to perform a test that could create some information leakage.



FR. ROBERT:  Right, a response-type test.



STEVE:  Yeah.  So the worst that, I mean, and it's worth noting also this was all, I mean, an awful lot of fur flying for theoretical problems.  And I salute the industry for taking this seriously because we know, had the news gotten out and it not been taken seriously, there would have been exploits developed.  But as far as we know, there haven't been any.  I mean, as far as we know, not a single other than theoretical proof-of-concept, look, we got a few bits - although I do remember seeing a rather surprising, I guess you'd call it "bandwidth" of information leakage.  That is, the number of bits that you could suck out across a process boundary was surprisingly high.  It was like, whoa, okay, you could actually do some damage at that rate of exfiltration of data.  And you were able to determine where you pulled the data from.  So if you knew of which areas were sensitive - and you could also use it presumably to probe the ASLR in order to derandomize the Address Space Layout Randomization by determining what was where in memory.



And so, I mean, it's becoming increasingly difficult to actually pull off these things because over the last decade security really has gotten a lot better.  But in the worst case, this was an information leakage across process boundaries.  The point is you could turn off your own branch prediction mitigation in your process, but there's still no way that we know of to turn it off in the process that you're trying to get information from.



So I think that terminates any cat-and-mouseness.  I mean, maybe someone will come up with a way of not getting the microcode applied.  I mean, there's that possibility, if you did some sort of like you got a root in, like a rootkit-ish attack, because after all Microsoft is - it's a Microsoft driver loading early which is patching the microcode on the fly.  So if you were able to get in somehow in EFI or in the BIOS or in the boot sequence early and subvert that, then you could keep the microcode patch from being applied and then go about your business.



FR. ROBERT:  Is this something that you can actually make UEFI aware, just so that the system knows if someone's trying to disable the launch of the updated firmware at launch?



STEVE:  Well, yes.  Secure Boot operates from a trust anchor which cannot be compromised.  And we'll be talking about trust anchors at the end of this podcast because that's the core of Microsoft's idea for how to keep somebody from turning off the coolant on your nuclear reactor.  So as long as you've got Secure Boot, and as long as it is secure, that is, I mean, as long as the guarantee that it offers is met, then we're covered.  And remember that the BIOS is patching the chip also.  That is, already we've got BIOSes patching our processors.  Microsoft is just doing a downstream repatch of what the BIOS has probably already patched because the BIOS doesn't know about the newest stuff and probably never is going to.



FR. ROBERT:  Yeah, it's a whole lot easier to download a patch than trying to instruct your users on flashing the BIOS so that it loads the right microcode.



STEVE:  Yes, yes.  And we know that users can get in trouble, too, if the dog trips over the cord while the EPROM is being flashed.  It's like, oh, now my entire system is dead.  It's down hard.



FR. ROBERT:  There are some people who are going to look at this, and they're going to say, oh, well, all of that was just panic.  It was overblown.  I don't see it that way.  I'm with you.  I think if this had not received the attention that it received, Intel would have just quietly kind of brushed it off, and they would have said exactly what we're saying right now, which is the possibility of using this exploit is so low we don't think it's - you don't really need to do anything.  But once someone knows that it's there...



STEVE:  There is a problem, well known, with proving a negative.  I'm put in mind of the W2K scare, the whole - I mean, I'm sorry, Y2K, the Y2K.  Everyone ran around, like, oh my god, oh my god, oh my god.  When we go from 1999 to 2000, the entire computing infrastructure is going to collapse.  Well, it's absolutely for sure that it would have, had we not spent the previous six months leading up to that actually fixing all of the problems.  And so when nothing happened, I mean, I was deliberately awake with my friends on that particular New Year's Eve.  I haven't bothered since.  But it was like, okay, what's going to happen?  And, like, eh, nothing.



So there were all the naysayers who said, ah, see?  This was all a big scare.  No, you know, those of us in the industry know that there were actually a huge number of problems widely distributed that would have really caused havoc if there hadn't been some attention paid to it.  So this is like that, I think.  People say, oh, it really was never anything.  It's like, well, no, it probably could have been something.



FR. ROBERT:  That's the old, wow, you spent all that money on securing your house, and no one ever broke in, so obviously it was a waste of money.



STEVE:  Exactly.  All those automatically turning on floodlights, you just didn't need those, even though they did turn on a few times.



FR. ROBERT:  A couple, yeah.  Now, Steve, last year I think quite possibly one of the biggest stories, I mean, yes, you had this Intel kerfuffle.  But one of these stories across that magical boundary between the technorati, the digerati, the geeks, and the normals was WannaCry because it brought out the worst fears that someone is going to lock your files, and you won't be able to get access to them.  That was one of the highest viewed videos here at TWiT.  We did a thing on Know How.



STEVE:  And it made you - and it made you - WannaCry.



FR. ROBERT:  WannaCry.  A lot.  A lot.  And then there was a lot of political intrigue.  But one of the stories that really started the political and press move was the Department of Health and Human Services or whatever they call it over in the U.K. having a large number of their workstations locked out by WannaCry.  And you say that they've figured out a way to not have that happen again.



STEVE:  Well, I just got a kick out of this.  This is a quickie.  We don't need to spend much time on this.  But I did note that the U.K. health agency, the official name is the U.K. Department of Health and Social Care, has announced that it will transform all NHS [what they call their National Health Service] computer systems to - wait for it - Windows 10.



FR. ROBERT:  Tada.



STEVE:  Officials cited the operating system's more advanced security features as the primary reason for upgrading current systems such as Windows 10 SmartScreen technology, which we know is built into Microsoft Edge, and Windows Defender.  And the coverage for this was in Bleeping Computer.  And of course they're our ransomware guys.  They have been literally on the bleeding edge of ransomware.  That's where they first came to light, really, although I guess they'd been around for about 10 years.  But it was interesting that Bleeping Computer characterized Windows Defender as "Microsoft's sneakily good antivirus product."



FR. ROBERT:  Sneakily?



STEVE:  Sneakily.  So I thought, oh, I mean, this is coming from a source whose opinion I trust on something like AV.  And the writers at Bleeping Computer, this was their terminology.  They characterized Windows Defender as "sneakily good."  So just keep that in mind, you people who are still installing third-party junk.



FR. ROBERT:  You know, us hardcore Windows users have been using Defender for years now, and most of us have not installed an AV because Defender actually works a whole lot better than those third parties.  Just saying.  Just going to throw that out there.



STEVE:  Yes.  I'm with you.  Thank you for making it more clear for those listeners who are saying, wait, what is he trying to say?  So anyway, that's really all I had to say here was that they were - we know that most of the WannaCry-vulnerable machines were Windows 7, but that's because there are so many of them.  What NHS still had was XP, which really upset people.  But again, their medical systems, they had the, well, if it's not broke, then we'll not fix it next month.  Just like let's leave it alone.  And of course they got hugely hit.  Basically the entire U.K. health system was offline as a consequence of it having swept through them.  So they're moving to Windows 10.  And I guess they missed the upgrade Window, didn't they.  So I guess they're going to have to pay for it?  Or Microsoft will [crosstalk].



FR. ROBERT:  You have to remember that the U.K., they actually cut a huge deal, multimillion-dollar deal with Microsoft to continue support for XP. 



STEVE:  Right.



FR. ROBERT:  So, I mean, they're back in the news.  It's sort of like, maybe updating to a new operating system isn't a bad idea.



STEVE:  Maybe it's time.



FR. ROBERT:  Maybe, maybe a good time.



STEVE:  It's only been, what, 10 years.



FR. ROBERT:  And, okay.  I give Microsoft a lot of guff because I have found my machines updating at the wrong time pretty much on a regular basis.  And it's normally right before I start Know How, when it's the laptop I only use once a week.



STEVE:  And you get the little, I call it the "stretchy rollercoaster."



FR. ROBERT:  Right, exactly.  And it's one of those situations where, even though I have active hours set, it's gone so long between updates that the operating system just says, no, I'm not even going to give you the option anymore.  We're doing this now.  But there is some merit to that.  The fact that my computer can be used in a nefarious way against another network, it kind of says, okay, well, if you want this to be connected to the Internet you...



STEVE:  With that comes some responsibility.



FR. ROBERT:  Yeah.  It's like immunity, group immunity here.  It only takes one person who says this is all a crock to hurt the herd.



STEVE:  Yeah.



FR. ROBERT:  Speaking of hurting the herd, Steve, two years ago was it, at Defcon, Black Hat/Defcon, there was this wonderful presentation by two guys who were not security people, they were just programmers who went on to be security people, showed not just a proof of concept, but a demonstration of taking over a vehicle through its Internet connectivity.  And the world has said, okay, well, now we've learned, that's never going to happen again.  Unfortunately, it has; right?



STEVE:  And this is a meaty story, so let's make this a tease.  Tell our listeners about our second sponsor, and then we're going to get into some really nice research that was recently done and found some vulnerabilities that are, well, that's made their manufacturers scramble.



FR. ROBERT:  Scrambled manufacturers.  I think that's enough of a tease to me.  All right, Steve.



STEVE:  Okay, Padre.  Before we start this, I just have to say I was doing a little bit of poking around just now, wondering what the problems were with yesterday's update to Windows 10.  Martin Brinkman over at Ghacks has some nice coverage of this.  And get a load of this.



FR. ROBERT:  Uh-oh.



STEVE:  There's some enumeration here.  First of all, we were talking about Focus Assist.  So one of the problems people are having is slow alt-tab performance when tabbing out of games.  The fix?  Disable Focus Assist in the options under System Focus Assist.  But the bigger one, Martin writes, no microcode update to protect the system against Spectre attacks.  Can you believe this?  He says Microsoft released updates only for Windows 10 v1709 and earlier, and has not incorporated the updates into Windows 10 v1803.  If you installed the update on the PC in an earlier version and ran the update afterward, you will notice that the PC is no longer protected.



FR. ROBERT:  You know, it sounds like someone could have taken a USB drive from one side of Microsoft to the other and just said, hey, can you include this in the patch?  Yeah, thanks.



STEVE:  Oh, my god.  And this list, I'm just scrolling down through this nightmare list.  It's like, you've got to be kidding me.  Okay.



FR. ROBERT:  I mean, they don't do these very often.  You figure that they'd actually sit down with everyone and say, is this ready to go out?  I mean, that was the whole promise of this continual patching process, which is, look, we'll get it right.



STEVE:  Wow.



FR. ROBERT:  Okay.  Now, you know what, Microsoft has done some good things.  I'm going to just tell them that's a bonehead thing, and we'll move on.



STEVE:  Yeah.  I really did like Windows 3.1.  I thought they nailed it on that release of Windows.



FR. ROBERT:  I still have a copy of both Windows 1.0...



STEVE:  Good for you, yup.



FR. ROBERT:  ...and IBM OS/2.



STEVE:  Nice.



FR. ROBERT:  And I think I still can't install OS/2 because it would run too slowly.



STEVE:  Okay.  So back to business.  Anyway, people may, if you're interested in maintaining your security protection, you may want to wait until - I guess it'll be a new KB because they have a KB per version.  So hopefully there will be a Knowledge Base manually installable update for the Spectre v2 patches for v1803 of Windows 10.  My goodness, Microsoft.



FR. ROBERT:  Right.



STEVE:  Anyway.  Okay.  So it's called the CAN bus, C-A-N, which is a convenient acronym since, once you gain access, you CAN do anything you want.  The CAN bus appeared and became standardized during the '90s; and it's now present, it's become like the way things interconnect in our vehicles.  It's present in every automobile built since because every component in today's cars are hooked up to this single bus.  It's used to control everything in the vehicle from steering to unlocking doors to the volume of the radio, the fuel/air ratio mixture, I mean, the transmission, everything.



FR. ROBERT:  ACU, power steering, power brakes, basically everything in the car is connected to the Control Area Network.



STEVE:  Yes.  It's a straightforward protocol, which is part of the reason why it succeeded.



FR. ROBERT:  Not encrypted.



STEVE:  Correct.  Each message has an arbitration ID and a payload.  No authentication, no authorization, no signing, no encryption.  It's like rev point one.



FR. ROBERT:  What could possible go wrong with that?



STEVE:  Exactly. 



FR. ROBERT:  See, Steve, here's the thing.  Here's the thing, though.  That was fine.  The CAN bus was fine because cars were isolated.



STEVE:  Yes.  Yes.



FR. ROBERT:  But then they were connected.  And auto manufacturers never thought, hey, you know what, maybe that connected device shouldn't be connected to the completely unauthenticated, unencrypted network inside the car that runs everything.



STEVE:  And to their credit, I mean, after the first generation of problems, they said, ooh, maybe we shouldn't have one bus.  Maybe we should have a couple buses.  And in fact there are some use cases for a high-speed bus where you need much closer to real-time stuff like, oh, I don't know, braking, steering...



FR. ROBERT:  Airbags.



STEVE:  ...collision avoidance radar, that would be handy to have a low latency on.  And then things like door locks and seat movement back and forth.  I mean, everything is on the bus.  So there's slow speed and high speed.  What you would wish was that they were isolated.  But there's a gateway that interlinks the buses.  So we don't have complete isolation because that was just a bridge too far.  So, yes, it's wide open.  Once you're on the bus you can send arbitrary messages as an equal peer.  It's just a complete peer communication backbone.  Send arbitrary messages which will be received by all parties connected to the same bus.  There's no sender or recipient information, and each component decides for itself if a message applies to it.  So exactly as you said, and as we often say on this podcast, what could possibly go wrong?



So a couple researchers working for a group, Computest, in the Netherlands, decided to sit down and see what they could do.  In their research they wrote:  "We started this research with nine different models from nine different brands.  These were all leased cars belonging to employees of Computest.  Since we are not the actual owner of the car, we asked permission" - which I thought was very nice - "for conducting this research beforehand from both our lease company and the employee driving the car."  Hopefully not while they were driving the car.



"We conducted a preliminary research in which we mapped the possible attack vectors of each car.  Determining the attack vectors was done by looking at the architecture, reading public documentation, and by a short technical review."  They said:  "Things we were specifically searching for were cars with only a single or few layers between the cellular connection and the high-speed CAN bus; cars which allowed us to easily swap SIM cards; and cars that offered a lot of services over cellular or WiFi."  Meaning if it was a recent car, but didn't have much in the way of radio link feature set, one would expect it to have less opportunities, basically a smaller attack surface.



And then they said:  "From here we chose the car which we thought would give us the highest chance of success."  And they go on to describe their attack.  I wanted to show you, though, Padre, in the show notes, on the next page from where I am is the result of an Nmap port scan.



FR. ROBERT:  I love this.  Actually, by the way, this is how the original duo did it.  So Charlie Miller and Chris Valasek, they started doing port scans, and that's how they figured out, oh, my gosh, everything's open.



STEVE:  Yeah.  They found port 23, telnet is open, is like responding to connections.  I mean, and there's a whole bunch of - 23 is the only one down in the service port range.  The other ones are like 10123, that's open.  15001, that's accepting connections.  21002, 21200, they look like they were kind of chosen by programmers who said, oh, let's just choose, you know, 22111, 22222.



FR. ROBERT:  Aside from 23, none of these are standard ports, and no one will be able to figure out what they do.



STEVE:  Yeah.  Oh, no one will even think of looking up there.  And then 49152.



FR. ROBERT:  My favorite.



STEVE:  Yes, that had Universal Plug and Play.



FR. ROBERT:  What are they doing?



STEVE:  Yes.  You want UPnP in your car.  Oh.



FR. ROBERT:  Oh, and by the way, the default credentials for that are "admin" and "linksys."  So, you know, just FYI.  That's horrible.



STEVE:  I know.  They said:  "After further research, we found a service on the Golf" - they ended up choosing a Golf from among their nine starter cars - "with an exploitable vulnerability. Initially we could use this vulnerability to read arbitrary files from disk, but quickly could expand our possibilities," they write, "into full remote code execution."  Yes, run code on the car's processor.  They said:  "This attack only worked via the WiFi hotspot, so the impact was limited."  They said:  "You have to be near the car, and it must connect with the WiFi network of the attacker."  But, they said, "But we did have initial success."  And then they show obtaining a shell, a dot slash exploit.  And the IP was 192.168.88.253.  And they show it says, "System seems vulnerable.  Enjoy your shell."  And then they do a uname -a.  And sure enough, QNX is the little version of OS running on this machine.



FR. ROBERT:  Now, Steve, a little background for our audience.  The reason why this only works through the hotspot is not because of any engineering that they did.  Back to the Miller and Valasek hack, after they showed the world what was possible, the auto manufacturers worked with the carriers to make it so that you couldn't just randomly ping those devices.  But we have, like Emily the Strange in the chatroom is saying that's why she has all that stuff turned off on her new car.  It doesn't matter.  The device is still active.  It's still there because they need a way to be able to turn on the service if you happen to start paying for it.  So that device is still in your car, and it is still accessible, but this hack was done locally.  So they'd have to actually be within WiFi range.  If it wasn't for those carriers, however, and if you happened to change to a carrier that doesn't block that port scanning, you are now vulnerable over the Internet again.



STEVE:  Yeah.  So they did not disclose - they were responsible.  They did not disclose publicly what they found.  But Volkswagen is scrambling.  They said in their conclusions:  "Internet-connected cars are rapidly becoming the norm.  As with many other developments, it's a good idea to sometimes take a step back and evaluate the risks of the path we've taken and whether course adjustments are needed.  That's why we decided to pay attention to the risks related to Internet-connected cars.  We set out to find a remotely exploitable vulnerability which required no user interaction in a modern-day vehicle, and from there influence either driving behavior or a safety feature."



They said:  "With our research, we have shown that at least the first is possible.  We can remotely compromise" - and then they used some acronyms, the MIB IVI, which is sort of the vehicular entertainment and extra features side - "and from there send arbitrary CAN messages on the IVI CAN bus."  Which, again, is not the sensitive one.  It's the more entertainment side.  But it's also the one that shows your dashboard instrumentation.  And as we know from the previous hacks, you can do things like  put up "Hi there, how you doing, fella?" messages where it's supposed to be showing you your remaining miles of gas and so forth.



And they say:  "As a result, we can control the central screen, speakers, and microphone."  They said:  "This is a level of access that no attacker should be able to achieve.  However, it does not directly affect driving behavior or pose any safety risk due to the CAN gateway," which we mentioned is isolating multiple buses within the car.  "The gateway is specifically designed to firewall CAN messages, and the bus the IVI is connected to is separated from all other components.  Further research on the security of the gateway was consciously not pursued."  And actually they did some moralizing later on about proper conduct and what you need to do and advice for people who are doing this kind of research so you don't end up being jailed and so forth.



But anyway, we are driving computers, and they are becoming connected computers.  And in a way they're a little IoT-like, inasmuch as the manufacturers just want to sell cars.  They don't want to sell computers.  People want the features of mobile computers, so that's what they're selling.  But they're certainly paying attention to security.  And the good news is this kind of coverage keeps the automotive manufacturers focused on that need, which is all for the best.



FR. ROBERT:  You know, Steve, ultimately this is just a pivot attack.  This is the ability to get into a device, own the device, because now you can run code on its processor, and then you can connect to the other side.  And the other side is the CAN bus.  We had talked about this at the last Defcon, and one of the ideas that I kept pushing was I said, look, the reason why manufacturers want this is because they want access to the error codes on the CAN bus because that's where they can give you your value-add.  And so that's why they're so reluctant to separate that entertainment system from the CAN bus, because if they do that then they don't have that value-add.



I said it actually would be trivial for them to create an interface that only allows one-direction communication.  And it really should only be one direction.  There's no reason why the entertainment system should be issuing CAN bus commands to the rest of the vehicle.  So you can set it up so that the CAN bus can issue error codes to the entertainment system, but nothing can come back.  That's just a simple firewall.



STEVE:  Right, right.



FR. ROBERT:  And the fact that they haven't done something like that yet, it really kind of tells me that maybe they're not putting as much into security as they say they are.



STEVE:  Yeah.  Well, and again, features versus security.  They want to have the features.  I had a car - I don't remember now if it's the one I'm still driving, it might be - that was firing off some spurious "service engine soon" warnings, like the little idiot light on the console would just come on.  And I remember taking it in once, and there was nothing wrong, and so they reset it.  And then a few weeks later it turned on again.  I thought, okay, this is dumb.  So I just got myself a dongle and reset it myself.  So that was kind of cool that I was able to do that.



So it's the power of that bus that gives you features.  But you sure do wonder where they are from a security standpoint.  And we also know the famous example was trying to have encrypted keys on a DVD.  If you're going to play the DVD in the living room, it's got to decrypt it.  So the keys are in the DVD player.  Similarly, there's nothing you can do, like having a secret password or secret keys in a car that only the service technician's equipment has because, if the keys are in your car, hackers can get them.  So that's futile.



FR. ROBERT:  Isn't it kind of strange, Steve, that since the early days of what we're doing right now, the topics haven't really changed?  The technology has gotten more interesting, but it's still this whole idea of does it matter what kind of security precautions you put into place?  If you leave the key in a public area, someone's going to own it.  There's no way around that.



STEVE:  Yup.



FR. ROBERT:  All right.  Let's get off of VWs because they've had enough hardship over the last couple of years. 



STEVE:  So it's interesting, a hacker does the right thing with a major vulnerability, a horrible remote code execution vulnerability, discovers it, notifies responsibly the party who's in charge of the software, the owner of the intellectual property, in this case Oracle, and says, oh, you've got a problem.  This was last November.  After a few months a patch is released to fix this problem.  Then it turns out that it was misfixed such that it's possible to work around it.



So that's where we are at this moment in time.  We discussed this problem maybe a month ago.  There was a highly critical flaw in Oracle's WebLogic servers which was responsibly disclosed.  We learned that it was a Java deserialization flaw.  And we had never discussed on the podcast serialization and deserialization in Java.  Essentially, as we discussed at the time, Java maintains structured objects that is the way Java looks at things as they're highly structured.  They have an internal structure.  So you just can't, like, send it over the wire.



Serialization is the process of essentially turning this structured object into a byte stream which you can then store or shoot across a network.  Deserialization is the other end, taking the serialized descriptor of a complex structured object and sort of rebuilding it, expanding it.  Well, a deserializer is necessarily an interpreter.  And one of our constant refrains now on the podcast is it is so difficult to get interpreters to be security sound, to have a complex interpreter that doesn't have some exploitable edge cases.  And that is exactly what was found in Java's implementation in the code for their WebLogic servers, which allows anybody over the Internet on TCP port 7001 to now remotely execute their own code on Oracle's WebLogic servers.  This is now loose and in the wild.



Somebody tweeting as @pyn3rd on Twitter, who claims to be part of the Alibaba security team, has found a way to remotely bypass the security patch which was recently deployed and exploit the WebLogic vulnerability once again.  So with any luck, the only thing attackers will care to do is mine some coin.  WebLogic server admins should consider themselves lucky that most people these days won't care what's going on within your enterprise or on your server.  They just want to make some money.



FR. ROBERT:  They just want the processor cycles.



STEVE:  Exactly.  So in that sense there's sort of a - Oracle gets a break because probably all that's going to happen is that the processor will get bogged down, somebody will come along and go, hey, why is this thing generating extra heat or consuming extra power or our processor monitor is showing that something is sucking up cycles.  And then they'll reset it and reboot it.  Maybe, I imagine, Oracle must be scrambling to fix their fix, and we'll be pushing out another one, this time without the benefit of knowing in advance what the problem is.  And as will always be the case, some of these things will never get patched.



FR. ROBERT:  Steve, help me to wrap my head around this exploit.  So I understand in Java you serialize to create a package.  And then that package goes to a deserializer, and that's where you get your code.



STEVE:  Right.



FR. ROBERT:  So is this exploit about messing with the package before it gets to the deserializer?



STEVE:  Yes.



FR. ROBERT:  Okay.  So I can take any package, make a couple of changes to it.  Now I'm taking advantage of the way the deserializer works to inject my own commands into it.



STEVE:  Right.



FR. ROBERT:  Okay.



STEVE:  Right.  The deserializer was written with the assumption that a valid serialized stream would be coming to it, and it wasn't protecting itself from malicious serialized streams.  And being an interpreter, someone cleverly discovered that they could give the deserializer some work to do that would allow them to cause it to execute their code.



FR. ROBERT:  Again, this is going back to the DVD conversation and the CAN bus conversation.  Some engineer of Java in the early days said, "Why would anyone want to do something bad with the deserializer?  They wouldn't be able to do their work."



STEVE:  Right.  It's such a good thing.  It's a fabulous feature.  It's a benefit.



FR. ROBERT:  And these packages are not signed in any way.  I mean, Java just assumes that, if it's receives a packet, it's valid.  



STEVE:  Yup, exactly.



FR. ROBERT:  All right.  Good stuff.



STEVE:  Exactly.  Okay.  And what's interesting is that our next story is exactly the same problem in an entirely different context.  Somebody on GitHub has hosted an NTFS file system image which will crash Windows.  Period.  It's a little 10MB NT file system image which he handcrafted to cause Windows to barf and collapse horribly when it attempts to mount that file system.  He reported it to Microsoft and said, hey, look at this.  I've got an NTFS file system.  If you stick it on a thumb drive, and then you plug it into a USB port on somebody's Windows machine, it crashes the machine.  And they said, "Yeah."



FR. ROBERT:  Wow, you do seem to have that, huh.



STEVE:  And I'm paraphrasing here.  They said, well, yes, having Windows barfing and collapsing horribly is always a bit embarrassing.  But the attack, such as it is, as presented, requires physical access to the machine and for connected drives to be automatically enumerated and mounted.  Now, for his part, the discoverer of this problem was mostly concerned that even  machines in a locked state, even a locked Windows system, it turns out, will enumerate and mount a USB drive that you present it.  So you can do this to a machine even that is not interactively online.  Now, we know that the first stage in the development of any attack is a crash.



FR. ROBERT:  A crash, yeah.  Unexpected condition.



STEVE:  And so once again we have the NTFS file system interpreter which is looking at the file system structure and assuming it's benign and is - I'm always put in mind of Khan saying "We're all one big happy family" when Kirk doesn't raise his shields.  It's like, we're all NTFS.  Who would want to hurt us?



FR. ROBERT:  By the way, he is referencing "Star Trek II:  The Wrath of Khan."  Don't forget that every starship can be taken over by a simple prefix code.



STEVE:  That's right.



FR. ROBERT:  Oh, I'm sorry, I derailed us.  Continue.



STEVE:  Anyway, so once again an interpreter.  He did comment, because I caught myself up with his most recent GitHub postings, that the most recent update he got to Windows did not seem to be doing this any longer.  So even though Microsoft said we don't care, maybe they fixed it anyway, which would be nice because it would be better if an NTFS file system image were not able to crash Windows because, again, as we have often said, what follows is a takeover.  And if you were able to plug a thumb drive into someone's machine, when they were away and the machine was locked, and remotely run your own code, rather than just having it crash, having it actually execute something that you put in there, that would not be good.



FR. ROBERT:  We should mention that there have been USB device exploits for a while, but that it typically is code on the USB drive that you can somehow make run or autorun.



STEVE:  Right.



FR. ROBERT:  This is different.  This is different.  This is not something on the drive.  This is the structure of the file system itself.  And as you mentioned, this will run regardless of what state your machine is.  It always tries to mount a connected storage device.



STEVE:  It's, exactly, mounting.  It's not auto-executing something on the drive.  It's just saying, oh, let's see what you've got.  And that's all it takes.



FR. ROBERT:  Right, right.  Well, let's move from that to another Windows fun thing, NTLM, which actually has been in the news this past year.



STEVE:  Yes, yes.



FR. ROBERT:  Along with the WannaCry scare.  It's one of these protocols that probably should go away at some point.



STEVE:  It keeps on giving.



FR. ROBERT:  Keeps on giving.  What's going on?



STEVE:  Okay.  So get a load of this.  And our listeners will appreciate this.  Many, many moons ago, in fact it's probably on those OS/2 disks that you have, Padre, IBM and Microsoft got together and created LAN Manager, or LAN Man, as it's often called.  They designed what is by today's standards such a horrifically weak authentication mechanism, the so-called LM hash, the LAN Man hash, that it's hard to believe anyone ever took it seriously.



Okay.  So get this.  In terms of authenticating requests over LAN Manager, a hash is used.  The hash is a hash of the password which is used to authenticate.  Passwords are, A, not case sensitive.  They're always converted to uppercase before hashing.  Password characters are limited to a subset of the ASCII character set.  So it's not only that you lose the lowercase alphabetics, you lose a bunch of other stuff, too, that are not valid in passwords.  Passwords are limited to - is everybody sitting down? - a maximum of 14 characters.  Now it's like, okay, well, 14 characters, not great.  But get a load of this.  The 14-character password is broken into two seven-character substrings, each hashed independently.



FR. ROBERT:  Okay, come on.



STEVE:  You can't make this up.



FR. ROBERT:  Okay.  So I've got two seven-character hashes that I know are all going to be uppercase.



STEVE:  With a subset of ASCII.  What could possibly go wrong?



FR. ROBERT:  What could possibly go wrong?



STEVE:  And you really can't make this up.  If the password is seven characters or less, meaning that there was nothing in the second set of seven for it to crack into two pieces, the second half's null hash is always AAD3B435B51404EE.  I stated that because that's only 16 hex characters, which means it's eight bytes, which means it's 64-bit hash.  So you have a 64-bit hash of a maximum of seven characters, which we could cut through like butter in this day and age.  And of course we do, which is why this has been deprecated historically, but why it continues to cause a phenomenal amount of trouble because it was once ubiquitous.  It was everywhere.  And for the sake of backward compatibility, it's still around because you never know when an NTLM LAN Man hash might still be used by some server somewhere for authentication.



So the guys at Check Point - so with that bit of background, the guys at Check Point wrote in their research:  "A few days after it was reported that malicious actors could exploit a vulnerability" - and this was just a few weeks ago - "in Microsoft Outlook using OLE to steal a Windows user's NT LAN Manager hashes, the Check Point research team can also reveal that NTLM LAN Manager hash leaks can also be achieved via PDF files with no user interaction or additional exploitation needed."



According to Check Point researchers:  "Rather than exploiting the vulnerability in Microsoft Word files or Outlook's handling of RTF files" - remember, because we talked about this at the time, there was quite a chain of action required with Outlook, then RTF, then an embedded OLE object in the RTF file, blah blah blah.  So, I mean, you really had to work at it.  Now, no.  Much less work necessary.  It turns out that PDFs have a feature that allows them to embed remote content.  And if the remote content is a document located out on the Internet, the PDF reader will make a request for the document, including the user's LAN Manager authorization hash, in order to have the request approved, thus exfiltrating, exporting essentially, the LAN Man hash to anywhere out on the Internet that the PDF has been configured for.



Check Point believes from their testing that any and all Windows PDF viewers can be induced to exfiltrate the user's NT LAN Man credentials in this fashion.  And being responsible, they contacted Adobe, you know, the source of Acrobat and PDF readers.  Adobe, always quick off the starting line with security, and of course the home of Adobe Flash, so we know how focused they are on security, responded to... 



FR. ROBERT:  I'm sorry, I can't hold that in.  I was really, really trying to be serious there for a second.



STEVE:  ...responded to Check Point's private disclosure, saying:  "Thank you for checking in on this case.  Microsoft issued an optional" - that is, user go get it - "security enhancement late last year that provides customers" - who do decide to go get it, but who does - "with the ability to disable NTLM SSO [Single Sign-On] authentication as a method for public resources."  In other words, it's on by default; but, oh, yes, maybe that was a bad idea.  So we'll issue an optional security enhancement because we wouldn't want to break anything.  Oh, no.  Anyway, so they said:  "With this mitigation available to customers, we" - Adobe, the security-conscious company - "are not planning to make changes in Acrobat."



FR. ROBERT:  Oh, come on.  When you know the product is broken, it's time to say, you know what, maybe we don't want to be backwards-compatible anymore.  It's time.  It's time.  



STEVE:  I know.  I know.  



FR. ROBERT:  But on the upside, what I've learned today, Steve, is that, for example, if I have a 16-bit password, two eight-bit passwords are just as good.  Right?  I mean, if my math is right here.  This is a learning show, Steve.  We just learned something.



STEVE:  Yes.  We should make it clear to people - actually, I'm put in mind of the WPS flaw.  The WPS flaw is the push the button on your router in order to automatically pair your new device with your router's WiFi without needing to worry about a password.  It was an eight-digit code.  Ooh, secure, okay?  No.  Because the protocol was broken so that you could separately test the first four digits and see if you got those right, and then test the second four.  Oh, except that the last digit of the second four was a check digit, so you always knew what that should be.  So you only really had to worry about the second three of the second half,  or the first three digits of the second half.  Anyway, the point you were making clear, Padre, is that, if you have this 14-digit LAN Man password, you're able to hash the front seven separately from the back seven, assuming that it was 14 characters, and the hash was only 64 bits anyway.  So not difficult.



FR. ROBERT:  Yeah.  For the math people out there, if you have a 16-bit value, that's 65,536 possible values, versus two eight-bit, which are 256 each, so you'd have 512 total values.  So, yeah.



STEVE:  And you also have that extra kick of encouragement when you crack the first half.  It's like, ooh.



FR. ROBERT:  I got it.



STEVE:  I'm halfway there.  Whereas, if you were having to do the whole thing it's like, well, this could take forever, and I'm not getting any encouragement.



FR. ROBERT:  Can you imagine if you had a keypad to enter your house?  And the keypad, if you got the first two digits right, it said, hey, you got the first two, only two more?



STEVE:  Getting closer, getting warmer.  Stick with those first two, and now you only have to worry about the next two.



FR. ROBERT:  Exactly.  Okay.  The funny thing is I've actually known some of the people who have developed stuff like this, and they normally have a reason why they did it.  Constraint on resources or backward - actually, the biggest one is backwards compatibility.  But you kind of want to slap them and say, "Don't do that again.  Bad, bad.  Stop."



STEVE:  Yeah.



FR. ROBERT:  All right.  Before we go on to the good news - because we have to talk about how our most cherished devices might be spying on us.  Can we do that next?



STEVE:  Yes, let's do that next.



FR. ROBERT:  Let's do that next.  We'll get back to our future lords of IoT.  Now, Steve, the Echo was a breakout product not too long ago.  It was the center of Amazon's new voice assistant market.  And even at that time there was a worry that the devices are listening to us.  And now we actually know, well, of course they are.



STEVE:  Yeah.  So a group named Checkmarx has successfully demonstrated a persistent listening skill for the Echo.  As we know, the Echo is extensible.  Amazon produces and publishes an API which developers can use in order to create new skills which users can download and add to their Echo in order to do things.  What they did was created a proof-of-concept which to me seems sort of like, wait a minute, you mean it's vulnerable to that?



They created a calculator app which appears to be a voice-driven calculator.  Okay, I guess that's a skill.  But when it launches, it spins off a secondary background non-terminating task which causes any such skill-enhanced Amazon voice assist device, because there are a bunch of different ones, you know, different sizes and shapes now, to indefinitely record all surrounding audio, eavesdropping on users' conversations while also streaming the audio to a third-party server somewhere.



So, I mean, it's like everyone's worst fear about what could happen if you have an Internet-connected microphone, like deliberately in your midst.  Presumably located somewhere, like it's not in the closet, it's out where it can hear everybody because that's what it's supposed to be doing.  They reported the issue to Amazon, and the company has addressed the problem, though not very robustly, in my opinion.  In the reporting of this, they said they now regularly scan for malicious skills that, quote, "silently prompts or listens for unusual lengths of time," then kicks them out of their official store.



And I don't know.  To me, for something as important as the security, this feels like an immature and not very well thought out AP.  No skills, that is, these add-ons, should have any such access, direct access to the system's microphone.  Rather, any skill should register its microphone usage and access policy, whatever that is, and its needs as part of an unhackable signed policy packet, which Amazon approves, then the developers incorporate into their skill, and then it can't be hacked afterwards.  And then the device's API should grant the skill's requests, which are filtered through and enforced by the previously declared and approved policy.



I mean, this is how you solve that problem.  These are not hard things to do.  And I'm just sort of - I'm surprised that in this day and age an obviously security-conscious company could produce an API that allows a skill to establish a background task which leaves the microphone running and streaming data to the Internet.  It's like, how do you do that?



FR. ROBERT:  Right, right.  Yeah, I mean, my understanding was the voice activation part was tied directly to the operating system, and it did submit information to Amazon.  But you're right, why would a skill ever need that?  Because a skill is triggered by a voice prompt.  It's not as if it's running, waiting for the voice prompt.



STEVE:  Right.  Right.  It's just like, okay, let's hope they get this right.  And probably, I mean, what they would have to do now would be to reengineer the API with additional security, maybe taking the lessons that they've learned, because I'm sure they've learned some lessons since its release, and just do a v2 API and give developers some time to migrate their skill to the v2.  And then just shut down access to the v1 API that is obviously not secure.  I mean, the idea of, ooh, we didn't realize that skills could listen persistently and stream audio over the Internet.  So we're going to periodically scan to see if that's happening.  What?  No.



FR. ROBERT:  We can't keep people out of your home, but we'll drive by every once in a while to see that the lights are on.



STEVE:  That's right.  That's perfect.  See if everyone's still breathing happily.  My lord.



FR. ROBERT:  Now, I got an Echo early on.  And I hook it up every once in a while because there are some projects that I love doing with the Echo.  However, one of the very first things that I did when I got one was I set up an AP running through a Tap, connected the Echo to that AP.  And what concerned me was all the outbound traffic that I saw when I wasn't doing anything.



STEVE:  How noisy it is, yes.



FR. ROBERT:  And that really, really concerned me.  Now, it's gotten better, so obviously they've improved their software.  But I'm betting, if I were to slap the Echo back on this Tap and run one of those skills, I'd be very enthusiastic about the data that I'd be receiving.



STEVE:  Okay.  So hacking, well, I don't want to call them smartcards.



FR. ROBERT:  Just RFID cards.  



STEVE:  Yeah, RFID cards.  The guys at F-Secure, as I mentioned at the top of the show, tell the story that one of them had a laptop stolen from a hotel about a decade ago.  And in their own FAQ about this they asked themselves, why did the research take over a decade to complete?  And they said:  "Figuring out the complexities of how the lock system, software, and keys worked was very complicated.  Building and breaking an electronic access system is very difficult because there are many facets to get right.



"Assa Abloy [is the name of the company, A-S-S-A, second word A-B-L-O-Y] is a highly reputed lock manufacturer" - actually they're the largest in the world - "and aside from the seemingly innocuous security oversights in the software" - okay, so F-Secure is characterizing these as "seemingly innocuous security oversights" - they wrote "...their products are well designed.  These security oversights were not gaping obvious holes."  They wrote, "It took a thorough understanding of the design of the whole system to be able to identify small flaws in the system.  The researchers then creatively combined these flaws to produce the attack."



So, okay.  So what happened?  An RFID card is something which is externally powered.  Basically it's a...



FR. ROBERT:  It's a transceiver.



STEVE:  ...a transponder.



FR. ROBERT:  It's a transceiver, yeah.



STEVE:  Yeah, exactly.  So you ping it with energy, and it gives you back a code.  And we know that these particular keys were the "hold them up against the reader" style.  They weren't the mag stripe "insert and pull out quickly or swipe."  And apparently they are in millions of hotels worldwide.  They're called the VingCard, and the software behind it is Vision.  And so they simulated the attack with an ordinary electronic key to target the facility.  So basically they got one of these keys and built their own RFID reader and pinged it.



And again, they didn't articulate the nature of the attack over time.  My guess is that by getting a large number of cards over time - that is, not just one card.  They did say that the cards could be retired.  They could be obsolete.  They could have been, you know, how many people end up walking away and forgetting to turn their little card in.  I mean, they're meant to be cheap.  Sometimes they'll say "Drop into any mailbox, and it'll be returned to the hotel."  So they're not free.  But if you forget to return it, no biggie.  Well, what you end up with is a key that is some sort of something, basically probably a static serial number which that card has.  I'm guessing that it is relatively good encryption.  That is, that if you looked at one, you get a large bit stream out.



FR. ROBERT:  A blob, yup.



STEVE:  Yes.  You look at another, and you get a very different-looking bit stream out.  It's not as simple as, oh, look, these 64 bits are not changing, and these 64 bits are.  No.  It's like, oh, all 128 bits are different on this one, than this one, than this one, than this one.  But probably, if they took enough samples of the static bit streams, they were able to reverse engineer the algorithm which underlay probably the sequential generation of these keys, where in fact there was like a facility bit stream that did not change which all the locks in the entire hotel knew about, and then there was an instance bit stream which somewhere was an incremented counter.  And those two were then mixed together and encrypted to create something which was not at all obvious to a casual observer.



But if you took enough of them together, and if you were F-Secure, and you had a burr in your bonnet over having had a laptop stolen a decade ago, and you just liked a good challenge, you were probably collecting these keys from a given facility over time.



FR. ROBERT:  I've got probably about 100, 150 of them.  Because they all have the logo of the hotel.  That's kind of how I can go back and remember the hotels I've stayed at.



STEVE:  Yup.



FR. ROBERT:  The thing about this that makes it more interesting, from the enterprise side now, most of these locks are not networked.  It's not as if they're connected to a master system.  They are standalone.  And the way that they know that it's time to have a new authentication is because the first time you get your card, added to that is the command to reset.  So it's reset to this code.  Which means, if you want to update, you have to go lock to lock to lock to lock.  There is no way to do a central update.  Now, the more expensive ones do have that, but they are very rarely used at hotels.



STEVE:  Right, right.  And one of the things we know is that - so imagine that our model is a - it probably isn't 64-bit facility ID because that's too many bits for the facility.  So say it's a 32-bit facility ID.  That's how many IPs we have in the world; right?  That's 4.3 billion.  That's enough hotels.  So a 32-bit facility ID.  And say maybe a 96-bit serial number.  Now, imagine that that is synchronized by the network down in the hotel's management and is a monotonically increasing value.  So there's a counter which increments every time a new card is programmed.  And the resulting 32 plus 96, so 128 bits, is encrypted - and that's not hard, that's AES, any little microcontroller can do that now - with a secret key.  So the facility number, the 96-bit serial number that'll last forever, is encrypted with a secret to produce 128 bits which are, on their face, absolutely random.  So that gets programmed into the card.



Now, every door lock, as you said, Padre, they're not networked.  They're not talking to each other.  But the door lock has the same little microcontroller chip and the secret key.  And so when it pings the card, it runs the decryption, and it gets the facility number and the sequence number.  So first of all, it checks to make sure, again, assuming this is how this works - I'm making this up because there's no technical information provided, but this is probably something like this.  It verifies the proper facility.  And when that's done, it then looks at the serial number.  And its rule is never accept a lower serial number than the most recent one I've received.  And don't accept one that is ridiculously higher than the most recent one I've received.



And so what that does is, that allows it to notch itself forward such that, as soon as a new guest uses the key they've just been issued, it automatically renders the previous guest's key inoperable, which is exactly what you want.  You don't want any previous guest to be able to reuse their key.  So as soon as that lock sniffs a new card, it goes, oh, look, that serial number has now been moved up by, what, maybe 400, because you could have had 400 keys issued since that room's previous guest had their key issued.  But it's not going to be 20 billion.  That's a ridiculous high number.



So the point is there's a limit to how far in the future it will go, but it will never go into the past.  Which means that the instant it sees a new key, it obsoletes all previous keys and now honors that one until, again, it sees one that is reasonably close in the future.  And so that allows a non-network set of locks to be secure in the hotel deployment model.  And I would imagine it's something like that.  Who knows what the actual bit lengths were.  But these guys probably got a bunch of them, and basically they reverse-engineered the secret key, that is, that key that encrypts the bit stream from the RFID tag.  And then, once they had that, they could make their own keys.



FR. ROBERT:  Right.  I think what we've learned here is that you want to keep everything in the hotel safe because I've learned from Defcon that these hotel safes are absolutely secure.  Here he's putting a code of 1234567 - uh-oh, wait.  Oh, huh.  Yeah, by the way, the hotel safe inside your room, they're junk.  They're garbage.  They will keep out no one.  Just FYI.



STEVE:  Wow.  Yeah, so if you have anything valuable, take it downstairs and make it...



FR. ROBERT:  Take it with you.



STEVE:  Yeah, take it with you or make it the problem of the hotel management and say, look, you need to put this in your hotel safe.  I'll come back and get it.  Otherwise, no.



FR. ROBERT:  It's funny because those hotel - I always assume everyone can see everything that's in my room in a hotel.  I've spent way too much in hotels over the last 20 years.



STEVE:  You have to assume that.  Yes, I mean, that is the takeaway for our listeners is you have to assume that we don't have security.



FR. ROBERT:  I've put drop cams in my hotel rooms before.  And at minimum room service is coming, or the cleaning lady is coming through.  So again, just be smart.  We're a smart audience, so trust no one, including trust no one who might come into your hotel room.



STEVE:  I have a nice story, email that I received on April 27th from a - looks like he would pronounce his name Lachlan Gabb.  He's in Sydney, Australia.  And of course the subject is "Another SpinRite Success Story."  He said:  "Hi, Steve."  This is a nice one.  He said:  "Hi, Steve.  Just wanted to share another SpinRite success story.  I'm a security analyst living in Sydney, Australia, and also the go-to guy for computer problems within my family and close neighbors.  A couple of weeks ago, one of my neighbors called me to take a look at their computer that had been running very slowly recently.  Before I had even arrived to take a look at the system, it had blue-screened and was now refusing to boot at all, stating that no operating system could be found.  I asked, but predictably there was no backup, and the computer contained important photos of their grandchildren and documents dating back over five years.



"I removed the drive and connected it to my computer to see if I could recover any data; however, Windows would not recognize the drive at all.  Being an avid Security Now! listener I immediately thought of SpinRite, which I had been looking for an excuse to try out for a couple of months anyway.  I went over and purchased a copy" - thank you very much - "and let it run on the drive overnight.  I think we know how the rest of this goes.  In the morning, not only could I copy all the data from the drive, but it would even boot again.



"I promptly copied the data to a fresh drive and reinstalled it in the computer, along with a much-needed lecture on the importance of backups."  He says, "I was given immense gratitude and even a container of freshly baked cookies for saving the system."  He says, "I wanted to pass on the gratitude to you, but I ate the cookies myself.  Thanks for the wonderful product, and I'm looking forward to the next episode of Security Now!. Kind regards from Down Under, Lachlan."  And, wow, thanks for the great story.



FR. ROBERT:  Use SpinRite, get cookies.  I mean, I don't think we could be any more clear on that.



STEVE:  Pays for itself in cookies.



FR. ROBERT:  And, you know, it's just one of these things that's automatically in my toolkit.  I may not need it that often, but when I need it, I have no other tool like it.  That's my little pitch.



STEVE:  So a bit of errata before we talk about Microsoft.  Last week you and I talked about Azure Sphere.  And I misstated the openness of the system.  And it's funny because I read it, but I didn't listen to what I was reading, apparently, because, well, because it is a little confusing.  On their slide, which they present as a nice-looking PDF graphic that we linked to in the show notes, it says:  "Azure Sphere gives you choice."  And I thought, whoa, good.  You can connect data from the cloud, proprietary or public, or even to your on-prem infrastructure, to the Azure Sphere security service.  Okay, well, it turns out upon closer inspection they have the little arrows pointing around in different directions on this graphic.  And you're able to add data and telemetry is what can be connected to non-Microsoft services.



FR. ROBERT:  Ah, okay, that makes sense.



STEVE:  Yes, it does.  But the use of Microsoft's, quote, "Azure Sphere Security Service," which is inseparable from the rest, appears to be also inseparable as a component of their system.



FR. ROBERT:  See, that makes more sense because I was wondering about that last week.  I was like, wait a minute.  But if you're not using Azure, how do you get the benefits of the security package?  Because that's baked into Azure.  That's not a hardware thing, or a software component you can download.



STEVE:  And you could argue, too, and I do, and I think it makes sense, I wrote here in this errata, "and assuming that they price it all affordably, this offloads ALL [in caps] of the responsibility of maintaining the security and integrity of these IoT devices from hardware manufacturers who just want to crank out hardware without investing in huge aftermarket maintenance and security."



So we would all feel more comfortable if our light bulbs that were Azure Sphere-secured had a connection that was authenticated from a trusted environment in the chip, all the way to Microsoft's secured Azure Sphere security service that was monitoring it and maintaining it and updating it, not to random Chinese company whose employee set it up and quit a year ago and is no longer doing any actual security management.  So, yes, I wanted to correct the record.  A friend of mine said, "Steve, I think you read that wrong."  And so thank you, John, for the correction.  I appreciate that.



FR. ROBERT:  We actually talked a little bit about this on This Week in Enterprise Tech a while back. 



STEVE:  Ah, cool.



FR. ROBERT:  Microsoft has dropped, dramatically dropped the opening tier, so that the opening tier of their IoT services are now way less expensive than they used to be.  And essentially it's a very low monthly fee per device, and then you get a certain number of transactions. 



STEVE:  Ah, okay.



FR. ROBERT:  Right.  Now, the nice thing about this is they've designed it right.  So the big problem with IoT is not being able to design the devices to be robust enough to do what they need to do and also provide security.  So instead the devices, they don't accept any queries.  They only connect back to the Azure Services.  That's what you query to get data from the IoT devices.  And that's why the security is then taken care of by Microsoft because you don't have to secure the IoT devices.  They're not accessible to anything.



STEVE:  Well, and you know, that's nice, too, because I'm always nervous about the Internet needing to be able to connect into my network.  I don't want UPnP opening holes through my firewall.



FR. ROBERT:  Ever.



STEVE:  Ever, exactly.  So it's way better if all of the junk, connected things, if they're making outbound NAT traversal connections to a central service, and its security is strong because it does have incoming connectivity back through the NAT as a consequence of the devices having initiated the outbound hole punching through the NAT, and so then you query the Azure service to find out what's going on.  So, I mean, this all does feel a lot better.



FR. ROBERT:  It is a lot better.



STEVE:  And I would love it to succeed.



FR. ROBERT:  Now, though, on the contrary, if that main Azure service gets owned, then they can own all the IoT devices connected to it.  But that is so much less likely than someone owning millions of IoT devices, which has already happened.



STEVE:  Has already happened, yes.



FR. ROBERT:  All right.



STEVE:  Okay.  So I wanted to wrap up this week by talking about an initiative, I think is probably the way to describe it, and if anyone's interested I have a link to the PDF in the show notes.  Microsoft calls it TCPS, Trusted Cyber-Physical Systems.  And so this is their moniker for - and cyber-physical is hyphenated.  The idea being sort of a catchall for things, like the problems we have now with water treatment plants hooked to the Internet because somebody thought that was a good idea.  And, well, it does allow remote monitoring and blah blah blah.  But, yes, unfortunately it requires that there be no security mistakes.  And exactly as we were just talking about, even if this were to occur with Azure Sphere, it might be that light bulbs, consumer light bulbs have more security than nuclear power plants.  Which could be a problem.



FR. ROBERT:  Makes total sense.  I mean, my light bulbs are essential.  Nuclear power plants are really optional.



STEVE:  Yeah.  So essentially, and I'm glad Microsoft has put this out there because it sort of feels bureaucratic, and it's not a spec.  Whereas Azure Sphere is actual bits and bolts and a developer board that will be available in a couple of months, and an open design for a multiprocessor, multicore-based chip which could actually be in light bulbs.  This is not there yet.  This is sort of the so-called TCPS.  This Trusted Cyber-Physical Systems is a forward-looking, I think it was like a 16-page, I think it was 18 pages, actually, 18-page document describing, like, noting the problems that we've had before, securing the Internet-connected valves in important facilities and keeping them out of hackers' hands.



And what this amounts to is them extending the model we already have of security, all the way to the I/O pins on microcontrollers, which is really what they're proposing.  We already talked about this notion of a Trusted Execution Environment, that's an acronym, TEE, that they use in this document.  A Trusted Execution Environment meaning that, from the moment it is booted, everything it does is bootstrapped, signed, and signature verified before it runs.  And there is then no way for a hacker to intercept the process, to get their own code in.  And nothing that runs code will run code which is unsigned.



In fact, I think I've mentioned it on this podcast, a few months ago the work on my SQRL client for Windows got to the point where it was time to think about an installer.  And it doesn't need one.  None of my, as we know, you just run my stuff.  But it's meant to be used by regular users, and they're going to click on a download link, and it's going to go into some download directory somewhere which isn't a permanent resting place, or shouldn't be, for anything.  So I thought, okay, you know, I have to hold their hand and kind of install.  And it may well be that a problem is found after release.  That's generally the case, certainly for a security thing.  So I need an upgrade facility.



So there's still only one EXE.  It's still, and we're at about release point, by the way, where it's 265K because, yes, it's all assembler.  And so you just run it.  And when you run it, it notices that it hasn't been installed, so it installs itself, meaning that it puts itself where programs are supposed to be and then runs from there from now on.  The point of this is that what about updates?  What I do is what you have to do if you're truly going to be concerned about security.  We've run across so-called "supply chain flaws" in the past where, for example, the CCleaner servers were compromised a few months ago, and lots of people downloaded a malicious version of CCleaner.  Well, that can't happen to GRC or SQRL because, if the existing SQRL client learns that an update is available, it downloads it into a temporary location as a non-executable named file, and then it itself verifies the Authenticode signature and that it was not just correctly signed, but that it was signed by me.  And only if all of that passes does it then move and rename it as an executable and then arrange to have itself replaced with that.



So again, it's maybe overkill, but I'm sure we'll be glad that nobody, even if GRC's servers were compromised with a bogus version, no SQRL client out there would accept that.  So this is that kind of Trusted Execution Environment thinking that we need.  And so essentially what Microsoft has done is they've laid out an architecture and also a way of getting from what they call "brown field" to "green field," brown field meaning just the way things are today where you've got compromisable devices which are not protecting their communications.  They're not protecting their code, their communications, or the execution microcontroller that is overseeing the state of the valve being open or closed.



And Microsoft lays out the process of - and again, they say right at the top, we're not inventing anything here.  We're just saying this is how this has to be done.  So let's start thinking in terms of moving the concept of a trusted execution environment all the way down to the field, the so-called now being green field, where at every stage of the process we are only running code which we know is safe to run.  And even the microcontroller is protecting itself if it gets updated from a malicious update, and only signed commands where the commands to open or close a critical valve are properly signed through a chain of trust.



And again, they also fully developed this notion of a mature chain of trust so that you have a trust anchor root somewhere.  And very much like we have a chain of certificates, there is a verifiable chain where any action chains back to a root that can be trusted.  And so long as you protect that, you are thwarting bad guys and knowing, essentially verifiably knowing that the security of your industrial control system is solid, not just hoping that it is so.



FR. ROBERT:  Now, you know, Steve, I don't think this is overkill at all.  I think this is exactly what is required because, unfortunately, in the era of SCADA, Supervisory Control and Data Acquisition devices, security was tacked on a generation or two after the devices had been created.  It just didn't work.  And what we'd seen in some of the most recent industrial IoT incidents is not only are they trying to gain access to the equipment, but they also try to erase the logs.  That's one of the biggest things right now because they realize it may take them a long time to get the access correct. And in the meantime they want to make sure no one catches onto them so they delete the logs.  This handles all of that.



STEVE:  And it's got blockchain, woohoo.



FR. ROBERT:  Yeah.  So, I mean, even if you're not able to keep people from probing your network, at least you'll know that they're doing it.  And that's actually a huge jump in security.  I want to see stuff like this on everything, in this sense.  There's been a really big push to secure, oh, well, that pump is connected to the water cooling system to a nuclear reactor.  This has to be on every valve.  We've also seen trials where people will gain access to a device far downstream or far upstream, but they'll use that to the effect of damaging one of the main devices.  That's already happened in Illinois.  So this is a very, very good step.  And the way that you describe it, it looks like this is very thorough.  This is soup to nuts.  This is ground-up security.



STEVE:  Yes.



FR. ROBERT:  And, yeah, I mean, I'd like to have this on my IoT devices.



STEVE:  Well, and we have now all of the crypto tools we need.  We now have little chips with sufficient processing power.  Once upon a time we didn't have that.  The microcontroller was already straining just to do its job.  Now it's like how many ARM cores does it have?  It's ridiculous how much power we have.  So it's wrong not to use the power there which we can to wrap crypto around everything, which is now just not a problem.  We've got bandwidth.  We've got high-speed connections.  Whereas once upon a time we were sending a DC signal - high means open, low means closed.



FR. ROBERT:  That's right.



STEVE:  Now we can send megabits.  We can send data.  So let's do that.  I mean, let's wrap this around, wrap it in a cone of silence, in a cone of crypto, so that bad guys are just locked out from the beginning.  And I'm glad you pointed out the secure logging because that is a key part of this that I forgot to talk about, and it's important.



FR. ROBERT:  It really is.  Well, you know what, I think this might be one of the first times since I've started co-hosting with you that we can end the show on a positive note.  That was actually a good story.  That's a hopeful story, Steve.



STEVE:  Let's just hope it spreads.



FR. ROBERT:  Indeed, indeed.  I want to thank you very much for allowing me to do the show with you.  Leo is back next week.  And unless he goes on an unscheduled vacation or needs to step away for a while, this may be the last Security Now! I host with you for quite a while.  It's been an absolute pleasure, my friend.



STEVE:  It has indeed, and I really appreciate your standing in for Leo and being here with me for it.  It's been great, always. 



FR. ROBERT:  Indeed.  Folks, that does it for this episode of Security Now!.  Now, don't forget that we're live every Tuesday on the TWiT TV network at 13:30 Pacific time.  Steve will always be here to inject you with a sense of healthy paranoia.  Yes, folks, it's healthy because you do need to know the threats out there, and you do need to trust no one, and you do need to listen to our own personal security guru, Steve Gibson.



You can find all of our shows at the show page at TWiT.tv/sn, all the way back to Episode 1, including the show notes and a place to subscribe if you want to make sure that you get all the security goodness in your device of choice every week.  We've got an audio version, a video version, and a high-definition video version, all for your pleasure.  You can also find Security Now! anywhere that fine podcasts are aggregated, as well as on the GRC website, where you'll be able to download high-quality audio versions.  That's also where you're going to find Steve's wonderful products, like ShieldsUP!, SpinRite, and coming soon, SQRL.



Steve, did I miss anything?  Do you want to throw any more plugs in here before we sign off?



STEVE:  No, you've got it good.  Everybody knows that Elaine does transcripts for us every week, so we've got transcripts of the podcast, as well.  And it's been great, Padre.  Thank you so much for standing in for Leo.



FR. ROBERT:  Thank you.  And in your honor, I will be going back to Windows 3.1.  I'm Father Robert Ballecer, the Digital Jesuit, just reminding you that, if you want to keep your data well into the future, you need to think about Security Now!.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#662

DATE:		May 8, 2018

TITLE:		Spectre - NextGen

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-662.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we begin by updating the status of several ongoing security stories:  Russia vs. Telegram, Drupalgeddon2, and the return of Rowhammer.  We will conclude with MAJOR new bad news related to Spectre.  We also have a new cryptomalware, Twitter's in-the-clear passwords mistake, new Android "P" security features, a crazy service for GDPR compliance, Firefox's sponsored content plan, another million routers being attacked, more deliberately compromised JavaScript found in the wild, a new Microsoft Meltdown mistake, a comprehensive Windows command reference, and signs of future encrypted Twitter DMs.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and I'm back.  Hello.  Thanks to Robert Ballecer for filling in for the last couple of weeks.  I came back just in the nick of time.  Turns out Spectre's back, baby.  Steve has the details next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 662, recorded Tuesday, May 8th, 2018:  Spectre NextGen.



It's time for Security Now!, the show where we cover the latest in security and privacy with this guy right here, this cat, Mr. Steven Gibson of the GRC Corporation.  Hi, Steven.



STEVE GIBSON:  It's true, Leo.  Welcome back.  You were missed.



LEO:  Nice, nice.  I missed you guys.  And thanks to Robert, Father Robert Ballecer, the Digital Jesuit, for filling in.  



STEVE:  And I heard you discussing on This Week in Google, which just preceded this podcast as a consequence of the change of schedule, thanks to Google I/O, that you were mentioning Azure Sphere, which actually was the podcast topic week before last.  So indeed, as you would expect, we covered it in great detail.



LEO:  And do you concur that it is a good idea?



STEVE:  Yes.  Absolute win, for exactly the reasons that you reiterated on This Week in Google, the idea that...



LEO:  Cheap Chinese manufacturers can actually make something secure.



STEVE:  Yes.  And all they want to do is pump out the silicon and sell consumer products.  They don't want the downstream responsibility that comes from the fact that they're actually shipping Internet-connected computers. 



LEO:  I don't know all the details.  But is Sphere cheap enough that, you know, somebody makes a cheap camera would still use it?



STEVE:  That is the question, whether the manufacturer decides that it is so inexpensive that it's better for reputation damage, I mean, really...



LEO:  But, yeah, I'm sure there'll be a little icon they can put on the box or something like that.



STEVE:  Yeah.  And so the licensing of the chip, it's open licensed chip.  It's open architecture.  It's open source for the Linux.  So one thing that isn't is you do have to tie back to Microsoft Azure Cloud to get the monitoring and maintenance.  So we just don't know yet what the pricing is going to be.  But fingers are crossed.



LEO:  Good, good, good.



STEVE:  Speaking of fingers being crossed, today's - I can't believe this.  Well, yes, I can.  Spectre NextGen.



LEO:  What?



STEVE:  Yes.



LEO:  More?



STEVE:  Yes.  Eight new problems.  Intel is scrambling once again.  One of them is way worse than Spectre ever was.  And I wish I were more surprised.  But it really was the case that for decades we were blissfully, willingly not looking at the fact that an architecture that we were sharing was not secure in a shared environment.  And as we've said, any time there is the residual effects of running code on a processor that then switches to a different environment, it's possible to very cleverly, I mean, these are not easy things to do.  But you can figure out how the processor was changed by what it was just previously doing.  And that leaks information.



So anyway, that's the topic of the podcast.  We'll get into it in more detail at the end of the show.  But we've also got a bunch of other stuff.  We've got first sort of like continuing on monitoring some stories that we've been covering.  Russia vs. Telegram, there's new activity there.  Drupalgeddon2, new activity there.  Including, you're going to love this, a Google spreadsheet of the 350-plus websites that have been now compromised.  In fact, I made that the bit.ly shortcut link for the show.  Anyone who's interested, bit.ly/SN-662, and that will take you to the rather distressing Google spreadsheet.



We've got, believe it or not, the return, better than ever, of Rowhammer that affects Android smartphones.  We have major new bad news, as I said, related to Spectre.  Also new cryptomalware.  Twitter's mistake with their passwords, which has gotten a lot of coverage.  We have the new Android P security features.  And I don't think we heard, did we, what P stands for?



LEO:  No, no.  They won't announce that till the fall.



STEVE:  Oh, okay.



LEO:  Pumpkin Pie, I don't know.



STEVE:  Or pudding or...



LEO:  Pudding.  I think they should change it before then because, frankly, it's a little unseemly to keep talking about their P.



STEVE:  Okay, yes.  We also have a crazy service for GDPR compliance.  This is the EU's privacy and security compliance, which we've touched on before.  It comes into effect on May 25th, which is not far from now.  And what catches everybody's attention is the apparent penalties for anyone whose services are global, meaning that an EU citizen might be your customer, are breathtaking, in millions of dollars.  Anyway, we'll talk about that and this crazy service that has an answer for it.



Firefox has a plan for sponsored content.  We've got another million routers being attacked through a new means.  Another instance of JavaScript being deliberately subverted, that is, a JavaScript library.  And I want to talk a little bit about what this really means in a bigger context because it's very troublesome.  Believe it or not, a new Microsoft Meltdown mistake.  And the good news of a comprehensive 924, I think it is, page Windows Command Reference PDF that Microsoft has finally published.  And a little indication that Twitter may be experimenting with encrypted DMs, so like secure messaging coming to Twitter.



LEO:  Oh, wow, interesting.



STEVE:  Yeah.  And based on the screenshot I saw, it looks like they did it right.  It looks like elliptic curve keys are being sent back and forth.  So lots of news to talk about.



So our Picture of the Week I got a kick out of.  This is from the coverage of a disturbing new cryptomalware.  Unfortunately, even though cryptocurrency mining is the big thing, there's still people encrypting all the files on individuals' drives, trying to get ransom from them.  And I got a huge kick out of the note, which I'll share when we get down to the story.  But this is the Windows 10 login screen, which an unwitting user is greeted by after their system has fallen prey to this particular new SynAck version of cryptomalware, where it just sort of says - and in fact the reason I got a kick out of what was written was it sort of sounds like, oh, we didn't do this to you, but we'd like to help you get out of this pickle.  And it's like, uh-huh.



LEO:  Uh-huh.



STEVE:  Anyway, so here on the login screen your first experience of this is "Hello.  Your files are encrypted.  To restore files, please contact us by email."  And then it's synack@scryptmail.com or synack@countermail.com.  So it's like, oh, something encrypted your files.  Oh, goodness.



LEO:  Hello.  Hello.  Hello, file-encrypting person.



STEVE:  So anyway, we will discuss this a little bit later.  It uses a technique which we've already talked about.  But essentially these things that are effective never go away.  And unfortunately, this is making some money for some people, and so it hasn't gone away.



Speaking of what's not going away, we're going to doubtless be covering, both in the U.S. here and everywhere else, this tension which is not going to go away between a state's, you know, a government entity's feeling of their right to and need to monitor communications within their boundaries, and the fundamental problem that cryptography robustly prevents that.  So we've been - and you didn't miss any of this, Leo, because it's been quiet.  Well, maybe - no, I don't think you did.  Were you here when we talked about how the Russian government blocked about 20 million IPs?



LEO:  Yeah, yeah, yeah.



STEVE:  Right, okay.  So that was the last news we had.  So this is Russia vs. Telegram.  And first Russia...



LEO:  Well, this was part of it, wasn't it, that they had to block all those IP addresses - was it for Telegram?  Yeah.



STEVE:  Right.  Yeah, yeah, exactly.  So they first said, please give us the keys to all of the communications.  And Telegram predictably said no.  Then they blocked Telegram's IPs.  So Telegram moved over to Amazon and Google Cloud services specifically because that would conflict with all these other services using the same things.  And the Russian intelligence service said, we don't care.  And so they just used like four massive /16s and /15s, so like huge blocks of cloud space got blacklisted.  And of course it did, it killed thousands of other - of access to legitimate services.  So that only lasted about a week and a half because there was so much back pressure, as you can imagine, I mean, this was a horrible thing to just black out, I think it was like 19 million IP addresses.



So then what happened is these IP restrictions got lifted, but Amazon and Google both started preventing what is technically an abuse of their service, which it's not clear whether they'd been allowing this deliberately or not.  But they're not happy about it.  And it's something that human rights organizations and people who wanted to prevent censorship have been using.



So here's the idea.  It's known as "domain fronting."  And it's been in the news a lot because Amazon did tell Moxie at Signal that they were going to have to stop using one of Amazon's domain names to sort of sneak their Signal communications through the Amazon Cloud.  The idea is that communications happens when you establish a connection to a remote site on several levels.  There's the transport level, where you use DNS to look up the domain name's IP.  Then you connect to that IP.  Then you do the TLS handshake where, among other things, you use the SNI, the Server Name Indication, to indicate which domain within a shared hosting environment you want to connect to.  That allows the endpoint, the other endpoint that you're connecting to, the server, to select among all the certificates it might have as part of the TLS handshake to provide the proper one that matches the domain.  Now you've established an encrypted connection.  Then, when you make a query, you also in the query headers have the host header, which specifies which domain you want to connect to.



Well, get this.  The way both Amazon and Google and Microsoft, the way all these big cloud providers have things set up is it doesn't really matter for the first couple steps who you say you're connecting to.  You can say you're connecting to any service that that particular provider offers at the DNS lookup, and even at the TLS handshake level.  And if you're really clever, once you've established that encrypted connection, which nobody observing, nobody doing deep packet inspection can see into, then you can use, or you have been able to - this is what just changed in the last week or two - you had been able to specify a different host name, an actual different, like, server you want to connect to in the tunnel, in the TLS-connected tunnel to the cloud, to get to anyone else.



And so the point is this was being - it turns out lots of people were doing this who were using cloud hosting in order to sneak their traffic through passive packet and even deep packet inspection.  So somebody monitoring packets would see an individual connecting to a non-censored, no reason to worry domain.  But once that connection was established to an Amazon property or a Google property or a Microsoft Azure Cloud property, then the actual query in the encrypted tunnel, which could no longer be observed, would be to a different domain.



And so the idea is that these cloud providers were terminating the encryption at one layer, but then servicing the actual requests at a different layer.  And they weren't connected, these layers.  So they didn't know or care that the original handshake was to a domain different from the following queries being made in that tunnel.  Now, this is technically an abuse of the service.  I mean, this is not the way it's supposed to work.  And cybercriminals have been abusing this, too.  So it hasn't just been people who we might want to be rooting for, wanting them to have freedom within repressive regimes or people trying to avoid censorship.  Bad guys were actively doing this, too.



So what has happened is that this is being shut down.  Microsoft and Amazon and Google have begun to shut this down.  And in fact, in their letter to Moxie Marlinspike, of course, regarding Signal, Amazon wrote to Moxie saying:  "Moxie.  Yesterday AWS became aware of your Github and Hacker News/Y Combinator posts describing how Signal plans to make its traffic look like traffic from another site, popularly," they write, "known as 'domain fronting,' by using a domain owned by Amazon," and that's Souq.com.  Amazon writes:  "You do not have permission from Amazon to use" - I don't know how you pronounce that.  S-O-U-Q dotcom.



LEO:  Souq.  It's, I mean, I don't know, but it is an Arab bazaar.  They call them the "souqs."



STEVE:  So they say:  "You do not have permission to use Souq.com for any purpose.  Any use of Souq.com or any other domain to masquerade as another identity without express permission of the domain owner is in clear violation of the AWS Service Terms."  Then they reference Amazon CloudFront, Section 2.1:  "You must own or have all necessary rights to use any domain name or SSL certificate that you use in conjunction with Amazon CloudFront."  It is a violation of our Acceptable Use Policy by falsifying the origin of traffic and the unauthorized use of a domain.



"We are happy for you to use" - meaning Signal, of course, for encrypted communications - "to use AWS Services, but you must comply with our Service Terms.  We will immediately suspend your use of CloudFront if you use third-party domains without their permission to masquerade as that third party."  So this has sort of been going on.  It hasn't come up in our coverage because it's interesting, but it wasn't very controversial.  Now basically this hack - which is what it is.  It's a hack to establish a connection under one domain and then issue queries to another domain.  It's only a consequence of the architectural split in the way the cloud providers have been operating until now that has allowed this to be possible.  So it's going away.



And unfortunately my feeling, I think there's a - my feeling is this is all abuse of technology.  I mean, while I'm sympathetic to what people are doing, it's bound to fail.  Russia chasing after Telegram demonstrates that Telegram is going to lose this.  And there's another one, it's not Ving, it's - I can't remember the name of it.  There's another very popular encrypted application which the Russian intelligence organization has now notified they're next.  So it's been nice while it's lasted, but it's not going to last.  If a country like Russia decides that they absolutely refuse to allow encrypted communications that they cannot see into within their borders, they're going to win that battle.



So anyway, what Russia has now done, unfortunately, in pursuing telegram, is to block 50 VPN and proxy services completely because they were being used by users to avoid the Russian filters.  And so once again it's like, okay, if this is what's going to happen.  So VPN services, even when they were being used for benign reasons, they're all now suffering because some users are using them to still have access to encrypted communications with Telegram.



So again, it's not something which is going to survive in the long term.  I don't know, I'm sorry for the people within these repressive regimes.  But the technology which sort of permits it to happen can't withstand someone who refuses to allow it to happen.  The nation that you're in is going to win this.  And of course we're interested to see how this all comes down in the U.S., as well.  But Telegram is already out of China. They're going to end up being out of Russia.  And it will have just been a transient capability which ends up getting lost.  There's just no way to avoid that kind of filtering.



We've also been following for the last, what, I guess about a month and a half, this drama with Drupal, the so-called Drupalgeddon2, where a very critical vulnerability was discovered.  All Drupal sites were given one week's notice of the availability of an update.  And we could paint the future in this case, and it's unfolded exactly as expected.  Leo, in the show notes here I have a link to the Google Docs spreadsheet which has a list of more than 350 known Drupal sites that have been compromised and are currently serving a cryptocurrency miner into the browsers of everyone who visits them.  Most notable among them is Lenovo.  Every visitor to Lenovo is running, unless their own browser blocks it, a malicious cryptocurrency miner in their browser.



LEO:  The main Lenovo site?



STEVE:  Yes.



LEO:  That's crazy.



STEVE:  Yes.  I noticed, I don't know why, a lot of religious sites were there.  Boy, there was the National Labor Relations Board in the U.S. was breached and had this.  It's "jquery.once.js?v=1.2" is the cryptocurrency miner which has been edited into those Drupal sites, which is then downloaded by visitors' browsers and run.



LEO:  So on this spreadsheet some stuff is pink or red, and some stuff is yellow.  Is the yellow less [crosstalk]?



STEVE:  I don't know why.  I saw that, too, and I didn't see any key or indication of what that meant.  But it's quite sobering to see the actual list of sites there.  And cybersecurity firm Imperva also discovered another campaign, also against Drupal sites, which they named Kitty because the JavaScript which is being maliciously installed is M-E-0-W.  So me0w.js.  Thus it's the Kitty malware campaign.  That one is running a Monero miner from Webminerpool.com.  And in addition, they're installing a persistent PHP backdoor into any site that they compromise, giving them long-term access after the site's owner updates.



Of course, that follows onto the longstanding wisdom which is, after your site has been compromised, you can't trust anything about it.  I mean, you really have to wipe and reinstall from scratch because, if bad guys get in, they can install all kinds of persistent mischief that you have no way of detecting in some cases.  And they're also running a server-side miner.  So this goes from, in our coverage, from here comes a problem.  Everybody patch as soon as you can.  Inevitably that will not be heard because Drupal is the second most popular content management system on the 'Net, WordPress being first.



Many sites will not patch.  They will immediately, within a day - the patch was on a Wednesday.  The next day, on the following Thursday, there was already sniffing traffic being picked up of sort of a precursor.  That matured into attacks.  Then we covered, yes, attacks are happening.  And now we have lists of compromised sites.  So unfortunately, no surprises here.  This is going exactly the way we now know these things go, unfortunately.



So hopefully people will see the list of sites that are compromised, know somebody, you know, individuals, know somebody there and say, "Hey, do you realize your site's been compromised?  You've got bad guys running around in your system."  Because this is a remote execution attack, so a very, very serious and critical attack.



Okay.  It's being called GLitch, G-L-I-T-C-H, and GL is in caps in the name of this, as in WebGL.  A group of researchers at the Vrije University in Amsterdam have developed a new, startlingly effective remote attack against Android smartphones using the user's web browser and the device's built-in GPU via JavaScript and the WebGL API.  So what this means is they have demonstrated that a user of Android visits a particular website.  JavaScript is delivered and runs either under Chrome or Firefox.  They tested it under both.  And get this.  In two minutes, the attack is so potent that - and I should mention this is Rowhammer.  Within two minutes their phone is compromised.



So it's the first Rowhammer attack to leverage GPUs to leak the computer's memory contents and allow bad guys remotely to compromise a phone.  We've been talking about Rowhammer for quite a while.  Remember that it leverages essentially noise in RAM access to cause a bit to flip in the actual memory grid of dynamic RAM as a consequence of it sort of being a malicious access pattern.  And it turns out that GPUs are far more effective than CPUs.  The reason we haven't seen this on desktops before is that GPUs have their own memory.  On lower end devices like smartphones and tablets, the GPU shares main memory with the CPU.  So by leveraging WebGL, which gives the JavaScript extremely accurate timing information, they're able to perform a cache attack with GPU because the GPU has a much shallower cache than the CPU does in order to compromise the device within about two minutes.



So what's not clear is whether there has been some fuzzing in the WebGL support on those browsers.  There had been some fuzzing in timing in response to Meltdown and Spectre earlier this year.  Under the Cert.org page, I went looking to see specifically whether Mozilla and Google with Chrome had responded, and nothing was there, although in digging around I saw some reference to perhaps this already having been taken advantage of, but nothing yet posted publicly.  So it's not clear.  Certainly both companies know, and I'm sure they will be arranging to limit the timing accuracy of WebGL.  And essentially that's the crucial thing, as we have been seeing a lot lately, to determine whether - essentially for Rowhammer to be used to penetrate caching is you have to be able to sense whether a particular access was cached or not.  And for that you need to know with very good accuracy what the timing was.



So anyway, we do have, again, another instance of our famously often-quoted, and I think it was Bruce Schneier who said, "Attacks never get worse; they only get better."  And so here's Rowhammer, which continues to cause us trouble.  And hopefully downstream we will see some improvements at the hardware level.



What we know is that it's possible to more intelligently refresh DRAM.  Either increase the refresh rate, which lowers the bandwidth that we can use the RAM, so it essentially slows the DRAM down, but hardens it against attack, or probably in the future we're going to see Rowhammer-resistant hardware design, which notices that there is an access pattern which may not be malicious because there's nothing inherently malicious about accessing DRAM.  It just turns out you can leverage it.



So next-generation DRAM hardware will probably preferentially refresh rows adjacent to those being accessed heavily in order to prevent these bits from flipping and thus essentially over time adapt the hardware to the attacks.  And of course downstream that's where we have to end up being with this continual flow of Spectre problems with the hardware that we'll be talking about a little bit more in a second.



I wanted to soften the blow to our listeners who are using Firefox as their preferred browser, as I am, with the news first of all that Firefox 60 will be returning to a sponsored content model for newly opened tabs.  They purchased Pocket last year, and Pocket will be providing some sponsored content.  So for people who want to support Mozilla, I think this is a good thing.  First of all, it is important to know that Mozilla is absolutely protecting the privacy of their users who view this sponsored content.  There's no age, gender, location, user data being sent back to the advertisers who are on these promoted stories, which will be appearing on Firefox's newly opened tabs by default.  And if you really object to this, you can turn them off.



So first is maybe you want to support Firefox, as I certainly do.  So know that Firefox is keeping the interaction privacy enforced.  But if you do object to it, rather than abandoning Firefox, you can just, under the little gear icon on a newly opened tab, you can simply turn off - it's just a checkbox, "Show Sponsored Stories," and that could be turned off.  So I hope that Firefox and Mozilla stay with us because it's good to have a solid second to Chrome, if only that, and I still prefer it.  Until Chrome gives us much more manageable memory footprint and tabs on the side, I mean, integrated side tabs, which there's been some talk of, but we haven't seen it yet, Firefox is the only browser I can use because it just makes it possible to have lots of tabs open.



And so the news, Bleeping Computer had some good coverage about somebody trying to hide a backdoor in a popular JavaScript npm package.  And the specific story itself is interesting.  But the background and the trend we're seeing is I think deeply worrying, which is that there are an increasing number of instances of JavaScript packages being deliberately compromised.  Not mistakes being made that are found, but backdoors being installed in packages, being snuck into packages on an ongoing basis.  In Bleeping Computer's coverage they talked about, extensively, previous instances where this had been done.  And if anyone wants the full story, I have the Bleeping Computer link in the show notes.



But my bigger concern is that the entire - and I have to put this in air quotes - "security model," because, I mean, it's an insecurity model, which has just sort of quietly and innocently evolved, is deeply flawed.  As we know, the way it is today, websites you visit have links in their page headers which cause our browsers to go out and indiscriminately download JavaScript packages which their site developers, or sometimes their sites' packages developers, I mean, it's like multiple levels of, like, "it's not my problem" indirection caused these things, this executable code to be downloaded and run in our browsers.



In the case that Bleeping Computer talked about, it was a cookies package that was, like, three levels removed where a deliberate exploit had been inserted into it, which just happened to be found by people who noticed that you could use a certain format of query in a header to essentially cause the browser to execute a command that it wasn't authorized to by embedding this in a header.  So, I mean, somebody deliberately embedded a backdoor into this JavaScript that was in this JavaScript package being downloaded.



So the problem is, I mean, I have no solution for this problem.  I am profoundly unhappy with the way the world has evolved in this regard, this lack of responsibility and this tendency to just keep adding JavaScript packages from third-party sites for functions that the website uses in order to - and, I mean, we've looked at this.  Sometimes it's like 40 different sites are having JavaScript pulled from into the user's browser.  And of course we know that web security requires every single one of them to behave themselves and not to misbehave.  Any one of them that does compromises our security.



So the good news is the browser vendors understand this, and they're doing everything they can to create a sandboxed environment.  I don't blame anybody for running their own sandbox or running a browser in a VM, like adding belts and suspenders to the existing security model of the browser.  But it's bad enough, I guess is my point, where mistakes happen like as mistakes.  It really becomes worrisome when there's a strong incentive for packages to be deliberately backdoored and then leveraged against people who unwittingly download them.  So anyway, in looking at this coverage in Bleeping Computer, I just thought, wow, you know, I guarantee you we will be talking in the future about this kind of problem.  This is different than what got Drupal, but very worrisome.



So, okay.  This is just - it's not really comic relief because it's...



LEO:  I'm laughing.



STEVE:  Because if you're affected by this...



LEO:  It's not funny, yeah.



STEVE:  ...it's just terminal.  But the guys that did this crypto malware, which encrypts all of a victim's system, what you get is a "read this please" and then a unique string of eight characters dot txt file.  And I'm going to read this exactly as-is because it's clear that English is not their first language.  But it says, as if it's like a benefit of some sort...



LEO:  It's not our fault.  We're so sorry, yeah.



STEVE:  They have a nice acronym, too:  FES.  So it's SynAck FES, centered, Files Encryption Software.  "Dear Client."  Right?  Client.  "We apologize for inconvenience with your files."  Oh, yes, uh-huh.  It's a little inconvenient that they're all encrypted, and we can't get them.  "So we make a business offer to order file recovery service from us.  We do not extort money.  Files restore is an optional service."



LEO:  Absolutely.



STEVE:  Otherwise you could just give up right now and go home.



LEO:  It's optional.



STEVE:  Well, yeah, exactly.  Eat or die.  "Also we will do auditing of your network FOR FREE [all caps]" - they've already been in, right, and rummaging around destroying everything.  Oh, look, we found a bug - "for free if you order file recovery service.  So some details about SynAck FES.  This software uses" - and then it's absolutely true - "an ecies-secp192r1 algorithm."



LEO:  Oh, I thought that was made up.  That's real?



STEVE:  It's actually real, "...to create unique pair of private and public keys for the session," which basically means you're screwed.  The encryption is good and you can't get it back.  Each file is encrypted with random key using aes-ecb-256 algorithm.  And, you know, if I were nasty, that's what I would do.



"We strongly recommend you not to use third-party decryptors because they can damage your files."  Right, you know, we encrypted them, but don't try to decrypt them.  "But if you want to try to restore your files by yourself, make sure you have made backup copies of encrypted files."  So they're being really helpful here, Leo, because you might try to fix it yourself, but just make backup copies so that we can help you after you give up.



"And please do not remove files with text notes because they contain important information required for file restoring.  If you want to order file recovery service" - which of course is not extortion, it's entirely optional, up to you if you ever want your files back - "please contact them" - I'm sorry, back to them - "please contact our support using one of the following email addresses," and then those are the two addresses I gave at the top of the show, synack@scryptmail.com and synack@countermail.com.  "If you have not get a response in 24 hours, please do not panic."  Because they're very busy decrypting everyone's files and offering this great service.  "Please do not panic and write on Bitmessage [and then] using site https://bitmsg.me."  And then they give you their crypto key handle.  "Keep in mind that there are fake services offering decryption."  Leo, how could anyone set up a fake service to offer decryption?  "Do not believe them or you will lose your money."



LEO:  [Laughing]  We wouldn't want you to lose your money.



STEVE:  No, no.



LEO:  We want you to send it to us.



STEVE:  Exactly, so we can properly decrypt the files that we have encrypted.



LEO:  That's right.  It's not a loss.  No, no.



STEVE:  That's right.  It's a recovery.  Anyway, "There is one method you can use for proof.  Ask to decrypt some files for free."



LEO:  Oh, nice of them.  Free trial.



STEVE:  "No one except us will be able to do that."  And then we have, "Please include the following text in your message."  And then they've got a six-line gibberish key thing.  "Best regards, SynAck Team."



LEO:  I didn't know about this Bitmessage thing.  That's kind of cool.



STEVE:  Yeah, yeah.



LEO:  So bitmsg.me, and then you of course have to have a key.



STEVE:  Yup.



LEO:  I presume it's like a cryptocurrency key.  Is it actually using...



STEVE:  Not cryptocurrency, but it is a...



LEO:  It's like a miniLock or something.



STEVE:  Yeah, it's a cryptographically strong key, which you are then able to use to send them a message, and they're able to reply.



LEO:  Good.  Very, very thoughtful.



STEVE:  So for what it's worth, cryptomalware is still with us.



LEO:  Oh, I think it's worse than ever, Steve.  I think the numbers are actually up; aren't they?



STEVE:  Yeah, yeah, it is, unfortunately.  And people click links, and that's what happens when you click the link.



LEO:  They do, they do.



STEVE:  It's like, yeah, I'm an African prince, and I have all this money for you.  I just need to talk to you a little bit.  Click this link.



LEO:  You got that email, too?  Oh.



STEVE:  Well, you and I are both friends of his, Leo.



LEO:  Yeah, yeah, yeah.



STEVE:  So, okay.  Twitter discovers a mistake.  330 million people are asked to change their passwords.  I got email.



LEO:  That's everyone, by the way.  That's everyone.



STEVE:  That's everyone, yes, everyone.  The email from them said:  "Hi, @SGgrc.  When you set a password for your Twitter account, we use technology that masks it so no one at the company can see it," meaning no one at Twitter, and not to mention anybody else who obtains their database.  "We recently identified a bug that stored passwords unmasked in an internal log.  We have fixed the bug, and our investigation shows no indication of breach or misuse by anyone.  Out of an abundance of caution, we ask that you consider changing your password on all services where you've used this password," meaning Twitter and everywhere else where you also used it.  "You can change your Twitter password anytime by going to the password settings page."  And then they just explain about the nature of this.



So I guess my feeling is everyone should decide.  Hopefully, you don't use that password anywhere else.  So this is the common advice now of not using the same password anywhere else, annoying as that is.  If you're worried about somebody actually having obtained it and tweeting, basically impersonating you on Twitter, being able to log in as you, which could cause you to lose access to your Twitter account, then you may feel the need to change it.  It's not hard to do.  And then update your password manager in order to know what your new Twitter password is.  I think they've done the right thing.



GitHub also just like a week ago had the same problem.  They're completely unrelated.  Essentially part of their developer pipeline was they were making some changes or doing some maintenance.  They turned on a log at a point where incoming passwords were logged in the clear before being robustly encrypted with bcrypt, which is what both services, Twitter and GitHub are using, which is a good, state-of-the-art, password-based key derivation system.  And so I think you should consider changing your Twitter password.  I imagine everybody got that email.  So you probably are aware of that.  It doesn't mean that it was breached.  They believe it wasn't.  But again, as they said, out of an abundance of caution we recommend that people change their password. 



LEO:  Yeah.  I mean, if you have two-factor you're probably not at huge risk, although the real problem is that a lot of people use the same password on multiple sites.  So that's the one to watch out for.



STEVE:  Yes, yes, yes.



LEO:  If you've reused that password, you have other places to change it.



STEVE:  Yup.  Okay.  So we don't know yet what the "P" of Android P stands for.  "O" was Oreo, right, as we march through the Alphabet.  So whatever it is, we know a few things about it.  They've talked about some of the security and privacy features which are coming.  A nice one is that there was some worrisome cross-app leakage of network use.  Any app which declared that it needed network access, well, and really what app these days doesn't, would, just by getting that blanket permission, have access to the /proc/net process.  This has long been known, and I don't quite get why it took so long for Google to restrict  this, and they are now, but it wasn't a data leakage.  There's no way that would have been allowed.  But any app could see what any other app was doing from a standpoint of what things they were accessing.



So there was a clear privacy concern that all apps that had any access to networking features essentially were able to watch the global Android OS /proc/net process and see what other apps were doing.  That's being restricted only to VPNs or apps which Google closely looks at to understand why they have a need to access that.  So it is being removed from the way it has always been in P.  Also the good news is Android P will block cleartext, that is, HTTP traffic from all apps.  HTTPS will now be the only means of accessing the 'Net.



So, yay, we're finally at that point where that kind of enforcement is practical, which it hasn't been until now.  There have been some instances where you had to have non-encrypted traffic.  No more.  So anybody who has an endpoint with an app in Android needs to be able to encrypt it so that their app will be able to get to their endpoint.



Also P will use a uniform UI when requesting fingerprint authentication across apps and devices.  So there's been a concern that your fingerprint authentication could be spoofed.  And so now they're going to lock that down, and there'll just be one UI that all apps share in order to get this.  Also we were talking recently about apps accessing the phone's camera and microphone without any notification.  That'll be blocked from all background apps by default.  Also, P will encrypt backups on the device with a local secret key before sending it back up for storage on Google's Services.  So yay to that.



And two last things.  Android P will begin experimenting with support for MAC address randomization.  And this is an important feature.  We've talked about this before with regard to iOS.  All WiFi is Ethernet, and Ethernet uses a 48-bit MAC, the Media Access Control ID.  By design, that's a unique number so that you never have two devices that might be on the same Ethernet with the same MAC, or they will collide because the traffic is routed to the devices by their MAC address.



So if you were, in the case, for example, of iOS, which has resolved this, if you were just wandering around with your WiFi radio on, the MAC address of your device would, if steps were not taken, be available without associating to an access point, just because you're out looking, enumerating the WiFi in your neighborhood.  As we've discussed before, Apple has long ago solved this by using a randomized MAC address which is not actually you until you associate with the access point.  So once you're actually associated with the access point, then your real MAC address is used.  But otherwise it's just random.



So that's a really nice tradeoff for staying compatible with the standards, yet not actually being a unique MAC that can wander around.  So it's good news that Android P is going to begin rolling this feature.  I don't know what it actually means.  All I could find was, quote, "Begin experimental support of."  So maybe they're just being cautious about this.  We'll have to see.



And lastly, support for DNS over TLS, which is another great thing.  Remember that DNS by default is over UDP, which is unencrypted.  DNS over TLS means probably that Android will be using Google's DNS and set up an encrypted tunnel over which all DNS queries will flow, to hugely boost privacy.  So that's all for the best.



And I talked at the top of the show about this upcoming GDPR compliance issue.  I will be updating GRC and explicitly stating, in our privacy policy, explicitly stating why and how we are GDPR compliant.  It's easy for us.  And as a small organization, and you are, too, Leo, you're less than 250 people, so the requirements are substantially reduced.  But both of us, GRC and TWiT, have services available to citizens in the EU.  So the point is, any site that is offering services globally, and where global includes the EU as it does, needs to be GDPR compliant.  And enforcement begins on the 25th of this month, 25th of May.



Anyway, I got the biggest kick out of this.  This is truly a service:  GDPR-Shield.io.  GDPR-Shield.io.  And the banner comes up with the good news:  "You can save thousands on GDPR compliance."  And then the little subheading here:  "Simply paste our JavaScript snippet..."



LEO:  Oh, boy.



STEVE:  "...into your..."  Huh?



LEO:  Okay.  Sure.  No problem.



STEVE:  "Simply paste our JavaScript snippet into your website's code.  We'll check every visitor of your site and will block access to users located within the EU."  Yay.  Problem solved.



LEO:  That is one way to do it.  That's what Unroll.Me does.  They don't let anybody from Europe - I think, based on what you were talking about earlier, probably wouldn't be a good idea if people just paste Java snippets into their websites.



STEVE:  Exactly.  That's probably - exactly.



LEO:  Who knows what it really does?  And by the way, I think it's down because I just went to GDPR-Shield.io, and it's not responding.



STEVE:  Oh, no kidding.  Interesting.



LEO:  What a surprise.



STEVE:  Okay, well, who knows what's going on there.  I do have a picture of the site from Bleeping Computer's coverage, where they grabbed a screenshot of the home page, so showing it.  And, okay.  So the ultimate goal of GDPR, and we've not covered it extensively here, we just mentioned it in passing that it was coming, but it was a long way off, and now it's here, is to give individuals more rights over their data and to restrict how companies process private information.



What really, as I mentioned before, caught everyone's attention is the "impossible to sanely ignore" fines which the legislation imposes upon anyone whose site or services are accessible by citizens in the EU. Under GDPR regulation, firms can be fined - I hope everyone is sitting down for this - up to 20 million euros, about 28 million U.S. dollars, or 4% of the company's worldwide turnover, and I guess that's profits, whichever is - or revenue, rather - whichever is greater, not the lesser of, the greater of $28 million U.S., or 20 million euros, or 4% of your revenue, whichever is greater.  You have to have, for example, a Data Protection Officer, a DPO, which having one is mandatory.



But then it turns out, whoops, slow down, wait a minute.  If you have fewer than 250 employees, things are a little better for us - me, Leo, by a lot, and you also fortunately.  We do not need to - small firms do not need to have explicit documentation explaining why personal data is being collected and processed, which information is being stored or for how long, although I fully intend to explain, essentially the only stuff we have is what we collect for the purpose of selling SpinRite.



LEO:  Well, wait a minute, though.  You have server logs.



STEVE:  No, I don't.



LEO:  You don't have any server logs?



STEVE:  None.



LEO:  So when somebody visits your site, your servers do not log their IP addresses?



STEVE:  Nope, no record of it.



LEO:  That's general practice for most websites.



STEVE:  Not for me.



LEO:  You're smart.  Now you don't have to worry about it.  Do you actively delete them, or is there a setting you can turn it off?



STEVE:  I don't log.  No, I have no need to log.  No reason to.  But if - and I'll explain.  I'm going to update our privacy policy, as I mentioned.  If someone said I want you to delete your record of my purchase, fine.  But you won't be able to upgrade because we use your purchase record in order to, you know, previous proof of purchase.  But fine.  And that's essentially all that's necessary for compliance for a small firm is to be conscious of users' rights, to tell users what information you are storing about them clearly, and to give them the right to be forgotten, to give them the right to request deletion.  So we have that capability, and I'm happy to make that available.



So anyway, so for what it's worth, I wanted to sort of plant the bug in our listeners' ears.  I know we have a lot of listeners in enterprises of all sizes.  And so just be aware that this May 25th deadline is approaching.  Oh, and for what it's worth, it's not as if a single infraction immediately causes the levying of a 20 million euro fine.  That's a ceiling.  And historically these privacy fines have been, I mean, it's like only if after warnings, and you've been told, and blah blah blah, you don't fix things.  So it's there, but it doesn't mean suddenly you're going to get a bill from the European Union for 20 million euros.  Doesn't work that way.



But I'm glad that the EU is putting this stuff in place because this is what all sites should do.  All websites and services should say, you know, "Here's the data we're collecting on you.  And if you object to this, we'll delete it for you."  And I guess, if that deletion capability is absent, that's something that you would need to look at, I mean, immediately because this GDPR regulation requires that people doing business in the EU have the ability to delete EU citizens' data completely from their services and sites and so forth.  So if that's not there, you need to have that.



LEO:  I don't know how we're going to do that.  Geez.



STEVE:  What?



LEO:  Well, I'm sure we have server logs.  That I can turn off.  But we log every download because that's how you count the downloads.  And we log the IP address.  Is logging an IP address considered to be a violation of privacy?



STEVE:  Good question.  I don't know.



LEO:  I mean, you keep track of an IP address for at least as long as the conversation is going on with your website.  You wouldn't be able to have a conversation otherwise.



STEVE:  True.  True.



LEO:  But that's a transient log.  Hmm.  That's going to be interesting.



STEVE:  Be interesting to see whether IP address [indiscernible].  Now, I mean, certainly it only means something when you deanonymize the IP.



LEO:  Which you can easily do.  But you usually can't get to an individual.  Usually only to their Internet service provider.  Huh.  I'll have to - now I have to find that out.



STEVE:  Yeah.  It's worth - that's a really good question, whether an IP - I'm sure now that we've discussed this on the podcast, my Twitter feed will be full of information.



LEO:  Yeah, and that's all we know because we don't ask for additional information.  And we don't even sell anything.  There's nothing.  But we do, I mean, we use Google Analytics, which logs IP address and probably does geographic locationing.  And we certainly do that for downloads, whenever you download a show.  That's how we know how many people listen to the show.  That's how we charge.  So I don't know.  Huh.  We are well under 250 employees, but I don't think that lets us off the hook entirely.



STEVE:  No, it does not, no.



LEO:  Oh, I hadn't even thought about that.  Oy oy oy. 



STEVE:  Well, yeah.  It seems to me, I mean, in the case of the IP address, if someone said this is my IP, remove it from your logs, I mean, if you haven't deanonymized them, if you don't have a name associated with the IP, and neither of us do, I don't even have logs because I just don't need them, and I don't want them just for privacy, I mean, I just - I have no need for them.  Then the only thing I could imagine is someone says, "This is my IP.  Expunge it from your logs," which technically is possible.  But I doubt...



LEO:  Standard server logs with IP addresses must be disclosed in the privacy policy, but you don't have to get consent because they're being collected as part of a business's critical need to prevent fraud.



STEVE:  Nice.  Good.



LEO:  Yeah.  So you can collect them, but you do have to disclose that you collect them.  I think we do that already in our privacy policy.  I don't know how we would delete them, though.  You must routinely delete logs in 60 to 90 days, I think.  I'll have to look at this.



STEVE:  Well, and in fact in your case keeping an IP longer than that doesn't serve any purpose because...



LEO:  No, no, we don't need them, yeah.



STEVE:  ...they float around.



LEO:  We probably do delete them.  You typically delete logs because you don't want to have infinite - our logs are very large.



STEVE:  They do get big.  



LEO:  They are very big.



STEVE:  Yeah.  In GRC's privacy policy I explain that we don't log by default.  But if we need to track down a problem, we may turn logging on briefly while resolving a problem, and then...



LEO:  That's typically why somebody logs; right?  So you can...



STEVE:  Turn it off and shut down, yeah.  Okay.  So more than one million Dasan GPON routers are under attack.  So Dasan is a South Korean manufacturer that produces routers known as GPON.  That's Gigabit Passive Optical Network routers.  There are big networks in Vietnam, Kazakhstan, and Mexico.  And more than one million of these routers have been identified from scans of the 'Net.  The 360 Net Lab Researchers have identified a botnet controlled from Vietnam currently scanning and exploiting these devices.



What's worrisome, there are two CVEs that have been created for these, 10561 and 10562.  561 reads:  "An issue was discovered on Dasan GPON home routers.  It is possible to bypass authentication" - get this, Leo - "by simply appending '?images' to any URL of the device that requires authentication" - like, you know, the login - "as demonstrated by /menu.html?images/."  And then they give some more options.  Anyway, one can then manage the device.  And these devices have remote management exposed to the Internet, of course, by default, password protected, but it doesn't matter because you just put "?images" on, and that bypasses, essentially just skips over the check.



Then, as if that wasn't bad enough, the next sequentially numbered CVE, 10562:  "An issue was discovered on Dasan GPON home routers.  Command Injection can occur via the dest_host parameter in the diag_action=ping request" - and it goes on.  Essentially there is a remote command injection vulnerability that makes it quite simple to execute commands and retrieve their output on the router.  So we can bypass authentication and give them commands.  As a result, they are being swept up right now in a botnet being controlled from Vietnam, so another instance of home routers with a vulnerability that is discovered, and then immediately exploited by bad guys.  And it's good news to the individuals whose routers are exploited that people just want to use their bandwidth, probably mine crypto or use them to spread a worm, who knows.  But, yeah, the world we live in today.



Speaking of the world we live in today, Alex Ionescu, he's a security researcher we've referred to often with Crowdstrike, tweeted on May 2nd, so what, last Wednesday, he said: "Welp [W-E-L-P], it turns out the #Meltdown patches for Windows 10 had a fatal flaw.  Calling NtCallEnclave returned back to user space with the full kernel page table directory, completely undermining the mitigation."  He writes:  "This is now patched on RS4" - so that's I'm sure Redstone 4 - "but not earlier builds."  And he asks:  "No backport?"



Okay, now, what was funny was that, as I'm reading this tweet, I'm thinking, uh, wait a minute, that sounds eerily familiar.  Yes.  At the end of March, on the 28th of March, we discussed on this podcast the so-called Total Meltdown bug which exposed the system's memory, the virtual memory manager page tables for Windows 7 and Server 2008 R2 systems.  And remember we talked about it at the time.  Basically it was supposed to have the system privilege bit set, but it returned with the user privilege set, which allowed user access to the entire global memory map and read/write access to all of the system's memory.  Whoops.  So that was the Total Meltdown that was an emergency patch by Microsoft.



Now what was found was that the Meltdown patches for Windows 10, not including the most recent Redstone 4, which is the just-released-last-week version 1803 version of Windows 10, all previous ones apparently have, as Alex tweets, this very bad mistake made in the Meltdown mitigation.  So I also, just as we were going to start recording the podcast, I saw a note from Brian Krebs talking about this is the second Tuesday of May.  The podcast last week was May 1st, Mayday, the first Tuesday of the month.  So this is Patch Tuesday.  But I've been checking for patches all morning and getting nothing.



Brian just tweeted, I just saw him send actually email out to his email list, that there were a bunch of patches, 67 of them I think I saw him say.  So I've not had a chance to look at them.  Hopefully the fix for this is among them for Windows 10 users who have not moved up to 1803.  And as we talked about last week, there have been a lot of problems with this Redstone update.  Among them, apparently, the Spectre mitigations are not present, and there's no "go out and do them yourself" patch yet for those, although I have one report of it the Spectre mitigations being built in to 1803, which is what we would expect.  So still unknown in detail about that.



Leo, Microsoft has published a 948-page PDF.



LEO:  I was just reading that.



STEVE:  The long-awaited Windows Console Command Reference.  Never before in one place...



LEO:  It's never been documented, really?



STEVE:  No.



LEO:  There's no help file?



STEVE:  Well, it's just sort of all scattered around, and you kind of have to know who to ask.  Now here it is.  I've got the link in the show notes for anyone who's interested.  I also tweeted it, so @SGgrc.  If you're following me on Twitter, you've got a link.  It's the 250 Windows Console Commands on a just shy of 1,000-page PDF.  So all kinds of stuff.



LEO:  Wow.  



STEVE:  Yeah, really cool.



LEO:  You know, they're actually, I think, deprecating it.  They want you to use PowerShell, not the Command Console.



STEVE:  Right, right.



LEO:  Holy cow.



STEVE:  And just finally a note that Twitter has an unlaunched secret encrypted messages feature.  A security researcher, rummaging around in the pre-release land of the Twitter Messenger app saw a reference to the ability to provide your keys and exchange keys and apparently do encrypted DMs.  Shortly after the Snowden revelations, Edward tweeted to Twitter, to whoever the head honcho is or was at the time, that it would be nice to have encrypted Twitter messages.  And the response from Twitter was, "That's interesting.  We'll look into it."



And it appears that we may before long be talking about encrypted DMs, where you would arrange somehow to exchange - based on the length of the keys, they have to be elliptic curve keys, so they're manageable length.  You would somehow exchange those with someone and then be able to use Twitter to send encrypted DMs to each other, which by the nature of the way good encryption, properly done encryption works, nobody would be able to access your tweets, which would be a nice feature.



Speaking of a nice feature, I got a fun nice piece of email from a Bree Duffy, who's located in Oregon.  This was the 26th of April he sent this with the subject, he said:  "A security concern."  And then he said, comma, "SpinRite saves my sister's hard drive and me a headache."  And he said:  "When I heard about the virtual card numbers from Capital One" - that Leo, you were just talking about - "on your show I was highly intrigued."  And I would argue, as he should be.



He said:  "However, I'm also highly distrustful of apps from most companies due to concerns of snooping or downright sloppy programming causing issues.  Do you have any thoughts on the safety of Capital One's Eno plugin for desktop, or the phone app?"  And of course I have no, just to respond to that, I have no a priori explicit knowledge.  But I would argue, I mean, we've talked about per-use credit card numbers, and I think that is a huge benefit.  So I'm bullish on the service.  And I imagine that...



LEO:  I'm sure it's well coded.



STEVE:  ...this was well coded, yeah.  I mean...



LEO:  I mean, first of all, we should say they're a sponsor, as you know because you just heard their ad.



STEVE:  Right.



LEO:  But you know how many different programs and services you're running on a typical PC?  



STEVE:  Exactly.  Exactly.  Exactly.



LEO:  You know, I agree, and it is kind of my motto, don't install anything you don't need.



STEVE:  But if it's useful...



LEO:  But if it's useful, install it.



STEVE:  Yeah.



LEO:  A Chrome plugin is probably not...



STEVE:  That's exactly what I was going to say was that I was talking about how well sandboxed our browsers are now as a consequence of the risk of third-party JavaScript being downloaded.  These are one of the good guys, not one of the bad guys.  So for what it's worth, the idea of not exposing my credit card is clearly there's a value proposition here that has it making sense.



LEO:  Yay, thank you.



STEVE:  Yeah, I have no problem.  And as for SpinRite, he says:  "Now to SpinRite.  I have the rather unfortunate distinction of being the go-to guy in the family whenever there is computer trouble."  Leo, you and I both know about that, although I don't think it's unfortunate.  I love helping people.  But I think he does, too.



Oh, he says:  "I love helping them, but figuring out obscure problems can be quite the headache.  My sister brought me her computer which wouldn't boot into Windows.  It wouldn't even see the boot record.  I had fortuitously," he wrote, "just bought SpinRite" - thank you very much, Bree - "and decided to give it a go.  As with many such stories, while no errors were reported, all of a sudden the computer booted properly, saving me the headache of either formatting or buying a new hard drive, and reinstalling Windows and all of my sister's applications, et cetera.  Thank you very much for making this very useful tool.  I look forward to the new versions when you get to it, and SQRL when it becomes available."



And on that note I should mention two things.  I meant to note that, as for Twitter and their problem with storing their users' passwords in the clear or having logged them by mistake, that's something that SQRL completely eliminates.  As I have mentioned before, the SQRL system gives websites no secrets to keep.  That is, what websites store is the user's public key per site.  So their per-site public key.  It's first of all not valuable to anyone else or useful to any other site.  And having their public key is not a secret.  I mean, like their public key is not a secret.  It's used to verify a challenge that they're able to sign a random challenge, and it's used as their identity, which is kind of cool because it's double function.  But there's nothing to lose.  The second thing I wanted to mention is it's done.



LEO:  Woohoo!



STEVE:  Friday, Saturday, and Sunday...



LEO:  Words I never thought I'd hear.



STEVE:  I know.  I have begun building up the forums, the SQRL forum server.  It's up and running.  I've got PHP and MySQL and Zen 4.0 as the forum's system all up and running.  I'm in the process of configuring it and customizing it and getting it set up.  That's the last thing I needed to do because I need somewhere a public web place to host all of the various aspects of SQRL for people to ask questions and get answers.  It's not something that I can ask either me or Sue or Greg to do.  Everybody wants me back on SpinRite, which is what I intend to do as soon as I get this thing up and running.  But it is ready.  It is finished.



LEO:  Nice.



STEVE:  And working on getting it public.  Then I'm going to come up and spend some time with you and Father Robert, Leo, to do a TWiT Special and tell the world about it.



LEO:  Fantastic.



STEVE:  So it'll be really fun, yes.  And lastly, Spectre NextGen.  I guess it shouldn't be a surprise.  And it's really kind of not because, as I said, I was surprised that Intel was even able to fix this kind of problem in the microcode.  And as we know, they were not able to fix it on all of their architectures.  Their earlier architectures didn't have the flexibility necessary in microcode to essentially add unplanned-for instructions to give operating systems the control needed over branch prediction in order to mitigate the vulnerability.



But the greater problem is that forever, ever since we began trying to squeeze more processing power out of our chips, we've been willingly ignoring the fact that to do that, processors have been storing the history of code execution and sort of doing a mini version of on-the-fly compilation.  That is, if you're watching branches that are taken and not, and remembering whether they're taken or not, and then using them to speculatively execute the branch you expect to be taken next time, then you're storing state based on what the code has done.  And if you then switch the context back to a different process, it can cleverly figure out how the processor's behavior has been modified by the code that was previously running.  That represents a cross-process boundary information leakage.  And that's Spectre.



So what has come to light, the site Heise.de, H-E-I-S-E dot D-E, had the first coverage of this.  There are eight new CVE numbers that have been allocated, bug identifiers, but their contents are currently being kept secret.  Google's Project Zero has discovered one of eight new Spectre flaws.  They're being called Spectre-NG for NextGen.  Intel has internally classified, and this is all still - there's a complete cone of silence lid on this, nothing being said yet publicly.  Heise got on the inside and had some dialogues.  And under agreement of complete secrecy, there's no specifics yet.  But they have verified with second and third parties all of their coverage of this.



Intel has internally classified four of the eight new Spectre-NG vulnerabilities as high risk.  The remaining four are rated as medium.  According to Heise's research, the risks and attack scenarios for the Spectre-NG flaws are similar to those for Spectre, with one important exception.  One of the Spectre-NG flaws simplifies attacks across system boundaries to such an extent - and I can't wait to learn what this is because now I'm really curious.  I mean, as we know, there was some early Meltdown proof of concept because it was easy to do, but it was also easy to mitigate.  That was the caching attack, an attack against caching that was easier to fix.  There were never really any Spectre proof of concepts.  There was just the spectre of Spectre.



This looks like something easy.  They said it simplifies attacks across system boundaries to such an extent that the threat estimate is significantly higher than it ever was with Spectre.  Specifically, an attacker could launch exploit code in a virtual machine and attack the host system from there, the server of a cloud host, for example.  Alternatively, it could attack the VMs for other customers running on the same server.  And Intel's Software Guard Extensions, which are designed to protect sensitive data on cloud servers, are not Spectre-safe.



Heise believes that Intel is planning two rounds of patches.  The first is scheduled to start in May.  Apparently this Project Zero timed out yesterday.  So maybe Google is going to punt on their rigid 90-day deadline just because of how bad this is.  I don't know that the one they discovered is this one of the eight that is really the most worrisome.  Nothing happened yesterday, so maybe Intel has said we need a little more time on this.  We just don't know.  And of course we also know that these microcode patches can be problematical themselves.  So I'm sure we'll be talking about this in the weeks to come.



Anyway, the first round is scheduled in May, and the second currently planned for August, which, wait, June, July, August, another 90 days downstream.  And remember also that Spectre was 60 days embargoed, not the normal 90.  I mean, these are really, as we know, this was the big news at the beginning of this year, and now it's back for more.  ARM systems are also believed to be affected by at least some of these eight new vulnerabilities.  And we don't know about AMD.  So stay tuned.



But we do now have another worry.  Now, again, understand that this is cross-process and cross-VM information leakage.  So it's a concern for hosting providers, much less concern for individuals.  You would have to have malware in your machine that was then stealing your credentials or stealing passwords and things.  I mean, that's not good.  But it's anomalous for you to have malware in your machine.  It's not anomalous in a hosting provider because a hosting provider is saying, here's a virtual system.  Run anything you want.  Well, bad guys could be running something, trying to break out of the VM to get to the host or to get to another VM which is legitimately running, sharing that server's hardware.



So end users, not such a problem.  Never has been.  Probably still won't be.  You know, you don't want malware in your machine at all.  So this does break process boundary.  And so in time it's worth fixing it.  But I would argue, given the problems we've seen with rushed out microcode patches, that it may be worth sitting back a little bit and not getting all hot and bothered over this until there's some reason to worry.  So I'm sure we will have more information next week and in weeks to come on essentially a serious collapse in the security, the fundamental security of the architecture of our processors as a consequence of how much was done to make them go fast.



LEO:  Wow.



STEVE:  Yeah.



LEO:  And what's your sense?  I mean, the next-generation Intel processor's probably not going to fix this.  It's going to be a year?



STEVE:  Yes.  The problem is that the processor development pipeline is years deep.  I mean, the stuff they're working on now that they will definitely resolve this for won't come out the other end until 2020 at the earliest.  So maybe they can fudge some improvements into what they've already got in the pipeline.  But they've got an install base they need to worry about.  They've got their already in the pipeline yet not released, I mean, it's a little difficult to buy a new Intel chip that has known bugs in it.



LEO:  Yeah.



STEVE:  So, yeah, ouch.  It's a mess.



LEO:  But it's not like you could go use somebody else's processor.  We kind of have to live with it.



STEVE:  We don't know about AMD.  It'll be interesting to see.  That could hurt Intel if their processors are vulnerable.



LEO:  AMD doesn't have this problem, yeah.



STEVE:  Yeah, exactly.



LEO:  But I wonder how likely that is, given that they have the other Spectre and Meltdown flaws.



STEVE:  Yes.  And the fact that ARM systems are also vulnerable.  So this sounds like another new type of bad problem.  I can't wait to find out what this is that's like more exploitable.



LEO:  Have we seen any exploits in the wild?



STEVE:  No, no.



LEO:  One thing I worry a little bit about is that we're starting, I mean, we talk about all exploits as if they're the same, but really they're not.



STEVE:  Right, right.



LEO:  And I think that exploit hunters are incentivized for a variety of reasons to publish these.  But they may be very, very hard to implement.



STEVE:  Yes.  And in fact it's easy to make the point that, when people updated their systems with that first round of Intel patches, and they started crashing and bluescreening and having all kinds of problems, the fix was much more of an actual problem...



LEO:  Much worse, yeah.



STEVE:  ...than what it was trying to cure.



LEO:  Yeah.  We need to start, I mean, they've always had severity ratings.  But everybody says everything's severe at this point.  So exploitability ratings, or real threat ratings.  Because "severity" really doesn't reflect the real threat.  It reflects merely how far in somebody who uses the exploit could get, how bad it would be if somebody used the exploit.  What it doesn't address is how...



STEVE:  We need the GAS rating, Leo.



LEO:  What's that?



STEVE:  G-A-S, give a s**t.



LEO:  That's exactly what we need.  That really is.  And I don't think we have good information about whether this is one to freak out about, or that's one to freak out about.  I mean, we know ransomware.  We know Petya was something to freak out about and still is, legitimately so.



STEVE:  Yup, yup.



LEO:  But we've not seen any of these exploited in the wild, despite the fact that most computers are still not patched and may never be patched.



STEVE:  So it's "Should I GAS or not?"



LEO:  Should I GAS?



STEVE:  Yes.



LEO:  Yeah.  Well, the best way to keep track of all this, of course, is keep listening to Security Now!.  We do it every - normally Wednesday.  We're on a - no, no, we're on our regular time.  What am I saying?  Everything else got moved.  You were the floating island.  Everything else moved around.



STEVE:  You're a little jetlagged, my friend, but other than that we're right on time.



LEO:  Well, no, you know, I've thought you were on Wednesday ever since you were on Wednesday.



STEVE:  That's right.



LEO:  But it is Tuesday, and has been for years, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to watch live, you can go to TWiT.tv/live.  We are a little later than usual, that's for sure.  You can also get copies of the show from Steve's site, GRC.com.  He has the 64Kb audio and transcriptions.  And I know that's hugely valuable to a lot of you, not the least because you can search for information from the shows because the transcripts are searchable.  Isn't that nice?



We have audio and video at our website, TWiT.tv/sn, and you can always subscribe, whatever podcast app you use.  I promise you we don't collect information about you.  We do collect your IP address.  Somebody does.  Maybe we don't do that.  But I think we do.  Cachefly does, the company that does the downloads.  And I just don't know what to do about that.



STEVE:  Well, and Leo, let's remember, anything your browser touches is an IP address.



LEO:  It's saying hello, yeah.



STEVE:  Your IP is just being sprayed all over the globe.



LEO:  Well, and I looked, and there is some debate about whether the EU will consider IP addresses personal information.  And if you were to err on the side of caution, you would say that it is.  In which case I don't know what we're going to do.  We may have to block this show to all people who live in the EU.  I think I found a JavaScript snippet I can use.  No, actually that site's down.  I think that maybe they got forced off the 'Net, yeah.



STEVE:  Wow, interesting.



LEO:  Steve Gibson, have a great week, and we will see you next time.



STEVE:  Okay, my friend.  Right-o.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#663

DATE:		May 15, 2018

TITLE:		Ultra-Clever Attacks

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-663.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we will examine two incredibly clever, new, and bad attacks named eFail and Throwhammer.  But first we catch up on the rest of the past week's security and privacy news, including the evolution of UPnProxy, a worrisome flaw discovered in a very popular web development platform, the first anniversary of EternalBlue, the exploitation of those GPON routers, this week's disgusting security headshaker, a summary of the RSA Conference's security practices survey, the appearance of persistent IoT malware, a significant misconception about hard drive failure, an interesting bit of listener feedback, and then a look at two VERY clever new attacks.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with all the week's news plus a couple of clever attacks - cleverly named, as well.  One's called eFail, and it really is an issue if you use PGP or S/MIME to encrypt your email.  The other's called Throwhammer, and I'll let Steve explain how that works.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 663, recorded Tuesday, May 15th, 2018:  Ultra-Clever Attacks.



It's time for Security Now!, the show where we cover your security and privacy online with this guy right here, Mr. Steven Gibson of GRC.



STEVE GIBSON:  And look, I'm lit up just exactly right today, Leo.  I'm not too bright and not too dim.



LEO:  Has it been a problem in the past?  You've always seemed just right to me.



STEVE:  With my brightness?



LEO:  Yeah.



STEVE:  See, it's bright.



LEO:  Super bright now.  What, did you get special lights?



STEVE:  No, no.  It's just that your screen behind you is sometimes - it changes.



LEO:  Oh, that shot.  Well, no, yeah, that's not a - yeah.  No, I changed your brightness on my end.  So we've got a big one today, I think.



STEVE:  Well, yes.  Two very clever, I mean, okay.  Let me finish the sentence.  Two very clever attacks, one against encrypted email in general, so it affects the two primary encrypted mail technologies, PGP and Secure MIME, S/MIME.  And it's not a flaw in either of them except that it exploits a weakness in the security protocol.  But it's just sublimely clever.  And as I was reading into it, and I realized what these guys had figured out, it was like, oh, that's just so - that's just, like, whoa.  So I thought, okay, I just have to explain what they did because it just - it's a toe curler.



And then, as we always have said, attacks don't get worse, they only get better, quoting Schneier, I think.  Bruce was the first person to say that.  We have the inevitable evolution of Rowhammer that we were just talking about a few weeks ago, in fact it was last week, about the use of GPUs to induce the Rowhammer attack because they're lightly, if at all, cached, unlike CPUs that have sometimes multiple levels of cache so you have to do all kinds of crazy cache avoidance in order to get down to the memory.



So now we have Throwhammer, which as its name suggests is remote Rowhammer.  And this is very worrisome because, as we've often said when we're trying to sort of place attacks into a fair context, well, yes, it's bad, but you've got to have bad guys' code on the machine in order to be pounding on the RAM to get some sort of leakage.  Well, not anymore.  Now you can do it over the 'Net.  So that's Throwhammer.  So we will talk about both of these very clever attacks.



But first we've got to catch up, of course, on the rest of the week's security and privacy news.  We've got the evolution of UPnProxy.  That was the use of Universal Plug and Play that Akamai discovered and we reported a few weeks ago.  Well, it's already evolved.  We have a worrisome flaw discovered in a very popular web development platform that's probably going to get exploited.  It hasn't yet, but inevitably.



Three days ago was the first anniversary of the revelation of EternalBlue, which the WannaCry cryptomalware used to such devastating ends, and then several other malwares adopted.  We're one year downstream, and we've got sort of a chilling graph of the evolution in the use of EternalBlue.  We also have those GPON routers that were found to be very insecure and exploitable.  There are now five botnets fighting over them.  We have this week's disgusting security headshaker.  It's like, okay, what year is this, and we're still making incredibly dumb mistakes?



We have an interesting summary of the recently completed RSA Conference's security practices survey, where all of the attendees of the conference were asked to, here, fill out this survey of some interesting and somewhat chilling results that we need to share.  The appearance of the first persistent IoT malware.  Traditionally it's been possible just to unplug your light bulb because the malware was running in RAM.  Well, that's not true anymore.



Also, I encountered a significant misconception about hard drive failure, relative of course to the use of SpinRite, that I wanted to briefly address.  And that is the misbelief that a failing hard drive needs to be replaced.  And it's not that that's not true, it's that hard drives can have problems when they're not failing.  And so I want to dig into that a little bit.  I have an interesting bit of listener feedback.  And then, if we're still on Tuesday, we'll take a look at these two very clever attacks.  So, yeah, I think another jam-packed podcast, as you suggested.



LEO:  I'm really curious about the PGP attack.



STEVE:  Oh, Leo, it's just sublime.  Oh, my god, it's so cool.  It's brilliant.  I can't remember what Matt termed - Matthew did a series of tweets, and he called it - I'm scrolling down here through it, he called it "a masterpiece in exploiting bad crypto."  He said, "It's an extremely cool attack and kind of a masterpiece in exploiting bad crypto."  Oh, and, yeah.  So, yeah, I think everyone's going to get a kick out of it.



LEO:  Good, good.



STEVE:  So our Picture of the Week was one that I've had in my picture file.  It's not apropos of anything in particular this week.  But I got a kick out of it because it does help to sort of, you know, there are a lot of people who are chafing at the idea of HTTPS.  We know you cannot have logged-on sessions using browser cookies, web cookies, which is the way you maintain session state these days, unless you have privacy in your communications.  Otherwise, anybody who's able to passively sniff traffic, as you can, for example, in an open WiFi situation, is able to impersonate you by grabbing that session state.  So HTTPS is where we're headed.



It happens that the listener feedback that we have today argues against the idea that HTTP is dead forever.  And I got a kick out of this because it's a cartoon showing the tradition snake oil salesman telling you - holding up a bottle, saying that this special elixir will cure all of your ills.  And so on the tree it says, "Dr. Marvel's amazing HTTPS, guaranteed to cure all website security problems except:  plaintext passwords, SQL code injection, buffer overruns, social engineering, malware, spyware, adware, ransomware, trojans, CMS hacks, cross-site scripting, foreign URL injection, Flash exploits, Acrobat exploits, Java exploits."



LEO:  Otherwise, it's great.



STEVE:  Otherwise, boy, this thing will fix everything that ails you.  So, yes.  I liked that because it's a nice counterpoint to everyone rushing to secure everything.  I mean, we have to have that, too.  But by no means is it the cure-all for all of our problems with the web.  It just means that a class of attacks are much harder to perpetrate, and there's still lots of work left to be done.  So pretty much like health in general, it's a large surface, a large attack surface.



So, okay.  I did want to follow up on last week's warning or caution or "watch for it" note about Spectre.  It was, as we know, Spectre NextGen, where the news is - and this came from Heise over in Germany and had been well researched and followed up and verified - that there are eight new problems of completely unknown type, one of which Google's Project Zero found; one of which is a biggie, and we don't know if that was the one that Project Zero found or not.  We know very little about this.



So I just wanted to say we don't know anything more than we did.  So I'll keep looking, but so far this is a mystery.  Certainly at some point we will know a lot more. And the teaser, I think, is that the big problem is bigger than Spectre ever was.  And that is either version one or version two of the original Spectre exploits.  Apparently what's been found is something substantially easier to exploit, which the reporting says really does represent a problem for a shared hosting environment.



So one of the bits that came out of the RSA survey, the people that were there, we'll get to that in detail a little bit later, is that three quarters of the attendees are in companies who use cloud providers - Google, AWS, Microsoft.  So three quarters of enterprises of the attendees of the most recent RSA Conference are using cloud services.  And cloud services are not the only, but certainly the biggest worry and the biggest target for this kind of exploit, for something that allows you to break out of a VM containment or any kind of a sandbox or process.



Okay.  So we talked about Akamai's announcement/discovery that there are a lot of consumer routers with Universal Plug and Play port, the configuration port, or maybe the file it turns out, exposed to the Internet; and that, as a consequence - and we put it in the show notes when we talked about this a week or two ago, there's that classic sort of spy Internet, like where the bad guy bounces their traffic off like all around the world nine times or 19 times, however many, in order to defy the authorities' ability to track them down, and then finally land somewhere.  Which was always a bit farfetched because that suggested there were, like, all of these insecure systems everywhere.



Well, turns out that with this ubiquitous presence of consumer routers on LANs, which of course I've been championing forever because it is, when properly done, it is a really good hardware firewall.  I mean, you absolutely want that.  I can't imagine putting just a computer right on the Internet ever without having the NAT, the stateful NAT layer which drops unsolicited incoming packets.  Well, unfortunately, with this crazy popularity of these routers, they're not all being done right.



And so what we first learned from Akamai is that the routers' NAT mapping, which Universal Plug and Play is able to manipulate deliberately, the routers do not check that the IP on either side of the packet are on either side of the router, meaning the only valid mapping that that translation table should provide is from LAN to WAN.  There is never a place, well, there's almost never a place for LAN to LAN, although there are some instances where you can reflect from the router back into the LAN.  But certainly never from WAN to WAN, from the Wide Area Network, the Internet, out to the Wide Area Network.  Turns out there are routers that don't check that, which makes them a perfect target for somebody who wants to hide their traffic as botnets and various attackers want to.  So that was the first instance.



What the Imperva security firm has since discovered is that it is possible, and it is actually being done, that the ports are being changed in addition to the IPs.  Now, that's not a surprise.  NAT has to do that.  That is, technically it's NAPT, that is, not just Network Address Translation, but Network Address Port Translation, because ports are almost always being changed also because the NAT table needs to use additional information for packets coming back to know where they're going.  If three people on the LAN were all going to Amazon, then when the packets come back, they're all coming back from Amazon.  So the destination port number to the router is the way the router knows which one of, in this example, those three people on the LAN that packet should go to because you just need additional bits in order to encode the identity of the machine behind the LAN.  So ports have always been changeable.



What the bad guys have figured out is that one of the easiest DDoS mitigations, the Distributed Denial of Service, where for example you're using a bandwidth amplification attack with DNS or network time, either DNS or NTP, is that the source address of the packets will be that of the service, like 53 or 123, respectively, for DNS and network time.  So one of the easiest things that a DDoS filter can do, a simple way of mitigating these attacks, is simply blocking all incoming port 53 traffic, or all incoming 123 traffic.  That is, it's trivial to do that at the border of the network and just drop all the traffic.



So because the bad guys are clever, they said, hey, this is another thing we can do using our NAT port translation, is we can enlist the services of these accessible NAT routers all over the Internet - and there's apparently on the order of 1.3 million of them available - in order to translate the port number.  And so the idea is they will use the DNS amplification in order to create additional traffic, then have that bounce through one of these NAT proxies and bounce off of the proxy so the traffic stays public, but changes the source port to something other than 53 or 123 in order to bypass static DDoS filters, which many networks now have in place.  So yet another way of using the unsecure and insecure technology in order to create additional traffic.



And specifically, and this is what was worrisome, what Imperva found is that many of these insecure routers actually have the XML file which describes the current mapping, its "rootDesc.xml," is publicly available.  Shodan has been searched for the presence of that file, and 1.3 million results were found.  So you remotely massage that file using the Universal Plug and Play protocol, or directly using that file, and are able to essentially turn these 1.3 million consumer routers into little switchyards, bouncing traffic off of them, changing IPs, changing ports, and getting up to a lot more mischief just due to the numbers which are available, and the fact that increasingly these are all very well connected devices.  So again, attacks only get better.



Also I mentioned a very popular app development platform having a problem.  There's a platform known as Electron which hadn't crossed my radar before because I'm about as far away from developing apps on a web platform, since I'm using assembly language, as you could get.  But Microsoft's Skype, Visual Studio Code, GitHub's Atom code editor, the Brave browser, including well-known desktop apps for services such as Signal, Twitch, Discord, Basecamp, Slack, Ghost, and even WordPress, that, is the desktop services for those are all written as a web app on top of this Electron app development platform.



LEO:  Yeah, it's really common.  It's basically Chrome.



STEVE:  Right, right.



LEO:  That's the negative is you have to bundle a copy of Chrome with every app.  It makes the apps quite large, which is, I know, anathema to you.



STEVE:  Yeah, exactly.  Well, and what's worrisome, Leo, is that the developers understood that the Node.js library, while powerful and deep, is also dangerous to have on the desktop.  So by default there's a setting in the configuration file.  There's a web preferences config file that has node integration set to "false," which blocks access to the Node.js APIs.  I'm sorry about - can you hear this background noise?



LEO:  Yeah, a little bit.  It's not bad.  It's all right.



STEVE:  Okay, good.  I've got the microphone turned away from it.  They're grinding up some trees outside.



LEO:  It's mild.  I hear every once in a while [mimics sound].  It's like "Fargo."



STEVE:  Yes, it's not my stomach growling.  Okay.  So they've got node integration set to "false."  But a security researcher, Brendan Scarvell with Trustwave, discovered that it's possible to flip node integration to "true."  This can occur if another setting in the file, webviewTag, which is normally set to "false," has not been explicitly declared.  In that case it's possible to use a cross-site scripting instance to create an instance which has node integration set to true, and then be able to run an attacker's code on the machine where this, as you said, Chrome essentially HTML JavaScript-based system is present.



He discovered this in March.  He privately reported the flaw to the developers, who immediately fixed it.  He now has finished, published proof-of-concept code which allows an attacker to exploit any cross-site scripting flaw that may exist in one of these applications to extend, essentially give them access to the underlying OS and run their own code.  And remember that cross-site scripting is only - the only thing required is that something that an attacker provides is shown on the page.  That is, that's the trick is you just want to get something that is unfiltered, that has some HTML tags in it that are not being properly encoded.  They will be interpreted by the JavaScript in order to create the exploit, the very common cross-site scripting.



So anyway, the problem is that this has been fixed.  It's been fixed for several months.  Yet there are - and if you look at this, at the /apps, electronjs.org/apps, it is very, I mean, it's incredible how many apps have been written in this.  Not just these mainstream big guys, but lots of other apps are written on the top of this platform, probably because, as you said, it makes it very easy to move something from a web page over to the desktop using this development platform.



So the problem is that, in the long term, if these things are not fixed, if people are not keeping them up to date, even if their publishers keep them up to date, which isn't clear will happen, this creates a vulnerability that could be exploited for a long time to come.  And, yeah, you have the list there.  And, I mean, just it scrolls on and on and on and on.  Many are obscure little special purpose things.  But also the big guys use it, as well.



LEO:  Yeah, I mean, it's a very popular web framework.  Probably not the best.  It's certainly, you know, fat.  But it probably gives you all the functionality you want because you've got a browser in there.  You know how to do CSS and JavaScript, you're set.



STEVE:  Exactly.  Yup.  And unfortunately you don't want to let things get to the underlying desktop unless it's on purpose.  And this provides...



LEO:  So I'm surprised it's not - WhatsApp uses it.  I'm surprised it's not sandboxed.



STEVE:  Well, they deliberately kept it Node.js disabled because there are so many ways that does give you access to the underlying OS.  No, it's just it's a very powerful library.



LEO:  Signal uses it at the desktop, yeah. 



STEVE:  Yes, yes, Signal.  So anyway, I hope that the developers will fix their apps, and people using the apps will keep them current.  What he found is that essentially any vulnerable version of Electron, less than 1.7.13, 1.8.4, or 2.0.0 beta 3 - so they apparently fixed it even after, I guess just before 2.0 got finalized.  Anything younger than that, that has apps built on those, would be vulnerable.  And it's not clear whether you could change the framework underneath the app.  I think that you can't, that it's all bound together.  And so you can't just replace one of the pieces.  You probably have to get a new updated whole app kit from the publisher.



So, okay.  Under the topic of old flaws never die, which of course we see all the time, three days ago was the first anniversary of WannaCry.  Leo, doesn't time fly.



LEO:  Wow, I can't believe it, yeah.



STEVE:  So, and WannaCry was, I mean, its major innovation was that it used and leveraged the EternalBlue exploit, which was believed to have been developed by, come from, and leaked from the U.S. NSA.  And of course it powered the spread of WannaCry, NotPetya, and Bad Rabbit malware.  Then it kind of got quiet.  What was interesting is that, whereas the original EternalBlue only worked with XP, Windows 7, and the equivalent server platform, Server 2008 R2, the underlying flaw in SMBv1, which is what this used, has since then also been made to work under Windows 8, the equivalent Server 2012, and even Windows 10.  This essentially broadening to the entire Windows platform hugely increased the exploit's ability to infect and has basically turned it into a commodity among malware authors.



I have a chart from ESET which shows over the last year, so here is a one-year period of time, essentially the lifecycle so far of the EternalBlue exploit.  WannaCry itself is still active and attempting to find and infect anything that comes online publicly, any systems that come online publicly.  And so we can see here, you have it onscreen now, a big jump in its activity.  Then it got blocked and kind of went silent.  But then, as other platforms have come onstream, and as it's been moved into sort of the default toolkit for exploitation, it's continued to grow and is beginning to approach the original - the exploitation level of activity.  So just as Code Red and Nimda continue their search for new victims, WannaCry itself is active.  But the EternalBlue exploit is being leveraged by an increasing breadth of malware in general.



We talked about these GPON routers that were the optical fiber-based routers being offered by several ISPs.  Naturally, the news got out.  Botnets that were active decided to increase their own activity and jump over to those.  So now, and you'll get a kick out of this, we have the Hajime botnet that we talked about.  



LEO:  Hajime.



STEVE:  Hajime, Mettle, and of course Mirai that has been a lot in the news; one that we haven't talked about, Muhstik; and then also the Satori botnet.  So we've talked about Hajime, Mirai, and Satori, three of those five in the past.  Well, they're now - they've switched their attention to these GPON routers.  And that's really what we're sort of seeing now is that botnets are being taken seriously.  They're being maintained and updated.  And at this time it looks like something just shy of one quarter million routers, that is, there are one quarter million GPON routers.  And when I say "shy," it looks like it's about 240,000 which are vulnerable.



The firm vpnMentor was the discoverer of this GPON vulnerability.  And unfortunately the ISP is very uncooperative.  They're still dragging their heels, denying that there are that many vulnerable routers still present, even though people are scanning for them and infecting them.  I mean, there's botnets running on them.  So they have an unofficial antidote patch.  I don't know whether any of our listeners have GPON routers.  But it's clever.  They have a means of patching the router where - and I've got the link here in the show notes.  It's www.vpnmentor.com.  Under their Tools directory is a GPON router antidote patch.  They caution that it is not official; that of course any official patch needs to come from the vendor and should.  But this vendor is not looking like it's in any hurry.



And Leo, if you scroll down, you'll see a field that you fill in.  Essentially, you give them the internal IP of your gateway and the Telnet password that would allow you to log onto your router.  So what happens is that brings up a page on your browser which is now inside your LAN.  It runs script which connects to your router's Telnet interface on the LAN side, uses the password you gave it in order to then load a patch onto the router.  And I'm sure afterwards they tell you to change the password of Telnet.



LEO:  And you've verified this is entirely safe and completely okay.



STEVE:  Well, again, at this point, if somebody had a GPON, one of these vulnerable GPON routers...



LEO:  You're no worse off, are you, I guess.



STEVE:  Exactly.  You're in bad shape already.  Your ISP is apparently...



LEO:  They didn't really - they didn't need you to do this.  They could do it themselves.



STEVE:  Exactly.  Exactly.  So probably the best course of action would be to say, ah, yeah.  And these guys appear to have their hearts in the right place.  So, yeah.  To the degree we can trust anyone, these guys seem about as trustworthy as you could get.  Or, again, what choice do you have?



LEO:  This wood chipper's getting louder.



STEVE:  I know.



LEO:  Are they getting closer?



STEVE:  I think the trunks are getting larger.



LEO:  That's probably right.  They were doing the little skinny branches.  Now they're doing the big boys.  Well, we'll just live with it.  It's fine.



STEVE:  Yeah.  So in this week's, as I described it at the top of the podcast, this week's shocking insecurity headshaker, it's like, what year is this?  Is this 1995?  Bleeping Computer reported the news of, get this, 5,000 routers with no Telnet password, exposed to the public.  It's just incredible.



Researchers with NewSky Security, which is a cybersecurity company specializing in IoT security, discovered that the exposed devices were Datacom routers from a Brazilian ISP, Oi Internet, which Oi Internet had provided to their customers.  There were three of these Datacom routers, DM991CR, DM706CR, and DM991CS.  They were found to have blank Telnet authentication, with the Telnet port wide open to the Internet, accepting all comers.  The researcher who discussed this with Bleeping Computer's reporter said that the routers' manuals clearly indicate that the devices are designed to come with a passwordless enabled Telnet service by default, and that users are then expected to configure the password for themselves.



So I'm just, like, how does this happen in 2018 that, first of all, a manufacturer can offer a router with an unconfigured Telnet service running on the WAN interface with no password?  How does it happen?  That's just - I'm stunned that that could be the case.  But I just, you know, I guess maybe they weren't meant to be consumer routers; or the presumption is, because they were OEMed, the presumption was that any OEM would of course turn the service off or lock down, just turn Telnet off completely on the WAN side, or provide a good username and password.  I just, again, somehow, some miscommunication.  But, wow.  Unbelievable that, I mean, imagine buying a new router which no consumer has any idea what to do with.  They just plug it in and assume it's an appliance.  Yet it's got Telnet wide open.  Wow.



Okay.  One more, then we'll take our second break.  I have a link here to the PDF that I put a copy of on GRC's server since you had to go through some rigmarole in order to get it, and I didn't want to ask all of our listeners to do that.  And it's not behind a pay wall or anything.  And they've got all of their advertising all over it.  This was the highlights of the security form from the recent RSA security conference.  And I got a kick.  Down lower are a bunch of pie charts.  But get this.  Only 47% of organizations patch vulnerabilities as soon as they are known, so less than half of organizations patch immediately; 16% wait for one month; while 8% admitted to only applying patches once or twice a year.



So, I mean, this is good news for bad guys.  And these are people attending the RSA Conference.  You could argue that this is not even a representative cross-section of enterprises.  This is RSA security conference attendees saying, eh, yeah, you know, half of them don't do them as soon as they're known; 16% wait a month; 8%, eh, we get around to it once or twice a year.  Wow.



Okay.  Also 16% of organizations have admitted - now, this is admission on a survey at RSA - 16% of organizations have ignored a critical security flaw because they didn't have the skills to rectify it; while 26%, okay, one quarter have ignored a critical security flaw because they didn't have time to fix it.  It's like, yeah, okay.  Critical, but we're busy.  We're busy here.  One in four firms.



When asked what route they would take to hack their own companies - and I thought this was a really great question.  When asked what they would do to hack their own companies, 21% said they would enter through the company's public cloud hosted computing; while 34%, so one in three, said they would use social engineering if asked to hack their own company.  When asked if they expected that attack would be successful, 71% said it was likely or highly likely that they could succeed from outside hacking into their own company.  Only 9% said it was very unlikely their attack would succeed.  And this is where I also mentioned that three out of four of the organizations use a commercial cloud provider as part of their infrastructure.



And then, finally, only 17% of organizations - and these tend to be sizable, you know, they're RSA attendees - have ever hired a penetration tester, a pen tester, to assess, objectively externally assess the security of their own networks.  So of those 17%, 46% found a critical flaw which could have put their organization at risk.  Which I would imagine our listeners do not find surprising.  However, 35% believe that, if they were to hire a pen testing service, although they haven't, they would not surface any new risks.  So to me that sounds like CIO or CTO hubris, where they're like, oh, no.  I'm in charge of security.  We haven't hired anybody.  And if they did, it would just be a waste of time and money because we're secure.  Right.



LEO:  They've chipped all the trees.



STEVE:  I think they're done.  They've ground it up into dust.



LEO:  Suddenly the view is of crystal clear skies.



STEVE:  So BitDefender Labs has identified the first persistent IoT malware.  We've talked many times about how these infections just live in our routers' RAM, and that just a power down and reset, reboot, power back up, whatever, is enough to clear it out.  Well, turns out that was then.  BitDefender Labs has found what they call the "Hide and Seek" botnet.  They first discovered it earlier this year and have had their eyes on it, tracking its evolution and progress.  It's infected close to 90,000 unique devices, not only routers, apparently IPTV cameras.  And as I mentioned, what we're seeing now is just much more sophistication in botnet seriousness than previously.



One of the things that caught their eye is that this botnet establishes a peer-to-peer command-and-control network using UDP with a fully custom homegrown peer-to-peer protocol.  So this creates a mesh of connected botnets which creates much more connectivity than we normally see.  Normally, as we know, botnets all refer back to some command-and-control server somewhere, which makes them easy to take down, just by taking that one server offline.  Suddenly they're all sitting around waiting to get instructions, and none are forthcoming.  Here, by creating a mesh of interconnected botnets, all you have to do is talk to any one, and this will propagate the command-and-control system throughout that highly interconnected mesh, which makes it virtually impossible to take this down.  So this is a big step forward.



Also, unlike the previous botnets which lived in RAM, this one uses the various brute-forcing of Telnet access to get root.  And now, even though previous infections, various malware might have been able to get root, it wasn't using the root privilege to write itself into persistent storage.  This Hide and Seek botnet does.  It gets into Telnet using brute force, identifying devices using a bunch of well-known default admin usernames and passwords.  And we've talked about how effective those can be in the past.  Once it has that, it copies itself into the /etc/init.d directory, so it gets run whenever this device reboots.  And this includes routers and IPTVs.  It also has 10 different binaries compiled for various platforms, including x86, x64, the 32- and 16-bit Intel chips, ARM, both Little Endian and Big Endian, SuperH, and PowerPC, among others.



So a lot of work has gone into putting this thing together.  And as I said, it's now installing itself permanently into the firmware of the devices it infects and establishing a persistent and very hard to kill UDP communication mesh between it and all the others.  Oh, and when it gets installed, it then begins scanning its neighborhood for other infected botnets.  It also closes the door behind it so that nobody else, no other malware is able to follow it in through the Telnet port in order to infect that device.  So, I mean, this is what you would design if you were very competent, and you wanted to create a persistent malware network out on the public Internet.  And it actually exists.  It's not science fiction.



Okay.  As I mentioned at the top of the show, I wanted to address a misconception.  And I want to thank, it looks like Ely Riggs in Tallahassee.  And the subject was VCC Citi, and I didn't know if that was an abbreviation for Vatican because of course Father Robert shared with us the fact that SpinRite was now being used at the Vatican for...



LEO:  Oh, that's neat.  I didn't hear that.  That's great.



STEVE:  ...recording hard drives, yeah.  He left his copy there with an associate, and it's been blessed now.  We had made it up to the altitude of the International Space Station.  Now we've made it up to an even higher altitude.



LEO:  I think it's even higher, yes, yes.  That's great.



STEVE:  So anyway, so this person writes,  "Hey, Steverino.  Great show, as always.  Your SpinRite testimonial was incomplete.  After repairing an unbootable drive," and then we have in all caps shouting, "BUY A NEW HARD DRIVE, or upgrade to an SSD.  Do not continue using a failing hard drive."



So I wanted to take a minute to address that because, first of all, I absolutely agree you should not continue using a failing hard drive.  But an unbootable drive, or a drive that SpinRite is able to repair, doesn't mean the drive is failing.  We need to remember, first of all, drives have a huge backup of spare sectors.  Why do they have spare sectors?  Because in the normal course of their service life, they're expecting sectors to fail.  And when sectors fail, the drive says whoops and essentially  takes that sector out of service, moving that sector's data into one of the spares in the pool of spares.  Sectors fail over time just through the actual - the head flying over the surface is flying very closely, and there is a tiny mechanical stress that the head and the surface platter put on each other.



So there's a little bit of flexure; and, over time, that can evolve a defect.  What normally happens is the drive will see that there's a problem forming.  But again, remember that drives are performing error correction now all the time.  And the use of ECC used to mean, oh, my god, thank goodness we're able to recover the data from the sector.  Stop using the sector.  Now ECC is, yeah, okay, fine, we corrected another error.  The idea is that...



LEO:  Just keep on going.



STEVE:  Yeah, yeah, exactly, exactly.



LEO:  I would imagine that some of these 4- and 8-terabyte drives are probably getting ECC failures pretty constantly; yeah?



STEVE:  Yes.  And in fact that's one of the things that SpinRite shows is the rate of error correction.  It's not zero.  It's startling.  And so what really happens is when the rate of error correction begins to increase, that's an indication that you've got a problem with your drive.  But what's so cool about ECC is that what the error correction math creates through this cool ECC technology, it creates an XOR bitmask which is a pattern of ones where the bits are wrong, and an offset for where in the sector's typically 4,096 bits, where the mask should be applied in order to flip the wrong bits to make them right.  That's called the "syndrome."  And so naturally the first bit of the syndrome is a one, and the last bit of the syndrome is a one.  Otherwise you'd remove the zeroes, and then the first bit would be a one and the last bit would be a one.



The point is the distance between the first one and the last one is the number of bits in error.  And the drive will watch that and happily correct them until some threshold because there's a limit to how long a run of error it's able to correct.  So as the problem grows over time, at some point the drive says, uh-oh, we're past a threshold of comfort.  So while I can still correct the sector, I'm going to correct it one last time and take it out of service.  Now, that's the "everything going well" scenario.



Of course SpinRite is typically brought in when the "everything going well" scenario fails.  And so that's where SpinRite comes in with the just give us the data one last time.  Please, just once more.  We'll never ask for it again.  Just one more time.  And it often does, in which case it and the drive breathe a huge sigh of relief, the sector is taken out of service, and the corrected data is relocated.  The point being this isn't the failure of the drive.  This is a sector that was allowed to get outside of the drive's correction tolerance band.  But once fixed, everything is fine.



So it's, again, SSD is using and relying on error correction.  Hard drives are using and critically relying on error correction.  I mean, we couldn't have, I mean, there just aren't drives that aren't correcting errors now all the time.  Some of them are soft.  Some of them, if you retried, you would get a perfect read, except the drive wants to be fast rather than safe.  It's biased in that direction.  So it just goes ahead and says, okay, I had to correct the data.  Maybe if I tried again I wouldn't.  But I have correction able to do on the fly without slowing down, so I'm going to just keep going.  So, you know, that's what's happening.



So anyway, I just sort of wanted to note that a failure that SpinRite corrects can just be a sort of a transient problem in one spot where something got a little bit out of control on the drive.  But the drive has plenty of spares and plenty of service life remaining.  SpinRite, again, running SpinRite from time to time keeps all of this from getting to the point where you may not be able to recover it.



Okay.  And from Eric Paul in Chesapeake, Virginia, and this was the guy that I referred to at the top as his subject line is "Still need HTTP sites."  And he wrote:  "I just heard you on Security Now! that there is no longer a need for HTTP sites.  I have an issue with that statement.  At my house I have an old Netgear router that supports HTTPS access.  But due to its age, it uses a self-signed SSL3 cert."  And actually certs don't have a protocol.  There's no such thing as an SSL3 cert.  I'm assuming he means, for example, an SHA-1 cert, which is probably what he's referring to.



"Since modern browsers have decided that they will absolutely not allow access to SSL3 sites" - and actually he means SHA-1 signed certs - "I had to use an old version of IE" - that would do it, that is, SHA-1 - "which still allowed" - he says SSL3, he means SHA-1 - "certs to remove HTTPS-only access so that I can use a modern browser to access the router."  Oh, I see.  So he used an old version of IE to turn off HTTPS-only on his Netgear, then allowing him to use HTTP only on modern browsers.  So that was nicely done, Eric.



He says:  "Due to this reason, I had to convert both of my routers to HTTP-only access since I assume that at some time in the future all browsers will punish the self-signed certs used by both of those routers with no exceptions allowed."  So I take his point.  And I, and I imagine a bunch of our listeners, are beginning to encounter things like that, where this assumption by browsers and enforcement by browsers of the latest security standards are beginning to collide and clash with some of our older devices that aren't updateable and don't support the latest security protocols.



So anyway, I just sort of wanted to thank Eric for noting that.  And I'll mention that I did mention that a recent Asus router of mine, I was delighted to see that it was obtaining its own Let's Encrypt certificate for itself.  I thought that was a very cool use of online real-time certificate generation.  But I have encountered myself older devices where it is necessary to jump through some hoops in order to access them because newer browsers just say no, or cough up a whole bunch of errors that you have to, like, fight your way through in order to still use what is older security.  But in this instance, for example, there's zero danger in using HTTP to access the management of your own router on your own LAN.  It's just like, you know, there's just nothing wrong with doing that.  Yet the browser says no, no SHA-1 certs are allowed.



Okay.  So the two ultra-clever attacks.  And Leo, on the second page here - or no, it's the third page down - I have a picture that captures this that I will be explaining, but you're going to want to show that one when we get to what this is.  So the first one is the PGP and S/MIME.  Okay.  So it's not a new vulnerability.  It's a brilliantly clever leveraging of an original design flaw in encrypted email which affects encrypted email, thus PGP and S/MIME.  The EFF wrote, and I'm quoting them:  "Users are advised to disable email encryption plugins to avoid any attackers from recovering past encrypted emails after the paper's publication."  And that did happen.  It was supposed to happen today; but news leaked out, and so it was published yesterday.



The EFF wrote:  "These steps are intended as a temporary conservative stopgap until the immediate risk of the exploit has passed and been mitigated against by the wider community."  They said:  "Users in dire need of using encryption to protect their communications channels are advised to use an instant messaging client that supports end-to-end encryption, the EFF recommended."  And then in a follow-up the EFF said:  "Not So Pretty:  What You Need to Know About eFail and the PGP Flaw."  And again, it's not a PGP flaw.



They wrote:  "A group of researchers released a paper today" - this was yesterday - "that describes a new class of serious vulnerabilities in PGP (including GPG), the most popular email encryption standard.  The new paper includes a proof-of-concept exploit that can allow an attacker to use the victim's own email client to decrypt previously acquired messages and return the decrypted content to the attacker without alerting the client.  The proof of concept is only one implementation of this new type of attack."  And that's the key.  This is a new attack.  This is not a new vulnerability.  "And variants may follow in coming days."  So the site where this is all put up for anyone who's interested is eFail, E-F-A-I-L dot D-E.



Okay.  And so the ultra-short attention-getting version is an attacker who had previously obtained any encrypted email, that is, someone's encrypted email, through for example passive monitoring, or maybe by pulling from an encrypted stored repository.



LEO:  Encrypted, not cleartext, the encrypted version.



STEVE:  Encrypted, yes.  So if there's email sitting on a server somewhere, or on a cloud somewhere, which is encrypted so that only the user's client is able to see it, they are able through this incredibly clever, I mean, just, as I said, this is just going to - this is just amazingly cool, or as Matt Green called it, a "masterpiece" - can induce the recipient's, that is, the intended recipient's email client to decrypt and exfiltrate, that is, to send the decrypted content.



Okay.  So a little more background first.  Matthew Green, on May 14th, yesterday, tweeted a series of, like, nine tweets.  He said, "New vulnerabilities" - and he gets it right - "in many GPG and S/MIME enabled email clients allows exfiltration of plaintext" - meaning decrypted content - "by mauling HTML emails."  And, he says, "A few thoughts," which then follow in these following tweets.



He says:  "In a nutshell, if I intercept an encrypted email sent to you, I can modify that email into a new encrypted email that contains custom HTML.  In many GUI email clients, this HTML can exfiltrate the plaintext to a remote server.  Ouch," he concludes Tweet No. 2.



Then he continues:  "It's an extremely cool attack and kind of a masterpiece in exploiting bad crypto, combined with a whole lot of sloppiness on the part of email client developers.  The real news here," he writes, "is probably about S/MIME, which is actually used in corporate email settings.  Attacking and modifying encrypted email stored on servers could actually happen, so this is a big deal.  Plus the attack on S/MIME is straightforward because it's," he says, "(A) a dumb protocol; and (B) a simple protocol not filled with legacy cruft; and (C) it's built into email clients."



He says:  "Dumb and simple and one vendor" - that is, your client, your email client vendor - "to blame.  But," he says, "of course the attack also implicated the garbage fire that is the PGP ecosystem.  So of course that's what everyone is talking about."  He says:  "Over on HN the 'it's not PGP, it's mail clients' dance has begun, so I guess we have to talk about that."  He says:  "When it comes to PGP, the quality expectations on the crypto are low because it was invented in the Precambrian era" - yes, 27 years ago, and Phil Zimmermann was way ahead of his time, but things have changed.  PGP hasn't.



He says:  "It was invented in the Precambrian era, so it doesn't do proper authentication except as an optional afterthought.  So in summary, PGP clients are vulnerable because 17 years after a vulnerability was known, the mitigation was not made a default in GnuPG, and defense was instead 'left to PGP clients,' which also make a convenient scapegoat when it goes pear-shaped."



Okay.  So what he's referring to 17 years ago is that we learned that it was necessary to not only encrypt, but to authenticate.  And we've talked about that often on this podcast.  It is not sufficient to encrypt.  You also have to absolutely guarantee against modification.  And that's what PGP is missing.  And as a consequence, the mistake made by GUI email clients can occur.



Okay.  So get this.  This is just so toe-curlingly cool.  Email wanted to be able to contain more than just hello, some text, and goodbye.  You wanted to have attachments.  You wanted to have images.  You wanted to do more than just a simple plaintext thing.  So that created MIME, M-I-M-E, which is an extension.  First of all, MIME stands for Multipurpose Internet Mail Extensions.  And it allows multipart email where you have well-defined boundaries marking the beginning and end of sections.



Okay.  So get this.  This "mauling," what a bad guy does is they create a new email where the first section is the beginning of an HTTP image tag.  So they do an open bracket IMG, space, and then SRC, where you're going to specify the source of the image.  And then they use HTTP://evildomain, right, because this is the domain that is going to receive the decrypted email.  Then they do a forward slash and end that section, that is, they leave the image tag source URL open.  Then, as the second part of this new multi-part email, they drop in the original encrypted email that they can't read.  It's encrypted, strongly encrypted by PGP or S/MIME, whatever.  And then they close that second section.



Now, the third section is simply a closing quote on the URL and the closing angle bracket for the image tag.  So what this is, is they've essentially created a new piece of email which is nothing but an image tag with a prepended domain to receive the email, and the encrypted body of the email as the URL of this image tag.  So this is sent to the client.  The email client says, oh, in order to display this, I need to decrypt this middle part, which is encrypted with PGP, or S/MIME, whatever.  It decrypts it, and now you have all in plaintext an image tag where the URL from the root of the URL is itself the decrypted plaintext message.  The client will URL encode it.  So, for example, a space character, which is not URL safe, will get turned into a %20, for example, and so forth.  And it will then query evildomain.com for the contents of the image, that is, to display the image.



LEO:  That is cool.



STEVE:  Isn't that cool, Leo?



LEO:  Yeah, that's very cool.



STEVE:  Oh, gosh, yes.  And so what happens is evildomain.com has a query coming in to it to have an image displayed by the user's client.  And the URL is the decrypted email.  Oh, it's just sublime and gorgeous.



LEO:  I do think, though, that it's a stretch to blame PGP and S/MIME.



STEVE: Oh, agreed.  It's not - no. 



LEO:  That's bad behavior on the email client's part.



STEVE:  Correct.



LEO:  Conflating these three parts.



STEVE:  Although had PGP been authenticating, it would have recognized that the envelope had changed and then refused to decrypt.



LEO:  Right.  Which it should have done, obviously.



STEVE:  Yeah, yeah.



LEO:  But this is just bad behavior.  In fact, the email client I use for the most part, Claws Mail, doesn't do this because it doesn't do HTML.



STEVE:  Correct.  And that is the advice is you want to use a client that will not do HTML because then it will not look at this image tag and go, oh, I need to go find the image.



LEO:  Oh, oh, I've got to render it, oh.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  So to conclude, the EFF says, under what to do about this, they say, and I think they're correct in this:  "We are in an uncertain state, where it is hard to promise the level of protection users can expect of PGP without giving a fast-changing and increasingly complex set of instructions and warnings.  PGP usage," they write, "was always complicated and error prone.  With this new vulnerability, it is currently impossible to give simple, reliable instructions on how to use it with modern email clients.  It's also hard to tell people to move off using PGP in email permanently.  There's no other email encryption tool that has the adoption levels, multiple implementations, and open standards support that would allow us to recommend it as a complete replacement for PGP."



They say:  "(S/MIME, the leading alternative, suffers from the same problems and is more vulnerable to the attacks described in the paper.)"  They conclude:  "There are, however, other end-to-end secure messaging tools that provide similar levels of security:  for instance, Signal.  If you need to communicate securely during this period of uncertainty, we recommend you consider these alternatives."



So anyway, very cool, I mean, mostly just sublimely clever attack.  The idea of subverting image rendering on a client handling encryption to induce it to decrypt on the fly and then get the decrypted text exfiltrated via the URL of a remote image is just so cool.  So that's really why I wanted to share this was I was like, just, wow, you know, very clever.



Okay.  And so yes, Leo, I think that the takeaway is, for people who want to use email - so the danger is, just to explain it, if there exists the ability of an attacker to obtain or to have obtained encrypted content, the danger is that now they will create a new piece of email and send it to someone whose HTML-enabled PGP or S/MIME or it doesn't really matter what encryption will attempt to render the image and in the process send the cleartext back out.  So anyone can disable that, prevent that by using a client to view email that will not render HTML.  In which case...



LEO:  Problem is everybody, every participant in the email has to be using that client.  So there is a weak link.  If there are multiple recipients, I'm not sure...



STEVE:  True.  Good point, yeah.



LEO:  That's a weak link.  And Matthew Green points this out.



STEVE:  Good point.



LEO:  The attacker can perform eFail attacks if only one of the participants is vulnerable.  Now, what is unclear to me, maybe you could explain it, is he implies that even if a sender is vulnerable - doesn't it have to be that somebody with the private key would be, you know, somebody who could decrypt the email would have to be vulnerable.  Because otherwise, I mean, sender can't decrypt. It can only encrypt it; right?



STEVE:  Yeah, I agree.  I don't see how the sender would be vulnerable.



LEO:  It implies somehow - but I think really, if I'm understanding this, really only somebody with the ability to decrypt the email.



STEVE:  Correct, correct.



LEO:  So that person...



STEVE:  Because you are leveraging the client's own decryption, yes.



LEO:  Right, right.  However, if multiple recipients are intended, and you've encrypted with multiple public keys, then I guess you could then use one of the recipients' vulnerable email.  So everybody should just stop using HTML email, which I've said for years.  One more reason to hate HTML email.



STEVE:  Oh, boy.



LEO:  I'm sure PGP will be fixed quickly on this.  I would expect.  No?  Let me go look at GPGTools because that's what - I don't use PGP, I use the Open GNU Privacy Guard.  So that means there's - this is in constant development.



STEVE:  The problem is this - yeah.  Assuming that Matthew's solution, which is to say to add authentication, the sender needs to append, needs to add authentication, and the client needs to verify.  So it's difficult, I mean, that's why it hasn't happened yet, even after 17 years.



LEO:  The GPGTools people say that the next version of the suite, 2018.2, will include mitigations against this vulnerability and will be released this week.



STEVE:  Good.



LEO:  So this is the one I recommend anyway, that people use GPGTools.  And I presume that means - this is for the Mac.  But I presume that means that GPG is also being fixed.  And of course the other fix is to not use your email client to read your mail.  But again, all intended recipients would have to be doing this.



STEVE:  Let's see.  Actually, don't use your...



LEO:  Use the command line.



STEVE:  Oh, okay.



LEO:  You see what I'm saying?  So I'm going to get a blob, an encrypted blob in my email client, take it, and decrypt it off to the side.  Don't give the email client access to the decrypted version.



STEVE:  Right, right.  Or actually, don't give anything that renders HTML access.  So, like, even if...



LEO:  Right, right.  So put it on the - you could do it in the command line.  That would be safe.  Or put it in the clipboard, and you can - in most cases GPG can decrypt the clipboard.  Maybe that would be risky, too.  Put it in the command line.  Save it as a text file and decrypt it with a command line.



STEVE:  And keep it away from your browser because of course browsers were the first things to do HTML.



LEO:  Right, right, right.  Wow.



STEVE:  Yeah.  So Rowhammer becomes Throwhammer.  Clever renaming.  I like that a lot.  And this is from some researchers from universities in Amsterdam and Cyprus that have very cleverly, I would say "invented," a brand new remote means for launching Rowhammer attacks via network packets and network cards.  I can just imagine them sitting around brainstorming new ways to pound on RAM.  And they realized that, in one of those aha moments, very much like whoever it was who figured out this incredible hack for PGP or encrypted email and using HTML rendering and multipart messages, these guys realized that the latest and fastest network connections were employing a technique known as RDMA.  And it's actually rather widespread in high-end networking.  As you might imagine, it stands for Remote Direct Memory Access.  It turns out that's what all cloud providers and high-bandwidth networking solutions use.



RDMA has a Wikipedia page, lots of resources on the 'Net.  And I'm going to share from their paper because they couch this, I mean, they explain this nicely, and also explain what they did.  They said - and I'm just jumping in the middle.  Now they're talking about the history.  They said:  "However advanced the attacks have become" - that is, previous Rowhammer attacks - "and however worrying for the research community, these Rowhammer attacks never progressed beyond local privilege escalations or sandbox escapes."  And that's of course true.  That's what we've been talking about.



They write:  "The attacker needs the ability to run code on the victim machine in order to flip bits in sensitive data.  Hence, Rowhammer posed little threat from attackers without code execution on the victim machine."  And that's of course how we were, to some degree, keeping from staying awake at night and not worrying too much about what was happening.  They say:  "In this paper we show that this is no longer true, and that attackers can flip bits only by sending network packets to a victim machine connected to RDMA-enabled networks commonly used in clouds and data centers."



They say:  "Rowhammer allows attackers to flip a bit in one physical memory location by aggressively reading or writing other locations, i.e., hammering the memory.  As bit flips occur at the physical level, they are beyond the control of the operating system and may well cross security domains."  Of course we've covered this extensively on this podcast.



"A Rowhammer attack requires the ability to hammer memory sufficiently fast to trigger bit flips in the victim.  Doing so is not always trivial as several levels of caches in the memory hierarchy often absorb most of the memory requests.  To address this hurdle, attackers resort to accessing cache eviction buffers or using direct memory access (DMA) for hammering.  But even with these techniques in place, triggering a bit flip still requires hundreds of thousands of memory accesses to specific DRAM locations within tens of milliseconds.  As a result, the current assumption is that Rowhammer may only serve local privilege escalation, but not be used to launch attacks from over the network.



"In this paper we revisit this assumption.  While it is true that millions of DRAM accesses per second is harder to accomplish from across the network than from code executed locally, today's networks are becoming very fast.  Modern NICs [Network Interface Controllers, LAN adapters] are able to transfer large amounts of network traffic to remote memory.  In our experimental setup, we observed bit flips when accessing memory 560,000 times in 64 milliseconds, which translates to 9 million accesses per second.  Even regular 10-gig Ethernet cards can easily send 9 million packets per second to a remote host that ends up being stored in the host's memory."



They ask the question:  "Might this be enough for an attacker to effect a Rowhammer attack from across the network?  In the remainder of this paper, we demonstrate that this is the case, and that attackers can use these bit flips induced by network traffic to compromise a remote server application.  To our knowledge, this is the first reported use of a Rowhammer attack over the network.  Specifically, we managed to flip bits remotely using a commodity 10-gig network.  We rely on the commonly deployed RDMA technology in clouds and data centers for reading" - and of course that's the ideal target for this - "for reading from remote DMA buffers quickly to cause Rowhammer corruptions outside these untrusted buffers.  These corruptions allow us to compromise a remote server without relying on any software bug."



So again, you couldn't get a better example of attacks never get worse, they only ever get stronger.  We have now Throwhammer, which uses the fact that we've got very high-speed connections to servers, and those servers being the ideal targets as victims, being the recipient of bit flips that can then be used to get up to mischief.  Like, for example, we saw, if you could flip the bit on memory access permission tables, you then immediately give a benign process running on that machine, but without any code execution capabilities, suddenly it gets access to all of memory on the physical computer.



So what this necessitates downstream will be that the communications buffers used by the NICs can no longer be in main memory.  They cannot.  They're going to have to be sequestered into memory, maybe static memory, maybe on NIC memory.  Something's going to have to be done.  Or the DRAM is going to have to be hardened so that it is no longer subject to Rowhammer attack.  So another beautiful piece of work, and something that was a local-only attack now becomes networkable.  So beautiful work; and, again, another piece of really nice engineering.



LEO:  Nice.  Throwhammer is a good name, too.



STEVE:  It's great.



LEO:  Of course that's very important, to have the right name.



STEVE:  Got to have a good name and a good website.



LEO:  Yeah.  I was looking at the email client I use on the Mac, MailMate.  And he said I was notified February 10th by Matthew Green and company, and mitigated mid-March.



STEVE:  Nice.  It's been out there for a while.



LEO:  It's been out there.  And so one of the problems with that list that EFF has published and Matthew published is that it's on older versions of the software named, including MailMate.  It's on a December edition, and he had fixed it by March.  So you should just...



STEVE:  So update your clients and see whether the clients have already mitigated it themselves.



LEO:  He said:  "At the time I didn't put any real information in the update notes, the release notes, because I didn't want to telegraph what was going on.  But I can tell you now that that was a mitigation for eFail."  So one hopes that a lot of these clients would have handled it by now.  Otherwise just keep using Mutt.  You know, it works.  It's good.  Harmless.



STEVE:  Yes.



LEO:  Mostly.  Steve Gibson is the guy in charge of this whole kettle of fish, and we're glad he's here.  You can find more about Steve at GRC.com.  That's his website where you'll also find SpinRite, the world's best hard drive recovery and maintenance utility, and all the free stuff, tons of which is on the site.  You could spend days just browsing through the site.  But among other things you'll find this podcast there, not only audio, but also transcriptions of every word, beautifully carved into stone by Elaine Farris.



STEVE:  Lovingly transcribed.



LEO:  Lovingly transcribed.  So go to GRC.com.  The transcriptions mean you can also search there for anything in any of the previous 600-some episodes.  Have you hit 666?  TWiT did.  Oh, we've got three more before the...



STEVE:  We're at 663, yeah.



LEO:  ...Mark of the Beast.  All right.  Coming soon to a podcast near you.  You can get audio and video of the show from our website, TWiT.tv/sn.  You can also subscribe.  Or one thing a lot of people like to do is watch live and chat about it in the chatroom.  There's always a lot of great back talk going on, and it's a great way to get additional information about the subjects we cover.



Here's the deal.  We do the show about 1:30 p.m. Pacific on Tuesdays, that's 4:30 Eastern, 20:30 UTC.  The chatroom is at irc.twit.tv.  The live stream is at TWiT.tv/live.  So enjoy.  That's a great way to consume it.  But even if you do it that way, you do want to subscribe so you'll have every episode on your Security Now! bookshelf.  And you can do that in any podcast program.  There are some written in Electron.  You know, Electron, you just like automatically have hundreds of megabytes of support files, just like automatically.



STEVE:  Wow, wow.  Horrible, horrible.



LEO:  A lot of people don't like the idea of having Electron apps because each one has its own copy of Chrome.  It's like installing Chrome Plus.  All right.



STEVE:  Okay, my friend.



LEO:  Thank you, Steve.  Have a great week.



STEVE:  Talk to you next time.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#664

DATE:		May 22, 2018

TITLE:		SpectreNG Revealed

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-664.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine the recent flaws discovered in the secure Signal messaging app for desktops, the rise in DNS router hijacking, another seriously flawed consumer router family, Microsoft Spectre patches for Win10's April 2018 feature update, the threat of voice assistant spoofing attacks, the evolving security of HTTP, still more new trouble with GPON routers, Facebook's Android app mistake, BMW's 14 security flaws, and some fun miscellany.  Then we examine the news of the next generation of Spectre processor speculation flaws and what they mean for us.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about including, yes, the new Spectre variants.  We finally have the details.  Steve will talk all about that and what you can do about it and what the mitigation's going to cost you.  We also will talk about a new attack on your Amazon Echo device or your Google Home device.  Steve and I may disagree about the seriousness of that one.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 664, recorded Tuesday, May 22nd, 2018:  SpectreNG Revealed.



It's time for Security Now!, the show where we talk about your privacy, your security, your health and happiness and well-being on the Internet with Mr. Steve Gibson of the GRC Corporation.  Hi, Steve.



STEVE GIBSON:  Yay, Leo, great to be with you again, as always.



LEO:  Always a pleasure.



STEVE:  Yeah, we scrambled here a little bit at the end because I was apparently watching a tape-delayed version of MacBreak Weekly and didn't - I thought, wow, they're kind of running late.  It's like, uh, no, [crosstalk].



LEO:  [Crosstalk] half an hour ago.  No, but I also figured you had some work to do on this new Spectre flaw.



STEVE:  Well, yeah.  Two weeks ago the news leaked through Heise.de, the German magazine, about this having been discovered.  And they did their due diligence to verify it.  And so two weeks ago the title of the podcast was "Spectre Next Generation."  Last week I mentioned that, okay, we were still waiting, we're still waiting, we're still waiting; and all the news dropped yesterday.  So today is SpectreNG Revealed.



LEO:  They were presumably waiting to disclose until companies had patched and things like that; yeah?



STEVE:  Yes.  And even so, I mean, this is - what we heard was eight new problems, one of which was a big concern.



LEO:  Right.



STEVE:  I can't square that with the news of two because all we got is two new problems.  But they are problems.  And every company, essentially every processor, in this case even the IBM power chips are vulnerable.  I mean, this is a fundamental - I think the thing that makes this so interesting and such a concern and not like oh, just do a patch is it's a consequence of the fundamental architecture which everybody in the industry has adopted in order to get more performance, in order to squeeze every last bit of speed out of the existing technology that we have for non-quantum single instruction at a time.



We've done all kinds of things like running multiple cores at the same time, sharing caches so that they share memory, and then going ahead.  Like for example it's not - if you run across a branch instruction, you take both forks of the branch.  It's like, what?  Well, it's because you may not yet know if you're reading ahead what the outcome of earlier instructions are that would determine whether the branch is taken or not.  But if the processor has extra resources that are not being used, why not let it run ahead and then discard the results of the branch that wasn't taken, thus speculating on its own future?



And it turns out that once - it was Jann Horn at Google's Project Zero who first sort of got this glimmer of, is this really secure?  And this of course then - this was last summer this happened, that this all began, when he realized, ooh, boy, because this modifies the state of the processor, it breaks trust boundaries.  It breaks security boundaries between processes and between the OS and its client applications.  So this is big, and it continues to give us interesting things to talk about.  Which is cool, too, because it's like fundamental technology, not, oh, some router had a default password that they shipped.  Of course we have some of that this week, too.



We're going to examine the recent flaws discovered in the secure Signal messaging app for desktops, the rise in DNS router hijacking, another seriously flawed consumer router family, Microsoft's Spectre patches for the earlier Spectre problems, which appeared just the day after last week's podcast, last Wednesday, for the April 2018 feature update.



A really interesting thing I know you're going to find fascinating, if you haven't already read into it, the threat of voice assistant spoofing attacks.  An interesting paper was released where some researchers did a very convincing job of getting the wrong malicious app to run for the Amazon and Google devices.  Also the evolving security of HTTP as being driven by Google and the strength of their Chrome browser.



Still, believe it or not, more new trouble with those GPON routers.  A mistake that Facebook made with their Android app that upset a lot of people.  BMW's 14 security flaws.  Oh, yes.  Some fun miscellany, and then we'll take a look at what we just learned yesterday about variant 3a and 4 - we already had 1, 2, and 3, now we have 3a and 4 - and what that means.  And if you really want protection, get this:  an 8% hit on processor performance.  So ouch.



LEO:  Yeah, yeah.



STEVE:  So I think lots of good stuff to talk about.



LEO:  All right, Steve.  On we go.



STEVE:  Our Picture of the Week is one that I've had in our log of pictures to get to, since there was nothing particularly that jumped out at me about this week's topics.  But it's something that we've all experienced often.  It's a three-frame cartoon with a laptop that is shown saying, "There's a new update, but there are a few unsaved projects, and you are gone for like five minutes now.  May I update and restart anyway?"  And then the center frame sort of shows the laptop kind of waiting.  And then finally, the third frame, "I'll take that as a yes."  And of course it restarts and loses the unsaved data and so forth.  So yes, the nature of automated software updates in the current PC era.



Okay.  So the Signal protocol is secure.  As far as everyone knows, Moxie and company have a lot of experience with security and security protocols.  It's been looked at closely.  We've done a podcast about exactly how it works.  And as I was getting into it, I remember thinking, wow, this is really kind of overdesigned.  What?  But then as I've understood the set of features which they wanted the solution to have, the cleverness of what they had done was clear.  And of course I shared that on our podcast about the Signal protocol at the time.



So that's different than mistakes being made in the implementation of an application which implements that protocol.  So anyone who has the Signal desktop app for Windows and Linux needs to make sure that you're running the latest, which is 1.10.1; and the prerelease, which is currently in beta, of 1.11.0.  Those are safe.  The earlier ones, not so much.



We remember Juliano Rizzo, who was on the beach when he and his security researcher buddy discovered or came up with or realized the Beast and Crime attacks on SSL and how it was possible to cause an information leakage there.  Well, they're still doing security stuff.  And a couple weeks ago they discovered a severe vulnerability in the Signal messaging app for those two desktops, for Windows and Linux, which allows remote attackers to execute malicious code on the recipients' systems simply by sending them a message which requires no user interaction.



What was interesting was that this vulnerability that these guys discovered occurred purely by accident.  They were chatting on Signal using their desktop apps, probably for Linux, knowing those guys, and one of them shared a link of a vulnerable site with a cross-site scripting payload in its URL.  And the cross-site scripting payload got executed on the Signal desktop of that message recipient.  And of course being a security researcher, I mean, if it happened to many people they would have gone, "Oh, what?  That's odd."  But these guys instantly realized, wait a second, this thing just ran on my desktop.



So they dug into what was going on and tested a whole bunch of different payloads and figured out that the vulnerability resided in a function which was responsible for handling shared links which allowed attackers to inject user-defined and provided HTML and JavaScript via an iframe, an image, a video, or an audio tag.  I mean, basically there were lots of ways in.  And they were even able to inject a fillable form onto the recipient's chat window which could have been used in a social engineering attack to trick them into revealing sensitive information.



And it's interesting that Signal is another of those apps which is built upon the Electron application platform framework that we talked about as having some problems last week.  Not clear, in fact it doesn't look like, now that we know what's going on, that Electron was to blame.  It turns out that the Open Whisper Systems guys, the people who publish Signal, had already addressed this issue previously, but somehow there was a regression in their code.  The April 10th release of a Signal update reintroduced a problem which they had previously added some functions to prevent.  So I imagine they'll go back and figure out how this regression occurred.



However, this was responsibly disclosed.  The Signal guys refixed what they had previously fixed immediately, and within hours the desktop update was made.  But this was regarded as a pants-on-fire big security problem while it was there because, I mean, any bad guy simply had to send a Signal user a message, and it would have been executed.  So you do want to make sure that you're updated to the latest version of the desktop app.  Now, Signal does have an auto update facility, so I imagine unless you did something to deliberately deny updates, you were probably safe before this even was known publicly because this was also responsibly disclosed.



Oh, and also there was another problem - this is the second problem.  There was another problem in the Signal desktop for macOS where disappearing messages could be recovered and be undisappeared.  And that was also fixed at the same time.  So as far as we know, the Signal desktop app is up and running and clean.



In general, I think probably, you know, there's always sort of like best practice advice.  What's the number one thing you would ask someone to do or tell someone to do or make sure people do, very much like - I guess number one is probably related to passwords, where you don't want to be using the same password everywhere because of the vulnerability that creates, and you want to use a good password and so forth, not "monkey."  Probably right up there, although arguably much less easy to deploy as a device, is the concern about router security.



Routers are really coming under attack.  I don't know when we've had a week in this podcast when we haven't talked about one or more new or newly discovered or newly exploited or big problems with consumer routers.  It seems that it's one of these problems where they're deployed en masse before a problem is found, and then it's very difficult to get the problem fixed.  And it just sort of seems like it's another, you know, they're a classic case of an IoT device, but it's unique because they're very complex.  They're typically running some flavor of Linux in order to offer all the features that people want in routers.  And it's not even features people want, it's that manufacturers have a competitive checklist.  So there's a ridiculous amount of power in these things that most people never even know about or ask for, yet it's there.  And of course we know that complexity is the enemy of security.



So the other thing that makes these unique is that, unlike most IoT devices, which are safely hiding behind a router, its role is to be there on the frontline.  So it's the device more than anything else in your entire network where security is crucial because it's got a public-facing surface or attack surface that is potentially vulnerable.  So what we're finding is that DNS hijacking is, if the router's not being used, as we've been discussing in recent weeks, for hosting botnets or for using Universal Plug and Play to bounce traffic off of its public surface in order to disguise the source of attacks, one of the other ways that routers are being abused that is being seen now in the wild is DNS hijacking.



This is a problem because most devices behind the router simply use DHCP, Dynamic Host Configuration Protocol, to configure the devices, whether it's your Android, your iOS, your light bulbs, or all your computers.  It's the "obtain IP address automatically."  Well, you're not only obtaining the IP address for the main machine, you're also through DHCP, unless you deliberately configure it otherwise, you're obtaining the DNS address for that machine, that is, the two IPs which all of the systems on your network will use to look up domain names' IP addresses.



Well, what this means is, if that's a malicious server, that is, if something has hijacked your router's DNS so that the IP address your computer has is to a malicious DNS server, then one of the crucial assumptions of the Internet for security, which is you are really going to the server you think you are going to, when everything else is correct, you know, you look at the URL in your browser, www.amazon.com, no misspellings, no umlauts, no unicode snuck in there, it absolutely is www.amazon.com.  You're sure of it.  Well, you need DNS to send you to the proper IP for Amazon.com.



Now, the problem that hijackers have is unless they're able to somehow get a security certificate for Amazon.com, which is increasingly difficult thanks to the CA system really working hard to improve its security, then it's probably unlikely that they can get you to an HTTPS connection.  But it's often the case that users aren't paying attention.  And so you might well be at http://www.amazon.com and think everything is fine, when you're in fact at a spoofed site.  And it then pops up a page saying, oh, your login has timed out.  It's been too long since you've been here.  Please log in again.  And so that's a way the bad guys have of obviously acquiring your username and password at a site which has been spoofed.



All of this to say that in the news this week is yet again widespread DNS hijacking malware which has recently been found to be targeting Android devices and has also recently updated its capabilities to target iOS and desktop users.  It's called the "Roaming Mantis" malware and has been hijacking Internet routers in the last month to distribute various kinds of malware.  Kaspersky Labs have been tracking this and have been posting the news that this thing is spreading strongly.  There's a router that it's been compromising from DrayTek, which is a Taiwanese manufacturer.  They make router switches, firewalls, and VPN devices.  And there is a zero-day vulnerability in those particular routers which is being used to change its DNS settings.



So it's not clear where the geographic spread of these routers is, if they're only being sold to specific ISPs to provide them to their customers, or what the retail path is.  But if you happen to know that you have a DrayTek router on the shelf, you want to absolutely make sure that you've got it updated.  DrayTek is aware of the problem.  They were informed by Kaspersky, who found this particular DNS hijacking malware roaming around the Internet.  They've got a really long list of router model numbers of their manufacturer which are vulnerable.  So if you happen to have a DrayTek router, it's the latest victim of DNS hijacking.



But even if you don't, it's, I would say, probably worth just maybe logging into your router and just making sure that nothing has configured your router's settings.  Or just take a look at your own DHCP settings for your machine and make sure that it's what you expect them to be.  In some cases that may not be enough because your system might be using the router's gateway as its DNS, that's often the case, so that your queries go to the router, which it then forwards to public DNS servers.  So it's probably necessary to check the router.  As I said, it's not simple consumer-oriented advice to follow.  But unfortunately the danger is real.  This is something which is becoming increasingly widespread as routers are falling victim to one exploit after another.



I also wanted to mention last podcast we talked about the firmware updates, that is, the latest Intel microcode firmware updates for all versions of Windows 10 up to but not including, which was sort of a head-scratcher that we talked about at the time, for this latest April 2018 feature update, which is v1803 to Windows 10.  The day after that, last Wednesday, Microsoft added the microcode updates for 1803 and 1803 Windows Server.  So those are available.  I've got the link in the show notes.  It's KB4100347.  So if you are running 1803, and you haven't updated to this, KB4100347 will give you the latest, I mean, just across all of the chips that Intel is going to update for that latest version of Windows 10.



I did see that this was going to be pushed out both through the "go get it yourself" and the regular Windows Update mechanism.  I didn't look, I could have - I shoulda woulda coulda - checked to see whether my Windows 10, which I did verify this morning that there are no updates for it available, whether it had already grabbed that and brought itself current.  But in any event you may want to.



Okay, now, Leo, this is what I think is going to catch your fancy.  And perhaps it was inevitable.  As we know, we were just talking about making sure that Amazon.com was actually A-M-A-Z-O-N and not A-M-A-Z-0-N dotcom because for years lookalike phishing attacks have used lookalike domain names.  Well, now we have voice assistants, and we have phishing of sound-alike trigger phrases, which when you think about it shouldn't surprise us at all.  In a just-published research paper titled "Understanding and Mitigating the Security Risks of Voice-Controlled Third-Party Skills on Amazon" - and you know the name of that device, the Echo - "and Google Home," a group of Chinese and U.S. researchers - in that paper they describe their various methods of attacking smart assistants, those made by - and I'm working hard not to say the "A" word.



LEO:  Say Echo.  That's okay.



STEVE:  Oh, Echo, yes.  But everywhere in the coverage they're talking about the Amazon A-word.  So we've already talked about previously the problem of applications maliciously and deliberately leaving the microphone on, staying active in the background, and then streaming whatever the microphone receives off onto the Internet.  These guys do describe that.  But they also describe, and they convincingly demonstrate, a more worrisome - and I don't know what Amazon and Google are going to do about this - attack which they call "voice squatting."



The idea is to trick the user into opening a malicious application by using voice triggers which are acoustically similar to the ones of authentic apps, and then of course using the malicious apps to phish users for sensitive data or eavesdrop on their surroundings or do whatever.  So, for example, an attacker might register an app that triggers on the phrase "Open Capital Won," but spelled W-O-N, that is, as in winning and losing, won.  Which obviously sounds similar to "Open Capital One," O-N-E, which is the real intended trigger for that phrase.  How does the voice assistant know?



So they demonstrate in their paper, for example, there is an Amazon Echo skill named "rat game," and they registered what they call an attack skill, "rap [R-A-P] game."  And the very fact that I had to spell it demonstrates the problem of the skill.  It sounds almost identical.  And sure enough, in their testing, having registered this "rap game," after it being registered, when someone wants to open "rat game," "rap game" with a "p," the attack skill wins preferentially.  They did that over on the Amazon side.  For Google Assistant they registered the attack skill "intraMatic opener," which sounds very similar to an existing target skill, "Entrematic Opener."  And again, when users tried to invoke "Entrematic Opener," they got "intraMatic opener" instead.



And then in one additional and I thought very clever attack they experimented with making a more specific request.  For example, they registered "rat [R-A-T] game please" so that, if the user says to the Amazon Echo device, "A" word, "open rat game please," well, that's a more specific match than just "rat game."  And again, if the extra "please" is added on the end, it matches more strongly than without it, and again the attack skill is executed.



So these guys put together the paper, demonstrated their proof of concept.  They've notified both Amazon and Google, who have both responded and said, "Thank you for your report.  We're looking into it."  But, boy, Leo, I don't know how you solve this problem.  I mean, imagine...



LEO:  Yeah, but so what?  So how would you use this maliciously, besides you got the wrong game?



STEVE:  Well, or the wrong anything.



LEO:  It then says tell me your bank password, and you're going to tell it your bank password?  No.



STEVE:  You don't think so?



LEO:  No.  You never do that on an Echo.  No one would fall for that.  I mean, I don't think so.  I think this is more an equivalent of getting somebody's computer to do something silly than anything seriously dangerous.  I mean, I'm trying to think of an attack surface.  With a banking app you can't transfer  money to somebody.  I mean, most of these apps are really very limited.  So you could, I mean, it's more like a kid could get something funny happening to you.



STEVE:  Okay.  So I guess...



LEO:  I'm trying to think of how this would be maliciously abused.



STEVE:  If nothing else, it causes the device to misbehave, to do something that you don't want it to. 



LEO:  Right, right.  That's annoying.  But that happens all the time, by the way, anytime you're using an Echo.  I mean, it's not unusual for - I mean, you use Siri sometimes; right?  Doesn't she get you wrong about three quarters of the...



STEVE:  I don't use any voice things.



LEO:  Well, try, I mean, Siri gets you wrong half the time.  It's annoying.  I mean, you'd have to be - I guess you could make, okay, I'm playing rap game now, and it's asking for my bank password, I don't know why, but I'm going to give it to you.  I mean, that's a pretty long stretch.



STEVE:  Yeah.  I guess the problem is that we're using a very soft match technology...



LEO:  Oh, yeah.  It's not good.  I agree.



STEVE:  ...to cause things to happen.  And so it's a fundamental problem with voice.



LEO:  It's similar to the problem, though, of a web page, as you said, Amazom instead of Amazon.  But I think it has less long-term utility in terms of maliciousness.  If I get you to Amazom.com, I could ask for your Amazon password reasonably.  But I don't know what you're going to do with a voice app to screw somebody up.  You've entered the wrong address, I mean, we all do it.  And you get some strange page; right?



STEVE:  Yeah, yeah.



LEO:  It's equivalent to that.



STEVE:  Well, in this case, though, I mean, both Amazon and Google want their systems to work as well as they can.  And this is a problem which will be - it's hard to see how they mitigate this.  I mean, like...



LEO:  Well, that's right.  I mean, you'd have to have - I think what you do is you block duplicate accounts based on phonemes, not on spelling, something like that.



STEVE:  Exactly.



LEO:  That would eliminate Capital W-O-N.



STEVE:  Exactly, yes.  So you start to look for deliberate attacks for sound-alike registration phrases and disallow those.  Which clearly they're not doing at this point.



LEO:  No.



STEVE:  I just think it's, at this point, we're in the early stages.  But I immediately think of Stacey, who's got like her entire world is controlled by, you know, her whole house and everything.  And if you're running a mischievous voice-controlled app that is there in place of something legitimate, it could probably get up to some mischief with you.  But again, it's not going to probably steal your bank account stuff, you're right.



LEO:  No, it's a vandalism kind of attack.  Oh, we're going to turn the thermostat up to 80 <evil chuckle>.



STEVE:  Well, and again, though, it is an attack on the integrity of a system that more and more people are using and wanting to use.  So I wouldn't be surprised if someone clever were to come up with something that'd cause some trouble.  So we'll see.



LEO:  It's conceivable.  I mean, you could have some sort of sales app that would - I don't know.  I mean, I'm really working to come up with something here.



STEVE:  So Chrome is Google, and their Chrome browser is continuing to move their intent to secure the web through using the power of the presence.  Like they're now the majority web browser, so they're able to force change, as we have seen with them, for example, forcing the early wind-down of SHA-1 certificates prior to its already sort of more slowly planned withdrawal because their browser was going to stop honoring them earlier than the rest of the industry.  Right now, when we go to a secure TLS-encrypted site using HTTPS, they show us the little lock icon and then a big comforting green "Secure" to say, yes, this is a secure connection.



So with Chrome 68, which is slated for release in a month and a half, in July, they will start showing "Not Secure" for HTTP.  So right now they're silent on HTTP.  They give you the green happy "Secure" for HTTPS.  In 68 in July they'll start saying "Not Secure."  Then for 69, Chrome 69 in September, they're going to remove the "Secure," the happy green "Secure," probably because they'd like to just have more room for the URL.  It takes up a chunk of space up there in that space.  So they'll only show the lock.



Google has stated that, since most traffic is now HTTPS anyway, they feel that it's no longer necessary to draw the user's attention to the "Secure" indicator, that being just sort of the de facto.  And so instead Chrome will start focusing on highlighting situations when the user is accessing an insecure website.  So again, in September - well, in July 68 will start saying "Not Secure" if you're on HTTP.  And then in the next one after, in Chrome 70, which I don't have the date for that, probably either the very end of 2018 or maybe the beginning of 2019, they're going to further amplify that "Not Secure" indication by if you start entering data into a form on a non-HTTP site.



And what would really be interesting is to know whether it's - because we've talked about this at length - whether it's the submission URL which is not HTTPS or whether the page where you're filling the data in is not HTTPS.  Be interesting to see which of those they choose, or maybe both.  Maybe they just want both to be an HTTPS page and an HTTPS submission.  Anyway, they're going to animate that "Not Secure" by turning it red to, like, really call the users' attention to the face that they're entering data into a site which is not HTTPS.  So again, they're determined to move the industry away from HTTP over the grumblings of people who argue that their site just has no need for it.



On the other hand, I do support the idea that a site that is wanting data from a user could be benign.  It might be that it really doesn't need security.  But chances are what a user is putting into a form they would like to not have eavesdropped on.  So highlighting the fact that where they're putting the data in isn't secure is probably a good thing.



GPON routers are still and again in the news:  17,000 of these GPON routers that we've been talking about the last few weeks have been infected with the Satori botnet.  They're now scanning port 3333, which is used by the Ethereum miner known as Claymore.  It turns out that there's a zero-day flaw in the Claymore mining software which exposes it when it's publicly exposed on port 3333.  This new botnet, this Satori botnet has added its own skill, looking for those Ethereum mining rigs.  And, if found, they reconfigure the Ethereum miner to join the DwarfPool mining pool and use the attacker's Ethereum wallet.  So yet another example of where routers are being put to a bad purpose.



And just to finish up on GPON routers, we already knew of two zero-day exploits which we talked about a couple weeks ago that the researchers, the Chinese researchers at 360 Netlab had discovered.  Well, they found a botnet running a third, previously unknown flaw.  So we also talked about how there were five different botnets fighting for dominion over those GPON routers:  Hajime, Mettle, Mirai, Muhstik, and Satori.  To that we now add a sixth one called TheMoon.  And it's currently winning because it's exploiting a new, previously unknown zero-day for which there is no patch, allowing it to get into and take over these GPON routers.



And of course remember that, since GPON stands for Gigabit Passive Optical Network, this suggests that the routers in question are well connected to the Internet, which makes them extremely valuable as DDoS attack platforms, as scanners for other vulnerabilities, and maybe, if they're able to run gigabit fiber, they have beefier processors, so they could even be used usefully for some bitcoin mining or cryptocurrency mining.



So, wow.  I mean, it's really looking like, as I said at the top, consumer routers are the target for attackers.  And we had previously been talking about how the good news was they were just being used for attack reflection or purposes that didn't cause a subversion of the user's network.  But if they start saying, well, while we're here, let's point DNS to our own malicious DNS server in order to perpetrate those effects, then that becomes certainly a lot less than benign.



LEO:  Is GPON a brand name or a description of a type of router? 



STEVE:  It's not clear to me.  The company is Dasan, D-A-S-A-N, a South Korean manufacturer of these GPON home routers.



LEO:  It's a technology; right?  So there might be multiple routers using GPON?



STEVE:  Well, we know there are multiple ISPs using them.



LEO:  Ah.



STEVE:  And so GPON stands for Gigabit Passive Optical Network.  But it might be an acronym that they use on their own routers like as a brand name of what they also are.  So sort of both.  



LEO:  All right, Steve.  Back to you.



STEVE:  Yeah, thank you.  So this was sort of a tempest in a teapot.  There was a mistake made in the Facebook app for Android that probably could not have come at a worse time, given all of Facebook's recent privacy controversies.  It turned out it was the result of an innocent coding error in a beta release of the Facebook app for Android.  But what it did was to cause a permission pop-up, asking the user to give the app superuser root-level access.  And of course a lot of people said, "What?"  And there was a lot of Twitter traffic.



Security researchers looked into what was going on and determined that the package that appeared to be triggering the superuser popup was something known as "WhiteOps," which is an SDK which Facebook was in the process of deploying, used for detecting fraud and implementing domain white and black lists.  So they didn't intend to have anything get superuser root-level access.  A Facebook spokesperson confirmed that the popup dialogue was caused by a coding error.



Facebook said:  "A coding error in one of our anti-fraud systems caused a number of people running the Facebook app and certain permission management apps on rooted Android phones to see a request for additional access permissions."  They said:  "We do not need or want these permissions.  We have already fixed the issue.  We apologize for any confusion."  So it generated a bit of a firestorm on the Internet with people saying, wait a minute, what's going on here?  And it was a mistake, and those things can happen.



We've talked in the past about the Tencent team at Keen Labs.  They have a great, well-earned reputation for hacking autos.  And they spent, it turns out, pretty much all, actually a little more than 12 months, all of 2017 starting from January 2017 through the first month or two of 2018, looking closely at the security of BMW automobiles.  And the short version is, as a result, BMW is now working on firmware updates to fix 14 flaws which were found to affect the I series, the X series, and the 3, 5, and 7 series cars, dating back to 2012.  And that's an estimate based on the components that they found to be insecure, and when those components first appeared in use in the BMWs.



The good news is the exploitation of the flaws is difficult.  It can be done remotely, but it apparently requires access to a GSM cell tower, which we know that there are spoofed GSM cell towers, so maybe you could do it creating a spoofed connection, a cellular connection to the car.  And it can also be done with a USB connection.  But both the researchers and BMW agreed that the dangers are high end, that is to say, requires a high level of skill in order to perpetrate the attack, and do not represent imminent danger to the car's owners.  Yet they do need to be fixed.



While you were on vacation most recently, Leo, Father Robert and I talked about the topic of the CAN bus in cars and the fact that there are high-speed, high-performance CAN buses and low-speed CAN buses.  And in fact there's not just one CAN bus.  There are now in contemporary vehicles multiple CAN buses with firewalls that are deliberately there to decide what can cross from one CAN bus to the other.  So it's often the case that the point of entry is through the so-called "infotainment" systems in the cars; but while, yes, you may be able to change the radio or play with the dashboard, you're not able to affect steering or braking or driver-critical vehicle safety-related systems because those are deliberately on an isolated bus.  It wasn't clear because in the paper they talked about being able to penetrate the car's CAN bus security.  But the reason it's not clear is they're giving BMW all of this year to get this fixed.



LEO:  Yeah, we talked about it before, too.



STEVE:  Yes, yes.



LEO:  I mean, part of the problem is sometimes car manufacturers only have one CAN bus network, and then they depend on some sort of barrier that is not always perfect to prevent the telematics systems from getting to the drive systems.  I mean, that's the real attack; right?  The attack is hard enough to get on the bus; but once you get on the bus, then to get through the security that keeps you from accessing the brakes and stuff.



STEVE:  Yes, unless you want to...



LEO:  We've talked to guys who do this.  I mean, you saw the "60 Minutes" story.  I mean, people can do that, yeah.



STEVE:  Yeah, yeah.  I mean, and it is absolutely the case that you don't want something malicious in your car, regardless.  Because, I mean, even spoofing things like how much gas you have left could be an annoyance if it led you to believe that your tank was full and you suddenly ran out of gas.  It's like, wait a minute; you know?  Because that kind of stuff, the dashboard stuff, we have seen multiple compromises of that.  So it looks like all of the passenger UI is on one side, often; and then, like, engine and collision and braking and acceleration and even steering, there have been instances of compromises there, but I think more for demo purposes.



LEO:  Well, Charlie Miller, remember he had that hack where he wirelessly could get into a car and steer it into a ditch.



STEVE:  Yes.



LEO:  That was pretty scary.



STEVE:  Yeah.  So it's certainly the case that car manufacturers are taking the security of these systems very seriously.  And I saw that they're not going to disclose details.  They have a nice paper talking about, with charts - I have the link to their PDF in the show notes - showing a chart of things found, like 14 different flaws and where they are and so forth, but no explicit discussion of them because they want to give BMW, first of all, time to fix them.  But then it also requires that the car, maybe it could be an over-the-air fix, but maybe take the car in to have it serviced; and then, while the car is in the shop having its oil changed, it gets itself updated.  It's not clear what the path would be to get it fixed.



But they are giving BMW plenty of time, which I think is due.  And especially given the fact that none of this appears to be - it requires a high-end attack.  These guys had to work to make it happen.  And even then it's not clear that it's more than just a nuisance.  But as we know, many attacks start as a nuisance and can escalate from there.



I have some fun miscellany.  We've talked about often Star Trek.  Of course ShieldsUP! is the name of my rather famous port scanner at GRC.  I just wanted to mention that "Discovery" on CBS, even though it was all-access, and it upset a lot of people, including myself, it's been renewed for a second season; and that there are a whole collection of additional Trek movies in development.  So Star Trek continues to live.



Also, many of our listeners were huge fans, as I was, of "The Expanse" series on Syfy.  Really, the books were great.  I read them deliberately before the series began.  I have not yet caught up and watched the second and third seasons.  We'd just finished with Season 3, and Syfy announced that they were going to cancel the series.  However, Jeff Bezos is a huge fan, and he's now in talks to pick up the production and plans to produce, given that everything proceeds as expected, to produce a fourth season.  So for fans of "The Expanse" who have watched all three - I'll be catching up here at some point because the first season was excellent - there will be a fourth.



LEO:  Yay.



STEVE:  And Leo, you'll get a kick out of this.  You were right to be skeptical about the EM-Drive which we talked about.  I couldn't sell you on the idea that maybe there was a reactionless space drive that was somehow possible, that apparently we were told NASA researchers had built a chamber and had somehow this reactionless drive that used microwaves in a specially formed cavity to resonate, and it was apparently generating some thrust.  And at the time you were like, uh-huh,  yeah, I'm not buying it.



LEO:  Well, it just violates Newtonian physics.  But, you know, not that that couldn't happen.  It's a high standard, high bar to cross.  



STEVE:  So it turns out that Germans got involved.



LEO:  Oh, they know what they're doing, now.  You've got to trust the Germans, yeah.



STEVE:  They know what they're doing.  They were not - they left no stone unturned.  They used a super high vacuum so that heating of the air in the chamber could be ruled out.  They used mu-metal in order to mute and absorb magnetic lines of force.  And they really, really - they said, okay.  We need to determine for sure whether these fringe drive space drives work or don't.  So they built up a system.  They implemented their own version, very carefully, German-ly crafted, of an EM-Drive, and it produced thrust.  Then they said, what?  And they looked more closely.



It turns out that the magnetic shielding was incomplete just for convenience sake.  Turned out it was very difficult to shield it from the Earth's magnetic field completely, and that the DC power delivery to the EM-Drive was interacting with the Earth's magnetic field and electromagnetically pushing this thing.  So that kind of rules it out because, by definition, we cannot take the Earth with us into space to get acceleration since leaving the Earth behind is the whole point.  And a space drive which reduces to a simple magnet and coil is not going to get us anywhere, literally.



LEO:  Oh, too bad.



STEVE:  Yeah, too bad.



LEO:  And it doesn't have enough impulse to use it as a subsonic transporter around the world.  Or does it?



STEVE:  No, no.  And even in space, Leo...



LEO:  You'd be going, like, two miles an hour.



STEVE:  Yes.  And even in space it would require a great deal of patience to, like, okay, we're accelerating.  Wait, are we going any faster than we were last year?



LEO:  Little bit, little bit.



STEVE:  A little bit.



LEO:  Little bit.



STEVE:  Not much.  Not enough to make a difference.



LEO:  Oh, well. 



STEVE:  I found a nice note from a Jeff Karpinski in Elizabeth, Colorado.  His subject was "SpinRite does you know what."  On May 18th he sent it to me.  He said:  "Hey, Steve.  Happy SpinRite owner for over a decade.  Used it many times on conventional drives, but today was a first for me and SSD."  He said:  "My shop PC, which runs CNC for my laser cutter and mill, became sluggish all of a sudden."  Oops.  And we know what that can often mean.  He said:  "A peek at the Windows system logs showed an explosion of drive write failure warnings.  Promptly rebooted into SpinRite, and I did a Level 1 for a quick assessment."  He said:  "It's only a 250GB drive, so the scan took just a few minutes.  Sure enough, some issues in the early sectors."  He says:  "Reran SpinRite at Level 2, and it took an hour or so to beat through the problem spots.  After SpinRite finished 'blessing the bits,'" as he put it, in quotes, "everything was back to normal.  PC performance was back, and Windows logs were quiet."



He says:  "Now my question.  If SSD writes are truly randomized across the media as marketing claims, why were the bad sectors on my drive largely contiguous and localized at the beginning of the drive?"  He says:  "Also, the SMART window showed no issues being reported whatsoever.  Not very 'smart' in my book."  He says:  "I'm not trusting this drive anymore, so I'll pick up a second drive this weekend and let Windows mirror it.  Thanks as always for a fabulous product and keep up the great work with Security Now!."  He says:  "Now I'm retired, it's the only podcast that I listen to."



So Jeff, thank you.  And to answer your question, the SSD controller performs the mapping from logical sectors on the drive to physical sectors spread out across the actual physical surface.  So the sectors at the beginning are where the OS typically resides, and they're the ones that get the most use.  But they're logically positioned at the beginning of the drive, not physically positioned at the beginning of the silicon.  So they are in fact spread out.  But normally they're not being recycled unless you're writing to them; and probably there was a lot of reading being done, but not a lot of writing.



So what happens, remember that SSDs are just little tiny capacitors.  All they have is they have charge stranded on a floating gate which the technology is able to sense that charge.  Over time, the charge can and does leak off.  And so this is one reason why running SpinRite on an SSD is so useful is, whereas hard drives' magnetism tends to be a fundamentally permanent facet of a magnetized surface, the SSD has just got some electrons kind of suspended in limbo, and you're hoping they all stay there.  But there's a tendency for them to wander off.  So what happened was, when you ran SpinRite on Level 1, which is only a read pass, it immediately detected, whoa, we've got some problems here reading, just passively reading these sectors.



And so what it then did when you ran it on Level 2 was, when it found problems reading, it then did recovery and rewrites which strengthened, essentially recharged those little capacitors on the SSD.  And maybe then the controller relocated physically, again, used its drive leveling which is trying to level where the writes occur.  It may then, when SpinRite rewrote those troubled sectors, it would have physically moved them.  But when you're only reading, that's not considered a destructive process.  But it is unfortunately one where after time you can start having significant problems.  And as we know, something slowing down is a good early sign of reading requiring more time, which is a clue that it's probably a good time to run SpinRite.



And as for SMART not being very smart, it is the case that not all drives choose to publish what they're doing publicly.  This is one of the biggest annoyances of hard drives is that there's no way to force a manufacturer's behavior.  Back in the day, Compaq was able to, and did, they had so much marketing strength that they were able to force hard drive manufacturers to publish what was going on behind the scenes.  Otherwise, Compaq would not have purchased their drives.  So they forced this to happen.  And over time people are somewhat belatedly still doing it, or in this case just saying, "Trust us, it's an SSD."  It's like, eh, no.



LEO:  So you shouldn't use an SSD for backup because it'll eventually just leak out all the information.



STEVE:  Correct.  Long-term, you know what is still the most reliable is magneto-optical storage or just standard optical storage.  The archivists are still using really good optical storage.  But you're right, Leo.



LEO:  Like a CD or a DVD kind of?



STEVE:  Yes, yes, yeah.



LEO:  Okay.



STEVE:  Because that's a state change which is reliable.  There can be an oxidation problem, so gold is good to use as a substrate.  And then of course keep them in a dark place.  But yeah, an SSD, if you did do it, you'd want to put it - oh, and by the way, I heard a comment the other day on one of your podcasts about whether Steve still had his Treos in the refrigerator.



LEO:  Yes.  And do you?



STEVE:  Yes, I do.



LEO:  Why?  Now, Steve...



STEVE:  My tungsten - well, because nothing made me - I don't need the room.



LEO:  Well, that's what I'm wondering.  Did you buy extra refrigerator space for this?



STEVE:  No, no.



LEO:  And do you keep it in the freezer or the fridge?



STEVE:  I have no food in my fridge, so there's lots of room.  I have lots of room for unused PDAs.



LEO:  So you open Steve's fridge, there's no mustard, there's no ketchup, there's...



STEVE:  There's milk for lattes.



LEO:  There's milk, and there's hard drives.



STEVE:  And batteries.



LEO:  And gadgets and batteries.  That is hysterical.  And not surprising at all.



STEVE:  Chemical things that you want to keep cool, yeah.  So, okay.  As I mentioned at the top of the podcast, it was pretty clear when we first learned at the top of this year, the beginning of the year, that speculative execution and also caching, because that's what Meltdown was about, all which were in place for performance, were in trouble.  The problem was that they fundamentally used resources that were shared among processes.  And in order to function, a cache is inherently a history of what's been done using the fact that what is about to happen is often related to what has just happened in computer code.  So if you remember what you've done, maybe you have it on hand if you need to do it again in the short term.



So any kind of a history means that what has happened recently alters the speed of what happens next.  And if you are very careful at noticing your own execution speed, it tells you about the past.  Which, incredibly enough, attackers can use in order to leak information from areas of a system they should not have access to.



Okay.  So we initially had three variations of this problem.  Collectively they were known as Meltdown and Spectre.  We had Variant 1 and 2 were Spectre, and Variant 3 was Meltdown.  One and three were relatively easily solved with some software changes.  That is, like OS-level changes.  That second one, that Spectre Variant 2, that was the pesky one.  That was the branch prediction speculation problem which required turning branch prediction off.  And that's why it had a big performance impact if you chose to do it because being able to predict which way a branch would go in the ability of the processor to fetch instructions ahead allowed it to keep the memory bandwidth channel saturated.



Memory is still the slowest thing.  DRAM is slow, which is why we have typically three levels of caching in order to just try to feed the hungry multicore chips we have with DRAM, which stubbornly refuses to go any faster.  We've talked about promising and hopeful future technologies.  There are some.  HP's working on that cross-point memory, which would potentially give us DRAM density at static RAM, that is, at cache RAM speed, which would completely change everything.  I mean, it just would revolutionize.  But it hasn't happened yet.  And they seem to be stalled because they were supposed to already have test devices out by now.  So who knows what's happening there.  But the problem with turning off one of these features is that it hurts us.  That is, it's there because it helps our performance so much.  So if you make it not there anymore for the sake of security, you get a performance hit.



So now we know what happened two weeks ago, that is, the news hit that there were some more problems found.  And it shouldn't surprise any of us that that was the case because the things we've done in our chips, and by "we" I mean everybody - AMD, ARM, IBM with several of their processors, and of course Intel.  Everybody has done this because it's the way you squeeze more performance out of your architecture.  So consequently now everybody is in trouble and is watching all of these things very carefully.



What we knew two weeks ago is that new problems had been found.  We heard, you know, Heise.de reported eight new problems.  I don't know where the other six are, but we now have good information, complete information about two new problems which are being called 3a and 4.  Of the two, Variant 4, which is a Spectre problem, is the big problem.  What's interesting is that both Google and Microsoft security researchers independently discovered these problems just because everybody is now looking deeply into other possible ways that our contemporary speedup architectures can be leveraged.  And remember that for decades we have all been sort of merrily going along with processing architectures that could be exploited in this fashion.  Yet it wasn't until last summer that - I'm blanking on his name.  I have it here.  The guy at Google's Project Zero.



LEO:  Travis Kalanick?



STEVE:  No, not Travis in this case.  Oh, Jann, J-A-N-N, Horn at Google.  He was the original discoverer of the Meltdown and Spectre flaws.  And he was the guy at Google, along with people at Microsoft, who figured out that there was more trouble that still had to be resolved.  So Variant 3a is known as the "Rogue System Register Read," which is a variation of the Meltdown flaw.  Variant 4 is "Speculative Store Bypass."  And that's the one that's most worrisome and the one that requires yet another microcode update.  So we've got more microcode updates coming.



Red Hat is involved because of course they want to secure Linux against this.  So they explained that Variant 4 relies upon the presence of a precisely defined instruction sequence in the privileged code.  Now, of course, that's not hard to achieve because everyone can disassemble other people's code.  So open source makes it super easy.  But even closed source, the idea is that, as we know, contemporary security technology does not rely on unknown code, that is, unknown algorithms.  It relies on unknown keys.  So there's no bar being set by requiring precisely defined instruction sequences to be known or unknown.  Everybody has that.



And they continue, saying:  "...as well as the fact that memory read from an address to which a recent memory write has occurred may see the previous value and subsequently cause an update to the microprocessor's data cache, even for speculatively executed instructions that never actually commit."  That is to say, remember that, if the processor executes speculatively, assuming that a certain branch will be taken, then it can cause some change, even though that it ends up throwing away all of that work it did if a different branch got taken.



Okay.  So Microsoft just yesterday, I mean, this has just all come to light and happened.  Yesterday Microsoft said:  "In January 2018, Microsoft published an advisory and security updates for a new class of hardware vulnerabilities involving speculative execution side channels known as Spectre and Meltdown.  In this blog post [yesterday], we will provide a technical analysis of an additional subclass of speculative execution side channel vulnerabilities known as Speculative Store Bypass (SSB) which has been assigned [and they gave it] CVE-2018-3639.  SSB was independently discovered by Ken Johnson of the Microsoft Security Response Center (MSRC) and Jann Horn of Google Project Zero."



Microsoft says:  "What is affected?" and answers, "AMD, ARM, and Intel CPUs."  And it turns out in other reporting IBM is there, too, with their chips.  "What is the risk?" Microsoft says.  "Microsoft currently assesses the risk posed by [this CVE, this SSB] to our customers as low.  We are not aware of any exploitable instances of this vulnerability class in our software at this time; but we are continuing to investigate, and we encourage researchers to find and report any exploitable instances as part of our Speculative Execution Side Channel Bounty program."  Remember, that's the one, what was it, Leo?  I think it was a quarter million dollars?



LEO:  Yeah, it was a big one, yeah.



STEVE:  Yeah.  It times out at the end of this year.  So you've got through 2018.  But they're offering a lot of money to anybody who can actually find an exploit.  They say:  "We will adapt our mitigation strategy for [this SSB] as our understanding of the risk evolves."  So they say:  "What is the mitigation?  Microsoft has already released mitigations as part of our response to Spectre and Meltdown that are applicable to SSB in certain scenarios, such as reducing timer precision in Edge and IE."  So because this is, as we've talked about before, it is subtle variations in the speed at which instructions execute, which is the root of this information leakage, if you just don't let the software know what time it is with sufficient resolution, you just shut down that exploit.



LEO:  That's funny.  That's such a funny fix.



STEVE:  Isn't it?  Yeah.  And so this is a different thing, a very different thing.  But it's still ultimately, because the way the information leaks is in timing of individual instructions.  So they said:  "Software developers can address individual instances of [this exploit] if they are discovered by introducing a speculation barrier instruction as described in [blah blah blah]."  And that's that LFENCE that we've talked about.  There's a longstanding instruction where if the processor hits it, it forces everything to complete before it goes on.  And so what can be done and what has been done is this instruction can be salted through areas which are known to be sensitive in order to specifically block information leakage from that little bit of code.



So the beauty of that, I mean, that's like the most labor-intensive, most error-prone solution because it can be hard to find every single instance where this might be necessary.  But the beauty of it is that it's like virtually undetectable performance hit because it's only in particular places that you're saying, okay, everything needs to catch up before we move forward.  So that LFENCE instruction will slow down that tiny bit of code, but let everything else run at full speed.



The hurtful mitigation is the next one.  So I'll just continue.  They said:  "Microsoft is working with CPU manufacturers to assess the availability and readiness of new hardware features that can be used to resolve [this].  In some cases, these features will require a microcode or firmware update to be installed."  And I should say we're beyond "may."  Intel has already announced they're working on it.  "Microsoft plans to provide a mitigation that leverages the new hardware features in a future Windows update."  So in other words, when Intel has yet again new firmware across the family of chips, Microsoft will support that in Windows.  Probably not old Windows, but newer Windows.



So then under "Preventing speculation techniques involving SSB," they said:  "As we've noted in the past, one of the best ways to mitigate a vulnerability is by addressing the issue as close to the root cause as possible.  In the case of SSB, there are a few techniques that can be used to prevent speculation techniques that rely on SSB as the speculation primitive."  And then they talk about what I just was, this notion of the LFENCE to force a catch-up.  Then they say, as another mitigation:  "Speculative store bypass disable," so that's SSBD.



They say:  "In some cases, CPUs can provide facilities" - and Intel doesn't have it yet, but it's coming, thus the microcode update - "for inhibiting a speculative store bypass from occurring and can therefore offer a categorical mitigation for SSB."  They said:  "AMD, ARM, and Intel have documented new hardware features that can be used by software to accomplish this.  Microsoft is working with AMD, ARM, and Intel to assess the availability and readiness of these features.  In some cases, these features will require a microcode or firmware update to be installed," and blah blah blah.



And under "Generally applicable mitigations" they talk about, as we were saying, the things that they've already done that can work.  However, Intel acknowledges that it can be up to an 8% decrease in system performance if you do this global mitigation; that is, if with updated firmware, which offers a new bit in a feature register, that is, the SSBD bit, the Speculative Store Bypass Disable, that will produce a performance hit.  And Intel has announced new patches.  Intel said:  "We've already delivered the microcode update for Variant 4 in beta form to OEM system manufacturers and system software vendors, and we expect it will be released into production BIOS and software updates after the coming weeks.  This mitigation will be set" - and this is important, and Microsoft concurs with this.  "This mitigation will be set to 'off' by default, providing customers the choice of whether to enable it."  So they're saying, and of course the reason is it's a performance hit.



They said:  "We expect most industry software partners will likewise use the default off option.  In this configuration, we have observed no performance impact."  Well, yeah, because you didn't do anything.  They said:  "If enabled, we've observed a performance impact of approximately 2 to 8% based on overall scores for benchmarks like SYSmark 2014 SE and the SPEC integer rate on client and server test systems."  They said:  "The same update also includes microcode that addresses Variant 3a" - which is that Rogue System Register Read - "which was previously documented publicly by ARM in January.  We have not observed any meaningful performance impact on client or server benchmarks with the Variant 3a mitigation."



So they said:  "We've bundled these two microcode updates together to streamline the process for our industry partners and customers.  This is something you will see us continue, as we recognize that a more predictable and consolidated update process will be helpful to the entire ecosystem."  In other words, they decided to group these together because why not.



So what will happen is it looks like Microsoft plans to also leave this off unless we learn at some point that it's important to turn it on, probably because it represents, I mean, a very significant performance impact on our systems simply to turn this on.  Maybe they will be able to go through and, for example, sequester.  One of the recommendations has been to recognize that it's no longer safe to mix the physical location of secrets with just less sensitive data, and that secret information needs to be physically placed somewhere in memory where it can be safe.



And so if you worry about, if you understand the nature of this kind of speculation and the attacks on it, it's possible to sequester your secrets so that they are not susceptible to this, even if you leave speculation on.  And this is what we were talking about in the future where the immediate worry has been to turn these things off that creates a performance impact, but probably over time we will be able to incrementally protect our secrets and then get back some performance that we initially lost.



In this case it looks like the hit of shutting down this SSB is so big that no one wants to do that, and they're expecting that it'll be possible to go in and, both by softening timing resolution in browsers and eventually by protecting those sensitive areas with speculation prevention instructions like this LFENCE instruction, that it'll be possible to just now take responsibility for the potential security problems that speculation introduces, which as of the beginning of the year we are now soberly aware of.



So bottom line, it's not the end of the world for users.  Microsoft will be giving us Intel's updated firmware, which mostly will allow them to respond fast if a problem is found.  That is, we'll already have in our chips the ability to shut this down if a catastrophe comes to light, although no one expects that.  So more updates from Intel, more from Microsoft, and more awareness of problems which the developers will have to keep in mind as they move forward and continue to massage the code to keep it secure.



LEO:  Yeah.  I think that's the other question is what happened to the other six?



STEVE:  Yes, exactly.



LEO:  Where are those?



STEVE:  Because we heard there were six.



LEO:  Do you think we've heard the worst ones?  Do we know about the worst ones?  Or there could be worse?



STEVE:  What kind of happened was that this leaked.



LEO:  Oh, okay.



STEVE:  Yeah.  So both Microsoft and Google found this.  It has that 90-day...



LEO:  Disclosure requirement, yeah.



STEVE:  ...disclosure.  And in Jann's posting it does show that they're in - I can't remember what he called it.  Oh, "grace period."  They exceeded the 90, and they're being given an extension.  So it could be that there are still six more problems that we'll be talking about as they come to light in the future.  Wow.



LEO:  Yeah.



STEVE:  It's what happens when you have a fundamental hugely widespread, I mean, fundamental technology, this whole concept of caching and speculative execution, which suddenly everyone goes, "Oh, shoot.  We really can't do that safely."



LEO:  Yeah.



STEVE:  Yet everybody is.



LEO:  Right.  Whoops.



STEVE:  Wow.  Oops.



LEO:  And now everybody's banging on it, so I expect that we haven't heard the end of this anyway, even if there may be other ones, as well.  Well, there you go.  Spectre - I guess NG is Next Generation?



STEVE:  Yup.  Spectre NextGen.



LEO:  Next bad thing?



STEVE:  That keeps on giving.



LEO:  Keeps on giving.  You'll find Steve is the guy who keeps on giving.  You'll find his stuff at GRC.com.  Of course the podcast is there.  Just go to GRC.com and you can download the audio versions plus read the transcriptions, the human-created transcriptions of each show, a few days after the show is done.  You'll also find all the freebies that he gives away like ShieldsUP! and Perfect Paper Passwords, SQRL, all the information about SQRL.  And his one sole piece of software that he sells, which is SpinRite, the world's best hard drive recovery and maintenance utility.  You can help support Steve and the show by buying a copy.  Buy two.  They're cheap.



STEVE:  They last a long time, that's for sure.



LEO:  The site license is how many copies?



STEVE:  So the idea is an individual can use it in all the machines they own.  We ask corporations that want to use them on all the machines they own to hold four licenses.



LEO:  Four is all you need.  After that it's unlimited.



STEVE:  Four is all you need.  Then a corporation can use it throughout their organization with my blessing.



LEO:  It's very fair.



STEVE:  And again, they do last a long time.  So you get a lot of use out of those.



LEO:  A fair way to do it.  Thank you, Steve.  GRC.com is where you'll find Steve, or @SGgrc on Twitter.  He takes DMs, so you can always contact him there.  Or leave a feedback form at GRC.com/feedback.  You can find this show, audio and video, on our website, TWiT.tv/sn, or wherever you subscribe to podcasts.  If you want to watch us do it live - we had a big studio audience here today, Steve, from Louisiana, Connecticut, Pennsylvania.  All big fans.  They're a little sleepy now, but that's okay.



STEVE:  Hi, everybody.  Wake up, wake up.



LEO:  You've got the comfy chair.  That's why he's very relaxed.  If you want to be here, just email tickets@twit.tv.  We'll make sure we do get a comfy chair out for you.  Of course you can watch it on the stream, too.  You don't have to come all the way to Petaluma.  Just go to TWiT.tv/live and watch the live stream.  If you do that, make sure you're in the chatroom.  Nice bunch of people there at irc.twit.tv.  That's about it on this side of the microphone.  Steve, thanks so much.



STEVE:  Okay, my friend.  Talk to you next week.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#665

DATE:		May 29, 2018

TITLE:		VPNFilter

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-665.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss Oracle's planned end of serialization, Ghostery's GDPR faux pas, the emergence of a clever new banking trojan, Amazon Echo and the Case of the Fuzzy Match, more welcome movement from Mozilla, yet another steganographic hideout, an actual real-world appearance of HTTP Error 418 (I'm a Teapot!), the hype over Z-Wave's Z-Shave, and a deep dive into the half a million strong VPNFilter botnet. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about today, including how not to do a GDPR privacy email.  More information about a bank hack called BackSwap that might be in your memory.  And of course the deets, even some information not reported elsewhere, on VPNFilter.  Why did the FBI tell us to reboot our routers this week?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 665, recorded Tuesday, May 29th, 2018:  VPNFilter.



It's time for Security Now!, the show where we talk about your security and privacy online with this guy right here, the king of security, Mr. Steve Gibson.  Hello, Steve.  Nice to see you.



STEVE GIBSON:  Leo, great to be with you again for 665.  One episode shy of the big 666.  Which of course puts it exactly two thirds through the entire life of this podcast.



LEO:  What?



STEVE:  Since we run out of digits at 999.



LEO:  Yeah, 666, 999, very nice.



STEVE:  We're now two thirds of the way through.  I think we'll see all the security problems are going to be resolved by the time we get to 999.



LEO:  Are you waiting for that?  Is that what's keeping you busy?



STEVE:  Yeah.  When we hit 999, everything will go quiet.  No more viruses, botnets, weird stuff, nothing.



LEO:  Nothing.



STEVE:  It's just going to get all boring on the Internet.



LEO:  And then we can go home.  We can go home.



STEVE:  We're only two thirds - or stay home.  We're only two thirds of the way through, and hell is breaking loose.



LEO:  What?  What?  Tell me.



STEVE:  So of course we need to talk about the week's big news, VPNFilter, which is what the Talos security group of Cisco named - I guess this must be the largest botnet in history, more than 500,000 mostly routers, a few NAS devices, but more than half a million routers commandeered into a single botnet, which so freaked out the world that special dispensation was made available to deal with it.  But we've got to talk about it because this is sort of a state-of-the-art, state-sponsored-level botnet which deservedly was in the news.  And my best buddy a couple days ago said, "The FBI says I have to reboot my router."  What?  And it's like, so we'll explain about that, and why they said that, and why that isn't enough for our listeners.  But that's at the end.



We've got lots of other stuff to talk about, of course.  The really good news of Oracle's planned end of serialization in Java, which I'll explain.  Ghostery's really kind of embarrassing - not even kind of - GDPR faux pas.  The emergence of a clever new banking trojan which manages to avoid all the things that browsers and AV tools have over the years amassed in order to prevent what it does from doing.  Amazon Echo and the Case of the Fuzzy Match.  We also have more welcome news from Mozilla.  Yet another stegano - okay, I can't do it.



LEO:  Steganography.



STEVE:  Steganographic hideout.



LEO:  Steganographic.



STEVE:  Steganographic hideout.



LEO:  Just think stegosaurus.



STEVE:  A place for the steganographies to hide.  An actual real-world appearance, Leo, of the HTTP 418 "I'm a Teapot" error.



LEO:  No.



STEVE:  It actually happened last week.  



LEO:  No, no.



STEVE:  We also have the hype over Z-Wave's Z-Shave attack, which the press just went ballistic over, the idea of more than a hundred million IoT devices vulnerable.  People can walk through your Z-Wave locked front door.  Except not so much.  And a deep dive into the half a million strong VPNFilter botnet.  So I think lots of fun for our listeners this week.



LEO:  Yeah, I really want to hear your story on that one.  That's going to be very interesting.  It's kind of a fascinating one.



STEVE:  Yeah.



LEO:  Okay.  On with the show, my friend.



STEVE:  Cool.  So our Picture of the Week relates to the main topic of the week, VPNFilter.  It's a great photo that demonstrates the complexity of the trojan that we'll be discussing.



LEO:  This is how it works.  This is that diagram, yeah.



STEVE:  Yes, yes.  And, I mean, it does some cool stuff.  I'll get into it in detail.  But, for example, after the router is first infected, it reaches out via a URL for a photo at Photobucket, and it uses the GPS coordinates where that photo was presumably taken to get the IP address of what's known as the Stage 2 server, which is what provides the in-RAM component of the trojan.  And then there's a fallback.  If the Photobucket image for whatever reason doesn't provide it with an IP address, then it's able to do more.  We'll talk about it at the end of the podcast.



LEO:  Yeah, okay.  Yeah, I won't ask you the details because I'm curious about this Photobucket thing.  This is not steganography.  It was simpler than that.



STEVE:  Yeah, exactly.  All they did was they contrived the GPS coordinates where the picture was supposedly taken in the EXIF data tagged onto the photo.



LEO:  Got it, got it.



STEVE:  So just sort of a way of it doing something that looks innocuous, but actually wasn't.  So anyway, very cool.



We've talked actually a lot, one of the recurring memes of this podcast is the danger - danger, Will Robinson - the danger of interpreters.



LEO:  Don't make me play the sound effect.



STEVE:  I was just going to say, Leo, I'm sorry...



CLIP:  Danger, danger.



LEO:  I have a button, you know.  I can, any time...



STEVE:  I knew you had to be sorely tempted.



LEO:  You said it.  All right.  I promise, no more, no more.



STEVE:  So interpreters are a recurring problem, I would argue a nightmare, when for example you can be attacked by a JPEG image.  That's an interpreter failure because the JPEG is a structured image which some code reads in order to recreate, like to decompress the JPEG image, and clever hackers figure out a way to abuse that interpreter.  Well, another one that's been in the news recently because of the Equifax breach and the problems with Apache Struts was Java.  And we've talked about the so-called serialization/deserialization.  It's a mechanism that was stuck into Java back in 1997, back in the early days, and it's very convenient in that it allows a Java object to be stored or sent over the network through the process known as serialization.  That is, it takes a structured complex thing and turns it into a stream of bytes.



Well, at the other end you need to reconstruct the thing that you essentially deconstructed in order to stream it, and so that's the deserialization.  And, unfortunately, that's an interpreter.  And it turns out that somewhere between one third to maybe as many as one half of Java's historical security problems have arisen from this mechanism, from this serialization/deserialization process.



As a consequence, Mark Reinhold, who's the chief architect of the Java platform group at Oracle, labeled this whole serialization thing a, quote, "horrible mistake," unquote, in the Java language and has said it's being removed.  They're essentially going to replace it with, at some future point in Java's life - and Java has turned out to be a very important implementation technology that's getting a lot of use in the enterprise because it is a write once, run anywhere technology because you have the so-called JVM, the Java Virtual Machine, which interprets the Java bytecode which the Java compiler turns Java language into, which then allows you to get lots of portability.



So, I mean, there's a place for it.  We've run into it on this podcast for a long time.  That's finally ended.  It only ended when Java stopped being invokable by browsers.  And that took, like, 10 years of disaster before finally it's like, okay, well, I guess we're going to unhook Java from our browsers, oh, boohoo.  Nobody misses it from the browsers.  It just never should have happened, but it did.  The good news is it's going away.  And with it will come an increase, sort of a belated increase in the Java systems security.  Remember that...



LEO:  I hate computer scientists because instead of just saying, oh, it's the ability to interpret bytecode coming from the 'Net, they call it "serialization."



STEVE:  Right.



LEO:  Which just obfuscates its purpose.



STEVE:  Exactly.



LEO:  Yeah.  But computer science is full of these things.



STEVE:  Yes.



LEO:  Okay.  Thank you for explaining.  I appreciate it.



STEVE:  And like reference docs.  I'm often reading a lot of research.  And you'll read the beginning of it, and it's like, oh, what?  Sounds like it's a big deal.  And then you get into it, and you realize they're just using big words.



LEO:  Yes.



STEVE:  And it's like, oh, why didn't you just say you added two things and you got the sum?



LEO:  Exactly.



STEVE:  Instead of, well, you know...



LEO:  It's a closure.  It's all about the lexical scoping of the...



STEVE:  Exactly.  



LEO:  Oh, please.



STEVE:  So the good news is it's going away.  What they're going to do is they're going to believe a plugin framework so that JSON or XML could be used as application-specific simple serialization for specific needs.  So rather than just having it always there and always vulnerable, if somebody needs it, they can just stick in a JSON or an XML plugin to perform that job for them.  So yay.  That's like, what is it, it only took, what, 20, 30 years?



LEO:  Well, I think it's often the case that you write things to be as powerful as possible, and then you discover the malicious applications.



STEVE:  And, you know, I took a lot of heat 12 years ago when I innocently said that the Windows metafile format had an undocumented hook that allowed...



LEO:  This is a serializer; isn't it.  That's what it does.



STEVE:  Yes, it's an interpreter, exactly.  And you would be stuck with what the interpreter did, except there was one bytecode that said execute what follows as native code.  It was like, even Mark Russinovich said, yes, I've looked at it.  That's what this does.



LEO:  Yeah.  So you could, theoretically, you could do the equivalent of another computer science term, sanitizing your inputs to make sure that it doesn't just execute random code off the 'Net.



STEVE:  Yes.



LEO:  You could write a good serializer, in other words.



STEVE:  Well, and as we've discussed, it's so often the case that this stuff is hard to get working.  So the moment you get it working, and you're behind schedule...



LEO:  Yeah, it freezes.  It's like, done.  Ship.



STEVE:  Your boss is furious with you.



LEO:  Ship it.



STEVE:  You haven't seen your friends or even your spouse for a month or two, and you're not having any fun.  You're all hyped up on Coke or Mountain Dew or something.  And then when it finally works, it's like, oh, thank god.  And frankly, that's been sort of one of the advantages I've had with SQRL is that I didn't have someone making me ship this thing before it was ready.  And I was even able to do this level of verification throughout it in order to make sure I got this thing cleaned up.  So anyway...



LEO:  Serialization isn't bad.  But allowing code to come in over the 'Net and be executed is bad.  And that's the thing.  It's what you do with this serialization that's the problem.



STEVE:  Well, and you never want stuff to be slow.  So if you put - or, for example, and you've coded enough.  How many times have we seen coding examples where they said, for the sake of clarity, error checking has been left out of this.



LEO:  Yeah.  But imagine it's there.  Right.



STEVE:  What they didn't say was please don't ship it as is.  I mean, and that famous example of the Intel sample, or with UPnP,  where here was like demo code, and it ended up just being dropped right into the routers.  Nooooo.  But that's what they did.



LEO:  Yeah.  The programming book I'm reading right now says, look, you're going to steal from yourself, even.  You're going to copy code.  You're going to because everybody does.  So we're going to make it hard for you to do that because it's a bad idea.



STEVE:  Yeah.  So in other bad ideas, we have Ghostery.



LEO:  Now, you love Ghostery.



STEVE:  I do like Ghostery.  Ghostery itself is not a bad idea.  But they sort of screwed up last week.



LEO:  Uh-oh.



STEVE:  They thought to celebrate all of their privacy wonderfulness...



LEO:  Oh, I got this email.



STEVE:  Uh-huh.  They were going to send out a notice to all of the registered Ghosteryiers, telling them about the GDPR compliance and how they're holding themselves.  In fact, on May 25th they tweeted:  "We at @Ghostery hold ourselves to a high standard when it comes to users' privacy."



LEO:  Mm-hmm.



STEVE:  Uh-huh.  You go.



LEO:  Which is what Ghostery was written to do, by the way.  It protects plugins on pages.



STEVE:  Yes, it's a privacy-enhancing web extension, web browser extension.  So you're Ghostery, and you've recently also decided to take your previously outsourced email list management in-house in order to carefully manage the privacy of your users and take full responsibility for their care.



LEO:  Sounds sensible.



STEVE:  Yeah, yeah.  In the wake of the new GDPR regulations, you decide to send an update email to each of your registered users containing a privacy policy update.



LEO:  Of course.



STEVE:  Unfortunately, the person who is put in charge of this task is apparently insufficiently familiar with the operation of the shiny new in-house email system.  So what is sent out to users, against all intention I'm sure, is email containing, I kid you not, a series of emails, each containing in their "TO" header, stuffed to the brim, 500 email addresses of Ghostery's privately registered user email.



CLIP:  Doh.



LEO:  I had this one, too.  I had to do it.



STEVE:  So not the BCC.  Not the, I mean, it looks like you're the only one that received this, and you're just assuming everybody else did.  No.  You've got the email addresses of 499 other people who may be in alphabetical order, I didn't see it myself, who Ghostery also sent, in that particular block of 500, that single piece of email to.  Yikes.  So whoopsie.  Of course, Twitter had some fun with this.  We have Matt saying:  "You cannot make this up.  The privacy-driven Chrome extension @Ghostery for blocking third-party trackers has done its GDPR mass email exposing all of its users in the email 'TO' field."



Daniel says:  "Hey @Ghostery.  You sent your privacy policy update email without blind CC'ing the contacts.  Now I'm on an email chain with a whole bunch of randoms, replying to your no-reply email address."  Can you imagine?  Reply all.  



LEO:  Do not reply all.  Do not reply all, whatever you do.



STEVE:  And then Dan Barker tweets:  "Weird move from @Ghostery.  1.  Accidentally share thousands of email addresses with users in a GDPR email.  2.  Apologize for doing so on Twitter.  3.  Delete the apology tweet."  Who knows.  Anyway, ouch.  As I say, stuff happens.  And Ghostery stepped in it.  So, yikes.



Okay.  Now, this is cool.  Well, worrisomely cool.  The ESET Security guys have found kind of a diabolical new banking trojan.  And Leo, you're going to get a kick out of this one.  Okay.  So first of all, banking trojans are still, like, at least a third of phishing email are banking trojans.  In recent years they've been a little endangered because there have been other ways of getting money.



Once upon a time, before cryptocurrency, before it occurred to somebody that you could encrypt everyone's files and ransom them, all of these things generating cash, sort of the obvious "why do you rob the bank" question was, oh, let's put trojans in people's computers that watch them do banking and get their money.  So years ago, the beginning of the podcast, people were - their checking accounts were emptied because they got some stuff on their family shared computer after Junior had his friends come home and stick a floppy drive - back then, remember those - in the computer, and off it went.



So we here haven't talked recently a lot about banking trojans, although their names are well known:  Zeus, remember Zeus, also known as Zbot, which also begat Citadel, Atmos, and Floki Bot.  Then there's Neverquest, Gozi, Dridex, Ramnit, Gozer, GozNym, Tinba, Gootkit, I mean, the list just goes on and on because that's where the money is.  And so these things want to get into people's machines.



So in their article, in their description of this new, like there's been an upsurge in what ESET calls BankSwap, is their trojan.  I'm sorry, BackSwap.  BackSwap.  They said:  "Banking malware, also referred to as banker, has been decreasing in popularity among cybercrooks for a few years now, one of the reasons being that both antimalware companies and web browser developers are continuously widening the scope of their protection mechanisms" - this is key because this thing has gotten around this - "against banking trojan attacks.  This results in conventional banking malware fraud becoming more complicated to pull off every day, resulting in malware authors shifting their time and resources into developing easier-to-make and more profitable types of malware like ransomware, cryptominers, cryptocurrency stealers and so forth."  Again, those are the things we've been talking about recently because they've been a workable working source, and arguably easier to perpetrate against today's hardened AV and browsers.



So BackSwap has taken a new tack which is, again, is sort of diabolical.  It passively monitors the user's computer activity, watching for banking activity.  Then when it observes some, it uses two tricks to get its own interception code to execute.  So first of all, this gets into people's computers, piggybacking on some legitimate-looking application.  It's been seen in SQLMon, DbgView, the WinRAR uninstaller, 7Zip, OllyDbg, the FileZilla Server, among other apps.  So apps are trojanized and then salted out on the Internet, where unwitting users say, oh, I want to uninstall WinRAR, and so they go get it in order to do it.  So that allows it to set up shop in the person's machine.



What it then does is, now, traditional trojans have a whole bunch of problems.  That is, they want to get into the browser, which is where online banking happens.  Yet browsers might be either 32 bits or 64 bits.  Well, if you're going to inject some code into the browser's process, you have to know which.  And since you don't know which, you've got to have both.  Also you need to be able to intercept the nonencrypted content, that is, before it gets encrypted, on the way out; and after it gets decrypted, on the way back, in order to get a shim, to get a hook in there.



Well, in the case of the Chromium-based browsers, it is those locations are buried deeply in the browser.  They're not easy to find.  And they are always changing.  So they of course change from one browser make to another.  Mozilla Firefox and Chromium and IE, none of them have the same architectures.  So if you're going to do something that's generally going to be effective, you have to special case each of the browsers, each of the bitnesses, 32 or 64.  And every time there's a new version, all of your work gets thrown out, and you have to do it again.  So when you couple that with just the general difficulty of attacking the browser process with the heightened security...



CLIP:  [Indiscernible].



LEO:  I'm sorry.  I'm sorry.  That was not me.  That wasn't me.  It was somebody else.



STEVE:  Along with the heightened pressure against browser processes being interfered with and third-party AV, which is itself watching all this and trying to protect browsers.  The bar has been really elevated.  So BackSwap comes up with a whole new approach.  It bypasses all of this by working with the Windows GUI elements and simulating user input.  You can kind of think of it as malicious keystroke macros, which are running on the user's desktop, and which are therefore browser agnostic.  Doesn't care what kind of browser you have.



What it does is it hooks into the Windows accessibility interface on the desktop so that it's able to watch what's going on.  It scans the URLs coming and going.  And if it sees a pattern match with a bank that it knows how to handle, that it recognizes, it's able to immediately essentially jump in and inject JavaScript into the user's browser.



How does it do that?  It does it by simulating the user keystrokes, by typing them.  In the older samples that ESET found, the malware originally inserted the malicious script onto the clipboard, and then simulated pressing the key combination for opening the developers console, Ctrl-Shift-J in Google Chrome, Ctrl-Shift-K in Firefox.  Then it would issue a  Ctrl-V, which is the Windows key combination for Paste, which pastes the contents of the clipboard into the console.  Then it would press Enter to execute the JavaScript.  It sends all of that so quickly that the user doesn't see it and is able to hide the window so that the user just sort of experiences a brief pause and doesn't know that anything happened.



In later versions of this malware they've upgraded the approach to take advantage of another feature in all browsers.  It's like one of those things where it's like, okay, why do all browsers do this?  You know how we have http://, https: and so forth.  You could also have FTP and other things.  And in fact I use SQRL.  The SQRL client registers that as a means for the browser to talk to the client in the system.  So that's called the URL scheme.



Well, browsers are also aware of the JavaScript scheme, J-A-V-A-S-C-R-I-P-T colon, and you can then put JavaScript after that which the browser will execute.  You just stick it in the URL.  So, not surprisingly, this BackSwap banking trojan is able to type J-A-V-A-S-C-R-I-P-T colon into the URL.  It first does a Ctrl-L, which is the universal "give the URL the focus."  It then types J-A-V-A-S-C-R-I-P-T colon.  And what's interesting is it doesn't paste that because browsers won't let you paste that.  There's an actual prohibition against pasting a URL containing JavaScript colon anything because they said, oh, that would be a vehicle to attack.  Well, yes.



It turns out that, however, they cannot discriminate between the malware typing it in manually and the user typing it in manually because, in terms of the Windows message loop, which is the way all this is done, it looks the same.  And it's just like a keystroke macro that would be entering it.  So it types J-A-V-A-S-C-R-I-P-T, then enters the long JavaScript blob, basically the entire JavaScript injection, into the URL, hits Enter, and then blanks the URL as if nothing wrong had happened, and is able thereby to inject its own JavaScript into the user's browser.  And this has now jumped onto the scene and is back in the game, essentially browser agnostic, doesn't care about bitness, performs no in-process injection.  AV doesn't know how to block it, and browsers aren't blocking it.  And this thing has just taken off and is doing a gangbuster business as a banking trojan.



So we see where there's a will, there's a way.  And some very clever people behind BackSwap have figured out a means of getting past all of our AV.  So the takeaway for our listeners is don't get yourself infected with this thing, basically.



LEO:  Okay.  Thanks for the tip.  I'll do my best.



STEVE:  If you do, you're probably in trouble.  I would argue that, because banking trojans are still a thing - it's easy to get phished.  It's easy to get yourself hooked by one.  With computers being as inexpensive as they are now, and if you're someone who does a lot of online banking, if there's a possibility that you've got a lot of money sitting in an account, and you're doing funds transfers, take an older laptop or an older machine and just stick Debian Linux on it.  It costs nothing.  It's free.  It comes with Firefox installed.  And just don't use it for anything else.  Let that be the machine where all you do is your banking.  It boots pretty quickly.  I think it's a great solution.



LEO:  Don't use email on it, though, because you could in theory still be phished.



STEVE:  That's true.  Exactly.



LEO:  Just banking.



STEVE:  Really you only want to just - just banking.  So that requires some self-control, but probably worthwhile.



LEO:  I know a lot of people now are using Chromebooks for banking.  And I think that's equally secure.



STEVE:  I agree.  That's another great solution.



LEO:  Cheap.  You can buy a couple hundred dollar Chromebook and just keep it for the stuff you need to be secure.



STEVE:  Yup.  It's interesting, too.  As I was reading this, I was thinking, oh, well, that's nice.  One of the things that I also did in the SQRL client, as our listeners will all be seeing soon, is I use a secondary darkened Windows desktop, much like the UAC.  The UAC in Windows is much more than just a visual effect.  It's actually a second desktop which is invoked which is secure.  And so that prevents keystroke loggers and keystroke inserters and so forth from getting literally their hooks into it.



And it's funny because I initially implemented this because I wanted the SQRL authentication prompt not to be spoofable.  That is, I didn't want a web page to be able to pop up a dialogue that looked exactly like the prompt for logging into SQRL because a lot of users would go, oh.  Like when they were logging in with SQRL, they would expect that.  And so I needed to do something that a browser could not spoof.  And switching to a secure desktop, I monochrome it and darken it in order to make it look very distinctive because no code in a browser is able to mess with the exterior of the browser.  So in the process, a side effect of that is that keystroke loggers that might be monitoring what you're doing get disconnected.  So it's just another benefit of sort of the belt-and-suspenders approach.



Okay.  So Amazon Echo and the Case of the Fuzzy Match.  This initially seems scary, but Amazon explains it.  And I would argue, within the bounds of what we want, they were not at fault.  KIRO, which is the Channel 7 ABC affiliate in Seattle, carried a story last week about a misbehaving Amazon Echo device.  A family in Portland contacted Amazon to investigate after they said a private conversation in their home was recorded by Amazon's Echo, and that the recorded audio was sent to the phone of a random person in Seattle who happened to be on the family's contact list.



The wife of the family, Danielle, who did not want her last name revealed, said in the KIRO reporting of this:  "My husband and I would joke and say, 'I bet these devices were listening to what we're saying.'"  And they were big Echo users.  Every room in their home was wired with Amazon devices to control heat, lighting, and security.  But Danielle said two weeks ago their love for the Echo changed - and I would argue, well, we'll explain how this happened in a second - with an alarming phone call.  The person at the other end of the call said, "Unplug your Echo devices right now.  You're being hacked."  That was not true, but you could understand why they might think so.  That person was one of her husband's employees calling from nearby Seattle.



So she says:  "We unplugged all of them, and he proceeded to tell us that he'd received audio files of recordings from inside our house."  She said:  "At first my husband was like, 'No, you didn't.'"  And the recipient of the message then explained that "You sat there talking about hardwood floors."  And so they realized, oh, yes, we were just talking about hardwood floors.  You did hear us.



So she listened to the conversation, which was then sent back to her, and in this reporting said that she couldn't believe that someone nearly 200 miles away had also heard their conversation.  Yeah, well, welcome to the world of technology.



Okay.  So they got a hold of Amazon.  Took them a while to get to an engineer who finally said that the Amazon engineers went through the logs, and they saw exactly what had been described, and apologized.  She said that the engineer apologized, like, 15 times in a matter of half an hour, said that they really appreciated it being brought to their attention, and that that was something that they needed to fix.  Although Danielle said that the engineer did not provide any specifics about why it happened and, for example, whether it's a widespread issue. 



Okay.  So KIRO, using the strength of their being a network affiliate, did get through to Amazon and got some answers.  They spoke to someone, Shelby Lichliter at Amazon, who sent them the following statement:  "The Echo woke due to a word in the background conversation," sounding like the trigger word, which I won't repeat, we all know.  "Then the subsequent conversation was heard as a 'send message' request.  At which point [the Echo] said out loud, 'To whom?'  At which point the background conversation was interpreted as a name in the customer's contact list.  [The device] then asked out loud [repeated the contact name] and said, 'Right?'  [And the Echo] then interpreted the background conversation as, 'Right.'  As unlikely as this string of events is," said the spokeswoman from Amazon, "that's what occurred."  And, she said:  "We are evaluating options to make this case even less likely."



So as I said, Amazon Echo and the Case of the Fuzzy Match.  This is inherent in a voice-controlled system where you want to do everything by voice, and you are doing fuzzy matching.  And it's to my mind entirely feasible and reasonable that there could have been a conversation in a home where every room has one of these devices.  And if you don't have at some point something that says for a high-security action - and this isn't particularly high security.  If you want to voice-enable sending something that you say to somebody on your contact list, and that's an interactive interchange with confirmation checkpoints by the device, and the confirmations are also audio, and at no point do you have to touch it or push a button or do anything else, there's going to be some percentage of time in the universe of these devices always on, always available, always listening, like wanting to reliably catch their trigger phrase and not frustrate their users.  It's a balancing act.



And so it's entirely reasonable, if you want this much convenience, that you're going to, I mean, these people were freaked out.  And it's like, well, yes.  But you have Echo-enabled your whole home, and you're somehow missing the confirmation audio prompts, and that can happen.



So this got picked up by the press.  It's like, oh, my god, another Amazon Echo spying thing.  And it's like, no.  We talk about it on this podcast.  One of our other memes is the security versus convenience tradeoff.  And it's hugely convenient to be able to use your voice to order things to happen, but you might get an unasked for package of toilet paper at your front door from time to time.



LEO:  I mean, it's a highly unlikely coincidence of things.  And I think they could do some things to improve it.  For one thing, it sends those messages off without confirmation.  And Siri and everybody else will say, "Oh, is this the message you wish to send?" and ask for confirmation.  And I think that that's a simple thing Amazon will almost certainly do to the Echo to prevent these kinds of things.  But, yeah, we've all heard the Echo respond to TV, or Google Home.



STEVE:  Yes.  It suddenly lights up.  Especially loud television.  It's like, looking around, saying, what?  What?  Me?



LEO:  You talking to me?  I have a Home, a Siri-based HomePod, a Google Home, and an Amazon Echo in my kitchen.  And not a day goes by without them interacting.  And we just get used to it.  It's like you hear her mumbling in the corner, we go, oh, yeah, whatever.  It happened this morning, and I didn't even know what she was talking about.  It's just like your crazy aunt over there in the corner.



STEVE:  Remember, I think it was on "The Jetsons," it was Rosie the Robot?



LEO:  Yeah, Rosie the Robot, yeah.



STEVE:  And so she's just sort of...



LEO:  Mrmrmrmr, under her breath, mrmrmrmr.  



STEVE:  Yeah, just sort of, like, rolling around dusting stuff.  And it's like, okay, Rosie.



LEO:  Rrrrrrrr.



STEVE:  Yeah.



LEO:  So anyway.



STEVE:  Yeah, anyway.  I just sort of wanted to say that, yes, it doesn't have to be hacked.  It doesn't have to be malware.  It just can be voice-control where some percentage, I mean, there's like there's a non-zero probability that the device could interpret, I mean, any length.  Yes, as the interaction is more back and forth, the probability of it happening is going to go down.  But if you're not at any point requiring the person to press a button to confirm something, to like physically take an action, if it's all 100% voice, then there's a percentage of misfire that will happen.  And so you have to sort of have that, you know, understand the tradeoff of convenience versus security.



LEO:  And when Echo talks to you, listen, because she's trying to do something.  See what she's trying to do.



STEVE:  Yes.  Bad Echo.  Bad Echo.



LEO:  Bad Echo, no.



STEVE:  So we're currently at Firefox 60, and it remains my go-to browser, mostly just because of tabs.  I just - I have all the tabs I want, and I want a lot of tabs.  And Chrome refuses to deal with that.  So, fine.  Until they come up with a way of giving me more tabs, I'm sticking with Firefox.  And the good news is Firefox is keeping up.  With 63, and we're at six zero now, so three versions from now when that hits mainstream, they're adding a number of new features.  And I still like my term "mal-mining," which to me is just much clearer than "cryptojacking."  We've got so many crypto this and crypto that, that it just doesn't stand out.  Cryptojacking doesn't feel like cryptocurrency mining hijacking, which is what it is.  Mal-mining, yes.  Anyway, the good news is Firefox 63 can block that.



Okay.  So what they've done is they're sort of overloading this category known as tracking protection.  They're throwing all kinds of other stuff in there.  They probably ought to just stop calling it "tracking protection" because, if it's also blocking crypto-mining and fingerprinting, well, I guess fingerprinting is tracking, but certainly crypto-mining is not.



Okay.  So just to back up a little bit.  Remember that right now Firefox private browsing mode has tracking protection on by default, but normal browsing doesn't.  Yet it's now recommended for people who want increased privacy and, significantly, performance, that you go under Tools > Options > Privacy & Security, and under Tracking Protection switch it to Always.  What you get then, even when you're not using private browsing, is significantly, noticeably, higher performance because, unfortunately, pages are just loading so much crap from third-party servers where they've got to do DNS lookups and go get the stuff and download it, that it really does slow down our pages.  So just saying no, thank you, don't want that stuff, it makes your pages run a lot snappier.



What is being added under tracking protection is granular control over analytics, advertising, fingerprinting, crypto-mining, and social media stuff.  And for each of those five categories you can say you always want it, you only want it in private windows, or you never want it.  So they're breaking it apart, giving you granular control and, happily, going to somehow, probably using some browser heuristics, look at a tab which is apparently doing mining.  I mean, maybe they'll track miners on the fly, or they'll just do it behavior-based.  But you'll be able to say no, thank you, and not do that.



Oh, and significantly, there are also per-site exceptions.  So if there's a site, for example, say that crypto-mining to support the site you're visiting actually catches on someday.  Well, the site might say, you know, you're blocking our ads, and you're blocking our crypto-mining.  We'd like you to support us.  So either turn on ads or turn on mining so while you're here we can mine some cryptocurrency in the background.  So if you decide that seems reasonable, you'll be able to do that.  Also, they're unburying this somewhat.  You'll be able to get to it from that menu in the upper right-hand corner of Firefox, so quicker and easier access to tracking protection.



In the future, also with 63, they will be adding the ability to wash the content that a site gives you, cookies and other content loaded into your browser on behalf of that site in the little lock icon.  So you'll also be able to clean that up.  And they're adding one-time password support, Authy-style one-time password, time-based one-time passwords, to the Mozilla account preferences.  And so that'll just be appearing in coming weeks.  So Firefox is still in the game, and I'm glad.



I mentioned the tongue-twister "steganographic," steganography and so forth.  Steganography we've talked about from time to time, and I've sort of said, eh, it doesn't excite me that much.  It's the act of hiding in plain sight.  And so in our digital era where we've got crazy resolution of things, it's just not that difficult to do.  And I guess that's why it doesn't get me all worked up.  For example, if you've got a beautiful photo, where you've got 32-bit color depth, you've got high-resolution representation of each of R, G, and B color.  Our eye cannot see the least significant bits.  That is, if a color which ranges from zero to 255, if it's 167, if one pixel is 168 or 166, there's like plus or minus one on either side, we can't see it.  But it would be possible to take the entire image's least significant bits and hide a message there.



So you've essentially built a message into an image where the bandwidth is relatively low.  That is, you have a large image and a message which is a fraction of the image size.  But it doesn't require any particular trickery.  So in a sense you've hidden it in the image.  And only if someone knew where to go to get it would they be able to find it.  It's a little bit like we were talking about, the VPNFilter bot, which hides the IP address of the server in the GPS coordinates of a photo, where the photo was supposed to have been taken.  You know, that kind of thing.



Anyway, some researchers at Columbia came up with yet another place to hide data, and that is in the shapes, the detailed distortions of the shapes of font characters, characters in a font.  I avoided the word "glyph," but that's the technical term.  "Font glyphs" are these individual character instances.  And, boy, I've got the links in the show notes for anyone who's interested.  The science that they applied to inserting and distracting data from slight perturbations in font glyph shape is impressive.



As I've said, I'm sort of not that impressed in general with steganography.  It's like, yeah, okay, I guess that's kind of cool.  I would argue these guys took it all the way where - and they give an example of some text printed on a poster where they had encoded a relatively low-bandwidth message into tiny, I mean, non-visualizable distortions, so that you could stare at it, and you don't see any difference.  But their technology, given 200 pixels per inch resolution, is able to, from a photo of this chunk of text printed on a poster, is able to recreate the original image that was encoded into tiny distortions in the text.  So, I don't know, I wanted to put it on everyone's radar, just as another place that you could hide data.  And if you're curious about the details, it's an amazing piece of research.  But it's like, yeah, okay. 



Oh, it also occurred to me when I was thinking about this, where else could you hide information?  You could, in our digital era, you could use tiny distortions in the frequency of notes in a song.  Just detune notes a little bit sharp or a little bit flat, or just skew their frequency a little bit off beat.  I mean, there are just - there are so many ways that you could distort something such that, if you knew to look for it, you would know that that was non-random, and then you could extract a message.  But it's sort of, I don't know, it's like it's so possible to do, it's so easy that it's - except that these guys really applied some serious technology and math to it.  Other than that, eh.  Still, it was very impressive.



LEO:  Wait a minute.  Say that again so I can record it and put it on one of my buttons.  Eh.



STEVE:  Other than that, eh.



LEO:  Eh.



STEVE:  Okay.  So we've actually had, Leo, a real-world occurrence of HTTP Error 418 I'm a Teapot.  



LEO:  How can I do that?  Can I make it happen here?



STEVE:  To remind our listeners, it was April 1st, not surprisingly, of 1998 when RFC - and Leo, you probably want to look this up, RFC 2324.  So it's an official IETF.org document, RFC 2324.  It is the HTCPCP protocol, which is the Hyper Text Coffee Pot Control Protocol.  And we're all familiar with the infamous HTTP 404.  That's the page not found.  You don't normally see the 300s, like a 301 or a 302, because those are the redirects.  And you don't see them because your browser normally does them for you.  You click on a link to content that is moved somewhere else, and what you get back is a 301 redirect  which then your browser sees and goes, oh, that content is over at this URL, and you go there instead.  



LEO:  This introduces the brew method.  You've got posts, of course.  But this is brew.



STEVE:  Yes.  And in fact the RFC begins under rationale and scope.  It explains very seriously there is coffee all over the world.  Increasingly, in a world in which computing is ubiquitous, the computists want to make coffee.  



LEO:  There's also a "when" method.  So when you're adding milk you can say "when."  Oh, lord.



STEVE:  It says:  "Coffee brewing is an art.  But the distributed intelligence of the web-connected world transcends art.  Thus there is a strong, dark, rich requirement for a protocol designed espresso-ly..."



LEO:  Espresso-ly.



STEVE:  "...espresso-ly for the brewing of coffee.  Coffee is brewed using coffee pots.  Networked coffee pots require a control protocol, if they are to be controlled, naturally.  So the Hyper Text Coffee Pot Control Protocol," which you have been exploring, Leo.



LEO:  Yes.  Beautifully written.



STEVE:  In the RFC 2324 dated April 1st, 1998.



LEO:  Uh-huh.



STEVE:  So what happened?  For seven hours yesterday many developers using NPM - I'm not kidding - the Node.js Package Manager...



LEO:  Yes, I use it all the time, yes.



STEVE:  ...for JavaScript were receiving an error message:  "ERR! 418 I'm a Teapot."  The trouble was seen by those teams operating behind a proxy that was appending the port specifier ":443" to the domain name on its way across the proxy out onto the public Internet. 



LEO:  Well, that's SSL, isn't it?  That's - yeah.



STEVE:  Yeah.  But it was confusing the NPM registry's servers, which resulted in them returning an "ERR! 418 I'm a Teapot" declaration.



CLIP:  Doh.



STEVE:  Yup.  Somewhere, someone decided to return that error for unforeseen conditions.



LEO:  I love it.



STEVE:  And it actually did happen in the real world.



LEO:  So great.  



STEVE:  So, yes, "I'm a Teapot" lives, well past 1998.  



LEO:  So awesome.



STEVE:  Okay.  So this is one of those where the press is hyperventilating.  And I meant to grab the headline from Forbes because I saw that even Forbes picked it up with a breathless, oh my god, 100 million IoT devices potentially exposed.  And I'm sure you'll be talking about it tomorrow on This Week in Google with Stacey, since she's the IoT goddess of the network.  So this is the Z-Shave attack against Z-Wave IoT devices.  Z-Wave is the protocol, the technology which appears to be winning.  It has about a hundred-meter range, so it's much greater range than Bluetooth, which is one of its advantages.  And they finally got security right.  The bad news is they didn't get it right in the beginning.  And so it is true, I mean, what was found by these guys at Pen Test Partners is true.  But it's not as bad as people would think.



The headlines were:  "Z-Shave attack could impact 100 million IoT devices."  "Z-Wave downgrade attack left over 100 million IoT devices open to attackers."  "Z-Shave lets you open Z-wave based locks."



LEO:  That would be bad if you were using it to lock your front door.



STEVE:  That would be bad.  And so here's how I would characterize this.  Yes, a determined attacker who was also patient and who installed a battery-powered eavesdropping thing might be able to obtain the household's secret encryption key, after which it, yes, would have the ability to unlock the doors.  So it's not good.  I mean, it's not perfect.



LEO:  How long is the key?  Is it six digits?



STEVE:  No.  So here's the background.  When we did talk about all of this some time ago, so if any of our listeners don't remember us doing a podcast on Z-Wave, because we did earlier on because of the compromise in its security, and I didn't have a chance to - didn't occur to me to go back and get the Security Now! number, the podcast where we delved into this in great detail [SN-560].  But here's the deal.  You can have with Z-Wave no encryption security.  And, okay, that's bad.



LEO:  Yes, yes, I'll grant you that, yes.



STEVE:  That's bad.  You do not want that unless, I mean, maybe you've got an old hub that doesn't support encryption.  Well, okay.  Don't hook up a door lock or a security system to an unencrypted Z-Wave.  That's bad.  Or you could have S0 level encryption, which is not bad, but not perfect.  Or you can have S2 level encryption, which is perfect.  I mean, and I don't use that word loosely.  No one's ever heard me say that.  I mean, it's really, really, really, really, really good.  So, okay.  If either S0 or S2 security is in place in the environment, in the home, then an encrypted key is known to all Z-Wave devices, and all communication is securely encrypted under that secret key.



So the Z-Wave network, the way it works is it's got a secret.  And pairing a new device is the process of informing that device of the household's secret.  S0 encryption used an all-zeroes initial key, and that was its weakness.  If it used all zeroes, then a passive eavesdropper who could capture any pairing of a Z-Wave device could capture the traffic, know that they were using an all-zeroes key to communicate the actual, that is, to encrypt the actual home's secret, and then decrypt it themselves and be on the network.



Okay.  So that's the weakness of S0.  It is encrypted, but the pairing event creates a vulnerability that is significant.  As a consequence, S0 has been completely deprecated.  In fact, the Silicon Labs that are the owners of Z-Wave have loudly stated that, as of a year ago, no newly certified Z-Wave devices can have anything but S2.  Yet some 48-plus devices, as I recall, did receive certification with S0 after that deadline.  So, whoops.



The problem is, and we understand this, we've seen this over and over again, it is very difficult to move something that is working from "it's good enough" to "it's really good security."  Okay, but now here's the problem.  S2 exists.  Everyone knows how it works.  It uses Elliptic Curve Diffie-Hellman key exchange, thus perfect, as far as we know.  Diffie-Hellman is fabulous.  Notwithstanding the fact that that's what SQRL uses to do its crypto in several places.  The idea is that with S2 Z-Wave security, each of the devices in the pairing situation, rather than this transient use of an all-zeroes key to encrypt the actual key - which really is, you know, why bother?  If anyone's going to listen, then they're going to be able to get in under S0.  After that I guess the network is encrypted, so that's certainly good.



With S2, each of the two devices to be paired creates a nonce, a random chunk of nonsense, and sends it to the other.  The magic of Diffie-Hellman key agreement means that each end can obtain a secret in common which even somebody listening in and seeing that exchange occur, capturing all the traffic, cannot figure out what key it is that they each have.  And that's what's so cool about this.



So under S2 security, they each come up with a random number.  They share a version of that with each other.  And that allows them to independently arrive at a shared secret which the hub then uses to encrypt the household key, which it sends to the device being brought onto the network, since it has that ephemeral key that it got through Diffie-Hellman key exchange.  It uses that to decrypt, and now it's on the network.  Perfect security.  I mean, just it doesn't get any better than that.



Now here's where the attack comes in.  Turns out there is, because of the legacy, even, well, first of all, obviously both ends have to support S2.  If either end is only S0-aware, then you have what we've seen before, a protocol security downgrade.  That is, the more secure is forced to drop down to the level of the lesser secure in order to reach a protocol they both understand.  So the problem is that S0 devices are still predominant.  And in fact S0 hubs are still the majority.  Even Samsung's very popular Z-Wave Hub is still S0 security, despite the fact that S2 is what we want to be using.  So again, this is another thing that we see over and over is, even after security is figured out and made strong enough, everybody's got to play.



Okay.  Well, the fact that you have to support the security of the lesser end, that opens you to a real-time downgrade through active interference.  And that's what the guys at Pen Test Partners did that got them some headlines.  And I would argue okay, yes, it's true.  If, at the moment of two S2 devices pairing, you shot out a little bit of active noise, now you can't be passive anymore.  You've got to actively interfere with the communications in RF spectrum between the devices in order to intercept and force them to believe that each other is S0.  So it's high speed.  They couldn't even pull it off without generating some custom tools that could respond quickly enough.  So it's sort of even still theoretical, although they were able to demonstrate it in sort of one class of this attack.



So again, it's a bit of a tempest in a teapot.  It's just not a huge problem.  The vulnerability exists briefly.  It requires a very sophisticated attacker to be interfering with the radio communications during the brief interval of interchange between the two devices.  So I don't think there's anything to worry about here.  It's unfortunate that Z-Wave ever offered no security or S0 because it's going to just take a while for everything to get up to S2, and all the S0 devices to finally leave the ecosystem.  Eventually they will.  And then we've got a good system with very robust security.



Okay.  And one last final bit.  I got a very nice note from the Netherlands from someone who just said his name was Wouter, W-O-U-T-E-R.  He gave me permission to pronounce it Walter, but he also told me how to pronounce it.  So it's Wouter.  He says:  "Hi, Steve and Leo.  Thanks for talking about the difference between broken hard drive sectors and failing hard drives.  If the described problems and errors do not indicate a failing hard drive, what does?"  That's actually a really good point.



And he says:  "I once read a general recommendation to replace spinning hard drives every three years, but that sounds pretty costly.  I do have some older hard drives for use in Time Machine backups.  Should I worry?  Some general information on, A, how to spot failing hard drives; and, B, when to replace a hard drive would be greatly appreciated.  Regards, Wouter."



And we've talked about this.  We've skated around this a number of times.  As I mentioned recently, it is quite possible for sectors to be failing in the normal course of a drive's use.  That's why drives have spare sectors is because they may be needed.  They're expected to be needed.  And the drive won't start complaining until it starts running out of spare sectors.  So it expects that defects will arise over time, and they're called "grown defects."  That's a term of art in hard drives, "grown defects."  Defects grow.  So the sectors that acquire those defects need to be taken out of service and spares put in.  So one way that a drive could start getting in trouble, if it notices that its spares pool is drying up.  That's not good.  That will be reflected in the SMART data as a drive health problem.  But you need to know to look at this SMART data.



The other really more sensitive way to tell is the dynamic rate of error correction.  That is, and this is something that really doesn't get enough attention, and it's one of the coolest things about what SpinRite is able to do.  If a drive is just sitting there doing nothing, that is, not working at a given workload, then it's not producing.  It's not needing to do correction.  And so none of the metrics that SMART shows have any meaning.  But if you put it under load, as SpinRite does, then suddenly the rate at which errors are having to be corrected is a beautifully sensitive analog meter of the health of the drive.



If you, when you get the drive new, run SpinRite on it and look at the SMART data, one of the screens in SpinRite is a SMART data monitor screen.  It will show you the rate of error corrections in corrections per megabyte of data transfer, data read.  And you can make a note of that.  And from time to time, when you run the drive, take a look at that and see that it sort of remains constant.  If at some point you see a spike in that, well before the drive has collapsed on you, has failed, that rate will increase.  And that's the key to say, okay, that's an actual valid indication that the drive is having to work too hard in order to recover data.  It probably means that it's consuming its spares pool much more quickly, and you'll be in trouble soon.



So there is an indication.  If you just google "SpinRite SMART," S-M-A-R-T, the first link that Google shows is a link to the page at GRC that takes that SMART data monitor screen apart and shows you what all the different meters and metrics and things are.  It's kind of cool.  Take a look at it.  Just google "SpinRite SMART," and you'll see what I mean.



LEO:  Nice.



STEVE:  And Leo, our last break, and then we're going to dive into VPNFilter, a really interesting event in the industry this past week.



LEO:  I wanted to show you something I thought was really cool.  I know you'll kind of get it.  It's kind of a video, though, so I apologize for people listening.  Earlier today there was a BGP misrouting.  I'm sure you read about it.  Cloudflare...



STEVE:  Border Gateway Protocol, yeah.



LEO:  Yeah, Quad One's was accidentally hijacked by a Chinese ISP.  Quickly fixed, so not a big deal.  But Cloudflare has, I think, the coolest thing ever, which is an animated replay of the event.  And you can watch the routers, the BGP routers get hijacked and then reassemble.  It's really actually, I mean, you would understand what's going on here better than I do.  But it's just kind of fun to watch as the routers go, wait a minute, what happened here, and then figure it all out.  It's really neat, yeah. 



STEVE:  Right.



LEO:  Cloudflare's replay is just...



STEVE:  Yeah.  The idea, of course, is that routers advertise the routes that they're responsible for.  And quite non-maliciously, I mean, without any malicious intent, and it does happen from time to time, a router can declare that it has a much shorter path to a network than it actually...



LEO:  A shortcut.



STEVE:  Exactly, a shortcut.



LEO:  A shortcut.



STEVE:  ...than it actually does.  And so it advertises this short route, and all the routers it's connected to go, oh, cool.  And they send their traffic there instead of going the long way, which unfortunately was the right way.



LEO:  Yeah.  It was fixed later in the day.  But it was not even for that long.  And I don't think it was malicious.  I think they just were using 1.1.1.0/24 as a test route, which then Hurricane Electric propagated to the rest of the world.



STEVE:  Yeah.



LEO:  Thank you.  Thank you, Hurricane.  But it's fun.  This is so cool.  I never knew they had this tool.  So to watch these at work is really neat, really neat.  Little JavaScript animation there.



STEVE:  So unfortunately, so is this botnet.  So last week we had an evolving story regarding concerns surrounding a massive botnet of more than 500,000 routers.  Yes, more than half a million routers commandeered into a single botnet.  For anyone who's interested, the Talos Intelligence Group of Cisco did beautiful coverage of this.  I've got the link in the show notes.  And this is one of those situations where the answer to the question "What could it do?" is a chilling "Anything it wants."  I mean, if you've got half a million routers connected to the Internet, that's bad, I mean, when they're under control of a malicious entity.  Actually any entity because, I mean, you could even make a mistake like the Border Gateway Protocol routing mistake.



LEO:  Yeah, yeah.



STEVE:  Forensics evidence strongly implicated Russia as the botnet's creator, and it's believed to be intended as a network for attacking Ukraine.  Cisco found more than 500,000 instances of this VPNFilter malware on routers manufactured by - and this is what's important for our listeners because there's a takeaway here.  Some of these are not old junkie routers that no one has.  I ran across a note, for example, there was a D-Link something or other, DL-620 router that had a backdoor that was discovered, except that it's really, really old, and they don't support it anymore, and five people have them.  So it's like, okay, I'm not even going to bother talking about that.



These routers are mainstream routers.  So Linksys, MikroTik, Netgear, TP-Link, and also some QNAP NAS devices.  The Linksys E1200 and the E2500, as well as the Linksys WRVS4400N.  Three routers from MicroTik.  Did we decide it was MicroTik or MicroTik?



LEO:  Tik.  MicroTik.  MicroTik sounds like necrotic.  That's not a good...



STEVE:  Exactly, no.  Three MicroTik routers, the 1016, 1036, and 1072.  A bunch of Netgear routers:  the DGN2200, the R6400, the R7000, the R8000, the WNR1000, and the WNR2000.  And then  QNAP devices:  the TS251, the TS439 Pro, and other QNAP NAS devices running the QTS software.  And, finally, the TP-Link R600VPN.  If you have any of those routers, you need to pay attention because those routers are the routers that were hosting more than half a million instances of this VPNFilter malware.  It is a highly sophisticated complex botnet with a bunch of moving parts and a plugin architecture.  We talked about earlier this month the Hide & Seek botnet as the first botnet that had been seen to survive reboots.  Well, this is the second one.



So VPNFilter can survive a reboot of the router because it's able to install sort of a little stub of itself, which Cisco calls Stage One, into the router's flash memory.  So there are now two pieces of malware, Hide & Seek that we talked about earlier this month and VPNFilter, able to statically infiltrate and remain in a router.  So the takeaway is what has been recommended is certainly that you reboot your device.  I'll explain what that does and doesn't do in a minute.  But it's only by performing a factory reset back to the original device settings that we're told that flushes the malware out of the device.  I'm not sure why that's true.



So I think checking for any firmware updates and using this as a good time, a very good time to update your router firmware makes a lot of sense.  So if you're on that list of routers, see if there's an update.  And I would reflash my router, if you're concerned, with the latest firmware, even if it's the one you've already got, if your router will allow it to be reflashed with the same firmware.  It might say, oh, I've already got the latest one.  Go away.  Don't know.



LEO:  So rewriting the firmware eliminates the stub that's reloading the malware.



STEVE:  Yes.  Yes.



LEO:  Given that they've got the command-and-control servers, I guess reloading the malware's not going to happen anyway; right?



STEVE:  Yeah, actually it does.



LEO:  Oh, it does.



STEVE:  Yeah.  It's a little more complicated.



LEO:  So the FBI's advice is wrong.



STEVE:  It's not complete.  Okay.  So here's the deal.  Cisco breaks this thing down into three stages:  Stage One, Stage Two, Stage Three.  Stage One we've talked about.  That's the thing that goes persistent by modifying the flash and getting itself rerunning whenever you start the router.  When that starts up, what we were talking about at the top of the podcast occurs.  It first tries to go out to the photo sharing site and grab a photo from a URL and then know that the GPS pseudo coordinates are actually the IP address of the command-and-control server.  If that fails for any reason, then its backup plan is to do the same thing, pull a photo from a well-known domain.  And that domain is T-O-K-N-O-W-A-L-L dotcom, ToKnowAll.com.



So from either of those instances, the Stage One is able to obtain a larger RAM-resident Stage Two.  And this is clever because that means that you're making - the footprint in flash is very small, which means it's going to be able to find a home in many more routers.  If you had to load the whole blob of malware into Flash for persistence, there's much more chance that the file system that might not have much empty space wouldn't be able to accommodate it.  So these guys said, aha.  We're going to create a tiny little stub, and then we'll get the rest on the fly.  That part lives in RAM.  Okay.  So that part does something else.  And, well, okay.  So Stage Two sets up a plugin architecture which Stage Three are the plugins for.  But the other thing that didn't get mentioned by the FBI is that Stage One, the resident portion, also opens a listening port.



LEO:  Oh.



STEVE:  Yes.  And if the botnet authors had obtained a list of IP addresses that had been contacting the command-and-control server, then they're able to spray all of those IP addresses with new command-and-control stuff and maybe take it back.  So it seemed, as you think about this, this thing is so sophisticated and so nicely designed that you would expect there to be some means of recovering command-and-control once what's there has been reverse-engineered and the command-and-control servers have been taken down.  And in fact that recovery system exists.  So that's why - okay.



So the reason the FBI wants people to reboot is twofold.  First of all, it is true that after having taken down the main command-and-control servers, what happened was later last week they got a court order from a judge that allowed them to compel VeriSign, who I'm sure was quite happy to, but VeriSign said please compel us, to hand over domain control for the ToKnowAll.com domain.  The FBI pointed that domain to their own servers.  And then the botnet, that is, this half a million plus strong botnet, began pinging their server.



So now that allows the FBI to themselves obtain the IP addresses of all of these compromised routers.  And since they're not going to do anything nefarious with it, if users now reboot, a couple things happen.  In the reboot process, the Stage One does its outreach to these domains.  In the process of doing that, the FBI will get an updated fresh list of all the IPs of the routers which are compromised.  And since Stage One will not be able to obtain the Stage Two malware from either the photo sharing site or ToKnowAll.com, it doesn't go any further.  But the listening port is still open.



Now, the other thing I didn't talk about that Stage Two does, aside from creating a plugin framework, which allows some powerful plugins to be installed, and this is one of the things that really upset people, there is flash wiping capability in the Stage Two payload.  And this is what had people worried.  If this was meaning to be more than just a DDoS attack, which is a bandwidth flood, as we know, against targets in Ukraine, imagine if IP addresses throughout Ukraine were identified and all of the routers, all of the consumer small home, small office, small business routers were permanently wiped.



This Stage Two payload overwrites flash and then reboots the router, bricking it.  I mean, and permanently bricking it.  At that point you don't have firmware in order to reload the firmware.  It's over.  So the big concern was that this thing could have, I mean, like while these people were in control - and we may be talking about this next week because, as I said, there is a means for them to recover control, unless the FBI was prepared to outreach into those routers and do something themselves.  But, unfortunately, that's not legal.  You're not able to go and modify anybody else's hardware, even if it's for a well-intended benign purpose, like disinfecting them.  At least in the U.S., the law on that is very clear.



So the Stage Three plugins are able to sniff the local network traffic to detect the presence of the SCADA, S-C-A-D-A, industrial control network traffic, and also further communicate with the botnet's command-and-control servers through the Tor network.  All the communications is through Tor or TLS-encrypted connections.  So this thing is, you know, this is a seriously powerful, not a benign botnet.  The big concern was that it might have been, I mean, imagine if more than half a million routers self-destructed.



Presumably, if it really is Russia - and it looks to me like the attribution is very solid on this.  Cisco's Talos group wrote:  "We assess with high confidence that this malware is used to create an expansive, hard-to-attribute infrastructure that can be used to serve multiple operational needs of the threat actor.  Since the affected devices are legitimately owned by businesses or individuals, malicious activity conducted from infected devices could be mistakenly attributed to those who were actually victims of the actor.  The capabilities built into the various stages and plugins of the malware are extremely versatile and would enable the actor to take advantage of devices in multiple ways."



The Ukrainian Secret Service believed an attack was planned to take place on Saturday when the Ukrainian capital of Kiev was to be hosting the UEFA Champions League soccer final.  The FBI confirmed that the botnet was created and was under control of the famous Russian state-sponsored cyberespionage unit known under many names.  We've referred to them on the podcast as APT28 and also Fancy Bear.  They're also known by Sednit, Pawn Storm, Sofacy, Grizzly Steppe - actually we've talked about them under those terms - Strontium, Tsar Team, and others.  And the Estonian Foreign Intelligence Service identified APT28 as a unit of Russia military's Main Intelligence Directorate, the GRU.



So the command-and-control domain is under control of the FBI.  Everybody has been asked to reboot.  If you own one of those routers, do that at minimum, but also I would reflash.  Given that you are able to, certainly our listeners can.  If you have one of those routers, get the latest firmware available for it and reflash it while you can.  Oh, and Cisco did say that it did not appear to them that any zero-day vulnerabilities were responsible.  They thought it was publicly known old vulnerabilities that had not been patched.  Which suggests that the affected routers do need updated firmware, while after you flash remember disable all WAN-side network management.  If you can, turn off Universal Plug & Play on your LAN side, as well.



And by all means use a non-default strong username and password to set this thing up.  Every week now we are talking about how these little appliance routers, which are workhorses - it's funny, too, because I was thinking about how Stage Two and Three live in RAM.  On the other hand, our routers are probably the least often rebooted devices in our entire network.  Everything else is being plugged in, unplugged, moved around, turned on, turned off.  Those little routers sit there running for years at a time.  So it's really not a handicap that this thing, most of it lives in RAM.  That's not a problem.  



LEO:  I think that's how the other ones, Mirai bot and all those, are also resident.  They probably don't have a way to write enough code in the firmware to keep it persistent.



STEVE:  Right, right.  It takes doing more.  And they're quite happy to live in RAM because their devices are unlikely to get rebooted anytime soon.



LEO:  Well, and even if they do, they just reload.



STEVE:  Yup, exactly.



LEO:  So you put a couple of lines of code in the firmware that reloads it.



STEVE:  Yup.



LEO:  So if you didn't have an affected router - do you think that list is complete?  Or is it conceivable other routers are affected, and they just don't have the full list?



STEVE:  The problem is we have a sophisticated actor, in Cisco's parlance, a threat actor...



LEO:  It's a state actor, yeah.



STEVE:  Yeah, a state actor that wants to install their stuff everywhere.



LEO:  Right.



STEVE:  And there are routers we know have been vulnerable to other things that we've been talking about recently that are not on that list.  So it's certainly reasonable that in the future other routers are going to get infected with this.  So, I mean, more than half a million routers.  I would say there's never been a better time to make sure you're running the latest firmware.



LEO:  Yeah.  Reboot anyway, everybody, and then firmware update.  And that's one reason we always argue for IoT devices that automatically update, that have firmware patches.



STEVE:  Oh, yes, yes, yes, yes.



LEO:  Because it's a pain to reflash your router.  



STEVE:  Yes.



LEO:  And I bet you a lot of routers don't let you - I know my ASUS could reflash.  You could take...



STEVE:  With the same version?



LEO:  You could roll back if you wanted to, yeah.  But a lot of the consumer-grade routers are made to be as simple as possible.  And they just say, oh, no, you're up to date, and that's it.  In which case I don't know what you'd do.



STEVE:  Yeah.



LEO:  Get a better router.  This is an opportunity to get a better router.  Steve, good show, as always.  Very informative.  Every week we do this about 1:30 Pacific.



STEVE:  That's what we do here.



LEO:  4:30 Eastern, 20:30 UTC if you want to watch on Tuesdays at TWiT.tv/live.  You can also join us in the studio if you email tickets@twit.tv.  Be my studio, not Steve's.  Steve, as far as I know, does not have a live studio audience.  Where would you sit?



STEVE:  Yeah, no.



LEO:  Nowhere to put them.  But we do love having the studio audience up here in Northern California, if you come by.  You can also get copies of this show on demand at your convenience.  Of course go to GRC.com, that's Steve's website.  That's where you'll find the audio copies and the transcripts, as well, if you like to read along, or use the transcripts to search for parts of all of the 665 shows.  You can also get other things at GRC.com, including SpinRite, the world's best hard drive recovery and maintenance utility, a must-have for all hard drive owners, GRC.com.  Everything else there is free.  I used, when I was suffering from jet lag, just couldn't get my schedule back, I used your Healthy Sleep Formula.  Worked wonders.  Worked like a charm.



STEVE:  Cool.  Cool.



LEO:  Because the problem with jet lag, at least when you're going west to east, is you wake up in the middle of the night.  And so the ability to sleep through the night was critical to overcoming my Japanese jet lag.



STEVE:  Nice.



LEO:  Thank you, Steve.



STEVE:  Well, next week is 666.



LEO:  Yes.



STEVE:  So hold onto your seats, folks.



LEO:  The podcast of the beast coming up.  We will see you all then.  Thank you, Steve.



STEVE:  Thanks, buddy.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#666

DATE:		June 5, 2018

TITLE:		Certificate Transparency

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-666.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss yesterday's further good privacy news from Apple, the continuation of VPNFilter, an extremely clever web browser cross-site information leakage side-channel attack, and Microsoft Research's fork of OpenVPN for security in a post-quantum world.  Microsoft drops the ball on a zero-day remote code execution vulnerability in JScript, Valve finally patches a longstanding and very potent RCE vulnerability, Redis caching servers continue to be in serious trouble, a previously patched IE zero-day continues to find victims, and Google's latest Chrome browser has removed support for HTTP public key pinning (HPKP).  And, finally, what is "Certificate Transparency," and why do we need it?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a lot to talk about including the changes Apple's making to Safari and why they're really good for your privacy, and a little insider action on the certificate thing.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 666, recorded Tuesday, June 5th, 2018:  Certificate Transparency.



It's time for Security Now!, the show where we protect you online, thanks to the good auspices of this fellow right here, Mr. Steven "Tiberius" Gibson of GRC.com.



STEVE GIBSON:  Yo, Leo.



LEO:  Yo, yo, yo, what's happening, dude?



STEVE:  Well, we have made it to Episode 666.



LEO:  Oh, no.



STEVE:  So as I tease our listeners, we are two thirds of the way through the lifetime of this podcast because all of my systems only support three digits.  



LEO:  We'll talk about this in 333 podcasts.



STEVE:  That's right. But until then, an event that happened a couple days ago, Chrome 67, caused me to think, you know, we should do an introduction to something that we have never yet introduced, and that is certificate transparency.  This is an initiative that Google rather quietly initiated, like, four years ago.  My favorite certificate authority, of course we all know, DigiCert, was the first CA onboard.  And through the podcasts, through the 666 previous weeks that our listeners have endured, we have often talked about, because it's like a big issue for security, the problems with the CA system, with the Certificate Authority system, certificates being misissued, them being maliciously issued and so forth.



So certificate transparency is a publicly posted and publicly auditable log of certificates.  And it very elegantly - oh, and the other thing, of course, that I annoyed Google with was the revocation problem, the fact that, as we know, certificate revocation really never has worked.  So this is an interesting and way more present solution than anyone has really been aware of.  And I've touched on it from time to time.



The reason it was brought to mind for me this week is that Chrome 67 just dropped and removed a previous sort of attempt at solving this problem with something called HPKP - HTTP Public Key Pinning.  Well, that just got removed from Chrome, and that doesn't happen very often.  So we'll talk about that.  Lots of other stuff, too.  We've got some nice privacy-advancing news that, as you know, because you were present during yesterday's WWDC presentation where Craig Federini - Feder what?



LEO:  Federighi.



STEVE:  Federighi.  I want to make sure I don't say Ferengi.



LEO:  No, he's got better ears than that, yeah.



STEVE:  That's right.



LEO:  Well, his hair, I don't know.



STEVE:  Well, we want to talk a little bit about Apple's movement in the privacy direction.  Also a little bit of follow-up on last week's VPNFilter takedown by the FBI and them asking everybody to reboot their routers and what's been going on since.



The Picture of the Week involves an extremely clever web browser cross-site information leakage side-channel attack.  In other words, we've talked often about the same-origin policy and how rigorous browsers are to separate what can be run, what code or images can be displayed from what site and so forth.  This is a way of breaking the iframe sandbox that is just so cool that of course we have to talk about it.  And the browsers have scrambled in order to fix this.



We're going to talk a little bit about quantum crypto today which is - it's also been one of those things kind of lurking in the wings.  Microsoft Research, the research branch of Microsoft, has forked the OpenVPN project and developed an OpenVPN version which is quantum-safe, so they believe.  Also, at the same time, the front office of Microsoft dropped the ball on a significantly serious zero-day remote execution vulnerability.  Hopefully next Tuesday they're going to - because they're scrambling right now.  They were given 120 days.  It's like, whoops, sorry, the clock expired.  So we'll talk about that.



We've got Valve finally patching a longstanding and, geez, it had been horrifyingly potent remote code execution vulnerability that has existed in all 50 million instances of the Steam client for, like, 10 years.  We're going to revisit the Redis caching servers, which continue to be in serious trouble.  We have a little more information about that.  A previously patched zero-day in IE continuing to find victims.  And then, as I mentioned, we'll talk about Chrome 67, their removal of HPKP, why, what that means; do a little bit of housekeeping miscellany; and then talk about certificate transparency, which is I think without question where we're heading in the future.  So yes, another information-packed week.



LEO:  Yes, yes, yes.



STEVE:  So our Picture of the Week relates to a story that we'll be getting to a few stories from now, but it's just so pretty that I wanted to put it up there.  For those who are listening, it just shows a stack of layers, and the bottom one says "cross-origin iframe."  And then there's a target pixel sort of focused on that iframe; and then a bunch of layers on top of it labeled "difference," "lighten," "difference," "color-dodge," "multiply"; and then some transparency layers labeled "saturation."



And the point is that this is a multilayer, many layer stack which web browsers, contemporary web browsers that support an advanced feature of CSS3, which both Mozilla and Chrome do because they share a common graphics library in their foundations which does this.  This turns out to have been leveraged into what is essentially a side-channel leak in browsers that, for example, allows an unwitting person who visits a web page that does this to them to have, for example, their Facebook profile photo and their name divulged, even though it's explicitly against all of the browser rules for that to happen.  So the way this works is very cool, and we'll be talking about it in a minute.  But I wanted to put the picture up at the top because it's a pretty picture.



So since it just happened, I just thought I would mention that Apple is continuing, as they announced at the Worldwide Developers Conference yesterday morning, to move in the direction they've been going for some time, stating that one of the things that they're selling, one of the things their customers want, is increased privacy.  And so one thing that they've done in Safari, they announced macOS Mojave as the next major MacBook OS, and iOS 12.  I did note somewhere just digging around in the last day that apparently an iPhone 6s, which is now an old iPhone, would do something a lot faster under iOS 12.  I don't remember what it was, but it was like, oh.



LEO:  Everything.  Oh, no, it's a big, big change.



STEVE:  Oh, so it's a big performance improvement?



LEO:  Overall improvement, yeah, 70% improvement in opening the camera times, a double improvement in apps, I want to say app switching, 50%, big improvement.



STEVE:  Yay.



LEO:  And people already reported with the beta that they're seeing this, yeah, yeah, yeah.



STEVE:  Oh, yay, yay, yay.



LEO:  They really focused on performance.



STEVE:  Nice.  So that's good.  In Safari for both platforms, both macOS and iOS, we've talked a lot about fingerprinting, and that there's that Panopticlick site which makes it - I think the IETF sponsors it.  And that's that site which has experimented with and shows a user when you go to Panopticlick how unique the fingerprint is that results from hashing a bunch of the browser headers.  And we've talked about this through the years that, for example, the user-agent header will tell you not only what browser you're using, but what library of ActiveX, and their version numbers, you know, 3.29.724.0326.  And somebody else will have .0327, and so their browser headers hash to a different fingerprint than yours.



And when you combine that, like all the version numbers of the plugins, in some cases the browsers will enumerate which fonts they have, which font libraries they have installed, so that a web server could know what fonts are available to be rendered in the browser.  The browser headers have exploded over the years with all this extra information which with surprising accuracy can, without the user doing anything, allow sites that want to track them to lock onto them with this fingerprint.



So one of the things that we learned yesterday is that Safari is going to deliberately take action to thwart that kind of fingerprinting.  So that's good.  And it's funny, too, because in thinking about this, I realized how easy it is to confuse tracking with advertising.  I think that the two get mixed up a lot together because, when you start talking about blocking tracking, there's this, oh, but wait a minute, how are sites going to survive?  And it's like, wait a minute.  Advertising and tracking are separate things.  You can still have ads if you're not being tracked around the Internet.



So tracking blocking is different than adblocking.  And it seems that what we're seeing is a pushback, well, obviously, against both.  But there is an understanding, I think, and some websites are taking action on this as we've talked about, where if they see you running an adblocker, they can say - and I'm encountering that from time to time because I run uBlock Origin.  And I'll often say, oh, yeah, okay, fine, and allow ads on a site that I want to support.



Now, that sort of follows into the other feature that I'm going to be interested to experience.  And Leo, I don't know if you've talked to anybody who's using this next Safari.  But what they showed us yesterday during the opening "what's new" at the WWDC, was a pop-up which you would get, asking for permission for another site to track you on the site you're visiting.



LEO:  That's for like the Facebook Like button.  So that means on the page that you're on there's JavaScript that then sends data to a third page.  It's like third-party cookies, except it's not.  It's using, I presume, JavaScript to do that; right?



STEVE:  Well, actually it could be using first-party cookies.



LEO:  Right.



STEVE:  Because when you have an object like a Like button, then...



LEO:  It looks like it's coming from inside the house.



STEVE:  Exactly.  It actually is sourced by Facebook, so when you're logged into Facebook, as you move around the 'Net...



LEO:  But the browser knows where - so the thing is the browser knows what page you're ostensibly on, and then sees a link back to Facebook.  And that's when it says, uh-oh.



STEVE:  And so that's what I'm confused about.  They're calling this "Intelligent Tracking Prevention 2.0."



LEO:  Well, there's two things they're doing.  So there's that.  And then it looks like they're blocking supercookies.



STEVE:  Okay.  So that's the browser fingerprint I was just talking about, right.



LEO:  Yeah.  So I think those are separate.



STEVE:  They are.  And so what Craig said, for our listeners who didn't see this, he showed a Safari page with a popup notification reading, "Do you want to allow Facebook.com to use cookies and website data while browsing..." and then wherever it was you were.  And their example was Blabbermouth.net.  And so here's the problem, Leo.  There's nowhere you go today that doesn't have, like, all this third-party crap loaded onto the page.  So like I said, I'm going to be really interested to see what they're doing.  I mean, are they going to - I can't imagine they're going to single out Facebook or Google.



LEO:  No, although this was intended to single out Facebook and Google in the announcement.



STEVE:  Yeah.



LEO:  I mean, that was very clear that it was a shot to the Facebook and Google.  But, yeah, I mean, I guess they could treat any call to a third-party site like a third-party cookie; right?



STEVE:  But you'd be buried in permission dialogues.



LEO:  Yeah.



STEVE:  It's freaky.  And but on the second page of the show notes is my own chart from my own site that has always stood out in my mind.  Years ago browsers were not handling cookies correctly.  So I created this cookie forensics system.



LEO:  I remember this, yeah.



STEVE:  But what stands out among all the browsers is Safari.  Look at the Safari bar because Apple has always blocked third-party cookies by default.  And they're the only browser to do so.  And it's just - it's always impressed me that here among all the browsers - and obviously this chart is dated now.  We've still got Firefox v2, v3, and v4.  I haven't messed with this for a long time.  But there is like this little tiny bar of Safari.



Now, this protection fails, as I was just mentioning, when you start hosting content, like active content, from other sources because then you've got JavaScript and other technologies present.  But still Apple has, in fact, a long history of blocking this kind of action.  So anyway, I just wanted to share with our listeners that Apple is continuing to move forward.



LEO:  But you do raise an interesting point.



STEVE:  Oh, yeah.



LEO:  It's not beyond Apple to kind of trumpet something like this, but do something less aggressive under the hood.



STEVE:  I just - I don't know how it could possibly work.



LEO:  They can't do this, yeah, no, you're right.



STEVE:  It's just going to bury you.



LEO:  Be like NoScript.



STEVE:  Yeah, nothing would work.



LEO:  Nothing would work, yeah.



STEVE:  And maybe there's like a "don't ask me again" sort of thing.  I mean, that's the only way I could imagine it is, yeah, you immediately start using the new version.  So like it's not a per-site question, unless maybe you want it to be.  But it's like, no, and don't ever ask me again sort of option.  I just don't know.  It'll be really interesting to see how this plays out because, boy, I mean, these days there's nowhere you go that doesn't have third-party stuff.  So either they're explicitly looking for particular domains - of course then Google and Facebook would cry foul, saying, hey, you're singling us out for abuse compared to everybody else.  And it's going to be really interesting to see how this happens.  I don't know.



LEO:  Yeah.



STEVE:  So as we know, last week's podcast was VPNFilter, talking about this collection of routers that had known vulnerabilities being exploited by, we believe, actually there's strong forensic evidence to suggest that it is the APT28 group, aka Fancy Bear, which is operating out of Russia, largely targeting the Ukraine.  Since the takedown of this large botnet composed of all these routers - remember that the ToKnowAll.com domain was taken away from VeriSign's control over to the FBI, that were then using it in order to monitor the botnet and to get command and control away from those who had it.



What's happened is since that time the Ukrainian IP space has been actively scanned.  And a number of security firms, JASK and GreyNoise Intelligence, have both been watching the 'Net in the wake of this takedown and have seen, let's see, one, two, three, four, five, six, seven, eight IPs scanning port 2000 of the Ukrainian IP space.  Port 2000 is the one that the MikroTik routers unfortunately have opened that allows them to be taken over.  And so this strengthens the sense that existed before that this was from Russia and aimed at Ukraine networking devices, and that this is still an active campaign, even post commandeering of the command-and-control server.



And what's odd is that one of the IPs, 9.110.0.5, well, nine-dot anything, the whole nine-dot network is owned by IBM and always has been.  So I presume some obscure machine somewhere, I mean, it's not at IBM central headquarters, it's just some random machine somewhere on the IBM network at that IP, probably, I mean, unless it's a security scan, which is certainly possible, has probably got some malware in it and is scanning for Ukraine-based routers.  Also there's two IPs closely related to each other in Russia, two in Brazil, one somewhere in the U.S.  Oh, no, actually three in Russia, one in the U.S., and one in Switzerland.



So it looks like there are a few systems, without looking to see what they're all about, it's hard to know if they're malicious or not.  But I think what we're going to see, and we'll be talking about another class of attacks here in half an hour or so, these Internet-exposed devices are going to be a constant source of trouble.  And I don't know how they get fixed over time.



Okay.  So Leo, this incredibly clever hack.  Obscure as this is, it was independently discovered by security researchers who didn't know about each other - and the word I've used in the past for something like this is "sublime" - in CSS3, the latest standard of Cascading Style Sheets for our browsers.  As we know, HTML5 and CSS3 have just piled features onto browsers.  I mean, they are rapidly becoming full capable citizens on the desktop and in fact are beginning to host full-blown applications as they become faster and the script that they're able to run is having every feature that you would want from a desktop app.



Now, unfortunately, some of the press didn't get this right.  Ars Technica ran with the headline "EXPOSED [in all caps] -  Chrome and Firefox leaks let sites steal visitors' Facebook names, profile pics," et cetera.  Okay, well, yes.  But it's really not the fault of Chrome and Firefox.  As we've talked about, one of the core security requirements for browsers is the same-origin policy, the enforcement of the same-origin policy, the idea that if we're going to have web pages with content from multiple sources, as I was mentioning just a minute ago, virtually all - except GRC's web pages - virtually all web pages have stuff coming from everywhere else, all kinds of other places.



So sandboxing those is crucial because, for example, any web page that has ads is now using an ad network to fill in the rectangular space on the web page where the ad network will put up an ad.  So that is inherently a third-party source for security since the server hosting the web page is giving the user's browser an ad network URL saying fill this frame in with whatever you want.  It is crucial that that be contained, that that frame be sandboxed and has no access to other frames on the page or the page itself.  Otherwise, we'd have no security.



Well, for example, that page has a persistent cookie which is how the user is logged in.  So you absolutely don't want third-party content hosted on that page to be able to access, for example, the cookie of the hosting page or you'd have a problem like we did once upon a time with Firesheep, before HTTPS was in place virtually everywhere that it mattered, where session cookies were not being secured.  So containment is crucial.  Therefore, the idea that some researchers broke containment is significant, and that's what happened.  And they did so in just the coolest way.



What they realized was that the CSS3 feature called "mixed-blend mode" is not constant time.  So we've talked, for example, about mistakes in ciphers in the past.  For example, AES did not make the mistake because we knew about the mistakes by the time AES was being designed.  But in explaining this in the past I've explained that you don't want secrets to cause, for example, the bits of a key of a cipher to cause the decryption to take different amounts of time, depending upon those bits.



So what you want is you want a cipher that is constant time relative to the bits of its key because, if you don't have that, and you're able to time how long decryption takes, you can start inferring what the key's bits are from that.  And so that's a classic example of side-channel leakage.  Another instance is constant power.  You don't want the power consumed to be a function of any secrets.



So what several independent researchers simultaneously realized - and there had been some related work involving SVG, the Scalable Vector Graphics system.  So there had been sniffing around this before, and some other work in this area.  They realized that this mixed-blend mode might not be constant time.  And in fact it had not been.  So the idea with mixed-blend mode is it's very much like applying filters to graphics.  And in fact, Leo, there's a CSS-tricks.com site - I have a link here in the show notes - on the basic CSS blend modes that shows you examples, where for example you have a black-and-white photo of a building, and then a red rectangle, and you merge those two together, and it just sort of gives you a red tint on the black-and-white photo.



So the idea is that in our browser we are now doing compositing, image compositing of addition, subtraction, multiplication, saturation, whitening, lightening, darkening, and so forth, all kinds of different features.  And that requires math on the pixels.  Individual pixels that line up through these layers have math operations performed on them.  And if it's just addition, subtraction, and multiplication, then those are probably constant time.  But if you have something like saturation, where you might add two pixels, and you need to check to see if the sum overflows, then you need to clamp the pixel at maximum luminosity rather than allowing it to overflow.  So suddenly you have a code path that depends upon the brightness of the pixels.



And so what these guys did, and they've done a proof of concept, they arranged to create a code where an unwitting web browsing person would go to a website and in 20 seconds could have their Facebook name obtained by that site that they're visiting.  And that's done by the site having an iframe to Facebook.com.  That will, since the user has a presence at Facebook, if you go to Facebook.com it brings up the user's page in the iframe.  And they hide this under another image, and then they're able to scan the iframe graphics with their own scanning pixel, measuring the length of time required to perform this compositing math as this pixel scans, and thus infer from the length of time it takes the color of the pixel in the iframe of a foreign site.  And so it's just an amazing hack.



It turns out that this was disclosed responsibly some time ago, and that there is a graphics library that Facebook and Google share.  It's called S-K-I-A.  And it's an open source graphics library.  Google purchased the company, Skia, Inc., some years ago and so brought them into the Google fold and then made the whole thing open, open sourced it under the new OpenBSD license, I think, yeah, the BSD free software license.  When they fixed the problem, in order to basically focus on this layer compositing problem, they also in the process not only made it constant in time, thereby eliminating the ability to acquire any information based on how long things take, which you have to do if you're going to have security, but across the board it is way faster than it was before.



So basically they focused on it, made a constant time and constant power graphics processing pipeline, and at the same time, I've got the numbers in the show notes, it looks like in some cases better than a factor of three faster.  So like in less than a third the time it took before they're able to do that.  But anyway, just another example of how, despite our best efforts to keep things secure, when we add all kinds of new features to existing technology, because there's an exponential effect of every feature interacting with every other feature, when you add a couple new features, suddenly the possible number of interactions among them just explodes.



And so we have a whole bunch of new HTML5 features, a whole bunch of new CSS3 features, and here's an example of where you would never have thought that what you're seeing in an iframe could somehow be leaked into code running on the page that hosts the iframe.  Yet that has just been pulled off.  So I salute the cleverness of these attackers, or, well, these security researchers who thwarted what, I mean, it's not like it's a huge problem.  But you can imagine some situations where you're logged onto a site and there is some sensitive information that the site displays when that's the condition you're in, and this would have allowed that displayed information to leak onto a page that you're visiting.



So, I mean, it does break the security guarantee which is so important for web browsing as we go forward.  It's not a fast attack.  It takes time to essentially measure the rendering time of individual pixels.  But they demonstrated the recovery of the person's name in Facebook given about 20 seconds of just sitting there, staring at a page, doing other things, and this thing's happening in the background.



LEO:  Would you have to be able to watch their browser?  I mean, how would you get the information?



STEVE:  So you as a user, an innocent user, would just go to some site.  And you would have no idea that this was going on in the background.



LEO:  The site would have to be malicious.



STEVE:  Correct.



LEO:  And it could measure it because of course it's loading it.  Okay.



STEVE:  Correct, yup.  Amazing.  So OpenVPN is to this day my favorite virtual private network platform.  It's what I use for connecting to GRC.  It's what I use between sites.  I mean, the OpenVPN guys just nailed this problem.  It supports UDP and TCP protocols.  It's always hosting the latest OpenSSL library, so it's up to date on all of its crypto.  It's the one.  And so if you were a user wanting a VPN, what you would look for would be an OpenVPN host, but OpenVPN is what you want to be using.  You don't want Tom and Jerry's VPN.  This problem has been solved with OpenVPN.



Except what about quantum crypto?  As we know, cryptography is potentially endangered by the specter of quantum computing because it has been known now for some time that the hard problems upon which some classes of our cryptography depend may fall apart.  They may no longer be hard problems, if we finally have quantum computers that are more than just at the toy scale.  Right now quantum computing is still a toy, but that's the way all this stuff starts.  Computers were once toys.  Now they're certainly not.



So way back in 1994, a mathematician named Peter Shor, whose algorithm bears his name, Shor's algorithm, showed that, if a theoretical quantum computer exists with the set of properties we expect them to have, that integer factorization was no longer hard.  So as we know, for example, classic RSA public key crypto, where we take two very large primes, and we multiply them, and that creates the public key, and that the reason we're able to, like, give it away and make it public is that, even though it's there, out for everyone to see, it is composed of the multiplication of two primes, and nobody has figured out how to unmultiply them, how to factor their product back into the individual.



So one of those primes is the private key, and it's also sitting there out in public view, except since it's been multiplied by another prime, you don't know where it is.  You can't find it in the product of the two.  Yet the cleverness of crypto allows you to use the properties of the private key without knowing what it is.  So that's RSA-style public key crypto.  It utterly, all the public keys floating around out there contain the private key.  But it's only the fact that we don't know how to crack them back apart that makes it safe.



So Peter Shor showed us that, oh, give me a quantum computer, and I'll crack all those public keys back into their two components, and it's game over for RSA-style public key crypto.  So that's a little worrisome for cryptographers.  Elliptic curve crypto in its current form is similarly vulnerable.  So it's not like ECC crypto isn't in similar trouble.  Shor's algorithm can also slice right through the so-called "discrete log" problem, which is what protects elliptic curve's point multiplication, which is the hard problem that elliptic curve crypto offers.



Now, what's interesting is that it's that public key crypto, whether it's RSA or elliptic curve, where the vulnerability exists.  Our symmetric cipher, like AES, it is believed to be fine.  Everyone wants to double the key from like 128 bits to 256.  But AES with a 256-bit key is currently believed to be plenty strong, even in the face of quantum computing.  And the same is true for our hash functions.  SHA-2 with a 256, so SHA-256, or the SHA-3 functions, which have now been settled on, and they're just sort of like on the back burner, waiting for us to need them, they don't have to change post-quantum.  Yet the public key components are based on difficulty.  So that's why we talk about an RSA key length of 1024 bits, or 2048, or 4096.



The problem is that's a lot of bits because the thing that they're trying to make hard, which is in the case of RSA prime factorization, is not that difficult.  So the Microsoft Research Group have forked OpenVPN and have a running, operating, you can download it and install it today if you want, post-quantum OpenVPN implementation.  They have three different algorithms, one called "Frodo," which is a key exchange protocol based on what they call the "learning with errors" problem.  Then there's one called "SIKE," S-I-K-E, which is a key exchange protocol based on what's known as Supersingular Isogeny Diffie-Hellman.  And I imagine, probably before we get to Episode 999, I will be talking about what that is.



LEO:  Ooh, that'd be interesting.



STEVE:  Yes.  And then "Picnic," which is a signature algorithm using symmetric-key primitives and non-interactive zero-knowledge proofs.  And again, I imagine we will be talking about that in the future.  I won't get into it in any more detail except to note that those things exist, and that we don't need them yet.  The reason cryptographers are thinking about post-quantum crypto is not that we're in any obvious danger, but that we now understand these things take time to develop and prove, and we would like to have them ready, much like we have SHA-3.  That's a whole 'nother next generation of secure hashes, just sort of sitting there on the backburner.  We'd like to have well-researched and tested algorithms which are resistant to the things that we think quantum computers will get good at doing when quantum computers get good at doing anything.  And right now they're just toys.  But everybody's excited about them, and so we know that, famously, these things are always going to be getting better.  Quantum computing is going to be no exception to that, I think.



So anyway, it's called PQ Crypto, Post-Quantum Crypto.  I have a link in the show notes for anyone who's interested.  It's up on GitHub.  And, oh, by the way, I'm sure, I don't know if you've talked about it yet, Leo, but Microsoft bought GitHub for $7.5 billion.



LEO:  Yes, they did.



STEVE:  So they are GitHub's new parent.



LEO:  This is upsetting to a lot of open source folks.



STEVE:  And GitLab has seen a huge spike of incoming projects, moving from GitHub to GitLab.  There are other people who have no problem with Microsoft being a benefactor for GitHub.  And in fact the new, or I guess the incoming Microsoft CEO of GitHub has said - well, of course he's Microsoft, so he would - that Microsoft will not be changing anything.



LEO:  Oh, yeah, yeah, yeah.



STEVE:  Nothing happens.



LEO:  I've heard that before.



STEVE:  I know, I know, yeah.



LEO:  GitLab is not much different, frankly, than GitHub.  It's going to have the same issues and could easily be acquired by somebody.  The problem is, I mean, I use a private Git.  But if you want to have, like this project you were mentioning, if you want to have your code be public for people to download, look at, and contribute to, you need a public-facing server to do that.  And I think GitLab might in fact just - SourceForge has also said, hey, we're not as bad as we used to be.  Come on over to SourceForge.



STEVE:  Oh, god, it's a sewer, Leo.  Have you seen, I mean, like, it's...



LEO:  Well, I think they're maybe not as bad.  I don't know.  It was sewer, you're right.



STEVE:  Whenever I'm looking for something, and I have to go get it from SourceForge, it's like, oh.  Really?



LEO:  SourceForge used to be the place, and actually GitHub kind of replaced SourceForge.  And I'll be curious.  I mean, Microsoft loves open source.



STEVE:  They are loving it more every day.



LEO:  More than they did.



STEVE:  Yes, yeah.



LEO:  I don't know.  You can run your own GitLab instance.  So that may be what people what people end up doing is just hosting it themselves.



STEVE:  Yeah.  And just say...



LEO:  And GitLab's looking at federating, which would really be cool.  Then you could have multiple GitLab sources - including I could run my own GitLab server, which I do - and then federate it so that you could go to GitLab.com and see all of them.  That would...



STEVE:  And that solves the problem of fragmentation, where...



LEO:  Exactly.



STEVE:  Yes.  And that's what's been so nice about GitHub is it was one-stop shopping for all these different projects.



LEO:  Right.  There were issues with GitHub, and frankly they were losing money, and that was what really happened.  They were struggling.



STEVE:  So when four months is not enough time to fix a bad bug.  Back in, oh, a little after the middle of January, a researcher, Dmitri Kaslov of Telspace Systems, discovered a significant problem in Microsoft's Windows JavaScript engine.  He responsibly disclosed it to Trend Micro's Zero-Day Initiative to start the clock.  The ZDI group, the Zero-Day Initiative group responsibly notified Microsoft on January 23rd and received a same-day acknowledgement from Microsoft.



Then a week ago, last Tuesday, May 29th, more than four months later, the ZDI Group decided to hold Microsoft accountable for not patching this still-unpatched serious remote code execution bug which exists in their JavaScript engine.  They posted last Tuesday.  Their post was titled "(0Day) Microsoft Windows JScript Error Object Use-After-Free Remote Code Execution Vulnerability."  And we've talked about use-after-free problems, the idea being that in languages which incorporate automatic garbage collection, so that the programmer is not required to indicate when they're no longer using an object, the system releases it.



But in some cases, when mistakes are made, you can still have a pointer to the object whose memory has been released.  And using that pointer, there are various ways to exploit that in order to get your own code to run, which is what happened here.  Their posting says:  "This vulnerability allows remote attackers to execute arbitrary code on vulnerable installations of Microsoft Windows."  And at the moment, that's all of us. "User interaction is required," they write, "to exploit this vulnerability."



And before you go "Whew," it turns out that the target must visit a malicious page or open a malicious file.  I mean, so it's not like somebody can reach out from across the world into any Windows system.  But all it takes is going to a website which has leveraged this.  Which means, as soon as the information about this gets loose, it will be actively exploited because that's what happens with these things.  So in other words, just allowing Microsoft's JavaScript engine to interpret JavaScript code received from a visit to any website.



LEO:  So that's on Edge or Internet Explorer.  That's, like, Microsoft's browsers.  Wow.



STEVE:  Yes, yes.  "The flaw exists within the handling of error objects in JavaScript," they write.  "By performing actions in script, an attacker can cause a pointer to be reused after it has been freed.  An attacker can leverage this vulnerability to execute code under the context of the current process."



So the timeline of this is interesting.  They posted it on their page.  On January 23rd they say ZDI sent the vulnerability report to the vendor, in this case Microsoft.  On the same day,  01/23/2018, the vendor acknowledged and provided a case number.  04/23, so 90 days later, like to the day, exactly 90 days later Microsoft apparently suddenly woke up and said, so ZDI says here:  "The vendor replied that they were having difficulty reproducing the issue report without a proof of concept."



The next day, on April 24th, ZDI confirmed the proof of concept was sent with the original, and sent it again.  A week goes by.  Now we're at May 1st.  The vendor acknowledged receipt of the proof of concept.  Seven days later, a week later, on the 8th, Microsoft requested an extension.  On the 18th, ZDI replied:  "We have verified that we sent the proof of concept with the original.  The report will zero-day on May 29," which was last Tuesday.



The good news is there's no additional - there has not been a full disclosure.  They have not published anything.  So it's not like they're jumping up and down and saying, sorry, here's all the details.  This is a bad bug, apparently.  And so, I mean, nobody wants to be malicious about this.  Hopefully, since today, which is June 5th, is the first Tuesday of June, second Tuesday the 12th is next Tuesday, and let's hope that this thing gets fixed.



So for mitigation, because these guys do know exactly all the details, having had a proof of concept, basically demonstrating remote execution of code on a JavaScript recipient's computer, for mitigation they said:  "Given the nature of the vulnerability, the only salient mitigation strategy is to restrict interaction with the application to trusted files."  Essentially, and somewhere else I saw them saying, you know, don't use IE, Edge, or anything that could run JavaScript.  Unfortunately, that's even Microsoft Office.  So anyway, we will hopefully next week have this thing fixed.



The problem is that, in our modern ecosystem we know that patching or that the manufacturer patching a problem doesn't fix it universally.  There is another instance of a zero-day VBScript, so not JScript, which is the last one, but a Visual Basic, a VBScript problem in IE and Office that was fixed previously, and has now been added to the popular RIG exploit kit, which is exploiting what was a zero-day, even though it's been fixed.



So it's been added to this active exploit kit.  A target receives a Microsoft Word document, opens the document, which launches the second stage of the exploit, which is enabled by an HTML page in the document which has VBScript, which also has a use-after-free vulnerability and executes shell code of the attacker's choice.  So even though that has been patched, it was a couple months ago, they're still actively exploiting these.  And unfortunately I imagine that this JScript exploit, even if it gets patched, as we hope it will next week, it'll get added to the toolkit.  And people who aren't keeping themselves current risk being vulnerable.



So for us, listeners of the podcast, the takeaway is do not wait long to update your systems.  I know that, with systems taking in some cases a long time to update - as we know, Windows 10 just got better about that.  I heard you and Paul talking about this a couple weeks ago, Leo, the fact that - and we did on this podcast - the fact that Microsoft had moved a lot of the housekeeping needed for these major monthly rollups into the background so the system was much more ready to go when it finally said, okay, now you need to do a reboot so we can do the final work and bring the new version of the OS online, making it a lot easier.  And it is possible now, as we know, to schedule these updates for a time when the person's not using the machine.



LEO:  Sort of.



STEVE:  Yes.  Ah, yes.  I was going to say many people unfortunately turn their machines off when they're not using them.



LEO:  Well, and also there's a problem of detecting when you're using the machine.  Some people just sit there and look at it for a while, and I've seen machines restart just on their own while you're sitting there.  It's very frustrating. 



STEVE:  Yeah.  In fact, I think I told a story when I was attending that DigiCert conference in Utah, like in November of last year, one of the people there opened their laptop, and it just sat there for like an hour.  It was unbelievable how long it took.  And he was completely offline, couldn't do anything.



LEO:  It's stuck.  I'm updating my Mac right now.  I'm stuck.  I can't do anything.



STEVE:  Yup, the little spinning rollercoaster dots.



LEO:  Yup, no fun.



STEVE:  Okay.  So the good news is Valve immediately patched a worrisome security bug in their Steam client, which had apparently been present for the last decade, always there.



LEO:  Wow.



STEVE:  Yeah.  And bad.  A single packet remote code execution vulnerability.  Valve designed their own UDP packet-based, connection-oriented protocol, which is fine.  But it meant that their own code was receiving UDP packets.  And that's also fine.



One of the things that can happen to packets on the Internet is they can be chopped up into smaller pieces.  Years ago on this podcast, when we were talking about how the Internet works, I extensively covered this incredible, at the time, conceptual breakthrough with packets and routing where you would have this loose federation of routers that sort of know what direction packets should be sent to, and that's kind of it.  So that packets come in, and they say, oh.  They look at the destination IP, and they say, oh, that came in on interface four, and interface two is the direction that packet wants to go in.



And of course we were talking just last week, that BGP failure is an example of what happens when this really lovely system fails us.  When routers advertise that they have shorter routes than they really do, the routers that they connect to go, oh, you're a better place to send this packet, even if it's not true.  So anyway, the point is that not all router-to-router links are identical.  And they're more identical now than they used to be.



But, for example, there are some links where you can use larger packets.  And a large packet could go a few hops and then hit a link where the recipient doesn't support large packets.  That forces the sender to fragment the packet into multiple smaller ones in order for them to cross this inter-router link that can't do big packets.  And there are things called "jumbo packets."  So at the receiving end, once a packet has been fragmented, it is not reassembled.  The designers thought, well, if we reassemble it, and then it goes another couple hops, then it has to be fragmented again, then routers are just doing a lot of extra work.  So once a packet's broken up into smaller pieces, they stay small until they get to their destination.



The mistake that the authors of the Steam UDP protocol made was surprising, but maybe understandable.  The first packet, the first UDP packet, because fragmentation is possible, it contains both a fragment size and a total datagram length.  Normally they're the same.  But if the packet has been fragmented, its own size will be smaller than the datagram length.  Well, what's missing from the code to check the first packet in a set of fragments is whether that's true.  And that missing check to make sure that the packet's length is no greater than the datagram's length was missing from the code.  And that allowed a large packet to overflow a buffer in a classic remotely supplied buffer overrun attack which could be leveraged into a remote code execution, basically very trivially.



Now, the good news is last summer Steam independently, or Valve rather, independently, separate from this, added to the Steam system ASLR, Address Space Layout Randomization, which from that point on would have made exploiting this much more difficult.  Not impossible, because we've seen various ASLR bypasses which can be done, but it's much more difficult.  Anyway, just another example of how longstanding protocols can have errors.  They're kind of a little suspicious sometimes.  I mean, like the test for successive UDP fragments is there, but not the test for the first one.  It's like, okay.



Well, there is a different code path for the first fragment in a series of them, so you could explain it that way.  But of course what you always wonder for a problem this simple to implement, I mean, like this is way easier than Heartbleed, that you didn't know maybe you were going to get data from a server or not.  I mean, that was a very low probability key extraction, and the world had a meltdown over it.  Here you just send a malformed UDP packet to any of 15 million Steam clients, and you're running code under the rights of the Steam client, which is, you know...



LEO:  That's terrible.  That's terrible.



STEVE:  It is.  And so it's been there for a decade.  And so you have to wonder, did anybody else ever know about this?  And was it used?  The good news is it won't be any longer.  And to Valve's credit, they fixed it immediately, like within hours of being informed.  So credit to them.  And, yes, everybody can make a mistake.



LEO:  The scary thing about this is that Steam typically on a Windows PC is running all the time.  Once you install it, it's just sitting there in the system tray.



STEVE:  Yup.  Woohoo.



LEO:  It's not one of those things you run it and then close it, yeah.



STEVE:  Yeah.  So it's like having your own little open vulnerability on the Internet, unfortunately.



LEO:  Yeah.



STEVE:  Speaking of open vulnerabilities on the Internet, it turns out we talked a couple weeks ago about these Redis servers, R-E-D-I-S.



LEO:  Redis.  We use Redis, actually.



STEVE:  Redis, okay, yeah.  They are RAM-based indexed data stores which are, you know, they're very cool.  They are used normally...



LEO:  Caching.



STEVE:  Yes, exactly.  It is sort of a big index meant to be used on an internal Intranet, never meant to be exposed to the Internet.  That is, it's not a web server where it makes any sense at all for it to be publicly exposed.  It's meant as an internal cache.  As a consequence there is, believe it or not, the default installation.  Well, I mean, yeah, it is believable. The default installation assumes that position, that is, a non-publicly exposed position, so there is no network or authentication security enabled by default.  They are wide open.  So naturally, in the world that we have, rather than the world that we want, although it was never their designer's intent, many thousands of these servers, and actually somewhere around 72,000 of them, to give it a number, are sitting happily out on the public Internet for anyone to horse around with, even though not one of them should be.



Okay.  So Imperva Security, whom we've spoken of often, they're an active Internet security, network-based security firm, they've been running a network of Redis honeypot servers in order to observe and characterize the behavior of the attacks which are coming against them.  As a consequence of their honeypots, they discovered that these servers have the SSH, the Secure Shell authentication keys installed so that persistent future access could be obtained if the servers were ever to be secured in the future.  That is, if they're not taken off the public Internet, but if someone goes oh, my goodness, we've got malware in our Redis server, let's add authentication so that no one can make an unauthenticated access.



Well, so what's happened is the bad guys have already taken advantage of their access to install persistent SSH keys so that presumably they'll be able to telnet in with authentication that they have provided for themselves.  So hopefully anybody, if the security of these servers ever comes on the radar of people for whom it already should be, they'll just wipe and reinstall.  They're RAM-based servers, so there's really no reason not to just clean them up and bring them back up on the Intranet without any public exposure.  They should not be exposed publicly.



Anyway, so it's funny, as I was thinking about this, I'm just struck by how bizarre today's Internet, I mean, how bizarre today's climate feels.  Leo, like just 10 years ago during this podcast, when the podcast was almost three years old, because we're coming up now on the end of our 12th year, this was the stuff of science fiction.  Like this kind of problem didn't exist.  But here we are now, Episode 666 of Security Now!, and it's just like, oh, well, just another episode.  Unbelievable.



LEO:  Oh, I can't imagine what it's going to be like in 999.



STEVE:  Oh, boy.  Okay.  So Chrome 67 they did, as Google always does, moving things forward.  One of the things that they moved backward, just like a day or two ago, I think it was late last week, was the removal of one of the experiments - because not all of their experiments fly, as we know.  They do stuff.  They see how they go.  Sometimes they stick, sometimes not.  This one did not stick:  HPKP, HTTP-based Public Key Pinning.  So the idea was kind of cool, but it never happened.  In fact, the number I saw somewhere was 0.04% of websites ever tried this.



So the idea was that a web server with HTTPS security, so a standard SSL, now TLS, security certificate, having a public key, could publish the public key in its response headers to browsers.  And it would be saying, this is my public key, sort of formally.  The browser would store it, and thus any future visits where a connection was established with a server, if the browser had previously received a declaration of the server's public key, it would reverify the public key declaration with the public key now just received from the server.  And we call that "key pinning."  The key would be pinned in the browser or by the browser so that, if the public key did not match, the connection would be denied.



So the point is, this only makes sense if you rigorously enforce it, that is, if a site has said proudly, here is my public key.  And the only reason it's saying that is it wants the security of HPKP, HTTP-based Public Key Pinning, of anyone who might spoof it to be denied because there's no way - as we know, if certificates are misissued, like WoSign or Comodo or one of these sort of sketchy CAs were to issue a certificate for the domain that they should not issue it for, the certificate would be valid, but the public key would be different.  And there's no way for a third party to copy the public key because that's based on the private key that hopefully never does leave the server.  So you can create a duplicate certificate that is valid for the same domain.  You can never duplicate the public key.



Now, of course, the problem is there are many ways this could go wrong.  If a bad guy somehow intercepted your connections, like with a DNS spoofing attack, or somehow arranged to get an invalid public key pinned in browsers, then that browser would never accept the valid connection from the website.  I mean, the idea is this public key pinning only works if the valid public key is what gets pinned.  If there's any way for a malicious one to get pinned, you're in trouble.



And we also know that sometimes websites are surprised by the expiration of their SSL certificates, or their TLS certificates.  From time to time people will say, I mean, you'll go to a website and get the notice that the certificate is expired.  Well, not only are the website's visitors surprised, but the website's administrators are scrambling around like crazy, trying to get a new certificate issued.  The problem is that new certificate will have a different public key because it's a new certificate.  And if they had previously pinned the expired certificate, now they're in trouble because they can't use the expired certificate's public key in order to update the PIN that the browsers have previously obtained.



So the point is the only way to use this safely is to use it very responsibly.  And people who understood the risk just weighed that versus the benefits and said, eh, no thanks.  So it didn't happen.  Consequently, no adoption rate, and it's been removed from v67 of Chrome.  And I don't even - I meant to look to see whether Firefox ever even supported it.  It may not have ever gotten off the ground beyond Chrome and Chromium-based browsers.  So anyway, now it never will.



All of this is my introduction to the topic that we will end the podcast with, which is Certificate Transparency.  In Google's own coverage of this, when they talked about this, they announced it in their developers.google.com.  They said:  "Deprecate HTTP-Based Public Key Pinning."  They said:  "HTTP-Based Public Key Pinning was intended to allow websites to send an HTTP header that pins one or more of the public keys present in the site's certificate chain."  And then they said:  "It has very low adoption; and, although it provides security against certificate mis-issuance, it also creates risks of denial of service and hostile pinning."



So anyway, then they said:  "To defend against certificate misissuance, web developers should use the Expect-CT header."  And we now know that CT stands for Certificate Transparency.  So there is a new header, Expect-CT; and, they said, "including its reporting function."  They wrote:  "Expect-CT is safer than HPKP due to the flexibility it gives site operators to recover from configuration errors, and due to the built-in support offered by a number of certificate authorities."  And we'll be talking about that in a moment.



I did want to just note the final availability, and I know you've been talking about this, Leo, of Intel's previously known as XPoint, but officially trademarked as "Optane," memory.



LEO:  We were all excited, remember, when we heard about XPoint?



STEVE:  Yeah, yeah, yeah.  It's exciting because, just to remind our listeners or to inform those who don't know, it's an incredibly dense, very high-speed memory, non-volatile, like flash.  But if you can imagine a grid of conductors on the bottom layer, and then another grid at a 90-degree angle to them so that you form a bunch of intersections between the grid points.  And there you put a little blob of goo, to be unscientific.  This blob of goo has memory.  So unlike flash memory, where it depends upon stranding some electrons to create an electrostatic charge that can influence a metal oxide semiconductor gate, this actually is a state change technology where a previous zap of current permanently alters the state so that the resistivity of that intersection changes.



The reason it's exciting is that it is nonvolatile and four times the density of DRAM,  Dynamic RAM, which is what we have now, is the most, has previously been the most dense storage technology we have.  But of course it is famously volatile, which is why we have hard drives, and we've got caches that are faster.  DRAM is just persistently slow.  So it's going to be interesting to see how this changes the architecture of computing over time.  I mean, it's a game-changing technology.  You can buy Optane SSDs on Amazon right now.  They're not that expensive.  And there are some things that look like DIMMs, which are like 512GB, so half a terabyte of Optane memory for particular applications.



I think what this is going to do is it's going to change the way computer systems are structured because it has such a different set of speed and performance characteristics, and our existing hard drive to DRAM to three layers of caching to processor core to registers in the processor, all this kind of changes.  So it's going to be interesting to see how it evolves over time.  It's not clear.



LEO:  Yeah, it's pretty exciting.  I mean, we'll see what happens.



STEVE:  Yeah.  We don't often see this kind of a, like, major  potential changing technology hit our industry.



LEO:  Yeah, I've been waiting for a laptop with Optane and an i9.  But they're coming.



STEVE:  Yeah, yeah.



LEO:  I was hoping for a Mac.



STEVE:  I got an interesting note from a Gary Foard, F-O-A-R-D, I guess that's how you pronounce his name, in England, on the first of June.  He said:  "Hi, Steve.  I'm a big fan of your show and happy SpinRite owner.  However, as your next episode is 666" - so he wrote this last week - "I think it is time to ask my awkward question."  He said:  "I recently stumbled upon an old screenshot of Norton's Disk Doctor, and I was surprised to see how similar one of the screen images look compared to the main graphic status display of SpinRite."  He says:  "Although I've followed your Security Now! show for many years now," he says, "I don't really know what your personal background, timeline, progression, education is.  All I know is I see a sincere, experienced IT chap who does [and he has an acronym here] GAS."  And he says, parens, "(Give A Sh*t, as you once said)."  And he says:  "I guess that is why we all keep listening."



So to his question, he says:  "Did you work for or with Norton at some point?  Did DOS-based graphics dictate the identical look?  Or something else?  Regards, Gary Foard."



And I don't know if I've ever talked about this.  But unfortunately, I refused to sell SpinRite to Norton after a lunch that Peter and I had where he wanted to buy it.  SpinRite was only about two years old at that point, and he said over lunch that it was the number one thing that the purchasers of the Norton Utilities wanted was SpinRite.  And he said, "So we're going to buy it."



And I said, "Gee, Peter, I'm very flattered.  But maybe when SpinRite is in its waning years that would make sense.  But it's brand new, and it hasn't even - I'm not finished with it yet.  And it's my bread and butter, and I don't want to sell it to you."  And so he said, "No, really, name your price."  And I said, "There's no price, really.  I mean, except an insane price that you would never consider.  So thanks for lunch, but no."



And so we went back up to their offices in Santa Monica and met someone who was the CEO of Norton at that time.  Peter didn't want to manage the company.  He'd sort of stepped aside from management.  But there was someone named Ron Posner who...



LEO:  Oh, I remember him.



STEVE:  Yeah, yeah.  Not a good person.



LEO:  No.



STEVE:  Unh-unh.  And so Peter and I walk into evil Ron Posner's office.  And he says:  "So, we have a deal?"  And really, I will never forget the look on Peter Norton's face because it was like - it looked sort of like the dog that had been caught doing something bad, like digging up the flowerbed or something.  I mean, he really looked kind of sheepish.  And he said "Um, no."  And Posner said:  "What do you mean, no?"  And Peter said, well, you know, "Steve doesn't want to sell."  And Posner, like, I don't remember the details now, I mean, I wasn't happy either.  It was all kind of awkward.  But I said, "I'm sorry, but no."  So maybe, I'm not sure now exactly what the timing was.  But it was around the era of SpinRite 3, or I was working on it or something.  And there was another guy, John Goodman, who actually wrote a book about SpinRite.  There's a SpinRite book.



LEO:  Really.  I didn't know that.



STEVE:  Yeah, it got written.  And in the process he was covering other hard disk utilities.  And he interviewed someone who explained why it was that what they created, which was called the Norton Disk Doctor, looked identical.  What Ron Posner reportedly did was hand a copy of SpinRite to one of their developers and say, "Go home.  Stay home until we have our own version of this."  And what really upset some of my developers, some of my people, was that I'll never, I mean, it annoyed me, too, was there were some functions in the BIOS where support was not guaranteed.  And in my code I was testing for the presence of the functions by putting some arbitrary data into the registers and calling the function to see if the function modified the data.  We looked inside Norton's Disk Doctor, and they had copied the code verbatim. 



LEO:  Wow.



STEVE:  Yeah.  So...



LEO:  Did you go after them?



STEVE:  No, no, because it's just not my nature.



LEO:  Where did they get the code?  Did they reverse-engineer it?



STEVE:  Yeah.  They disassembled SpinRite.  And what was sad was that they didn't even, like, innovate the screens.  I mean, it looked like SpinRite.  I mean, I know exactly what Gary, who wrote this email, said because everyone in the industry at the time assumed I had sold SpinRite to Norton.  And the good news was they ended up abandoning it because, since they hadn't actually written it, they couldn't support it.  And we ended up, their own tech support ended up sending their Norton Utilities customers to us because they...



LEO:  That's nervy.



STEVE:  They needed help.  And so we would say, oh, well, you know, we understand you have a problem.  Buy a copy of SpinRite because we're actually the people who wrote apparently what you've got from Norton.  And then we'll be happy to support you, fix your drive, and answer your problems.  So, yeah, that's the story of Norton Disk Doctor, as far as I know, from third-hand reports.  But, I mean, from first-hand knowledge, there was absolutely my code in their product because they didn't know why I had put those random numbers, those particular numbers in the registers to call the BIOS.  And so they didn't dare change them.  So it was like, wow.



LEO:  Wow.



STEVE:  Okay, yeah.  And I had some employees who absolutely wanted me to sue them.  But I was a little guy, and they were Norton, and you end up in court forever.  And it's like, no, thanks.  I'll just - well, and it had no future.  SpinRite's still going strong.  So it's like, it was the best decision I ever made not to sell it to Peter.  I mean, he would have paid me a couple, what, hundred thousand dollars back then or something?  It's like, eh, no, thank you.  So the right decision made.



LEO:  Fascinating story.  Wow.  All right, Steve.  It's time to learn about our subject of the day.



STEVE:  Okay.  So we have talked throughout the history of this podcast about the way the certificate authority system functions.  We have a, well, once upon a time a few, a handful of certificate authorities who were responsible and trustworthy.  And their public key is contained in our so-called "root store" in our computer.  And a website asks them, proves the ownership over the domain, TWiT.tv, GRC.com, whatever.  And so the trusted certificate authority uses their private key to sign the certificate, which it is like a certificate.  It says, "We certify that the holder of this certificate is the owner of this domain."



So they use their private key to make that assertion, and then that certificate is what the website sends when you connect to it, saying you're really connecting to us.  And so it has two functions.  It authenticates the other endpoint; and, as part of the negotiation with TLS, it provides both authentication and privacy thanks to encryption.  So keys are negotiated and so forth.  But the key part of what the certificate's doing is it's a self-expiring assertion of ownership of a domain.  And if everything worked like that, then we'd have no problems.  But there are problems with it.



First of all, there are no longer a handful of certificate authorities.  There's a gazillion of them.  That's the technical term.  And among them are many that have unfortunately misbehaved, in some cases deliberately.  Was it Trustwave, I think it was Trustwave that deliberately issued an intermediate certificate so that a customer of its could mint certificates on the fly for a middleware box.  That was the story.  I'm still suspicious that that was the case.  But they got caught.  And as a consequence that certificate was revoked.



Normally a middlebox like that, like a corporation has on their network, requires that all of the clients receive its own certificate so that they're trusting things signed by that box because they're inside the network, not like all browsers in the world trusting all certificates signed by everyone.  This is the problem that we have is that, as the number of certificates has exploded, we have to contain - our root stores, which used to have a few public keys, have to have the public keys of all certificates that we might encounter.



So the system didn't really scale that well.  So there's the misissuance problem.  There's the fraudulent issuance problem.  And then of course there's also the revocation problem that we've talked about, where no one could have made a malicious mistake.  But due to a mistake made by the holder of the certificate or the private key that goes with that certificate, that could escape, and they would then need to revoke the credentials of the certificate that had been signed.



So through the years there have been different attempts to solve this problem.  We were talking earlier about HPKP and how that was an attempt to do Public Key Pinning, and that never really worked.  And Chrome has had that CRLSet where it's kind of revocation, but it prioritizes only the certificates that they feel are important enough.  Maybe those are the EA certificates, but not the EV certificates; and not OV or DV, not the organization or the domain validation certificates, and so forth.  And then there's the OCSP, the Online Certificate Status Protocol, and so forth.  So this has been an ongoing problem.



About four years ago, Google came up with another solution.  And it's gaining traction, or I should say it's gained traction, rather quietly.  And it's a nifty idea.  It is going to still take some time to go from sort of advisory status to enforcement status because it does require all certificate authorities to participate for it to work, in the same way that all certificate authorities have to have their root certificate in the store, although in this case participation is probably a little more involved than that.



Okay.  So what is it?  Quite simply, it's a public log.  It is a special, cryptographically secured, append-only log of certificates which are issued.  And it's called Certificate Transparency because this public log intends to make the issuing of certificates be transparent.  Right now, if random certificate authority XYZ issues a cert, nobody knows.  I mean, it's a transaction between the domain holder and their CA, who they have chosen to pay for the signing of a certificate.  And that signature is trusted by everybody's browser in the world. So it's a quiet transaction.



Notice that one of the things we've talked about is how the Chrome browser's ability to know if the Google certificates were valid gave it a lot of leverage.  That is, over the years there have been a number of instances where somebody somehow got a non-authentic Google property, like Google.com, certificate.  And a user of Chrome went to a site that was spoofing Google, and instantaneously the jig was up.  Chrome does have pinning for the Google property certificates.  And so if anybody tries to pull a Google.com certificate that isn't valid on a Chrome user, the Chrome browser instantly phones home to Google and says, whoa, we just received a certificate that should not exist in the wild.



And so as a consequence of Chrome's dominance and Google's special position in the world, there has been this particular closed loop.  So certificate transparency is sort of an effort to extend that model globally.  There are three components to the CT system.  There's a certificate log.  Then there are certificate monitors for the logs.  And there are certificate auditors.  And I should mention that there's a site where all this is explained.  It's www.certificate-transparency.org is where this stuff lives.  Everything is open and open source.  And this is another significantly useful effort which Google has created and is helping to manage and run.  And it seems to me like it's the future.



Google writes, as they're explaining it, about the logs, the monitors, and the auditors.  They say:  "These functional components represent discrete software modules that provide supplemental monitoring and auditing services.  They are not a replacement for, or an alternative to, the current SSL certificate system."  In other words, all of that stays in place.  This is a means of watching the certificate authority system operate and catching mistakes.



Google continues, saying:  "Indeed, these components do not change the fundamental chain-of-trust model that lets clients validate a domain and establish a secure connection with a server.  Instead, these components augment the chain-of-trust model by providing support for public oversight and scrutiny of the entire SSL certificate system."  And that's why this is so cool.



So they say, under "Basic Log Features," they say:  "At the center of the Certificate Transparency system are certificate logs.  A certificate log is a network service that maintains a record of SSL certificates.  Certificate logs have three important qualities.  First, they are append-only.  Certificates can only be added to a log.  Certificates cannot be deleted, modified, or retroactively inserted into a log.  They are cryptographically assured.  Logs use a cryptographic mechanism known as Merkle Tree Hashes to prevent tampering and misbehavior.  And, finally, they're publicly auditable.  Anyone can query a log and verify that it's well behaved, or verify that an SSL certificate has been legitimately appended to the log."



They say:  "The number of logs does not need to be large.  There need to be enough logs so that log failures or temporary outages are not a problem, but not so many that they become difficult to monitor."  And Google says, "Say more than 10, but much less than a thousand.  Each log operates independently of the other logs," which is to say there's no automatic replication among the logs.



And they said:  "The append-only nature of a log allows it to use a special type of cryptographic hash to prove that it's not corrupt and that the log operator has not deleted or modified any certificates in the log.  This special hash," they write, "known as a Merkle Tree Hash, also makes it possible for auditors to detect whether someone has forked a log or inserted backdated certificates into a log."  And it's funny, too, because at some point as I was reading into this, I don't remember now where it was, it was some time ago, but the author of what I was reading said, "And, yes, it's inevitable that this is going to be compared to the blockchain, or a blockchain, or some blockchain, and that's not what it is."



So, however, of course the reason it's sort of subject to comparison is that it's got some of those sorts of features, a publicly auditable, non-modifiable thing, although it doesn't work the way a blockchain does.  And it is a standalone, append-only data construction that I imagine we will talk about at some point in the future.  And what I got a kick out of and what was significant to me also, as I mentioned at the top of the show, is my favorite certificate authority, DigiCert, was the first CA to get themselves going with a certificate transparency log.  Since then, and that was February 1st of 2015, so more than three years ago, other CAs have been establishing logs.  There is a Let's Encrypt log.  And the idea is that there is a JSON, a computer-readable list of logs, which allows CAs to publish the announcements of certificates that they're issuing in the log.



And so as I said at the top, once upon a time there was no knowledge of the issuance of a certificate until it appeared  being received in a browser by someone going to a site that was asserting that certificate.  This provides a repository of certificate issuance transactions over time which is publicly available; cannot be altered after the fact; and, once the system sort of has moved from the reporting stage to the enforcement stage, sort of solves this problem of the system not having sort of a closed-loop feedback into the issuing process.  This closes the loop, providing a declaration that it can be machine searched quickly in order to determine what the status of any domain certificate is.



So I don't want to go into any further detail at this point.  We're out of time, for one thing.  But I imagine, as this continues to gain traction, we'll be coming back and talking about some of the very cool underlying technologies of the certificate transparency system.  I've touched on the topic, just used the term a few times, but never really established what it is.  So I thought with the opportunity of Chrome 67 removing another attempt at solving this problem, talking about the right way, the way that is clearly gaining traction, and it looks like it's going to win, is worth discussing.  So now we have.



LEO:  You bet it is.  Thank you, Steve.  Always interesting.  I learn so much.  And I understand so little.  Steve does this show, and I'm here for the ride, every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC if you want to tune in live and watch us live.  You can be in the chatroom at irc.twit.tv.  Nice bunch of people in there.  And you can also be in the studio.  We had a lovely family here from St. Louis.  Alan, Whitney, and Bennett were visiting.  One of the three's eyes weren't glazed over.  One and a half.  But they made it.  They survived.  They got through it.  Actually, Alan does security at a bank, IT security, so he kind of, I think, understands what we're talking about.  If you want to be in-studio, you can.  Just email tickets@twit.tv.  You can also watch live on the stream at TWiT.tv/live.  On-demand versions of the show available from Steve at GRC.com.  That's his website.  That's where you find the software even Peter Norton couldn't buy.



STEVE:  The original, for real.



LEO:  But you can, for a lot less.  GRC.com is the home of SpinRite, world's finest hard drive maintenance and recovery utility, and proudly written in assembly since 1984.  When did you first write it?



STEVE:  I think that's right.  It was the late '80s, yeah.



LEO:  Wow.  And still going strong.



STEVE:  Because I bought my home in '84, because that's of course George Orwell's book.  And SpinRite was a couple years later, so like '86.



LEO:  Nice, nice.  GRC.com.  There's lots of other good stuff there, too - this podcast; handwritten, human-composed transcriptions which actually are great because it helps you, if you want to search for a bit of the show, you can search the text and jump to that part of the audio.  We have video as well as audio at TWiT.tv/sn.  All our shows are here on demand.  And you can always find any show we do on your favorite podcast application.  Just subscribe.  That way it'll be in your inbox the minute it's available, Tuesday evenings, thereabouts.  Thank you, Steve.  And we'll see you next week on Security Now!.



STEVE:  Thank you, my friend.  See you then.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#667

DATE:		June 12, 2018

TITLE:		Zippity Do or Don't

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-667.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we update again on VPNFilter, look at another new emerging threat, check in on Drupalgeddon2, examine a very troubling remote Android vulnerability under active wormable exploitation, and take stock of Cisco's multiple firmware backdoors.  We discuss a new crypto mining strategy, the evolution of Russian state-sponsored cybercrime, a genealogy service that lost its user database, ongoing Russian censorship, and another Adobe Flash mess.  We check in on how Marcus Hutchins is doing.  And, finally, we look at yet another huge mess resulting from insecure interpreters.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have a lot to talk about.  VPNFilter turning out to be worse than we thought and affecting more routers than we thought.  And then there's a flaw in a protocol used by dozens of programs.  You probably use it every day.  The problem with zip files, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 667, recorded Tuesday, June 12th, 2018:  Zippity Do or Don't.



It's time for Security Now!, the show where we get together with Mr. Steve Gibson, right here over my left shoulder, and talk about security, privacy, technology.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.  Great to be with you again as we plow into the final third of the podcast.  We're on 667.



LEO:  It's only the final third if you know that we're only going to 999.  Now, somebody did point out that you could always use hexadecimal and stay in the three-digit range.



STEVE:  Yes.  I've had many helpful suggestions.  Others have suggested, well, we never did zero, and we haven't gone negative yet.  And I try not to go negative.



LEO:  No, no.



STEVE:  I just think that that's really - that doesn't really serve our listeners.  And, after all, that's why we're here.  And with a title such as "Zippity Do or Don't"?



LEO:  What's that all about?



STEVE:  Yeah.  Well, it turns out that, as one of our recurring memes here, we note how difficult it is to get interpreters right.  And it turns out that unzipping, that is, decompressing, is a widespread, very, well, important and crucial process to get right.  And if you make a mistake in your decompressor, it can have quite widespread ramifications.  And in fact that has been found.  So we will wrap up this week's podcast with Zippity Do or Don't.  In the meantime we're going to get an update on, unfortunately, VPNFilter, whose - I hope everyone's sitting down for this - grasp has expanded from 16 identified make and models of routers to I think the number was 71.  So, yeah.



LEO:  Oh, wow.  I knew that was - I was not sanguine that the list we had was complete.



STEVE:  Yeah.  And in fact I think at this point we have to assume that not being on the list no longer confers any proper sense of safety.  So we've got an update on VPNFilter.  We've got another about 40,000-strong network threat that is emerging.  We need to check in on Drupalgeddon2, now that it's been several months and things have probably reached steady state, to see where it is.  We have a very troubling remote Android vulnerability under active wormable exploitation.  And in fact that is that chart.  Our Picture of the Week is...



LEO:  I don't know how you didn't make that the title of the show because "Active Wormable Exploitation" is an excellent title.  It's a really felicitous phrase.



STEVE:  Well, and the acronym is AWE, A-W-E, so it's like, ooh, we're in awe now, baby.  Also we need to take stock of Cisco's recent problems with multiple backdoors.  Not just one, not just two, not just three, but four that we know of.  And it's like, okay, what has been going on over there in Cisco land that somebody has been deliberately putting backdoors in Cisco products?



LEO:  Huh.



STEVE:  Yeah.  Also we're going to take a look at a new crypto mining strategy.  The evolution of Russian state-sponsored cybercrime.  Those who watch have been noticing that Fancy Bear has been dancing a little differently recently.  A genealogy service that lost its user database.  But why at the same time that sort of upsets people, it's like, oh, my god, my DNA, but doesn't have anything to do with DNA.  And this outfit, MyHeritage, did everything right.  Also a look at Russia's continuing march toward censorship, fighting the technology that the Internet was supposed to give us.  Another look, or a look at another Adobe Flash mess that caused them to release an emergency update last Thursday.



Also a check-in on how our buddy Marcus Hutchins is doing.  Things were looking better, and then the government slapped him again.  And I just wanted to sort of touch base since, as we know, he was nabbed at McCarran, the Las Vegas airport, as he was trying to go home after last summer's hacker conferences.  And, finally, another huge mess we will wrap up with, resulting from insecure interpretation of data structures and how that could be manipulated, thus Zippity Do or Don't.



LEO:  You know, ages ago I wrote, before there was Zip, before there was StuffIt, before there was...



STEVE:  Remember Arc.  Arc was the early one.



LEO:  Arc, okay.  So there was Arc for Windows, but there was no Arc for Mac.  So I wrote, and probably you'll be proud to know in assembly code, I'm pretty sure it was in assembly code, an Arc for Mac.  But it wasn't a compressor, it was just a decompressor.  And I'm trying to remember, but I think it was kind of a table lookup process that was involved.



STEVE:  Probably would have been Huffman code, and that would be table lookup.



LEO:  Yeah, yeah.  It was probably Huffman as opposed to Lempel-Ziv.  I don't know if Lempel-Ziv was out yet.  That's when everything changed, when Zip came along, is Lempel-Ziv.  But yeah, it was probably Huffman run-length encoding.  And so a table would be the fastest way to do it.  And I can't imagine any way that that could be risky, but I'm sure you'll - I don't have my source code to check.  But I'm sure you'll come up with some way.



STEVE:  Well, Leo, if you had your source code, we'd have to find something that it ran on.  That, of course...



LEO:  Yeah.  You got a 68000 lying around?



STEVE:  That's the problem.  Beautiful chip.



LEO:  It was a beautiful chip.  It had a flat memory model.  I didn't have to do any of that weird stuff you have to do on your x86 platform.



STEVE:  And an orthogonal instruction set.



LEO:  Orthogonal, yeah, whatever that means.



STEVE:  Everything applied to everything, whereas Intel was just an abomination.



LEO:  Oh, the 68000 was a beautiful chip to code assembly language for.



STEVE:  I really wish it had won the war.  There was a battle there for a while, and it gave out.



LEO:  Yeah.  It was my - I had done a little 8086, and I hated it, mostly because of memory segmentation.  And I just, when I got the 68000, it was like, oh, this is beautiful.  It was actually fun to write assembly code for that.  And Apple had a really good tool, the Macintosh Programmer's Workshop, that did a lot - it was like MASM.  It had a lot of macro capability.  So you really were working almost in a high-level language.  Once you wrote your macros, it was pretty easy to work with.  Otherwise I would never have been able to do it.



STEVE:  So not pleasing us is the march forward of the VPNFilter malware.



LEO:  Wow.



STEVE:  All evidence is that this is Russian state-sponsored cyberwar, cybercrime.  The fact that there's a strong bias toward routers and IoT devices in Ukraine is what leads a lot of people, and there's been other breadcrumbs that security researchers have also tracked back to known IP addresses and command-and-control servers and so forth that are known to be controlled and have been used in the past.  So Russians, the group that are doing this, that APT28, a.k.a. Fancy Bear, and it's known by a whole bunch of different names, they're not even really trying to keep themselves clean in terms of no accountability for this.  And of course the Russian government denies any knowledge.  Oh, this is fake news.  It's like, oh, wonderful.



So what we have is the Talos threat intelligence group of Cisco, who were the original prime discoverers and trackers, have continued watching this and have further reverse-assembled the code.  Remember that, while there are oftentimes hints left behind in code, it's very often just ones and zeroes that are running on a given device.  So you need to know what device it's running on.  You need to have the ability to disassemble the binary into the assembly language for the device.  Sometimes it's interpreted code, which can make that task easier.  But the point is just saying oh, look, there's malware in this router, it's like, okay, what does it do?  Well, it's mal.  But to know exactly what it does takes time.  And so it's not at all surprising that several months have gone by, during which I'm sure that hackers, white hat hackers inside of the Talos group have continued to study what it is that they've found.



So as they've been watching it, the first thing that they have found is that, to the list, the previous worrisome but kind of modest list of Linksys, MikroTik, Netgear, TP-Link and QNAP NAS devices, we must now add routers by ASUS, D-Link, Huawei, Ubiquiti, UPVEL, and ZTE.  So essentially we've gone from, and I have here in my notes, I did quote them earlier from memory correctly.  We've gone from 16 makes and model numbers to at least 71, 71 we know of.



And in fact it just didn't make sense for me to list them in the notes here.  Normally I do, but the list is too long.  So what I have in the notes is a link to the talosintelligence.com blog.  I imagine you could just go to blog.talosintelligence.com, and you would find vpnfilter-update is their URL there, if you were curious.  But as we were saying at the top of the show, Leo, not being on that list really should not confer any great sense of security any longer. 



LEO:  I was worried about that even from day one, that that list wasn't complete.



STEVE:  Yeah, I mean, this is a determined group.



LEO:  Do you think that it is that they are similar chipsets, similar firmware?  Or that they're extending its capabilities?  And furthermore, didn't the FBI discontinue, cut off the servers, the routers?



STEVE:  Yeah.  And you gave me a perfect segue because what I had here in my notes to remind myself, I wrote, "Also frightening, due to the R&D and development resources it implies, is the presence of many device-specific modules."



LEO:  Oh, boy.



STEVE:  This is not a one-size-fits-all opportunistic shotgun.  This is a, quote, "We want," literally, "We want to perform passive packet sniffing of all traffic passing through a TP-Link R600-VPN router."  And the Talos group found exactly such a module.



LEO:  Would that crack, if you're using it with a VPN, would that crack into your VPN?  I guess it would.



STEVE:  Yes, yes.



LEO:  And expose your traffic, wow.



STEVE:  Yes.  What they found was, in their additional watching of I'm sure their honeypots and tracking down the network, remember that this thing has three stages.  Stage one is the little sort of the hub, the little kernel, that plants itself in the firmware.  And they made it small to increase the probability of it being able to become persistent.  The larger it is, the less chance there would be a space in the file system for it to live.  So they kept it very lean and very small.  And then it reaches out to, it turns out, a much larger number of locations than was believed, which is one of the things that has allowed it to stay alive.



LEO:  So not ToKnowAll and Photobucket.



STEVE:  Yeah.  There were many more locations.



LEO:  I thought that was a little suspicious.  That was a little too easy.



STEVE:  Yes.  And it does also open a listener, which allows the bad guys to access it directly.



LEO:  You mentioned that, that that's why you have to rewrite the firmware.



STEVE:  Right.  So they had been monitoring their network, building up a list of IP addresses.  And when the FBI did its takedown, they said, oh, okay, and they just...



LEO:  Big deal.



STEVE:  ...reached out and poked an update directly.



LEO:  So do we know how big, I mean, the botnet FBI said was 500 million routers at that time, and they told everybody to reboot your router, which obviously was inadequate.  Do we know how big it is now?



STEVE:  I would imagine some probably fell off.  The number I'm still seeing is more than half a million.



LEO:  Half a billion.  Oh, half a million, that's right, 500,000, right.



STEVE:  Yeah.  It's still about 500,000.  What we now know is there are additional stage three plugins.  One in particular is...



LEO:  Oh, man.  This is a nation-state for you, right there, yeah.



STEVE:  Oh, yeah, yeah, yeah.  This is like, you know, so...



LEO:  Oh, man, it's like Stuxnet or something, yeah, wow.



STEVE:  So the stage three things are what live in RAM.  And, for example, they found an HTTP man-in-the-middle interceptor so that, when a page is coming in that it is able to intercept, it drops the "S" from the HTTPS, turning everything into HTTP.



LEO:  Oh, my god.



STEVE:  Yeah.  I mean, so it's basically doing an HTTP security downgrade attack on anybody who's inboard of one of these affected and infected routers.  So, I mean, it is clearly some serious intent when you find a module written for a particular make and model of VPN router which would not run on a different router.  It means that someone, somewhere, wanted to target routers of that make and model, which happens to be a VPN router, in order to intercept its traffic and have access to it.



And as I was thinking about just sort of overall this podcast today, I was thinking about the world we're in today, which I've increasingly referred to as, 10 years ago even, this would feel like science fiction.  And it was.  It was the work of fiction not that far back that this kind of network - it was Daniel Suarez that was basically painting this kind of picture for us.  And also Mark Russinovich has done the same thing, people who have extended what's possible.



And what I'm increasingly feeling, when we learn about things like little glitches in Zip, and these widespread problems in so many different makes and models of routers, one of the analogies that I've drawn on often is the notion of security being porous, that the barrier is not absolute.  It's porous.  And as you press on it more, it will spring leaks.  You'll push molecules through this semi-permeable barrier.



And so what's happened is we've sort of gone along happily for a few decades, oh, look at the Internet, isn't it wonderful, and we can talk to everybody, and everybody can talk to us.  And ooh, email, and ooh, messaging and web and wow, you know.  But none of this stuff that has been built was, I mean, it was built to work, not to be secure.  I mean, yeah, everyone said it was secure.  Steve Ballmer was prancing around the stage, talking about how Windows XP was going to be the most secure operating system ever.  I saw XP referred to online somewhere the other day as a "steaming stack of security vulnerabilities."  It's like, ugh.



LEO:  Although one could argue anything that's got 10 million lines of code is going to be a sieve; right?



STEVE:  Well, yes.  And this is - well, but our philosophy, that suggests then, is broken.



LEO:  Right.



STEVE:  That is, the development philosophy is, oh, just add more code to it.  It's like, no.  Don't add more code.  So I really...



LEO:  Also, I don't know if we anticipated these state-sponsored actors who were so adept and so well funded.



STEVE:  Yes.  Well, and who could have anticipated malicious cryptocurrency mining?



LEO:  Yeah, that's another one, right.



STEVE:  Or blackmailing people by encrypting their system's files.  I mean, we've had like these weird things that have like, arose - arisen.



LEO:  Arisen.  Arise.  Arose.



STEVE:  And, like, out of nowhere.  And so because once upon a time cute little Word macro viruses didn't make anybody any money.



LEO:  Right.



STEVE:  They were just sort of...



LEO:  It's vandalism.



STEVE:  They were an itch.



LEO:  It was because you could do it.



STEVE:  Yeah, exactly.  Just like, you know, just like an itch.  It's like, okay, go away, you annoyance.



LEO:  And yet I think science fiction authors like Daniel Suarez probably could have anticipated this.  And there were people who were saying, you know, cyberwarfare is going to be an issue.  We're in it now, boy.  And very vulnerable.  What is this - you didn't mention the Picture of the Week.  Is this related to VPNFilter?



STEVE:  No.  It's actually a story later.  That's what happens when, and we will get to it, when a vulnerability is found in a port that was mistakenly left open on Android devices.



LEO:  Oh, this is the Android one, ooh.



STEVE:  Yes, yes, yes, yes.  And it's not good.  So anyway, I just, you know, VPNFilter is apparently - I wish it had a better name.  You know, VPNFilter, that's just, you know, Heartbleed, that's something to get behind.



LEO:  That's scary enough.



STEVE:  Yeah, exactly.  Or Spectre or something.  But VPNFilter?  That sounds like something you want...



LEO:  Yeah, yeah, it does.



STEVE:  ...rather than something you don't want.  Going to filter my VPN, oh.  Okay.  So as if that weren't enough, Guardicore Labs has just published details of their discovery of something they've named Operation Prowli, P-R-O-W-L-I, which is a network of more than, yes, 40,000 compromised victim machines owned by more than 9,000 companies around the world, which is using some vulnerabilities in CMS, content management systems.  And they didn't say Drupal specifically.  Also, HP backup servers apparently have a vulnerability or a weak password.  Also, believe it or not, DSL modems, I guess some of them are smart enough to be infected.  So at some point you're just too dumb to have an infection.  But unfortunately, DSL modems have gotten smart so they can be infected.  And of course IoT devices.



So once again - and this doesn't feel state sponsored.  They're mining cryptocurrency.  They're redirecting traffic to promote fake websites and running tech support scams.  So this just sort of feels like a larger independent group who have focused on a number of vulnerabilities.  By watching the traffic, Guardicore basically knit an awareness that this was a single entity together because they spotted overlaps in the command-and-control servers of what would otherwise look like different groups operating independently.  Because, for example, to attack these HP backup servers using weak passwords or a vulnerability is different from maybe using Drupal, the known Drupal vulnerabilities, and whatever it is that's wrong, probably an exposed management interface, on the DSL modems.  Those are all different vectors.



But what they noticed was they're, like, all phoning home to a common set of IP addresses for command and control.  So they said, oh, this is a bigger group than we thought.  So they found it using various attack techniques, for example password brute-forcing, and weak and known vulnerable configurations.  So just, again, this is one of the other things, as I was saying before, that just sort of, unfortunately, you can't not call this a trend.  I mean, we're seeing this trend sort of emerging of this huge buildout of not fully secure Internet-connected stufferage, stuffage, stuff.  And, boy, getting stuffed, unfortunately, is what's happening.



So again, I wanted to put this on our radar.  We'll keep an eye on it, just in case it grows.  At some point there really will be some turf wars over the available stuff.  And the other thing that I think is going to happen, I refer to this a little bit later, or actually in the next story about Drupalgeddon2, is at the moment, as I've said before, we can take a little bit of solace in the fact that, when people get into a server, when the bad guys get into a server, their intention is to use its compute resource.  Which is sort of like, okay, that's good because what you don't want is them turning around.  You want them facing outwards toward the Internet.  You don't want them turning around and looking inwards into your network, which they also do have now access to.  It's like, okay.



So I have a feeling, before long, when maybe, who knows, it becomes too expensive even to mine bitcoin on stolen hardware, if that ever happens.  But something could happen where this mass exploitation, this mass compromise of devices is no longer generating money enough that it keeps people from worrying or wondering, the bad guys, what's behind the servers that they're inhabiting because that'll be a whole 'nother problem.



So speaking of that, we have Drupalgeddon2, which of course it's "2" because several years ago was the first real problem with the Drupal Content Management System.  And of course we've been speaking for the last few months, I think we're, what, three months in now from when the announcement was made on a Wednesday that the following Wednesday would be a big reveal, and everybody - the hope was of the Drupal devs that by putting out - like everybody should get ready.  I remember you and I talked about it.  And your TWiT crew, because you guys are Drupal users, your guys were already aware and ready to patch the moment this fix was released because the presumption was bad guys would be on it immediately.  And within 24 hours, that following Thursday, indeed there were scans on the 'Net for this, and then a couple days later the beginnings of exploits.  And now it's full blown.



So taking advantage of, like, a few months to give things time to settle, Troy Mursch at Bad Packets recently performed a passive scan of the Internet to get an appraisal of Drupal installation version numbers because versioning is one of the ways that we're able, well, passively to get some sense for what's going on, like are Drupal sites keeping themselves current or not.



So he wrote:  "In my previous post," he says, "I detailed a large cryptojacking campaign that affected hundreds of Drupal websites."  And this is just related to Drupalgeddon2, but not it itself.  He said:  "Multiple campaigns remain active today and are documented further in the latest SecurityTrails report. An important question was raised during my initial investigation:  How many Drupal sites are vulnerable?  To find the answer I began looking," he writes, "for sites using Drupal 7."  This is, remember, there's 7 and 8, and then the really old ones are 6.



He said:  "This is the most widely used version per Drupal's core statistics.  Using the source code search engine PublicWWW," he says, "I was able to locate nearly 500,000 websites using Drupal 7."  He says:  "I promptly began scanning all the sites to establish which were vulnerable and which were not."  He says:  "I regarded sites that were using at least v7.58 as not vulnerable to Drupalgeddon2.  This critical flaw is detailed in" - and this is the Drupal security advisory that is Drupalgeddon2, SA-CORE-2018-002, which has been assigned the CVE number 2018-7600.  And you're showing now the pie chart on the screen of what he found.



He said:  "Upon completion of the scan I was able to determine 115,000 sites were outdated and vulnerable."  That is, older than the version of 7 which was the fixed one.  134,000 were not vulnerable, and 225,000 he was unable to determine one way or the other.  He could not ascertain the version used.



Now, he notified the Drupal developers of this, and they were not happy with his assessment and responded to him, and publicly, that the number might be much lower than he's claiming, the 115,000.  Well, of course they're wanting it to be true that the number is much lower.



LEO:  Well, but his methodology was also pretty janky.  I'm sorry.



STEVE:  Yes.  He said...



LEO:  If you look at our site, you can't see the changelog.  I mean, come on.



STEVE:  Right.  Well, but this is the changelog he did find.



LEO:  We'd be in the red bar chart because they don't know if we've updated.



STEVE:  Correct, correct.  But still, for 115,000 to be publishing their changelog which allowed him to determine what the version number was.  But the developers commented that it was possible to remediate the problem without it being reflected in the changelog.  So he of course, in order to go any further, would need to attempt an exploit of the vulnerability, which would be illegal, and so nobody but bad guys are able to get an accurate assessment for exactly what that count is.  But if nothing else, it's a little worrisome that, of the versions which are visible, because the changelog would show the most recent version, 115,000 are older than the v7 which was fixed, which was 7.58, and are not being kept current.



So what we do know is that best practice says keep yourself updated to the most recent version.  Even if you're not going to go to 8, for example, maybe compatibility problems, at least be maintaining currency.  And it looks like there are - and it's not surprising.  It should not surprise us because this is what we see, not only with Drupal, but pretty much with everything else, is that lots of exposed systems on the Internet are not being maintained.



LEO:  Yeah.  Drupal's a little bit of a different beast because there have been some very big changes between major version numbers on Drupal.  And, for instance, we didn't update our site for a long time because Drupal 5 to 6 was a really...



STEVE:  Big change.



LEO:  Yeah, you couldn't use your existing stuff.



STEVE:  It, like, broke everything.



LEO:  Broke everything.



STEVE:  Broke everything, yes.



LEO:  So there are a lot - there probably are a lot of Drupal 5 sites out there.  Doesn't mean they haven't mitigated the flaw.  They just haven't updated to Drupal 7 because it's such a big change.  Drupal's unusual in that regard, I think, as content management engines go.  For some reason they really make massive changes in between major version numbers.



STEVE:  Well, it's good to know.



LEO:  I'm familiar with this only because we've been running Drupal since 2005.  So I know how hard it is to do an upgrade.



STEVE:  Yeah.  So, okay.  Kevin Beaumont is a well-known security researcher who tweets from his handle @GossiTheDog.



LEO:  Oh, wow.



STEVE:  Yup.  He titled his blog post four days ago "Root Bridge" - and I was thinking to myself, or root canal.  He said:  "Root Bridge - how thousands of Internet-connected Android devices now have no security and are being exploited by criminals."  It turns out the Android OS, which is used not only by smartphones, of course, but by cars and televisions and DVRs, and lots of people have taken Android OS and packaged it for their own purpose.  It's a little general purpose operating system that can be useful for all kinds of things.



LEO:  And it's free, and it's open source.



STEVE:  Yup, all the good things.  Unfortunately, well, okay.  Yes.  Unfortunately, it can be misconfigured in an alarming way.  There is a very handy feature which I actually have used myself in the past with an Android device known as the Android Debug Bridge, ADB for short, which allows developers to communicate with whatever the Android device is, typically over a USB connection.



LEO:  Yeah.  Everybody who's ever rooted an Android phone has used this.



STEVE:  Yes, exactly, exactly.  It turns out, for example, the Android Developer Portal describes ADB, saying:  "The ADB command facilitates a variety of device actions, such as installing and debugging apps, and it provides access to a Unix shell that you can use to run a variety of commands on a device."  Okay, Leo.  Now, imagine if that ADB interface, with all its power, were posted by mistake on port 5555?



LEO:  That would be bad.



STEVE:  That would be bad.



LEO:  That would be very bad.



STEVE:  That is what you call bad.  Kevin explains:  "ADB is completely unauthenticated, meaning anybody can connect to a device running ADB to execute commands."



LEO:  That's true.  There's no login.  No, yeah.



STEVE:  Right.  The presumption is, if you've got it, you have to have it in your possession in order to plug a USB connector into it, in order to plug it into your Mac or your Windows or your Linux box, in order to do whatever you want to do with it.  Unfortunately, vendors...



LEO:  Are stupid.



STEVE:  Get a load of this, have been shipping products...



LEO:  Oh, lord.



STEVE:  ...with Android Debug Bridge enabled on WiFi, listening on port 5555.  It's too bad it's not 666, but that's actually...



LEO:  Why would they do that?



STEVE:  It's pure mistake.  They had to have just, you know, the developer had it enabled in the config.



LEO:  Yeah, they just never turned it off, yeah, yeah.



STEVE:  And they forgot, yes, they forgot, allowing anybody to connect over the Internet to a device.  It's also clear some people are also rooting their devices, but then leaving - because you can, once you do this, once you get in, if you want the convenience of turning on ADB over WiFi, you can turn it on.  But you certainly don't want to leave it on.  Anyway, during the research that Kevin did, they found, as he wrote, everything from tankers in the U.S. to DVRs in Hong Kong to mobile telephones in South Korea.  And, he said, as an example, a specific Android TV device which he has not named was also found to ship in this condition.  He says in his blog it's highly problematic, as it allows anybody without a password to remotely access these devices as root and then to silently install software and execute malicious functions.



LEO:  Oh, you have full access to the device.  You can do anything you want.



STEVE:  Oh, yes.



LEO:  You could read every folder.  You could do anything you want, yeah.



STEVE:  Yes.  So our Picture of the Week I repeated here in the show notes just to sort of further drive the point home.  This is what happened just after February 1st.  Rapid7, the security project, their Project Heisenberg, and also GreyNoise Intelligence, whom we referred to a couple weeks ago, looked at what happened to port 5555 scanning.  That's four fives.  It just went vertically, like, straight up and has been high ever since.  So what has happened is that an active campaign created an exploitable wormable piece of malware which is crypto mining.



LEO:  Oh.  Over that port.



STEVE:  Yes, yes.  So whatever this device is gets infected.  It begins mining.  And then also, being a worm, it itself starts looking for other open port 5555s that it can infect.  Which means we now have essentially a permanent presence of yet another thing on the Internet which will probably never go away completely.  And if at any point a new Android device appears, if this mistake gets made again - and this is not just one device.  That's what's weird is we've got tankers in the U.S., DVRs in Hong Kong, Android TVs.



Unfortunately, this appears to be, I don't know, it's probably not common, but it's not also impossibly rare.  For example, in China alone, the use of Shodan, the Internet indexing vulnerability or open port indexing service that we refer to often, Shodan returned 82,274 devices in China that are accepting connections over port 5555.  We don't know that they're all Android ADB, but that's the port that that service listens for connections over, unfortunately.



Now, I do have a takeaway for our listeners because the link in the show notes here, www.grc.com/x/portprobe=5555 will allow anyone to instantly check their device to see whether it is accepting connections over port 5555.  And you've got that on the screen.



LEO:  And I'm safe, thank god.



STEVE:  The good news is it's green, yes, you're stealth.  You are safe, as you should be.  Somebody with an Android device, for example, some smart device, you're welcome, obviously, to use GRC.com's ShieldsUP! port probe with a specific customized port in the URL that allows you to do pinpoint port probes immediately.



LEO:  That's nice.



STEVE:  And, boy, if it comes up, what, blue, that would be - well, blue's not that bad.  That's non-stealth.  But if it's red, that means that, in response to GRC sending your device a SYN packet to port 5555, we received an ACK, meaning that the TCP stack in your device is saying, yeah, I'd love to talk to somebody on port 5555, which your device should not want to do.



LEO:  You'd be protected if you're behind a router; right?



STEVE:  Yes, yes, yes.



LEO:  I mean, most cases.  And also this is probably not an easy thing to put on a device.  You'd probably have to buy a device that already was misconfigured; right?  I mean, malware...



STEVE:  Oh, yes.  I'm sure, given the...



LEO:  ...couldn't probably do this.



STEVE:  Yeah, given the number of devices which we're seeing exposed, it looks like it was just misconfigured out of the factory.



LEO:  Yeah, yeah.



STEVE:  But you don't want one of those.



LEO:  No.  Thank you, I'll pass.



STEVE:  So what the heck has been going on at Cisco?  Throughout this year Cisco has been performing some internal code auditing for which they should be encouraged and congratulated.  And I've been watching these reports, and I haven't said anything because it's like, okay, I mean, I've been a little - I've wondered why Cisco keeps finding backdoors in their own products.  That's a little worrisome.  But it's not the end of the world.  I mean, while it's disturbing that for some reason they're finding backdoors buried in their own products, at least they have the maturity and the foresight to be looking at their own code and not just assuming it's all fine.



And this actually was triggered, I think, by an earlier discovery by an outside party of a worrisome backdoor.  And we did talk about that even further back.  But now an external security researcher, Aaron Blair of RIoT, I got a kick out of that, RIoT as in R-I-O-T Solutions.  He was researching a vulnerability in some Cisco software, their Wide Area Application Services, WAAS.  And he leveraged a vulnerability that gave him access to the underlying file system on the platform he was using, which not even a normal device admin would get.  With Cisco you could log in with extra, like, root administrative privileges, but that gives you more commands.  You don't get underneath the OS in order to see the actual file system.



Well, this vulnerability allowed Aaron to do just that.  And what he discovered was another previously unknown hardcoded backdoor, which he responsibly reported to Cisco, and they are fixing or have fixed.  There's now an update for this Wide Area Application Services software.  So that's good.  And it even wasn't a really bad problem.  In all of these networking devices there's a service called SNMP, Simple Network Management Protocol, which is a - I think it runs over port 161, if I recall.  I use it a lot.  Anytime you are monitoring, like, traffic on a remote device, you're sending SNMP, typically UDP packets, querying for specific counters to be told to you.



And they have a bizarre protocol.  It's like there's a unified - they're called MIBs, M-I-B, which are sort of a dictionary of dotted numerical tree.  So it's like 1.3.7.4.16.2., it's like that.  And at each dot is a multiway branch through this tree.  And after about 20 of these dots, you finally get down to a leaf node where is a counter, which is like bytes in or bytes out or packets in or firewall firings of this packet or whatever.  So it is cool because it allows - it's a standardized mechanism for allowing remote over-the-network monitoring of devices.



Now, write access is significantly more dangerous because it's possible also to configure devices over SNMP, if you have write access.  For example, that tree, benign as it is, allows you to do things like add and remove filters and rules and NAT mappings and so forth.  So it can be powerful.  Still, nobody who's concerned about security wants somebody remotely, without authorization, to access the entire SNMP statistics tree of any of these devices where this WAAS software is.  And who knows what other devices may also have this secret.  So the big problem is here is like the fourth in just a few months of discovered hardcoded backdoors in the devices of a major, like the major - I mean, there's a lot of competition now, but used to be Cisco was it - Internet big iron hardware manufacturer with routers and switches and so forth.



And so, if you step back, you just sort of have to say, as I did at the top of this, what the heck has been going on at Cisco?  I don't want to do the conspiracy theory thing; but of course in the post-Snowden era, where we have seen clear evidence of prior involvement by the NSA, their fingers seem to be in these things, and the idea that employees could be implanted in corporations or turned once they're there or believe that they are supporting U.S. domestic security by just putting a cute little backdoor into something.  The problem is, as we know, a backdoor of this nature can be used by anybody who knows about it.



And so here's Aaron discovering this because he gets access to the file system through a vulnerability, which he then reported responsibly to Cisco.  But then he also had to say, oh, and by the way, I found an undocumented backdoor in your SNMP service which you might want to look at, too.  So of course it begs several questions.  Why are there backdoors that it sounds like even Cisco themselves do not officially know about?  How long have they been there?  Why are they there?  And which other ones are there that aren't known?  And this clearly got onto Cisco management radar somewhere because it had to have come, apparently it came as a surprise to them, too.  And so they started performing an internal code audit of their own code to figure out, okay, we can't deny the truth any longer, that somebody has been putting code in our own products.  So, yikes.



LEO:  Yeah, I'd love to know who.  Wow.



STEVE:  Yeah, yeah.  I mean, and these are, unfortunately, the things we will never have answers to.  But still, it's worth proceeding with caution.



I guess it was inevitable, I'm sure it was inevitable that cryptocurrency miners, which want to mine on consumer devices, would attempt to become stealthy.  And they have a problem because what they want to do in order to mine is use resource, CPU and hopefully GPU, when they can get it.  Yet if people are using the computer, like for their own work, especially a gamer who intends to saturate both the GPU and the CPU, they're going to notice if their frame rate suddenly falls to 2, or if the game just stops playing the way they're used to it.  Or if somebody who is somewhat savvy realizes that their computer has suddenly gotten sluggish, then, I mean, I know that I, and I know you and other people, Leo, will fire up a task manager, a task viewer, to see what process in our system is hogging all of the memory and/or, you know, sometimes it's memory.  In this case, with a crypto miner, it's CPU resource.



So Bleeping Computer's Lawrence Abrams described a new miner which they had become aware of which is doing exactly this.  Now, it's not particularly clever.  I'll describe the clever solution in a minute.  But what they're doing is - so this thing gets into someone's machine.  It establishes an entry in Task Scheduler so that, at midnight, the Task Scheduler will first trigger.  So it doesn't do anything when it first gets into the machine.  And using Task Scheduler is a time-honored means for malware to execute itself because Task Schedule is able to run things for which it has been set up to do so.



At midnight, the Task Scheduler triggers and then repeats every minute.  So that launches the miner, which checks to make sure that there isn't already an instance of it mining; and, if so, then the newly relaunched instance immediately terminates.  While mining, it proactively enumerates the system's process list in order to - and this is what I'm thinking is, well, okay, this is kind of brute force and not really very clever - to look for instances of Process Explorer, Task Manager, Process Monitor, Process Hacker, and the AnVir Task Manager running in the system.  It infers, if it sees those running, that a savvy user may be wondering what the heck is going on.  And so if it sees those, it terminates itself immediately, disappears.



It also looks for, and this is a weird list, but for Counterstrike: Global Offensive, PlayerUnknown's Battlegrounds, Rainbow Six, or Dota 2, which are games which want to have full use of the system and where, if you've got a crypto miner trying to also share the system, that game is not going to run as well as it should.  If any of those things are found, it terminates itself immediately in order to keep from being more permanently removed from the system.  And then it knows that Task Manager will continue to do its job, which is to trigger a reawakening every minute.  Presumably the first thing it does before it starts to mine is do this process enumeration to see whether any of these things are present; and, if so, it terminates again.



So this just seems, I mean, it's like, okay.  We've now entered a world where cryptocurrency miners have realized they're giving themselves away if they just sit there and squat on a person's machine.  Their strategy will be much more successful if they arrange not to cause themselves to be deleted.  So as a developer...



LEO:  It's called "nice" mode.  It literally is; right?



STEVE:  Exactly.



LEO:  Nice.



STEVE:  Right.  So, well, yes, nice in Unix, exactly.



LEO:  Yeah, yeah.



STEVE:  What I'm a little bit surprised about is that whoever it was who did this didn't do it cleverly.  So, for example, what I would do if this were my focus, and it's not, but first of all I would reduce the process's own priority to the lowest possible so that it essentially uses the idle time of the system and gives up all processing preferentially.  Then I would, if I were a malicious crypto miner, monitor my own hashing rate, which I can certainly do, to notice if the hashing rate drops because that would tell me that this system is not idle, it's in use, and that I should consider fully backing off and pausing all mining and then just checking from time to time.  I mean, it's easy to determine whether the system is idle or not.



So watch the system, wait for it to go idle, then go in with low priority background task, which you could have 100% of the system.  You just can't have it if anything else wants 100% of the system.  In which case you get to mine, you're not in the way, you are probably going to get to stay around a lot longer because you're not going to cause somebody to go run Malwarebytes or whatever on your system in order to spot and remove you.



And I think what we're going to see - because it's very clear that crypto mining has supplanted the previous file encrypting malware as the preferred means of raising money because lots of people had backups, or they objected to paying, or they didn't have anything valuable on their system.  Well, the actual presence of their system is now becoming valuable.  So it'll be interesting to see where this evolves.  I think we're going to be seeing some evolution.



I did want to mention that, as I talked at the top of the show, that Palo Alto Networks has a group they call Unit 42 which is kind of fun because I'm sure 42 came from Douglas Adams.  That's of course the ultimate answer to all things is 42.  Russia's Fancy Bear, APT28, this Sofacy, I mean, they're known by many names.  They've been observed to have changed their tactics from a highly targeted, go after a few people in an organization with phishing attacks specifically designed to stay under the radar.  They're now doing more of a scattershot.  Palo Alto Networks Unit 42 group has noted that these attacks from this Russian state-sponsored group appears to be using email addresses easily found with search engines to spray organizations that they want to get into.



And the first thing that occurred to me is to wonder whether maybe the group is under new management.  I mean, literally.  I'm not kidding.  Like somebody just decided, okay, we're going to take a different approach, and so said to the techies, let's try something different.  There's a lot more detail in the report that I don't want to get into.  But it did occur to me that it's interesting that we're seeing a shift from what has been a well-proven strategy to something that they described it as a focus-fire-style attack in the past to something much less focused and a little more impatient to get inside of organizations.



I've talked also about a genealogy site, MyHeritage, which had lost control of their users' login account data.  And of course when you hear that a site to which you have uploaded your DNA has lost control of their data, that would be a privacy concern for some people.  The good news is there are apparently three entirely separate databases.  There is the user login account database.  There is the financial credit card charging-people-for-the-service database.  And also, separate from either of those two, is the genealogy we've-got-your-DNA-that-you-uploaded-to-us database.  All are separate.  And they did lose control, they discovered, when someone found the archive of 92,283,889 users on an online archive.  What was there was email addresses and password hashes.



To MyHeritage's credit, in addition to having all of the databases separate - which is good security policy, so that if this happens it's not people's DNA; you don't have to tell people, gee, we're really sorry, but we leaked your personal and private DNA data that we promised we would keep secret.  Also they disclosed it the day of the breach, that is, well, the same day they learned of the breach.  The breach occurred on October 26th of 2017.  They know because that was the latest date found in the archive.  And so that was when the snapshot was made of the database.  And they have employed a forensic security firm to dig in further and see what more can be learned about what happened.



The other good news is, and more reasons to give them some credit, is not only did they immediately disclose the breach, but the passwords were hashed with a per-user salt.  So if they used per-user salt, although they didn't specifically say in their sort of generic disclosure, it's very likely they also used some sort of PBKDF, a Password-Based Key Derivation Function, meaning it wasn't just a salted SHA-256 where they used a random salt, but they may have iterated that some useful number of times to make it very difficult to reverse the hash back to a password.  So the per-user salt, of course, means that they're preventing any sort of massive gang reversing of all 92-plus million users, which could have been done if they'd used a single shared salt.



The good news is it really looks like these guys have their security nailed.  They also indicated that they would soon be deploying second-factor authentication to further strengthen their users' login safety.  But when this happens, when password hashes escape, standard advice is you've got to change your passwords as soon as possible.  So even though you're probably safe, anyone hearing this who didn't receive notice already directly from MyHeritage, hopefully you already know this, you do want to change your password.



It's also worth noting that another possible reason for their speedy responsible disclosure could be chalked up to the EU's GDPR regulations, which as we have been discussing are now in place.  And the GDPR requires companies doing business in the EU to disclose within 72 hours, that is, three days, of learning of a breach which affects the privacy and data of EU citizens.  So I think we're going to see a change of behavior.  We're not going to see anymore of this Equifax nonsense where, oh, yeah, we knew six months ago, but we didn't get around to telling anybody about it.  So anyway, these guys look like they've got security in place.  They did a good job.  And hats off to them.



LEO:  Wow, good news.



STEVE:  Yes.  Okay.  Adobe Flash cannot die soon enough.  Once again, an actively exploited in the wild zero day had come to life.  They announced an emergency patch last Thursday.  I don't know.  Like in other words, when malware - of Flash you would have to say "when malware has a large, reputable publisher."  I mean, that's what this is.  This is just ridiculous.



Okay.  So Adobe's own summary for this update said:  "Adobe has released security updates for Adobe Flash Player for Windows, macOS, Linux, and Chrome."  Even Chrome's built-in Flash player was vulnerable.  "These updates address critical vulnerabilities in Adobe Flash Player 29.0.0.171 and earlier.  Successful exploitation could lead to arbitrary code execution in the context of the current user.  Adobe is aware," they write, "of a report that an exploit for" - and then there's a vulnerability for their Flash Player - "2018-5002 exists in the wild and is being used in limited, targeted attacks against Windows users.  These attacks leverage Office documents with embedded malicious Flash Player content distributed via email."



In some of the coverage of this, Leo, I saw some observers saying, well, browsers have hardened themselves proactively against Flash, so now the miscreants have figured out another way to leverage the continuing existence of Flash, even though browsers won't use it any longer, but it's still present in people's systems in order to exploit them.  And look at this exploitation path.  I have the picture here in the show notes.  It's like, yikes.



The user opens a weaponized Office document which contains a Flash ActiveX embedded object.  That reaches out to a remote server to obtain a Flash exploit which is retrieved using public key crypto and then executed, which decrypts itself and then, in Step 7, uses an exploit in Flash, Loader.loadBytes, to load and trigger the exploit, which then reaches out again to the same server to download shell code, which it's then able to execute on the user's system.



So remind me again why it is exactly we're waiting until 2020, the end of 2020, through 2020, to finally be rid of this Internet menace called Flash.  I mean, we have routers infected, half a million routers infected with self-destructing firmware, ready to be triggered on word from Moscow.  Yet how difficult could it possibly be for Adobe to push out just one last update to self-destruct all remaining instances of Flash in the wild and make everyone safer?  It's just like, my goodness.  It's only now being used by websites that still want to use it to display video.  Yet we've seen that this slow end-of-lifing doesn't work on the Internet.  I mean, we've covered multiple instances where Chrome, it's only by refusing to allow things to run any longer, and creating well-published deadlines, that people have been forced to move away from technology that, well, you know, it's working, so let's not change anything.  Except its presence there is hurting everyone.



So anyway, I just had to rant again on another occasion of a zero-day being exploited, of Flash, by virtue of the fact that it still exists in people's machines.  And what Adobe is going to have to do to be responsible, I mean, I guess we're going to have to wait until the end of 2020.  But they need to have it self-destruct.  If they're saying we're going to no longer support it, then please, Adobe, kill it.  Shoot it in the head.  Just end of Flash.  Goodbye.  That's what we need.



And, finally, checking in on Marcus Hutchins.  I didn't dig into this in depth because there's just too much of this is opinion.  And you can find - it's the Internet; right?  So you can find any opinion that you want about anything on the Internet.  But a pro freedom of the press blogger who has her Ph.D. in something, I don't remember, I mean, I read her CV trying to figure out who this is.  An independent journalist is of the opinion that the government has always been on shaky ground with Marcus.  I mean, you and I have talked about the plight that he's in, that it looks like in the past he did some things when he was a kid that he's not proud of and that he would take back if he could, that he probably regrets.  And of course, as we know, he was trying to leave after the last summer's Defcon and had to go to the U.K. and got nabbed by law enforcement in Las Vegas, and has been under some form of arrest ever since while a jury trial has been moving along slowly.



And I was surprised to see that.  I mean, I guess it must be that the prosecution felt that they would have a better case with a jury than with a judge.  But some of the opinion that I've seen suggests that the government's case is beginning to fall apart, and that there may be a dismissal, a full dismissal of the case against Marcus pending.



LEO:  Interesting.  Hmm.  Wow.



STEVE:  Yes.  Which, you know, we could hope for.  The title of this update was "To Pre-empt an Ass-Handing, the Government Lards on Problematic New Charges against MalwareTech," and that's of course Marcus's handle.  This journalist wrote:  "But the government, which refuses to cut its losses on its own prosecutorial misjudgments, just doubled down with a 10-count superseding indictment."  That is, in many cases supersedes the previous counts of indictment.  "Effectively," she writes, "the superseding creates new counts, first of all, by charging Hutchins for stuff that, one, is outside a five-year statute of limitations; and, two, he did when he was a minor, that is, stuff that shouldn't be legally charged at all; and then adding a wire fraud conspiracy and false statements charge to try to bypass all of the defects in the original indictment."



And she writes:  "The false statements charge is the best of all because, for it to be true, a Nevada prosecutor would have to be named as Hutchins' co-conspirator because his representations in court last summer directly contradict the claims in this new indictment."  So again, I haven't read into all this.  I don't know.  I'm not an attorney, so I can't render an opinion.  But it's nice that it's not just a slam dunk, you know, you're guilty and we're locking you up forever, thank goodness.



And we'll keep an eye on this.  It would be wonderful if in fact this, I mean, I did read much more about this than I have said.  But again, I'm unable to render an opinion one way or the other.  We can just hope that in fact this is going to fall apart.  It does look like these new charges, they found some other piece of malware which, if the timeline is correct, he would indeed have had to write it when he was not yet 18, and the statute of limitations would have expired since then.  So we'll see.



And lastly, before our final break, I'll share a fun story from Mark in Merced, who sent this on the 2nd of June.  The subject was "Fun SpinRite Story."  He said:  "I wanted to share a quick story with you.  My wife was using our household laptop when it started to lock up and display a 'Wait/End Task' message when opening the Start menu and doing other things.  So," he writes, "I ran SpinRite on the machine.  It found a few bad sectors on the hard drive, and I said 'Aha' to myself.



"Needless to say, the laptop now works great again.  So I hand it back to my wife, and she gets back to work again on a Photoshop project.  A bit later, she wants to show me something she was working on before the incident.  So that I can get a better look, she goes over to the coffee table with the laptop sitting on top of it and drags the table my way on the carpet.  I watch the laptop jiggling, bouncing, and jarring on the table top and think to myself, 'No wonder the drive needed SpinRite.'"



And of course this has been a longstanding observation.  Hard drives are serious high technology.  I mean, it's astonishing to me today that we have the data storage density that we do.  And so it means that these devices are fragile.  And I can't think of a more hostile environment, okay, except arguably an external USB plugged-in drive, that's an even greater danger, because everyone's heard of a head crash.  It literally means the heads crash into the disk because, other than the Iomega devices, those ZIP drives that were so famous, and the Bernoulli boxes back in the day, heads are not in contact with the magnetic surface.  The spinning pulls air underneath the head to create essentially a cushion, and the head is being pressed down hard against this air cushion, but it is flying very closely, yet not in contact with the magnetic surface.  And this is the hard drive technology we've had for many decades.  Consequently, this is fragile.



And, boy, you want a hard drive mounted in a server, bolted to a rack, bolted to the ground, in an area with no earthquakes, in order to keep it immobile.  And we've seen that even screaming at a hard drive can cause it to have problems; or when the fire suppression system whistles loudly, that can cause problems.  So, wow, I can certainly understand what Mark meant when he said he watched the laptop bouncing on the tabletop and just sort of shook his head and thought, well, maybe I'm going to need to explain to my wife how delicate this is and run SpinRite again before there are problems that can't be fixed.  Wow.



LEO:  Well, there you have it.  It's good he had SpinRite.  That's all I can say.



STEVE:  Yup.  So Zippity Do or Don't.  Boy.  Okay.  So as I said at the top of the show, one thing we keep hitting is how difficult interpreters are to get right.  I mean, and for a good reason.  Whether you are decompressing a JPEG, or you are re-rendering a Java byte stream back into a Java object, or you are decompressing a blob into its original form, the way this is done is instructions are read, little tokenized - it could be just a combination of bits which has a larger meaning.  And so if it's 001, that means go over in this direction and then look for further instructions.  If it's 010, then go in this direction and then see what comes next.  I mean, and this is the way we decompress or we reassemble or we expand something from an encoded form to the decoded form.



The problem is it is so sort of implicit to assume that the encoder produced what we are decoding.  That is, we assume a benign source of the encoding which we are then going to decode.  It's difficult for me to deeply enough articulate how hard it is to break that assumption.  You invent this amazingly cool compression, and you're proud of it.  And then you write the decompressor that reverses the compression, and you go, look, I got back what I put in.  Isn't that cool?  Or in the case of JPEG, I got back something that looks the same as I put in, even if it's not the same.  Which is kind of even cooler, when you think about it.  It's like, my eyes can't tell the difference; but look, you know, it looks the same.  It's a photo.



So it turns out that yet another rock got turned over recently with Zip compression.  And we were just referring to it.  We were talking about Huffman and Lempel-Ziv, the original patent holders at IBM in 1977 - it's a patent I studied extensively years and years ago - came up with this cool solution for - and we did a podcast on it quite a while ago, how Lempel-Ziv compression works.  But fundamentally it's an interpretation.  And just this week several different types of problems came to light.



Back two months ago, in April of this year, some researchers at a British software firm, Snyk, S-N-Y-K, named a discovery of a problem "Zip Slip."  And they began informing users of a large array of compression libraries that they had discovered a widespread flaw.  This affected RAR, which is hugely widespread; 7z, which is the 7-Zip compression, also very widespread; tar; jar...



LEO:  Tar, really?



STEVE:  Yes, tar.



LEO:  Uh-oh.



STEVE:  I know.  War, cpio, and apk.  And, I mean, yes, tar is, like, universally used in Unix and Linux systems.



LEO:  Well, apk is the app file format for Android.



STEVE:  Yes.



LEO:  Yikes.



STEVE:  Yes.  So as a consequence, literally thousands of projects written in programming languages including JavaScript, Ruby, Java, .NET, and Go, published by everybody - Google, Oracle, IBM, Apache, Amazon.  They listed Spring Pivotal, LinkedIn, Twitter, Alibaba, Eclipse, OWASP, ElasticSearch, JetBrains, and more.  All contained vulnerable code and libraries.



LEO:  Ai yai yai.



STEVE:  Yeah.  Now, it can be exploited using a specially crafted archive file that holds - get this, Leo - an old friend of ours, directory traversal filenames.  When extracted, any vulnerable code or library would allow attackers to unarchive malicious files outside the folder where it should reside.  Now, we should stop for a minute and explain directory traversal.  Also back from the dawn of time, in a filename or file path name, "dot" has referred to the current directory, and "dot dot" has referred to the parent directory.  So, for example, I'm often, if I have, like, /asm/sqrl, and I want to go to /asm/ne, which is GRC's net engine, I could do a "cd /asm/ne," but I often just do ../ne, meaning move up one level in the hierarchy and then down to a different branch from the parent.



LEO:  Do that all the time.



STEVE:  Yup, exactly.  It's very handy, especially if you're way down in a hierarchy, like nine levels down, slash this, slash that, slash something else.  You don't want to restate that whole deep hierarchy in order to just change to a different leaf at the same level.  So you do ../ and then rename the leaf, and you're immediately there.



Well, it turns out that that little shortcut has been for decades a source of vulnerabilities because you can repeat that.  You can go ../../../../../, and each of those moves you back up the hierarchy toward the root.  And once you finally hit the root, redundant dot dot slashes don't hurt you.  So you can just do 10 of them, and pretty much...



LEO:  Oh, it'll always get you to the root.



STEVE:  Yes, it will always bring you back to home base.  Now that you're there, first of all, that's a scary place to be because the root is the root, and you can then navigate back down a different path to somewhere else.  It turns out that all of those unarchiving libraries were not protecting against directory traversal, meaning that, if somehow they were asked to unarchive a file within the archive that contained a long directory traversal exploit, they would do so.



And the archive itself contains exactly that.  It contains essentially relative filenames that are relative to where you're unzipping.  So like you say, okay, I want to unzip this to this file, to this folder.  Well, you assume nothing can go outside of that folder.  It's going to be there and deeper, not waltz itself back up to the root of your directory and then dig back down into /windows/system32 and replace a DLL, for example, or rewrite a configuration file somewhere that it shouldn't.  But in fact all of these libraries can, at the time that this was discovered, be abused in this way.  Now, many of them...



LEO:  You can't write to the root directory without admin permissions.  So this is not an escalation attack.  You'd have to have access.



STEVE:  Correct, correct.  Although what they found was many places where this could be leveraged as part of that, or that you're able to rewrite some configuration file which does not itself look like it's a problem, but can then be used in order to stage a larger attack.



LEO:  Right, right.  And often people use these commands as you do escalation.



STEVE:  Exactly, exactly.  So for the last two months Snyk has been quietly and privately disclosing what they call the "Zip Slip" vulnerability.  Of course it's named that because you're able to slip backwards up the file system and then back down to all the vulnerable libraries and project maintainers that they were able to find to give them a chance to update this.  And of course the trouble is this is such a longstanding problem, and so far and widespread that it's going to continue to exist for quite some time.  So as I said at the top of the show, it's sort of scary that we're finding as many vulnerabilities as we are in systems that are as widespread as we are using them.



So in their posting they propose the question:  "Are you vulnerable?"  And they write:  "You are vulnerable if you are using a library which contains the Zip Slip vulnerability, or your project contains vulnerable code, which extracts files from an archive without the necessary directory traversal validation. Snyk is maintaining a GitHub repository listing all projects that have been found vulnerable to Zip Slip and have been responsibly disclosed to, including fix dates and versions.  The repository is open to contributions from the wider community to ensure it holds the most up-to-date status."



So in the show notes I have a link to - there is a PDF with a full technical whitepaper, and then it's Snyk.io, S-N-Y-K dot io.



LEO:  I think it's Snyk, not Synk.



STEVE:  Oh, it's Snyk?



LEO:  Snyk.



STEVE:  Okay, Snyk.  The Snyk Zip Slip, yeah.



LEO:  Here's all the libraries.  They have a GitHub list of all the libraries, wow.



STEVE:  Yup.  And it is extensive.



LEO:  Package manager, Java, .NET.



STEVE:  Yeah.  They noted a bias.  When they sort of stood back, they noticed that some packages tended to be, or some languages tend to have less instances of vulnerability.  For example, they said that Go had fewer because it tended to provide the services natively.  Whereas, for example, Java had more problems because Java doesn't have a central library offering high-level processing of archives, for example, zip files.  And they wrote:  "The lack of such a library led to vulnerable code snippets being handcrafted and then shared among developer communities such as on Stack Overflow."  So there did tend to be some sort of a bias in general.  But overall, you want to make sure that you're safe from this.



And so I will conclude with one more, believe it or not.  There was a problem found in another series of rar and zip instances which affected AV.  Avast, Bitdefender, and F-Secure were all found to have problems with their unzipping.  We've talked about the attack surface vulnerabilities created by AV.  And in order for AV to do its work, it needs to look inside the compressed archives that you may be downloading, if they're a zip or a rar or whatever, a 7z, in order to see what's inside them because they're often used to obscure malware of various kinds.



But all of these decompressors are themselves interpreters, and they have had a series of flaws that could be deliberately abused.  In the case of rar, which was the most recently fixed, which affected F-Secure, there's something known as "solid mode."  In solid mode - and I've used it myself because you get a denser compression if you encrypt with solid mode.  Solid mode prevents you from afterwards editing the rar to, like, add or remove individual files.  So it's the sort of thing you want to finally only do when the rar is going to be, as the name suggests, solidified and then no longer treated as sort of a quasi file system that you could add and update and remove things to and from.



Well, the reason is that, if you don't generate a solid rar, every file resets the state of the compressor to its initial condition so that each file starts being recompressed with no knowledge of the past.  It is the brilliance of the Lempel-Ziv compression, which all of these things use, where the history of what it has just recently seen informs it about what it may be seeing in the future.  And by simply pointing to what it has just seen, you're able to eliminate the redundancy and then achieve compression.  So if you compress in solid mode, then the state of the compressor is not reset at the beginning of each file inside of an archive, which gives you greater compression, assuming that a run of files is going to be largely the same kind of information, like a bunch of source code or a bunch of exes or whatever.



So it turns out that they did not check, that is, the rar unpacker did not check to see whether it had ever encountered a non-solid flag in a file.  Which is to say, the first file in a rar should always say "initialize the state of the decompressor."  Subsequent files could say "don't bother initializing the state of the compressor because I'm a follow-on file, and I want to take advantage of the state which already exists."  So consequently the mistake was made that, if the first file in the archive claimed to be a follow-on file, then the decompressor state was not initialized.  And an uninitialized buffer, especially when it's going to be interpreted by an interpreter, is a huge opportunity for exploitation.



And it turns out it was possible in order to leverage that into a remote code execution.  Bottom line was, if someone knew that you were using F-Secure's AV, if they simply, in any way, caused your F-Secure AV, which I should mention has been fixed since, so you want to make sure you're up to date because it was just recently fixed, if your system touched one of those files, it could take over your computer.



So again, another instance of an interpreter where the designers knew what they were doing, but they failed to look at the concept, failed to take into account the ways in which the meta language that they produced as a consequence of compression could be abused by an attacker.  And this was another clever example of that happening, which you really don't want to happen, you don't want to have in your AV because anything coming in through any external mechanism into your computer could trigger the AV which is trying to scrutinize everything.  And if you've got problems there, your machine can be victim.  So once again, another instance of an interpreter gone bad, or a weak, insecure interpreter biting us.



LEO:  I guess you could call Flash an interpreter.



STEVE:  Oh, it's an interpreter from hell.



LEO:  I mean, a lot of the exploits we see are interpreters.  And the reason that's worrisome is because it means a document file, which normally would be benign no matter what, can be used to attack you if you have an unpatched interpreter, as it were.



STEVE:  Yup.



LEO:  Steve Gibson has done it again, has he not?  A fun two hours, thank you, Steve.  We do Security Now! every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to stop by and watch it, you can, the live streams.  And they are moving around a little bit.  We've got some new ones.  I think we're saying goodbye to some old ones.  But TWiT.tv/live should always have a good choice for you, both audio and video live streams, if you want to watch live.  You can also come in the studio.  We have some nice people in the studio audience today visiting from Atlanta, Georgia, and San Ramon, California, just up the road apiece.  Thank you, Matt and Joan and Paul.  And if you want to do that, just email tickets@twit.tv, and we'll put a chair out for you.



If you want to download on-demand versions of the show, Steve has really a great place to start at his website, GRC.com.  Not only can you download audio of the show, he also does transcripts.  So that's a way you can read along with the show as you listen or after you listen or before you listen or you never listen.  You can just read it.  We have audio and video at our site, which is TWiT.tv/sn.  And of course every podcast app carries Security Now!, so you could subscribe in those.  When you're at GRC, don't forget to check out Steve's bread and butter, his great, must-have hard drive recovery and maintenance utility, SpinRite.  You should also...



STEVE:  And if you're jiggling your laptop around, then keep an eye on it, for sure.



LEO:  You know, that's one of the reason I like solid-state drives.  There's no moving parts.



STEVE:  Yes, yes.



LEO:  They really are more robust.



STEVE:  And the good news is SpinRite fixes them, too.



LEO:  Yeah.  From other things.  GRC.com.  Steve, we'll catch up with you next Tuesday.



STEVE:  Thank you, my friend.  Always a pleasure, thanks.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#668

DATE:		June 19, 2018

TITLE:		Lazy FP State Restore

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-668.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine a rather "mega" patch Tuesday, a nifty hack of Win10's Cortana, Microsoft's official "when do we patch" guidelines, the continuing tweaking of web browser behavior for our sanity, a widespread Windows 10 rootkit, the resurgence of the Satori IoT botnet, clipboard monitoring malware, a forthcoming change in Chrome's extensions policy, hacking apparent download counts on the Android store, some miscellany, an update on the status of Spectre & Meltdown - and, yes, yet another brand new speculative execution vulnerability our OSes will be needing to patch against.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about this week, including Microsoft's Mega Patch Tuesday, a clipboard attack on cryptocurrency, and Steve has suffered a sad loss.  No, it's not that bad.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 668, recorded Tuesday, June 19th, 2018:  Lazy FPU State Restore.



It's time for Security Now!, the show where we cover your security, privacy, and safety online.  Oh.  I didn't press the record button.



STEVE GIBSON:  Oh, I hate when that happens.



LEO:  I hate it when that happens.  Well, this recording is actually just for the reruns.  But it's time for Security Now!, the show where we cover your privacy and security online with this cat right here, Mr. Steve Gibson of GRC Corporation.  Hello, Steve.



STEVE:  Yo, Leo.



LEO:  Welcome to the show.



STEVE:  Great to be with you again as we cross past the middle of June.



LEO:  Yeah.  How did that happen?



STEVE:  I don't know.  It's, like, getting to be summertime already.  So, well, early in January, when we first discussed speculative execution attacks, or the vulnerabilities, rather, that had come to light, and everyone ran around with their hair on fire, it seemed likely, and I said at the time on the podcast that I didn't think we had seen the end of this, that it was going to be, like, a real problem because so much work had gone into the engineering of shortcuts and, in some cases, surprising architectural complexity for the sake of squeezing every last possible bit of performance out of our systems.  As we know, a couple weeks ago we talked about SpectreNG, also known as Variant 4 of the Spectre problems.  Well, this week we have another one.  This is the Lazy FP, or in some cases FPU, state restore.  And patches will be flying.



LEO:  Uh-oh.



STEVE:  Oh, yeah.  So we're going to wrap up - sort of I want to do, as a consequence of last Tuesday's Patch Tuesday, take stock of where we are with what patches are available for which operating systems and which flavors and whether the mitigations are enabled and disabled by default.  I've pulled all that together.  And then also take a look at the latest gift from some German security researchers, who realized that another feature of the Intel core processors, so this only affects Intel core processor families, I think I read from - what was the bridge?  Shady Bridge?  That sounds wrong.  Sandy Bridge.  Sandy Bridge forward.  No AMD problems.  This one doesn't affect the ARM architectures and so forth.  This is only Intel.  But it's another way of leaking information.



But before that, we need to take a look now at last week's, what I would consider a Mega Patch Tuesday and what it means.  We do the podcast as this is all happening, so it's just sort of with the timing of when the updates are happening and what's been released, there's just no chance to cover it.  And normally I just sort of let it go because our listeners know it's important to patch.  This time, well, let's see.  Eleven critical vulnerabilities and 50, five oh, more than 50 updates.  And several of them are really horrifying.  So I think we're going to end up seeing some downstream consequences of what Microsoft has just fixed.



There was also, in the security news, a very clever and nifty hack of Cortana in Windows 10 from security researchers at McAfee.  Microsoft put out an official "when we patch," or "when do we patch" guidelines summary which is sort of interesting because there's a little more discipline, or so it seems, to their decisions than was obvious before.  We've also got some continuing tweakage of web browser behaviors to increase, and I guess maybe help, our sanity.  A worrisomely widespread Windows 10 rootkit.  We haven't talked about rootkits for a long time. They're really interesting things, of course.  And the whole reason that Secure Boot exists is to prevent them.  But it doesn't.



We also have the resurgence of the Satori IoT botnet with sort of a vengeance; some clipboard monitoring malware; a forthcoming change in Chrome's extensions policy; kind of a clever hack for apps in the Android store; a little bit of miscellany; and then, as I said, we'll wrap up talking about where we stand today with Spectre and Meltdown mitigations for Windows, and then what is this Lazy FPU State Restore.  So it's been a busy week.



LEO:  All right, Steve.  I've got the Picture of the Week all queued up.



STEVE:  This is a particular poignant one for me, Leo.



LEO:  Aw.



STEVE:  Now, of course, for those who don't have video, this is a picture of the famous 14-pin DIP.



LEO:  I've been called worse.



STEVE:  DIP stands for Dual Inline Pins, or sometimes Dual Inline Plug.



LEO:  This looks like the memory chips that I used to have to put in my motherboard.



STEVE:  Oh, they were ubiquitous.  In fact, they were typically the 7400 series.  The 7400 was a quad NAND.  So you'd have 14 pins.  Pin 14 is 5V VCC.  Pin 7 on the exact opposite is ground.  That left you 12 pins.  And so a 2-input NAND gate needs three.  So you'd have four 3-input NAND gates in one of these little 7400 chips.



LEO:  So that was, what, 8K of RAM or 2K or 4K?



STEVE:  Oh, no.  Well, no.  In fact, probably what you're thinking of was the original Intel Dynamic RAM?  That was 1K.



LEO:  1K.



STEVE:  They were, yeah, you needed a bunch of those to get anywhere.



LEO:  But the motherboard had little sockets for these pins, and you'd just drop them in.



STEVE:  Yeah, yeah.



LEO:  But this is solid-state memory, then.  This is NAND memory.



STEVE:  Correct.  Well, no.  Oh, no, no.  What I was talking about was a NAND gate, a Not AND.  So it ANDed the two inputs, and then inverted the output. 



LEO:  So this is, what, a processor?



STEVE:  No, no, no.  It's just...



LEO:  It just does that?  That's all it does?



STEVE:  That's all it did.  That was how low-density...



LEO:  What good is that?  One NAND gate?  That's it?



STEVE:  You do need a lot of them in order to get any work done.  But I've had the picture sitting in my Security Now! pictures archive just because, first of all, the quote is famous Dr. McCoy, where all throughout Star Trek one of the recurring memes was Kirk would tell Bones to go fix some alien or some Horta that was tunneling through rock or something.  And in some cases Bones was unable to revive this entity.  And so he would say, you know, "It's dead, Jim," meaning that's it.  Well, so is my Windows XP machine.



[Clip] BONES:  It's dead, Jim.



LEO:  What?  Your Windows XP had that in it?



STEVE:  Well, no.  Well, probably, actually.  Probably old enough.  But all of our listeners know that I have rather infamously been sticking with Windows XP because it works.



[Clip] BONES: Its brain is gone.



STEVE:  Because it works just fine.  And I don't know when I set it up, but probably in 2003, when it would have happened.  So it's 15 years old.  And I had...



[Clip] BONES: It's medically impossible.



LEO:  Sorry, I keep playing Bones.



STEVE:  I had VPNed into it Sunday evening in order to get something done.



LEO:  To do what?  You were working on that thing still?



STEVE:  It's my main workstation.  It's everything.  SQRL has been written there.  All the podcasts have been produced on it.



LEO:  No.



STEVE:  It's been running for 15 years, yeah.  That's my main workstation.  And Chrome began to complain that it didn't like the version of Windows I was still using, and Firefox began to complain.  So it was becoming a little long in the tooth.  Well, I got back to my office Monday morning, yesterday morning, and went into my little workstation area, and it was quiet.  



LEO:  Uh-oh.



STEVE:  And the machine was off.  And in 15 years it had hung from time to time, so you'd just reset it, and up it would come.  And it was getting kind of finicky where it wouldn't restart immediately.  I had to let it cool off for about 30 minutes if I needed to do a reboot because it thought it was getting overheated.  And so it would say, no, no, no, I can't boot.  So it was giving me plenty of warning.  But every day it just - it was there, and it kept working.  And so I just kept using it.  Until yesterday morning, when I was unable to bring it back to life.  I thought maybe the power supply had died.  So of course I had another system of similar age.  Actually, it's the one that my bitcoins might be on.  I had to dig that machine out from underneath the rubble.



LEO:  Oh, that's good.  So now you'll finally get that back.



STEVE:  I probably, well, I will at least have a chance to figure out whether or not I do in fact - I think what I did was I moved the bitcoin wallet and miner over to that other machine, planning to sort of just like, oh, see what else can happen, since I got 50 bitcoins overnight without trying.  But I don't think I really - I don't know what happened.  I got bored or didn't, I mean, they weren't anything back then.  Anyway, so swapped the power supply, no change.  So I think the motherboard just bit the dust.



So anyway, our listeners will remember when Microsoft had announced that they were not planning on supporting Windows 7 on newer hardware, and that was a couple years ago.  And I said, what?  Okay, wait, stop.  So I immediately purchased the last motherboard family that they would promise to support Windows 7 on because nothing was going to make me go to 8 or 10.  So I built that beautiful system, remember, 128GB of RAM and a state-of-the-art, superfast, direct-on-the-PCI-bus SSD, and a pair of multi-terabyte mirrored hard drives.  I mean, it's a monster.  Oh, and an 8-core processor.



LEO:  This was the last computer you were ever going to build, I think; right?



STEVE:  Yes.  That is.  And it has sat patiently on the sidelines because...



LEO:  We thought you were using that.



STEVE:  There's never a good time.



LEO:  There's been no reason, yeah.



STEVE:  Yeah.  There hasn't been a reason.  And right now, I mean, now I'm facing it.  I have nothing.  I can't edit the audio, I mean, I had to, like, quickly put - normally I have a nice little stopwatch, like a timer, that shows me how far into the podcast we are so that I know it's time to take a break.  I don't remember the name of it.  So anyway, I didn't lose any data.  I have multiple images and snapshots and things, although the very, very most recent stuff is on the RAID on the dead machine.  So in the next day or two I will rehost that RAID on a different motherboard, bring the file system back up.  Oh, it just - the system just blanked on me.  See, for example, that.  So anyway, I'm in the process of - oh, and my development environment had lots of 16-bit code.  Brief, that I've still been using.  I mean, I wrote - SpinRite 1 was written in Brief.  Actually, FlickerFree before it was written in Brief.



LEO:  It's really time.  It really is time.



STEVE:  You know, in 1986 or something.



LEO:  It's just really time.



STEVE:  So, yeah.  And I was explaining to Lorrie last night that, well, you know, there is never a good time.  I have not wanted to have any time off or out or pause because I'm always in a hurry trying to get as much done as I can.  So it's like, okay, well, it works.  So if it's not broke - anyway, so and finally it is broke.  So this picture of the chip on its back with its legs up in the air, I just thought, okay, it's the perfect time...



LEO:  It's appropriate.



STEVE:  ...for the "It's dead, Jim."



LEO:  Do you need anything on that hard drive?



STEVE:  Yeah, everything.



LEO:  Oh.  Good luck recovering it, if it's a RAID.  Did it have a dedicated RAID card?



STEVE:  Of course, yes.  So I just...



LEO:  He's no fool, ladies and gentlemen.



STEVE:  I moved it and its four drives over to another chassis.  And I will then - I'll probably move everything, just camp it on the Drobo and then begin to pull things back from there.



LEO:  Yeah.  But you're going to use a more modern version of Windows; right?



STEVE:  I'm on 7.



LEO:  Good, okay.



STEVE:  Yes, I'm now on 7.  All updated, oh, like an 859MB update because of course it hadn't had any updates for two and a half years.  So it's all current now.  And I'm glad to be here.



LEO:  I think you'll be faster.



STEVE:  Oh, boy, is it fast.  Oh, you know, like I'm just not used to it.  



LEO:  Steve, it's normal.  You've just been on a slow machine.



STEVE:  I used to think, boy, these pages have gotten so big since the Stone Age.  They just take forever to load.  And so I have, like, 300, what is it, megabits, yeah, 300Mb of download from Cox.



LEO:  Oh, that's nice.



STEVE:  And it's great.  Well, but then it hits the brick wall of XP that's got a stack from TCP-1.  And so it says, what?  And it just sort of - it's the large pipe hits the little straw.  Anyway, oh, I'm having a great time.  It's like, whoa, wait.  That's downloaded already?  Oh, okay.



LEO:  Nice.



STEVE:  Yeah.  Anyway, so I announced to the SQRL newsgroup that I was limping along at the moment.  It'll take a few days for me to pull my - and, I mean, I have to rebuild a development environment because my whole tool chain was 16-bit.  Everything, you know...



LEO:  Steve.  Steve.  You're using MASM?  Is there a 32-bit MASM?



STEVE:  Oh, I guess there must be somewhere.



LEO:  Maybe.



STEVE:  Yeah, probably, yeah.



LEO:  Oh, my god.  That's hysterical.



STEVE:  Actually, there's a 64 because I did for - what was it I did?  Oh, for InSpectre I had to...



LEO:  That's right, you wrote 64-bit, yeah.



STEVE:  ...create a little bit of 64-bit code for InSpectre.  So I did my first 64-bit MASM for that.



LEO:  You didn't do that on the XP machine.  You probably did that on the new one; right?



STEVE:  Actually, I did that on my Lenovo laptop.  I did that.  That was a little project that I had in the evening.  So, I mean, like everything else is Windows 7, and I have a bunch of Windows 10 machines because I have to make sure that SQRL works on them properly and everything.  So, I mean, it's not like I'm stuck.  It's just that - and Leo, over 15 years you just, like, there's so much tuning and tweaking and things done to a system.  Like I just keep thinking of things.  Ooh, I don't have that.  Ooh, I don't have that.  Ooh, you know.



LEO:  Oh, yeah, I bet, yeah.



STEVE:  So anyway.



LEO:  And a lot of those things won't run, I'm sure, in a modern machine.



STEVE:  No, no.  In fact, it was Mark Thompson who made the jump before I did.  And he made the comment, he said, "I didn't realize how much 16-bit code I was still using until things I used to do no longer worked any longer."  Like my Gravity newsreader.  I can't run the one I had on 32 bits because it was 16 bits.  And, I mean, I do have the, what was it, there is a WinXP VM that you can run under Windows 7 if you absolutely must run some 16-bit code.  But I'm not doing that.  It's time.  I mean, I agree with everybody it's time.



LEO:  I had no idea, I mean, I guess I kind of knew, but I didn't realize that was your main machine.  I had no idea.



STEVE:  Yeah, that's what I sit in front of every day.  So I had to go out, and I got a bunch of display port cables because all of my monitors were DVI, and the new machine is an 8-head Matrox, two Quad Matrox cards, but they're all display port.  So I cabled up all the existing monitors, and now I'm sitting in front of an array of monitors, but there's nothing on them.



LEO:  Welcome to 2010, Steve.  You're going to love this new decade.



STEVE:  And I've got no icons.  I used to have all these icons.  I just, like, went right off the screen.  And now, like, I've got nothing.  But anyway, yeah.  So anyway, Microsoft's June Mega Patch Tuesday, it's the only thing we could call it.  More than 50 vulnerabilities were patched affecting Windows, IE and Edge, MS Office, Exchange Server, the ChakraCore which is the core of the JavaScript engine in Edge, and even Adobe Flash Player.  Microsoft said, oh, we're going to push that out, too.  Among those 50 vulnerabilities, 11 were critical and 39 were important.  And for a change there were no surprises.  There were no Windows zero-day fixes this month.



What we had hoped would happen, I guess I was talking about it Tuesday.  Maybe I was talking about it the Tuesday before because I think I would have checked.  Maybe it was happening.  And that is, remember that there was a very bad JavaScript flaw which Microsoft dropped the ball on.  It was discovered by Dmitri Kaslov of Telspace Systems, who responsibly disclosed it to Trend Micro's Zero Day Initiative because it was a zero day.  He discovered it being exploited.  That's how he found it.  And it was a problem with the JavaScript engine.



And Trend Micro reported it to Microsoft in January, who let 90 days go by.  They immediately acknowledged the receipt of the news, then let 90 days go by before they worried about it again.  And something, maybe a reminder in their calendar said, you know, Trend Micro's going to go public with this any minute now.  So they said, oh, we were unable to reproduce it without a proof of concept.  And Trend Micro responded with, well, here's the proof of concept we already sent you 90 days ago.  And then Microsoft asked for more time, and Trend said no.  So Microsoft said, okay, well, we'll get this fixed as soon as we can.



So it is now fixed as of last Tuesday, so that's good.  That was a zero day which Microsoft just didn't respond to in time.  Trend did everything right, and I would argue also made the right call because this was already being exploited in the wild.  So it's not like it was unknown.  Dmitri found it in use.  And without pressure on Microsoft, they just clearly aren't demonstrating a great need to fix things.  So this was probably the right thing to do.



They did also fix that Adobe Flash zero-day which we talked about last week.  That was the one where, because browsers are really locking down Flash now, even though Flash refuses to die until 2020, some clever malicious hackers figured out how to get Office to invoke the zero-day in order to get remote code execution capability.  So Microsoft said, okay, we're going to take responsibility for that since it's leveraging our infrastructure.  And they pushed that out, and Adobe had also updated their player.



Okay.  But there are two things that were fixed Tuesday that should give us some pause.  As we know, the worst vulnerabilities are those which are remotely executable, where no action is required from the user, which are fundamentally wormable and exist in a large number of systems.  There are two that were fixed last week.  The first one has the CVE number 2018-8225.  That was a flaw, believe it or not, in the dnsapi.dll, which has affected all versions of Windows from 7 to current.  And I don't know that it didn't affect XP because Microsoft doesn't talk about anything before because it's like, okay, those don't exist anymore.  Maybe it's always been there.



But it turns out - oh, and this includes the Server editions of Windows.  So all versions of Windows that Microsoft is talking about, 7 through 10, there has been a flaw, only just patched last week, in the way Windows was parsing DNS responses, which allowed it to be exploited by sending a specially malformed DNS response that would allow attacker-controlled malicious code to be run in the context of the local system account, which is the highest privileged account in the system, essentially in the kernel.



So what that says is that, until last week, and still for any systems including Windows servers that have not been patched with last week's updates, the receipt of a malicious DNS reply takes the system over.  And as we know, DNS uses UDP, so it's trivially spoofable, and it's nonencrypted.  So this has been hanging out there since Windows 7, at least, and until last week.  So if you haven't updated your Windows systems yet, and hopefully everybody has, don't wait any longer.



And if you have any responsibility for servers, you know, most Windows machines today are behind a router.  The problem is this doesn't even protect them because a bad guy monitoring outgoing DNS queries simply drops a UDP packet on the line heading back in, and that will cut right through the NAT traversal firewall and the Windows firewall to get into the machine.  Servers are - they feel more vulnerable because they're deliberately exposed.  But they're probably not going to care about an unsolicited DNS reply.  They are going to care about one that looks like it's answering a question that they just asked.  And servers are always performing DNS queries to a much greater degree.  I mean, so are end-user machines.



But anyway, this is bad.  So if you're involved in a company not directly responsible for IT, do make sure that the IT people responsible for any Windows servers, any Windows machines, really, that are in the network, got themselves updated.  What will happen is, as we know, now everybody knows that a change was made to the dnsapi.dll to fix this very worrisome problem last week.  The change will be reverse-engineered.  The bad guys will figure out what it was that got changed and how to leverage this.  And, I mean, it's low-hanging fruit to be able to send a DNS query into a system and take it over.  So this is bad.



The other problem is not much less bad.  It's CVE 2018-8231, which is another remote code execution flaw in another publicly exposed network component.  In Windows, some time ago the various pieces got broken up.  There's a kernel driver called http.sys which is the HTTP protocol stack that parses all incoming HTTP stuff.  So many non-servers, non-Windows servers do run a service or a server.  In some cases HTTP gets invoked.  All Windows servers do.



So the good news here is that this problem only affects - well, only - Windows 10 and Windows Server 2016.  So people using earlier versions of Windows, this one doesn't affect them, although any Windows Server 2016 there's another very worrisome, I would argue horrifying, remote code execution flaw in this component of Windows Server that is, again, directly exposed to the Internet.  So I'm speechless.  Last week's security update, which just sort of slipped under the radar, as I do, I went back and just sort of looked over it.  And you normally don't hear me talking about these every Tuesday, or every second Tuesday of the month, because it's like, okay, fine.  So a bunch of things got fixed.  In this case they are worrisome.



And I guess I have to back off on saying it's wormable because at least in the DNS case I'm sure - I'm hoping - that you can't send a blind reply to a machine and have its dnsapi.dll cause a hacker-controlled remote execution.  I'm sort of assuming you need to send a response to an outstanding request.  But if not, then it's worse.  So anyway, yeah, if you're responsible for any Windows machines, really last week's June Patch Tuesday was a biggie.



And everything else, you know, there were seven other critical memory corruption bugs, less horrible than those two.  Both the ChakraCore and the Chakra scripting engine that drives the Core, they both had them.  There were three in the Edge browser.  There was another in Windows Media Foundation.  And all of those could lead to remote code execution.  So it was a busy month, and maybe that explains why Microsoft didn't fix the outstanding zero-day from Trend.  Who knows?



I'm trying to think where they were - oh, it was McAfee.  I mentioned that at the top of the show.  Cedric Cochin in McAfee's Advanced Threat Research team came up with a clever hack.  And we've seen things like this, Leo, with voice assistants before, and I'm sure that there have been some for iOS, where features are added without fully vetting all of the possible ways that they can be abused.  Turns out that in Windows 10 Cortana can be abused.  Until last Tuesday.  This is another of the things that Microsoft locked down.



So this one is not a remote problem because it involves Cortana, but a local one.  It turns out that, if you activate Cortana, and she comes alive, on a locked machine - so the machine is powered on, but locked so that it's not supposed to do anything.  Turns out Cortana is still listening.  So anyone can come along and activate Cortana.  It turns out that you can either phonetically or use the keyboard to start typing.  You can type while Cortana is listening for more verbal command.



And these guys came up with a multistage means, to make a long story short, of running an externally provided power shell script from a USB drive that's plugged into the machine.  That ends up getting registered with a system.  Cortana is able to see it.  And when you ask for a search, its contents are indexed, which allows you to bring up a file which in this case is a power shell script which is the first stage of the attack.  It's then possible to get that to execute code, your own hacker-provided code, from the USB device, and go as far as, for example, completely taking over the locked session, and even changing the system's password on a locked machine.  The blog posting that McAfee put up was titled, "Want to break into a locked Windows 10 device?  Ask Cortana."



So that's yet another thing that last Tuesday's patch round fixed, fortunately.  And they noted that, because they felt that this was - now there's full details published.  It takes no advanced hacking ability in order to abuse this.  So they were suggesting, and I would absolutely concur, if you do not update, for whatever reason you have not updated to last Tuesday's patches that fixed this, by all means disable Cortana when the system is locked.  And I would argue that's just sound security practice.  There are just too many opportunities for this kind of exploit on a system which should just be shut down and not listening to anybody while it's locked.



So I would say, even after having patched the June updates, unless you have a compelling need to have Cortana listening while your system is locked, I think it makes a lot of sense to disable that.  And had you done so already, this would have never been a vulnerability.  I expect there will be some people getting up to some antics with this one because it's so easy to do.  And if anyone is interested, I have a link in the show notes.  They have a super-detailed, blow-by-blow, how do you pull this off explanation in their [crosstalk].



LEO:  You'd have to have physical access to the machine, of course.



STEVE:  Yes.  Yes, yes, yes, you absolutely have to have physical access.  So it's not nearly as worrisome as the remote code execution problems that we talked about first.



And Microsoft also published a written policy which we've never encountered before on when things get patched versus when they feel comfortable deferring them to the next release of whatever it is they're doing.  They called it, yeah, their Microsoft Security Servicing Commitments.  And it's interesting enough that I'll share the beginning of this.



They wrote:  "Our commitment to protecting customers from vulnerabilities in our products, services, and devices includes providing security updates that address these vulnerabilities when they're discovered."  Okay.  We wish that was true.  That's generally true, although sometimes it takes many months.  They said:  "We also want to ensure we are transparent with our customers in our approach.  This document helps to describe the criteria the Microsoft Security Response Center" - we've talked about it often, the MSRC - "uses to determine whether a reported vulnerability will be addressed through servicing, or in the next version of the product.  For vulnerabilities in products, this 'servicing' takes the form of a security update, most commonly released as security updates on Update Tuesday."  That's what they call it, "Update Tuesday." "The purpose of this document is to clarify the commitments as they pertain to Windows."



So they break down their security servicing criteria into two categories.  Well, it's sort of a bit of a tree, sort of a decision tree.  They said:  "The criteria used by Microsoft when evaluating whether or not to provide a security update for a reported vulnerability involves answering two key questions.  First, does the vulnerability violate a promise made by a security boundary or a security feature that Microsoft has committed to defend?  And the second, does the severity of the vulnerability meet the bar for servicing?"



Okay.  So security boundaries are also something that they then rigorously define.  And, roughly, those are the things that we talk about all the time.  For example, you could have a network boundary where you want to protect anything from having access to network traffic; a kernel boundary where application code is a pure client of the kernel and is unable to cross that boundary, unable to penetrate it.  And in fact all of the Spectre and Meltdown problems, those are clearly security boundary violations, which is why the whole industry went crazy at the beginning of the year.



Process boundaries.  Also processes are supposed to be contained.  We can sort of think of these all as sandboxes, where the sandbox boundary is - the whole reason you have a sandbox of one form or another, network or OS or process, is for containment.  Even the logon session boundary, Microsoft specifically says, if you've got two users logged onto the same system, they have a boundary that is dividing them and protecting them, and you have to preserve that commitment.  Web sessions have boundaries, both the browsers' sandbox and also the same-origin policy that rigorously enforces a boundary about where the code came from and not allowing code from one origin to have anything, any visibility into resources from a different origin.  And also finally their VSM, the Virtual Secure Mode is another boundary.



So they explain that all of those things, those security boundaries, receive Microsoft's, as they called it, a "servicing commitment," and warrant a bug bounty.  So first of all, if you find something that is able to cross one of these security commitment promises from Microsoft, they take it seriously because it's a breach of the commitment they've made, and you can get remunerated for providing the information to them.



They then also have security features which they consider to be distinct from boundaries.  And they define them:  "A security feature provides protection against one or more threats.  In some cases, a security feature may make a promise related to the threat they're protecting against, and there are not expected to be any design limitations that prohibit delivering on that promise," meaning it's a feature that's supposed to work under all circumstances.



And then they give some examples.  BitLocker and Secure Boot are not boundaries.  Those are features.  Windows Defender is a platform security feature, and the Application Guard in Defender is an application security feature.  Identity and access control - so like logging on, biometrics, and Windows management over what resources an individual has access to - those, again, are not boundaries.  They consider those security features.  And they also put the Cryptographic API as a feature, along with authentication protocols.  So those are sort of services within Windows which they also treat as, like, take them seriously services.  They call those commitments, and those also receive bug bounties.



The things that don't, sort of the second-class citizens, are what Microsoft calls "Defense-in-Depth" features.  So those are features like UAC, AppLocker, Controlled Folder Access, and even Data Execution Prevention, Address Space Layout Randomization, Kernel Address Space Layout Randomization, DLL signing protection.  Those are things that they consider important, but the breach of those does not cross a boundary.  So some of those warrant, in Microsoft's opinion, bug bounties, and some don't.  And so essentially their position is that, if the defense-in-depth things have a critical or important vulnerability level, they will be fixed in an upcoming Tuesday.



All of the boundary breaches and all of the security features - BitLocker, Windows Defender and so forth - all of those things that also warrant bug bounties, those also, if there's a breach in those, that gets their attention.  That gets fixed as soon as possible.  So essentially where they feel they've made a firm commitment in the architecture of Windows such that something should not be possible, but it's not about more security, it's more about absolute functional security, that gets fixed.



But apparently there is a large class of things that people report that are, well, you know, this isn't working the way it should, or we were able to get around it, but for example it doesn't rate critical or important, then Microsoft is taking the liberty here, and they've now spelled it out explicitly, of, okay, we should fix that, but we're not going to run around worrying about pushing it out immediately.  When we do a next version of whatever it is, we'll probably fix it then.



So it was nice to have a formal enumeration of what their policy was.  And also, I think, people were wondering what is subject to a bug bounty and what is not.  And most of these things are.  Microsoft certainly wants to encourage people to submit problems which are discovered.  So anyway, I was glad to see that they laid it all out in detail.



LEO:  Very useful.  Okay, Steverino.



STEVE:  So Leo, you will be glad to know, I know you will be glad because I heard you talking about this elsewhere, that Microsoft has decided to join the group of browsers that will not be playing audio and video...



LEO:  Well, hallelujah,



STEVE:  ...by default.  Yes.



LEO:  Everybody else does this already, though; right?  They're the last one?



STEVE:  Yes.  Well, Chrome actually was first.  They've been blocking autoplay since Chrome 66.  Mozilla has formally stated their intention of adding it to Firefox, but they're just saying sometime later this year.  So they're definitely going to do it.  Microsoft has stated that Edge would have this, and in fact it was expected to be in the most recent insider build, 17692.  But apparently, in fact, some people thought it was in there and were surprised when it wasn't.  So it just barely missed whatever closing deadline the insider builds have.



However, it will not be enabled by default.  So under Edge Advanced Settings there will soon be "Allow sites to automatically play media."  Users who, like, probably everybody, like who wants that, should just set that to Off.  Microsoft is going to - so they'll give us the option.  I don't know if maybe someday they'll flip it the other way so that it's enabled, that is, well, so that autoplay is disabled by default.  They're taking it slowly.  But at least the option will be there.



So for people who are not using Chrome, who are using Edge and who say, wait a minute, why am I getting these videos playing all the time, soon you'll be able to just turn that off.  And I don't know about IE.  There was no mention of that.  I'm assuming they're just going to leave it alone and sort of use it as one more reason to push people away from IE and over to Chrome.  Over to, well, yeah, Chrome.  Over to Edge.  So anyway, that's good.



Bitdefender Labs has been following for, oh, I think about five years now a very pernicious piece of adware which has, more recently, very sophisticated Windows 10-based rootkit behavior.  We talked about rootkits, boy, a long time ago.  And you'll remember this, Leo, because, I mean, it was a series of famous episodes for us.  There was something from Sony.  Was it a game?  I don't remember what it was.  It was some software which for - oh, I think it was maybe DRM.  It might have been a DRM rootkit.  Essentially a rootkit is something...



LEO:  Oh, yeah, yeah, yeah.  Installed a rootkit, yeah, yeah, yeah.  It was [crosstalk], yeah.



STEVE:  Yeah, I think it was digital rights management was like their justification.  And so what a rootkit does is - it's hard to think...



LEO:  Oh, no, it was on music.



STEVE:  Oh, my god, you're right.



LEO:  It was on CDs.



STEVE:  It was just Sony audio CD.  Oh, wow.



LEO:  BMG, Sony BMG distributed a copy protection scheme with music CDs that secretly installed a rootkit on computers.  Geez Louise.



STEVE:  I know.  Can you believe it?  We've come a long way.  So for those who don't know, who have not been listening forever to the podcast, it's sort of counterintuitive.  But a rootkit is software which hides itself.  And the way it hides itself is by modifying what the operating system shows when you do, like, a directory listing.  And we're used to looking in Windows Explorer, or if you brought up a command prompt and you typed "dir," you're going to see - that's a directory command.  You're going to see the files there.



Well, it's the operating system itself which is reading the file system and putting up in Windows Explorer the tree of files, or enumerating them, listing them on a command window.  So if the operating system itself is subverted, then there can be files there that don't appear.  And you can't unhide them.  You can't show system files, show hidden files.  They don't exist, like from any external view.  The only way you could see them would be to boot like an external OS and have it look at the drive because then the drive essentially wouldn't be able to hide things from the person viewing.



Anyway, so as a consequence, rootkits have always made people very uncomfortable because they're hiding in plain sight.  They're invisible.  And so the way this is done is that, when you're doing a directory listing of something, when you're displaying the file contents, you're making calls to the operating system, asking it to return a list of things.  Well, if the return runs through some malware, if there's something intercepting essentially a man-in-the-middle attack is what this is, a man-in-the-middle attack on the operating system API itself, then it's able to remove from that returning list of files references to anything it doesn't want to have appear.  Things just vanish, and you can't see them.



So anyway, the idea that there is a Windows 10-based adware rootkit which does this is a little unsettling, especially because it's affecting the most recent version of Windows, which is supposed to have very proactive anti-rootkit technology.  The whole Secure Boot system that we've talked about and detailed over several past podcasts is all about maintaining a firm chain of control, essentially a chain of trust, from the moment the motherboard powers up, and the BIOS is signed and protected.  And through a series of modules which are loaded as the operating system boots, each one is verified for its not having been tampered with and having been published by a trusted source.  So the idea that this has been and can be subverted is troubling.



BitDefender Labs has published a very detailed 104-page PDF whitepaper that I'm not going to go into in great detail.  They called this Zacinlo, Z-A-C-I-N-L-O.  Zacinlo is the name of this adware-based rootkit.  They've taken a look at the population of exposure:  89% of the infections are Windows 10; 5.3% are Windows 7; and the rest is made up of 8 and 8.1.  What's also interesting is that, unlike most of these or many of these things that we see, where it's like, okay, well, most of the infections are in Asia or over in Europe somewhere, or maybe in the Middle East, almost all of the infected systems are in the United States.  So this is adware clearly focused upon and targeting U.S. users.



In the beginning of this long whitepaper they say:  "Last year we came across a digitally signed rootkit capable of installing itself on most Windows operating systems, including the newest releases of Windows 10.  Since rootkits," they write, "these days account for under 1% of the malware output we see worldwide, this immediately drew our attention and prompted us to carry out an extensive analysis of the payload, its origins, and its spread.  We discovered an ample operation whose central component is a very sophisticated piece of adware with multiple functions."



And as I said, they have been tracking this for about five years, although it has recently evolved into this Windows 10-capable rootkit technology.  So they've identified 25 different components and found almost 2,500 distinct samples.  So this is making people money somewhere and doing everything it can to stay off of the radar, unlike very loud botnets like Satori that are attacking people and commandeering crypto mining and so forth.  This just wants to operate quietly in the background.  It has a rootkit driver.



And I did, I should mention, I looked through this extensive whitepaper.  And I'm very impressed, unfortunately, with the technology that has been brought to bear.  This thing has received a huge amount of engineering work in order to create something that burrows into people's Windows 10 machines and then hides and makes itself undetectable.  They run through some of the things that it's able to do.



They said:  "The presence of a rootkit driver that protects itself as well as its other components is the main feature.  It can stop processes deemed dangerous to the functionality of its own adware, while also protecting the adware from being stopped or deleted."  And of course that's what a rootkit is about.  It hides itself, and then it makes sure that - and also defends itself.  "The presence of man-in-the-browser capabilities which intercepts and decrypts SSL communications."



So this thing is buried so deeply that it is able to even intercept encrypted HTTPS web pages, which allows its adware component to inject custom JavaScript code into the web pages being visited by the user.  And since the injection can appear to be first-party, we're sort of lucky that this thing is only wanting to be adware and not be more malicious.  I mean, this is a very potent piece of malware.



It features an adware cleanup routine used to remove potential competition in the adware space.  So apparently there are some other companies that they don't like, and they get rid of them as part of their coming in and taking over the system.  They describe that portion as "generic and does not target a particular family or type of adware."  It can uninstall and delete services based on instructions it receives from the command-and-control infrastructure.  So it is also, in addition to adware, it is a botnet with a mature command-and-control infrastructure.  It's able to report some information about the environment it's running in back to central command; whether antimalware solution is installed and, if so, which one; and which applications are running at startup.



So, I mean, it's also spyware, spying on the machines that it has infected.  It takes screen captures of a desktop and sends them to the command-and-control center for analysis.  And BitDefender notes that this functionality has a massive impact on privacy because of course, as we know, screen captures may contain sensitive information - email, instant messaging, or ebanking sessions.  I guess it's one reason for blanking the password while you're typing it in.  I've never, as we've talked about here, I've always felt that not displaying the password while you enter it causes more trouble than it's worth.  But with the one exception being, if something is sitting around capturing, like snapshotting your screen, you'd rather have black dots showing than what it is you're typing.



It says it can accommodate the installation of virtually any piece of software on the fly to extend its functionality.  So it is a full trojan with rootkit technology, and it has an automatic update mechanism.  It's able to redirect pages in browsers.  It's able to intercept and replace advertisements on the fly with those of its own.  It supports many different platforms from which to pull advertising for revenue-generating, including Google AdSense.



So essentially it's creating its own advertising network.  You go to websites that hope to be ad supported.  You assume they are.  But the ads you're seeing are coming from a rootkit in your system generating revenue for these cretins instead.  It's able to rotate and obsolete and expire ads.  It's also able to run a web page in the background in hidden windows and interact with them just as a normal user would - scrolling, clicking, and providing keyboard input.  So what it's doing is it's also perpetrating advertising fraud.  So it's pulling up pages, replacing the ads with its own, and generating clicks in order to simulate the user doing that, even if they're not, all in the background, and all completely invisible.  It uses open source projects and libraries - chromium, cryptopop, jsoncpp, libcef, libcurl, zlib, and so forth - and uses Lua scripts to download several components, most likely as a way to fly under the radar of some antimalware solutions that detect suspicious downloads and block them.  So, I mean, it is extremely configurable, highly modular, and hiding.



The good news is it looks like its point of entry is a bit limited.  From everything that they have seen, the only place they have found this installing itself, although you want to make sure you're not a victim of this, it presents itself as a free and anonymous VPN service called s5Mark.  It is distributed in the installer for this VPN, which actually is just sort of a weak proxy.  It sort of is VPN-esque.  But since that's not its intention at all, it's intended to install itself as a rootkit and set up a little adware ad fraud shop inside your system, the idea that it provides VPN services is just sort of to get itself into your system once.



Anyway, s5Mark.  If that rings a bell, if anyone listening to this podcast has ever thought, oh, I'm just going to use this little free VPN, you need to maybe take some action about seeing if something like this has been installed in your system, and see about getting rid of it.  s5Mark is a simple little GUI that says it establishes a VPN.  And for whatever reason, it's like all in the U.S.  In their 104-page whitepaper they have a complete map of where they have found infections, and it's the continental U.S. and Alaska is all red, and very low infection levels anywhere else.  So just be sure you haven't done anything that would allow this thing to crawl into your machine.



Last week our old friend the Satori IoT botnet went into overdrive.  A whole bunch of researchers who watch Internet activity started tweeting when they suddenly saw port 8000 scanning activity just explode on the Internet.  Well, it turned out that this was the botnet itself scanning for new victims.  Proof-of-concept code was published to exploit a buffer overrun vulnerability in the lightweight web server.  So it's an HTTPD, which is used in many embedded routers and IoT equipment sold by some Chinese vendors.  It's XiongMai, X-I-O-N-G-M-A-I, and it's the uc-httpd which is this little web server built in.  Turns out it opens up port 8000.  It's a web server.  And we've seen, for example, that as know, the default HTTP port is 80.  And sometimes because you have to have root privilege or admin privileges to open up ports below 1024, some web servers will open port 8080 as a means of opening a port up in user space if a user wants to host a web server for whatever purpose.  In this case, they're using 8000 as theirs.



Well, it turns out that sending a malformed packet to this little HTTPD server, which is listening on port 8000, can take it over.  So the people behind the Satori IoT botnet, a few days after the proof-of-concept code went public, had incorporated that into Satori and turned their botnet loose, looking for other victims.  And of course we know that there were the GPON routers, the optical network routers, largely located in Brazil.  They got quickly taken over by Satori.  This spike in port 8000 traffic wasn't long-lasting, apparently because in rather short order they had captured all the available devices that had port 8000 exposed.  They then switched to scanning port 80 and 8080, the standard HTTP ports and the alternative, for instances of the D-Link DSL 2750B devices.  Turns out similarly a proof of concept had been posted.  They incorporated that into their network and went about capturing all of those.



So what we're seeing here with Satori is a botnet which is not going away.  There was an attempt to take it down in December.  It has survived that.  And it is, as I mentioned earlier, unlike this rootkit adware, which is trying to stay under the radar and basically not be seen and just be generating revenue for the people who put a lot of time and effort into its development, Satori is just sort of blatant.  It's scanning like crazy, taking over things as they become available.  What it looks like is that its developers are actively monitoring the so-called "dark web" and hacker forums, looking for any published proof-of-concept code.  It doesn't look like they are interested in developing their own.  But the moment some proof-of-concept code appears, they send it off to their existing botnet and tell all their bots to go exploit it and find everybody else who is vulnerable to this.



So, I mean, if you wanted to have a botnet that was going to stay current and be aggressive and essentially actively work to supplant other botnets, this is the way you do it.  As soon as something appears that offers an opportunity to access another chunk of exposed devices, you have an existing delivery system which is able to exploit that new discovery and jump in and take over before anybody can compete with you.  And that's what we're seeing now on an ongoing basis with Satori.



So I have a feeling we're going to have it for some time to come.  And remember it's also been responsible for some very powerful DDoS attacks.  So it's not staying hidden.  It's hijacking cryptocurrency mining.  It's stealing funds from people and launching attacks at will against targets of their choosing.



And speaking of stealing cryptocurrency, there's something we talked about a couple months ago.  I just wanted to keep it on our listeners' radar.  And that is the malware which watches people's clipboards for the presence of bitcoin addresses, either bitcoin or ethereum.  The guys at 360 Total Security found 300,000 machines infected with something that they call "ClipboardWalletHijacker."  It's malware which takes up residence in a Windows machine and just sits in the background very quietly, polling the user's clipboard.



If it sees something that looks like a bitcoin address, which is a properly sized number beginning with a 1 or a 3, and depending upon actually the date in the month because it uses several different ones, it assumes that's a bitcoin address which a user has temporarily copied from some payment request, put into their clipboard, getting ready to submit it to their wallet.  So on the fly they simply replace that clipboard, that bitcoin or ethereum, with their own.  And so when you then paste it into your wallet in order to make a payment, they get the money instead of the intended recipient.



Now, despite being in 300,000 machines - and again, this is not going to be nearly as lucrative as the adware rootkit.  A rootkit adware system is going to be generating money to the degree it can on every machine that it infects.  This thing is only going to ever generate money if a user is using cryptocurrency, if they're using bitcoin or ethereum, and if while infected they do use the clipboard in order to move a bitcoin or ethereum address from one app to another.



Now, the fact is I don't know how you would do it without using the clipboard.  I mean, any Windows user it's just second nature to copy something on the clipboard.  But as a consequence of knowing, having reverse-engineered these 300,000 instances of this on machines, the 360 Total Security guys know the wallets to which the payments are being made.  So they're able to state that, as of the time of their report, this trojan has successfully hijacked five bitcoin transactions.  So it's not making money hand over fist.  However, the amount of the latest transaction was 0.069 bitcoins, which was about $500.  So it's not nothing, but it's a lot of work to have gone through.



As I said, the problem is, from a standpoint of making money, there has to be a whole bunch of things come into alignment in order for anything to happen.  So it's not generating a huge windfall of revenue for these people.  I thought that I had some other information in my notes about - oh, I'm sorry, yeah.  There was a different address.  There have been 46 successful transactions in total.  And those were ethereum.  And there were also five bitcoin transactions, one of which was $500.  So again, it's not making tons of money.



But I just did want to remind people that maybe the thing to do is double check, once you paste - if you're an active user of cryptocurrency and moving money around, when you copy the address of the payment recipient from the request for payment into your wallet, you can easily look at the first handful of digits and see if they're the same.  Just verify they've not been changed because presumably you'd like to send payment where you intend to.



Google has announced a change of policy about where Chrome extensions can come from.  At the moment, and I imagine that we have seen this as we cruise around the web, third-party websites often offer visitors who are using Chrome the option of installing a Chrome extension to enhance their experience in one way or another, typically on that site.  The Chromium blog from last week was titled "Improving Extension Transparency for Users," which boils down to, uh, we're not going to let that happen any longer.  Essentially, what Chromium has announced, or the Chromium team in their blog have announced, is that they're going to wind down to zero the ability to install Chrome extensions anywhere else.  And it's probably worth quickly running through what they said here.



They said:  "We strive to ensure choice and transparency for all Chrome users as they browse the web. Part of this choice is the ability to use the hundreds of thousands of extensions available in the Chrome Web Store to customize the browsing experience in useful and productivity-boosting ways.  However," they said, "we continue to receive large volumes of complaints from users about unwanted extensions causing their Chrome experience to change unexpectedly" - yeah, things like bitcoin mining in the background - "and the majority of these complaints are attributed to confusing or deceptive uses of inline installation on websites."  They call this "inline installation" when you're somewhere else, and you install an extension from some third-party site into Chrome.



They said:  "As we've attempted to address this problem over the past few years, we've learned that the information displayed alongside extensions in the Chrome Web Store plays a critical role in ensuring that users can make informed decisions about whether to install an extension.  When installed through the Chrome Web Store, extensions are significantly less likely to be uninstalled or cause user complaints, compared to extensions installed through inline installation.  Later this summer, inline installation will be retired on all platforms.  Going forward, users will only be able to install extensions from within the Chrome Web Store, where they can view all information about an extension's functionality prior to installing.  This change will roll out in three phases."



So they wrote, and this was last - I think it was last Tuesday, exactly a week ago.  Oh, yeah, June 12th.  So they said:  "Starting today, inline installation will be unavailable to all newly published extensions."  So game over.  Anything new coming online, it just will not work to install it inline.  Those extensions will have to be registered with the Chrome Web Store.



They said:  "Extensions first published on June 12, 2018 or later that attempt to call the chrome.webstore.install() function will automatically redirect the user to the Chrome Web Store in a new tab to complete the installation.  Starting September 12, 2018" - so, what, July, August, September, so 90 days from then, September 12th - "inline installation will be disabled for existing extensions, and users will be automatically redirected to the Chrome Web Store to complete the installation."



So essentially, publishers of existing extensions have 90 days from the date of this policy publication last week to decide, to know that they're not going to be able to do them inline, and they're going to be redirected to the web store.  And, they said, in early December 2018, the inline install API method will be removed from Chrome 71.  So it goes away altogether.



They finish by saying:  "If you distribute an extension using inline installation, you will need to update 'Install' buttons on your website to link to your extension's Chrome Web Store page prior to the stable release of Chrome 71.  And if you haven't already, be sure to read up on how to create a high-quality store listing, and consider using our 'Install' badge on your site."  So anyway, I think this represents sort of the inevitable evolution of extension management in Chrome.



And actually it's interesting because this ties into something else that I saw that I just thought I'd mention because I thought it was interesting.  Speaking of misleading and deceptive, cheesy developers, or at least developers of cheesy, probably cheesy Android applications, have figured out a clever means of misleading people about the popularity of their applications on the Android store.  They are using developer names which display under their applications' icon of, for example, "more than one million downloads" is the developer's name of some of these applications.  And there's more than one of them.



So the unwitting user who is using the popularity of applications as a signal for whether or not it seems like a good thing, are actively being misled by just this renaming of the developer's name.  Some developers have gone as far as to put that in the icon of the application itself, which again is probably not going to catch out any attentive listeners, probably not people of this podcast, but it's just interesting that here the whole social media side of, well, not social media, but the metadata surrounding how popular applications are as a signal for whether it seems like a good thing is being subverted.



So it does look like we're moving, of course, Microsoft has famously set up the Windows Store.  We've got Google and Android Play Store.  And now Chrome is also moving all of its extensions in-house and asking people to come there.  And I'm sure that that's what the Chromium people were talking about when they talked about the metadata surrounding the extension being useful for helping people decide that this was an extension that made sense for them.



So I wanted to take a minute to tell people where I am with SQRL.  I haven't talked about it for a number of weeks.



LEO:  SQRL.  We missed SQRL.



STEVE:  SQRL.  It remains finished, as I had [crosstalk].



LEO:  Well, that's a good - it would be sad if it became unfinished.



STEVE:  Well, you know, there have been some...



LEO:  Regressions?



STEVE:  Well, somebody will do something that somehow got through all of our testing.  And it's like, oh, okay, good, you know, thank you for reporting that.  And in fact, I mean, as you remember, Leo, it's been running for quite a while as I and the group that I'm working with in the GRC newsgroups have been nailing down every last aspect of it.  And so when I say it's remained finished, that's sort of the way something that's been this large a project happens.  It's not done instantly.  It's, okay, I think it's done.  Whoops, okay, here are a few more problems.  Okay, I've got those resolved.  Now I think it's done again.  Oops.  Oops.  Okay.  Fewer this time, but, you know.



And so it's been done for a while.  So much so that the SQRL public web forums exist and have been online for a while.  We've got, I mean, the groups are populated.  Everything is ready to handle the announcement except that I don't yet have SQRL login on the SQRL forums.  And seems to me that being able to use SQRL to log into its own forums would be a good thing to have.  I of course have a demo server at GRC, which is what we've been using for quite a while.  But it's not the same as to actually log into a real website.



So where I am right now is in the process of bringing up the use of SQRL for logging into the SQRL forums.  And at that point we're ready to release it to the world.  So getting close.  Again, I got a little bit of a hiccup here with my main machine having died on me early yesterday morning.  But I'll get the system put back together, and we'll be off and running again.



And I got an interesting note from someone, a little bit more on the propeller-head side, for SpinRite.  But I know that we have a lot of propeller-enabled listeners of this podcast, so I thought it would be interesting for people because it's not something I've ever talked about before.  This was sent by Brian in Albany, New York.  The subject was "SpinRite reboot after scan completed."  And so he addressed you, Leo, saying "Longtime listener, sometime feedback provider."  Then he said:  "Steve, I'm not sure this will make the June 5th episode or not."  And he sent it on the 5th, so that was two weeks ago.



He said:  "Is there a reboot option in SpinRite?"  He says:  "I have some machines that I would love to run this on for maintenance.  If only there is an option in Settings to reboot after a scan.  This would allow the SpinRite operator to start a scan and walk away."  He says:  "So long as there are no issues, if the machine would reboot after 100% of scan, the SpinRite operator would not have to intercept the machine prior to the user coming back to work at 8:00 a.m."  He said:  "I'm using the ISO made from the EXE.  Is this a possibility in a future release?"  He says:  "I searched the SpinRite manual, and there's no mention of this that I could find.  All the best, Brian in Capitaland, New York."



And the answer is that could be done today.  In fact, that could be done since SpinRite 2, I think.  SpinRite is set up to be easiest to use for the non-techie.  So when you create a bootable ISO or a bootable floppy, for those who still have floppy drives, it boots FreeDOS and uses a feature in the DOS config.sys to execute SpinRite as the DOS shell.  So it just - it runs SpinRite.  And when SpinRite ends, it just stays there so that the typical SpinRite user can browse through the screens, check out the SMART data, look at the final map of what SpinRite did, which often provides a lot of information, and then say, okay, I see how things went, and then terminate SpinRite.



There is, however, a mature command line vocabulary.  And SpinRite can be run from the command line or from a batch file just as easily as from being the Windows shell in config.sys.  So, Brian, and anybody who's interested, if you edit the image of the little DOS file system which is there, it's a little FAT file system, first of all, run SpinRite and do /help, and you'll get a help system which is built into SpinRite.  There's a graphical user interface; a full little explanation of all the verbs.  And there is an autoexit verb which tells SpinRite, don't remain in, but exit once you're finished.



And so it would be very possible to add the autoexit to the command line in SpinRite.  And then what you would do is you would set up the system so that the boot priority was the main drive.  That is, so that it doesn't normally boot from the USB.  It boots from the main drive.  But all BIOSes now allow you to do, typically it's F12, and so a one-time boot override where you say boot from the device I select.  So the idea would be - oh, you also need to get a little reboot command.  There are reboot commands around for DOS.  They've been around since the dawn of time.  And so the idea would be the batch file would run SpinRite with the autoexit verb, and then would reboot.



So you boot the system with the USB dongle plugged in, override the normal boot sequence, which would go to the system's main drive, tell it to boot just this one time from the USB.  That fires up SpinRite.  You get it started, and you walk away.  The system will then, once SpinRite is finished, drop back out, issue the reboot, restart the system, ignore the USB drive, and go directly to the hard drive.  So it's absolutely possible using existing technology.  And it sounds like a good feature for - and what's interesting, too, is it's possible to have the reboot bypassed.  So that would be a nice feature for a future version of SpinRite, sort of a "run once" capability where it would reboot the system and then not accept the boot the second time, but just allow it to bypass and go directly to the hard drive.  So I'll keep that in mind.  And Brian, thanks for the great idea.



LEO:  FPU.  PU.  PU. 



STEVE:  Yeah.  I should have done that.  That would have been good.  I missed that one, Leo.



LEO:  Oh, yes.  You have to have a mind of an eight year old, that's all.



STEVE:  Well, no.  Zippity Do or Don't, that was last week.



LEO:  That was you, yeah, okay.



STEVE:  I could have done FPU.  So, okay.  So first of all,  we had the much-needed second Tuesday of the month update last week.  As of that, we have the availability of mitigations for the SpectreNG, the Spectre Next Generation we talked about a couple weeks ago.  Okay.  So here's where we are in terms of what's available and what's enabled by default and where and when.  Windows 7, 8.1, and 10, so all three platforms.  And you know, in this listing they didn't enumerate x86, that is, 32-bit versus 64.  So I'm not sure about 32.  This certainly is the case for 64, which is where Microsoft's focus is.



But Windows 7, 8.1, and 10 have Meltdown mitigations available and enabled by default.  Those three platforms - 7, 8.1, and 10 - also have both of the first two Spectre variation mitigations available and enabled by default.  But things are different with Spectre Next Generation:  7 and 10 are the only two versions of Windows with Spectre Next Generation mitigation available, but it is disabled in both cases.  And 8.1 does not have Spectre Next Generation mitigation available at all.  So it's not clear whether 8.1 will follow.  I sort of think it's going to.  Maybe 7 and 10 they focused on, just because those are the larger install bases, and then they'll catch up with 8.1.



But even so, even while Spectre Next Generation, that is basically v4, as it's often called, because we have 1, 2, and 3 with Meltdown and the two Spectres, and this is number 4, even though they're available, they're disabled in Windows 7 and 10, the reason being the performance hit, balanced off of the fact that there's no known exploit of them anywhere.  So I think, to be responsible, Microsoft decided, okay, well, we've got to offer the mitigation, but we don't want to slow things down as much as turning this on by default would.  So we'll have it there so that it's ready in case it's needed.



And what is really significant is although they are available for the server variations, that is, 7, 8.1 and 10 have the server versions, Server 2008 R2, Server 2012 R2, and Server 2016, those are the server variants of those desktop platforms.  They all have the mitigations disabled by default.  So again, I think this is Microsoft recognizing that, for many platforms, the cure is worse than the problem.  And it's just not worth slowing systems down for what is even today, here we are, six months downstream - because this was the first thing we talked about at the beginning of the year.  Even now, six months later, this is still a theoretical problem and is not practical.



There is guidance online for anyone who wants to turn this stuff on.  Again, probably the most important ones are on by default.  The v4 is not enabled, the Spectre Next Generation, again because there the performance hit is profound relative to nobody having demonstrated an active exploit of this.  And remember, too, that for your typical user, I mean, the real danger is in a virtualized environment with multiple virtual machines, where one or more might be nefarious, being able to crack the VM boundary.



Again, it's a theoretical problem.  It's never been seen.  And maybe Microsoft has some inside information about how difficult it would actually be to pull off.  But for the typical end-user it's just probably not worth worrying about until there's any sign that this has actually happened.  I mean, we're routinely talking about stuff that's devastating in the wild, you know, ripping across the Internet, flooding the Internet with port probes from botnets, versus oh, darn, maybe there's a chance of information leaking inter-virtual machine in some situations that could cause a problem.  I mean, yes, the industry did what it had to do because this was a theoretical problem.  But here we are half a year later, nothing.



LEO:  Not one exploit; right?  Nothing.



STEVE:  Nothing, yeah.



LEO:  Is there safety in numbers?  I guess not because, if you're a target, it doesn't matter that there's a billion other targets.



STEVE:  Right.  So I would say in a high-risk environment you want these things enabled.  But the typical end-user, you know, there's really what information, I mean, here's the problem.  That suggests that you've got malware on your machine; right?  So if you've got something malicious on your machine, it's game over anyway.  The reason it's interesting for virtualized environments is that bad guys could deliberately be running malicious virtual machines, trying to get across into other virtual machines on the same hardware.



LEO:  That's the issue, yeah.



STEVE:  Yes.



LEO:  If you're running a server, you need to protect yourself, yeah.



STEVE:  Yeah.  And so here on a personal workstation, if you've got something, you know, if you've got something that is in your system, the last thing it's going to do is use Meltdown and Spectre.  It's just going to look around and do whatever it wants to.



LEO:  Maybe that's what killed your XP machine.  Maybe that was the real reason.  It melted down.



STEVE:  Okay.  And here's the other thing is there are some weird bugs, Leo, some weird side effects with these that have been introduced...



LEO:  With fixes, yeah.



STEVE:  Yes, with the Spectre and Meltdown patches.  Get this.  Some non-English Windows platforms may display the following string in English instead of the localized language.  Your non-English Windows may say, "Reading scheduled jobs from file is not supported in this language mode."  What?



LEO:  Okay.  Talk about side effects.



STEVE:  Exactly.  It says this error appears when you try to read the scheduled jobs you've created and Device Guard is enabled.  Doesn't come out.  And they don't know why.  Doesn't come out in the foreign language.  Just that one sentence comes out in English.  Oh.  And then of course this makes them a little nervous, too, because they're thinking, well, what else is broken that we don't know about?  Also, when Device Guard is enabled, some, again, non-English platforms may display the following strings in English instead of the localized language.  So suddenly it'll say "Cannot use ampersand or dot" - you know, period - "operators to invoke a module scope command across language boundaries."  That just comes out in English because, you know, why not?



LEO:  Why not?  No one knows what it means anyway, so who cares?  Who cares what language it's in?



STEVE:  Exactly.  Exactly.  Or "Script resource from PS desired state configuration module is not supported when Device Guard is enabled.  Please use script resource published by PSDSC resources module from the power shell gallery."  So some bizarre power shell thing doesn't work with Device Guard.  So again, it's like, okay, that just sort of, I mean, like, what?  So anyway, yeah.



Also, some users, a little more on point, some users running Windows 10, the latest version, 1803, may receive an error:  "An invalid argument was supplied when accessing files or running programs from a shared folder using SMBv1 protocol," which as we know has been long deprecated, but still generally enabled unless recently disabled.



Also, bizarrely enough, a stop error, meaning a blue screen, right, occurs on computers that don't support the streaming single instruction multiple data, the SIMD Extensions 2, the SSE2.  Boom.  Blue screen.  Whoops.  That's not good.  And there's an issue with Windows and third-party software that's related to a missing file.  And there's like an OEM and some number dot inf that causes the network interface controller, your LAN, essentially, controller to stop working.  So you just go off the 'Net.  Or, I mean, like completely offline.  It's like, okay, that's not good.



So these things make Microsoft a little uncomfortable.  And so I think they're thinking, okay, we can't turn these on on servers.  Servers can't go offline all of a sudden.  And so there's some stuff they haven't figured out yet and haven't fixed.  Which seem like bizarre side effects from, like, turning off a performance optimization feature.  But there it is.



LEO:  Hmm.



STEVE:  Okay.  And, finally, so that's where we are with Meltdown and Spectre.  Basically everybody's been patched now.  The important things are turned on.  The more expensive things are turned off.  And still six months later we've never seen this ever, ever used.  And arguably there's zero real, like, obvious danger for any end-user.  The only way to be infected is if something is running in your computer.  There was some concern that maybe a malicious web page could do this.  But web pages do not, you know, the timing information has already been fuzzed by other mitigations against other attacks.  So web pages don't have the resolution - the code, the JavaScript running on a web page isn't a means into your system.  You've got to have native code running on your machine.  And if that's the case, you've got malware, buddy, so you've got bigger problems than something maybe trying to breach one of your boundaries.



Okay.  Lazy Floating Point State Restore.  I have to say I wasn't surprised when I saw this.  It's like, okay, yeah, makes sense.  In all of our multitasking systems, meaning all modern computers, it used to be a big deal that you could be doing two things at once.  I'm unable to do more than one thing at once, but that's another...



LEO:  But if you switch really fast, it'll feel like multitasking.



STEVE:  Exactly.  So there's this sort of an abstraction that we've talked about on the podcast often known as a "thread," a threaded execution, which is where the program counter in the processor is as it steps along and executes instructions.  And it reads things from memory, and it writes them back, and it moves data around the registers.  At some point it's time for somebody else to get a chance to do something.  Like if you've got multiple things going on, you've got your PDF reader is open, and your browser's open, and your clock is ticking, I mean, there's a lot of stuff going on in your system.



What's actually happening is the processor is being yanked around between different things to do, with varying scheduling algorithms and so forth.  And if you have more cores, then you've got more actual execution threads.  But if you even had a single core - before we had multiple cores we just had one processor.  We still had multitasking.  It's because that one processor was very busy jumping all over the place, working on something, and then having what's called a "context switch."  A context switch is the entire execution state of that thread is saved somewhere, and the execution state of another thread which had been suspended is restored.



So what that actually means is the value of all the registers is written to memory so that they could be read from memory and restored when it's time for that thread to start up again.  Now, context switching is an expensive thing to do because our modern processors have a lot more registers.  There are larger registers.  There are more registers.  There's also what we were talking about before, the SIMD bank.  There's XMM and EMM and just like, I mean, if you look at the context of a state-of-the-art processor, there is just a ton of stuff.  And the problem is every time, since you only have some number of cores, and there are many more things that have to be done than the number of cores.  So you have to put those registers, all of the registers, away somewhere, and you need to bring them back.



Well, what Intel in their somewhat less than, it turns out, infinite wisdom decided was, huh.  If a thread doesn't use any floating point instructions, then we don't need to restore the context from what it was the last time we switched away from that thread.  Now, they had to make, again, sort of a value judgment.  You can't do that with the main registers because those are actively used all the time by everything.  But, you know, floating points, kind of off to the side a little bit these days.  Maybe, I mean, they must have done an analysis and saw that a substantial number of threads were never touching the floating point unit while they had processor, while they were in context.  They just didn't ever get around to doing floating point.  Maybe they would half an hour later, or 10 minutes later.  But not this time.



And so the engineers said let's allow lazy floating point state restore.  Because it's expensive, because we've got to, like, read all of this from main memory through the caches, flushing the cache and waiting for memory, because it's an expensive thing to do, for threads that don't touch the floating point registers, let's not bother.  Let's be lazy about it.



And so there's two terms.  There's "eager restore" and "lazy restore."  And what Intel did was they created a bit in the CPU for whether the system would be supporting lazy floating point state restore.  And you know where this is going, if you've been following along this year.  Because it takes time to restore the floating point state, it's possible for an attacker thread to learn information about the previous state of other threads or processes in the system by leveraging whether the floating point state was restored or not.  And this has been leveraged by some clever security researchers in Germany to exactly this end.  This affects all of the Intel core-based microprocessors, none of the AMD, none of the ARM architectures, none other than Intel.  Also, even recent versions of Linux from kernel v4.9 on, and 4.9 came out in 2016, so for the last two years Linux has not been affected by this.



And I don't know what the history is.  I didn't have a chance to dig in and decide whether they just decided it wasn't worth the complexity to support it.  But that kind of would be my feeling.  They thought, eh, how much time is this really saving?  Although older versions of Linux would be at risk if this ever became a thing.  I mean, again, here we are just having said that Spectre and Meltdown really, like, okay, well, we're ready for them if they're a problem, but so far we haven't ever seen it.  Recent versions of Windows, including Windows Server 2016, the latest editions of OpenBSD and DragonFly BSD are also not affected.  And again, I don't know why Server 2016, what it was that got it out of there because Microsoft has published a security advisory  offering guidance for this lazy floating point state restore vulnerability, explaining that they are already working on security updates to be released in July's Patch Tuesday.



So three weeks from now, basically lazy FP state restore goes away.  It was sort of nice while Intel offered it.  Probably didn't save that much time.  Looks like other operating systems had already abandoned it.  These German researchers figured out there was a way to abuse it in the same way that speculative execution can be abused.  Basically, any of these things that are optimizing for performance that leave some state information behind can be used to leak information across thread and process and security boundaries.  And so as an industry we're backing off of those things.



LEO:  OpenBSD announced that they're disabling hyperthreading by default.



STEVE:  Interesting.  Oh, that's interesting.



LEO:  Yeah.  Let's see.  Simultaneous multithreading implementations - this is from the OpenBSD CVS list.  "Since simultaneous multithreading implementations typically share TLBs and L1 caches between threads, this can make cache timing attacks a lot easier.  We strongly suspect this will make several Spectre-class bugs exploitable," especially on Intel's SMT implementation, which they call "hyperthreading."  We really should not run different security domains on different processor threads on the same core.



STEVE:  Yeah.



LEO:  So basically, "Since we suspect there are serious risks, we disabled them by default."



STEVE:  And, you know, hyperthreading, it was something Intel did back in the mono core days.



LEO:  Right, multithreads on a single core.



STEVE:  Yeah, exactly.  And it gave, I don't know, was it like 10%?  It wasn't like, oh, I got a second processor.  No.



LEO:  No, because there's a lot of overhead in the switching.



STEVE:  Yeah, you've got a little 10% kind of more processor.  It just never really offered a huge amount of really that much leverage.  Certainly not enough to justify the potential danger of two separate threads executing in different processes and sharing any sort of state.  So that sounds like a good move from BSD.  So anyway, that's really all we know at this point.  Intel has said nothing more than that this problem exists.  Red Hat has an advisory published.  We're going to get patches in three weeks.  I doubt anybody is going to notice any performance hit because it looks like a bunch of OSes have long since backed off from it and just decided it's not worth having one more thing that's exploitable.



So anyway, we're continuing to see instances where this kind of leveraging of performance that breaks our trust boundaries can be abused, and we're having to give it up.  So we'll have to find more - actually, I think if the super fast nonvolatile RAM technology, the XPoint stuff...



LEO:  Optane, yeah.



STEVE:  If Optane happens, I bet you we're going to see some real acceleration there because our processors are running at 3GB.  They're not getting 3GB of data.  They're starved.  They're sitting around saying, okay, you know, can anybody do anything right now, or are we all waiting for memory?



LEO:  Optane might be the next hyperthreading, though.  You never know.



STEVE:  That's true.  I was sorry also to see that it doesn't have infinite writability.  I mean, it fatigues also over time.



LEO:  Oh, interesting.



STEVE:  Yeah.  And so there's, like, now they're talking about fatiguing in terms of how many times you can write the size of the whole storage per day and for how long it lasts.  So if you had a 2TB memory, then it might be like you could write 20TB per day for some length of time before it dies.  I think it's very robust, but it's not infinite.  So that's one place that hard drives really seem to be out there is that the technology is inherently reversible.  All the other things we've come up with so far are, eh, mostly reversible, but not quite.  There's a little bit of fatigue that happens.



LEO:  It's interesting because this could really in the long run just end Intel.  Apple's talking about using ARM chips.  Microsoft, I didn't realize this, but Microsoft Research has its own processor design they call E2.  And they have successfully ported Windows 10 and Linux to E2.  So "E2 is a radical departure from the computer chips in use today.  It uses an instruction set architecture known as Explicit Data Graph Execution, or EDGE."  I have no idea.  I have no idea.



STEVE:  Cool.



LEO:  Yeah.  Yeah, I mean, there's no E2 processor.  They're all running on FPGAs.  But still I think it's no accident these companies are looking at alternatives to Intel.



STEVE:  Yeah.



LEO:  I would be, too.



STEVE:  Well, Intel had a good run.



LEO:  They had a good run.  They had a good run, yup.



STEVE:  Yup.  They had their heyday.  And it was inevitable that no, I mean, look at OSes.  So did Windows.  And now there's been a lot of shred.  Yup.



LEO:  Yup.  You know, that's the good thing about being in this business is when you're covering technology, you're never bored.



STEVE:  No.



LEO:  Your computer might die.



[Clip] BONES: It's dead, Jim.



LEO:  But you're never bored.



STEVE:  Were we to rename this podcast, "Never Bored" would be good.



LEO:  Never bored.  Stimulating security news every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, right here at TWiT.tv/live.  Join us in the chatroom if you do that, irc.twit.tv.  And of course you can get on-demand versions of the show from Steve's site, GRC.com.  He has not only audio, but also transcriptions.  A few days after the fact you can read the transcriptions from all 668 shows.  It's an easy way to search, too.



While you're there, check out SpinRite, the world's best hard drive recovery and maintenance utility, and all the other projects Steve's got his fingers in.  But SpinRite's his bread and butter, so that's the one we want to make sure everybody knows about.



STEVE:  Yes, please.



LEO:  Yes, please.  Go to TWiT.tv/sn.  We have audio and video there.  Actually, really the best thing to do is find your favorite podcast app, there are lots of them now, and subscribe.  That way you'll get Security Now! the minute it's available, in a hot second.  We're going to push this out.  We're going to edit it up, push it out, usually by the end of the day Tuesday.



STEVE:  Yeah.



LEO:  Thank you, Steve.



STEVE:  Thank you, my friend.  Next week who knows what will have happened.  But we'll be here to cover it.



LEO:  Never boring.  See you then.



STEVE:  Thank you, my friend.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#669

DATE:		June 26, 2018

TITLE:		Cellular Location Privacy

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-669.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine some new side-channel worries and vulnerabilities.  Did Mandiant "hack back" on China?  More trouble with browsers, the big Google Firebase mess, sharing a bit of my dead system resurrection, and a look at the recent Supreme Court decision addressing cellular location privacy.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There's lots, well, there's actually not that much to talk about.  Steve said it's a light day.  I think we'll find a few things, though, on the agenda.  A Picture of the Week that involves magnets.  A lot more about side-channel attacks.  Even WebAssembly is vulnerable, believe it or not.  And the scoop about the scandal at FireEye's Mandiant Group.  It's all coming up next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 669, recorded Tuesday, June 26th, 2018:  Cellular Location Privacy.



It's time for Security Now!, the show where we cover your privacy and security online with this guy right here, Mr. Steve Gibson of the GRC Corporation.  Good afternoon, Steve.



STEVE GIBSON:  Good afternoon, Leo.  Good to see you again.



LEO:  Nice to see you.



STEVE:  I decided to name this podcast in honor of a recent Supreme Court decision about cellular location privacy.  I read the decision.  The good news is we'll summarize it.  But I'm going to spend some time with it because what I love about Supreme Court decisions is the clarity of the writing, I mean, and how exact it is, and how careful they are about what they say.  I mean, that's sort of what I try to do in a whole different realm on this podcast, as we know.



And this is the result of a challenge from, I think it was 2011 is when, you know, because these things all take a long time to roll through the courts and then to get appealed and then the appellate court says, no, we think that was probably the right decision, and then they go, oh, we're taking this to the Supreme Court.  Well, this is about the bar required by law enforcement to obtain the so-called business records of cellular service providers, which in this case can be used to locate people.  And anyway, so the case is interesting.  But the exact way the result was phrased I think is really interesting.  So that's sort of our main topic.



And for whatever reason there wasn't a huge amount of stuff this week.  But that's good because a number of the topics that we want to discuss are bigger.  So there's a surprising amount, actually, of breadth and depth to new side-channel attack worries and vulnerabilities.  There's an interesting question about whether the security firm Mandiant, which was, after they did what they're alleged to have done, purchased by FireEye, whether they actively hacked back some Chinese hackers and showed that to a very respected journalist, David Sanger, who explained all this in his recently released book, which has caused a bunch of furor because it's like, whoops, that's illegal.



Anyway, we also have more trouble with browsers, a big mess with a company named Firebase that Google purchased, I think it was in 2014, about four years ago.  They offer a bunch of cloud connectivity backend stuff which makes it really easy to add cloudness to apps, largely Android, but also some iOS.  Unfortunately, somehow, nobody was worried even recently about security.  And so a startling number of apps that affects well over half of industries due to their prevalence have completely insecure connections to the cloud.  And this company whose name I can't - anyway, it's in the notes - did a complete analysis and report that we need to talk about just because it would be good to know if anyone is using any of these.



Also I had an interesting adventure.  You know that here I am today, Tuesday, a week and a day after having come to my work area and found my trusty beloved Windows XP machine dead, unrevivable, unresuscitatable, just gone.  Anyway, I decided I had a need to get to reboot it, essentially, because I wanted to move some licenses off of it that you can only do from the running instance.  You can't do it from the file system.  The hard drive was fine.  It was a big four-drive RAID and all that.



So I solved the problem, and I thought our listeners would find it interesting, just as an interesting forensics experiment.  And, boy, I also came away very impressed with how far the virtualization industry has come since I last had an occasion to look at it.  So, and we have a couple little bits of miscellany stuff.  And we'll talk about what the Supreme Court thinks.  So I think another great podcast.  And a fun Picture of the Week.



Now, it's funny, when you mentioned it, I was choosing among them, and I had it in my head that it was different than this one.  And I agree with you, this one looks a little bit made up.  But still kind of - it's definitely...



LEO:  Not a few of yours look like they might have been staged.  But, you know, people have done worse; right?



STEVE:  Yeah.



LEO:  Should we save it?



STEVE:  Well, we can just tell people now.  It's just great.  It shows a 3.5" floppy that's labeled with a nice Sharpie marker:  "System restore disk.  Do not erase."  So this was somebody who put all of their system restore information there, whatever that is, and wanted to make sure that nobody would erase it.  It's shown stuck to a refrigerator with a big bar magnet.



LEO:  Save this.  Keep it here.



STEVE:  That's right.  Do not erase.



LEO:  Do not erase.



STEVE:  It's called "bulk erasing," for any of you who do not know, yes.



LEO:  Or degaussing.  We used to call it degaussing. 



STEVE:  Degaussing, yeah, right.  Remember when, exactly, our old-school televisions would have kind of like a green tinge on sort of one area of the screen.  And later they got automatic degaussing.  But for a while there was actually a coil you could buy, and you would move this hoop around the screen and then very slowly pull it away in order to leave it with no net magnetic field.



LEO:  I wouldn't be surprised if young people today look at the way we lived 20 years ago and think, "They were savages.  Just savages."



STEVE:  Yes.  And it required three people to lift the CRT tube.  



LEO:  Yeah.



STEVE:  Remember those huge - the Sony XBRs?  I mean, they were doing big screen the old-fashioned way.



LEO:  But it wasn't that big.  A big screen TV in those days was 21 or 27 inches.  But it was at least that deep.



STEVE:  Oh, you needed to cut a hole in the wall and protrude into the next room, yeah.  Ah, those were the days.



LEO:  Ah, the days.  Those were the days, my friend.



STEVE:  So I stumbled into this because the idea or the issue caught my eye, and that was that the OpenBSD project announced they were suspending support for hyperthreading due to a forthcoming emerging concern that they were rather disgruntled about not being told of.



Okay.  So backing up a little bit, OpenBSD is the branch of the Unix project which prioritizes security above all else.  There's NetBSD, FreeBSD, OpenBSD, YourBSD, MyBSD, OurBSD.  Anyway, OpenBSD security.  And the reputation is well deserved.  I think, I don't remember now what the stat is, but something like since 1996 they've only had two remote code execution vulnerabilities, whereas everybody is drowning in them.  So focusing on security I think pays off.



Okay.  So Theo de Raadt, and that's not R-A-T, it's R-A-A-D-T, he's German, Theo de Raadt...



LEO:  Dutch.



STEVE:  Dutch, okay.  Of course, Dutch, right.



LEO:  And he's just kind of a legend, frankly.



STEVE:  Oh, and it's funny, too, because, yes, I saw his headshot, and I thought, wow, I recognize him.  I mean, it's like he only ever took one picture, and it's still...



LEO:  It's the same one.  It's like Linus.  Kind of like Linus, yeah, yeah.



STEVE:  Yeah.  It was exactly the one I remembered of him.  It's like, oh, wow.  He's not aging, either.  



LEO:  Yeah.



STEVE:  Although you wouldn't know that from how grumbly he is about this.  So he heads the OpenBSD project.  And in their coverage of it last Thursday, he told IT Wire...



LEO:  Oh, I'm sorry.  He's South African.



STEVE:  Really.



LEO:  It's a Boer name, and he lives in Canada now.  I'm just looking at his bio.



STEVE:  Oh, okay, good.  Well, Theo de Raadt.  Do you say de Raadt?



LEO:  I say de Raadt, but I don't know.  Who knows.



STEVE:  Yeah.  We don't want to call him "The Rat" because...



LEO:  Theo the Rat.



STEVE:  Anyway, so he was interviewed by IT Wire, who said that he'd spent about a month trying to discern the problems that he said, quote, "Intel would not disclose to us."  So here he is - and actually, I mean, he's the perfect guy to run the project because I did a little bit of, I mean, I've known of him, and he really is, I mean, he's just a security hardliner.  So what he explained is that a paper is coming out which will be presented at the forthcoming Black Hat USA 2018 Conference, this coming August, which he got access to.  So I thought, oh, that's interesting.  What's that?



And so I tracked it down in the Briefings Schedule.  It's a paper titled "TLBleed:  When Protecting Your CPU Caches Is Not Enough."  Well, and I smiled when I saw that it was by an old friend of the podcast, Ben Gras.  Ben and I corresponded, and I shared this with the podcast, a couple years ago.  These are the guys who, with their Professor Herbert Bos at VU Amsterdam did the Flip Feng Shui, which was the weaponization of RowHammer.  And back then we covered this on podcast 577 in 2016, and he wrote to us.



He said:  "As one of the authors, thank you for your knowledgeable and detailed exposition.  We're honored by it and your kind words."  Oh, because I called it a sublime righteous hack, I mean, it was really - they really did some clever stuff.  And so I said, oh, thanks, and wow.  And then he said: "Thank you again for your Flip Feng Shui coverage.  Because of more recent coverage, I'm a bit of an expert on how well our work is understood.  I don't hesitate to rate your coverage at a 95th percentile rating for expertise and quality of exposition."



So we've established a relationship.  And I imagine that, before long, August is month after next, we'll probably be hearing from Ben when he has something to talk about because he's kept in touch since, and we've covered a bunch of their work since then.  So it's apparent that he and his team, presumably still led by this Professor Herbert Bos, who we wanted to make sure in our earlier correspondence I always gave credit to, have been up to more mischief.



The abstract for the Black Hat paper reads:  "We present TLBleed."  And it's no coincidence that the first three letters are TLB.  He says:  "...a novel side-channel attack that leaks information out of Translation Lookaside Buffers.  TLBleed shows a reliable side channel without relying on the CPU data or instruction caches," which puts it in a different category similar to Spectre and Meltdown, but importantly different because this isn't microcode fixable.



He says:  "This therefore bypasses several proposed CPU cache side-channel protections, such as page coloring, CAT, TSX, and so forth.  Our TLBleed exploit successfully leaks a 256-bit EdDSA key from cryptographic signing code, which would be safe from cache attacks with cache isolation turned on, but would no longer be safe with TLBleed."



So basically they've got like a next-generation side-channel attack that, once again, leverages cross-thread state leakage in contemporary CPUs.  He says:  "We achieve a 98% success rate after just a single observation of signing operation on a co-resident hyperthread and just 17 seconds of analysis time.  Further, we show how another exploit based on TLBleed can leak bits from a side-channel-resistant RSA implementation."  So, I mean, this is cutting through a lot of exploit mitigation.



He says:  "We use novel machine-learning techniques to achieve this level of performance.  These techniques will likely improve the quality of future side-channel attacks," suggesting that this is also an advance in technology.  "This talk contains details about the architecture and complex behavior of modern, multilevel TLBs [Translation Lookaside Buffers] on several modern Intel microarchitectures that is undocumented and will be publicly presented for the first time."



Okay.  So that's what had Theo's nose out of joint is he found out this was happening.  He tried to get information from Intel, and Intel was not being forthcoming.  So Theo said, in his interview with iTWire, said he could not be more specific about the nature of the vulnerability that had led to the disabling of hyperthreading because the paper has not yet been published.  So he's under NDA to have access to the paper.  And Ben has done the same thing with me several times when he's wanted me to see what he was doing, but I couldn't talk about it until it was released publicly, of course.



So as a consequence, though, well, and fact is, I mean, knowing the technology, knowing what the abstract says that's public, knowing what OpenBSD did, there's really not much left to know except some of the details.  Theo said of the paper:  "It was shared with us, allowing us to fix it.  The solution is trivial and temporary."  He says:  "Maybe Intel tells us a different way.  I'm waiting...  I'm waiting...  I better not keep waiting," he said.  And then he gets grumbly, saying:  "Only Tier 1 companies received advance information, and that is not responsible disclosure.  It is selective disclosure.  Everyone below Tier 1 has just gotten screwed," he told iTWire.



Okay.  So what we know from the many conversations we've had about these various speculative attacks is that when different interests share the same hardware, the optimizations which have been employed for years to speed up the performance of our processors, when those optimizations leave a residual state that has changed the future of what the processor will do - and after all, that's their whole point is to change the future.  They want to speed up the future.  When that happens, it's possible for an adversary running on the same system to obtain information.  We understand that.



Okay.  So hyperthreading, as I mentioned, I was talking just I guess it was last week or the week before about thread context switching and how when you're executing code you have - oh, yeah, it was in the context of the multimedia buffers, the MMX and so forth registers.  I remember, in fact it was last week's topic, it was the Lazy Floating Point Restore, where I was explaining about threads and context switching and where, in order to switch to executing a different, like at a different location in the system, you have to store away all the registers that have been in use and then reload them all from the thread of execution that had been suspended, and then let it go.



And the idea is by - what Intel did that we talked about was probing to see whether a thread actually attempted to do any floating point operations because restoring those registers - there's a lot of them, and they're big - that's an expensive thing to do.  So they said, hey, we can save some time, which is the whole point.



Okay, well, another thing that Intel did years ago that I referred to just briefly before I knew about this this week, is hyperthreading, which was - it was a low performance gain, but it was inexpensive for them.  I think I recall it was like 8% maybe, which is not nothing, but still it wasn't - the idea was it would allow you to - this is before multicore, so you just had one core, and you were happy to have one.  They were looking for more performance.  And so they realized that they could, with minimal additional hardware, have like a shadow set of registers, that is, have two full sets of registers and, with just flipping a single bit, switch register contexts.  And what that would so is it would give you what they called "hyperthreading."  You didn't actually - you were sharing everything else.  But it saved the time-consuming register save and restore.



Now, the problem with that is that a modern operating system doesn't just have two threads.  It'd be wonderful if you only had two threads ever.  Then you could just ping-pong back and forth between them really fast.  Typical operating systems are now running thousands of threads.  So the idea that you can bounce between a pair, well, okay, it's become less useful as we've become much more aggressive with threading.



And so what's happened is, rather than for example just extending that to more hyperthreads per core, they actually did more cores.  But because it was there, because it's been there forever, because operating systems know how to turn it on, that is, the hyperthreading - and in some old BIOSes you see that option, Enable Hyperthreading.  It's like, oh, okay.  Well, yeah.  There has been, until now, no reason not to have it because - so Intel just sort of left it in there, even though its value has been diminishing as the opportunity to switch between one additional context when you actually have thousands of them in the system, that's not that useful.



So what this means is that what Ben and his group discovered was a high reliability way for the other thread of the pair of a hyperthread pair to obtain information about its mate, essentially.  So the immediate reaction of the security-conscious OpenBSD folks was just to say, okay, fine, we're turning off hyperthreading.  Again, it'll be a little bit of a performance hit.  Probably not a lot.  And they're just doing it preemptively.



Intel has apparently responded to all of this, saying, you know, this is really not that big a deal, folks.  Intel's position is that the existing mitigations surrounding cache hit and cache miss and cache eviction will be enough to solve this problem.  I don't see how that's the case.  I don't see how the microcode updates that we've received for Spectre apply to this because what the Translation Lookaside Buffers are, and I've also talked about my amazement with that portion of Intel's architecture over the years, they're the things that enable virtual memory.  Essentially, the TLBs are the hardware implementation for virtual memory.  Virtual memory creates an abstraction between the memory any of our applications, any of our processes running on an operating system see, with the actual memory, the actual physical memory in the system.



So, for example, every process can think that it loads at location 10,000.  Normally there is like a base processor load point.  Every process thinks it's running starting at location 10,000, when in fact they're scattered arbitrarily through physical memory.  And it's a series, it's a hierarchy of translation tables which maps or converts the addresses that the process sees to the actual physical memory that the processor reads and writes to and from.  And in Intel systems there's like four layers of these TLBs, Translation Lookaside Buffers, in order to support context switching and process switching and all of the virtual memory operations that our modern systems perform for us without us being aware of it.



So this is a form of caching, and you can't turn it off.  I mean, there's just - our systems wouldn't even, I mean, I've always been amazed that they run at all through this level of hardware abstraction.  But somehow they manage to.  Pages are allocated typically in 4K blocks, but they can also be, I think it's 2MB and one - couldn't be 1GB.  Anyway, you're able to change the page size.  Normally 4K is what all of our modern processors use, and operating systems.



So what's clearly happening is that, because a thread executing and asking for data forces these Translation Lookaside Buffers which implement the virtualization of memory, that forces them to be loaded, which forces the eviction, because they're limited in size, of what was loaded before.  Certainly it would be immediately clear to a hardware architect, as it was to Ben and his group, that here is another way in which two threads sharing the same core are able to leak information.



What will probably happen - so I guess my first reaction to Intel's denial of this being yet another problem, I mean, they must be getting tired of this over there, is I don't see how you work around this.  You need to do something - it'll be really interesting to see, when we have the information from the Black Hat presentation, what these guys are able to suggest as a means for solving this problem.  The only thing that comes to mind is that you change what's known as the "thread scheduler," the process scheduler and the thread scheduler, in operating systems so that you are never having different processes sharing hyperthreads.  And that requires restructuring and rearchitecting our systems to some degree, but I don't see it as a huge problem.



Most processes are already multithreaded.  So they're already asking for multiple threads of execution for their own work.  So all you have to do is make sure that both threads of the hyperthread are being used by the same process, rather than by different processes.  And if you do that, then you prevent cross-process information leakage via Translation Lookaside Buffers.  And I can't think of any other way to solve the problem than that.  Again, it's not a patch, it's some restructuring.  And it would be a shame, too, because this SMT, Simultaneous Multi Threading, is the feature that Intel offers with hyperthreading.  And, I mean, it's a benefit to have it in today's architectures.  So not having it would be a loss.  But it's not at all clear how we get around that problem.



So we'll stay tuned.  I'm sure that we will be hearing from Ben and his group and probably come back and take a look at this after Black Hat and after the information has been made fully public.  In the meantime, OpenBSD doing what they do, they just said, okay, we're turning that off.  We're just going to - I mean, and again, not a huge performance hit.  And I like it that these guys are prioritizing security first.  I mean, that's their deal.  So yet one more way.  I mean, it's really looking like everything which has been done historically to give us more performance from clever engineering is now, as they say, these chickens are coming home to roost.  It's like, well, we've been sort of happily, naively skipping along, enjoying the performance that we have.  We're going to have to now, in today's climate, scale this back and get some more control.



And I think ultimately it's going to mean that the microcode provides more hooks for next-generation operating systems that can arrange to prevent these sorts of problems robustly.  And probably it means that, in the case of hyperthreading, that the multiple threads offered by a single core will need to be kept within a single process.  But it'll be interesting to see if these guys come up with a different solution.



Okay.  So second question.  Did FireEye's Mandiant Group hack back at China?  What happened is, David Sanger, who is a well-respected journalist, he's with The New York Times, he's their national security journalist, and he's a Professor of National Security Policy at Harvard's Kennedy School of Government.  I've seen endless interviews of him through the years.  I mean, he's the real deal.  So when he writes a book, as he has, and describes what he saw in it, you have to take it with more than a grain of salt.



First of all, the book is itself interesting, and I think it would be of great interest to our listeners.  It's just out.  He's been making the rounds, talking about it.  It's titled "The Perfect Weapon:  War, Sabotage, and Fear in the Cyber Age."  I've not read the book.  However, I have seen David interviewed about this book, and he makes an interesting case, and that is that launching missiles is an overtly real-world physical act, and attribution is absolute.  You know where the missile came from.  Cyberattacks sort of slip under the radar.  I mean, literally.  And they are much less expensive.  They require much less physical real-world development and so forth.  So this whole cyberwar thing fits into a different category.  And David's position is that we need to be paying more attention to this.



Okay.  So the book is $15 for Kindle, $20 hardback, and free for an Audible trial.  So I really think, again, it's titled "The Perfect Weapon:  War, Sabotage, and Fear."  Because it's just out, there aren't many reviews on Amazon.  But one that caught my eye I wanted to share from someone who is himself an author.  He says:  "I came to this book the long way around.  Knowing that I had just published a military thriller in which North Korea crashes the electrical grid for the greater D.C. area, my brother-in-law sent me a link to David Sanger's recent interview on NPR," regarding this book.



He said:  "Listening to Mr. Sanger confirmed some of the scariest parts of my own research.  I discovered that my fictional scheme for robbing the U.S. government of electrical power is uncomfortably similar to an actual cyberattack that flatlined large segments of the Ukrainian grid in 2015.  Far from being worst-case imaginary scenarios, some of the concepts I've written about," he writes, "have already played out in the real world, usually in countries distant from the United States and under circumstances that either don't make the news or don't create an impression on the public consciousness."



He says:  "I burned through this book in less than a day.  'The Perfect Weapon' has the page-flipping intensity of the best techno-thriller novels, with the gravitas of meticulously sourced nonfiction.  If I had to sum up this book in one word, it would be 'terrifying.'"  He says:  "With true stories from the cyber sabotage of the Democratic National Committee to the penetration of the White House computer networks, this book is a wake-up call for our technology-dependent civilization.  I just hope we don't hit the snooze button and go back to sleep."



Okay.  So that's the book.  David Sanger is the real deal, a serious old-school journalist.  So that makes this controversy which has arisen interesting.  Bleeping Computer, our friends Lawrence Abrams and his team, have some terrific coverage of this that helps to sort of summarize this.  They said that U.S. cybersecurity firm FireEye has denied claims that have been ramping up on social media all of last week about illegally hacking back a Chinese nation-stage cyberespionage group.  The claims and social media discussions started after the publication of "The Perfect Weapon."  And they go on to explain what that is, and we just talked about that.



And then Bleeping Computer writes:  "In the book, Sanger recounted a series of events from 2013, in the lead-up to FireEye publishing a report called 'APT1, Exposing One of China's Cyber Espionage Units.'"  And actually we talked about this at the time.  It was one of the things that we covered on the podcast five years ago.  And then Bleeping writes:  "At the time, the report was a landmark moment in the cyberintelligence community as it exposed the activities of Chinese hackers in a depth of detail like never before, even going as far as pinning the hacking on Unit 61398 of China's People Liberation Army (PLA), an attribution level unheard of at the time."



Okay.  So what Sanger wrote, and I have in the show notes for anyone who's interested, is a link to a tweet by Thomas Rid, who's a political scientist known for his work on the history and risks of information technology in conflict.  He's also a professor of strategic studies at Johns Hopkins University School of Advanced International Studies.  So he was the one who sort of brought this to light, snapping some pieces of Sanger's book.



Because what David Sanger wrote was:  "Ever resourceful, Mandiant CEO Kevin Mandia's staff of former intelligence officers and cyber experts tried a different method of proving their case.  They might not be able to track the IP addresses to the Datong Road high-rise itself, but they could actually look inside the room where the hacks originated.  As soon as they detected Chinese hackers breaking into the private networks of some of their clients" - now, remember, this is Mandiant previous to the acquisition by FireEye - "mostly Fortune 500 companies, Mandia's investigators reached back through the network to activate the cameras on the hackers' own laptops.  They could see their keystrokes while actually watching them at their desks."



Sanger writes:  "The hackers, just about all of them male and most in their mid-20s, carried on like a lot of young guys around the world."  He writes:  "They showed up at work around 8:30 a.m. Shanghai time, checked a few sports scores, emailed their girlfriends.  Then, when the clock struck 9:00, they started methodically breaking into computer systems around the world, banking" - I think he meant banging - "on the keyboards until a lunch break gave them a moment to go back to the sport scores and girlfriends."



He says:  "One day I sat next to some of Mandia's team, watching the Unit 61938 hacking corps at work.  It was a remarkable sight.  My previous mental image of PLA officers was a bunch of stiff old generals sitting around in uniforms with epaulets, reminiscing about the glory days with Mao.  But these guys were wearing leather jackets or just undershirts, and probably saw Mao only if they visited his mausoleum in Tiananmen Square."



LEO:  I don't know where else they would have seen him.  They're dreaming.  They're dreaming about him.



STEVE:  Yeah, exactly.



LEO:  He hasn't been around a lot lately.



STEVE:  No.  He says:  "'They were such bros,' Andrew Schwartz, one of Mandia's communications specialists, recalled later."  So now FireEye claims it was a misrepresentation or misunderstanding.  And of course people are like, wait a minute.  How does seeing what the clothes of these people wearing, how was that a misunderstanding?  Okay, so I'm getting ahead of myself.



Bleeping Computer says that, in a statement that [FireEye] just released:  "FireEye refutes these claims.  The company says that Sanger mischaracterized what really happened and might have simply misunderstood what he was shown that day when he was allowed to sit with Mandiant employees.  FireEye says Sanger never observed real-time hacking, but only prerecorded videos of APT1 operators interacting with computers on the network of compromised companies."  And I should mention that they went as far as to show a video of a remote terminal where the cursor is moving and menus are clicking themselves and so forth.  But it's like, okay, that's way different than seeing the output of a video camera looking at people banging on their keyboards.



"Furthermore, FireEye says it obtained permission from these companies to leave the compromised PCs intact and observe what the hackers were doing, and at no point its employees used offensive hacking techniques."  And then they say specifically, because they're really worried about what Sanger has said in his book:  "Mr. Sanger suggests our '...investigators reached back through the network to activate the cameras on the hackers' own laptops.'  We did not do this, nor have we ever done this.  To state this unequivocally, Mandiant did not employ 'hack back' techniques as part of our investigation of APT1, does not 'hack back' in our incident response practice, and does not endorse the practice of 'hacking back.'"



They say:  "The conclusion that we hacked back, while incorrect, is understandable.  To someone observing this video over the shoulder of one of our investigators, it could appear as live system monitoring.  Nevertheless, Mandiant did not create these videos through hacking back or any hacking activity.  All of these videos were made through information obtained via consensual security monitoring on behalf of victims' companies that were compromised.  The videos Mr. Sanger viewed were from Windows Remote Desktop Protocol network packet captures of Internet traffic at these victim organizations."  Okay.  "Mandiant has never turned on the webcam of an attacker or victim system.  In short, we do not fight hackers by hacking, but by diligently and legally pursuing attribution with a rigor and discipline that the cause requires."



Okay.  So naturally this has caused quite a furor.  And in an emailed response, Dave Sanger responded to Mandiant's statements, Mandiant/FireEye's statement, saying:  "Mandiant gave us" - meaning him - "extraordinary access to their investigation as we were preparing to write about Unit 61398 in late 2012, and the result was our story in the Times, and the company's report, in February of 2013."



David wrote:  "I spent considerable time with their investigators and saw the images of the hackers as described in [his book] 'The Perfect Weapon.'  Mandiant now says that all those images came from 'consensual monitoring,' in other words, that everything they received, from code to message traffic to imagery, was visible because the hackers themselves were transmitting them in the course of breaking into the systems owned by Mandiant clients.  While that wasn't my understanding at the time, passive monitoring is a reasonable explanation of how the company came to link the hacks to specific individuals, several of whom have since been indicted by the United States."



Okay.  So that's the end of David's response.  And then in their conclusion Bleeping Computer said that some industry insiders - oh, actually I think this was some coverage in CyberScoop, yes.  Some industry insiders told CyberScoop they were less than shocked by the claims.  Broadly speaking, hacking back to gain novel insight into an attacker is fairly well known, despite it being illegal under U.S. law.  The practice is often the subject of commonplace rumors amongst those in the industry.  So just sort of an interesting look into the world and how it works.



People who've been listening to this podcast for a long time, and I think this actually occurred before the podcast, back in the early days when GRC was suffering its first DDoS attacks, may remember that I did my own forensic sleuthing in order to figure out who was behind the DDoS attacks that we were suffering.  And this is a long - this is, like, 15 years ago.  And I tracked down somebody whose handle was Wicked, learned that his name was Michael, and talked to him on the phone and said, "Hey, knock it off."



So it is possible to do this stuff without hacking back.  But it sure sounds like, given David Sanger's reputation, I'm inclined to believe that he was shown some cool things.  And who knows.  You would expect that savvy hackers would have black tape over their own cameras.  Maybe they don't because they were using them for video conferencing or videoing with their girlfriends.  Who knows.



LEO:  Well, that's kind of why I believe Mandiant's story, because that would make more sense, frankly.



STEVE:  Yeah.



LEO:  Because you're right.  I mean, now, even maybe in those days they didn't put black tape over cameras.  Maybe that's a new thing.  Why even have cameras; right?



STEVE:  I agree.  They're just, well, I need one to see you, Leo.



LEO:  Right.  That's all you need.  



STEVE:  And that's all I use it for.



LEO:  That's all you need.



STEVE:  I turn it off after we're done and get back to work.



LEO:  Do monitors now routinely have cameras on them?



STEVE:  Yeah.  I think every laptop.



LEO:  Laptops I know.  But monitors, I mean.  If you have a tower computer, you're not going to have a camera.



STEVE:  I'm not aware of it.  I have - I'm looking at one, two, three, four, five, six, seven, eight.  I have got monitors around me, and not one of them has a camera.



LEO:  Yeah.  I mean, I guess some might because it's a convenience.  But, yeah.



STEVE:  Yeah.  Okay.  So, boy.  As I said at the beginning of the year, when we first ran across the Spectre and Meltdown problems, this felt like something that would be going on and on and on because it was such a fundamental mistake.  It was something that wound the industry up because it wasn't something you could easily patch.  Well, another technology already exists and is evolving and has hit the same sort of problem, well, yeah, essentially the same problem with Meltdown, Spectre, and now potentially TLBleed.  And that's something that we've spoken about, an evolving technology that was adopted very quickly across the web browser space, which is WebAssembly.  WebAssembly solves the JavaScript performance problem by creating a - by shifting the compilation away from the browser so that the browser receives a precompiled, almost ready to execute blob of code.



Traditionally, as we know, JavaScript is ASCII.  It's text.  And so the browser downloads this text.  And then in the browser the compilation process/interpretation, the interpreters have gotten so fast that they now do sort of incremental, or it's called JIT, Just In Time compilation, only they're not bothering to compile or deal with code that they haven't encountered yet.  But as they do, they compile it on the fly in order to then be able to execute it repetitively, quickly.  You can think of it as sort of a caching approach for compilation.



Well, the problem is that takes time, and people want their pages to appear in front of them and run immediately.  So WebAssembly allows, first of all, different high-level languages to be used to compile to a common .wasm is expression, .wasm for web asm file, which is a binary file.  It has a text representation so that you can actually read the WebAssembly.  But what the browser downloads is a binary blob which allows I to just run immediately.



So it's very popular.  I think it started, the work began actually well before it was on our radar, back in 2012.  But just last year it got adopted across the board - Chrome, Firefox, Edge, Safari, all the major browsers now support WebAssembly, which is a faster adoption than we've typically seen with new technologies, probably because it's so performance oriented.  No browser - as we know, they all compete on performance.  No browser wanted to be the one didn't support WebAssembly and therefore suffered from benchmark comparisons against all the others.  So anything that could make the browser faster, they all jumped on.



The problem is that it breaks, WebAssembly inherently breaks the mitigations which the browser vendors immediately put in place at the beginning of the year to protect us from Spectre and Meltdown.  As we know, the problem with Spectre and Meltdown is cross-process leakage, where a hostile entity would have some way of getting into or running code on a core which is shared by others.  And as I've been saying, this isn't a problem really for end users because, if you've got hostile code in your computer, it's already in your computer.  The big threat was in cloud or shared hosting environments, where one of the hosted systems could be malicious and could be working to obtain secret information from other entities that are hosted in the same environment.



Well, one place where end users do have exposure is when they're surfing the web and downloading code into their browser, which is running in order to do the things that we want our browsers to do now.  So there you are constantly running code from third parties.  It's not technically on your computer.  It's contained in your browser.  And as we know, lots of work has gone into maintaining the sandboxing of the browser, maintaining that containment.  But it is a place where we're constantly exposed to the potential of threats.



So the first thing that the browser manufacturers did was they fuzzed the time that was available to threads in JavaScript.  There are APIs which used to be high resolution, where the JavaScript could say, "What time is it," and it would get back an instantaneous timestamp which was as accurate as the system was able to provide.  Typically, extremely accurate.  And then the JavaScript would say, okay, thanks very much.



Well, by taking a snapshot of now, and then taking a snapshot again after setting up a bunch of circumstances, the JavaScript could detect how much time elapsed between now and then, and use that to infer, believe it or not, the content of caches globally that is outside of the browser, thus creating a trans-browser leakage of information.  So because you could argue the browsers don't need super high resolution, like script running in a browser, JavaScript doesn't have a clear need for super high-resolution timing or super-accurate timing, deliberate jitter was added, and the resolution of the API's returned results was reduced so that JavaScript could not know exactly what time it was inside of its own code execution.  So it's like, okay, whew, problem solved; right?



Well, it turns out that there was, again, people are clever.  There was another technology which allowed high-resolution access to time indirectly.  And that is, in JavaScript you can have workers, which are essentially threads, individual threads of execution, which are going about doing their own thing in the code.  So what developers worked out and security people worked out was a backdoor, essentially, to high-resolution timing, not using timers.  There's something known as a Shared Array Buffer, SAB for short, Shared Array Buffers in JavaScript, which is a means of allowing communication between these worker threads in JavaScript.  They share an array of memory.



Well, if one of them has the job of doing nothing but incrementing a value, incrementing a counter in one cell of this shared array, then another thread that wants an accurate timing can read repetitively from that rapidly incrementing value in order to recover high-resolution timing information which has otherwise been deliberately hidden from code running in browsers.



So what happened is that the WebAssembly project has a roadmap of the features that they are planning to implement in the language, and they had not yet gotten to implementing shared array buffers as the language spec is maturing over time.  And in an interesting back-and-forth discussion that I saw online and saw some coverage of, they ended up deciding to punt.  Their feeling is there is no clear way of allowing shared array buffer support in WebAssembly where the whole point of WebAssembly is running full out, I mean, just absolutely as fast as you can.



Oh, and I forgot to mention that in digging around in this, I ran across a comment that just made me smile, which is there is no browser-based crypto miner which does not use WebAssembly.  Which how many times have I just scratched my head, saying I just can't believe - you just can't do crypto mining in JavaScript.  Well, it turns out, right, nobody does.  All of this cryptocurrency mining is courtesy of WebAssembly because it is as close to native code execution as you could get.  So that explains to me and for the record, for all of our listeners, how it is possible to do cryptocurrency mining in a web browser.  Well, it's actually essentially native code, courtesy of WebAssembly.



And so the point is that the only way the WebAssembly developers can see to prevent essentially cache-busting timing would be to somehow stall the WebAssembly threads so that they just can't know whether they were explicitly stalled or it took longer as a consequence of a cache miss.  The problem is stalling WebAssembly threads is exactly what you don't want to do.  You don't want to cripple the performance that you've created WebAssembly for in the first place.



LEO:  I'm surprised, though, that something running in a virtual machine could execute these kinds of timing attacks.



STEVE:  I know.  I am, too.  And in fact there have been proof of concepts done, and I've seen code snippets where one workers just sits there and does nothing but just as fast as it can increments a counter.  And another worker thread reading that counter is given enough information about the passage of time in order to do cache-busting, so much so that the Shared Array Buffers have been removed from JavaScript and will not be implemented by the WebAssembly people.  And everybody is just, like, basically they're saying we want this, but we can't offer it safely at the moment.  We're hoping that Intel and/or the OS vendors working together, basically we're hoping for some other resolution to this problem.



And the big concern is, with this advent now of the TLBleed, which is hyperthreading, I think the only solution there is going to be to keep the threads in the same process so that there isn't any - so that adjacent hyperthreads can't be running in different processes.  That's the only solution I see there.  And I just think we're going to have to tolerate a performance hit in the short term until we end up with next-generation processors.



And as we know, Leo, Intel's got a deep pipeline of processor architecture.  There's no question, if they've had to scramble around and change the microcode in as many of their existing chips as they have, that there was some immediate design impact on the things that were further down the production line and probably had to get pulled back in order to have some features added in order to solve this problem in the right way.  So serious impact on our industry.



And I think this is the last big piece of information before we get on to our miscellany stuff and then talk about the Supreme Court.  And this is Firebase.  Firebase, Inc. is a mobile and web application development platform which was created by, well, Firebase itself, created by Firebase, Inc. seven years ago in 2011, which was acquired by Google four years ago in 2014.  So they are now at Firebase.Google.com and have a very nice professional-looking website.  And they claim that Firebase helps mobile apps to succeed.



They say:  "Build fast apps without managing infrastructure.  Firebase gives you functionality like analytics, databases, messaging, and crash reporting so you can move quickly and focus on your users.  Backed by Google, trusted by top apps."  They say:  "Firebase is built on Google infrastructure and scales automatically for even the largest apps." 



So essentially what this is, is a cloud-based database service where, rather than asking applications which are already connected apps, that are already cloud-based, mobile, connected, Firebase is saying, we'll store all your data for you.  You can offload all of that stuff, and we'll handle it.  So they have something called Cloud Firestore, which is store and sync app data at a global scale; Cloud Functions, which allows to run mobile backend code without managing servers.  They have authentication, and therein lies the rub:  "Authenticate users simply and securely."  Which turns out to, unfortunately, not be as true as we and the users of these apps would wish.  Also Cloud Storage:  Store and serve files at Google scale.



So the firm whose name I could not remember at the top of the show is Appthority, which is kind of a clever name, authority, but Appthority.  And they've got, Leo, probably one of the best-named vulnerabilities around.  They're looking at data leakage through backend stores.



LEO:  Oh, dear.  Oh, dear.  I'm worried now.



STEVE:  Uh-huh.  They called it...



LEO:  Backend data leakage would be really a problem.  What did they call it?



STEVE:  They called it "HospitalGown" vulnerability.



LEO:  Oh, dear.  Oh.  Well, that's better than it could have been.



STEVE:  Yeah.



LEO:  Data leakage?  You're sitting in it.  No, no, no.



STEVE:  So they said:  "In 2017 the Appthority Mobile Threat Team discovered" - actually and they named - "the HospitalGown vulnerability.  The vulnerability, named for data leaking through backend data stores that are unsecured, results from app developers' failure to properly secure backend servers with firewalls and authentication, leading to data exposure."  They wrote:  "Our initial report in May 2017" - so just about a year ago - "revealed that weakly secured backend databases were being accessed via apps used by employees, partners, and customers and resulted in numerous security risks including extensive leaks of sensitive data, easier data access and exfiltration, and increased risks for spear phishing, social engineering, data ransom, and other attacks."



Okay.  And they go on to update us basically with their new report, which was just released, focusing on this Mobile Threat Team's latest discovery, which is "a new variant of the HospitalGown vulnerability, which occurs when app developers fail to require" - it's hard even to believe I'm saying this - "app developers fail to require authentication to a Google Firebase cloud base."  And then they explain:  "Firebase is one of the most popular backend database technologies for mobile apps but does not secure user data by default."  It's like, what?



Okay.  I'll say that again.  "Firebase is one of the most important backend database technologies for mobile apps, but it does not secure user data by default, does not provide third-party encryption tools, or alert developers to insecure data and potential vulnerabilities.  To secure data properly, developers need to specifically implement user authentication on all database tables and rows, which rarely happens in practice.  Moreover, it takes little effort for attackers to find open Firebase app databases and gain access to millions of private mobile data app records."  And then they go on to explain how easy it is to find this.  I won't bog us down in those details.



But in the show notes, Leo, I have a table from this report, from Appthority's report.  It's a grid showing total apps with Firebase IODBs, Android predominantly.  For example, there's a total of a little over 28,000.  27,277 are Android; 1,274 are iOS.  Then the number of the total 28,502 which were vulnerable, a little over 3,000 - I'm sorry.  Of the 28,502 total apps using Firebase, a little over 3,000 of those were vulnerable.  I'm just... 



LEO:  The big one is that half of all iOS apps that use Firebase are vulnerable.



STEVE:  Yes.



LEO:  That's kind of stunning.



STEVE:  Yes, yes.



LEO:  Now, I gather that Firebase is very popular.



STEVE:  So, yes, it's very popular.



LEO:  Geez.



STEVE:  I know.  Okay.  So what did they find when they looked?  Under Scope of Threat they explain.  Out of a total of, now, they analyzed 2,705,987 apps.  That's from which they found 27,227 Android apps and the 1,275 iOS apps that use the Firebase database.  So a lot of them do.  Of those connected apps they found that one in 11 Android apps, that's 9%, and almost half of iOS apps, 47%, that connect to a Firebase database were vulnerable.  As you said, Leo, almost half of iOS.  And still, one in 11 of Android.  And actually, because it's so much a stronger usage in the Android platform, the number, the gross number of Android apps is higher.  More than 3,000 apps were leaking data from 2,300 unsecured servers.  Of these, 975 apps were in active customer environments.  One in 10 of the Firebase databases are vulnerable.



So Firebase, this company that's making this service available, 10% of their databases are vulnerable.  Vulnerable Android apps alone were downloaded over 620 million times.  So these are not obscure apps.  These are popular apps.  Over 100 million records containing 113GB of data has been exposed.  They said most organizations were affected.  They found 62% of enterprises have at least one vulnerable app within their mobile environment.  No type of organization was immune.  Organizations with data exposed to this HospitalGown variant, as they call it, the Firebase variant, included banks, telecommunications companies, postal services, ridesharing companies, hotels, and educational institutions.  And there's no geographical preference or boundaries.  The U.S., Europe, U.K., Argentina...



LEO:  Argentina.



STEVE:  I always say that wrong.  I get an extra word.  I paused because I knew I was going to get it wrong.  Argentina, Brazil, Singapore, Taiwan, New Zealand, India, and China all exposed.



Okay.  So what about data?  They looked.  2.6 million plaintext passwords and userIDs.  More than 4 million protected health information records, including chat messages and prescription details.  25 million GPS location records.  50,000 financial records, including banking, payment, and bitcoin transactions.  More than 4.5 million Facebook, LinkedIn, and Firebase and corporate datastore user tokens.  And a lot of it is regulated.  Some of the data leaked includes highly sensitive private information subject to regulatory requirements, including HIPAA and the GDPR and PCI.



Medical information:  Chat messages between patients and pharmaceutical sales representatives, together with their prescriptions and orders, leaked from a Mexico-based pharmaceutical app.  Information about medical consultations, medical history, and diagnostic information was also leaked.  Information was protected, of course, by HIPAA regulations and subject to breach notification requirements.  They found sensitive personal data, email addresses, phone numbers, full names, geolocations, and Facebook OAuth tokens were all leaked in violation of data privacy protection laws such as the GDPR.



Vehicle license plate numbers:  Some apps exposed vehicle license and registration numbers as well as geolocation data.  California, as we know, has numerous data privacy laws, some of which require companies that expose names accompanied by license plate numbers and other automobile details to disclose these breaches.  And credit card numbers, which were exposed, subject to PCI DSS regulations.  And on and on and on.  So personal data.  Sensitive enterprise data.



Anyway, I'm glad that this has come to light.  I'm sure that Google's owned Firebase property will get on this, if they haven't already, and get this cleaned up.  It's clear that, if 90% of applications did employ security, you'd have to consider it negligent that one in 11 Android apps and certainly half of iOS apps did not do so.  It's cool that there's this service that allows apps to offload all of these functions to the cloud.  But it's just sort of stunning that in this day and age this information would be stored in the clear.



I didn't go into the details, but you're able to easily find this on the Internet and add like a /.json file extension and immediately begin retrieving data, essentially in plaintext, containing all of this.  So yet another example of I guess the triumph of convenience over security.  It's very convenient to do this, but stunning that this company would not enforce security to be used for anyone who is using their service.



Okay.  So I had a little adventure.  As our listeners know, my trustee Windows XP machine croaked late Sunday night, early Monday morning a week ago.  And I've been picking up the pieces ever since.  I'm now in front of that system that our listeners will remember I built about two and a half years ago, when Microsoft had announced that they were no longer going to support Windows 7 on newer hardware.  The uproar from that, after I immediately purchased the hardware that would still be supported by Windows 7, Microsoft said, whoa, whoa, whoa, that's not quite what we meant.  There was a lot of pushback from enterprises, and they have since said, okay, we'll go further.  I think it was Sandy Bridge was the last architecture they said they were going to support.



So since that was 15 years newer than the system I was using, I built up a beautiful machine.  But because I knew it was going to be a pain in the butt in order to have downtime to switch over, and XP was still working for me, and I've got lots of my own layers of security surrounding me, I've just continued using it till it died.



Okay.  So on Thursday morning I brought up the RAID that had been down since Monday and got access to the very latest snapshot of all of my files.  So I lost no data whatsoever.  I had images and things, but it's good to have the last instance of everything.  And then I immediately rebuilt my toolchain, and I can now reassemble SQRL and the Net Engine, and I'm still in the process of deciding what I want to do about an editor because I've decided, okay, it's time to give up completely on using Brief in a DOS box, even though there are a number of, you know, Brief was so popular that there are a number of updated modern clones of it.  But it's like no, no, no, it's time to - since I'm doing this, it's time to move on.



But there were some licenses that I had on software on that machine that could not be extracted except using the running live program.  And they were expensive.  So I thought, oh, crap.  I need to boot this.  So I thought, okay, well, I have the C drive.  It's sitting here.  I'm looking at it.  And some people in the GRC newsgroup where I've been hanging out, in the SQRL newsgroup, suggested that I just move this to a virtual machine, bring it into a VM.  And I thought, really.  Okay, except wait a minute, this is an old Windows XP OS that has all of its drivers, I mean, it's booting a RAID, so it's got a RAID driver.  And I've got different graphics cards in it than I have now, and on and on and on.  And anyone who's used Windows long enough knows that you really, if you change the motherboard out from under an installed Windows system, you're in for some pain.  I mean, it maybe won't even boot at all.  Or, if so, you boot in safe mode, and nothing really works and so forth and so on.



Anyway, so I documented in the show notes a page that I found.  First of all, lots of information about this sort of stuff.  But normally what people are doing is they're cloning a running system into a VM.  That is, and there are plenty of tools you can use for capturing the current system and creating a virtual machine image from that.  Unfortunately, that opportunity was foreclosed.  That is, I had the C drive, but I had it as just a mounted file system.  I didn't have it running.  So most of what I found when I was digging around the 'Net for how to pull this off didn't apply.



I did find a page at UltraGeek.com which was exactly what I needed.  So I have a link in the show notes for anyone who's interested.  And doing this, to give you a sense for what was necessary, was a multistep process.  First of all, the only tool that I could find which this page pointed me to was by our old buddy Mark Russinovich from Sysinternals.  Mark created something called Disk2VHD.  VHD is Microsoft's version of - VHD, as the acronym sounds, is a virtual hard drive.  So Mark has, and there's a free, you can download it from the Sysinternals site, sysinternals.microsoft, Disk2VHD.  You give it a mounted drive letter like I had, in this case it was Q, and it will virtualize that drive letter, creating a Windows VHD.  Okay, so that's the first step.



LEO:  That's handy.  That's neat.



STEVE:  It is very cool.



LEO:  Cool tool, yeah.



STEVE:  Very cool.  Okay.  But I wanted to use VMware.  And in fact VMware has the third tool that I need.  So I needed to convert a VHD into VMware's version, which is a VMDK.  So there's a company called StarWind that has what they call a V2V converter.  That is, it is an inter-virtual machine converter, and it's free.  They get an email address from you for the privilege, but fine.  I gave them my Gmail account.  So it's a free tool called StarWind V2V Converter.  And I used it - by the way, this was a 93GB file.  So none of this was fast.  But I wanted it.  So it converted the VHD to a VMDK.



So that got me over into VMware land from Microsoft land, which is where Mark's tool allowed me to get.  Then VMware has something called the vCenter Converter Standalone, VMware vCenter Converter Standalone, which again I downloaded from VMware at no cost.  And that's the miracle.  And I am so impressed with this thing.  You give it a virtual machine image, which is what StarWind V2V converter produced.  But of course that wouldn't boot because, I mean, it's got all of the Windows drivers and configuration and everything for a motherboard which is long since dead.  Well, actually not long since.  By that time it was four days.



LEO:  Long in seconds.



STEVE:  But it's 15 years old.  So I'd be hard pressed to find another one.  And it was never really clear, didn't seem to be the power supply.  I think it was the motherboard.  And it had been kind of dying slowly, like the PS/2 port on the back died, so my clanky keyboard had to have a USB interface in order to still get used.  I mean, it was sort of showing signs of getting tired.



Anyway, this thing, this VMware vCenter Converter Standalone, it's able to reach into an offline virtual machine image and change the drivers, to go into the registry which is offline, it just exists there in this virtual machine image, and go in and change the drivers to the drivers that VMware environment offers - IDE, SCSI, keyboard, video, I mean, everything, USB - and install them into the image and then insert them into Windows.  So this thing that would have never in a million years booted in VMware, can.  So once it was through, I just kind of scratched my head, thinking, okay, can this possibly work?  And I fired it up in VMware, and it was like an old friend coming back to life.  It was like, oh, there's my world.



LEO:  You know, it will probably still be faster on that new machine than it was on the original hardware.



STEVE:  Oh, it was wonderful.



LEO:  And I bet it's faster.



STEVE:  It booted up much faster.  And then I was curious, I opened up Device Manager.  Not a single wrong device.  All of the old things that were not there had disappeared.  There were no yellow triangles warning you of anything unknown.  It was a miracle.  Anyway, I just wanted to share it with our listeners.  Just if at any time in the future you find yourself, like you have a file system, a bootable file system from where, somewhere, doesn't matter when because this thing goes back to Windows 95, you can bring it back to life.  It's amazing.  It's wonderful.  Oh, and you can use VM Player.  So again, I think VM Player is free.  I have a full VM Workstation because that's one of the reasons I got this big monster machine was I wanted to be able to run multiple VMs with different environments and so forth, and have Linux and so forth.



So anyway, I just wanted to share my adventure and my amazement that this thing works.  Oh, and I did, I fired it up, and I was able to recover the licenses.  And also I was able to - I have a couple very mature registry trees.  And so by bringing the machine back up and being logged in, and then I'm able to export the registry and then was able to import that registry tree into my new machine, which is not something you can do from just the file system laying there because it's all in reg files.  I've wanted to do it in the past, and it's just you can't.



So anyway, it's very cool.  It's very possible now, and not spend any money, to bring up an old drive under any of these new virtual machines.  And I'm sure that - I didn't look at all the other V2V things.  I only wanted to go from the Microsoft virtual hard drive to the VMware.  But it looked like it supported a number of different options.  So anyway, I'm impressed with where we are with virtualization these days.  This may be old news for some, but I just wanted to make sure that our listeners knew about it because it was - I was just amazed.  It was really cool.



And lastly, I saw a bit of feedback that I wanted to share.  It's sort of feedback/errata.  Ciprian in Germany, his subject was "Huge Cisco Security patches number."  And his English has a German lilt to it.  Anyway, he said:  "Hello.  Cisco released again tons of patches for big iron routers."  He says:  "I got 24 individual emails.  What I wanted to remind is that Cisco had, is, and will acquire companies for their products; and in those purchasing, not all the backdoors are known.  My 2C.  Regards."  Oh, and he also said:  "P.S.:  Currently SpinRiting a huge 24-drive Dell enclosure to see what I can reuse."



Anyway, I thought it was a very good point.  I was a little rough on Cisco a couple weeks ago, just scratching my head, wondering how in the world you could possibly explain, like what was the culture there that they would have all these unknown, apparently even to them, or certainly to officialdom, backdoors in their products and then be launching, to their credit, audits of their own stuff and finding things they didn't know about, and in some case having other people finding things.



And so I thought this was very on point.  And so I thanked Ciprian for this, that Cisco is acquiring companies.  It's very much like FireEye, who bought Mandiant.  If that hacking back of China occurred, it was before FireEye owned Mandiant.  So you really can't hold FireEye responsible, even though Mandiant is now their employees.  Similarly, things you acquire from other people, well, you get what you get.



And I did want to mention that through the years a lot of people have used SpinRite as Ciprian is.  You get drives of unknown background and heritage, you run SpinRite on them to just make sure they're okay before you decide that you want to put them into use.  So anyway, thanks for the feedback, Ciprian.  And I'm really glad for the observation that these may not have been homegrown backdoors as a consequence of some bizarre culture at Cisco.  They could have just been acquired.



LEO:  We didn't write the backdoor, we just bought it.



STEVE:  Yeah, that's right.  Handy little thing to have.



LEO:  All right.  Back to you, Steve.



STEVE:  Okay.  So as we know, and this has been in the news a lot recently, increasingly local police and other law enforcement agencies have been obtaining court orders to compel local cell carriers to disclose the location data of people they suspect of crimes.  But significantly, these have not been probable cause search warrants as permitted under the Fourth Amendment to the U.S. Constitution.  They've simply been requests from courts for the business record documents of cellular carriers.  And in previous decisions it's been held that business records are subject to court order and can be obtained by law enforcement for whatever purposes they wish.



Okay.  So there was a burglar ring operating, I think it was in 2011 they were caught.  The ringleader was a guy named Timothy Ivory Carpenter, who was accused of planning these crimes.  I guess some of his other cohorts fingered him as the boss, and so he got in trouble.  He petitioned, I mean, so he was first convicted from - I should explain that the FBI, after they got his name, got a court order to cause the cellular carrier in the region to turn over the records of his cell phone location, which did identify him as being present in the location and time of four of the burglaries.  He was found guilty in court, sentenced for like a huge number of years, more than several lifetimes, as I recall.



He appealed, and the appeal was heard.  The appellate court said, no, we think this is proper, so sorry about that.  And it went to the Supreme Court.  The decision of the Supreme Court a day or two ago came down and overturned what the appeals court and the lower courts had said.  And this was interesting enough, and I thought would be interesting enough to our listeners, that I've pulled together sort of a summary of this because I wanted to go over exactly what this is that was decided.



Chief Justice Roberts delivered the opinion of the court.  And from the document which I've excerpted from it reads:  "This case presents the question whether the Government conducts a search under the Fourth Amendment when it accesses historical cell phone records that provide a comprehensive chronicle of the user's past movements."



Okay.  So I'll skip over what we already know from a technology standpoint, but I'll just say that it opens with a clear and perfect succinct explanation of cell tower location, how the increasing popularity of cellular communications has caused providers to increase the density of towers and thus increases the accuracy of tower-based location, approaching that of GPS, especially in urban areas.  And I do have in the show notes at the beginning of this a link to the PDF of the full, it's 158 pages, I think it was, or that might have been one of the other PDFs, it was like at least 119, of the opinion.



Anyway, so here's the summary of the decision.  It reads:  "Cell phones perform their wide and growing variety of functions by continuously connecting to a set of radio antennas called 'cell sites.'  Each time a phone connects to a cell site, it generates a time-stamped record as Cell Site Location Information (CSLI). Wireless carriers collect and store this information for their own business purposes.



"Here, after the FBI identified the cell phone numbers of several robbery suspects, prosecutors were granted court orders to obtain the suspects' cell phone records under the Stored Communications Act.  Wireless carriers produced CSLI" - that is, the Cell Site Location Information - "for petitioner Timothy Carpenter's phone, and the Government was able to obtain 12,898 location points cataloging Carpenter's movements over 127 days, an average of 101 data points per day.  Carpenter moved to suppress the data, arguing that the Government's seizure of the records without obtaining a warrant" - that is to say a search warrant - "supported by probable cause violated the Fourth Amendment.



"The District Court denied the motion, and prosecutors used the records at trial to show that Carpenter's phone was near four of the robbery locations at the time those robberies occurred.  Carpenter was convicted.  The Sixth Circuit affirmed, holding that Carpenter lacked a reasonable expectation of privacy in the location information collected by the FBI because he had shared that information with his wireless carriers."



So what the Supreme Court held is, one, the Government's acquisition of Carpenter's cell site records was a Fourth Amendment search.  Which is to say, qualifies under Fourth Amendment protection, and a search warrant with probable cause needed to be obtained in order for the cell site records to be had.  They wrote:  "The Fourth Amendment protects not only property interests, but certain expectations of privacy, as well."



And then they cite a previous case, Katz v. the U.S.:  "Thus, when an individual 'seeks to preserve something as private,' and his expectation of privacy is 'one that society is prepared to recognize as reasonable,' official intrusion into that sphere generally qualifies as a search and requires a warrant supported by probable cause."  Then they cite previous case Smith v. Maryland.



"The analysis regarding which expectations of privacy are entitled to protection is informed by historical understandings 'of what was deemed an unreasonable search and seizure when [the Fourth Amendment] was adopted.'"  And again a previous case, Carroll v. U.S.  "These founding-era understandings continue to inform this Court when applying the Fourth Amendment to innovations in surveillance tools."



Then they said, next paragraph B:  "The digital data at issue, which is personal location information maintained by a third party, does not fit neatly under existing precedents, but lies at the intersection of two lines of cases.  One set addresses a person's expectation of privacy in his physical location and movements," and then they refer to U.S. v. Jones, "where five Justices concluding that privacy concerns would be raised by GPS tracking."



Then they say:  "The other addresses a person's expectation of privacy in information voluntarily turned over to third parties."  Then they cite U.S. v. Miller and also U.S. v. Smith.  And this comes back a couple times, these Smith and Miller cases, "where expectation of privacy in records of dialed telephone numbers conveyed to a phone company."  And those were found not to be protected.  That is, there was no expectation in using a telephone dialing numbers.



Then they say, the next paragraph, C:  "Tracking a person's past movements through CSLI partakes of many of the qualities of GPS monitoring considered in" that Jones case, where it was decided that GPS tracking had privacy concerned.  "It is detailed."  So they're saying:  "CSLI partakes of many of the qualities of GPS monitoring.  It is detailed, encyclopedic, and effortlessly compiled.  At the same time, however, the fact that the individual continuously reveals his location to his wireless carrier implicates the third-party principle of Smith and Miller.  Given the unique nature of cell site records, this Court declines to extend Smith and Miller to cover them.



"A majority of the Court has already recognized that individuals have a reasonable expectation of privacy in the whole of their physical movements.  Allowing government access to cell site records, which 'hold for many Americans the privacies of life,'" and they cite Riley v. California, "contravenes that expectation.  In fact, historical cell site records present even greater privacy concerns than the GPS monitoring considered in Jones.  They give the Government near perfect surveillance and allow it to travel back in time to retrace a person's whereabouts, subject only to the five-year retention policies of most wireless carriers.  The Government contends that CSLI data is less precise than GPS information, but it thought the data accurate enough here to highlight it during closing argument in Carpenter's trial.  At any rate, the rule the Court adopts 'must take account of more sophisticated systems that are already in use or in development,' and the accuracy of CSLI is rapidly approaching GPS-level precision.



"The Government contends that the third-party doctrine governs this case because cell site records, like the records of Smith and Miller, are business records, created and maintained by wireless carriers.  But there is a world of difference between the limited types of personal information addressed in Smith and Miller" - that's the phone dialing - "and the exhaustive chronicle of location information casually collected by wireless carriers.



"The third-party doctrine partly stems from the notion that an individual has a reduced expectation in information knowingly shared with another.  Smith and Miller, however, did not rely solely on the act of sharing.  They also considered the nature of the particular documents sought and limitations on any legitimate 'expectation of privacy' concerning their contents.  In mechanically applying the third-party doctrine to this case, the Government fails to appreciate the lack of comparable limitations on the revealing nature of CSLI.



"Nor does the second rationale for the third-party doctrine, which is voluntary exposure, hold up when it comes to CSLI.  Cell phone location information is not truly 'shared' as the term is normally understood.  First, cell phones and the services they provide are 'such a pervasive and insistent part of daily life' that carrying one is indispensable to participation in modern society," citing Riley v. U.S.  "Second, a cell phone logs a cell site record by dint of its operation, without any affirmative act on the user's part beyond powering up."



And so they trim this a little bit, saying:  "This decision is narrow.  It does not express a view on matters not before the Court; does not disturb the application of Smith and Miller or call into question conventional surveillance techniques and tools such as security cameras; does not address other business records that might incidentally reveal location information; and does not consider other collection techniques involving foreign affairs or national security.



"The Government did not obtain a warrant supported by probable cause before acquiring Carpenter's cell site records.  It acquired those records pursuant to a court order under the Stored Communications Act, which required the Government to show 'reasonable grounds' for believing that the records were 'relevant and material to an ongoing investigation.'  That showing falls well short of the probable cause required for a warrant.  Consequently, an order issued under 2703," which is what was cited before, "is not a permissible mechanism for accessing historical cell site records.



"Not all orders compelling the production of documents will require a showing of probable cause.  A warrant is required only in the rare case where the suspect has a legitimate privacy interest in records held by a third party.  And even though the Government will generally need a warrant to access CSLI, case-specific exceptions, for example exigent circumstances, may support a warrantless search," and they provide some citings, "reversed and remanded."



So, bottom line, and good news for individuals, in my opinion, is that the Supreme Court ruled that the acquisition of CSLI data, that is, the entire history of an individual cell phone's movement back through time, as far back as five years, can only be acquired by law enforcement if they have probable cause and are able to get a search warrant from a court in order to request the acquisition of those records from the cell carrier.



So I think the Supreme Court did us right in this case, and I'm glad that this will happen.  There have been recently an increasing number of instances, I don't think I've talked about it on the podcast, but I've encountered it in reading, where law enforcement is just making sweeping requests for cell phone location information, just asking a court to say, hey, we're able to have this.  So from now on they'll need to show probable cause before getting it.  And I think that's probably a good thing.



LEO:  And there you go.  Although the disclaimer, you're not an attorney, so consult your attorney.  This is not a legal judgment.  This is Steve's reading of it all.



STEVE:  That's what the Supreme Court just said, yeah.



LEO:  Yup.  Again, consult your attorney.  I'm sure they talked about it on This Week in Law in great detail on Friday.  Some interesting results came out this Friday morning, yeah.



Steve is the host of the show, does it every Tuesday at about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to tune in and watch live, you can.  All you've got to do is go to TWiT.tv/live.  You should also chat if you are watching live at irc.twit.tv.  If you can't watch live, you can get on-demand versions.  Steve has his version, audio version, plus full transcripts, which is really nice to read along as you listen, at his website, GRC.com.  You'll also find all the other stuff Steve does, including his SpinRite, the world's finest hard drive recovery and maintenance utility.  You'll find it at GRC.com.



We have audio and video at our website, TWiT.tv/sn.  And of course you can always subscribe.  Every podcast app in the world has Security Now!.  All you've got to do is search for Security Now! and add it, and you'll get it the minute it's available every Tuesday afternoon.  Steve, thank you. 



STEVE:  Okay, my friend.



LEO:  You said it was a light day, but it wasn't that light.



STEVE:  No.  We'll be back next week with more news.



LEO:  I'll see you then.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#670

DATE:		July 3, 2018

TITLE:		Wi-Fi Protected Access v3

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-670.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the interesting case of a VirusTotal upload - or was it?  We've got newly discovered problems with our 4G LTE and even what follows; another new EFF encryption initiative; troubles with Spectre and Meltdown in some browsers; the evolution of UPnP-enabled attacks; an unpatched WordPress vulnerability that doesn't appear to be worrying the WordPress devs; and an early look at next year's forthcoming WPA3 standard, which appears to fix everything!



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about VirusTotal and the interesting case of a virus leaked before its time.  Also, what WPA3 will mean to you, and when we'll get it.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 670, recorded Tuesday, July 3rd, 2018:  Wi-Fi Protected Access v3.



It's time for Security Now!, the show where we protect you and your loved ones online with the help of our Explainer in Chief, Mr. Steve Gibson of the GRC, Gibson Research Corporation, GRC.com.  Hi, Steve.  Good to see you.



STEVE GIBSON:  Yo, Leo.  Great to be with you again, as always.



LEO:  Of course, of course.  And today we have a fabulous show lined up for you.  I shall only be interrupting it three times.  I'm going to let you just take it over.



STEVE:  This is the pre-Fourth of July episode of Security Now! #670.  And it was announced at the beginning of the year that this was going to happen, and then early last week it finally did.  But these are not my favorite people, as we all know.  This is the Wi-Fi Alliance.  And we've often said that they haven't yet gotten everything right, yet it's one of the most important protocols, arguably, WiFi security, probably increasing in importance fast.  And it's just been a lesson in how not to develop really important protocols; whereas the fundamental architecture of the Internet was all done academically in full view.  I mean, there's this amazing trail of public discourse where problems were resolved.



Unfortunately, this Wi-Fi Alliance is a closed group, nothing in public.  I was excited this morning as I began digging into the details of WPA3, where there were links to the specification.  It was like, oh, my goodness, like they've changed.  It turns out, after filling out a form telling them who I am, where I am, what's going on, I mean, basically just here's all of my contact information, please, please, please let me have the specification, I get like this seven-page header with nothing in it.



LEO:  We wouldn't want you to know how it works, Steve.



STEVE:  And, I mean, it had to, I mean, the WPA3 spec has to be a thousand pages, or hundreds.  And then I thought, oh, well, okay.  And then there was one on opportunistic encryption specifications.  I thought, okay, maybe I'll do better.  So I immediately went back and got the same form, so they didn't remember me after I'd just bent over, so I did it again.  It's like, okay.



LEO:  You did this all for us, Steve.  Thank you.



STEVE:  I really want this.  So did it all again, clicked the link, seven pages.  You know, basically an abbreviated table of contents and some acronyms defined.  It's like, ugh.  So anyway, same story.  But the good news is, if we give them a lot of benefit of the doubt, what they're saying about WPA3 is all good.  I mean, I'm bullish about the promise.  Unfortunately, there's no ability whatsoever to, like, verify any of this.  But we're going to talk about what it is that they say they've done.  But it doesn't affect us yet, either, because it's expected late 2019.  So, okay.  Because of course everything has got - it's got trademarks on everything, and certified everything, and it's all locked down, and they're in control, and they're going to put their WPA Wi-Fi Alliance stamp on everything, and that takes time, you know, to make those stamps.  So someday WPA2, which has been broken - and I'll remind people about that later, with the KRACK attack.  And of course everything else earlier is badly broken.  Someday we're going to have an improvement, and it's good.



So in the meantime we've got an interesting case of a VirusTotal upload mistake, or was it?  Newly discovered and reported problems with our current 4G LTE cellular system - worse even than we knew, and we already knew that was a problem - and even apparently what follows it, the 5G system.  The EFF has launched another encryption initiative, a detailed look into which will probably be the topic for next week because it's big and I want to do it justice, but we'll talk about it.



We've got some troubles with Spectre and Meltdown as it exists now in browsers post-mitigation of those problems.  Sort of the predictable - but sure enough, things that get predicted often happen - evolution of attacks leveraging the Universal Plug and Play features of today's so prevalent routers.  An unpatched WordPress vulnerability that got a lot of press last week, but doesn't appear to be worrying the WordPress devs because they've known about it since last November.  And then we'll talk about the WPA, what we can of the WPA3 standard, at least the hope and the promise that they're offering, and why it looks like it's a good thing.  So I think a fun and interesting podcast to get everybody ready for their Fourth of July barbecues tomorrow.



LEO:  Sounds like it.



STEVE:  Our Picture of the Week came from my archive of photos, since there was nothing particularly pertinent for this week.  But I got a kick out of it.  It was some conference, a security conference sponsored by, because you could just see this at the bottom of the screen, Oracle, Red Hat, ING, and Google.  And it's a big slide that says the first law of software quality.  And it reuses the famous expression e=mc2.  In this case they redefined "e" to be errors and "mc" to be more code.



LEO:  More code, more problems, as the kids would say.



STEVE:  Yes.  And of course this is something our listeners are so familiar with me going on about endlessly, the idea that the more code you have, the more opportunities for a mistake there is.  Less code, fewer errors.  So anyway, errors equals more code squared, meaning yes, and it does.  It goes up.  And we've also talked about this, the fact that it goes up exponentially, because all the code you have is interacting with all the other code you have.  So as you add code, the opportunities for interactions goes up nonlinearly, much more quickly than the code you're adding would suggest.  So anyway, keep it small.  Keep it simple.



LEO:  Yup.



STEVE:  Okay.  So this was an interesting story that I got a kick out of.  And at first it looks like it may have been a mistake.  But upon further reflection, I'm wondering.  It involves the discovery on VirusTotal of a previously unknown, in other words zero-day, exploit.  So, okay.  So we've never talked about VirusTotal before, so we should do that.  VirusTotal...



LEO:  Yeah, who are these guys?  What...



STEVE:  Excuse me?



LEO:  Who are these guys when they're at home?



STEVE:  So VirusTotal is a very cool system.  And I'm surprised it's never come up somehow in nearly 12 years of this podcast because I have used it from time to time.  I know that a number of our listeners know of it.  Simon Zerafa, a friend of the podcast, is frequently checking things for me.  And as we know, a lot of our antiviral systems have, by necessity, needed to become heuristic.  There's this cat-and-mouse game where they're trying to stay current, and they have so-called "virus signatures" which cue them that there may be a problem.  But viruses can morph, and do.



And so increasingly, antiviral systems have been using signals to tell them that this could be malicious, rather than knowing it's malicious, because they previously saw exactly the same thing.  And this is why, from time to time, as a software publisher myself, I'll get reports that, like, oh, the DNS Benchmark has malware in it.  And it's like, what?  First of all, the DNS Benchmark hasn't changed in, you know me, I think I finished it in 2010, so eight years.  I mean, nothing has happened to it.  And it's digitally signed, and the signature is still valid, so no one has horsed around with it on my server.  And it's like, there's nothing wrong.



But suddenly some AV tool just decides, oh, this looks suspicious.  Let's warn people.  And so, okay.  How do you know, like what does an end-user do in that situation?  Well, VirusTotal is there for us.  And it ought to be in everybody's toolkit, everybody who's listening to this podcast who doesn't already know about it.  And VirusTotal is the domain name.  I don't know if it's dot org or dot com.  But if you put "VirusTotal," all one word, into Google, it'll take you right there.



LEO:  It's owned by Google, we should mention, if that makes a difference.



STEVE:  I didn't know if it was.  I thought it was, but when I was digging into it, it's something that looked Irish or something, CSLI, or I don't remember what the acronym was.  But I also thought it was owned by Google.  But when I dug into it, it looked like it had a different owner.



LEO:  Well, it's owned by a Google - an Alphabet subsidiary.  I guess that counts.



STEVE:  Okay.  Right, right, right.  Yes.  And so, okay.  So what it is, is sort of social networking comes to antivirus and malware research.  An individual who suspects something on their machine might be, like, hinky, can upload it for free, the service is completely free, to VirusTotal.  And that service, that web-based service runs that thing, that file that was uploaded, through over 70 antivirus scanners that are shown by name.  And so it's like all of the AV engines of the major players in the industry are accessible to VirusTotal and are interested themselves in seeing new things.  So it's sort of a win-win-win-win-win.  Everybody wins because the end-user gets to say, uh, my computer started acting weird after I downloaded this thing that automatically sends love letters.  Is it safe?



So somebody questioning some software can upload it to VirusTotal.  And what you get is this really gratifying, comprehensive report of, by engine, by AV scanner, what each of them thought of what you just submitted.  So it's just this cool system.  And part of the terms and conditions of doing so, which you implicitly agree to, is that you're turning over the intellectual property, I mean, you're alleging that it's yours to offer for analysis; and that, in doing so, you're explicitly letting other people look at it for this purpose.



So what happened was that, back in March, ESET, which is one of the well-known AV players, was closely examining a PDF which had been uploaded to VirusTotal and suspected that they might have found a potential exploit for an unknown, to them, Windows kernel vulnerability which was hidden within the workings of that PDF, which somebody put up on VirusTotal.  So they forwarded this possible discovery to Microsoft, who subsequently confirmed the presence of two previously unknown zero-day exploits in that PDF.  One was leveraging a flaw in the JavaScript handling of Adobe Acrobat and reader.



The other turned out to be a potent, never before seen elevation of privilege in the Windows kernel.  And as we know, elevation of privilege is like a key component of forming a powerful exploit chain because you're wanting to get - you wanted to elevate yourself to system privilege so you can get up to more mischief.  And as we're always saying, and as is commonly known, not running as a privileged user is a good way of protecting yourself and your system since the things you can do and the things that can be done on your behalf are dramatically curtailed.



So, okay.  So what was believed was that this malicious PDF was likely in the early development stages since the exploits were fully functional, yet the PDF did not contain a malicious payload.  There was just sort of a proof-of-concept stub, as is often the case, for example, calc.exe, the little Windows calculate gets launched, which is a safe thing that is often used as a proof of concept to say, look, we just ran something that you didn't explicitly run.  So it was like that.  It was like all of the machinery was there for an unknown exploit, but it wasn't itself malicious.  It was like the development stage.



So what was assumed was that the exploit's developers, well, what was initially assumed was that the developers made a mistake by uploading this because this proof of concept got away.  But in thinking about this further, I wonder if that's the case.  Some reporting of this in the industry suggested that it was a mistake.  In fact, quoting from some of the coverage I have in the show notes, it seems someone could have combined both the zero days to build an extremely powerful cyberweapon.  But they had intentionally and mistakenly lost the game, as it was written, by uploading his or her under-development exploit to VirusTotal.  And certainly that could be the case.



Microsoft wrote:  "The sample does not contain a final payload, which may suggest it was caught during its development stages.  Even though the sample does not contain a real malicious final payload, the authors," Microsoft wrote, "demonstrated high-level skills in vulnerability discovery and exploit writing."



And so there's another way to process this same set of facts.  Given that there's a 60-day response Window from the VirusTotal upload to the patch - oh, I forgot to mention that this is the story, this is the back story behind two of the fixes from last Patch Tuesday, from the second Tuesday of May's updates.  There was an Adobe zero-day patch and vulnerability and fix, and there was one for Microsoft's kernel.  These are those.



So it took two months to go from the event of the upload on VirusTotal - oh, and by the way, this PDF, because as I said it was zero-day, it looked clean.  VirusTotal gave it a clean bill of health.  Looks like you just uploaded a regular PDF, says VirusTotal, when in fact something induced ESET to look more closely.  And they said, whoa, this is not benign.  So imagine that there's a 60-day window, and that the bad guys know this, know that the whole process, the cycle takes about two months.  So this could have also been a carefully and deliberately calculated gamble on the part of the attackers because, if they were as good as Microsoft admits they were, it seems unlikely that they would have made such a simple mistake.



LEO:  But why would they upload it then?



STEVE:  Well, because based on their target - so this kind of a PDF would be part of a phishing attack where the malicious payload would be bound into a phishing PDF aimed at someone.  And it might have been more valuable to them to determine whether it would set off any alarms with the recipient relative to the size of the length of time they knew they had to use it.  So, for example, if they developed it, gave it a benign payload, sent it to VirusTotal, and it was immediately recognized, then that would tell them that this was not - they didn't have zero days.  They had something that somebody already knew about.



LEO:  And now the clock is ticking, they have three months to use it.



STEVE:  Exactly.  And in the world we live in today, this is what it's come down to is you can imagine, if a target was valuable enough, and the probability of a successful spearphishing infection at that target was high enough, in an environment where raising an alarm by its detection would be sufficiently detrimental, then they could have made a cold calculation.  It's like, well, because they don't need 60 days to launch their spearphishing.  They only need one day.  But so it might have been, I mean, they would know that sooner or later these were going to be found, like this is what happens.  Zero-days get seen in the wild, they get detected, they get found, they get added to everybody's AV heuristics.



So these zero-days have a limited life always.  And they're only valuable while they're not yet known.  And so but there is this 60-day window.  We saw it.  From March to May this thing was not recognized by any AV and patched against being used.  So anyway, it's interesting that, I mean, where now we're in this spearphishing mode where we're deliberately targeting specific individuals to get into, you know, like state-level sponsored cyberwar, where it's very valuable to establish a foothold if you can, when someone comes up with a zero-day, they need to know if it's going to have a chance to succeed.  And 60 days, that's plenty of time for them.



So just I wanted to take the opportunity to talk about VirusTotal for any of our listeners who aren't aware of it.  It's a very cool system, the idea that you can easily send something there and have it checked.  In the early days of my work on SQRL - actually when I added the installer facility to SQRL.  SQRL contains - it's sort of a self-installer.  It's the same size EXE, didn't grow very much.  But when you run it, it notices it hasn't been installed.  And it doesn't actually need installation, but users are used to that, and they're going to download it into their browser's download folder, and you don't want to run it from there.  So we needed a process for moving it into a final place and kind of getting it configured in the system, although it also was doing that for quite a while without any help.



But something about my addition of the installer, because I'm messing with the registry, like registering it for autostart optionally and a few other things, something that I did set off Windows Defender so that it was slowing it down for a while.  That's gone now, so it learned to trust SQRL, which is nice.  But several times I uploaded it when I was trying to figure out what was it that I did that caused it to suddenly get examined?  And the point was it slowed things down because Defender was jumping in there and saying, okay, wait a second, let's take a look at this.



And so I was playing with different types of EXE compressors, seeing if there's anything I could do that wouldn't upset Windows Defender.  It must have been something fundamental about the fact that I was doing installation-y things where some heuristics got triggered.  Now they don't slow it down at all.  But I was using VirusTotal myself, sending copies, like various tests of the SQRL EXE, up to see if anybody else was upset by it.  It never set off any alarms.  But it's like, well, okay.  I don't know.  And it finally just kind of went away on itself.



But for what it's worth, VirusTotal is cool.  And I wouldn't be at all surprised if this was not a mistake on the part of these guys.  If they are that sophisticated, they know the world cannot respond instantly.  But what's more important to them is knowing if they do actually have a zero-day, or if what they have discovered - I mean, because really, how would you know if this was already known?  You would know that your exploit was effective on systems that you had, so you would know that the systems you had today were not patched for it.  Thus it's an exploit.  But you wouldn't know if any third-party AV that you don't have might not know about it.



And remember that all of our AV now is phoning home.  So even letting third-party AV sniff this work in process, that could let the cat out of the bag, by having the AV go check in with the mothership and go, hey, we just discovered somebody working on a new zero-day.  So I could see they, like, stayed offline, developed this in a cleanroom environment, knew that none of the OSes they had were patched against it, but they didn't know about the rest of the world.  And so the decision was, rather than tip off individual AV, let's just wait till we're ready, put a null payload in, send it to VirusTotal, see if any bells go off.  And, if not, immediately take the weaponized version and launch it at our target.  So as I said, such is the world we're in these days.



LEO:  Really interesting, yeah.



STEVE:  Yeah.  And again, there is a response time window, and spearphishing has, what a few hours of response.  So it's very likely, if that interpretation of the facts is correct, this strongly suggests that someone somewhere was probably a victim since not a single alarm went off at the time.  And if that person had reason to open the PDF, that system probably got compromised.



LEO:  Wow.



STEVE:  And somebody's mission got accomplished.



LEO:  Now, it's all fixed now, though; right?



STEVE:  Yes, yes.



LEO:  Okay.



STEVE:  At the beginning of May is when both of these zero-days were addressed.



LEO:  Why 60 days to fix it?  Why does it take so long?



STEVE:  Well, you know, we've seen instances, like where Microsoft, remember, was told of something in January, and then they said - they finally, after 90 days, came back to, it wasn't Google, it was - we were talking about it just a couple weeks ago.  Shoot.  The Zero-Day Initiative guys gave them 90 days.  And then so finally Microsoft says, oh, we were unable to duplicate the proof of concept.  And the Zero-Day Initiative guys said, well, we sent you a proof of concept three months ago.  And Microsoft said oh, yeah, really?  And they said, well, here it is again.  And so they gave them a couple more weeks and then went public with it because it was like, hey, you guys have used up your time.  So I just think everybody's probably busy, and they're having to juggle these things, and it took 60 days.



LEO:  Well, even if it takes two days, it's enough.



STEVE:  Yes, good point.



LEO:  Right, yeah.



STEVE:  Good point.  Okay.  So a group of researchers took another hard long look at the LTE cellular network.  This is of course 4G that we're all using.  LTE stands for Long-Term Evolution.



LEO:  In other words, nothing.



STEVE:  Or we're still waiting for it, yes.  We're still waiting.  And that of course replaces the 3G, which was GSM, which stood for Global System for Mobile.  So these guys have basically - they've just released a research paper which pounds yet another nail in the already well-nailed coffin of LTE.  They'll be delivering the paper at the IEEE Symposium on Security and Privacy in 2019, so at least half a year from now.  But it's available today, titled "Breaking LTE on Layer 2."  I'll talk about layers in a second.  But I've got a link in the show notes to the whole paper, which is very interesting for anyone who wants to get into more details than I will as appropriate on the podcast.



But in their abstract they said: "Long Term Evolution (LTE) is the latest mobile communication standard and has a pivotal role in our information society."  Yeah, like we're all using it.  "LTE combines performance goals with modern security mechanisms and serves casual use cases as well as critical infrastructure and public safety communications."  That's building the case for why what they've done is important.  "Both scenarios," they write, "are demanding towards a resilient and secure specification and implementation of LTE, as outages and open attack vectors potentially lead to severe risks."  Previous work on LTE protocol security identified crucial attack vectors for both the physical layer, which is layer one, and the network layer, layer three.  The one in between, the data link layer, layer two, they said, has remained a blind spot in existing LTE security research.  And thus that was their focus.



Now, just to remind people, one of the neatest innovations in networking, I guess kind of not really to apology, but networking thinking or design architecture, was the separation of communications into well-defined network layers.  Rather than just - previous to this you'd just sort of write something that was just a big blob that did what you wanted it to, but it wasn't compatible with anybody else's blobs.  It was, as being a big blob, if you needed to do something different, you'd have to rewrite who knows what parts of the blob because it was just a big blob.



So layering solved that by deliberately - and there's something known as the OSI network layer model.  What is that?  Open Standard something, Initiative or something, OSI.  So, for example, at the bottom layer is the physical, or like the electrical layer.  So, for example, in the old days RS-232, the old serial communications, the specification for it being plus or minus 12 volts, and input on pin 2 and output on pin 3 and ground on pin 5 and the various, the electrical level spec would have been its layer one.



Also, for example, 10BaseT coax, the way the signal frequencies are pushed through the coaxial fiber would have been, for coax networking, layer one.  In our modern RJ45 Ethernet we've got the way differential electrical signals on a twisted pair go back and forth as layer one.  So that describes the electrical layer.  Then for us in modern networking, on top of that we have Ethernet, which we've talked about often on the podcast, things like the way you deal with packet collisions, that is, so you go to the data, the raw data living on top of layer one, on the electrical layer.  And that's where, for example, you say, okay, we're going to have these things called packets, and there's going to be a MAC address, which is the two 24-bit or the 48-bit address that's globally unique for the network interface card.  And that's going to be a packet that has a payload.  But that's as far as you go.



That is, so the idea is that, by deliberately constricting what you specify at each layer, you create interlayer boundaries that give you sort of a plug-and-play architecture.  You can, instead of everything just being, like the whole solution just being a blog that's one chunk of code, and god help you if you want to change it so it runs on something different, instead by deliberately constraining it to a layered architecture, you're able to change things around.  So layer three is the network layer, where for example in our networking today we have IP packets, or ARP packets, or ICMP.  Those are three different protocols which can all be carried by the data link layer, the Ethernet packet, which gets around the room using the physical layer, the first layer, and so on.  And so on top of IP, ARP, and ICMP, then you'd have the transport layer where you have TCP, UDP, and sort of that SSL and TLS, which are kind of grafted on at the same layer.



So anyway, what these guys have done, and the reason I've gone into this detail, is the ultimate solution to this and other problems is where we need to talk in a second.  But so there has been previous work at attacking LTE's physical layer, that is, the radio link portion, and the network layer, the protocol that runs on top, but not the data link layer that is the equivalent of essentially the encryption of the radio signals themselves.



So they say they've got a number of different attacks.  The first is kind of a yawner.  They said:  "In this paper we present a comprehensive layer two security analysis and identify three attack vectors.  These attacks impair the confidentiality and/or privacy of LTE communication."  And I should say I'm sharing the yawner here first.  But the one they found is horrifying.  We'll get there in a second.



"More specifically, we first present a passive identity mapping attack that matches volatile radio identities to longer lasting network identities, enabling us to identify users within a cell and serving as a steppingstone for follow-up attacks. We demonstrate how a passive attacker can abuse the resource allocation as a side channel to perform website fingerprinting that enables the attacker to learn the websites a user accessed."



So as I was digging into this paper, I was thinking, what?  Okay.  We know about side-channel attacks.  And so how are you going to fingerprint a website if you cannot decrypt the data?  Because they don't arrange to do that.  If that were possible, we would probably know about it already.  So it turns out it's sort of weak and obvious in retrospect.  In describing it, they say:  "Meta information on the data link layer leak information about the consumption of data per unit time."  In other words, like bandwidth flow.  And they say, for example, and they have a made-up person Bob, who's their victim:  "For example, if Bob watches a video, he uses more traffic compared to when he accesses a simple website.  As a preparation step of the attack, Eve records popular websites and their layer two patterns."  Thus their term "fingerprinting."



"During the attack, she eavesdrops the meta information and looks for similar patterns.  In case she finds a match, she knows which website the victim visited with a certain probability."  So it's like, okay.  So they're basically saying you've got radio, so eavesdropping is possible.  So different websites, if you look at them with sufficient focus and clarity and granularity, they will have characteristic bandwidth signatures.  And so, if you built up a catalog of those and called them "fingerprints," then you could see somebody else's traffic doing exactly the same thing that your analytical traffic had earlier done and conclude with some degree of confidence that they were at the same place, even though you have no visibility into the data of their traffic at all, just the fact of their traffic and the amount and timing and so forth.



So it's like, okay.  Again, sort of obvious.  Not very significant.  However, the main attack is potent.  They called it the "aLTEr" attack, and they played with the fact that the middle three letters of "aLTEr" are LTE, so aLTEr.  They said:  "We present the aLTEr attack that exploits the fact that LTE user data is encrypted in counter mode, but not integrity protected, which allows us to modify the message payload."  And we talked about this a long time ago, and I remember at the time sort of shaking my head.  It's like, how did this ever happen? And it's just that it's so old.  You know, this spec has been around for so long that I guess we forgive them for not authenticating the communications.  We'll talk about that in a second.  I'll just finish what they said.



They said:  "As a proof-of-concept demonstration" - and they do this - "we show how an attacker can redirect DNS requests and then perform a DNS spoofing attack.  As a result, the user is redirected to a malicious website.  Our experimental analysis demonstrates the real-world applicability of all three attacks" - the first two I lumped together because, again, they weren't very powerful - "and emphasizes the threat of open attack vectors on LTE layer two protocols."



Okay.  So here's what's going on with that.  LTE uses a useful but flawed encryption.  AES counter mode, you know, everyone gets wound up when you say "AES," like oh, that's a good cipher.  Yes, except that, as we know, it takes more than just the cipher to deliver useful encryption.  AES counter mode uses a 128-bit counter, which is incremented one step at a time.  The value of the counter is encrypted by the AES cipher under a key.  And the output is a pseudorandom bitstream.  So we've dealt with these before.  This was a bad weakness in the early WiFi days, when we had the WEP protocol, and RC4 was our cipher.  RC4 was a very cool, developed by RSA, proprietary, although you can't keep these things secret, cipher, which economically generated a pseudorandom bitstream.  It suffered from an initialization problem that it hadn't sufficiently randomized itself when it was starting up and so had problems there.  Thank you, Wi-Fi Alliance.



AES counter mode has a different problem, and that is that the AES cipher driven by a counter beautifully produces an unpredictable, tremendously good pseudorandom bitstream.  The problem is we then XOR it with our data, that is, with the plaintext, in order to get the ciphertext.  Now, we know that, sort of contrary to intuition though it is, if you simply XOR plaintext with random noise, which is for all intents and purposes what pseudorandom data is, what you get out is random noise.  And remember that what the XOR does, you can think of it as conditional bit-flipping, that is, where there are one bits in the bitstream, those bits of the plaintext are inverted.  And though it doesn't seem like that's powerful encryption, it turns out there's no way to decrypt it.  I mean, it's really true that there's no way to decrypt it.  So doubtless the people who are doing this thought that, wow, this is really great encryption because there's no way to decrypt this.



What happens, of course, at the other end is that, if the other end has the same counter, so that it's synchronized, and the same key so that the same cipher is encrypting the same counter, it will be able to generate the same bitstream.  So it will produce the same one bits in the bitstream which will reflip the bits that were flipped during the encryption back, and it becomes decrypted.  So, yay, you're back to plaintext.



Here's the rub.  If you know what it is that you're encrypting, then you're able to change the message that is sent.  Think about this.  If you know what the plaintext is, and you have the cipher text, if you XOR the known plaintext and the cipher text, you get back the bitstream.  That's a weakness with this XOR in this situation.



So it turns out that DNS queries have a fixed known packet format.  We know in a DNS query exactly where the IP address is in the DNS packet.  That allows us to take the DNS that the user is using, XOR that IP address with the malicious IP address of a spoofed server, and alter the traffic without, I mean, technically we can decrypt it, but we don't have to decrypt it.  Because we know what it is, even though we have a bitstream that we can't predict, because it's just a simple XOR, that portion can be known, which allows us to change the IP address of all DNS queries in LTE traffic that we cannot otherwise decrypt and route a victim to our own DNS server to perform a DNS spoofing attack.  And they demonstrate it, and they show it, and it is absolutely a real-world vulnerability.



The good news is it's not passive.  That is, it does require a man-in-the-middle position, so like the fake cell tower, the stronger signal cell tower style, the Stingrays.  And of course we all remember, Leo, how many more "cell towers" appeared in Las Vegas in the environment of the Convention Center.



LEO:  And Washington, D.C.  Yeah.



STEVE:  Exactly, during the conference.  So it's no one's imagination that, like, fake cell sites exist.  And that gives you a man-in-the-middle position.  Now, here's the problem is that there's no authentication.  The actual packet has been changed, and this attack is point to point, that is, it's an attack only on the radio link portion, thus LTE.  So somebody has to position themselves so that they look like the cell tower to receive this information, alter the data, and then send it on.  They're unable to decrypt the whole message because they don't know everything else in there.  They're just able to change the IP address or, well, actually they can modify any known data in a known position in any of the packets.   But practically, these guys just demonstrated this as a DNS spoof attack.  But the problem is, since they can't change anything else, they have to forward the otherwise unchanged packets unchanged to the actual cell tower.



Now, the problem is LTE lacks authentication, meaning that there is no way for the recipient to verify that there hasn't been some alteration of the data in transit, that is, since the sender emitted the packet, there's just no way to detect any changes.  We've talked about authenticating and encrypting, how one without the other is, I mean, it provides something, but it doesn't provide what you think.  And it always is necessary to encrypt before you authenticate because that way the recipient authenticates, verifies the authentication before they decrypt.



We have encountered attacks where that was wrong.  It was backwards in some of the early versions of SSL.  That's one of the things they got wrong.  They authenticated, then they encrypted.  And what that allowed was it allowed bad guys to probe the encryption by having the authentication fail after a test decryption by the recipient.  And you don't want to allow that.  So the reason you encrypt, then you authenticate before sending is that the moment authentication fails, you fail, that is, the verification at the receiving end fails.  You immediately fail the entire transaction, thus providing no additional information to an attacker.



So where we are today is we do have a significant problem in the radio link integrity of LTE.  The reason I talked about layers is that they're only able to change this one aspect.  And the good news is the higher level layers provide us robust protection that, given that it's present, that is missing at the lower levels.  We would like to have good protection all the way down to the bottom.  In this case, due to its age and just a fundamental design problem - this was built in a different era, essentially - we now know that it's possible to induce DNS spoofing attacks, and no doubt other clever attacks because we know how clever bad guys can be.  When they understand that they're able to change the data, change any known data in a known position in the radio link, there's going to be some mischief that follows.



LEO:  All right.  You said you were going to - you teased us.



STEVE:  Yes.  So we have a fully feasible attack on LTE radio link.  It's not passive.  But as we know, no one has any idea what cell tower they're connected to.



LEO:  That's right, yeah.



STEVE:  I mean, that's just sort of - you just, oh, look how strong my signal is.  Yes, because there's an evil tower in the next room.  You don't always want to have a strong signal.  However - so, okay.  So the problem is that the design of LTE is where the attack is.  So that's not getting changed any time soon.  I mean, nothing can change that.  That's in all of the deployed cell towers everywhere, and in the baseband processor of our smart phones.  So nothing's happening to change that.



The team of three researchers - they were from the Ruhr University in, looks like Bochum, B-O-C-H-U-M, Germany - and a researcher from New York University were the team that did this research.  They dutifully notified the relevant institutions:  the GSMA, which is the GSM Association; also 3GPP, the third-generation partnership project; and the various telephone companies about the issues they discovered.  But not like any of them could do anything about it.  This is a bug that cannot be patched.  I mean, it's a fundamental flaw in the radio link protocol that we're all using.  And as I said, it's built into every LTE device that we have right now.  So it simply will not be fixed.



The problem is the researchers are also concerned about the future because it turns out, unless some sort of pressure is applied, it's very likely that we're going to see this problem persisting in the 5G standard.  5G is the successor.  It's already being deployed by some large providers, at least in the U.S.  I don't know where it stands internationally, but we're generally sort of behind things.  So I wouldn't be surprised if other - I know that Verizon and AT&T have already begun implementing the 5G protocol.



Of course, 5G is backwards compatible, and therein lies the rub because that means that the newer reliability and security features, for example, 5G does provide authentication, but it's optional.  And as we know from studying the evolution of SSL, you offer stronger protocol options, and it's nice to have them, but unless everybody knows about them and uses them, you don't get the benefit of them.  And you've got the classic downgrade attack problem where - and this is something that a man in the middle could certainly do.  That cell tower in the middle could pretend to be a 4G tower, even though there's a 5G tower further away, but its signal is weaker, so your device locks onto the stronger signal and says woohoo.  Oh, but only 4G?  Okay, fine.  And so now you're open to that level of attack.  And then of course that tower appears to be a 4G device to the 5G tower, which also downgrades itself to 4G.  And so we've made no progress.



So there has been some research already to look into the 5G specification implementation, and it has been found wanting.  Now, in a related technology, we've talked about the SS7.  And I know the SS7, Signaling System 7, is the very old technology, decades old, which is what glues together different providers.  And I know, Leo, on other podcasts I've heard you talking about SS7 and the problems that we know it has.



LEO:  It just can't be fixed.  That's the sad thing.  There's just no way to fix it.



STEVE:  Exactly.  It's deployed now.  It's global.  And it's in place.  So there's a new system known as the Diameter system, which is the formal successor to SS7.  Which, as with SS7, it has been designed to manage the internetwork handshaking.  It's the way a Verizon customer can talk to an AT&T customer is that the Verizon and AT&T systems use at the moment SS7, in the future Diameter, in order to themselves interoperate in order to pass traffic back and forth.  So that's the internetwork handshaking.



But the problem is Diameter is being deployed along with the 5G protocol and also appears to be fraught with problems.  It does provide messaging encryption, which was completely absent from the design of SS7.  But some security researchers have looked at the actual implementation and have found problems there, too.  They said that Diameter misconfigurations that have been spotted are often unique to each network, but they repeat enough that they were able to group them into classes.  There's subscriber information disclosure, network information disclosure, subscriber traffic interception, fraud, and denial of service.  And apparently all of these problems that were known under SS7 continue to dog us under Diameter.



So this shouldn't be a surprise.  Much as we wish this were not the case, if there's a lesson that keeps being learned on this podcast, it's how difficult it is to retire widely deployed technology that is functioning, even if it's not secure, or as secure as we wish it were.  So what do we do?  As I mentioned before, we fall back upon the upper layer, the encryption and endpoint authentication provided by TLS.  Remember that DNS spoofing can take you to a malicious DNS server that can give you a malicious IP for a valid domain.  But unless that domain has a certificate that your browser trusts, and if you are using HTTPS or some other network protocol over TLS, then the inherent authentication at the certificate level, at the higher level, will protect you from that spoofing.



So it's really looking like the move several years ago to we just have to have encryption everywhere, and it has to be authenticated encryption, and the only way we know to do that today is with certificates, so we've got to make certificates readily available.  It's looking like the Let's Encrypt movement, and just the broader availability and awareness of the need for encryption which has become pervasive, has been a really good change to the industry.  And in fact, because we're relying on upper layers to encrypt and authenticate, even if shenanigans are played down at the lower layer, it makes it much more difficult to pull off an attack.



The requirement to have a fraudulent TLS certificate, we know absolutely that anyone who controls a trusted certificate authority can selectively produce such certificates, which is to say certainly any state-level actor.  There's no way that the U.S. federal government, the CIA or the NSA, probably even the FBI under certain circumstances, I mean, technically there's no way they don't have the ability to produce a certificate if they want to which is currently trusted by all our browsers.



The same is certainly true of Russia and China, any of the major players who have control over a certificate authority trusted by our browsers because, as we know, the weakness of the current model is that we need to trust, in order for connectivity to work, we need to trust any of the certificates signed by any of the CAs that we trust in order to establish secure connections.  So it's not a perfect system.  But at least it moves it probably above the level where a casual attacker is able to spoof a website by intercepting traffic and redirecting someone to a fake Google or to a Facebook or something where TLS encryption with a known certificate is in place.



So it's looking like we're going to end up relying on the higher layers.  And that takes me into what will probably be next week's topic.  It's too big to add to everything else I want to talk about this week, but the EFF has launched another encryption initiative which is known as STARTTLS Everywhere.  STARTTLS is an optional protocol which email servers, that is, SMTP servers, are able to advertise their support of.  So traditional email has, as we've often said, has never had encryption bound to it.  Port 25 is the SMTP port where email servers talk to each other in order to exchange and forward email on behalf of their clients; port 110 is sort of the same for client to server, POP; and 143 client to server over the IMAP protocol.  There are now the equivalent, in the same way that 80 is unencrypted web and 443 is encrypted web, there are encrypted ports defined for email, which I'm going to get into in detail next week.



But STARTTLS is an earlier solution which has been moving forward, but hasn't had as much push as it could receive.  I think something like 89% of the current email servers on the 'Net support STARTTLS.  And what this is, it's a port 25 resident protocol.  So it runs over a nominally nonencrypted connection, where over port 25 one SMTP server connects to another, and in the Hello message from the server connected to, it advertises whether it supports STARTTLS.  And what that is, is essentially it is a mechanism for upgrading the unencrypted dialogue the servers are about to have to one which is encrypted.  And as its name suggests, STARTTLS Everywhere, this is the EFF's initiative, which we'll go into in detail next week, to move this; to solve some of the remaining problems behind the deployment of STARTTLS.



But again, way up at a much higher level, this allows then not only clients to obtain encryption, for example, if you're using Gmail, you're already using HTTPS in order to do your webmail interchange between client and server.  But where any of the endpoints in the hops between the source and the destination are not secure, that transit of email can be unencrypted and viewed visibly, and there's no question that there has been a large sucking sound made by the NSA at all of their nodes where they're grabbing traffic.  Once upon a time it was HTTP that was almost never encrypted, except during logins.  We remember those days.  We were talking about them here.  And it's decreasingly the case that email is not encrypted.  On the other hand, email downgrade attacks are possible because STARTTLS is still optional.



So anyway, we'll go into this in much greater detail next week, when we get into what exactly STARTTLS Everywhere is all about.  And I salute the EFF again for another probably, well, certainly useful initiative.  Yeah, very cool.



LEO:  Yeah.



STEVE:  Okay.  So as I was looking at some other research that Aleph Security has produced, I was thinking it's good that the term "mitigation" was used about Spectre and Meltdown.  No one said "prevention."  They didn't say "solution."  They said "mitigation."  Because that sort of says we made it harder.  Which it turns out is all that was done by the browsers.  In Edge, Chrome, and Safari, the Spectre and Meltdown mitigations have been partially defeated.  The research by these guys has demonstrated that, except for Firefox, which incorporated a different mitigation, which they haven't yet defeated, all the other browsers have, that is, the mitigations that were put in place have had the effect of slowing down the rate at which protected data can be acquired through inference using these attacks in browsers, but have not eliminated them.



We talked last week in the context of WebAssembly about the disabling of the SharedArrayBuffer.  There's also something known as "index masking of array objects" is another mitigation; site isolation feature in Chromium-based browsers; reducing the precision of the performance.now timers; and adding jitter to the response of the performance.now API, the idea being make it less certain what time it is.  And since all of this, as I've described several times in the past, is about detecting a difference in performance based on the history of the processor's execution, thus what Spectre is trying to protect us from, it turns out that all that was done was a slowdown was introduced.



Their paper, or their blog posting, is titled "Overcoming (Some) Spectre Browser Mitigations."  And basically they said:  "Because of this vulnerability discovery" - and I snipped a bunch of the top out where it just talked about stuff we all well know about this.  So in the third paragraph they said:  "Because of this vulnerability discovery, browser vendors implemented different mitigations for this vulnerability.  Some of them are meant to disable known methods of querying CPU cache state of memory slots, i.e., JavaScript variables.  These mitigations include the resolution reduction of the JavaScript timer performance.now and adding jitter to its results.



"In our research we were able to overcome the cache access timing specific mitigations.  Although these mitigations cause a serious slowdown in our proof of concept, they are not effective in preventing this attack.  These mitigations also have some negative performance implications and are hurting the functionality of some legitimate Javascript web application use cases."



In other words, these guys took the position that, okay, let's see how much our access to private information is reduced by these mitigations.  And what they found, almost across the board, was they are able to determine one bit per second of secret information.  So it's not a lot; but if a malicious page or malicious JavaScript is allowed to run over time for some period, basically what's been done is the bandwidth channel has been constrained with the addition of this jitter, but that's all it's been.  It hasn't robustly solved the problem, which is why I was saying at the beginning of this that, as I was thinking about this, it's a good thing that we're using the term "mitigation" because we mitigated, but we did not solve or eliminate the problems.  I mean, they are still fundamentally there.



And in some instances it may well be that one bit per second, although it would take two minutes to get 128 bits, that's a key these days.  So if you know which 128 bits you want in protected memory, it only takes two minutes to get it.  Their position is, the position of these developers is, given that these changes are ultimately ineffective in browsers, except Firefox, and I didn't have a chance to dig into what it is that Mozilla did differently, but it looks like maybe Mozilla did something right, or maybe the performance impact is more onerous in what Mozilla did for Firefox.  I don't know.  But their position is, given that all that's been done is the rate at which bits can be obtained has been constrained, and the cost is relatively high in terms of its hurting the performance of specific Java application uses, maybe it makes more sense to back these out and look for some other solution.  I don't know.



But anyway, I just wanted to bring up the fact that there was this interesting piece of work.  There's proof of concept.  They've got their code up on GitHub, and it's able to extract a bit per second from private memory on a browser which is trying to protect against this.  And so, as we've said from the beginning of this year, this is a big problem.



So I've talked a couple times about the increasing use of Universal Plug and Play exposed to the WAN side.  Crazy as it is, there is a large population of browsers exposing their UPnP interface.  And we first talked about this years ago, and I immediately added a test to ShieldsUP! at GRC, our listeners who've been around for a long time will remember, because I wanted to, as quickly as I could, let anybody know if their port 1900, that's the SSDP port, if port 1900 on their router is publicly exposed.  And what is it, 53,000 some, I don't remember now what the count was.  I do show the count, if you go to GRC.com, ShieldsUP!, and then do the UPnP test.  It'll first of all tell you probably that your port is closed.  If you're a listener of this podcast, you've done this already.  Certainly you want to make sure.



So in any of the DDoS attacks which reflect off of servers - and we know that many of them do.  For example, DNS amplification attacks are popular.  You spoof your source IP and make a DNS query to a publicly accessible DNS server, asking it a question that generates a large reply.  And it will send that large DNS reply, it thinks back to you.  But because you have put your victim's IP address as the source of the query, it sends that big response to your victim.  Thus it's participating in a DDoS flood.



So what happens at the victim side is a torrential flood of traffic from DNS servers.  Innocent themselves DNS servers all over the Internet come pouring in.  What's significant is they all come in from port 53.  That is, they come in - because it's a query, DNS is on port 53 - they come in from all of those DNS servers' port 53s.  So the first thing somebody who's mitigating DNS attacks will do is block all traffic coming in on port 53.  It's simple.



Now, if you've blocked all traffic on port 53, then legitimate users of that server or that site, that is, that site itself or somebody in the network would not be able to get DNS from their own external DNS servers.  So first of all, they might not have external DNS.  They might have internal DNS.  But you could whitelist port 53 only for a couple IPs, that is the legitimate DNS servers that are probably not going to be attacking their own network, just probabilistically, but block all the others.



The point is that, from a network plumbing standpoint, blocking a port is simple to do.  It just doesn't take any time at all.  It's a single rule in a router:  Drop all traffic incoming from any IP with a source port of 53.  And bang, that attack is blocked.  The bandwidth is still hitting that router very hard.  But if you're a large ISP, and you've got a distributed perimeter with lots of connections to the Internet, then the individual places where it's entering your network, those bandwidths are smaller, and at least you're preventing them from getting inside.  And the same thing is true, for example, of NTP, the Network Time Protocol.  That is another popular source of DNS reflection attacks.  Again, you generate a query.  You ask what time is it, and you pretend to be your victim's IP, and the network time server blasts its response back to the person who requests it.



So the point is, port-based filtering, as it's called, is a simple and potent solution for DDoS.  And it's what most of these DDoS sites offer is they know that there's no reason to have a torrential input of DNS queries.  So either they will quickly block DNS if it's under attack, or maybe they just leave a block up all the time so that foreign DNS incoming queries are blocked because there's no reason that they should ever be valid coming into that network.  So you can see where any technology which allows the source port of an attack to be other than DNS or NTP or any of the valid services is powerful.  And even better would be if every different stream looked like it was coming from a different port.



And so what's happened over time, and again, entirely foreseeable, but it went from oops, this could be bad, to oh my god, DDoS attacks are now far harder to block than they were.  Because it doesn't take long for the bad guys to figure out how to leverage open UPnP ports for their own purposes.  And as we were saying before, back when this was a theory, we were talking about it as a means of masking traffic in like the sci-fi bounce around seven different nodes around the world before sending your traffic somewhere.  That is, it was possible to expose Universal Plug and Play to not only map the traffic back into the network behind the Universal Plug and Play-equipped router, but to map it to another public IP on the WAN side, outside the network.



Well, DDoS attack tools have now matured.  They are now corralling and bringing to use all of the Universal Plug and Play exposed routers to randomize the source ports of their attack traffic so that what DDoS mitigation providers and services are now seeing are significant floods which are significantly more difficult to block.  Their primary previous tool of blocking on source port because these were server reflection attacks is now, to the degree that Universal Plug and Play routers exist - and it looks like there are many, many tens of thousands of them, and unfortunately probably new ones coming online all the time.  Once they're found, they get grabbed up by a DDoS attacker and then used as a reflection point for traffic which has the ability to randomize the source port, making DDoS attacks way more difficult, significantly more difficult to block.



And my last bit of coverage, I wanted to mention WordPress.  They are a sponsor of the TWiT Network, so full disclosure there.  But this doesn't really impinge upon that.  We were just talking last week about path traversal bugs.  And it turns out that all versions of WordPress have a known path traversal bug vulnerability, but one which apparently doesn't have the WordPress devs very worried since they've known about it since last November.  So I saw a bunch of hair-on-fire stuff last week, and I just wanted to explain to our listeners, if anyone was wondering or worried, exactly what is going on with this.



WordPress, as I mentioned, was notified of this last November by security researchers who made the discovery.  And I imagine it'll be fixed at some future point when WordPress does another rev of their work, if they choose to.  I would be surprised if WordPress hasn't been revised in nine months.  But this problem is still there.  So maybe it fell through the cracks.  In any event, there is no chance that it could be used for a widespread attack, which is probably why it hasn't gotten more attention.



The flaw was discovered in the PHP functions that are used to delete thumbnails for images uploaded to a WordPress site.  What the researchers discovered was that users who already have access to a site's posting editor and can thereby upload or delete images and their corresponding thumbnails have the ability to insert code into the site which could delete crucial files which are part of the WordPress content management system core.  Which is something that should not be possible without direct access to the server's FTP login or the server itself.  So all that can be done is that some files that should not be deletable can be, and only by somebody who already has credentialed login capabilities to the site.



So maybe, and this is what's been postulated, it could be a limited form of privilege elevation attack, although it's not immediately clear how useful deleting other files would be.  What's been suggested, again, sort of far-fetched, is that a site could be hijacked by deleting the main wp-config.php file, which is the site's main configuration file.  If that were done, then it might be possible to reinitiate the installation process and install the site using a different set of database settings to essentially hijack the domain and deliver custom or malicious content.



But again, this is like, okay, the WordPress devs have understood that this problem exists since November.  It requires that you already have the ability to post and delete and that with that you then leverage this vulnerability in order to make it happen.  It wasn't clear to me yet that even now there were like a proof of concept or it had been made public.  However, the guys who discovered it, if this is a concern to anyone, there is a hot fix which they have produced which can be downloaded and added to the functions.php file on the site's active theme in order to prevent this.



But anyway, I wanted to - the headlines were, oh my god, any WordPress site can be taken over.  It must be that, since the devs know about this, there's no way that it could be mass exploited, and it seems hard to imagine that it's going to be useful to anybody who has access to the WordPress site itself anyway.  And I'm pretty sure it'll be fixed as soon as WordPress gets around to it.  And I looked for some comment from WordPress, but I was unable to find anything either way, although that shouldn't be considered authoritative.



I did get an interesting note from Ben in Greensboro, North Carolina that discusses an aspect of SpinRite that we've touched on, but not deeply, which caused some confusion for him.  He said:  "SpinRite recovers bad sectors, but afterwards SMART status shows 'good'?"  He says:  "Hi, Steve.  Long-time Security Now! listener and SpinRite user/abuser."  I'm not sure how you abuse SpinRite, but okay.  He says:  "I recently came across a situation I found odd.  A client's machine was having issues staying booted and BSODing."  So we know that's Blue Screen of Deathing.  "So, naturally, the first thing I did was run SpinRite at Level 2 on the Windows partition, and it recovered SEVERAL" - he has in all caps - "bad sectors," he says, "which typically tells me that the drive is going bad."  Ah.



Anyway, he says:  "Of course this fixed the trouble, and the system then worked flawlessly.  But since the machine was still under warranty, I figured that I might as well get the drive replaced.  So I contacted the manufacturer's tech support, and while on the phone it passed their built-in drive diagnostic, Windows WMI SMART check, and all tests showed the drive as good."  He says:  "Usually when SpinRite recovers sectors for a machine that won't boot, it will fail any subsequent SMART checks.  But not this time."



He says:  "Do I have a fundamental misunderstanding of what recovery of a sector means?"  He says:  "I'm currently running a second Level 2 scan on the whole C drive to verify it's operational before giving it back to the client, and the ticket with the manufacturer temporarily is still open," he says, "just in case I come across something."



So, yeah.  We sort of touched on this a couple weeks ago, where we had - it was that laptop that was being bounced on the coffee table by somebody whose wife was using it for, I think, Photoshop or something.  And he ran SpinRite to fix the problems, and then as he saw the laptop bouncing on the coffee table as she dragged it over to show it to him, he thought, well, I'm going to probably have to run SpinRite again.



First of all, drives are very resilient.  They obtain the resilience through having a large pool of spares.  And sparing out defective sectors is just now something drives do in the regular course of business.  One of the ways SMART can cause an error or SMART can show that the drive is in trouble is if the error rate starts going up to a point where the drive thinks, okay, something is not happy here somewhere.  So that can happen.  Another way that the SMART system can show trouble is if the pool of available spares starts being consumed at a high rate, or if the number of available spares starts approaching zero.



So I guess the point is that drives are surprisingly dynamic.  And that's probably, I mean, that's certainly one of the things that SpinRite works with when it's working with drives, is it's watching the SMART status while it's running.  It's learning about the drive.  It's watching the rate at which error corrections occur and the rate at which sectors are being spared out.



In the case of recovering bad sectors, the sector is bad only inasmuch as the drive was unable to read it normally.  SpinRite read it, fixed it, the drive replaced it with a new one, and the drive's kind of fine now.  So as long as the errors are not occurring too often to upset the drive, which it would then report in the SMART status, and as long as there are still sufficient spares for SpinRite to sort of do this kind of work, or the drive maybe to do them on its own if it's able to, then we're sort of in this elastic phase of the drive where, yeah, it's not perfect, but with the density that we're storing data on drives these days, no drives are.  And so everything's okay.



So I would say, Ben, if you run that second pass, and I imagine that Level 2 showed everything was fine, since SpinRite fixed it the first time you ran it, and SMART still says everything's good, the manufacturer's going to say, "We don't see any problems here."  I think you're probably good to go.  It's just probably the fact that somehow a couple sectors suffered some damage.  But the truth of today's drives is that's kind of happening all the time.  SpinRite works hand in hand to maintain them.



And I often see, I don't talk about it because it's sort of a non-story, but I often have people writing, saying, you know, I don't have any SpinRite miracles to report because I run it periodically for maintenance.  And that's what happens is SpinRite is able to help the drive see these problems before they become critical.  And so it sort of enhances the drive's built-in resilience and keeps any problems from actually biting anyone.  So that's a good thing.  And that's SpinRite.



LEO:  Indeed it is.



STEVE:  And with any luck, WPA3 will help us do that better.



LEO:  Maybe.



STEVE:  Yeah.



LEO:  But is there something wrong with WPA2?



STEVE:  Oh, yeah.  There is a known password attack known as the KRACK attack.  If somebody sniffs your authenticating to an access point, they can then perform an offline brute-force attack.  So consequently the strength of your password to the access point is important.  And so in various settings where maybe the password is sort of predictable or guessable, like it's the car dealership or a company that may have their name or address or something in the password, there is some vulnerability.  So there is a known problem with WPA2.  But the good news is WPA3 fixes not only that, but offers some new features which we've been wanting, everybody's been wanting for a long time.



So it's been 15 years since WPA2 arrived back in 2004.  So that's been around a while.  And you might say that's 14 years, since it's 2018, except that we're not actually going to get WPA3 till late next year.  So, okay, it will be 15 years by the time we actually get WPA3.  It's unclear whether, just because of the Wi-Fi Alliance and their stickers and their certification programs, whether existing hardware will be upgradeable.  We have to imagine that our smartphones, like Apple and Android and widely deployed devices, will be upgradeable.  This probably requires the baseband processor to be upgraded, which I assume can be done over the air by the phone provider.



But it's not clear whether we're all going to have to get new WiFi routers if we want to use this, or whether a firmware update would be able to bring these features to us.  Again, this is all licensing based and certification based, and the Wi-Fi Alliance is the Wi-Fi Alliance.  And as I said, I got excited this morning by the idea that they had published the specs, and it turns out it was just an absolute tease.  So it was like, okay, fine.



So it is of course backward compatible so that any device that isn't WPA3 will be able to fall back to WPA2 functionality.  There are two flavors.  There is WPA3 Personal and WPA3 Enterprise.  The Personal brings better protections to individuals by, as I mentioned, and as you prompted me for, Leo, providing more robust password-based authentication.  It's known that there is a weak password authentication problem.  WPA3 uses something known as SAE, Simultaneous Authentication of Equals, which replaces the longstanding PSK that we've talked about and pounded on for years, the Preshared Key approach, which was built into WPA2.  So this SAE, this Simultaneous Authentication of Equals technology, is resistant to offline dictionary attacks which the KRACK attack makes possible under WPA2.  So that strengthens WiFi.



And again, I don't have to tell everybody that the world has just become WiFi.  I mean, it's incredible to me.  Once upon a time it was do you have WiFi?  Now it's like, well, do you have a cell phone?  It's like they just - they don't ask you.  They say, "What's your mobile number?"  So it's like, "Yes, what's your WiFi?"  So it's just ubiquitous, obviously.  So it was the KRACK attack, K-R-A-C-K, which stood for Key Reinstallation Attack.  And at the time we covered it on the podcast extensively.  So if anyone is interested in a refresher, you can find the podcast [SN-633], KRACK, K-R-A-C-K, which is an attack on WPA2 authentication, which allowed someone sniffing the transaction to grab it.  And remember also that it was possible to deauthenticate anyone by sending a deauthenticate packet to the access point, which would force a reauthentication, allowing an attacker to capture the authentication traffic if they showed up too late otherwise, and then launch the offline attack.



So a determined attacker who is either passive and patient or active would be able to, given the complexity of the password - and WiFi passwords probably suffer from a lack of sufficient complexity just because people don't think that's important enough.  That could be a problem.  So the Wi-Fi Alliance brags that this WPA3 allows users to choose passwords that are easier to remember because a traffic analysis attack which WPA2 does suffer from would not function, would not succeed under WPA3.



Oh, and WPA3 also gives us perfect forward secrecy.  So we know what that means.  That means that traffic captured now cannot be later decrypted if in the future the key is determined.  So perfect forward secrecy is another good feature of encryption which arguably all modern crypto should have.  And when we have WPA3, we'll have it in our WiFi for the first time.  We don't have that now.  We have that in the higher level links in TLS, so it's good to have it there, but not at the WiFi link.



There's also an Enterprise version of WPA3, as there is sort of nominally for WPA2, which increases the encryption key strength.  And it's not clear to me why they just don't have that in WPA3 Personal.  But I guess they want to have a higher class grade.  In their scant coverage of this, they said that WPA3 Enterprise offers an optional mode using 192-bit minimum-strength security protocols to better protect sensitive data.  And it uses just larger communications protocols.  Presumably it requires a little more oomph at each of the endpoints, and so it may not be something that a weak IoT device wants to or needs to deploy.  And again, it's not clear to me that there's a compelling need.  But of course the Enterprise WPA offers Kerberos key negotiation and lots more fancy features that the typical end user just doesn't need.



However, in terms of good features, we've got a couple things.  Top of the list, I think, well, there's two.  We now encrypt opportunistically open WiFi networks, which is like, yay.  Why has this taken this long?  We've often talked about it.  I've lamented its lack because it is so easy to do.  I was talking about it just a couple weeks ago, how in a world where we have Diffie-Hellman encryption, even though you don't have authentication, which means you're still susceptible to a man in the middle, with Diffie-Hellman key exchange - that's what I meant to say, sorry, Diffie-Hellman key exchange - in plain sight, endpoints can establish a secret that no passive observer can determine.



So for a long time there's been no need for unencrypted WiFi.  You could have WiFi where you walk into any coffee shop, or car dealership waiting for your car, or airport, and it's like free WiFi, which is also per connection encrypted.  But we haven't had that because the Wi-Fi Alliance didn't give it to us until now, and until next year, late next year.  So there is something known as OWE, Opportunistic Wireless Encryption, which is defined by the IETF.  It's an IETF-defined RFC 8110, which the Wi-Fi Alliance has adopted and will be part of WPA3.



So unfortunately, it will require the retirement or the upgrading of each endpoint for us to have this, but it does mean that, in the absence of an active man-in-the-middle attack, because remember that Diffie-Hellman key exchange doesn't authenticate, it allows - as long as you know who you're talking to, thus the authentication component, it allows a visible exchange of data to establish a secret between two parties.  But if somebody gets in the middle and pretends to be the other person to each end, then that person ends up establishing secrets with each end and is able to decrypt the traffic moving through.  But still, much better than nothing.



And the last new feature of WPA3 is what they call Wi-Fi Easy Connect, which they're not describing, and I hoped to get it, but it was one of those I got the seven-page teaser with acronyms defined, okay, thank you.  They did say that it uses visible QR codes printed on access points and IoT devices, and that it uses public key crypto.  So we can pretty much reverse-engineer what it is from that.  What that will mean is that a router will have a factory-set asymmetric private key burned into its firmware with the public key shown as a QR code on its exterior label.  And the same will be true of an IoT device.  It'll come with a per-device private key, and printed on it will be a little QR code, which is the matching public key.



And as we know, just that allows us to solve this problem.  It allows a device with a camera to obtain the public key for which the matching private key is only known to the device on which that public key is printed.  The public key would allow the device seeing the QR code to generate a random number, we'll call that the ephemeral key, encrypt that under the public key, which can then only be decrypted by the device's secret private key.  So it allows the secure establishment of a temporary link because it doesn't matter if somebody even in this case is a man in the middle.  A man-in-the-middle attacker can't change anything or has no way of decrypting it.



Essentially, you can think of the QR codes as optical channel out-of-band information exchange.  It's out of band in that it's just sitting there as a label, and if you don't have optical access to it, then you have no way of knowing what the device's public key is.  So they don't explain how this works in detail.  They talk about a device like a smartphone being used for a device that has a limited user interface.  So a router has a very limited UI.  And a light bulb or an IoT device, a burglar alarm switch on the door or something, very limited UI.



So the idea would be you just scan the QR code with your phone.  It then participates in this interchange in order to get the devices set up in a way that is - now, and people have been setting up IoT devices using a temporary hotspot in a smartphone for a while.  The problem is, it is vulnerable to an attacker who is present watching that process occur.  So there's a window of vulnerability.  That's what this eliminates.  And it also means that just moving forward, anytime you want to establish a secure association, if there's an optical channel that allows that to happen, it can be done with true security.



So they're calling it Easy Connect.  We know from having discussed WPS, that was the previous easy way of connecting that ended up being horribly flawed because it turns out it was an eight-digit code that could be chopped into a four-digit and a three-digit because the last digit was calculable, was just a check digit.  And it allowed the whole thing to be brute-forced very easily.  So we're going to hope that the Wi-Fi Alliance has not made any similar mistakes.  In any event, we're getting a bunch of welcome features.  We'll have to wait another year and a half, and then it will require that hardware be changed and updated.



But as that happens over time, I mean, given that we were with this last WPA2 spec for 15 years, there'll certainly be time for us to be obsoleting and replacing devices with WPA3.  And so we just get more security.  And again, as with the case with LTE, we would like these radio links to be secure.  But to the degree that they are simply the carriers of the lower layer traffic of sessions which are themselves much more secure and authenticated, we could argue that, so long as what they are carrying is providing for its own security, then the worst that can be done is a denial of service attack, or things break for some reason.  But there's no active vulnerability or lack of privacy or information disclosure.



So anyway, that's WPA3.  I'm sure we'll know much more about it.  I hope we know more about it as we get much closer.  It'll be about a year and a half from now.  So I'm sure we could calculate which episode of Security Now! that will be.



LEO:  And it comes in conjunction with 802.11ax; right?  It doesn't require it, but I know that that's part of the improved spec for ax.



STEVE:  Yeah.



LEO:  Which also is a year and a half out.



STEVE:  Yeah.



LEO:  Okay.  Thank you, Steverino.  We now know everything we ever would want to know about WPA3.  Well, no, that's not true at all.  We know everything they'll let us know.



STEVE:  Yeah.



LEO:  Then get back to us.  Steve does this show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to stop by and watch, please do.  We'd love you to do that.  Just go to TWiT.tv/live to watch the live stream.  You can see us making all the shows we do roughly at those times.  If you do that, do go into the chatroom because that's a big part of the behind-the-scenes crowd.  Irc.twit.tv is the place to go to talk to the persons involved.



You can also get the show on-demand.  Steve's got copies and transcripts.  It's the only place you can get the transcripts of the show at his website, GRC.com.  While you're there, pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility, and all the other freebies.  SpinRite's his bread and butter, but everything else is free at GRC.com.  We have audio and video at TWiT.tv/sn.  And of course, as with all our shows, you can subscribe with your favorite podcast program or application.  Just look for Security Now!, and that way you'll get it automatically every week, the minute it's available.



Steve, have a great week.  Enjoy your Fourth.  You doing anything fun in Irvine?



STEVE:  Nope, just going to hang out with Lorrie and enjoy the day.



LEO:  You should.  It'll be beautiful, I think.  



STEVE:  Yup.



LEO:  We'll see you next week on Security Now!.



STEVE:  Okay, buddy.  Thanks.



LEO:  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#671

DATE:		July 10, 2018

TITLE:		STARTTLS Everywhere

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-671.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss another worrisome trend in malware, another fitness tracking mapping incident and mistake, something to warn our friends and family to ignore, the value of periodically auditing previously granted web app permissions, and when malware gets picky about the machines it infects.  Another kind of well-meaning Coinhive service gets abused.  What are the implications of D-Link losing control of its code-signing cert?  There's some good news about Android apps.  iOS v11.4.1 introduces "USB Restricted Mode," but is it?  We've got a public service reminder about the need to wipe old thumb drives and memory cards.  What about those free USB fans that were handed out at the recent North Korea/U.S. summit?  Then we take a look at email's STARTTLS system and the EFF's latest initiative to increase its usefulness and security.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, we're going to talk about a hack to Fortnite cheaters.  Couldn't happen to a nicer bunch.  Also why your browser might be freezing.  More privacy leakage from a fitness app.  And at the end Steve's going to talk about a new way, a new proposal to keep email secure.  Actually, it's an old proposal that might be getting new life.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 671, recorded Tuesday, July 10th, 2018:  STARTTLS Everywhere.



It's time for Security Now!, the show where we cover your security and privacy, explain it all in-depth, thanks to this guy right here, Steve Gibson of the GRC Corporation.



STEVE GIBSON:  I love it how you turn around to look at me, Leo.  It's like...



LEO:  Well, you're right behind me.  You're over my shoulder.



STEVE:  I'm right there.  I'm right there.



LEO:  If I don't look around, I don't see your salute.



STEVE:  That's right.  Got to have the Vulcan whatever it is, salute.



LEO:  The ancient Yiddish salute, yeah. 



STEVE:  So as we expected, when I added an extra "T" to STARTTTLS, and it's actually just STARTTLS, for those who were...



LEO:  You could say it's STAR TTLS.



STEVE:  Or I could be stuttering, STARTTTTTTLS.



LEO:  There's a lot of T's in there.



STEVE:  I can't count them all.  As expected, we have a lot of interesting fun stuff, but nothing dramatic that pushed back my intention, as I mentioned last week, to talk about the latest EFF initiative, STARTTLS Everywhere, which should remind us of HTTPS Everywhere.  And there are a lot of overlaps and similarities, but of course some differences.  This is the effort to push email encryption further towards, well, let's just say usefulness because it isn't very right now.  And this is not PGP or S/MIME or you encrypt it, then you send it.  This is an attempt to shore up sort of this weird intermediate solution which had actually kind of already been abandoned.  But as with everything on the Internet, if it kind of works, then it's really hard to kill.  So rather than trying to kill it, they're trying to fix it.



So we're going to end the podcast talking about that, sort of understand where we are with not additional encryption, which of course an end user can always decide they want to employ, which would give you true end-to-end protection.  As a consequence of the way email works, this doesn't, even in the best of times, you don't get end-to-end encryption, I mean, even using the latest solutions and systems, because email is a multipoint hop system, store and forward, by design.  And so essentially any servers along the way are man in the middles who get your unencrypted email.  Again, when everything is working perfectly.  So this is why I've never been super excited about the prospects of encrypting email unless an individual goes to extremes.  But anyway, we're going to wrap up with that.



First, we're going to talk about another new and emerging worrisome trend in malware, yet another fitness tracking mapping incident and mistake, something to warn our friends and family to ignore if it happens to them, the value of periodically auditing previously granted web app permissions, and some malware that has actually become picky about which machines it infects and what it does depending upon what it finds.  We have another kind of well-meaning Coinhive service which didn't take long to get abused.



We're going to look at the implications of D-Link having lost control of its code-signing cert, which did happen.  Some good news about Android apps.  We will talk, as you did on the previous podcast, Leo, about last night's iOS update to v11.4.1, which among other things introduced something new that the user has control over, known as a "USB restricted mode," and whether it actually is, or at least something to be aware of as a wrinkle in the way it works.  We have a public service reminder about the need to wipe old thumb drives and memory cards, I mean, wipe them proactively before you let them out of your control, and what a team in the U.K. discovered when they just bought a bunch.



Also, the Picture of the Week is just - I got a kick out of this, and our listeners will.  It turns out that free USB-powered fans were handed out at the recent North Korea-U.S. Summit, where our fearless leader met theirs.  And of course it really upset the security mavens, who were like, wait a minute, folks, those were made in China.  Please don't plug them into your phones without - it's like, what could possibly go wrong?



LEO:  Was this about the one that just happened, or the one with Trump earlier?



STEVE:  The one - it was the 14th.  It was the 14th.



LEO:  Yeah, Pompeo was there talking to them last week.



STEVE:  Right, right.



LEO:  So this is...



STEVE:  So this was the first one.



LEO:  The first one.



STEVE:  And then we...



LEO:  I could just see some - I could see, you know, god, you can just see Donald Jr. picking this up, saying, oh, this is great, this is great.  That'd just keep me cool in the summer.



STEVE:  Well, and it looks like it was nicely designed from the photo, which is our Photo of the Week.  I used the caption "Free Chinese-made USB fans handed out during the recent North Korea/U.S. political summit.  What could possibly go wrong?"



LEO:  So I'm guessing on the other side of that wire is a USB plug.  You plug it into your laptop, it's got a little gooseneck so that you can aim the fan at you while you're sitting in front of your laptop.



STEVE:  Well, actually to me it looks like the fan module itself is a USB Type C.



LEO:  Get it, I get it, yeah, yeah.  There's a Lightning on it, as well, yeah.



STEVE:  And then they have a little - exactly.  So you could plug...



LEO:  So it's for your phone.



STEVE:  Exactly.  And you would plug it, and the way it's tilted, you would plug it into the bottom of your phone where the power connector is, and that would give you a fan at the bottom of your phone facing your face.  So it's, like, convenient.  And you can imagine the security people were quite upset.



LEO:  USB-protected mode on this one.



STEVE:  You need a USB condom is what you need.



LEO:  Well, do you, though?  I mean, I guess phones have data access.  But there's not, I mean, what would they do with this?  Could it infect a phone?



STEVE:  Yeah, it could have malware.



LEO:  Yeah.



STEVE:  And that was the concern.  Anyway, we will talk about all of that in the fullness of time, Leo.



LEO:  Yes, yes.



STEVE:  Before we let our listeners go off about their normally scheduled lives.  They are captive until then.



LEO:  We like to keep them captive.  On we go, Mr. Gibson.



STEVE:  So I've heard you talk about Fortnite a lot.



LEO:  Yes, love it.



STEVE:  Amazed by the amount of money that those guys make for a game that they give away for free.  Predictably, there would be add-ons, and we've seen this with all games which achieve stardom and fame, which are cheats of various sorts.



LEO:  Yeah.  It's a big problem with multiplayer online games because, if you're playing against somebody who cheats against you, that really sucks.



STEVE:  Yeah, yeah.  And it is, you know, it's technology which is prone to manipulation.



LEO:  Right.



STEVE:  So there's a service known as Rainway which is - it's sort of similar to PlayStation now.  It's recently out of beta, earlier this year.  It's a videogame streaming service that allows users to run games on their PC and play them on devices over an Internet connection, play them on other devices over an Internet connection.  So sort of like Remote Desktop for gaming.



Okay.  Last week Rainway engineers, some of whose customers were naturally playing Fortnite because it's so popular, began detecting infections in their users' systems when the Rainway server logs began reporting hundreds of thousands of errors.  I think 308 or something thousand errors that they had logged in a short period of time.  The errors were the result of ads that had somehow been injected into user traffic.  Rainway uses whitelist filtering to permit its customers to connect only to approved URLs, so those connection attempts were being caught and blocked.



And note that, like, for non-Rainway users, this wouldn't have happened.  So it was just sort of a coincidence of the fact that some Fortnite users had downloaded a cheat whose behavior was being, like, raising flags in the streaming service that they happened to be using for playing videogames.  And that service had the foresight to whitelist specific URLs, blocking other ones, which then raised the flag.  Otherwise this would have - who knows how long it would have taken to catch this.



So it turns out that the attempts that were caught were fraudulent content injected into the sessions of these Rainway customers, attempting to connect to two different ad platforms, Adtelligent.com and SpringServe.com.  And there was also unauthorized JavaScript that was bound to those ads.  Which, again, this is sort of like, what?  The Rainway engineers were like, okay, what is going on here?  So this strongly suggested to Rainway that something was going on.



Rainway examined the profiles of the affected users, looking for common threads, because initially all they were seeing was just errors.  They didn't know what their users were doing that was causing this trouble.  They had no hardware in common.  Their ISPs were different.  All of their systems were up to date.  But one thing stood out.  All of them were playing Fortnite.



So then, suspecting that some malware might have been spread by one of the many Fortnite cheating hacks available, the Rainway guys started downloading thousands of add-ons, looking for something that might be causing this trouble.  And they found it.  They found an add-on which was promising in-game currency, I guess it was called V-Bucks is the currency within that world, and also promising to equip users with an "aimbot" which would automatically aim the player's gun at their opponents without any need for particular precision by the player.  Oh, and what they did was they scanned all of these different add-ons, looking for the strings which they were seeing being emitted and captured in their logs.  That's how they found this one.



So they then ran the add-on in a virtual machine and discovered that, among other things, it installed a self-signed root certificate into the user's playing platform, into their PC.  And of course, as we know, that would enable a man-in-the-middle attack on every HTTPS website the user visited.  And as they continued to look at it more closely, they discovered that it was indeed intercepting - it was using this newly installed self-signed root cert to perform a man-in-the-middle attack on all of the web pages that the game players were visiting on the fly to insert references to ad platforms.



So they reported the rogue malware to the service provider who was hosting it, and that provider immediately removed the malware and let them know that it had been downloaded 78,000 times.  So there are 78,000 instances of Fortnite players who had downloaded a cheat and got themselves hacked as a process, and had a rogue self-signed certificate stuck into their root certificate store in order to perform this basically ad fraud with their system.  And at the time of their reporting, Rainway reported 381,000 interceptions of blocked URL attempts in their own logs.  And I'm sure only a fraction, tiny fraction of Fortnite's users happen to also be using the Rainway service.  So the infection is probably massive.



The Rainway guys also reported their discovery to the Adtelligent and SpringServe ad platforms, who identified and pulled the ads from their platform.  And in Ars Technica's coverage of this, Dan Goodin reached out to Epic Games, who is the maker of Fortnite, but Epic declined to comment.



So I'm beginning to see this behavior, and I wanted to sort of put it on our listeners' radar.  When I say "this behavior," what I mean is instances of self-signed root certificates being stuck in people's root stores.  So I wanted to make sure that everybody understood what that meant.  A CA certificate that we often talk about how there are now thousands of them that we trust, what those certificates are, are self-signed certificates, that is, it is a certificate that no higher authority vouches for.  The authority is essentially vouching for itself.  So it signs its own certificate so that the certificate has a valid signature.



But the signature is valid only because the certificate is in the root store.  So when Windows or whatever platform, Linux, iOS, anything that is using root certificates, when the system checks the certificate, it finds it in its own root store because that's where the certificate is.  So it's automatically trusted.  But more importantly, the certificate is also the root of a chain of trust, meaning that any other certificates that come along are checked against the certificates in the root store to see whether they should be trusted.



So when malware drops a self-signed certificate into the user's root certificate store, what's happening is like really bad.  It's what you don't want because, as we've talked about, we already sort of have a fragile public key infrastructure based on trusting anything that is signed by any of the certificates in our root store.  And so this is where the CAs that have gotten in trouble, like famously Symantec did a couple years ago, and essentially just lost their certificate signing business as a consequence, if their certificates can't be trusted, they get kicked out of the root store, and it's game over for them.  So as I said, I'm running across more instances of this, which is why my radar perked up when I saw that this is now the behavior we're seeing.



Once upon a time, before all web traffic was encrypted, that is, when most web traffic was still HTTP under the somewhat mistaken belief that adding security was too processor intensive, it would slow things down, that really wasn't, except in the very early days, that wasn't the case.  It was just inertia.  It was just everything seems to be running fine.  Again, nothing seems wrong - except when we finally woke up to the fact that our session tokens were in plaintext cookies that allowed a passive eavesdropper to jump onto and impersonate a user simply by grabbing their cookie that was passing in the clear in any open WiFi scenario, for example, and be logged on as them.  So as we know, pretty much now, thanks to another EFF initiative, Let's Encrypt, but also just even before then, just a raising of the awareness of the importance of encryption, suddenly it was no longer feasible to alter web pages and inject content.



So unfortunately, what this has done is it's put pressure on the system which is trying to protect us, with the consequence being that malware is now routinely - or maybe not yet routinely, but again I think we're going to see this as an emerging trend - is saying, okay, fine.  All of the software for creating a little HTTPS intercepting server is open source and in the public domain now.  All we need to do in order to terminate the connection ourselves, just like a corporate middlebox does, which we've often talked about, is drop a certificate into the user's machine, just like a corporate middlebox does that wants to filter for antivirus and provide content filtering for the people being protected within the network.



So we've talked in the past about auditing our certificate root stores.  And in fact I played with one for a while that dynamically looked to see what certificates were being used and raised a flag.  I ended up having to abandon that particular tool because Google is themselves a CA, and I'm a user of Google properties, and they were constantly churning their own certificates.  They have every right to do so, but it was just causing - it was raising false positives all the time of certificates changing.  And so I finally just said, okay, well, this isn't working as it is.



But I think that what we're going to need to see, we're going to need to have our certificate stores hardened more than they are because they shouldn't need to be changed often.  I mean, I would argue that there's really not a good reason for the certificate store to be populated and managed by anything other than the official maintenance system of our OS vendor.  So, for example, in the case of Windows, it should only be Windows Update that has the ability to mess with that.  I think only due to, again, historical reasons is it something that is browsable, and you can poke in and look at and decide if you want to mess with it.



I was doing that myself just a couple weeks ago, needing a localhost server on my system in order to do some work.  And it wasn't hard to just drop a self-signed localhost cert into my machine in order for a server using it to be trusted.  So there are valid reasons.  But you should have to jump through all kinds of hoops to do that.  That should not be something that random malware that gets onto your system is able to do because I think this is, as I said, this is the next thing we're going to see is more instances of this happening in the future.  I think this is something we need to lock down, and so far that hasn't happened.



In another piece of global fitness tracking mapping gone wrong,  our listeners will recall that I think it was in 2016 we talked about the Strava fitness tracking app which was just sort of publishing all of the positioning information of its users because everyone kind of thought that would be cool, to look at maps of where people were using their app.  Unfortunately, it turned out that there were people on secret military bases going out for a jog around camp who were being logged, and satellite-looking maps had this Strava fitness tracking data overlaid on it, and suddenly there was a sort of suspicious lot of activity in areas of the desert where nothing was shown on the maps.  And so anyway, we talked about it at the time.



Well, it turns out that some investigations carried out by reporters from a Dutch newspaper, De Correspondent, and online investigations group Bellingcat have found another instance which caused Polar, the well-known - I think they're Dutch, aren't they?



LEO:  I don't know.



STEVE:  No.  I don't remember.  Polar is Finnish.  Finnish, yes.



LEO:  Finnish, yeah, that's right,



STEVE:  Yeah.  So what they found has caused Polar to suspend its global activity mapping feature.  It's kind of not Polar's fault.  But, well, anyway, so here's what happened.  The two groups of reporters discovered that Polar Flow, which is one of Polar's apps, was allowing anyone access to a feature called Explore, which is Polar's activity map.  The data exposed on this map included a user's past activity - running, biking routes and so forth - but included the user's personal details, such as their heart rate, various fitness attributes, and more.



Well, what Polar did was they made the mistake of exposing the username and personal details of each individual for each individual activity and route.  And upon digging into it deeper, it looks like the user needs to explicitly allow that information to be shared, but they probably didn't understand fully the consequences.  The reporters tracked down, were able to thanks to this disclosure, the real world identities of specific intelligence and military personnel.  The journalists used this feature to search the map for the location of known military bases and intelligence agencies' headquarters and training grounds.  They were then able to identify the people who reported fitness activity history at those locations.



And in several cases the researchers were able to identify the usernames, which led back to real-world identities, either because the military and intelligence agents used their real names for the Polar app or because they reused usernames that were associated with them elsewhere, and the reporters were able to make the connections.  Oh, and also in some cases the same routes which they ran while jogging on base also contained the jogging and biking routes in other locations for the same person, which then allowed that linkage exposing what looked like their user's home address.



So again, in response to these revelations, Polar immediately shut down the global activity mapping feature, but they then did seek to clarify that this Polar Flow app does not expose the user's activity and username by default.  Polar said that these details are shared only based on an opt-in system, and the data exposed via its activity map had to be willingly shared only by - or had to be and was being shared only by some of its users, with the majority of their users' data remaining private.



So I guess I would say that this is clearly a privacy concern, and anybody who is using this really needs to have it be made clear.  I mean, there's some tension here; right?  Polar, much like Strava, wants to brag about how many people are using their stuff.  It's people all over the world.  Look at how busy everybody is getting fit and healthy.  The flipside is, I mean, it is an information disclosure.  And so people, I think, who are saying, yes, I want others to be able to see my little red lines where I'm running, and/or my username and other data, I want to share that, really need to be made aware of the downside consequences from a privacy standpoint.



LEO:  I don't think you can blame Polar Flow for that.  I mean, yeah, maybe it's good for Polar to show how many people use their app.  But that's the point of it.  And Apple does the same thing with their activity rings.  You can share with other people, and then you compete with them and so forth.



STEVE:  Do you share it with specific people in the Apple case, though?



LEO:  Yeah.  In the Apple case...



STEVE:  Because this was shared globally.



LEO:  Yeah, but...



STEVE:  This was a global and, you know, share...



LEO:  So there were two things that were wrong.  I mean, clearly, if you're an NSA intelligence officer, and you're publicly sharing your run route, you need to kind of go back to school and learn some op sec stuff.



STEVE:  That's right.



LEO:  But there seems to have also been a problem...



STEVE:  You were absent for that day of training when they said...



LEO:  Uh, what?  But, I mean, it's actually not a surprise.  There are a lot of military folks in great shape.  And so they're exercising.  So, you know, clearly they're at fault for making it public.  The problem really also was there was an API that was exposing this, and they used sequential index numbers for customers.  So you could just query the API and get this information.  And that's clearly Polar's fault.



STEVE:  So a poor design from an anonymizing standpoint.



LEO:  Yeah, yeah.  But, I mean, there's lots of apps that, you know, Runkeeper does this, too, where you share your run.  And people like to do that.  They like to, you know.



STEVE:  Yeah, yeah.



LEO:  They probably shouldn't be doing it if they're in a secret location.



STEVE:  Probably not if your location should not be disclosed.



LEO:  Yeah, especially since this whole thing came up already, and you'd think...



STEVE:  Yes, exactly, two years ago it made the news.



LEO:  ...the memo would have gone out.



STEVE:  And it couldn't have been any more specific.



LEO:  It was a running program.  It was exactly the same.



STEVE:  Right.  Okay.  So Leo, on your Tech Guy show for Saturday and Sunday for three hours on the weekends, there's no question that people have called you asking what this is.  And it uses an interesting hack which I wanted to talk about and just sort of put it on our listeners' radar.  I doubt that any of our listeners would get caught out by this.  But probably they have friends and family who might.  So it turns out that freezing someone's browser is a thing.



LEO:  Oh, that's interesting.



STEVE:  Yeah.



LEO:  Because we do get calls about that, yeah.



STEVE:  Yes.  It is a deliberate tactic on the part of bad guys to sell an assertion they're making which users might otherwise just kind of blow off.  It's that "Your computer has been infected with malware.  We're Microsoft.  And so we have locked your computer.  Please call this toll-free number" - and, you know, it's 1-888 something or other - "in order to talk to one of our technical support people so that we can help you disinfect your machine and get you going again."  And so if it just came up on a web page, given all the nonsense people encounter on the 'Net, people would be inclined to go, eh, what, and just close the page.



But what is being done is the browser is locked completely.  The UI is frozen, which plays into the warning that is being presented and helps to sell it to people who would otherwise think, eh, this seems like a scam.  So, okay.  Back in February Malwarebytes first described a scam using a particular cross-browser web API, which is window.navigator.msSaveOrOpenBlob, which is cross-browser supported.  It's the means for a browser to save, as it sounds, a blob of just sort of opaque data for its own purpose on the user's machine.  And it provides locally storing and retrieving these blog files.



But it's an asynchronous operation.  That is to say, the call is made to save or open the blob, well, in the case of save.  And the JavaScript goes on, assuming that that's going to be done.  So this allows JavaScript to do whatever it wants to do while this save is occurring in the background.  Well, that also allows this call to be placed into a tight loop, with the blob save requests quickly becoming backlogged in the browser with the OS unable to do the work of creating lots of little files quickly enough.  This pins the system CPU at 100%, and within a few seconds completely shuts down the browser's UI.  You can't close the tab.  You can't even click the close button on the browser.  You can't get out.  The whole browser is locked.  And not only does the browser become nonresponsive, but in some cases the system does, too.



So of course people get freaked out by this.  So Chrome v65 in February fixed the problem, and then it came back in Chrome 67 and is today once again being actively exploited.  And it's not just Chrome.  The Microsoft browsers, both Edge and IE, are not affected by this.  I'm just guessing, I haven't looked, but they probably just don't bother to support this web API.  They generally are lagging a little bit behind.  Or maybe they just do it correctly.  But Firefox and other non-Microsoft browsers, including Opera, Vivaldi, and Brave, are all caught out by this.  And I didn't see any mention of Safari one way or the other.  So I'm not sure over on the iOS platform.



LEO:  Well, I'm thinking, because it says "ms blob," it might be Microsoft specific.



STEVE:  Except that it's Chrome, Firefox, Opera, Vivaldi, and Brave.  So I'm not sure...



LEO:  Yeah.  [Crosstalk] have to be on Windows, I'm saying.



STEVE:  Ah, okay.  That's a possibility, yeah.



LEO:  Because I don't know what "ms blob" would do on a Macintosh.



STEVE:  Yeah.  Anyway, so this behavior has returned, and lots of people are reporting that their browsers are freezing with this notice.  So first of all, I'm sure our listeners know that in Windows Ctrl-Alt-Del will bring up a menu.  Among the items there is Task Manager.  And you can then use Task Manager to terminate the visible processes under your browser name.  You'll see a bunch of Firefox.exes, a bunch of Chrome.exes, or whatever.  And under macOS there's a Force Quit that macOS gives you in order to just kill the browser, in order to get this thing to stop.  So I just wanted to sort of put it on our listeners' radar.  Again, I'm sure that our listeners would look at this and go, okay, yeah.  Something has deliberately hung this page and the browser.



But apparently this is a lucrative scheme, and it is being exploited whenever it can.  And there's a history of this even before this particular API.  The cretins have figured out how to lock up a browser after presenting this scary message in order to get people to call.  And what happens is hopefully, or I guess not hopefully, but the convincing-sounding tech support person talks the person out of their credit card information in order to pay for the technical support required to fix this.  And as we know, none of this is Microsoft behavior.  So again, this is not the way Microsoft works. 



LEO:  I'll have a new answer for people who call.  I mean, of course when people call saying I got this notice that I should call Windows about my bug, I usually tell them don't.  But I didn't realize that they could freeze the browser.  That's cool.  I mean, that's not cool.  



STEVE:  Yeah, that's not cool.  But interesting.



LEO:  But cool, yeah.



STEVE:  Yes, yes.  So I just sort of also wanted to put out a reminder that it's worth auditing your app permissions from time to time. We've talked about this over on the Android platform.  But there was a piece in the Wall Street Journal suggesting that, although Google had said they had stopped rifling through people's email for the sake of choosing ads and any kind of targeting, it's not the case if you have given apps permissions on your email.



There's a place you can go, myaccount.google.com/permissions, and I went there because I was curious.  It turned out that my favorite PDF reader for iOS, iAnnotate, has access to my Google Drive, and I want it to.  There were a bunch of things which were able to use Google for authentication, so login with Google.  Then Chrome itself had full access to my Google account; and YouTube TV, which I've started to experiment with, has access to YouTube.  But otherwise, I didn't have any things that looked worrisome.



However, the Hacker News covered this Wall Street Journal article and noted that, for example, an app like SaneBox, which is a third-party manager of a user's Google Mail inbox, would, as you'd expect...



LEO:  Of course it would.



STEVE:  ...have full access to it.



LEO:  So you have to tell somebody that?



STEVE:  I know.  Although the point is that many times you may have used something for a week or two and then decided, eh, I really don't care about this anymore.  So it's worth just going through and making sure there are no stale permissions that you're giving apps that no longer need them.



LEO:  Anytime I read an anti-Google article in the Wall Street Journal I always have to judge whether it's a hit piece, which I judge this to be, because there's kind of a hereditary animosity between the Journal and Google because of course Google's stealing all their advertising.  So you've got to read it with a grain of salt.  Yeah, basically the point of the article is, if you give applications access to your Gmail, they can read your Gmail.  Well, duh.



STEVE:  Well, it's also, though, that if you stop using applications, then...



LEO:  Oh, you should turn it off, yes.



STEVE:  Yes.  Apps don't remove their own permissions.



LEO:  Right.



STEVE:  So it tends to be a cumulative thing.  And from time to time just going through it, you know, you might well find, oh, what?  I forgot that I had given that thing that I'm no longer using full access to my email.



LEO:  It's a little like Louis in "Casablanca," though.  "I'm shocked - shocked" - to learn that applications are reading my email.  Of course they are.  And then they kind of say, and by the way, it's not just computers, sometimes it's humans, because during the machine learning process humans are used to train programs.  You know, let me tell you another thing you could be shocked by.  If you have an antispam filter, if any of your web-based emails have spam filtering, which all do, they're also reading your email.  There's no other way to filter spam.



STEVE:  Yup.



LEO:  So, I mean, I don't know.  I feel like the Wall Street Journal really was - this was link bait.



STEVE:  Yeah, still.



LEO:  Yeah, I mean, it's worth absolutely noting, and certainly we should note, that you should go through those permissions all the time, not just for mail, but everything else, see what else. And on Twitter and on everything else, Facebook, that you give permissions to because you want to make sure that those people still deserve it.



STEVE:  Yup.



LEO:  It's like saying "Windows reads the contents of your hard drive.  I'm shocked."



STEVE:  No, it's not, but okay.



LEO:  Well, it's kind of like that.  These are all apps that you specifically signed up for, like TripIt or Unroll Me, to process your Gmail and tell you something.  Well, there's no other way to do it.  I think they're pretty clear when they say that we're going to look at your email.  It's not like a human's doing it.  All right.  Back to you, Steve.



STEVE:  So we now have malware becoming a little picky about what it chooses to do when it gets into a person's computer.  Kaspersky Labs discovered a new variant of the Rakhni, R-A-K-H-N-I, ransomware, although it's more than ransomware, which has been upgraded to include cryptocurrency mining also.  But the malware faces a dilemma since mining cryptocurrency and encrypting all of the computer's valuable files tend to be mutually exclusive.  You can't do both.



So what's a malicious infection to do?  Well, it literally checks out the system that it's in.  It first performs some anti-virtual machine and anti-sandbox checks to decide whether it should infect the system at all without being caught.  It doesn't want to be analyzed, so it checks to see if it's running in a virtual environment to the best of its ability.  If it looks like the coast is clear, it looks for a bitcoin folder in the app data section of the user's machine, which suggests to it that the system's owner may have something to lose from encryption.  So it installs ransomware to essentially encrypt the user's wallet along with everything else.  It terminates all other running processes that might have files locked or that might stop it, then proceeds to encrypt the user's files and displays a ransom note in a text file.



But if a bitcoin folder doesn't exist, and the machine is a bit beefier, with more than two logical processors, which would mean more than a single hyperthreaded core, it decides that it's worthwhile to do some mining.  So instead it installs a cryptocurrency miner and gets to work doing that.  Oh, and the cryptocurrency miner is something called MinerGate, which is a service that simultaneously mines Monero, Monero Original, and Dashcoin.  MinerGate describes itself as "a mining pool created by a group of crypto coin enthusiasts."  They said:  "It's the first pool which provides service for merged mining.  This means that while mining on our pool, you can mine different coins simultaneously without decreasing the hash rate for major coins."



Anyway, so while that's all happening - oh, and this is again further indication of the trend that I was noticing.  It uses the Windows Cert Manager utility to install a root certificate, actually two certificates, one which claims to have been issued by Microsoft and the other by Adobe, in an attempt to disguise the miner as a trusted process.  So it pretends to have been signed by Microsoft, and it's able to carry that off, that spoof off by sticking a certificate in which is not actually a Microsoft certificate, but which does sign that piece of malware.  So a user who is somewhat sophisticated would check the signature on the malware and say, oh, it's valid.  It says the signature is valid, and it's signed by Microsoft.



Okay.  If neither of those two conditions are met, that is, there's no bitcoin wallet, which would suggest that encrypting the drive would be beneficial to it, or if it's on a rather weak machine, it switches into worm mode and begins to scour the network for any attached machines where it's able to gain access and install copies of itself there.  So anyway, here's another instance of certificates being installed in machines by malware, something I hope we're going to see our OSes become hardened to soon, and sort of the logical evolution of malware.  Since it's now easy to do cryptocurrency mining, and it's also possible to do ransomware, well, what's the best strategy?  So it decides on the fly, rather than only being a one-trick pony.



We've talked about Coinhive a lot.  That's the service which it's sort of in the gray zone.  They ostensibly offer a service, as we've discussed, where a website can decide that it wants to monetize its visitors' visits by forwarding Coinhive mining to people who visit that site to run a miner on their system.  And after that caused a big ruckus, then Coinhive took the initiative of making sure that the user had given permission to mine on their system while they were visiting the site to keep bad guys from injecting Coinhive mining into other websites in order to mine maliciously, or at least in the background.



Okay.  So the Coinhive guys are nothing if not innovative.  Now they've created a link shortener service which incorporates proof of work.  So you were just talking about a link shortener that you're enamored of, Leo, in MacBreak Weekly.  Our listeners know that I like bit.ly, and I sometimes will use a bit.ly link tied to something short, just so our listeners can get to a long and confusing link easily.  It's convenient.  They're of course also a bit of a security concern because you can't see the link that you're going to without jumping through a little more hoops.  So it can be used by bad guys to obfuscate a URL that you might not otherwise deliberately go to.  Instead, you click the link, and it uses a browser redirect to take you to where you're really going.



So the Coinhive guys said, okay.  Let's use browser mining to monetize link shorteners, or our link shortener.  Coinhive states on their page where they're selling this service:  "If you have a URL you'd like to forward your users to, you can create a [and then they have the domain] cnhv.co short link to it.  The user has to solve a number of hashes which are adjustable by you and is then automatically forwarded to the target URL after they've done so."  So, okay.  Seems kind of hokey, but it would allow the monetization of link forwarding.



So unfortunately it didn't take bad guys long, much as they abused the previous offering, to figure out a way to abuse this one.  Luke Leal of Sucuri blogged about this a couple months ago, and it's just come back onto people's radar because it's now becoming quite prevalent.  There are hundreds of sites which are now generally various types of content management based.  So they're using some sort of hacks to plant these links.



There is an obscured script which is being embedded on sites which, when turned back into text, that is, deobfuscated, will render an iframe which contains a 1x1 pixel window.  I saw some pictures of it.  You could easily, I mean, you'd have to look for it in order to find it.  It's in the upper left-hand corner, a one-pixel iframe.  And so it's of course doing the same thing.  It's being planted on websites' delivery pages.  People are as a consequence having their computer, after running through this link redirect, essentially back on Coinhive.com, running bitcoin miners on behalf of the people who planted the links on other websites.



So once again, the Coinhive people, I mean, again, they're being innovative.  But it turns out that, when Luke posted his piece, no AV engines as reported by VirusTotal were objecting to this little URL that's being planted.  I checked last night, and nine of 67 different AV tools were listing it as dangerous.  So I expect that more will be following.  And in fact I just clicked on it now to see how quickly this is going up, and VirusTotal says, nope, still nine out of 67.  Remember that we talked about VirusTotal last week as something you could upload malware to.  You can also give it suspicious URLs and see whether there's anything that anybody objects to.



So what's going to happen is that the AV tools will block this, and web browsers just are overall, I mean, I guess right now we probably have bit mining blocking for users who have deliberately installed them in web browsers.  I presume a lot of our listeners have because we've been talking about this, and this has been a thing now for months.  But there are certainly lots of people who don't yet have their browsers blocking through the addition of an add-on, and browsers are not yet doing so natively.  We need to have that happen, and then this whole annoyance will go away.



LEO:  This site was not a URL - you'd actually, if you don't mind a little sidebar, be interested...



STEVE:  Yeah, yeah, yeah, please, please.



LEO:  It's itty.bitty.site.  But it's not a URL.



STEVE:  Oh, your site.



LEO:  The one I was talking about.



STEVE:  Yeah.



LEO:  Because what it does is it base64 encodes whatever you put on the site into the URL.  So the URL, which is long as a result, would work offline.  Your browser renders it.  So it allows you to do about a page of text.  You could even do, if you look at the itty.bitty.site website, you could even - and this is the potential risk, same risk as with any site because you could put a script in it.  You could actually do an app in this thing.  Here's a little calculator.



STEVE:  That's very cool.



LEO:  That's completely encoded in the URL.  Which is amazing.  We can even generate - in fact, I'll show you this one.  If anybody trusts me, this is "Jabberwocky."  I could do the QR code for "Jabberwocky."  And if you scanned the QR code, it would actually show you...



STEVE:  Take your browser to that location.



LEO:  Yeah.  Without being online.  So if you scan this with your phone, you'll get a page that has the Jabberwocky in it, but without being online because it's entirely encoded base64 into the URL.  Which is wild.



STEVE:  Very cool.



LEO:  Yeah.  And of course potentially hugely dangerous.  I would never mention that to anybody unless they knew that.



STEVE:  I wonder if they compress the ASCII.



LEO:  They do.  They use Lempel-Ziv.



STEVE:  Ah, okay.  Yup, that's exactly what I would do.  I would compress it and then base64 encode the binary.



LEO:  Exactly what they do.  You are so smart.  It's kind of a neat hack, really.



STEVE:  Yeah, it is, yeah.  It really is.  So we all know D-Link.  They're one of the major consumer router and little switch providers.  I've got D-Link stuff around.  Unfortunately, as can happen, but really should not happen, they lost control of their own code-signing certificate, which is really bad.  Security researchers from ESET recently identified two malware families that were previously associated with the cyberespionage group BlackTech.  And that malware had been or has been signed using valid digital certificates belonging to D-Link and another reputable Taiwanese manufacturer known as Changing Information Technology.  Not as well known to us in the U.S., but presumably in Taiwan.



The first malware, named Plead, is a remotely controlled backdoor designed to steal confidential documents and to spy on users.  The second piece of malware is a password stealer designed to collect saved passwords from users' browsers - Chrome, IE, Microsoft Outlook, and Firefox.  So also apps in addition to browsers.  So the researcher notified D-Link and Changing Information Technology, both of these companies, about the issue, and the companies immediately revoked the compromised digital certificates on the 3rd and 4th, so last Tuesday and Wednesday of last week.



However, unfortunately, as we know, certificate revocation relies upon the entity trusting the certificate's identity assertion to deliberately and continuously check with, or at least the first time it's encountered for that particular use, to check with a certificate's issuer and signer, that is, the CA that issued the certificate, to reverify the certificate's present real-time validity.  Okay.  So, right?  Certainly lots of D-Link routers have firmware previously and legitimately signed with their cert.  So if the router, as I hope it would, would check that the firmware has been signed by D-Link, it would be nice if it also checked for revocation.



But almost nobody actually does that in practice.  It takes time.  And just it's not something that is often enough done.  And it turns out that indeed most antivirus software does not bother to check the certificate's validity.  The fact that it is a valid certificate signed by somebody in the root store and still within the validity dates, or - and this is interesting about certificates which sign code - those certificates contain a co-signature, that is, it's countersigned with a timestamp which is obtained at the time of the signing.  And that's important because, as we know, certificates themselves have a multiyear life, but like only three or four or five years.  I think code-signing certs have a longer life, but not forever.  But the code could live on on a repository somewhere.



So the code itself could be signed with a certificate which has since expired, yet it was valid at the time of signing.  So in order to know that the certificate wasn't signed with an expired certificate, the act of signing can bind a timestamp into the certificate which cannot be spoofed because it comes from a signing authority, a timestamping service, so that's bound in.  Okay.  So what this means is that the certificate's been revoked, and we need to honor - we need to continue to honor signings by that code-signing certificate which were signed as indicated by the timestamp previous to the revocation time.



Now, the BlackTech hackers were using a non-revoked certificate to sign their certificates.  So as long as they did that and got them timestamped, as they presumably did, revocation after that fact would not invalidate the certificate.  So this is a mess.  The D-Link certificate in question was issued by Symantec on September 29th, 2016, so not quite two years ago.  And the certificate is valid for more than a year from now, that is, until September 30th, 2019.  So this particular certificate had a three-year life.  After this date, that is, after the certificate's actual expiration date, new signings under the certificate will be no longer valid.  That is, the certificate itself would be invalid independent of revocation checks.  But until then, that is, until a year from now, more than a year from now, any new malware that is signed by this certificate, unless the AV checks revocation of all certificates, is going to be deemed valid.



So again, as I said, this is a mess.  What will probably have to happen is that this particular certificate's thumbprint will be kind of reverse-pinned.  It'll be blacklisted by thumbprint, which cannot be spoofed, as we know, in responsible AV vendors and maybe even hopefully in operating systems will no longer trust things that have been signed after the date that D-Link says they know that the cert escaped their control, and if any software has been signed with that thumbprint.  So it's a mess when code-signing certs escape because, even when the system, I mean, first of all, the system is not working as well as it should because that would say revocation would always be checked.  In practice, that doesn't happen.



So the best we can do, in terms of practical use of the system, is for notification to happen and for operating systems and antivirus vendors maybe who are able to respond more quickly.  I mean, they're desperate, AV vendors are, to continue demonstrating that they're adding value as operating systems continue to take on more responsibility for protecting users natively.  So not good when this happens.  And especially from a major company like D-Link.  I'm sure they're not happy that the cert got away from them.



Okay.  iOS v11.4.1.  This happened just last night.  I updated a couple of my iDevices today so that I could see this and experience this for myself.  There was a feature which first appeared in the iOS 12 beta.  And it wasn't clear whether Apple would hold the feature back for iOS 12.  We learned officially last night that they decided not to.  There's now a switch which has been added to the Face ID or Touch ID and Passcode section of the Settings app which allows for the essentially USB access to the phone to be disabled when the phone is locked.  So that's a cool new feature.  The caption under the feature explains that you can unlock the iPhone - it explains:  "Unlock iPhone to allow USB accessories to connect when it's been more than an hour since your iPhone was locked."



So what happens is you have an hour after the iPhone is locked before a timer expires which then suspends USB support until the phone has been unlocked.  Sounds like a nice feature.  What is believed is that this was Apple responding to the Lightning connector-based hacks which are used now to jailbreak or by law enforcement to get into phones.  Immediately after the upgrade, which is to say yesterday, ElcomSoft - which is a company we've spoken of a number of times.  They're in this business of messing with iPhones.  They confirmed and blogged in detail about a workaround which they had seen previously in the betas and which did survive in the final release result.



So no one's really exactly sure what's going on here.  It's a little complicated, so I'm going to quote directly from a couple paragraphs of their blog.  They said:  "The most spoken about thing about iOS 11.4.1 is undoubtedly USB Restricted Mode.  This highly controversial feature is apparently built in response to threats created by passcode-cracking solutions such as those made by Cellebrite and GrayKey on unmanaged devices.  The new default behavior is to disable data connectivity of the Lightning connector after one hour since the device was last unlocked, or one hour since the device has been disconnected from a trusted USB accessory.  In addition, users can quickly disable the USB port manually by following the SOS mode routine."  That's the click the power or sleep button five times rapidly.



They write:  "Once USB Restricted Mode is engaged on a device, no data communications occur over the Lightning port.  A connected computer or accessory will not detect a smart device.  If anything, an iPhone in USB Restricted Mode acts as a dumb battery pack.  It can be charged, but cannot be identified as a smart device.  This," they write, "effectively blocks forensic tools from being able to crack passcodes if the iPhone spent more than one hour locked.  Since law enforcement needs time (more than one hour) to transport the seized device to a lab, and then more time to obtain an extraction warrant, USB Restricted Mode seems well designed to block this scenario."  And they say:  "Or is it?"



They write:  "We performed several tests and can now confirm that USB Restricted Mode is maintained through reboots, and persists software restores via Recovery mode."  So basically they played with it, trying all kinds of different tricks, seeing if they could regress out of restricted mode.  They said:  "In other words, we have found no obvious way to break USB Restricted Mode once it is already engaged.  However, we discovered a workaround, which happens to work exactly as we suggested back in May."



Okay.  So they report that, if any Lightning device is plugged into a locked iPhone any time before the 60-minute timer has expired, whether or not the device is known to and trusted by the iPhone, the lock countdown timer will be restarted for another 60 minutes, and this can continue indefinitely.  Then I've skipped a bunch of stuff in the blog.



But they conclude saying:  "We've seen rumors about GrayKey being able to defeat protection provided by USB Restricted Mode.  At this time, these are nothing more than rumors.  The company's official policy is never issuing comments about pre-release software.  With iOS 11.4.1 just released, we'll have to wait to see if the new security measure can be defeated.  Either way, since iOS 11.4, the speed of GrayKey, and probably its competitors" - this is ElcomSoft writing - "is limited to slow recovery rates of one passcode guess every 10 minutes."



They say:  "While this allows breaking four-digit passcodes in reasonable time," they say, "about two months worst-case scenario, six-digit passcodes already make little sense to attack unless one has a custom dictionary; and, as we know, six digits is the new default length for the passcode suggested by iOS."



So anyway, an interesting new feature.  iOS moves forward, continuing to work to improve the security of their users.  And after an hour nobody sticking something into the Lightning port will be able to access it.  And Leo, how does this comport with what Rene explained?



LEO:  Well, we read the ElcomSoft article.



STEVE:  Oh, okay.



LEO:  And as you pointed out, ElcomSoft had some interest in saying, oh, no, no.  But basically we concluded was, if law enforcement wants to carry - ElcomSoft used as an example the Apple Camera Connection Kit.  But any USB Lightning port device is designed to keep the Lightning port alive, even after locking.  And that's the key; right?  



STEVE:  Right.



LEO:  Apple's decided, well, if we were to lock it, and you're copying photos off on your camera to the iPhone or iPad, and then it locks and suddenly stops working, you'd be peeved.



STEVE:  Right.



LEO:  So that keeps, even though the device is locked, it keeps the USB channel open.  So we said, well, if you're law enforcement, what you do is you carry USB, the Camera Connection Kit or something similar.  And you apprehend somebody, you grab his phone, presume that he has unlocked it sometime in the last hour, plug this thing in, and then Rene pointed out you probably also want to put it in a Faraday bag so that...



STEVE:  Oh, right, right, right.



LEO:  ...it can't be remote wiped; right?



STEVE:  Yup, yup.



LEO:  Put it in a Faraday bag and then take it back to the home office.  You've kept the USB port alive.



STEVE:  Got it.



LEO:  And nobody denies that that's the case.  That is the case.  So, and there's a good reason for it, and maybe Apple will respond.  But, you know.



STEVE:  Yeah.  It sounds like they're aware of it, and it was a tradeoff they had to make.  And, first of all, again, our listeners should know, if you click the power button five times rapidly, that immediately locks the USB.  It puts it into USB Protected Mode.  So that's protection.  Or as you said, if it's been more than an hour, then it's already locked.



LEO:  Right, yeah.  I think this is a fairly good idea.



STEVE:  Yeah.  Oh, I agree.  Again, Apple doing the right thing for us.



LEO:  So if I press my phone - if I press this five times, one two three four five, and then do what?  Just leave it?



STEVE:  Yeah, exactly.  That is now - and I don't know if you have to power it down, or if that's enough.  I couldn't get an...



LEO:  Well, once you've powered it down, you're screwed, you know, that's the whole thing; right?  Now they can't win at all.



STEVE:  Right.  Right, right, right.



LEO:  And this is the thing we also talked about.  If the data on the device is encrypted, even if it's locked, even if they have one of those devices plugged into it, they're going to pull off encrypted data, not unencrypted data, once the phone is locked; is that right? 



STEVE:  Correct.  Correct.  Well, what I thought was interesting was that it can, if they can get to a live Lightning port, these crackers can do an automated guess once every 10 minutes without triggering the phone's "you've guessed too many times" timer.  So what they're doing is they're just, like, sticking the phone on this cracking device and waiting maybe up to two months if the user has a four-digit passcode, or hoping they get lucky if it's longer than that.



LEO:  If it's longer.  And of course the best solution, which is something I do, Andy does, a lot of people do is don't use a passcode at all.  Use a long - I have an 18- or 19-character password.  It's a pain in the butt to enter it.  But then I don't have to worry at all because no brute force is going to [crosstalk].



STEVE:  Yup, [crosstalk] because what this does is it dramatically restricts the rate at which brute forcing guessing can happen.



LEO:  Right.



STEVE:  Just another quickie, another reminder.  I sort of thought it was interesting.  As we all know by now, ever since Peter Norton made a name for himself with his famous Undelete utility, you know, that was the one thing Peter came out with that, like, put him on the map.



LEO:  And it was pretty simple thing, too.



STEVE:  Oh, my god, Leo.



LEO:  He wrote it in Pascal.



STEVE:  Yeah.  And it turns out that the first character of the filename was turned into an E5, a hex E5, just a little bit of trivia that I remember.



LEO:  It looks like an upside-down E.



STEVE:  Yeah.



LEO:  If you look at it as an ASCII code.



STEVE:  Yup, yup.  And so what he realized was, oh.  Oh, and the FAT was - the entries in the File Allocation Table were also zeroed to free up the space.



LEO:  Right.



STEVE:  But of course, as we know, the data itself was not proactively removed from the hard drive.  And so, if you were lucky, you could restore the directory entry which was left there intact, pointing to the first cluster of the file.  And often the file was a contiguous run of clusters.  The directory entry also showed how long the file was.  That told Peter's Unerase how many clusters it needed.  And so it was able to judge whether there was a contiguous run of clusters that long.  In which case, chances were you had the file back.  And so that was, whoo, that made Peter famous and so forth.



Anyway, as we know, deleting files doesn't delete them.  Neither does formatting them.  Formatting is a stronger process.  It wipes out the front of the drive.  The whole directory, the FAT, the File Allocation Table gets zeroed and so forth.  But the data, again, is still out there on the physical drive surface.  Well, this of course is the same with memory cards, which use the same sort of file system.  So Bleeping Computer just carried an interesting little piece from researchers at the University of Hertfordshire in the U.K.  They purchased a bunch of random memory cards from eBay, from some auctions, from second-hand shops, and other sources over a four-month period.



They discovered, that shouldn't come as a surprise to us, that two thirds of memory cards contained readily recoverable data.  They wrote in their report that the memory cards they recovered were previously used in smartphones, tablets, cameras, satellite navigation systems, and even drones.  To perform the recovery they first imaged the storage to create a bit-by-bit copy, then just used freely available software to see whether they could recover any data from the card.  The team recovered, they wrote, a great deal of data from the memory cards:  intimate photos, selfies, passport copies, contact lists, navigation files, pornography, rsums...



LEO:  I'm going to have to erase my cards better.



STEVE:  ...browsing history, identification numbers, and personal documents.



LEO:  Of course.  You put all that stuff on USB cards.



STEVE:  Yes, yes.  And what you want to do is never, ever, ever throw them away.



LEO:  Is there any other way to fix that?



STEVE:  Well, I'll talk about that in a second.  Thirty-six were not wiped at all, so neither the original owner nor the seller took any steps to remove the data.



LEO:  Didn't bother.



STEVE:  Yeah.  Twenty-nine appear to have been formatted, but the data was still there, so it was recoverable with minimal effort.  Two cards had data deleted, but was still easily recoverable.  Twenty-five appeared to have been wiped using a data erasing tool that overwrote the storage area.  So from those cards nothing could be recovered.  Four were dead completely and could not be accessed.  They were just broken.  And then four had no data present, but the team was unable to determine the reason.



So there are, and it is worth using, data wiping tools.  I've mentioned previously on this podcast that I have an arc of my future development.  As we all know, I'll be working on SpinRite 6.1 and .2, which will dramatically speed up SpinRite.  That's like the main feature is speed, by talking directly to the hardware, first for the motherboard-based connected controllers, the AHCI, SATA, and IDE, if anyone has an older motherboard, basically circumventing the BIOS.  Then I'm going to do the same thing for the serial connected devices to give us good performance on attached USB.  And of course that also means thumb drives and smartcards.



So what I'm going to do before I start on 7 is to release another product called Beyond Recall.  And as its name suggests, whereas SpinRite is all about recovering data, Beyond Recall will be about absolutely and positively and quickly - because what it's going to do is, the idea is it's going to use the technology I develop for the 6.x series.  I will reuse that low-level direct-to-the-hardware technology to make Beyond Recall feasibly fast.  So, for example, you'll be able to wipe a multi-terabyte drive in a few hours; whereas it's just prohibitively, I mean, it's like incredibly painfully difficult to do that in a reasonable time otherwise.



LEO:  Or you could do what Mr. Robot did.  Throw them in the microwave.



STEVE:  Yes, yes.



LEO:  Would that work?



STEVE:  Well, if you have a throwaway microwave.  Because, I mean, it hurts the microwave.



LEO:  It's not good for microwaves.



STEVE:  It creates sparks and all kinds of sh*t.  I would drill holes in them.  I mean, you really want to physically destroy them until I have Beyond Recall, and maybe even before, or after.  But anyway, so I think there's an increasing need to securely wipe these things, and there's also sort of lots of tricks I'll get up to because, by talking to their controllers, you can deal with things like regions which have been swapped out and are no longer accessible to the front door.  You need to get in the back door in order to deal with regions which have been taken out because of wear, you know, the whole wear-leveling thing.  So anyway, that's going to be...



LEO:  That's what we really need because there's always that stray stuff lying around because of wear leveling.



STEVE:  Yup, yup, yup.  And lastly, just to close the loop on the USB fans which were handed out at the Trump/Kim summit, many security researchers were made very uncomfortable by this.  Everybody got them, and two researchers were able to obtain them from journalists who were present.



LEO:  Oh, good.  Oh, good.



STEVE:  Yes.



LEO:  Was there anything going on there?



STEVE:  No.



LEO:  Oh.  



STEVE:  They found nothing other than fan motors hooked directly to the USB's 5V power and ground lines.  The two data lines had no connections to them, so nothing smart was there that might get up to some mischief.  However, the security researchers noted that, first of all, many of these - noted that many fans were distributed, and a few smart fans might still have been used in targeted attacks, that is, given to specific individuals who would then maybe take some comfort from knowing that security researchers opened other people's fans and found nothing in them.  So the overall takeaway, not only from this but in general, is do not use untrusted USB without a USB condom.  It is always too dangerous.



LEO:  Which thanks to you I always have with me.



STEVE:  Yup.



LEO:  Steve sent me two condoms.



STEVE:  Yup.  And Leo, these are reusable condoms because, you know, you should really never reuse...



LEO:  Never use anything else, yeah.



STEVE:  ...the other kind.  Anyone traveling, especially anyone who might be targeted, should always use protection.  Or better yet, simply abstain while on the road.



LEO:  Yes.



STEVE:  However, if you can't...



LEO:  Sometimes you have to charge; right?



STEVE:  It's worth having a condom.



LEO:  You know, I'm sure the Secret Service vetted, I would hope, vetted them.



STEVE:  As I understand it, I mean, I don't know how they did.  You could put a - remember there were those USB killing devices we talked about a while ago that would, like, zap a USB - it was a gadget that you would hook to your computer, and it would charge up and then release a kilovolt burst into the USB port.



LEO:  Oh, geez.  Oh, that'd be handy.



STEVE:  So one thing you could do would be to just blast the crap out of the data lines, which would fry anything that was sniffing the data lines, but still leave the power lines alone.  So anyway...



LEO:  This is the PortaPow.



STEVE:  That's the one.  PortaPow.



LEO:  Data block plus smart charge that you just use as a little USB condom.



STEVE:  Yeah.  And it just - all it is, is the 5V and ground lines go through, and the data lines are disconnected.



LEO:  Yup.  And you're safe.  No problem.



STEVE:  So I got an interesting note from Eric, looks like - boy, I didn't pronounce his name ahead of time.  I didn't practice.  Theriault, T-H-E-R-I-A-U-L-T, Eric Theriault.  I hope I didn't mangle...



LEO:  Theriault.  It's French.



STEVE:  So it's pronounced...



LEO:  Theriault, I would guess.



STEVE:  Okay, I'm going to give up.



LEO:  Yeah, Theriault.



STEVE:  Anyway, he's in Quebec.  And is that Quebec or...



LEO:  Quebec.



STEVE:  I don't know what I'm doing.  Anyway, I do know what I'm doing about SpinRite.  So his subject was "SpinRite & RAID 6:  How to, please help."  And he said hi to everybody.  He said, "Hi Steve, Leo, Sue, Greg, and Elaine."  So he missed some of your staff, Leo, but he got all of mine.



LEO:  Close enough, yeah.



STEVE:  He says:  "I have a home server set up on a RAID 6, five disks of 2TB."  Each, I assume he means.  "This computer is always on, and multiple services depend on it.  I can easily shut down and reboot, if it's for a short period; but I would prefer not longer than 30 to 60 minutes.  Worst case would be to shut down during the night.



"My question:  How, if there's a way, can I do SpinRite maintenance on all five disks without having to shut down the computer for a week?  I could do a 'hot unplug' one disk at a time and SpinRite it on another computer.  But if my understanding is right, this would be useless" - well, no, but okay, we'll see where he goes with this - "because it would then make the data on that disk irrelevant.  And when I would reconnect the drive, the software RAID would have to rebuild all the data on the reinserted drive because the data in the still active four disks would of course have changed."  Okay, yes, he's right about that.



"Please share, as usual, any good practice for this kind of configuration, on or off the podcast, up to you."  Well, thanks, Eric.  He says:  "FYI, this is a software RAID on a Linux using MDADM."  Which I'm not familiar with, but sounds like a nice thing.  He says:  "Oh, by the way, I got in the show first back in July 2017" - so just exactly a year ago, he says - "and after three months decided to go back from the beginning.  I'm now up to the 217th episode and, of course, up to date since July last year."  So he's going back and catching up.  So yay, Eric, thanks.  He says:  "Keep going with that amazing podcast.  I learn so much every week.  Thank you so much."



Okay.  So I thought about his problem.  And the best I can suggest, it would be to take advantage of 6.1's speed when it's available.  Assuming they're five disks of 2TB each, rather than a total RAID 6 of 2TB, which would make all the drives maybe a half a terabyte, before I suspended the work on 6.1 in order to work on SQRL, I had benchmarked the preliminary driver that I wrote for AHCI controllers at half a terabyte per hour, which would mean that SpinRite could do a 2TB drive in four hours, making it at least feasible to do overnight.



So what Eric could do would be to down his system.  And presumably, if it's a Linux RAID, it's on a machine, so on a PC, so he wouldn't have to remove any drives, just run SpinRite on one of those drives.  And actually we talked about the command line option.  You could use a script to run it successively on one drive, then a second drive, which would be two drives overnight in eight hours, then bring it back up.  So you could, in this scenario, you could do the whole five-disk RAID in three nights in a way that would not break the RAID because SpinRite never changes the data on any of the drives it works on.  So the RAID wouldn't break, and in three nights you'd have SpinRited all of the drives.



And nothing I will ever be able to do will be able to improve on half a terabyte per hour.  That is absolutely, I pulled every trick in the book.  Even though I'm in DOS, I'm using a fancy real mode that gives access to extended memory from real mode, so I'm allocating a 64MB buffer, which is the largest transfer that any drives can do in a single burst.  And all drives are optimized for continuous streaming single-burst transfer.  So no revs are missed.  It's like catching every single revolution.  So we're limited by literally the platter speed.



Now, it might be that you have a higher platter speed, more sectors per, so that would increase its throughput.  But basically, when we get to 6.1, it will be running the drives at absolutely their maximum capacity, like never missing a revolution in a 64MB transfer of, what, 128,000 sectors or something?  Wow.  Anyway, whatever it is, that's my target for 6.1.  And as I mentioned before, Beyond Recall similarly will be employing the same technology to scrub disks.



I asked, when I was working on 6.1, I had suggested to the gang in the newsgroup where we work on this stuff that I add a Beyond Recall feature to SpinRite, and they universally said no, no, no, no, no.  Do not confuse SpinRite...



LEO:  That's probably a good idea.



STEVE:  Do not confuse SpinRite that recovers data with something that is called Beyond Recall that absolutely destroys the ability to recover.  So it's like, okay, fine.  That'll be a separate inexpensive product.  And I think it'll be popular.  So we'll see.



LEO:  There is Darik's Boot and Nuke, but that doesn't know about SSDs; right?  That's probably the problem.



STEVE:  No, DBAN, yes, exactly.



LEO:  DBAN, yeah.



STEVE:  So it'll be - I will have something as soon as I can.



LEO:  Nice.  Until there's DBAN.  Until there's Secure - what is it called?  Secure Erase?  Total...



STEVE:  Beyond Recall.



LEO:  Beyond Recall.  I love that name.



STEVE:  Isn't that great?  Yeah.



LEO:  Until there's Beyond Recall, there's DBAN.  But once Beyond Recall comes out, Darik, you're going to have to find a new line of work.  So, no, he doesn't charge for it, so it's not really a line of work.



STEVE:  Okay.  So STARTTLS.  I really do want to do an extra "T" in there, Leo, every time I see it.  START TTLS.  No.



LEO:  No.



STEVE:  I guess it's because of TTL, you know, Time To Live.



LEO:  Right, right.



STEVE:  And so I'm used to thinking about that instead of TLS.  So anyway, STARTTLS Everywhere is the name that the EFF, the Electronic Frontier Foundation, gave to their newest initiative.  Once upon a time, just like the web and HTTP, email was only plaintext.  Web pages went to and from in plaintext, and so did our mail.  With the web, however, we have a direct point-to-point real-time link between our web browser and the remote server that we're visiting.  So this point-to-point link made it a relatively simple thing to allow the remote web server to assert its identity and for the user's web browser client to verify that identity and to complain to us in real-time, you know, with warnings and stuff, if something didn't look right somewhere.



But email has always been different.  For one thing, it's designed to be non-real-time and asynchronous.  It's very much like the way packets route around the Internet that we've talked about.  Email is delivered as a best-effort store-and-forward system where autonomous SMTP, that's Simple Mail Transfer Protocol servers, receive and forward email toward its destination.  And in that sense store-and-forward is a bit like the Internet router's receive-and-route function, where the exact path to be taken may not be known, and so the various components along the way just sort of send the mail toward its destination.



So to do that in the beginning we had, as we know, two elder email protocols, still in use today, although one has sort of faded a bit, POP, the Post Office Protocol, and SMTP, Simple Mail Transfer Protocol.  POP is and was used by mail clients to receive mail from servers, from their so-called Post Office.  The clients would connect to their email server over port 110 and ask for any email that had arrived since their last check-in.  Typically they'd receive, then delete, the email from the server so that the email took its final hop to them and then had arrived at its destination.  And no encryption, just plaintext, and we were all happy.



Then later IMAP came along, the Internet Message Access Protocol.  And the way it's different is sort of hinted at by its name.  It operates over port 143, and it allowed clients to retain their email on the server and instead manage it remotely through their client, which became more of a viewer of the email on the server.  And again, port 143, no encryption anywhere.  No encryption in sight.



And then finally the SMTP, the Simple Mail Transfer Protocol, that's the incoming mail service and the protocol which is offered by servers listening for connections on port 25.  So whereas both ports 110 for POP and 143 for IMAP were getting mail from servers, port 25 is sending mail to servers.  Whether it's the client sending it to their own SMTP server for it to then in turn send to the destination, or for one SMTP server to send it to the next SMTP server.  And again, no encryption from the beginning.



And so even today we have email hopping around all over the place, and end-to-end encryption is far from guaranteed.  Remember that you may have a secure connection to your SMTP server.  But it's increasingly the case that the SMTP server you have sent your mail to directly connects to the SMTP server your recipient uses.  And so if that link is secure, you're probably okay.  But the protocol doesn't guarantee that.  It could be multiple hops.  And in that case the intermediate SMTP server is like a man in the middle.  Even if there are secure connections on both sides of it, it's decrypted.  That email is decrypted while it has it.  And even when everything is working with email encryption, that will still be true.  So it's only by the user taking the initiative to use PGP or S/MIME or to pre-encrypt a blob and then send it, that they can actually get end-to-end encryption.  Even once everything is working with email encryption, it still isn't true end to end.



So how do we fix where we are?  Well, unfortunately, it wasn't a straight line.  Just as the first web browsers originally connected, as I mentioned before, over port 80, without any authentication and encryption, what the architects of the Internet did was they said, okay, we're going to give web servers a new port, a different port, in this case 443, which will require encryption.  That is, anybody connecting to port 443 is still connecting to the web server, but has to negotiate in SSL originally and now a TLS protocol connection, meaning that the server will assume that and will send a certificate to assert its identity.  The client will obtain it, make sure that it's the server it wants to talk to.  They negotiate, and off they go, but all of that as a consequence of connecting on this different, dedicated port.



So the same thing was done for email.  But unfortunately, it didn't happen first.  It sort of happened after the - it's, like, not really clear.  I've attempted to sort of like disentangle the RFCs, but it's a mess of one obsoleting the other.  And I don't know, I can't really explain how we got into the mess we're in today because it seems unlike the orderly process we're used to on the Internet.  But what happened is there exists today similar dedicated encryption-only ports for these three protocols, for POP, IMAP, and SMTP.



For POP, which used to be and still is unencrypted by default, there is POP over port 995.  And any client connecting to port 995 of an email server is expected to bring up a TLS connection just like a web browser connecting to a web server on port 443.  Certificate is exchanged.  Certificate is verified, authenticated.  Encryption comes up, and we're off and running.  IMAP's original port 443 similarly has a twin, in this case with implicit encryption.  That's port 993.  And similarly, port 25 that is plaintext SMTP now has an encrypted version 465.



Okay.  So we had new ports that were encrypted.  Unfortunately, somewhere along the way, before this right way of doing it happened, STARTTLS was mixed in.  But it's not quite that simple, either.  The Internet wizards disliked the idea that port 25 and the encrypted version 465 was being used for both email forwarding between SMTP servers, also known as MTAs, Message Transfer Agents, but that same port was being used to receive email submissions from email clients.



So they further complicated things by attempting to split the submission and the forwarding roles by defining another SMTP port, 567, for email clients to use.  So this was defined for some reason as a plaintext port which could be upgraded by the use of STARTTLS, which we'll get to in a second.  And then, believe it or not, after adding this new SMTP alternative connection port for clients, just this year, earlier in 2018, the decision was made to discourage the use of port 567 for future email client submission in favor of returning to using SMTP over TLS 465 and sharing that single email submission port between clients and servers.  So it's a mess.



Okay.  So these secure ports are now widely adopted, but not universally adopted.  They use our well-known and well-proven existing CA-issued server domain certificates to work with these new ports, just as they work so well with HTTPS.  The problem is that an alternative sort of half-baked solution was also added to the RFCs which allows a negotiation between non-secured connections to elevate itself.  And so what we have today kind of works, but not very well.



As its name suggests, STARTTLS provides a means for, as I said, an insecure plaintext connection to start using TLS.  When a client connects - and this is over the non-implicit TLS ports, that is, not 995, 933, and 465, but over the old port, the old school, the original 110, 143, and 25.  When a client connects, the server will respond with a hello message which includes the notification that it supports STARTTLS if it does.  In which case, if the client also supports STARTTLS, it will accept that offer, essentially, from the server to switch over to TLS, and they will negotiate a TLS connection and switch the existing connection over a non-secure port to be secure.  Sounds great; right?



Except, and again, this was like there was a point in time where this must have made sense, where the idea was to provide an upgrade path on existing ports at the protocol level that wouldn't break old stuff, that wouldn't require new ports to be assigned, that would sort of be forward-compatible.  But, boy, in retrospect it's a mess.  Yet it's in the world.  We have it.  And so we're sort of stuck with it.



Okay.  So STARTTLS itself currently sits with about just shy of 90 percent, about 89 percent presence currently, as is shown by Google's email transparency report, which is not bad considering that it was at 39 percent just five years ago.  So with the move toward security and a heightened concern for encryption, new servers coming online look like, well, they're probably just, you know, they default to supporting it because why not.  You've got a certificate for the server.  Let the email server use it, and you've got TLS.  Except big problems.



Okay.  So now paraphrasing from the EFF because they summarized this nicely, they said:  "Although many mail servers enable STARTTLS [get this] most still do not validate the certificates.  Without certificate validation [as we know] an active attacker on the network can read and modify emails sent through supposedly secure connections.  Since it's not common practice to validate certificates, there's often little incentive to present valid certificates in the first place."  Right?  Certificates are a pain.  Which is why a lot of sites, a lot of websites just didn't bother.  And you could argue that email needs them even less than web servers do, where you actually do have sensitive data passing back and forth on a web server's pages.  Yeah, email has lots of sensitive data, too.  But somehow it's sort of out of sight.



Okay.  But get this.  Servers are not validating the certificates.  So what are they doing?  They are using self-signed certificates that no one trusts.  The recipient is not going to trust a self-signed certificate.  But it's valid.  So, I mean, it is a certificate.  So I guess the way to think of this is you would have opportunistic encryption, that is, the server is offering a certificate that does allow the TLS handshake to happen.  So even though the authentication is not present, at least you get protection from a passive observer.  You get encryption.  So somebody sniffing the line won't see anything.



So, okay.  The EFF writes:  "On the web, when browsers encounter certificate errors, these errors can be and are immediately communicated to the end user" - as I mentioned before - "who can then decide whether to continue to the insecure site" or not.  Or that is to say, the nonsecured site.  "With email," they write, "this is not an option, since an email user's client, like Thunderbird or a Gmail app on a user's phone, runs separately from the machine responsible for actually sending the mail."  That is, because it's this SMT server to SMTP server connection.



"Since breakage," they write, "means the email simply won't send, the email ecosystem is naturally more risk-averse than the browser ecosystem when it comes to breakages.  As a result," the EFF writes, "the ecosystem is stuck with a sort of chicken-and-egg problem:  no one validates certificates because the other party often doesn't have a valid one.  And," as they write, "the long tail of mail servers continue to use invalid certificates because no one is validating them anyway."  So why bother?



And there's more.  "Let's imagine," okay, they write, "a future where everyone did have STARTTLS enabled and did have a valid certificate.  So now it becomes safe for everyone to start to validate certificates and insist upon validation.  What could go wrong?"



Well, we still have the classic security downgrade attack.  Both communicating mail servers support STARTTLS.  And they both have valid certificates.  I mean, everybody's valid.  And certificates are being checked; right?  Unfortunately, the initially insecure connection is opportunistically upgraded to a secure one.  So as we know, a man in the middle could intercept the traffic and remove the declaration of STARTTLS support from the connection-receiving server, who is the first one to say, hey, I support TLS.  If you do, let's go.  So as a consequence, the client believes the server it's connecting to, lamely, doesn't support STARTTLS.  And so it shrugs and says okay, fine, we'll use non-encrypted connection.  And so a classic security downgrade attack.



It turns out that this is not just theoretical.  The EFF writes that U.S. ISPs and those abroad have been caught doing exactly this.  And in 2014, four years ago, several researchers found that encryption on outbound email from several countries was being regularly stripped.  So again, unfortunately, if you can, that's what happens.



Now there's DANE.  DANE we've talked about.  Everyone knows I am super excited about the idea of DNS-based Authentication of Named Entities.  That's the idea of using DNS as a global caching sort of universal reference system where this, for example, could be used to solve the problem completely.  If we had consistent and full DANE deployment, that would offer a scalable solution for mail servers which could clarify the certificate validation rules and prevent downgrade attacks simply by publishing in their DNS what it is they support for incoming email connections.  In that case, the connecting server could - it would have pulled DNS anyway in order to get the IP address it needs to connect to.



So in the process it could ask for the DANE record for email authentication; find out, oh, look, this server does support STARTTLS, so we're good to go.  In that case, if the server appeared to say "I don't support it," then the connecting server would say, "Whoops, something's wrong here.  We're not going to send email over an insecure connection that could be secured."  The bad news is DANE relies upon DNSSEC.  And DNSSEC deployment stagnated, as been stagnant for about the past five years and is stuck somewhere between 10 and 15 percent globally.  So that's not going to come to our rescue anytime soon.



Okay.  So fresh off their success with Let's Encrypt, the EFF has set itself three worthy goals to improve this current sad state of affairs.  First, they want to improve STARTTLS adoption.  They want to make it easy to deploy STARTTLS with valid certificates, not self-signed certificates, on mail servers.  So they are developing Certbot, as they call them, certificate bots, Certbot plugins for popular mail server software, starting with Postfix, which is currently in beta.



So the idea would be that this would allow Postfix, much like Let's Encrypt allows a web server to ask for a valid trusted certificate from the Let's Encrypt CA, this Certbot would allow web servers similarly to obtain a no-cost mail certificate, like automatically.  So certificates no longer need to be self-signed.  They can be valid.  They can be free.  So then the EFF says that this first work for Postfix will be followed up by support for the Dovecot and Sendmail servers, and they are open to and welcome contributions of installer plugins for other email servers.



Okay.  So improve the adoption, make it easy for servers to have valid certificates, and not cost them anything.  Second, prevent STARTTLS downgrade attacks.  They say that, in order to detect attacks, they (the EFF) will be hosting a web server policy list of mail servers known to support STARTTLS.  This list will act as a preload list of mail server security policies.  The list already includes a number of big player email domains - of course all the usual suspects, Gmail, Yahoo, Outlook and so forth - and they're actively seeking more.  Use of the list will require mail servers to become list aware, of course; but without this STARTTLS will always be susceptible to downgrade attack.  You know, I have to say my feeling is it would be better just to switch over to the TLS implicit ports and just give up on STARTTLS.  But I certainly respect what the EFF is doing with this effort.



And, finally, the last part of this initiative is lowering the barriers to entry for running a dedicated mail server.  And I was a little puzzled by this one.  They write, when they're explaining this:  "Email was designed as a federated and decentralized communication protocol.  Since then, the ecosystem has centralized dramatically, and it has become exponentially more difficult to run your own mail server."  Okay.  I don't know why that's the case.  I run my own at GRC, and it's not hard at all.



Anyway, they say:  "The complexity of running an email service is compounded by the antispam arms race that small mail operators are thrust into."  Okay, again, not a problem here.  Anyway:  "At the very least, we'd like to lower the barriers to entry for running a functional, secure mail server."  Okay.  Yay for that.  I'm all for it.  I just don't know exactly what it is they're talking about.  They say:  "Beyond developing and testing Certbot plugins for popular MTAs, we're still brainstorming ideas for the decentralization of the email ecosystem."  Ah, so I guess they don't know yet, either, but they have a goal.  "If you work on easy-to-deploy MTA software," they say, "let's get in touch."



So then finally they finish with a call to arms.  They say:  "You can help, too."  They write:  "All of our software packages are currently in a developer beta state, and our team is stretched thin working on all of these projects.  You can help make the email ecosystem more secure by preloading your email domain on our policy list, contributing to or reporting feature requests to STARTTLS Everywhere, helping implement and promote security features like DANE validation in MTA software, and contributing certificate installer plugins for MTAs to Certbot."



So anyway, that's STARTTLS Everywhere.  Certainly it would be good to have it, and there's no reason not to, maybe as a fallback for, if nothing else, it provides opportunistic encryption which is probably supported by both endpoints over the existing ports for systems that are really old and creaky and know about STARTTLS, but don't for whatever reason want to connect to the new TLS ports, which are available across the board for these protocols.  Anyway, a tip of the hat to the EFF.  I'm glad, you know, they always have our backs, and I'm glad for the work they're doing.  And that's the story on securing email.



LEO:  What mail server do you use, if it's so easy?



STEVE:  I use hMailServer on Windows, which is very nice.  It's free.  Lots of features.  My very, very, very favorite one is an anti-blacklisting feature.  Any time, with zero tolerance, a server connects to mine and asks for a non-existent account, they are blacklisted.



LEO:  Nice.



STEVE:  Because what the servers do is they guess.  They just start at Abby and go down to Zeke.  And so, as you know, Leo, I routinely obsolete my email addresses.  And so what that does is any spammers are using an obsolete address from previous years, bang, they are blacklisted.



LEO:  Nice.



STEVE:  So as a consequence, we get no spam.  It really is a nice system.



LEO:  You get a lot less mail, too, because nobody knows your address. But that's another matter entirely because you don't really care.



STEVE:  And that works for me.  That works for me.  



LEO:  I would like to do the same thing.  hMailServer.com.



STEVE:  Great.



LEO:  Yeah.  I've got to find something that simple for Linux.  That would be nice.  To run my own server would be nice.



STEVE:  Yeah.



LEO:  And you don't need a lot of bandwidth to do that because you're not getting a lot of mail.



STEVE:  No, email is typically - yeah.



LEO:  Can you reject HTML mail with it?  You do, don't you?



STEVE:  Oh, yeah.  You can do anything you want to.



LEO:  You could do that, as a matter of fact, yeah.  I think that's smart.  My friends, we've come to the concluding portion of this show in which I remind you that you can watch us do it live every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, at live.twit.tv or TWiT.tv/live.  You choose.  They're different players.  Some people prefer one; some people prefer the other.  In either case, though, you'll get access to a variety of different hosts.  We use YouTube and Twitch, Ustream and Mixer.  There's also audio streaming that we do ourselves.  Or, no, maybe we use Spreaker.  I think we use Spreaker and another service to do that now.



In any event, watch or listen live, if you can.  If you're going to do that, be in the chatroom, irc.twit.tv.  You can join the back channel.  You can also be in studio.  We have two great studio audience members who've been here all day.  They're very patient.  Email tickets@twit.tv if you're going to be in the Petaluma area, Petaluma, Northern California area, and come on by.  We'd love to have you.



If you can't do any of those, you can always get a copy of the show from Steve's server, GRC.com.  He's got audio and really nice transcripts.  You can read along.  And while you're there, check out SpinRite, the world's best hard drive, maintenance, and recovery utility.  Free upgrades; right?  If you buy SpinRite now, you're good through 7.  Is that right?



STEVE:  Yes, sir.  Yes, sir.



LEO:  Don't want to put any words in your mouth.  Okay.



STEVE:  Well, wait, wait.  Through the 6.



LEO:  Through 6.  Seven will be an upgrade.  Okay.



STEVE:  So you get all of the speed and compatibility improvements that are planned for SpinRite for the .x releases, yes.



LEO:  Seven's a total new - yeah, yeah, yeah.  



STEVE:  And actually 7 is going to blow the doors off of data recovery, but we'll save that [crosstalk].



LEO:  Oh, I like that, hmm.



STEVE:  Yep, it's going to do everything that everybody wants in data recovery.



LEO:  Oh, that would be very nice.  Well, we'll find out more about that as time goes by.  There's also stuff there that's free, that's fun, that's useful, that's informative.  It's a great site.  It's one of those sites that it's just you start, and you never - you continue to browse and browse and browse.  There's always something new and interesting there.  GRC.com.



You can reach Steve on Twitter.  That's probably the easiest way to message him or tweet him.  GRC is his company, Gibson Research Corporation.  His Twitter handle is his initials, SG, plus GRC, so @SGgrc.  That's the easiest way to reach him, but you can also go to GRC.com/feedback.  We have audio and video.  God knows why you'd want to watch, but you can.  You know, watch every once in a while, and then you can get an idea of what Steve's moustache looks like today and things like that.



STEVE:  That's right.  The important things.



LEO:  The important stuff.  Now, that's at TWiT.tv/sn.  And of course, if you have a favorite podcast application, and they're all over the place, use that and just look for Security Now!, subscribe, and that way you don't even think about it.  It'll just be there every Tuesday evening when you need it.  Steve, thanks, have a great week.



STEVE:  Thank you, my friend.  Talk to you next week.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.














GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#672

DATE:		July 17, 2018

TITLE:		All Up in Their Business

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-672.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we look at even MORE new Spectre-related attacks, highlights from last Tuesday's monthly patch event, advances in GPS spoofing technology, GitHub's welcome help with security dependencies, Chrome's new (or forthcoming) "Site Isolation" feature, when hackers DO look behind the routers they commandeer, and the consequences of deliberate BGP routing misbehavior.  Plus, reading between the lines of last Friday's DOJ indictment of the U.S. 2016 election hacking by 12 Russian operatives, the U.S. appears to really have been "all up in their business."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here to talk about, yes, even more Spectre-related attacks, highlights from last Tuesday's monthly patch event, GPS spoofing - yeah, it's as bad as it sounds.  And he was blown away by the amount of detail revealed in last Friday's Department of Justice indictment against Russian spies, basically.  He talks about that, too.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 672, recorded Tuesday, July 17th, 2018:  All Up in Their Business.



It's time for Security Now!.  Yay.  The highlight of the week for many of you, I know.  Steve Gibson is here, and we're ready to talk about a world at war.



STEVE GIBSON:  Well, so this was actually initially titled "More Spectre Madness" because, believe it or not, there is more Spectre madness.  Two researchers affiliated with MIT won $100,000 reward just recently by Intel.  Remember that Intel put up a bounty, I think it was a quarter million dollar bounty, through the end of the year for any additional discoveries.  Well, we had some.  And so that was all sort of set up to make that be the big deal.



But then I started the 29-page DoJ indictment which we got last Friday against the 12 Russian agents who, it is alleged, hacked into the Democratic side of the 2016 U.S. election.  And what stunned me, I mean, and it's almost chilling, is the degree of detail.  And of course we're all about technology on the podcast, we and all of our listeners.  And so it just changed the subject of the podcast.  Now this Security Now! #672 for July 17 is titled "All Up in Their Business."



LEO:  What does that mean?



STEVE:  We are all up in the Russian business.  I could not believe, I mean, on this day between this time and that time, such and so did the following Google searches upstream by three hours to research some English to get the English right on a blog posting where they created the Guccifer 2.0 identity in order to rebut the WikiLeaks claims and blah blah blah.  But, I mean, the level of detail, I mean, if I were the Russians, I'd be looking around thinking, what?  I mean, and of course it's always a problem because, when you divulge this sort of detail, there's this assumption of sources and methods.  And so oftentimes you're wanting to keep what you know somewhat under wraps because, if you say too much, then there can only be one or two ways such and such is known.



But anyway, so we're going to wrap up this week by taking a look at some of the allegations in the indictment.  But what's stunning to me and any technologist who listens to the podcast would be, okay, wait a minute.  How do they know that?  I mean, like how retrospectively?  Because this wasn't known at the time.  This had to be forensically, like, gone back and figured out, which suggests a number of things that we'll talk about, about the level of monitoring of the Internet that this suggests we have, which makes Edward Snowden look like he was using training wheels.  I mean, it's just astonishing.



So anyway, but other things happened, too.  As I mentioned, we have new Spectre-related attacks.  We've got some highlights from last Tuesday's monthly patch event; and a couple of interesting advances in GPS spoofing technology where the researchers went beyond, like, telling someone to turn left in the middle of a freeway, which is like, what?, to actually figuring out how to spoof GPS to the level required to give somebody believable, workable, alternative driving routes to take them somewhere of the spoofer's choosing, which opens up a whole - oh, and for less than $300, just using a Raspberry Pi and a software-defined radio.  I mean, so like the bar has been lowered and the capability expanded.  So we're sort of just keeping ourselves up to date on what's possible.



GitHub also added another piece to their very welcome help with informing repository owners or managers of dependency problems with their projects.  Chrome apparently has pushed out - but they didn't push it out to me, so I don't know.  It's supposed to be 99 percent pushed, but I don't know why I don't have it.  But something known as "site isolation" has been added that we're going to talk about.



Also I keep saying, when we're talking about how Shodan can be used to find vulnerable routers that traffic can be bounced off of, our listeners have heard me saying, yeah, just wait till those hackers start looking inside the networks of the routers that they're just currently bouncing traffic off of.  Well, that happened, and with some interesting consequences.



Also, there's been a Portuguese ISP abusing their role as an Internet exchange point.  Well, they're now dark.  Nobody is sending them any traffic anymore.  We'll talk about that, and then wrap up, talking about - oh, I did have a follow-up to last week's discussion of using SpinRite with a RAID where the guy who wanted to run SpinRite on his drives didn't take want to take his RAID down overnight.  Somebody who knows some details about the way that RAID works came up with a way that doesn't require a big rebuild afterward.  So I wanted to share that.  And then we'll talk about this stunning 29-page document that, I mean, it's just - it's amazing what we know.  So yikes.  Really interesting stuff.



LEO:  Yes.  And we can show you the illustration, which you won't be able to read.



STEVE:  Won't be able to read, but you get a sense of the connectivity, yes, our Picture of the Week.



LEO:  Okay, Steverino.



STEVE:  So our Picture of the Week isn't, as you said, super informative because it is the best resolution I could find, but it's blurry.  So I didn't expand it.  It's just not very clear.  But it does show the interconnection of the various players in this drama, what roles they had, where their identities were used and so forth.  So it's just sort of - it's the sort of thing we see with forensic investigations where who was in charge of what and who called who and who had which identities and so forth.  The details in the text are very clear, and we'll be talking about that in a few minutes.



I did want to mention, as I said at the top of the show, and also as I said at the top of the year, about the speculative execution problems, that the problem is, like, fundamental to the architecture of today's processors.  And as I've thought about this more, and I read this paper, these guys, they earned their $100,000, their bounty awarded by Intel for this work.  It is the clearest and most, well, I would say damning or worrisome or earthshaking indictment of any sort of tricks which are played for speculation that I've seen.  I would argue that they have advanced the state of the art in this paper significantly.



And there's really no immediate takeaway for us because the good news is end users, as we know, have never really been at too much risk.  The big risk is where there's a potential to have an adversary sharing the same processor.  Well, since in a single user system like we're all using with our laptops and desktops and our mobile devices, we're the only one using it.  So if malware is in the device, well, we're already compromised.  But in a virtual server, virtual machine cloud environment where the design of the system is multiple customers sharing the same hardware, the problem of the cross-VM information leakage which had until the beginning of the year been just assumed because we had implemented the architectures to isolate processes, that process and machine isolation was a given.  And it was the rare exception, I remember there was a floppy driver a few years ago in the VMware where you could give it some weird instructions and break into the kernel.  But that was the exception to this isolation rule.



Where I'm going with this is that I wouldn't be surprised if, moving forward, the only way we solve this problem is by not allowing sharing of a core and the core's resources, which means its caching also, among systems that may be hostile to each other.  The whole point of speculation is that this processor learns what it's doing and is better at doing it in the future.  That's what caching gives us.  It's what branch prediction gives us.  It's what all of these tricks where the system only runs at the performance it does because it's remembering, it's taking advantage of the fact that code that has just run tends to run again.



Without that, you know, main memory is just incredibly slow relative to the processor speed, which is why we have all these multimeg caches now in order to bring that local.  I just don't see how it's possible to share that without carefully tagging which core owns which items in the cache, which could be done.  And possibly just giving up on the idea of a multicore processor being considered as a symmetric thread pool where anybody can run any thread from any core.  It may be necessary to partition in the future, to partition in these environments where hostility is possible, to partition cores to virtual machines so that there just isn't cross-core leakage because I can't see any way for someone to securely share a single core, given that we need speculation, unless everything was tagged.



And frankly, tagging everything would not reduce performance, and it would solve the problem, but at horrendous hardware expense.  That is, right now the presumption has been there was no need to tag everything.  And when I say "tagging," I mean identify which process caused what modification for the future which would again, if the tags match, we reuse what we learned of that process.  Another process is not going to be using the same architecture.  So we actually could see an improvement in overall performance by heavily tagging what the processor learns about what the processes are doing on it, and we would get a performance benefit because we wouldn't be flushing what we learned from one process by another, which would be a benefit.



But, boy.  And it doesn't really - it's not a conceptual stretch.  Everybody knows how to do a fully tagged system.  But it would expand the die size and increase the complexity of the hardware significantly.  Either that or strict processor partitioning is probably where we have to go.  Anyway, so I'll just say that what these guys did, they named these two new findings which fell out of their very powerful research, they called them Spectre 1.1 and 1.2 because they're related to the original Spectre 1.0 flaw.  Spectre 1.0 was all about speculative reads.  What they realized was speculative stores could be used in a similar fashion.  And so they've extended our understanding of the speculation risk significantly, essentially by not missing anything in their analysis.



And what's significant to us, well, or to the industry, is that their approach bypasses the mitigations, the software mitigations which were put in place to solve the Spectre 1.0 problems, meaning that there is no current protection from what these guys have done.  And at the same time there also never was, as we've said, a huge risk to end users.  The risks have been bigger to cloud providers.



And so I don't know whether Intel will respond, whether they can.  It may be that we'll see another round of software mitigations.  We might see more microcode, again, thinking that we were finally done after half a year of this microcode mess that we went through.  This just happened.  So there is yet no real sense.



Intel has acknowledged that they - in addition to paying $100,000 they've acknowledged that, yes, their systems are vulnerable to these two 1.1 and 1.2 vulnerabilities, as has ARM.  AMD so far has been silent.  But AMD is traditionally slower to respond to security issues.  They're looking at it, I'm sure, and will probably end up saying, yeah, this gets us, too.  Because basically this just puts another nail in the speculation coffin.



And, I mean, the paper is amazing.  If anyone, again, I will just - I've got a link in the show notes for anyone who's interested.  It's a tour de force in really understanding the nature of this problem and argues convincingly that there isn't, as I said, there just isn't a safe way to do this.  If something your code does changes the future, and it has to in order to get the performance it needs, then if that changed future can also affect a different process, you've got cross-process information leakage.  And as we saw recently, and as these guys reiterated in their paper, things like fuzzing the timing in the browsers only slowed down the bandwidth or the bit rate of information leakage.  It didn't end it.  So it's still a problem.



Anyway, really, really great work.  And in fact Bleeping Computer covered this and had on their page a really nice summary of where we've been so far this year with Variant 1, and now we have 1.1 and 1.2.  And of course we have 2, 3, 3a, and 4.  And so it just, you know, and 4 was, remember Spectre Next-Generation, or SpectreNG.  So no word yet on AMD's effect, nor anyone's formal response.  This all just happened.  But there's no reason to panic because, as we've seen, even the original problems were more of a computer science concern that we had to take seriously because everything we've learned says these problems only get worse.  And this is what we're seeing.  I mean, now, as our understanding seven months later is continuing to mature, it's looking to be bleaker than we even thought.



So hats off to these guys.  And they really did earn their hundred grand from Intel.  And than you to Intel for offering a bounty.  I'm sure they would have done the research anyway.  But if it encourages people to spend the time, which was extensive in this case, to understand it, then yay.



Last Tuesday was the second Tuesday of the month, and Microsoft fixed 53 vulnerabilities occurring across 15 of their products.  Nothing really stood out except I did want to close the loop on a topic we discussed a couple weeks before, which was the Lazy FP State Restore.  This was yet another vulnerability, not technically speculative, but this was the one where Intel added a feature where the expensive-to-restore state of the floating point unit, due to all of the large registers that floating point math uses, having to reload them when a thread starts to execute is expensive.  And so someone somewhere I'm sure did some profiling years ago and said, hey, you know, sometimes a thread gets execution, and it never does any FPU stuff.  So let's not bother to restore the FPU registers unless it tries.



Anyway, so yes, there's another optimization.  And sure enough, it can be abused in order to leak information, as we saw.  So Microsoft did issue a patch for that, essentially not taking advantage of that optimization.  And as we also mentioned when we talked about it the first time, a number of other OSes had already decided the edge wasn't even worth the complexity that it added.  So they had backed off and were never affected by it.  So now, after last week, Windows isn't either.



The winner of last week's Patch Tuesday was Adobe, who fixed more than twice as many vulnerabilities, count them, 112.  And also what was interesting was how lopsided their location was.  Almost all of them were in Adobe Acrobat and Reader.  They published two for Flash Player, three for their Experience Manager, three for Connect.  And the balance, 104 vulnerabilities, were fixed in Acrobat and Reader.  And we would argue that Acrobat and Reader are Adobe's largest interpreters.  And as we know, interpreters are notoriously difficult to make work in a way that prevents their abuse.  And sure enough.



And what I'm interested in knowing - there's no information about this without a lot of digging.  It would certainly be possible to determine.  But I'm wondering how far back the vulnerabilities go and how many of them were introduced in the last, oh, say 10 years because we had Acrobat Reader 10 years ago, and it worked just great.  And PDFs popped up, and you could read PDFs, and everything seemed fine.  And I just wondering what percentage of these are new problems that Adobe has introduced into their own software as a consequence of continually adding new features because that's what they have to do for their own corporate interest.



I just wish, once the bugs are out, they would leave it alone because, as we know, we had that e=mc2 Picture of the Week a few weeks back, and we know that the number of security vulnerabilities tends to increase exponentially as code continues to grow in size because it's just difficult to keep everything straightened out and not interacting with each other.



Anyway, so all of those are fixed.  I'm sure everyone has updated Windows by now.  And of course, as we know, it's becoming increasingly important to do so, maybe not for the typical end user, but certainly if you're any sort of an attack target.  And I'll make sure to mention it when we're talking about the way the Russian agents managed to get into these systems.  Nothing was super high-tech or fancy.  Largely it was social engineering.  And I just completely distracted myself and lost the thread.



LEO:  What you looking at?  Squirrel!



STEVE:  Yeah, it's like, where am I going?  What is that about?  I have no idea.  Oh, about if you might be the target of attacks, then you really - we know that when vulnerabilities are patched they are now often very quickly reverse engineered to figure out what it is they fixed.  And there's then a window of opportunity that opens, during which time, from the time the vulnerability is known to the time the target system actually gets updated, someone can be taken advantage of.  And if nothing else, as a consequence of all the press that has occurred in the wake of the U.S. election two years ago, there can't be any navet on the part of those involved in the political system.  As we will see here by the end of the podcast, people were clicking on links they shouldn't have.  And the rest now is the subject of an indictment with lots of details.



Okay.  So the paper was jointly published by researchers from Virginia Tech, the University of Electronic Science and Technology in China, and Microsoft Research.  Those of us who have been around the Internet for a while, and certainly you and I, Leo, will recognize the play on words where the paper's title was "All Your GPS Are Belong to Us."



LEO:  He he he he.



STEVE:  Yes.  It was "all your base"; right?  "All your base are belong..."



LEO:  "All your base belong to us," yes.



STEVE:  Yes.



LEO:  It was a poorly translated videogame.



STEVE:  Right.  So this is "All Your GPS Are Belong to Us."  The subtitle was "Towards Stealthy Manipulation of Road Navigation Systems."  So essentially what's happened is, as a consequence of this research, the GPS spoofing state of the art has leapt...



LEO:  "Somebody set us up the bomb.  We get signal.  What?  Main screen turn on."  This is the game.



[Clip]  All your base are belong to us.



STEVE:  And that became an Internet meme that was quite entertaining for some time.  "All your base are belong to us," yes.



LEO:  "All your base belong to us."  From Zero Wing, a game that came out in 1989.  



STEVE:  Okay.



LEO:  Yeah, so it's a pretty old meme.



STEVE:  Yeah.



LEO:  But it's still wonderful.  



STEVE:  It lives on, lives on.  So, okay.



LEO:  Lives on.



STEVE:  So, okay.  So what these guys did was to take off-the-shelf, readily available equipment.  They said:  "We show that adversaries can build a portable spoofer" - that is to say a GPS spoofer with low cost, they say about $223 - "which can easily penetrate the car body to take control of the GPS navigation system.  Our measurements show that effective spoofing range is 40 to 50 meters, and the target device can consistently latch onto the false signals without losing connections.  The results suggest that adversaries can either place the spoofer inside or under the target car and remotely control the spoofer, or follow the target car in real time to perform spoofing."



So the spoofer hardware was four components:  A HackRF One based frontend, which is the software-defined radio; a Raspberry Pi; a portable power source, in this case it's a rechargeable battery; and an antenna.  It fits in a small box, like smaller than the length of a pen, which they showed in their photo to give some sense of scale, and just uses readily off-the-shelf equipment.  What they did, the way they went further than had been before, was that GPS, you know, fouling up GPS has been possible.  But essentially replacing valid GPS navigation data with spoofed navigation data which could be acted on had never been done before.  And that's what these guys did.



So one can imagine, I mean, we've talked about how, as autonomous vehicles happen, interstate and cross-country truck driving may end up being replaced by automation.  And certainly the GPS system would be one of the main signals that these systems would take in.  So you could imagine a scenario, and I'm sure it'll be the subject of fiction if it isn't already, well, now it's much less fictional than it was, where a high-value cargo was rerouted, literally, by not coming into contact with the self-driving truck in any fashion, but by spoofing its GPS so that it thought it was somewhere that it wasn't and causing it to take a wrong turn, literally, and then continue on a course to an alternative spoofed site.



That's what these people did.  They developed the technology.  They came up with a system which, when tested against 40 humans, 38 of the 40 did not detect anything amiss.  That is, they were following their turn-by-turn navigation.  Everything looked fine.  They believed what their navigation system was telling them.  And the system managed to bring them to a destination well away from where they were intending to go.



So again, it's not difficult to imagine this being put to various nefarious ends.  So the state of the art has jumped from something like where your navigation system is clearly wrong, and many people have had mapping software fail and tell them to drive off the pier, and they say, no, I don't think I want to do that.  In this case, it just tells them to take a turn or a series of turns where everything they're doing matches the map that's being seen, but they're not at the destination they believe.  So interesting piece of work by those three groups.  And they talk about the need to harden GPS and navigation, that is, not to just blindly believe the positioning information that we receive because, as this demonstrates, it can't be believed.



Back in November, GitHub announced a very cool new service for the maintainers of various repositories which they said was going to be applied to Ruby, JavaScript, and Python, although until last week Python hadn't been implemented.  It's not clear to me, except it's just a lot of work probably necessary to get it done.  But Ruby gems and JavaScript NPM all had the benefit from last November.  And Python just got it.  And what it is, is really a nifty service that I was delighted - I missed it last November and didn't pick up on it until just now, where GitHub is now looking at the manifests of the projects in the repositories and scanning them for known vulnerabilities in the dependent software packages upon which the target projects are built.



So there's a tab in GitHub called the Insights tab.  And one of the subcategories is Dependency Graph.  And what they've been doing for Ruby and JavaScript since November and have now added Python projects is taking responsibility, within reason, for notifying the maintainers of these repositories if at some point in the future a vulnerability is discovered in any of the packages that their projects depend upon.



So, for example, back when they were explaining this in November they wrote:  "When GitHub receives a notification of a newly announced vulnerability, we identify public repositories," and then they said, "and private repositories that have opted in to vulnerability detection that use the affected version of the dependency.  Then we send security alerts to owners and people with admin access to the affected repositories.  You can also configure security alerts for additional people or teams working in organization-owned repositories."



They said:  "We detect vulnerable dependencies in public repositories by default."  And this looks like they're repeating themselves:  "Owners of and people with admin access to private repositories can also opt into vulnerability detection for the repository.  For more information, see 'Opting into or out of data use for your private repository.'"  Oh, and they did also note that they will never show the information on this dependency graph page to anybody who's not an admin.  So you need to be an admin in order to see it.



But they will proactively notify, or if you go to the Insights tab under the Dependency Graph, you will then see, for projects that qualify, a clear security warning if your project depends upon something with a known security vulnerability.  And they encourage you there to update yourself to the latest version that has been fixed.  So just a note for those who are maintaining projects in GitHub that that's there; and, if you weren't aware, that they just added that feature for Python.  And I wasn't aware of it at all, so I'm glad to have found out.  Very cool.



Okay.  Now Google and Chrome.  The claim is that, starting with I think it's Chrome 63, a new capability was added, but was disabled by default.  And they call it Full Site Isolation.  And this week's news is that Google has enabled this site isolation feature for 99% of Chrome's desktop users.  Well, I'm using the latest 67.0 point whatever it was, let's see, 67.0.3396.99, which is current.  And in digging into this, I wasn't enabled yet.  So our listeners can check.  And Leo, you can.  If you open Chrome and put chrome://flags into the URL bar, there's just too many of them.  So then you need to search, just put the word "isolation" in, which brings up a nice subset of individual flags that Chrome knows about.  The first one that comes up is named Strict Site Isolation.  And are you disabled on all of those?



LEO:  No, it looks like, well, let me look at Isolation first.



STEVE:  Looks like enabled.  Oh.



LEO:  Disabled, disabled, no.  Here's disabled for strict site isolation.



STEVE:  Yeah, yeah, yeah.  So...



LEO:  The Trial Opt-Out is default.  The others are default.



STEVE:  Yeah.  So I don't know why, I mean, maybe this is like just about to happen.  Okay.  So what they're doing is interesting, which is why we're talking about it.  We've known that Chrome has adopted a process per tab approach, and that Firefox recently has gone to something similar, the idea being that we're trying to minimize the attack surface.  What Chrome is doing is utilizing the operating system's interprocess isolation and allowing the browser to inherit that so that web pages occupying separate tabs are in separate processes.  And essentially that means that the renderer for a tab, and that's the thing that reads the page, reads the JavaScript, does everything, it's in a process that has no contact with another tab's process and thus renderer.  Whereas in the old days, when you just ran a browser and up came one process, well, inherently there was some risk because all of the tabs were being shown by a single process.  And if something could break out of its own tab, potentially any other data currently open in the browser was available.



So first thing that Chrome did was to break out a single process per tab.  This goes further.  This is a single process per site, meaning per domain.  Which means multiple processes per tab if a single tab hosts content from multiple domains.  And as we know, many tabs do.  If you've got ads in iframes, those are from other domains.  Now, the reason this is off is that this incurs a significant overhead.  And Google's aware of that.  So they're saying between a 10 and 20% memory overhead and lots more processes.



So right now, I mean, maybe they've already backed off of it, which is why by the time I saw this happened it had already been turned off.  I don't know.  Our listeners can experiment with it if they're interested because it's right there.  You put chrome://flags, search for "isolation," and there is Strict Site Isolation.  And Chrome says it is a security mode that enables site isolation for all sites.  When enabled, each renderer process will contain pages from at most one site, that is to say, one domain, they say, using out-of-process iframes when needed.  Again, meaning that, if you have an iframe, the content in that iframe will not be rendered by the page's process.



Chrome will launch a new process containing a web rendering engine to render the contents of that iframe and show it in the tab.  So what this does is it really increases security by creating process boundaries, now with subpage process boundaries, but at a potentially significant expense.  What's exciting is that, as part of the announcement of them deciding to go mainstream with this - but it was supposed to be Chrome 67.  Like today's Chrome is supposed to have this on, and you and me - or you and I, sorry.  You and I, Leo, both have them disabled.



LEO:  By default, yeah.



STEVE:  So, yeah.  So maybe it's still coming.  Maybe they decided to take it more slowly.  There is an interesting - down toward the bottom was site isolation, or the second one that comes up is Site Isolation Trial Opt-Out.  What Google was going to do, with the instrumentation that they have, was to just sort of turn this on for some people and monitor what it means for the browser in the real world.  And so what they did was they offered an opt-out override for people who didn't want to have this turned on for them by Google and suddenly, I mean, I run often with Task Manager open, and I'm like looking at all the Firefox processes, and when I have Chrome open, all the Chrome processes, even when there's, like, not a lot is going on.  And so I'm thinking, wow.  If you went to a frame-heavy page with this turned on, you probably have to scroll your Task Manager through pages of Chrome.exes in order to see what's going on.



So, oh, what I was going to say was when as part of the announcement they were so bullish about this that they were planning, if this went mainstream, to back out of and remove the Spectre mitigations.  That is, their feeling was, if they could break a page apart into per-domain processes, then it's no longer necessary to fuzz the JavaScript timers.  And they specifically said that the shared array buffers that we were talking about a couple weeks ago as something, a feature that was not going to be implemented with WebAssem because it allowed high-resolution timers to be crafted by crafty JavaScript, they were saying they're going to put shared array buffers - allow them again, and not worry about Spectre because it was only - apparently it was hostile iframes that they worried could break out of local page containment and affect other and get information from the same page.



So now they're saying, well, if we move forward - and this has like been going on since 63, which is earlier this year.  If we move forward and give a process per iframe, then we don't have to worry about hostile iframes and ads and so forth, hosted content from other domains on the main page having that level of granularity and access.  So anyway, it'll be interesting to track this.



Again, anybody who's interested, you can turn it on and see what happens.  I haven't done that yet.  I think I will after the podcast because I'm just curious to see.  You have to restart Chrome afterwards so it can come back up in its new mode.  Be interesting to see what it means in terms of performance.  It's going to slow things down because you're talking about launching processes.



There is some logic in Chrome to recycle existing processes because they recognize it takes a while to launch a process.  So rather than, as you move among pages, rather than launching and destroying processes, Chrome probably creates a process pool.  But I salute them for the work they're doing.  I mean, this is the future of how we stay secure when we're using a browser which is increasingly becoming our portal to the world, and also needing to thwart the bad guys.  So the idea that you could use this rather than browser-based Spectre mitigations, which as we're seeing only slow things down, don't really solve the problem, that's really encouraging.



But at this point, anyway, I don't know whether they've already said ouch and backed off, or maybe it was a prospective announcement when they said it was enabled for 99% of Chrome desktop users.  Or maybe, Leo, you and I are just a small little 1%.



LEO:  I can see why they wouldn't want to enable it, if it's that much of a hit.



STEVE:  Oh, yeah.  I'm terrified.



LEO:  I'm surprised that they actually were thinking of enabling it by default.



STEVE:  Yeah, yeah.



LEO:  I mean, you think it's important for security, obviously; right? 



STEVE:  It would definitely enhance security, yes.  And I have a link to a support page in the show notes here under Google Chrome Help.  It says:  On your computer, open Chrome.  In the address bar at the top, enter chrome://flags/#enable-site-per-process, that's hyphenated, and press Enter.  Next to Strict Site Isolation, click Enable.  If you don't see Strict Site Isolation, update Chrome, meaning if you don't have the latest, and you and I both do, Leo.  And then click Relaunch.



And so under Threat Model they said:  "For a 'one-site-per-process' security policy, we assume that an attacker can convince the user to visit a page that exploits a vulnerability in the renderer process, allowing the attacker" - and remember, the renderer is a massive interpreter - "allowing the attacker to run arbitrary code within the sandbox.  We also assume that attackers may use speculative side-channel attacks, for example Spectre, to read data within a renderer process.  We consider attackers that want to steal information or abuse privileges granted to other websites."



And then they finally said, under Requirements for this mitigation, they said:  "To support a site-per-process policy in a multiprocess web browser, we need to identify the smallest unit that cannot be split into multiple processes."  That is, they don't want to break anything.  They said:  "This is not actually a single page, but rather a group of documents from the same website that have references to each other.  Such documents have full script access to each other's content, and they must run on a single thread, not concurrently.  This group may span multiple frames or tabs, and they may come from multiple subdomains of the same site."



So they are saying that you could have multiple tabs from the same site, that is, from the same domain.  Those would be safe to share a process.  But so they're sort of refactoring the processes and the tabs so that multiple processes might be using the same page but would not be sharing the same renderer, which seems like a good thing.  It'll be interesting to see whether this appears in the future, or whether it's just something that they were experimenting with, but when they actually ran real-world instrumentation turned out to be too expensive.



Okay.  So I've been saying for, well, for as long as we've been talking about Universal Plug and Play problems with routers and the fact that routers are now being used increasingly as routing nodes on the Internet, that we're all sort of, well, not we all, but those who are blindly ignorant of the fact that their routers have been commandeered and compromised are lucky so far that the hackers are more interested in outward facing DDoS redirections and spoofing and using the routers to anonymize their source than they are interested in what is going on on the LAN side behind the router.



Well, it turns out that at least one hacker did get curious.  The security firm Recorded Future discovered sensitive military  documents being offered for sale on various hacker forums.  Bleeping Computer in their coverage of this reported that some of the sensitive documents put up for sale, and this is from what Recorded Future had said, include maintenance course books for servicing the MQ-9 Reaper drones, various training manuals describing deployment tactics for improvised explosive devices, an M1 Abrams tank operation manual, a crewman training and survival manual, and a document detailing tank platoon tactics.  And the hacker was asking between 150 and $200 for these, which was considered, like, almost nothing, given how sensitive some of this information was.  Recorded Future said that it engaged the hacker online and discovered that he used Shodan to hunt down Netgear Nighthawk R7000 routers that are known to use a default FTP password, believe it or not, Leo.



LEO:  Admin/admin.



STEVE:  Ugh.  So the people using these routers - and oh, by the way, there's 4,000 of them - turned on the FTP server, exposed it to the WAN, and didn't change the default login.  So the hacker used the default FTP password to gain access to some of these routers whose owners had not bothered to change the default.  Based on the documents and the details he shared online and with the researchers in private conversations, that is, with Recorded Future, one such location was the 432nd Aircraft Maintenance Squadron Reaper AMU OIC, whatever that is, stationed at the Creech Air Force Base in Nevada.



Here, writes Bleeping Computer, "he used his access to the router to pivot inside the base's network and gain access to a captain's computer, where he stole the MQ-9 Reaper manual and a list of airmen assigned to Reaper AMU," which must be some sort of maintenance unit.  "The MQ-9 Reaper drones are some of the most advanced drones around and are used by the U.S. Air Force, the Navy, the CIA, Customs and Border Protection, NASA, and other militaries of other countries."  So anyway, as I said, the routers have default credentials.  When Netgear was notified of this two years ago, in 2016, they responded by putting up a support page with information on how users could change their router's default FTP password.  Which boggles the mind.  I mean...



LEO:  They've soldered it in or something.



STEVE:  Just incredible.  Oh, boy.  Again, I'm gobsmacked, as they say in the U.K., that first of all, I mean, I have to hold Netgear responsible for not somehow using a random admin and password which could have just, I mean, like the first time the router comes up it could see that they're blank and just use random stuff for the admin and password fields, which the user could copy and then use if they chose, or change to something that they want them to be if they don't like random gibberish.  But to have them start up in the firmware as statically global known things, I mean, it's just...



LEO:  Most routers do that, though.  Like just for users, admin/admin.



STEVE:  I know.  I know.  I mean, and this is a practice that just has to change.  The firmware should see that they've not been initialized when it comes up.  And then just based on - it could use packet noise.  I mean, there's plenty of sources of entropy available to a router.  So, I mean, now a lot of the new - these routers typically use ARM chips, and they've got good random number-generating hardware in them.  So just pull some random numbers.  Put gibberish in there that will annoy the user.  Well, which, first of all, the user could copy and use, which would be very good username and passwords, which would never occur again.  Or if the user doesn't like them, then they can change them to be what they want.  But just the idea that they're just, I mean, it's one thing to have the LAN side admin default to admin/admin.  But to have the WAN side ever default to anything is unconscionable from a security standpoint.



LEO:  If you defaulted to WAN Administration Off, that would do it; right?



STEVE:  Well, but clearly these people wanted remote access.



LEO:  Oh, they turned it on; right.



STEVE:  They turned it on.



LEO:  C'mon, though.  You've got to give the user some responsibility.  If you're going to turn on WAN access and leave the default password, you're kind of at fault; right?



STEVE:  I don't disagree, except that defense in depth is the goal here.



LEO:  Right.  The router needs to help you, yeah.



STEVE:  Yeah.  Why not?  Why not have it do everything it can?  



LEO:  I'll tell you why not.



STEVE:  I mean, it's just so trivial.



LEO:  Yeah, but you know why not.  Because Netgear doesn't want all those calls from people saying, "Hey, I didn't write down that password."  Right?  I mean, I've reset routers many times because I forgot the administrative password.



STEVE:  Yeah.  Which is a good thing.



LEO:  Yeah.



STEVE:  Because that means that you didn't use the default.



LEO:  Yeah, right.  That's true.  I guess it could do that, though, every time it's reset - generate a new set, new password, using your method.  Yeah, that would be a good thing to do.  It's just a lot of coding; you know?  That's work. 



STEVE:  Yeah.  Oh, darn.  Right.



LEO:  Oh, darn.



STEVE:  Oh, darn.  Okay.  So when BGP abuse becomes sufficiently blatant, even the reluctant to respond with anything anybody could call a kneejerk reaction, Internet managers finally do.  So we've talked about BGP routing in the past, and we've talked about mistakes that do happen from time to time.  As we know, the big routers on the Internet connect to multiple other routers, and they build a routing table which instructs them how to forward incoming packets bound for, like, trying to go out another one of their connections.  So what happens is, when the router has a table of routes that it knows how to forward packets for, it advertises that in this Border Gateway Protocol, BGP.  So that allows the other routers it's connected to to know that, oh, if the packet is bound for somewhere else, where should they forward their traffic to?



So this is the way that the Internet self-organizes, and it's brilliant.  And what it really does is it actually organizes itself, which thank goodness we have technology to do that.  Otherwise it would always be broken and out of date and so forth.  So when someone hooks up a new IP subnet to a router, they say, okay, this range of IPs is going to be connected to this interface that goes off to this customer.  So that update propagates through.  Using BGP, it propagates far enough out until that subnet becomes incorporated by a larger subnet that means that the route doesn't need to propagate any further.  In other words, and we've talked about this, how the classless interdomain routing, where you look at an IP address as a prefix and a suffix, where you can route everything with the same prefix to the same destination, hugely simplifies this task.



Okay.  So we're only human, we people who maintain routers on the Internet, and from time to time a mistake gets made.  So when that happens, somebody entering into the router which range of IPs this new interface should be connected to might put a typo in.  And the router doesn't know it's a typo.  It says, oh, okay, I'm now in charge of this block of IPs.  They go out that interface.  So just as happens when it's the right news, it happens when it's the wrong news.  And so suddenly an advertisement, as these things are called, these updates are called, an advertisement goes out, advertising that it is responsible for a block of IPs it is not actually in charge of.



And there's no, like, overlord.  It's all sort of a peer agreement system where everybody hopes not to do any typos, and things more or less work.  Except they don't.  Then something breaks.  And then suddenly all of the Internet gets routed to some one location that melts down because it's way too much traffic for it, and they go whoops, and they remove that mistake from the routing table, and then the Internet repairs itself, and we go back to business as usual.



Unless it's done deliberately.  And there's nothing to prevent this being done deliberately.  As we've discussed over the years, there's a lot of the Internet that is still dark.  There are a lot of people squatting on IP allocations that they may grow into over time.  There have been some great companies, like Hewlett Packard, that had three, I think it was, Class A networks, and gave a couple of them back, thus releasing, what is it, 16 million IPs back to the world, essentially, for reallocation, and things like that.  Or companies that have been able to, you know, that were sitting on a Class A and realized, you know, we're never going to need 16 million.  So they pushed themselves all down toward one end and then gave up half of their allocation.  Thank you very much.



But what there still are, are lots of small little bits all over the place, which nobody - the owners don't want to give up because they might legitimately grow into them.  And they're not making anymore IPv4 IPs, as they say, so people are holding onto the ones they've got.



Okay.  So it turns out that for years a BGP provider, an exchange provider in Portugal named Bitcanal has been reselling, for lack of a better term, profiting from certainly, IPs that it doesn't own.  It's been advertising, deliberately advertising small blocks of IPs that its router then says it owns.  And because they're small, they tend not to get coalesced with other prefixes - that's why I mentioned this before - which allows them to stay separate as they propagate because they're just small little /24 networks that don't stand out very much.  And also they sort of tend to stay off of people's radars that way.  They're just little bits of space that are slack in the system that everybody knows about, but they're sort of where they're supposed to be.



So NANOG, N-A-N-O-G, is the North American Network Operators Group.  And Ronald Guilmette started off his posting to the NANOG mailing list saying:  "I mean, seriously, WTF?"  And then he went on to say:  "As should be blatantly self-evident to pretty much everyone" - and Leo, if you're at this place in the show notes, I have a Pastebin link that's worth clicking on just to show these are the current misallocation of IPs by this Portugal-based, this Bitcanal ISP.  These are the IPs that they have stolen from the Internet.  Statically, they have them.



He says:  "As should be blatantly self-evident to pretty much everyone who has ever looked at any of the Internet's innumerable prior incidents of very deliberately engineered IP space hijackings, all of the routes currently being announced by AS3266" - that's the Autonomous System number of Bitcanal.  And he says, parens:  "(Bitcanal, Portugal) except for the ones in 213/8" - and so that's the one network that they are entitled to - he says:  "...are bloody obvious hijacks."



LEO:  Wow.



STEVE:  He says:  "That's 39 deliberately hijacked routes, at least going by the data visible on bgp.he.net.  But even that data from bgp.he.net dramatically understates the case, I'm sorry to say.  According to the more complete, up-to-the-minute data that I just now fetched from RIPEstat, the real number of hijacked routes is more on the order of 130 separate hijacked routes, for a total of 224,512 IPv4 addresses," and then the Pastebin link.



He says:  "In simpler terms, Bitcanal has made off with the rough equivalent of an entire /14 block of IPv4 addresses that never belonged to them.  And of course they haven't paid a dime to anyone for any of that space."  Anyway, he alleges, and I think it's pretty clear it's the case, that Bitcanal is doing all of this, this hijacking of BGP routes, for the purpose of reselling, and in fact they know for a fact, reselling the hijacked IP addresses to spammer groups.



LEO:  Yeah, of course.



STEVE:  Which in turn use them - yup.



LEO:  Because they don't care if the address stops working after a while.



STEVE:  Exactly.  They just want IPs that are not in the current blacklists.  And so this allows them to establish connections to SMTP servers from unknown IPs and to stay hidden that way.  So anyway, the upshot is this finally rose to the level where - and again, it's just like we've seen with Google making moves very deliberately, but also in a very considered fashion, or the larger industry pulling a certificate authority's rights to sign.  I mean, you have to really be bad in order to have the industry say, okay, look.  You made a mistake.  You didn't report it.  You lied about it when we told you.  Then you didn't tell us about the other things that you found out that you also did, blah blah blah blah blah.  I mean, we've talked about this through the years.  What finally happened was they went dark.  The Internet said goodbye, and basically no longer routes any traffic to that Portugal-based ISP, nor accepts any from them.  They are out of business as a consequence.  And maybe we'll see a drop in spam, which would be a nice side effect of that.



LEO:  Yeah.



STEVE:  And I did finally want to finish up, I wanted to follow up from a comment last week.  John Doe, as he described himself, writing from Moonbase 17 - which I don't think is actually a place.  The subject was "MDADM RAID 6 SpinRite Comment."  This was sent on the 12th of July.  He said:  "Hopefully this gets to you in time, or you can forward it to the person that asked the question."  Or I can share it on the podcast so that all of our other listeners who may find this interesting, and actually it is kind of cool, may benefit, as well.



He says:  "On your last podcast (SN-671) there was a question about running SpinRite on a five-disk RAID 6 with MDADM on Linux."  And get this.  Here it is.  "If the person turns on write-intent bitmaps, then the rebuild time can often be reduced to a few minutes when a drive is put back into the RAID."  He says:  "I see five- to 10-minute resyncs."  I have a link in the show notes to the raid.wiki.kernel.org that describes it.  And in that write-up they said:  "When an array has a write-intent bitmap, a spindle," they said, "a device, often a hard drive, can be removed and re-added.  Then only blocks changed since the removal, as recorded in the bitmap, will be resynced."



And then they add:  "Therefore a write-intent bitmap reduces rebuild/recovery time if the machine crashes with an unclean shutdown; or one spindle is disconnected, then reconnected."  And then they go on to say:  "Write-intent bitmap support is only available for RAID geometries causing data redundancy.  For example, as RAID 0 has no redundancy, it cannot be inconsistent.  So there is nothing to record in such a bitmap."



But anyway, this was very cool.  What it means is that, while the drive is offline with SpinRite running on it, the RAID can continue to stay up.  It can continue to function.  And when the SpinRited drive rejoins the RAID, the write-intent bitmaps will be compared, and essentially an incremental update will be made to the rejoined drive that only takes a few minutes and avoids a full RAID rebuild.  So thank you very much for the tip.  And for anybody who's interested, it makes it easy then to pull a drive out, let SpinRite run on it, even SpinRite 6 as it is now, and then have it rejoin the RAID later without a great impact to the RAID.



LEO:  That's cool.  I didn't know about that.  That's really fast, yeah.  Mr. "Zero Trust" Gibson is here.



STEVE:  Yes.  Leo, you would have fun pronouncing the Russian names of these agents.



LEO:  I love it.  I love it.



STEVE:  Yes.  I, however, am unable to do that.



LEO:  Viktor Borisovich Netyksho.  Boris Alekseyevich Antonov.  Dmitriy Sergeyevich Badin.  I love it.  You're right.  I am having fun.



STEVE:  I knew you would.  The good news is their first names are almost unique.  There's Victor, Boris, Dmitriy, Ivan, Aleksey, Sergey, Nikolay, Pavel, Artem, Aleksandr, Aleksey, and Anatoliy.



LEO:  Sounds like the bridge of the Starship Enterprise.



STEVE:  Well, there are two Alekseys.  So there's a little bit of - there's Aleksey Lukashev.



LEO:  Aleksey, yeah, Lukashev, yeah.



STEVE:  Lukashev and Potemkin, yes.  So other than that, their names are unique.  And as I said at the top of the show, I was going to talk about, I was going to title this podcast "Spectre 1.1 and 1.2" or "Spectre Keeps Delivering" or something.  But I read the 29-page indictment, and I was just stunned by the level of detail that it alleges about the behavior of these agents.  And, I mean, it's amazing.  And so...



LEO:  I think there are addresses.  The unit they're in command of.  What they did?



STEVE:  Yes, yes.



LEO:  It feels like we had a tap in here; right?  It feels like that.



STEVE:  Well, the only thing I could think is that, to be able to do this retrospectively, the NSA, and I guess it makes sense, that's what the huge server farm, the hard drive farm in Utah is doing is literally recording all traffic, maybe not interstate, but international.  That is, it must be that the trunks coming in and out of the United States are just - they're just being recorded.



LEO:  Which, by the way, I just want to point out, there's a reason why there's all this detail in the indictments.  It's just a little warning shot, isn't it, for anybody else who might be involved, that we know everything.



STEVE:  Yes.  So as I'm reading this, I'm just - I'm stunned by the level of what is understood.  First of all, it starts out with those 12 people.  It names them each and describes their job functions and their lines of reporting and which aspects of the hacking they were involved in.  So it sort of gives you a per-defendant overview of who did what.



LEO:  We rarely see this kind of detail when you hear about a hack.  I mean, they talk about exactly how they executed it.  It's amazing.



STEVE:  Yes.  And I highlighted - I didn't want to go through - I got overly ambitious as I [crosstalk]...



LEO:  [Crosstalk] the entire indictment into the show notes.



STEVE:  I know.  I was trying to skip things.  And I won't drag our listeners through it.  But, for example, well, actually it reads:  "For example, on or about March 19, 2016, Lukashev" - and I don't remember his first name.  He was, where is he, Aleksey...



LEO:  Lukashev, not Potemkin.  Get the right Lukashev.  Get the right Aleksey.



STEVE:  Right, right.  He's the - we have to disambiguate him.



LEO:  Yes.



STEVE:  Created and sent a spear - now, get this.  On March 19th this guy in Russia "...and his co-conspirators created and sent a spearphishing email to the chairman of the Clinton Campaign.  [Aleksey] used the account 'john356gh' at an online service that abbreviated lengthy website addresses (referred to as a 'URL-shortening service').  [Aleksey] used the account to mask a link contained in the spearphishing email, which directed the recipient to a GRU-created website.  [Aleksey] altered the appearance of the sender email address in order to make it look like the email was a security notification from Google (a technique known as 'spoofing'), instructing the user to change his password by clicking the embedded link.  Those instructions were followed.  On or about March 21, 2016, [Aleksey] and Yermakov" - I forgot his first name - "and their co-conspirators stole the contents of the chairman's email account, which consisted of over 50,000 emails."



And it goes on like that.  On or about March 20.  We have a March 19th.  "March 25th [Aleksey] used the same john356gh account to mask additional links included in spearphishing emails sent to numerous individuals affiliated with the Clinton Campaign, including Victims 1 and 2."  And those were earlier described in the indictment.  "[Aleksey] sent these emails from the Russia-based email account hi.mymail@yandex.com that he spoofed to appear to be from Google."



And then I've jumped down again, and I highlighted something else:  "On or about April 6 the Conspirators created an email account in the name (with a one-letter deviation from the actual spelling) of a known member of the Clinton Campaign.  The Conspirators then used that account to send spearphishing emails to the work accounts of more than 30 different Clinton Campaign employees.  In the spearphishing emails, [Aleksey] and his co-conspirators embedded a link purporting to direct the recipient to a document titled 'hillary-clinton-favorable-rating.xlsx.'  In fact, this link directed the recipients' computers to a GRU-created website."  And it goes on like that.



At one point - and they're not clear here about the network penetration.  They said:  "Beginning in or around March 2016, the Conspirators, in addition to their spearphishing efforts, researched the DCCC and DNC computer networks to identify technical specifications and vulnerabilities."  Now, okay.  Think about that.  These guys, in addition to the spearphishing, researched the computer networks to identify technical specifications and vulnerabilities.  How do we know?



I mean, the level of detail here demonstrates we do, as they're about to explain.  But it's amazing to me because this wasn't, I mean, this is not something that - we know from the reporting and actually from what's in this indictment that it wasn't until some time later that the presence of malware was discovered; and then Company 1, as it's called in this indictment, was a security firm that was brought in to figure out what was going on.  This Company 1 almost cleaned things up but left behind this X-Agent malware in a Linux machine that allowed them to maintain a persistent presence in the network.



What they said, for example:  "For example, beginning on or about March 15 Yermakov" - I guess that's still [Ivan] - "ran a technical query for the DNC's Internet protocol configurations to identify connected devices."  Now, I can't technically disentangle what that actually means, but that means something.  On or about the same day this Yermakov, [Ivan], searched for open source information about the DNC network, the Democratic Party, and Hillary Clinton.



So I think in this case they don't mean open source software, they mean like did Google searches.  And in fact there are instances later where they list the search terms that were used in their searches before, as I mentioned at the top of the show, generating a blog post under this Guccifer 2.0 moniker.  On April 7 he ran a technical query for the DCCC's Internet protocol configurations to identify connected devices.  And what we do know is that in or around April, within days of [Ivan]'s searches regarding the DCCC, the Conspirators hacked into the DCCC computer network.  Once they gained access, they installed and managed different types of malware to explore the DCCC network and steal data.



On April 12 they used the stolen credentials of a DCCC employee, who is identified here as DCCC Employee 1, to access the DCCC network.  Employee 1 had received a spearphishing email from the Conspirators on April 6 and entered her password after clicking on the link.  Between in or around April 2016 and June 2016 the Conspirators installed multiple versions of their X-Agent malware on at least 10 DCCC computers, which allowed them to monitor individual employees' computer activity, steal passwords, and maintain access to the DCCC network.



Now, probably some of this came from that Company 1, the security firm that was brought in while this was ongoing.  And so I'm sure that the DoJ researchers contacted them and said, okay, tell us everything you know about what you found when you went over to the campaign networks and looked around.  But anyway, this goes on like that.  There's multiple instances of malware.  There is an instance where somebody, I think it was on the DNC network, had access to the DCCC network.  They installed keystroking and screenshot functions.  Oh, yeah, here.



"X-Agent malware implanted on the DCCC network transmitted information from the victims' computers to a GRU-leased server located in Arizona."  So they weren't even exfiltrating directly from the U.S., but going to another server in Arizona.  There's also one, I think it's in Indiana.  "The Conspirators referred to this server as their 'AMS' panel.  Kozachek, Malyshev, and their co-conspirators logged into the AMS panel to use X-Agent's keylog and screenshot functions in the course of monitoring and surveilling activity on the DCCC computers.  The keylog function allowed the Conspirators to capture keystrokes entered by DCCC employees.  The screenshot function allowed the Conspirators to take pictures of the DCCC employees' computer screens," as we understand all that.



They also used a relay known as the "middle server" that was located somewhere else.  I've skipped over that.  I'm just looking for anything else that's particularly interesting, I mean, in terms of details.  They were proactively covering their logs and cleaning things up.  They even used CCleaner, Leo, at one point in an attempt to clean off evidence of their activity.



LEO:  Well, there's their problem.



STEVE:  Oh, yeah, Illinois.  On or about April 28 they connected to and tested a computer located in Illinois using what was described as X-Tunnel.  So it sounds like they had some sort of a VPN or an encrypted tunneling system.  They also used, sort of reverse-engineering some of the nonspecific language in the document, they used an open source compression tool to archive in some cases gigabytes' worth of documents and then exfiltrated them through this X-Tunnel software which was installed on the machine in Illinois in order to take it out.



Oh, and get this.  And, like, how do they know?  Between on or about May 25 and June 1, so a period of, what, about a week, the Conspirators hacked the DNC Microsoft Exchange Server - and that's understandable, that is, how they might know that from records, and stole thousands of emails from the work accounts of DNC employees.  But here it is.  During that time [Ivan], who's very busy, researched PowerShell commands related to accessing and managing Microsoft Exchange Servers.  So as I said, we really were all up in their business in order to have this level of detail of the things they were doing behind the scenes on their end to prepare for what we know they were doing at our end, apparently based on logs that somebody had somewhere.  Because it sure seems like the Democratic election group were clueless about a lot of this.  Deleted logs.



"Despite the Conspirators' efforts to hide their activity, beginning in or around May of 2016 both the DCCC and DNC became aware that they had been hacked and hired a security company, Company 1 [as it's referred to here] to identify the extent of the intrusions.  By in or around June, Company 1 took steps to exclude intruders from the networks.  Despite these efforts, a Linux-based version of X-Agent, programmed to communicate with the GRU registered domain linuxkrnl.net" - that's krnl.net - "remained on the DNC network until in or around October of 2016."



And they talk about keystroke logging, tons of snapshots.  They then created, it says, oh, they even found them - they know when they were unable to get something that they wanted, suggesting that it wasn't just monitoring DNC networks.  In or around September of 2016 they "successfully gained access to DNC computers hosted on a third-party cloud computing service.  These computers contained test applications related to the DNC's analytics.  After conducting reconnaissance, the Conspirators gathered data by creating backups or snapshots of the DNC's cloud-based systems using the cloud provider's own technology.  They then moved the snapshots to cloud-based accounts they had registered with the same service, thereby stealing the data from the DNC."



And also:  "More than a month before the release of any documents, the Conspirators constructed the online persona DCLeaks to release and publicize stolen election-related documents.  On or about April 19, 2016, after attempting to register the domain electionleaks.com, the Conspirators registered the domain dcleaks.com."



So, okay.  How can anyone know that they attempted but failed to register the domain electionleaks.com?  It's mindboggling.  So anyway, they got dcleaks.com "through a service that anonymized the registrant."  And we don't know if that's Tor or what, but maybe.  "The funds used to pay for the dcleaks.com domain originated from an account at an online cryptocurrency service that the Conspirators also used to fund the lease of a virtual private server registered with the operational email account dirbinsaabol@mail.com."  The dirbinsaabol email account was also used to register that john356gh URL-shortening account used by our friend Aleksey to spearphish the Clinton Campaign chairman and other related individuals.



And this is something we've seen before where any linkages between what would appear to be otherwise separate events, any reuse of IPs, of email accounts, of obscure domains and so forth, that can be used if you have, like, encyclopedic knowledge of every packet that transited the Internet during that time.  You can figure all this out.  But I don't know how you do this, short of having that kind of visibility into Internet traffic.  To me, this suggests there is a level of surveillance far in excess of what I at least have assumed was technically feasible.  It's amazing.



LEO:  Does it give you a little cause for concern?  Because...



STEVE:  Well, I would be concerned if I was Aleksey.  I'd be looking over my shoulder.



LEO:  Yeah, but, I mean, do you think they have this level of detail about everything that's happening?



STEVE:  You know, it certainly does suggest that the future will be interesting.



LEO:  Yikes.



STEVE:  Yeah, wow.  On or about June 14 the DNC through Company 1, remember, that's the security company they brought in, publicly announced that it had been hacked by Russian government actors.  In response, the Conspirators created the online persona Guccifer 2.0 and falsely claimed to be a lone Romanian hacker to undermine the allegations of Russian responsibility for the intrusion.  On June 15 the Conspirators - here again, get this - logged into a Moscow-based server used and managed by Unit - and I haven't talked about the unit numbers.  There's two different unit numbers, 25 something or other, but this one is 74455.  And we know the street addresses and like where the nearest Starbucks is.  It's just amazing.



Between 4:19 p.m. and 4:56 p.m. Moscow Standard Time, after just having gotten some lattes, Unit 74455, they searched for certain words and phrases between 4:19 p.m. and 4:56 including "some hundreds of sheets, dcleaks, illuminati, worldwide known, think twice about, and company's competence."  Later that day, at 7:02 p.m. Moscow Standard Time, the online persona Guccifer 2.0 published its first post on a blog site created through WordPress titled "DNC servers hacked by a lone hacker."  The post used numerous English words and phrases that the Conspirators had searched for earlier that day, which is to say between 4:19 p.m. and 4:56 p.m..  Three hours earlier they made sure their English was correct when they made the Guccifer 2.0 posting alleging that, no, I'm just some guy in Romania.



This says the Conspirators conducted operations as Guccifer 2.0 and DCLeaks using overlapping computer infrastructure and financing.  In other words, again, there were collisions.  There is a - I skipped some of the details, but here's one:  "To facilitate the purchase of infrastructure used in their hacking activity, including hacking into the computers of U.S. persons and entities involved in the 2016 U.S. presidential election and releasing the stolen documents, the Defendants conspired to launder the equivalent of more than $95,000 through a web of transactions structured to capitalize on the perceived anonymity of cryptocurrencies such as bitcoin."



And I'll skip a bunch of this because there's a whole bunch of bitcoin stuff.  But for example:  "The Conspirators used several dedicated email accounts to track basic bitcoin transaction information and to facilitate bitcoin payments to vendors.  One of these dedicated accounts, registered with the username 'gfadel47,' received hundreds of bitcoin payment requests from approximately 100 different email accounts.  For example, on or about February 1, 2016, the gfadel47 account received the instruction to 'please send exactly 0.026043 bitcoin to' a certain 34-character bitcoin address.  Shortly thereafter, a transaction matching those exact instructions was added to the blockchain."  So again, this is where we see bitcoin payments not actually being untraceable and anonymous if you have complete vision, complete sight of everything going on.



Anyway, and I finished, the last piece of this I grabbed was:  "On occasion, the Conspirators facilitated bitcoin payments using the same computers that they used to conduct their hacking activity, including to create and send test spearphishing emails.  Additionally, one of these dedicated accounts was used by the Conspirators in or around 2015 to renew the registration of a domain" - that's that linuxkrnl.net - "encoded in certain X-Agent malware installed on the DNC network."



So we know a few things after a reading of this indictment.  We know that standard kind of like off-the-shelf in the sense that we've talked about it on this podcast for years, social engineering attacks, spearphishing, email spoofing, link following was the way they got in.  They researched people to design email campaigns which would be convincing.  They managed to get some unwitting DNC employee to click a link which was pretending to be a Google security advisory, please reenter your password in order to prove you're you.  That allowed them to get the person's password, and then they were able to get into the network.  And once there, they transferred remote access trojans onto the network, and then from that point spread onto at least 10 different machines.



They were somewhere else I didn't read here, but it's in the show notes if anyone cares.  But it's in the - I have a link, by the way, to the PDF.  It's 29 pages, the entire indictment, if anyone is interested.  But they know how many machines in each of the two networks, the DCCC and the DNC, there was malware found on.  They established a presence in the U.S. where they had other staging machines.  They used bitcoin in order to register domains and to pay for the various services that were available, used anonymizing services, used VPN or some sort of a tunneling system in order to get the data out.



So all of that is sort of by the book.  I mean, this is the danger that any organization is in that is targeted.  And that is, you know, we talked about it with the Sony hack, where one executive assistant clicked the wrong link, and that's all it took in order to set up an advanced persistent threat inside of Sony and then cause all the damage that they did.  So the human factor is still the weak link.  So there's that.



So it was interesting to read this indictment from the standpoint of what exactly was done.  Is there anything new or surprising or, like, what, how did that happen?  And from the angle of what they did in order to perpetrate this intrusion, no.  We learned nothing.  In fact, we know the names of these things that are anonymized in this indictment, pretty much.  What is shocking is what we know.  And Leo, as you said...



LEO:  Total information awareness.  Remember that?



STEVE:  Yes.



LEO:  James Clapper.



STEVE:  Yes.  It is just astonishing.  And, whoa.  So, yeah, it has to be that last Friday, when this went public, the GRU agents in these two units said, whoa.  First of all, their mistakes were highlighted.  The instances where any overlap in infrastructure or email accounts, I mean, anything they reused was linked back together in order to build this web and build this case.  And really that's what that, even if we can't read the labels on the diagram which is our Picture of the Week, that's what that is.  That's a web of interconnection that demonstrates what the various actors were doing, what roles they had, what they registered and when and how they paid for it and, I mean, it's just like, wow.



LEO:  I think you could make the case, though, that while we have this total information awareness, it took a special prosecutor and the Mueller investigation to take these disparate silos of information and, as you say, knit them together.



There was a great movie with Sean Connery called "The Anderson Tapes."  He plays the ringleader of a gang that robs a whole apartment building.  And there's recordings.  There's phone taps.  They have the whole story.  But at the end of the movie all the agencies with all the illegal taps and all the information decided, rather than prosecute, just let's erase all the tapes.  And the movie ends with the tapes being erased because no - but, see, so I have a feeling that we could do this with almost any situation if you could get somebody with the extra legal ability and powers to say I need this, I need this, I need this.  That's why you get a hundred subpoenas at a time; right?



STEVE:  Yeah.  Well, and as I said, I'm sure that the company that was brought into the DNC, the so-called Company 1, I'm sure they've turned over all the records they had of what they found.



LEO:  There were 140 servers.  There was cloud.  But there's more than that.  There's clearly surveillance; right?



STEVE:  Because, Leo, retrospectively, that's just it.  It's like, I mean, they were talking about the way this began, not the way it ended.



LEO:  Right.



STEVE:  Which is like, oh.



LEO:  They knew how they did it.



STEVE:  Yes.



LEO:  But, I mean, I guess you leave breadcrumbs.  If you knew where to look, you'd see it, especially if you're recording everything.



STEVE:  Well, but also to know that they logged into their own machines to perform searches of English phrases, that suggests we're up in their business.



LEO:  Yes, we're up in their business, yeah.



STEVE:  Yeah.



LEO:  Pretty much, yeah.  The people reading this most closely are the GRU; right?



STEVE:  Oh, yeah.  They're thinking, time to stop using Windows.



LEO:  Right.  By the way, during this very show I want to say congratulations to Semstix.  He's a geocacher, and he found our TWiT geocache.  His comments, from today, "Found while listening to Security Now! live.  Glad to see Leo is a geocacher.  TFTC."



STEVE:  Very cool.



LEO:  So he's listening.  Our geocache is outside in the parking lot.  I didn't see him do it.  But he must have gone and found the geocache.  He was listening on headphones live.  So I presume you still are, Semstix.  Enjoy.



STEVE:  You know, and this does - it's hard for law enforcement, after seeing this, to argue that they don't have sufficient visibility as it is now.



LEO:  Oh, yeah.



STEVE:  I mean...



LEO:  Oh, yeah.  No, they didn't go dark.  Oh, no.



STEVE:  Yeah, it's like, where's the darkness?  I don't see any darkness here.



LEO:  Did I tell you we had Phil Zimmermann on a couple weeks ago on Triangulation, the creator of PGP.



STEVE:  Yeah.



LEO:  And he said this is like - and by the way, it's called Zimmermann's Law that, as technology advances, the ability to surveil advances at the same rate.  He says it's like they have a giant big screen TV, and they can see everything.  There's just one or two dark pixels in encrypted phones, things like that.  And they don't like any, you know, they don't want any black pixels.



STEVE:  Actually, that's a great analogy.



LEO:  Isn't that a good analogy?



STEVE:  That really is a good analogy, yes.



LEO:  Yeah.  Clearly, there's little they're missing.  



STEVE:  Wow.



LEO:  What a great piece.  Thank you, I appreciate your talking about this.  Very interesting.  



STEVE:  Well, again, it would have been easy to gloss over it and say, oh, they know a lot.  But I thought - what hit me was the level of detail and the dates and the times.  Oh, yeah, you know, in the afternoon they did this, and then that evening, three hours later, they did that.  It's like, whoa, okay.



LEO:  One story that you did miss, but I just wanted to mention, Election Systems & Software is a company that maintains the counting stuff at the county seats of all the voting.  They've been putting pcAnywhere on their counting machines.



STEVE:  Oh, I know, the remote access.  I just saw that.  Yeah.



LEO:  Ron Wyden, Senator Wyden sent them a letter in April saying, "Is there remote access software on our election boxes?"  Oh, yeah.  Starting in 2000 in a small number of cases.  A small number of cases.



STEVE:  Oh, goodness, yeah.  What could possibly go wrong?



LEO:  Well, and then I didn't remember this, but apparently pcAnywhere's source code had been hacked around that timeframe, even though Symantec didn't tell anybody till six years later.  So...



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  It had been stolen years earlier.  Steve, you did it again.  Thank you, sir.  I'm sure there will be more to talk about next week, as we continue to get all up in their business.



STEVE:  We are never running out of things to talk about, Leo.  My pleasure, and we'll see you next week.



LEO:  Yes, sir.  This show airs - we do it live.  You can watch us make it live.  Even if you're geocaching, you can listen as we're doing it live.  Every Wednesday, I'm sorry, Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Join us in-studio if you wish.  We have a visitor from Northern Ireland here today.  It's nice to have you.  All you've got to do is email tickets@twit.tv.  Or no matter where you are in the world, you can go to our live streams at TWiT.tv/live.  There's audio and video.



After the show is over, our editors get to work, and Steve posts an audio version of the show, followed a few days later by a transcript written by Elaine Farris, she does a great job, so you can read along as you listen.  We have audio and video at our site, too.  Steve's site is GRC.com.  Go there, by the way, not only to get the podcast, but to get SpinRite, the world's best hard drive recovery and maintenance utility.  GRC.com.  And there's all sorts of other stuff there, too.  It's really a treasure trove.  It's like being inside Steve's brain.



Our site is TWiT.tv/sn.  And of course it's on every podcast application.  You can even listen on your Amazon Echo or your Google Home.  Just ask for Security Now!, you'll get the most recent episode.  Thank you, Steve.  We'll see you next week.



STEVE:  Pleasure, my friend.  Till then.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#673

DATE:		July 24, 2018

TITLE:		The Data Transfer Project

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-673.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine still another new Spectre processor speculation attack, some news on DRAM hammering attacks and mitigations, the consequences of freely available malware source code, the reemergence of concern over DNS rebinding attacks, Venmo's very public transaction log, more Russian shenanigans, the emergence of flash botnets, Apple's continuing move of Chinese data to China, another (the fifth) Cisco secret backdoor found, an optional missing Windows patch from last week, and a bit of Firefox news and piece of errata.  Then we look at "The Data Transfer Project" which, I think, marks a major step of maturity for our industry.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a great show for you, including, my goodness, hell's freezing over.  The moon has turned blue.  Google, Facebook, Twitter, and Microsoft helping people move from one service to the other?  Steve explains all, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 673, recorded Tuesday, July 24th, 2018:  The Data Transfer Project.



It's time for Security Now!, the show where we talk about the latest security news from a guy who knows, who knows all and tells all, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be back with you again.  And you know, we're coming up on the end of year 12.



LEO:  Holy moly.



STEVE:  I think our first podcast was in August of, well, nearly 12 years ago, but I don't remember which one.  But I think that's always sort of the time it comes around.  So here we are, Episode 673 for July 24th.  So coming up on the end of year 12.  It'll be fun to be in year 13.  Okay.  So until an announcement, which was...



LEO:  First show was August 18th, 2005.



STEVE:  Ah, perfect.



LEO:  Just checked the website.  Number one.  Yay.



STEVE:  Yeah.  So, wow, less than a month to go, and we're in year 13.  So I was going to talk about, believe it or not, still another new Spectre processor speculation attack.



LEO:  No.  No.  Holy cow.



STEVE:  There's, yeah, like we said at the beginning of the year, our architecture is so wrapped up on using speculation in order to improve performance that getting out, like backing out of this is going to be a nightmare.  But something more important happened that I thought, okay, I'm excited about this because it represents, I think, a major step in the maturation of our industry.  And it's known as the Data Transfer Project.  And believe it or not, I mean, I can't quite believe it.  Facebook, Microsoft, Google, and Twitter, to name the four big ones, and with others, like, pending, have decided to agree on an open source trans-service data transfer standard, specifically to allow users to move themselves easily between services.



LEO:  Wow.



STEVE:  I know.  So it's like the reverse of the walled garden.



LEO:  Yeah.



STEVE:  I mean, it's shocking.  And it actually exists.  I mean, there's...



LEO:  Do you think it's GDPR that's kind of triggered this?



STEVE:  Yes, yes.  There is a mention of GDPR-ness.  But, and this is what we're going to talk about at the end of the podcast is, like, what their goals are, what they're trying to do.  Of course there are obvious security and privacy concerns because this allows a person to - well, and also business concerns.  I mean, this is what surprises me is that a company would be willing, in return for being able to have a user easily import themselves, also easily able to export themselves, to say, "So long, suckers, I don't want to do this anymore with you," and go somewhere else easily.  But that's the point of this whole thing.  So anyway, to me, this is a huge, as I said, a step of maturity for our industry that I want to talk about.



So we will talk about another new Spectre processor speculation attack.  We've got some news on DRAM hammering, which we've been talking about for a number of years.  The original hammerers have some news that I didn't talk about a couple weeks ago because it was like, okay, more of this?  But also now some good news.  So it's like, okay, let's talk about - we'll kind of catch up on that.



We have the consequences of freely available malware source code floating around the 'Net.  The reemergence of concern over DNS rebinding attacks, which you and I talked about in Episode 260.  But there's been a big furor about that recently so I wanted to remind our listeners what that is and sort of revisit this.  We have what I recognize now as, okay, I'm just not very hip, apparently.



LEO:  Why ever would you say that, Steve?



STEVE:  The idea that Venmo users are deliberately publishing, in public, every transaction by default, along with their comments to their fellow transactee, just boggles my mind.  I just can't get my head around that.  It's like, what?  Anyway, so there's an interesting link that I want to share with our listeners.  And you can, like, see the photos of these people who are sending money back and forth.  We've got a little bit more Russian shenanigans, following up on last week's, you know, we're "All Up in Their Business" podcast.  The emergence of a new trend, which I just called "flash botnets" because that's what they are.



Also some news of Apple's continuing move of Chinese citizens' data to China, basically this week or in the last week finally giving over the last vestige of control to a Chinese-owned telecom provider.  Believe it or not, another, which makes it the fifth, Cisco secret backdoor found in major Cisco software.  We have an optional missing Windows patch from last week, optional for users of Windows 7, Server 2008 R2, Windows 8.1, and Server 2012 R2, so just a quick note about that.  Some Firefox news, a bit of errata, and then we're going to take a look at this sort of amazing Data Transfer Project, which actually exists.  So I think another good podcast two hours from now will be in the can.  But right now the can is still open.



LEO:  Shows how cynical I am, that instead of thinking, oh, these companies are doing the right thing for their customers, I'm assuming it's something like the General Data Protection Regulation from the EU that's forcing them to confront this issue.  Maybe they're altruistic.  I don't know.  Maybe.  It's possible.



STEVE:  It could happen.



LEO:  Could happen. 



STEVE:  It could happen.



LEO:  Could have Microsoft, Google, Apple, yeah, it could happen.  They're working for us.



STEVE:  Why not?



LEO:  They care about us.



STEVE:  So believe it or not, we have the vulnerability that keeps on giving with Spectre.  Researchers from the University of California at Riverside have published a paper detailing yet another brand new attack that affects Intel, AMD, and ARM speculative execution which - get this - bypasses all existing recent software and firmware mitigations against speculation execution attacks.  That is, this is a new type of attack on speculative execution which none of the stuff we've done this year fixes because - I know.  It uses an entirely different mechanism in all modern architectures than what we've been focusing on, which is caching and branch prediction.  It uses something known as the "return stack buffer."  So consequently this is named SpectreRSB.



Now, okay.  In podcasts for the last nearly 12 years we've often talked about the stack.  It's been a mixed blessing, especially for security people, because it is in buffer overruns that the stack is often participating.  And it's because the stack has traditionally been in executable memory that, if you loaded a buffer that was, as the phrase is, "on the stack," and could somehow arrange to get program execution to jump into the stack, that is, into the buffer that you had provided, and it was executable, you could run the code that you had remotely supplied.  So we have a remote code execution situation.



More recently, because the stack is normally meant to store data and really isn't needed to store code, updates to architectures have marked the stack NX, nonexecutable.  So these problems have largely gone away.  In the early days of Windows it was interesting.  The movement of bits on the screen, the so-called "bit blit" operations where you would move a rectangle on the screen as part of the windowing operations, in the very early days, before we had GPUs, Windows would actually write code to perform that operation on the stack and deliberately execute it.



LEO:  So there was mass copy of data from one memory location to another; right?



STEVE:  Right, right.  But they didn't want to have, I mean, they were so concerned about performance that they couldn't use general purpose algorithms with lots of "if" tests and loop counts and things because doing that would slow it down too much.  So to do the operation they would actually build the code on the fly, stick it on the stack, and run it there in order to get the absolute maximum performance from this particular operation.  So, and of course those days have long gone, but executable stack remained and has now finally disappeared because it was just such a security problem.



But the way the stack works is it's a sort of a general purpose scratchpad.  And it comes into main use in two instances.  When you're in some code, and you want execute a subroutine, that is, a block of code that performs some function, you normally do that by pushing the parameters for that function onto the stack.  So you have some values, and you push them on the stack.  You then jump to a certain location as a subroutine.  And what that means is that, at the end, that subroutine needs to come back to the instruction after it was called.  So that the jumping to a subroutine, what the processor does is it puts the return address on the stack.



So you first push the parameters to be used.  Then, as you jump to the subroutine, the address of the instruction after that jump is pushed on the stack so that, when the subroutine has finished executing, it knows how to get back to where it was called from.  So the stack has a combination of parameters and return addresses.  And inside the subroutine it might need some scratchpad area.  It might need what are normally called "local variables."  So it's able to use the stack itself.  It'll allocate some memory on the stack to contain its own little working memory area.  And they're local because they don't need to be retained after the subroutine executes.  So it's perfect to put them on the stack because they just get popped off the stack, and they sort of disappear.



So all of this works.  But in thinking through every possible way to make that process faster, where the Intel engineers and in general processor engineers were like, okay, where else could we squeeze out a little more performance?  They said, hey, rather than having this software stack which mixes all this other stuff, let's have a separate hardware stack where we don't store the calling parameters, and we don't store local values.  It's only for the return addresses, thus it's called the "return stack buffer," and it's in hardware.  It exists, it's not very deep, that is, the buffer itself is like typically between four and 16 entries or return addresses deep.  And it doesn't have to be very deep because maybe you're only going to a subroutine and then coming back, which would only need one memory.  Or if that subroutine calls another subroutine, which then returns to the first subroutine, and the first one comes back to where it came from, that would require two and so forth.  So it doesn't need to be very deep because it normally gets reused a lot.



But what this allows is when you're speculating, and you're wanting your processor to run ahead - I mean, after all, that's what this whole thing is, is it's prefetching instructions that may be executed.  And it's doing as much work ahead of time as it can to keep the system as busy as possible.  So when the system is speculating, and it hits a return instruction, it knows, when the processor actually gets around to doing that, it's going to be doing a return from the stack.  But between then and now, the stack may go through a whole bunch of other things.



So the point is the speculator can't use the value on the actual software stack because it may be changed, like from popping some other stuff off of it, before the processor actually gets to the return instruction.  Yet the speculator, which is wanting to run ahead of that, needs to know where to return to.  So that's where the return stack buffer comes in.  Since the return stack buffer only contains return addresses, the speculator can know with high confidence that when that return instruction is eventually executed, even if the software stack has pushed and popped and all kinds of things happen to it in the meantime, that return instruction will be taking it back to the location on the return stack buffer.



So the point is that, by the time the processor actually gets ready to do the return, the return stack buffer and the actual physical stack will be synchronized.  But the processor can't wait for that because it wants to run ahead.  So the return stack buffer provides sort of a pure, simplified, mini stack that allows speculation.  And, not surprisingly, that can be leveraged.  And that's what these UC Riverside guys have done.  And it turns out it is a potent attack.



They said:  "In this paper we introduce a new attack vector, Spectre-like attacks that are not prevented by deployed defenses," meaning nothing we've done so far this year helps.  Because, again, this is a completely different, orthogonal from all of the other speculating systems, vulnerability.  They said:  "Specifically, the attacks exploit the return stack buffer (RSB) to cause speculative execution of the payload gadget that reads and exposes sensitive information.  The RSB" - and then they explain it - "is a processor structure used to predict return addresses by pushing the return address from a call instruction on an internal hardware stack," they say, "typically of size 16 entries.  When the return is encountered, the processor uses the top of the RSB to predict the return address to support speculation with very high accuracy.  We show that the RSB can be easily manipulated by user code."



They say:  "A call instruction causes a value to be pushed to the RSB, but the stack can be subsequently manipulated by the user so that the return address no longer matches the RSB."  That is, it's easy to cause a deliberate mismatch which can then be detected because that will change timing.



They said:  "We describe the behavior of RSB in more detail, showing an RSB-based attack that accomplishes the equivalent of Spectre Variant 1 through manipulation of the RSB instead of mistraining the branch predictor," which of course was all about what the first variant of Spectre was doing at the beginning of the year.  "We use this scenario to explain the principles of the attack."



Anyway, what I liked about this - oh, and by the way, this also cuts through Intel's highest level of protection known as the SGX, the Software Guard Extensions.  They demonstrate a successful attack on Intel's software guard extensions, which is Intel's Secure Enclave technology, just rendering it useless.  They finish, saying:  "Current systems are fundamentally insecure unless speculation is disabled.  However, we believe that it is possible to design future generations of CPUs that retain speculation, but also close speculative leakage channels, for example, by keeping speculative data in separate CPU structures than committed data."



And this is, as I've been saying, this repeats the feeling that I've had as we've developed over the last seven months a more mature understanding of just how bad this is.  I mean, where as I was talking about last week, maybe we end up having to just segregate processes on processors and not allow the fully free cross-hardware usage of threads and processors and VMs and all because there's just - speculation is too important.  We can't turn it off or performance will collapse.  Yet we can't have it on where threads are altering the processor's future as a consequence of its past, which is what speculation leverage does for us.  So, I mean, we've really painted ourselves into a box until we rethink how to do all this.



Okay.  So I titled this next thing "Rowhammer, RAMpage, and ZebRAM."  So our friends from the Systems and Networking Security Group at VU Amsterdam have been, as we have covered through the years, making a series of troubling discoveries about the potent effects from pounding on dynamic RAM memory, which we first talked about as Rowhammer.  And then there was Drammer and, like, a series of attacks.



Well, about a month ago there was news of yet another DRAM-based hammering attack which the news and their coverage said would affect all Android phones released since 2012.  That was RAMpage.  2012 is when Google introduced the ION memory manager which was part of Android 4.0, the Ice Cream Sandwich version.  So RAMpage came from these guys.  And basically it was malware that gets into any Android device since Ice Cream Sandwich, since Android 4, which is basically all devices since 2012, can obtain sensitive information using DRAM-based hammering.  And our listeners know that this is, basically, you allow software to repetitively read from DRAM.  And as a consequence essentially of noise, you can occasionally get a bit to flip in a physically adjacent row, thus the original Rowhammer name for this.



So I have a link in the show notes to a non-Google Play Store Android app which these guys have updated.  They created a Drammer app for Android to check for the vulnerability of their earlier Drammer attack.  So what they've essentially done is they have matured this attack, and they've updated their app to identify devices vulnerable to this newer and more powerful RAMpage attack.  I have the link in the show notes for anyone who's interested.  You need to side load it, so get it and load it into your Android device, but not from the Google Play Store.



Now, they have also produced an open source mitigation known as GuardION, G-U-A-R-D-I-O-N.  Of course ION is the memory  manager, so GuardION.  And I've got the link to that in the show notes.  What they now have is a new technique they call ZebRAM, Z-E-B-R-A-M, which the good news is, is a mitigation.  We don't yet know what it is.  They will be delivering a paper at the upcoming OSDI, which is a USENIX conference being held at the beginning of October this year, south of me in Carlsbad, California.  The paper, although there's no details on what this is beyond the title, which is ZebRAM, comprehensive and compatible software protection against Rowhammer attacks.



So that's good news, potentially, which it's nice to have some good news finally from these guys.  They have thoroughly looked into how to create these attacks and, based on what little we know, it looks like there may be a strong mitigation, which if added to Android, either as a third-party add-on - but I can't imagine why Google wouldn't incorporate it into Android if it works and it doesn't have any obvious downsides - may offer, as they say, comprehensive and compatible protection against DRAM hammering, which would be great.  So I'm sure at the beginning of October we'll be talking about what this is because that would be good news.



We also have an instance of a very powerful Android banking trojan source appearing on the Internet.  What appears to be the case is that the group that had created what has been regarded by the security community as the most powerful, because of the way it operates, Android banking trojan known as Exobot, it must be that the group decided to go do something else because they first began offering the source for sale, which is normally something that only happens when they've decided it's in their interest to leverage their investment in technology by sort of cashing out and then going to do something else.  So after it was sold a number of times, one of the purchasers apparently decided, oh, what the heck, it's everywhere now, I'm just going to put it out on the Internet.  So it became free.  And the free source for what is arguably the most effective trojan on Android for stealing banking credentials represents a problem.



So the security firm ThreatFabric, who's been following this, has stated that it's an unusually potent banking trojan, capable of infecting even smartphones running the latest versions of Android, which is something that very few trojans can do.  The spokesman and security researcher at ThreatFabric said that all threat actors have been working on timing injections, meaning various types of overlay attacks, to work on Android 7, 8, and 9.  However, he said, Exobot is something new.  Exobot gets the package name of the foreground app without requiring any additional permissions, which apparently is not something that is normally available.  And he says it's a bit buggy, but it works in most cases.



So he says the interesting part is that no Android permissions are required, whereas all other Android banking trojan families are using the accessibility or the use stats permissions to achieve the same goal.  But that means they require user interaction with the victim.  Exobot does not.  So what's expected, and we may be talking about this, is a result of the source code now being freely and widely available.  Security researchers expect now to see a rise in Android malware based on this technology.  And why not?  Hackers who now have a source of better solutions for their own malware will immediately start to use it.  So be interesting to see, unfortunately, interesting but worrisome to see what happens.



Okay.  So DNS rebinding.  A post over on Medium last month generated a lot of interest.  It wasn't news, but it's sort of fallen off people's radar.  Brannon Dorsey made the post on Medium.com.  I have a link to his post titled "Attacking Private Networks from the Internet" - again, listen, "Attacking Private Networks from the Internet with DNS Rebinding."  Lifehacker picked up on it, as did a whole bunch of other people.  If you put "DNS rebinding" into Google right now, you get, like, everybody all scrambling around as if this was something new. 



Well, this is not something new.  We did a podcast on it, podcast #260.  But it is of new concern, arguably, because in the past, well, this first appeared in 2008, so a decade ago.  Dan Kaminsky, who of course made a name for himself over in the DNS world, he brought it to focus in 2010, which is when we talked about it on this podcast.  And as a consequence of what's happened in the last eight years with the explosion in IoT, its significance does resurface, and the world has really not been paying it attention.  So I am, first of all really glad for Brannon's rekindling of this and refocusing.



There is a company, Armis, who sells IoT security products at the enterprise level, who has a page on their site.  As a consequence of their survey of their customers, their page is titled "DNS Rebinding Exposes Half a Billion" - with a "B" - "IoT Devices in the Enterprise."  And I have in the show notes and, Leo, you have it on the screen, a snip, a screenshot from their page where they show 87% of switches, routers, and access points are vulnerable, that's 14 million by their estimation;  78% of streaming media players (Apple, Google, Roku, Sonos, for example), 5.1 million globally; 77% of IP phones from all the regular players (Cisco, Dell, NEC, Polycom); 75% of IP cameras (the GoPro, Sony, Vivotek), and there's 160 million of those; 66%, two thirds of our printers, all of which are typically located on our Intranets, on our local networks (HP, Epson, Konica, Lexmark, Xerox), 165 million printers; and 57% of Smart TVs, those integrated with Roku, Samsung, Vizio, 28.1 million.



Okay.  So what are rebinding attacks?  What is all this?  Well, I will read from GRC's operation page for the DNS Benchmark because I incorporated DNS rebinding attack awareness there.  In the DNS Benchmark, GRC's DNS Benchmark, there's sort of a circular icon over on the left for every DNS server.  And if you are protected from rebinding, you get sort of an extra moat around the inner dot which is meant to show you that you're protected.



So I said on the operation page:  "The outer circle of the resolver status icon shows what, if any, DNS rebinding attack protection the corresponding DNS name server provides to its querying clients."  And I'll mention that OpenDNS does provide protection, and I used them in the screenshot because they're like the only one I could find that was doing this.



"DNS Rebinding attacks," I write, "utilize DNS to fool a browser's scripting security into believing that local resources such as the user's own computer or router or now IP camera, Roku devices, printers, Sonos speakers, whatever IoT devices are located in the same web domain as the script's source."  Okay, in other words - actually I just say:  "When this occurs, the browser's same-origin policy protection is bypassed, giving scripts unrestricted access to the local resource.  This allows scripts to do bad things such as change LAN router settings or access any resources and computers on the LAN."  I said, in parentheses:  "(That's not good.)  Security-conscious DNS nameservers are able to help block these attacks simply by never returning IP addresses that fall within the ranges of IP addresses commonly used with private LAN networks, behind a router, or the localhost IP, 127.0.0.1, which computers use to refer to themselves.



"GRC's DNS Benchmark tests each nameserver to determine whether it blocks, that is, filters the return of these reserved private IP addresses in both IPv4 and IPv6 formats.  At the time of this feature's release, only OpenDNS nameservers can be configured to do this, and then only for IPv4.  IPv6 versions of these queries are still able to sneak through.  Since there is never any reason to return a private IP address from a public DNS request, all nameservers should block the return of private IP addresses.  Hopefully more will in the future."



Now, I need to add a caveat there because it appears that there are some instances where corporations are deliberately using public DNS to refer to resources on their own internal LANs, when those internal LANs are using nonroutable space like 10.172, 192.168 and so forth.  So there are exceptions.  However, those should probably be handled with whitelisting.



Okay.  So anyway, I said at the bottom of this:  "Note:  If you would like to learn more about the consequences and prevention of DNS Rebinding attacks, this was the topic of our Security Now! podcast #260.  During that episode Leo and I explained the problem and discussed all the details of this at some length."  And I said:  "The whole story is available for download," blah blah blah.  That was Episode 260.



So, okay.  So what's going on here?  DNS Rebinding is a - we've talked at length about the same-origin policy in browsers, which is crucial, which has to be enforced, which browsers restrict what JavaScript can do to the origin, that is, the domain name from which that script was obtained by the browser.  So essentially it creates sandboxes, sort of dynamic sandboxes so that script running on your browser that came from domain XYZ, it's able to do other stuff with domain XYZ.  It's able, for example, using Ajax, to subsequently perform HTML queries to get additional resources, to examine and set cookies, and do pretty much anything it wants with that XYZ domain, that is, the domain where it first came from.



But script is explicitly, JavaScript explicitly prevented from doing anything with any other domain.  That's crucial.  Otherwise it could just reach out and mess with other domains globally or in the browser, other browser tabs, other pieces of the same page.  You know, an ad, JavaScript running in an ad could have access to the domain that is hosting the ad, which you absolutely don't want because this is untrusted script.  We require that containment.



Okay.  So here's the problem.  A clever DNS server for a malicious ad could deliberately use a short expiration DNS response, that is, when this malicious server is asked for the IP of its domain, it gives it, you know, whatever it is, a valid public IP from which resources are obtained.  But it gives it with a deliberately very short DNS expiration.  We know about DNS caching, how it is caching that allows the DNS system to survive because IPs for domain names tend to be relatively static.  If they change, it's typically very infrequently.  This allows that knowledge to be cached out across DNS servers on the Internet.



But there is a time limit.  There's a timeout.  Often it's a day.  Sometimes it's eight hours.  It's a function of how often IP addresses are expected to change.  Again, not very often.  And the longer you allow your own DNS server's IPs to live, the lower the load on your server because the IP records are expiring less often.  So the servers out on the Internet are having to come back and refresh their record of your domain's IP less often, which reduces the load on your DNS server.  So there's a tradeoff.



But a malicious server could deliberately set a timeout to, say, a minute.  Now, sometimes this just doesn't work because there are caching servers that deliberately ignore such short timeouts.  Although it's not in the spec to do that, some of them do set a minimum timeout.  But a lot of servers honor the timeout in the DNS record.  So this malicious script running on your browser in a malicious ad first gets a valid IP for its domain.  Thus that loads the malicious JavaScript into that window.  It then begins performing additional queries to that domain.  But because this was cleverly set up ahead of time, and there's lots of ways this could be done to make it robust, unfortunately, that domain name-to-IP mapping will expire, which will cause the DNS system to ask for an update, ask again for an IP.



Now imagine if 192.168.1.1, which is oftentimes the IP of your local router, if that IP is returned in response to additional queries by this malicious script.  The browser doesn't know about IPs.  The browser knows about domain names.  So it doesn't know, realize, or care that now that domain name which the script has legitimate same-origin policy access to, is pointing to your router, 192.168.1.1.  That now frees the JavaScript in your browser to talk to the router, to start guessing web admin usernames and passwords.  And if you're somebody who - oh, and of course we know that it makes a query to the router.  It immediately gets from the response headers the router's make and model and version and all kinds of other information that the router happily broadcasts in its response headers.  Now it knows what type of router you have.  So it says, oh, goody, and knows what the default username and password is.



The point is many users who are - even savvy users setting up a local network mistakenly assume that inside the network we're all one big happy family, and that you're not going to have something malicious on your LAN because, after all, it's in your house.  It's in your environment.  So imagine someone who doesn't take the trouble to change the internal web admin from their router's default.  This provides a mechanism which works today to allow script which you didn't write to get into your network with essentially breaking out of the same-origin policy using DNS rebinding, which I've just described, to have free and unfettered access to the inside web admin of your router, which as we also know allows it access to Universal Plug and Play.  It can open up incoming ports statically and allow bad guys in.  So this is all bad.



Okay.  So where does IoT come in?  Well, this is bad in an instance where you left the web admin authentication set to its default so that the script could see what make and model router you had and then guess the default, or just sit in the background making other guesses, trying to brute force.  But the big problem here, the thing that everyone's talking about now, what has changed in the last eight years is the prevalence of IoT devices and their implicit assumption that anybody on the local network is friendly.  And Leo, I know you're a Sonos user, as am I.



LEO:  Yes, yes.



STEVE:  And one of the coolest things about Sonos is that you don't have to do any weird login username or password stuff.  The other day, actually Lorrie lost her previous iPhone, so she had it insured, and she got a replacement.  But she said, "Hey, I want to control our music.  What do I have to do?"  And I said, "Nothing."  So we downloaded Sonos, opened it.  It came up and said, "Looking for your system."  And it said, "Found."  And that was it.  No authentication.



LEO:  Right, there's no authentication.  Right.  I'll vouch for that.



STEVE:  Which is - yeah.  I mean, and it's wonderful.  I mean, from a user ease-of-use standpoint, it's incredibly cool.  But it happens that the downside of this ease of use is - now, you might argue there's nothing bad you could do with Sonos.  It actually turns out Sonos allows you to run some Linux commands  through its HTTP interface.



LEO:  Uh-uh.  Oh, dear.



STEVE:  So it can be used as a pivot for them establishing a foothold in your LAN and working from there.  Sonos, by the way, just in the last month, Sonos and Roku, both of whom have had no authentication of their internal LAN API, have been reawakened to this problem as a consequence of Brannon's posting and have announced updates.  And in fact I did get a notice on our Sonos that there was a new version available, and I imagine they've decided they need to close some of these holes because, while it's convenient, this really is a problem.



So for the last eight years, since my DNS Benchmark was published, I've been running a "rebindtest" server.  If people who are a little tech-savvy are interested, we all have typically an nslookup command.  In Windows, there's nslookup, and I think it's called the same thing on Linux and Unix systems.  If you do "nslookup net192.rebindtest.com," that's my DNS server that I've been running for eight years in order to support GRC's DNS Benchmark that does rebind testing just as part of the service that it offers.  You can do net192, net4, net10, net127, net172, or net192, any of those .rebindtest.com.  What you will almost certainly see, unless you are an OpenDNS user and you've turned on rebind protection, you will get a local IP back.  For example, net192.rebindtest.com will return the IP 192.168.0.1.



LEO:  It did.



STEVE:  Yes.  And just to make sure, net4 will return 4.4.4.4, which is a valid public IP as we know.  That's one of - I think it's one of Level 3's DNS servers.  I just did that in order to verify that some public IPs are coming through and private ones are not, in order to prevent false positive rebinding notifications in the Benchmark.  But the point is there's no good reason for a public DNS server, which rebindtest.com is, to return the IP of an internal LAN, 192.168.0.1.  Yet it does.  If you are configured under OpenDNS, that will not happen, so you'll know you're protected.  So a simple way of checking is just to manually do an nslookup using my rebindtest service, or you can just use the DNS Benchmark if you're a Windows user, and it'll show you.  And I did it this morning.  There were none.  Not a single DNS server out there is doing this.



And in that screenshot, though, I think I do show one where there's like a circle, like a semicircle, looks like it's maybe, what, the sixth one down, is it OpenDNS?  I think it ends in 202.202?  I can't seen the screenshot from there.  But yes, you can see that - yes, that's the one, 220.220.  And so that almost enclosed outer ring, the top segment is 127.0.0.1.  They're not blocking that.  But they are blocking the other three RFC 1912 networks because in this case I had configured, for the sake of testing, I had configured OpenDNS to do that.



So there are some SOHO routers, looks like Asus may be among them, and I have an ASUS router - I'm using pfSense here in my workspace, in my work area, but I have an ASUS router at home.  So I'm going to check it to see if it's got DNS filtering feature.  There are some that are beginning to do that.  And users can protect themselves from this and then verify using the Benchmark or just doing an nslookup, see whether you're able to obtain an IP address for your own internal network or any private network from a public server.  This is a problem, and this is not something that we are currently protected from.



And so yay to Brannon for bringing this up, and to Sonos.  Oh, also Google.  Google's Home systems have similar default, no authentication required.  Apparently he had to poke them a lot by showing them example after example after example.  And on his page he does have some testing.  It requires that your network be 192.168.1.x.  But he actually has a testing page with JavaScript which will successively attempt to probe your LAN from a page running JavaScript and look for Google Home, Roku, and Sonos devices and tell you when it has found them because they are visible to script running in a web page.  So a little bit of a wakeup call for, yes, it's so convenient not to have to authenticate our IoT devices.  But it does require that everything be trusted on our LAN.  And this is an instance where that assumption of trust can be broken.



So as I said at the top of the show, Leo, I'm bemused by the idea that all Venmo transaction details - now, not all details of the transactions, but some surprising ones, are all being maintained in a publicly accessible log.



LEO:  Yeah.



STEVE:  And so if you click on this link, the first link I have in the show notes here, that just shows - it says "limit=1."  That'll just bring up one transaction.  And if I click on it, let's see, I'm seeing [Miss Tina Lashi] sent money to [Maya Zalastone].  And now...



LEO:  I'm getting a different one, of course.



STEVE:  Yes.  And if you refresh, you'll get a different one, and if you refresh, a different one.  Because, I mean, there's a lot of transactions happening on Venmo.



LEO:  Oh, yeah.  It's huge.



STEVE:  And actually, in Firefox actually it formats the page nicely so that I see...



LEO:  Oh, see, I'm not getting that.  Safari's not formatting it because it's a, I don't know, it's XML, yeah.



STEVE:  It is.  And so, for example, I mean, I've got picture links to these people.  And if I click on a picture link, I see them.



LEO:  Well, it's easier than that.  If you just install Venmo it's right on the front page.  That's why we know people know about this.  But I should point out that I don't know if everybody knows about it because it is the default.  And people might just be assuming that there's some privacy to this.  So we've brought this up several times.  See, this is on Venmo.  This is the public feed. 



STEVE:  Correct.  And I think, in thinking about this more, I think it's one thing to say, oh, look, you know, like here's the public scroll.  But I wonder if people understand that these transactions are memorialized in perpetuity.  There was a researcher who is a privacy advocate who was curious about this.  So he pulled all 207,984,218 transactions which occurred in 2017 from the public API.  I mean, it's just there.  I mean, so it's not a scroll that sort of goes away.  Your entire transaction history in Venmo is there forever as publicly available.



LEO:  The default is public, visible to everyone on the Internet.  I immediately, when I started using Venmo, set it to private, and that means it's not in that database.  It's not...



STEVE:  Logged.



LEO:  ...viewable.  Unless there's a flaw in Venmo's system.  But I presume that's not what we're talking about.  It's just that their defaults are public.



STEVE:  Yeah, exactly.  It's just that, I mean, in thinking about it, again, maybe I'm just an old fogy.



LEO:  People treat it like a social media feed.  And they have fun with it because usually they say, you know, "illegal stuff" or, you know.  I mean, they make fun with it.



STEVE:  Right, right.  So anyway...



LEO:  You should know about it, though.  It is a potential problem.  I mean, people might be paying for stuff and not want anybody to know it, and everybody does.



STEVE:  Yeah.  And in fact this one guy who pulled all of the 2017 public transactions, that is, nearly 208 million of them, he created kind of a fun page:  publicbydefault.fyi.  I guess he's a funny Chinese guy, Hang Do Thi Duc is his name.  But from looking at the data, he just pulled some random ones out.  He tracked the transactions related to a cannabis reseller, a corn dealer, a particular family, a few random couples, and the story of a woman with 2033 Venmo transactions.



LEO:  Wow.



STEVE:  So she's a big Venmo user.  And you can put comments, as you commented, on the transactions.  And they're all there for the world to see.



LEO:  They tend to be emojis.  People really like to use emojis.



STEVE:  Well, actually...



LEO:  That tells me that they kind of know what they're doing.



STEVE:  In the links I was seeing, I was seeing people, like that Tina what's her name.  She's got big glasses on and - yeah.



LEO:  Well, yeah.  You have your picture.  That's what I'm saying.  It's like social.  So if you download the Venmo app, and you go back here to - let me see.  Home and then the public stuff.  I always use emojis.  There are a lot of emojis in there.  But this is the person.  So it's just like a Twitter thing; right?



STEVE:  Yeah.



LEO:  But, you know, some of it is silly.  Some of it is intentionally kind of ribbing people.  But I do think most people know it's public.  It's just I worry about people who don't because...



STEVE:  Right.  And people are, like, sending money to each other; right?



LEO:  Yeah.



STEVE:  That's what it's for.



LEO:  Yeah.  I mean, I use it.  I send money to my daughter.  I use it to tip my massage therapist.  I use it to tip my manicurist or pay my manicurist, stuff like that.  A lot of people - this is a PayPal service.



STEVE:  Right.



LEO:  A lot of businesses now are using Venmo.  Originally it was used by millennials to share meal bills and stuff like that, or to reimburse for gas or rent or that kind of thing.  So I think millennials kind of know what's going on.



STEVE:  It is worth noting that the transactions can be canceled within some length of time.  I ran across that when I was trying to dig into what in the world this thing is.



LEO:  I use it.  It's a great service.  I mean, there's others.  There's Cash.me from Square.  There's Zelle, which is the banks are doing that.  But Venmo's probably the biggest.  And we've actually talked about this for some time on iOS Today because people should know, turn that off.



STEVE:  Good, yes.



LEO:  And you can do it on - you can also do it on a transaction by transaction basis.  On every transaction there's a public/private button.



STEVE:  Right.



LEO:  But I think people go very fast, and they're not paying attention.  Yeah.



STEVE:  So anyway, a little bit of a public service and just a head scratcher for me.  It's like, okay, I don't understand.



LEO:  You're just too old, Steve.



STEVE:  I am.  I'm an old fogy.  So a real quick note that the Russians are - I have here the Russians are (still) coming, because of course you and I...



LEO:  Great movie. 



STEVE:  ...are old enough, and half of our listeners probably, to remember the movie "The Russians Are Coming."



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  It was a comedy.  So in this case not such a comedy.  So following our topic, of course, from last week about the DoJ's indictment of those 12 Russian agents, I just wanted to note that during last week's very interesting for many reasons Aspen Security Forum, Andrea Mitchell had an interesting interview at length.  But in this case Tom Burt, who's Microsoft's vice president for customer security, said that earlier this year they had discovered a fake Microsoft domain that had been established by Russia as the landing page for phishing attacks.  Microsoft said it detected and helped the U.S. government to block Russian hacking attempts against at least three congressional candidates this year.



Although Microsoft declined to name the targets, they said the three candidates were "people who, because of their positions, might have been interesting targets from an espionage standpoint, as well as an election disruption standpoint."  And according to Microsoft, the Russian hackers targeted the candidates' staffers with phishing attacks, redirecting them to a fake Microsoft website in an attempt to steal their credentials.  And Tom Burt, who is this VP at Microsoft, said that they "discovered these fake domains were being registered by an activity group that at Microsoft," he said, "we call Strontium," which the rest of us know as Fancy Bear or APT 28.  So it's that same group.



So for what it's worth, this continues.  And I expect it will continue to continue.  I hope that - and I didn't really say this clearly enough last week.  After everything that has happened nearly two years ago, staff in elections, where they are inherently targets, really have to be trained up on security.  I mean, the way this happens is phishing attacks.  They are social engineering attacks of various sorts.  And they just have to be trained up in order to just avoid the temptation of clicking on things in email.  I mean, it's almost worth imagining filtering people's email to redirect the clicks to a local catch basin or just redact them from incoming email.  I mean, just, like, not have any because they're just too dangerous.  And people just, you know, what we see over and over is that people will get caught out by that.



Okay.  We're also seeing something that I've termed here "the emergence of Flash Botnets."  There was some transaction in Twitter.  A researcher, Ankit Anubhav, I'll just go with that, Ankit Anubhav...



LEO:  You're as good as I am on this one.



STEVE:  Boy.  So in his Twitter feed he noted that an IoT hacker identifying himself as "Anarchy" has claimed to hack more than 18,000 Huawei routers in 24 hours using the vulnerability from last year, 2017-17215, which was used last Christmas, or leaked last Christmas, and was used several times by the Satori botnet.  So he's taking responsibility for what was observed as a massive uptick in Huawei scanning, which several security firms had observed.  In his dialogue his motives were not clear.  But the attacker who Ankit corresponded with said he's making the biggest, baddest botnet in town.  So maybe for DDoS.



LEO:  Bad, bad botnet Brown.



STEVE:  Biggest baddest botnet, yeah.  And unfortunately, and what Ankit said in his tweet was it's painfully hilarious - I don't think it's hilarious at all, I think it's just painful - he says how attackers can now construct big botnet armies using known, I mean, like well-known vulnerabilities.  So anyway, so the 17215 was the well-known exploit.  It used a Universal Plug and Play access to the WAN side, obviously, of Huawei routers.  It had been abused by at least two versions of the Satori botnet and also by the Brickerbot and a number of smaller Mirai-based offshoots that we've also talked about.  So it's just sort of like there.  And there's 18,000 of these, and nobody cares.  It's now been, what, at least nine months, eight or nine months since this has been publicly known.



And again, we talked about this.  Even though Huawei has been responsible, they are responsible for the problem, but mistakes can happen.  They have an announcement on their site.  They have taken responsibility.  They've got updated firmware.  But they don't have the ability to push that to their routers.  Their routers are not checking periodically and notifying their owners or maybe even going the next step and maintaining their own security autonomously.  I think we're at the point now, I mean, we've got - nobody has to update their Chrome browsers anymore.  Nobody has to update Firefox.  These browsers just take care of themselves.  Same for Edge and IE.  And as we know, there is some interaction at the OS level.



But at the appliance level it doesn't make sense.  There ought to be something, I mean, nobody reads a license agreement anyway.  It certainly doesn't say in there that your router probably has known vulnerabilities that allow an attacker to take it over remotely and get inside and roam around your network.  Yet that's the reality.  So there's no reason it couldn't say your router may, at a period of time of low usage, like 4:00 a.m., update itself with new firmware all by itself.  I mean, you can have a checkbox that you have to turn off in the configuration if you want to explicitly disable that.  But it ought to be on by default.  We need to move to that place.  In which case it would be possible for all of these Huawei routers to be phoning home, discovering, oh, I've got new firmware, update themselves, flush out as a consequence of rebooting anything that might have crawled into their RAM in the meantime, and patched themselves.



We have to be doing that for these Internet-facing appliances.  It's time.  I mean, the consequence of not doing it is what we're seeing, which is long-term, never going to go away, now known persistent vulnerabilities that can be endlessly abused.  And now we have the ability to form, as we've just seen, an 18,000-strong router botnet within a day.  Oh, and this guy says, by the way, he's not done.  The Anarchy guy who did this using the Huawei vulnerability said he plans to next target an older vulnerability from 2014, that's 8361, which is a well-known vulnerability in Realtek routers, exploitable via port 52869.  That still exists, and he's going to go and add those routers to his botnet next.



Oh, and in an update in some reporting of all of this in Bleeping Computer, they noted that both Rapid7 and GreyNoise are confirming scans for Realtek have gone through the roof today.  So the guy was, yes, he followed through on his plan to go after Realtek routers next.  So the only way that we're going to get out of this mode is ultimately for these existing routers to be replaced.  But what they are replaced with has to then maintain itself.  It just has to in order to connect to the Internet.



I did want to mention just sort of in passing that we did see in the last week the formal final turnover of Apple's iCloud data to a Chinese state-owned ISP and cloud service provider that is going to be managing it from now on.  And so I just wanted to remind our listeners, it's not clear if you change your iCloud storage setting whether - and maybe you know from working with Rene and the guys over on MacBreak Weekly - whether that pulls your existing iCloud data back to the U.S., or is it only when you're setting up a new iCloud account and you opt to have your storage in the U.S.?  Because that's an option that Apple provides.  But the default would be for Chinese users of Apple iOS products, that their iCloud stuff would now be stored in China.



LEO:  I don't know.



STEVE:  Yeah.  So anyway, I did want to tell our listeners that there is that option.  Don't know if it will delete what's in China and start using it in the U.S., or even if it can be changed after the fact.  But for what it's worth, I regard what Apple has had to do as inevitable.  I mean, they want to operate in China.  They want the Chinese market.  The Chinese government has the right to decide by law what its citizens' access to data is.



So I don't see any choice here.  I mean, we watched Russia fight with Telegram, and Telegram lose, because ultimately a local government can control what happens within its borders.  So I don't see this, I mean, I know that civil rights people and privacy advocates are all screaming; but it's like, well, sorry, that's China.  So Apple wants the Chinese market.  At least users setting up iCloud for new devices can choose to have their data located in the U.S.  I don't know how long that's going to last and what that really means.  But it is an option.



Cisco has discovered and removed another undocumented backdoor.  Last week Cisco released 25 security updates, among them a patch they rated with a severity of eight, I'm sorry, of 9.8 out of 10.  So I don't know what it has to be to be 10.  I guess maybe a wide-open, unauthenticated admin with no password on the WAN side of their products.  That would definitely be a 10 out of 10.  This got a 9.8 because what they discovered was another undocumented root admin password.  This was one for the Cisco Policy Suite.  And what the patch did was remove an undocumented password which had been built into the root account of all previous Cisco Policy Suite software.  So it was a previously secret backdoor.  They tracked it with a CVE-2018-0375.  And it's significant due to the location of these systems within enterprises.



The Cisco Policy Suite has three editions - Mobile, WiFi, and Broadband Network Gateways which Cisco sells to ISPs and large corporate clients, allowing network admins to set up bandwidth usage policies and subscription plans for customers and employees by tracking individual users, the traffic tier, and to enforce access policies.  So Cisco said there's no workarounds or mitigating factors, that customers need to install the patch to remove the secret password.  They say they discovered the undocumented root password during internal security audits and believe that it may have been left behind during software debugging tests, as they say most of these incidents end up being.



But being the fifth such secret backdoor to be found, for me this is a headshaker, as I said when it was the third, and when it was the fourth, and now it's the fifth.  This really no longer seems like a mistake.  This seems more like a previous policy.  And as I said before, I'm impressed by the way Cisco is behaving now while apparently under new management because they're saying, whoops, we want to remove these.  And remember, someone had suggested, well, maybe this was in a product they acquired from somewhere else.  So you could hold them blameless for stuff being in a product that they purchased and they didn't know about, like they own Linksys, for example.  Who knows?  But in any event I am impressed that they are saying, whoops, we found another one.  At some point, though, when you get to five, it's hard to say, oh, well, yeah, this is another one got left in by mistake.  It's like, okay.



Bleeping Computer noted that there was a patch missing from users of Windows 7 and 8.1, both in the workstation versions of those OSes and also in the server versions, meaning Server 2008 R2 and 2012 R2.  It's not super critical, but it's not a patch that we will receive automatically.  So I just wanted to put it on people's radar.  Because I do run Windows Server 2008 R2, and this does bear on servers, I will definitely be applying it myself manually.  So I've got the links in the show notes for anyone who is interested.  For Windows 7 it's KB4345459, and for Windows 8.1 it's KB4345424.  So again, KB43454 and then ending in 59 or in 24.



And so it fixes three things.  It says:  "Addresses the issue in which some devices may experience a stop error when you run network monitoring workloads; addresses the issue that may cause the restart of the SQL Server service to fail with the error 'TCP port is already in use'" - so that would be a problem, if SQL Server couldn't rebind to its expected port.  And then, finally, "addresses an issue that occurs when an administrator tries to stop the World Wide Web Publishing Service.  The W3 Service remains in a 'stopping' state, but cannot fully stop or cannot be restarted."  Well, I do that from time to time in order to update the code running on the - the GRC net engine code that I've written for the server.  So that matters to me.



So anyway, I just wanted to put it on people's radar.  Thanks to Bleeping Computer for putting it on my radar that those patches - oh, and for some reason, what's interesting is they noted Windows 10 did receive these as part of last week's monthly rollup.  But 7 and 8.1 did not.  So don't know why.



Lastly, as I had mentioned before, Firefox will be getting the autoplaying video with sound suppression, thank god, on web pages, although I think one of my add-ons, probably uBlock Origin, is able to block that for me.  I go to sometimes use somebody else's machine or browser and stuff's, like, talking.  And it's like, what?  What?  Anyway, I know, Leo, that's been a bugaboo of yours recently.



LEO:  Oh, I hate it, yeah, yeah.



STEVE:  So Chrome and Edge are doing it.  It's looking like Firefox 63, due near the end of October, will be getting the same thing.  So yay.  It'll just be the default is "Always Ask."  And that's a little popup that comes in up under the URL, like sometimes I see them if I go to a local movie theater site, they want to know my location so that they can select the proper theater for me if they're a large chain.  And so there's like a popup you get in Firefox.  So this would be one that would ask whether you want videos that are wanting to autoplay, whether you want to allow them to or not.



So starting with Firefox 63 it is default to "Always Ask."  But if you know you never want them to autoplay, you can change that setting to "Block Autoplay."  Or if you know that you always want them to autoplay because you don't have enough noise in your life, you could change it to "Always Allow" autoplay.  So thank goodness that Firefox will be getting that, too.  I'm still using Firefox, just because I love the tabs on the side.  I just, you know, if Chrome would fix that, you know, there are some add-ons that have tried to do that, but none of them integrate as nicely as they do for Firefox for me.



I have a piece of errata thanks to Herzi was his Twitter handle, I think maybe his name, also, H-E-R-Z-I.  He said:  "In SN-671 [so that's two weeks ago] you imply that the PortaPow [that's that cute little red] USB condom is a dumb USB plug that only forwards the power and ground lines."  He says:  "It does a lot more than that as it contains its own little logic board."  And so I thought, what?



And so I dug into it, and it turns out that, yes, indeed.  In the features for that cool little PortaPow it says:  "SmartCharge - Built-in chip detects the type of device which is connected and swaps between Apple, Universal, and Samsung charging specifications.  This prevents the blocker from slowing down charging and can increase charging speed if your charger is sending the wrong signal, for example, charging an iPad from a charger which uses the Universal charging spec."



So in fact it sort of - it effectively enhances the thing that you have plugged it into.  And so, for example, maybe if you were using this in a car, and so you were just using a USB charger that was unaware of the iPad protocol, this essentially upgrades the charging port to one that is smarter.  And then it did say, under limitations:  "This adapter is not compatible with extra fast charging technologies such as the Qualcomm Quick Charge, the Samsung Adaptive Fast Charge," oh, and a Samsung...



LEO:  Samsung.  Samsung.



STEVE:  Samsung, thank you, Adaptive Fast Charge, "as these require data transfer to be enabled."  That's interesting.  "Your device will still charge at a high speed without these.  Some sat navs and dash cams use a proprietary signal and will still try to enter sync mode unless their own charger is used.  Some car USB sockets do not provide enough power to charge a device, so a dedicated USB car charger must be used."  So anyway, Herzi, thank you for that, and I appreciate being able to bring all the proper information to our users.



LEO:  Nice.



STEVE:  Okay.  So the Data Transfer Project.  As I said at the top of the show, I think this represents a major step forward in the maturation of our industry.  Major players are participating - I'm just stunned by this - Facebook, Google, Microsoft, and Twitter.  They explain that the project was formed in 2017 to create an open source service-to-service data portability platform so that all individuals across the web could easily move their data between online service providers whenever they want.



LEO:  It's wild.



STEVE:  It is.  It's incredible.  And the URL is DataTransferProject.dev, Leo, if you want to bring it up while I'm explaining.



LEO:  All right.



STEVE:  And it's the first link in the show notes.  "The contributors to the Data Transfer Project believe portability and interoperability are central to innovation.  Making it easier for individuals to choose among services facilitates competition, empowers individuals to try new services" - okay, now get this.  "Making it easier" - I'm reading from their page.  "Making it easier for individuals to choose among services facilitates competition, empowers individuals to try new services, and enables them to choose the offering that best suits their needs."



So under "What is the Data Transfer Project?" they explain:  "The Data Transfer Project" - and I've edited this down for size and added some clarification in various places.  "The Data Transfer Project is a collaboration of organizations committed to building a common framework with open source code that can connect any two online service providers, enabling a seamless, direct, user-initiated portability of data between the two platforms."  Just I'm speechless.



"The individuals," they say, "should be able to easily transfer their files and data directly between online service providers.  The Data Transfer Project extends data portability beyond downloading a copy of your data" - this is where you were talking about the GDPR before - "beyond downloading a copy of your data from your service provider to providing customers the ability to directly transfer data in and out of any participating provider.  The project is an open source initiative to encourage participation of as many providers as possible.  DTP will enhance the data portability ecosystem by reducing the infrastructure burden on both service providers and users, which should in turn increase the number of services offering portability."



What they mean by that, I'll just say that they recognize that users may have themselves a bandwidth limitation.  And over time you could accumulate, for example, a bazillion photos.  And it's just not feasible for you to download, if you wanted to move from a photo management facility, from one to another, they're literally making it possible to do that.  They're saying, well, due to bandwidth constraints, it may not be technically feasible or possible for an individual to download all of their photos to hold them for then upload to another service.



So what they're talking about doing is creating a direct point-to-point interoperability among every pair of partners involved so that, given proper authentication, and that's where our podcast and our discussion of this doubtless in the future comes in, an individual would be able to securely authenticate and authorize the direct transfer of all of their photos from one of these participating services to another, and their bandwidth would not get tied up.  They would say, "Yes, please, do this."  And then the system would arrange to make sure they're who they say they are and authenticate them securely and would then do a direct inter-provider transfer so that new photo sharing provider number two receives the entire photo history from provider number one.  It's incredible.



LEO:  It's great.  This is very much like Google Takeout, but with the cooperation of other services.  It's brilliant.  I can't believe they're doing this.



STEVE:  I know.  It's mindboggling.



LEO:  Yeah.



STEVE:  So they say - I'm scratching my head because it actually itches.  But it's like, yeah.  "The protocols and methodology, they say, of DTP enable direct service-to-service data transfer with streamlined engineering work."



So getting into a little more detail, how does it work?  "The Data Transfer Project uses services' existing APIs and authorization mechanisms to access data.  It then uses service-specific adapters" - that's what they're calling them, and I'll explain that in a second - "to transfer that data into a common format."  So this project is coming up with a set of common things that people transfer, and then creating a vendor-neutral format into which each of them can translate in and translate out of.  And so they say:  "It then uses service-specific adapters to transfer that data into a common format and then back into the new service's API."



So this comprises three main components.  They have data models which are "canonical formats that establish a common understanding of how to transfer data.  Adapters provide a method for converting each provider's proprietary data and authentication formats into a form that is usable by the system.  Task Management Library," they call it, TML, "provides the plumbing to power the system.  Data models represent the data when being transferred between two different companies."  So that's this common transfer format.  "Ideally, each company would use interoperable APIs to allow data to flow between them.  However, in many cases that is not the case.  In those cases there needs to be a way to transfer the data from one company's representation to another company's representation."



So they say that "Data models are clustered together, typically by industry grouping, to form verticals.  A provider could have data in one or more verticals."  So verticals could be, for example, photos, email, contacts, or music.  Wow.  "Each vertical has its own set of data models that enable seamless transfer of the relevant file types.  For example, the music vertical could have data models for music, playlists, and videos.  Ideally, a vertical will have a small number of well-defined and widely adopted data models.  In such a situation, the generally accepted standard will be used as the data model for that vertical across companies."  They say:  "This is not currently the case for most verticals because data models have emerged organically in a largely disconnected ecosystem.



"One goal of DTP" - and I think this is really amazing, and this will probably happen - "is to encourage organizations to use common data models in their systems, which will happen if organizations take importing and exporting data into consideration when initially designing their systems or providing updates.  Using a common data model will significantly reduce the need for companies to maintain and update proprietary APIs."



Okay.  So what they're saying is this effort will be producing a set of sort of generic and universal specifications which can be used for inter-provider transport of this stuff.  But if a new company comes along, rather than it reinventing the wheel and just using their own ad hoc system, it would be, if they - first of all, they're going to want, if they're a new service, they're going to want to always support DTP because that gives them the opportunity to acquire users because this creates user portability, against all walled garden logic.  So it makes sense that they would natively support these existing models and not have to create an ad hoc system and then an interface layer.  Why not just do it from the start?  So it's like, yes.  



LEO:  Yeah.  It seems like the right thing.



STEVE:  Yeah.  And so they also have what they call "company-specific adapters," which you would have if you had already existing ad hoc stuff.  They said:  "There are two main kinds of adapters:  data adapters and authentication adapters.  These adapters exist outside of a provider's core infrastructure and can be written either by the provider itself, or by third parties that would like to enable data transfer to or from a provider."  In other words, they're saying that a company might not themselves support DTP.  But if they do have their own APIs that are publicly accessible, like Venmo, then somebody could come along and create - a third-party could create a set of adapters to interface that proprietary company's APIs to the DTP system.



So they say:  "Data adapters are pieces of code that translate a given provider's APIs into these common data models used by DTP.  Data adapters come in pairs:  an exporter that translates from the provider's API into the data model, and an importer that translates from the data model back into the provider's API."  And then there are authentication adapters, which are "pieces of code that allow consumers to authenticate their accounts before transferring data out of or into another provider."  And this system will leverage OAuth.  They said it's "likely to be the choice for most providers; however, DTP is agnostic to the type of authentication."



And then finally that last thing.  I mentioned the TMLs, the Task Management Libraries.  They say:  "The rest is just plumbing.  The Task Management Libraries handle background tasks, such as calls between the two relevant adapters, secure data storage, retry logic, rate limiting, pagination management, failure handling, and individual notifications.  DTP has developed a collection of Task Management Libraries as a reference implementation for how to utilize the adapters to transfer data between two providers.  If preferred, providers can choose to write their own implementation of the Task Management Libraries that utilize the data models and adapters of DTP."  And I should mention that the third link in - oh, the second link is an overview whitepaper, I think it's 24 pages, which provides a lot more depth of information.  And the third link at the top of this is all of this stuff on GitHub.  It's all open source.



So they provide, just to kind of help people pick themselves up off the floor, some use cases.  They say:  "Individuals have many reasons to transfer data, but we want to highlight a few examples that demonstrate the additional value of service-to-service portability."  So, first:  "A user discovers a new photo printing service offering beautiful and innovative photo book formats, but their photos are stored in their social media account.  With the Data Transfer Project, they could visit a website or app offered by the photo printing service and initiate a transfer directly from their social media platform to the photo book service."  Meaning them not needing to be an intermediary, but saying to the social media platform, "Please send these photos to the photo printing service."  And now those two services would know how to interoperate.



Another example:  "A user doesn't agree with the privacy policy of their music service.  They want to stop using it immediately, but don't want to lose the playlists they've created.  Using this open-source software, they could use the export functionality of the original provider to save a copy of their playlists to the cloud.  This enables them to import their existing lists to a new provider, or multiple providers, once they decide on a new service."



Or:  "A large company is getting requests from customers who would like to import data from a legacy provider who is going out of business.  The legacy provider has limited options for letting customers move their data.  The large company writes an adapter for the legacy provider's APIs that permits users to transfer data to their service, also benefiting other providers that handle the same data type."  So in other words the going-out-of-business provider doesn't have their own support for this common data model, but somebody creates one because it's in their interest to do so, and makes it available.



Or:  "A user in a low bandwidth area has been working with an architect on drawings and graphics for a new house.  At the end of the project, they both want to transfer all the files from a shared storage system to the user's cloud storage drive," in other words, directly into the cloud.  "They go to the cloud storage Data Transfer Project user interface and move hundreds of large files directly, without straining their own bandwidth."



And, lastly:  "An industry association for supermarkets wants to allow customers to transfer their loyalty card data from one member grocer to another, so they can get coupons based on buying habits between stores.  The association would do this by hosting an industry-specific host platform of DTP."



So they explain, finally, that "The innovation in each of these examples lies behind the scenes.  Data Transfer Project makes it easy for providers to allow their customers to interact with their data in ways their customers would expect."  Well, in the future, wow.  "In most cases, the direct data transfer experience will be branded and managed by the receiving provider, and the customer wouldn't need to see DTP branding or infrastructure at all."  So that suggests that, once this is in place, it would be a get-data-from mode, where you would go to the - you'd be setting up a new service somewhere, and they would offer import from other services capabilities, just as other services would offer import from them.  So it would be mutual and, again, just sort of stunning.



And they conclude with:  "Why do we need DTP?  Users should be in control of their data on the web.  Part of this is the ability to move their data.  Currently users can download a copy of their data from most services, but that is only half the battle in terms of moving their data.  DTP aims" - I guess a typo - "aims to make moving data between providers significantly easier for users."  So, wow.



LEO:  Now, are these companies actually doing it?  Twitter, Facebook, Windows, and Google?



STEVE:  Yeah.  It is now in place.  They've been working on it since last year, and this was the wraps coming off of the project.



LEO:  Wow.  That's really awesome.



STEVE:  Yeah.  Like I said, it just feels like a huge step in maturity for our industry.



LEO:  Yeah.  I'm still wondering.



STEVE:  I know.  It's like, what?



LEO:  What are they up to?  What are they up to?



STEVE:  So let's hold our breath and hope it works and hope that the security is locked down.  I will certainly be taking a closer look at that as this actually surfaces, and we see how this works.



LEO:  Well, you know, Google's always had this.  This has been a been a big part of the Google services for a long time.  They call it, what is it, data freedom.



STEVE:  Yeah.



LEO:  And they've always believed in it.  But there was no interoperability.  It was just like a, you know, XML file or CSV file, and that's that.  Which is better than nothing.  I mean, I love that.  But now they're going even farther, which is great.  It's amazing.



STEVE:  Yay.



LEO:  Yay.  Wow, Steve.  A show that ends on an upbeat note.



STEVE:  Yay.



LEO:  Wow.  Wow.  Steve Gibson is at GRC.com.  You've already heard he's got quite a few little useful tools there you might want to check out.  Of course the most useful of which he didn't even mention, although somebody in the chatroom said, "I've got a SpinRite story for you."  Let me scroll back and see if I can...



STEVE:  Oh.  



LEO:  Yeah, yeah.  It was just a, you know, was a short little thing.  Oh, I wish I could find it.  Here's a SpinRite - this is Aneroid.  "Here's a SpinRite testimonial.  It's keeping two of my 12-year-old hard drives alive, 500GB SATA 1, now in a one-year-old gaming laptop."  Wow.



STEVE:  Nice.



LEO:  Twelve years.  That's almost as long as this show's been around.  That's awesome.



STEVE:  Very nice.



LEO:  Get your copy.  GRC.com.  SpinRite is the best data transfer hard drive - not transfer, data transfer, that's what we were talking about - hard drive recovery and maintenance utility.



STEVE:  Maintenance, yup.



LEO:  While you're there get all sorts of stuff.  SQRL.  When are we going to do our SQRL thing?  Now, you lost Father Robert unless you do this in August.  He's going to be back one last time.



STEVE:  So I'm in the process still of bringing myself up to speed on a system I know nothing about, which is the web forums which I chose to host SQRL.  We have to have SQRL log into its own web forums, but it's in minified JavaScript, which is impenetrable.



LEO:  Right.



STEVE:  And PHP written on top of the Zend Model View Controller platform.



LEO:  Oh, my god.



STEVE:  And so it's a little opaque, and it's proprietary, and it doesn't support doing what I want to do.  So anyway, that's where I am.  As soon as we get that done, then we'll be able to point everyone at it and have someplace for people to go with their SQRL questions because I just need to have public web forums available for handling the launch traffic.



LEO:  Nice. 



STEVE:  Getting there.



LEO:  GRC.com.  You can read all about it, find out what SQRL is.  You can also get all sorts of other cool stuff.  While you're there you might want to pick up a copy of the show.  He's got audio, and it's the only place you can get transcripts of the show.  Elaine Farris writes those, and they'll be out a few days after the show comes out.  We have audio and video at our website, TWiT.tv/sn.



And of course, as with all the shows that we do, you can get them on your favorite podcast application, Pocket Casts or iTunes or Google Podcasts or Stitcher or Slacker or whatever, whatever you use.  In fact, all of those home assistants now you can just say, you know, "Listen to Security Now!," and maybe you have to say - sometimes you have to say "Listen to Security Now! podcast."  I think on the Amazon you might have to say "Listen to Security Now! on TuneIn."  But just try saying "Listen to Security Now!," see what happens.  You'll probably get it, the most recent episode, and you can just listen.  Walk around the house, do your household chores while you're listening.  That's a good thing.



And we do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  So you can tune in and watch it live.  If you do that, visit the chatroom, irc.twit.tv.  Next week I am not going to be here.  I'm running out.  Jason Howell will be filling in for me.



STEVE:  Are you having fun somewhere? 



LEO:  Yeah, we're going up to Tahoe for a few days with the kid.



STEVE:  Nice.  Nice.  Summertime.



LEO:  Summertime.  Enjoy some summertime fun, go river rafting and stuff like that.  But I will be back the week following.



STEVE:  Jason will hold down the fort in the meantime.



LEO:  He does a great job.



STEVE:  And now I've got my Skype audio settings working, so we're good to go.  Woohoo.



LEO:  Your sync is good and everything.



STEVE:  Yup.



LEO:  Thanks, Steve.  Have a great day.  We'll see you next week on Security Now!.  Bye-bye.



STEVE:  Thanks, buddy.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#674

DATE:		July 31, 2018

TITLE:		Attacking Bluetooth Pairing

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.GRC.com/sn/SN-674.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we examine still another new Spectre processor speculation attack.  We look at the new "Death Botnet," the security of the U.S. DOD websites, lots of Google Chrome news, pushes by the U.S. Senate toward more security, the emergence and threat of clone websites in other TLDs, more cryptocurrency mining bans, and Google's Titan hardware security dongles.  We finish by examining the recently discovered flaw in the Bluetooth protocol which has device manufacturers and OS makers scrambling - but do they really need to?



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  Leo Laporte's out this week; and I, Jason Howell, will be filling his shoes, as best as I can, anyway.  Steve's going to talk about a whole number of things.  But he's going to dive pretty deep on a newly revealed flaw in the Bluetooth wireless protocol.  Is it something to be concerned with or not?  Steve's going to tell you all about it, next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 674, recorded Tuesday, July 31st, 2018:  Attacking Bluetooth Pairing.



It's time for Security Now!, the show where we talk about all the latest security news happening throughout the course of each and every week.  I'm a new voice on the show because Leo's normally sitting in this seat.  But, you know, I'm not the person you're tuning in to listen to and to watch.  It's Steve Gibson - the man, the myth, the legend.  How you doing, Steve?



STEVE GIBSON:  Hey, Jason.  It's great to be working with you this week.  Leo, I guess, is off on a little quick three-day trip.  But as I noted to you before we began recording, I saw him also prerecording content for his weekend show, and we just learned from John that he's pretty much gone all of September.  So you and I will be working together, I guess at least probably for three of the shows in September.



JASON:  I think it's probably three episodes during the time that he's gone.  I could be wrong.  There may be a fourth in there, but I'm pretty positive it's the three.



STEVE:  I guess Father Robert is back briefly in August, which starts tomorrow, since this is July 31st.  And then he's gone forever, so...



JASON:  Yeah, I mean, gone forever, that sounds so ominous.  Gone forever from TWiT until who knows what happens down the line, I suppose.  But, yeah, that's true.  I know he's back sometime in August, and he's going to swing by.  I'm not entirely sure how deep he's going to get when he swings by, but...



STEVE:  Right.  



JASON:  Oh, he'll be on TWiT.  Okay, so he's going to be appearing on TWiT when he swings by.



STEVE:  Oh, cool.



JASON:  And I think I also saw on last week's episode that you've got kind of the whole SQRL thing happening.  There was a plan to do something in tandem with Father Robert at some point; right?



STEVE:  Well, and I'm planning to use you, now, instead.



JASON:  Oh, all right-y.



STEVE:  I am.  Because, yeah.  Where I am is I've mentioned to Leo, and we'll be talking about this a little bit later because - in the context of Google's announcement of the Titan security keys, which we'll be covering on the podcast.  Where I am is that, because there's no way, you know, GRC has sort of off-the-beaten-path newsgroups, old-school, text-only, NNTP, you know, like Usenet-style newsgroups where, like, everything happens on the development front.  And it's a really great place for people who are willing to sacrifice the convenience of a web interface and are willing to set up a specific client for that.  I mean, there's a bar you have to get over.  But that's not practical for something that, you know, like a technology like SQRL is meant to be, that all of our spouses and friends and family and so forth would be using.



So the point is that I have needed standard, easy-to-use web forums.  Those exist.  But I built them - I chose a really nice forum package, XenForo, which is in its second major version release.  And those are the guys who previously did vBulletin, I think it was.  So, I mean, they really know their stuff.  But it naturally doesn't support SQRL authentication.  It doesn't know about SQRL authentication.  So I'm right now in the process of teaching it about SQRL so that - because you have to be able to use SQRL to log into the SQRL forums.  That would be crazy.



JASON:  I would imagine if you did anything different, yeah, people would be holding your feet to the fire on that one.



STEVE:  Yeah.  So anyway, so that's where I am.  And but my intention is to sit down with you, Leo, and I was thinking maybe, god, I'm blanking on his name, the guy who's traveling around eating everything in foreign countries.



JASON:  Oh, Mike Elgan.



STEVE:  Mike, yes.  Because I want to sort of do it as a presentation, but also sort of contentious, where like you guys say, okay, well, what about this?  What about this?  What about this?  What about this?  Because I think that would be most useful to people who are themselves saying and thinking, well, what about this?  What about this?  Because the point is I think that the SQRL system has an answer to every possible question.  And so, I mean, that's what it has to have.  And so that makes it unique among all the identity solutions that exist.  And I think that's the proper way to, like, vet it to our audience.  So anyway, I don't know when that's going to happen because I never know when anything is going to happen.  But as soon as we're ready, we will find a time, and we'll make that happen.



JASON:  Absolutely.  Count me in.  That sounds like a lot of fun.  I'd be happy to help.



STEVE:  So in the meantime, however, we've got Episode 674 for this last day of July, the 31st.  There were two topics vying for the title.  There is the news of a new attack on Bluetooth which generated a lot of press last week, which we couldn't ignore.  And then there's also, believe it or not, another Spectre processor speculation attack, which of course has been the gift that just keeps on giving to this podcast all year.



JASON:  Yeah, no kidding.  It's like a constant feed of information for you to talk about, basically.



STEVE:  Yeah.  And there's a lot to say about both.  And so it was like - and then I just thought, okay.  We'll make "Attacking Bluetooth Pairing" the title, only because I'm tired of typing Spectre into the title of the podcast.  And I'm sure our listeners are like, oh, not another one.  But we've got to talk about that.  So we'll talk about Spectre first, look at a new trend in botnets which I'm calling "flash botnets" because they're able to now, by leveraging old, well-known vulnerabilities, they're able to flash into existence in a matter of hours.  There's a new one called the Death Botnet.



We've also got some interesting stuff about the security of DOD websites.  There's a bunch of Google Chrome news.  One particular U.S. Senator, Ron Wyden, has been pushing both the DOD, which we'll talk about, and also just the government in general towards more security.  We've got an interesting bit about the emergence of and the threat of cloned websites which could easily slip under people's radar unless they're watching for them.  And so I want to put that on everyone's radar.  Some more cryptocurrency mining bans.  The announcement last week of Google's Titan hardware security dongles.



And then we'll finish by talking about, well, the technology in enough detail of this newly discovered Bluetooth protocol flaw to put it into context for our listeners and figure out whether, even though the OS and device manufacturers are actually all scrambling to update their implementations, whether we as end users really have anything to worry about, then and even now, before they get things updated.  So I think another typical great Security Now! podcast.



JASON:  Awesome.  I can't wait.  Looking forward to it.  And you mentioned Spectre kind of at the top, so that seems like the great place to start.



STEVE:  Yes, yes.



JASON:  It's always evolving.  What is NetSpectre, exactly?



STEVE:  Okay.  So you can imagine from the name; that sort of gives it away.  A couple weeks ago we talked about something kind of related.  It was a Rowhammer attack.  "D" hammer or DRAM hammering or row hammering is something we've also been talking about now for a couple years.  It turns out that the dynamic memory in our, well, actually all of our devices - laptops, desktops, and even smartphones.



Android has been a subject of hammering attacks.  The DRAM is susceptible to noise, like adjacent row noise, because DRAM memory is organized in rows and columns.  And if you read frequently from one row, it turns out that there's a probability - it's low, but not zero - that a bit will flip in the adjacent row.  And clever attackers, or actually at this point clever academicians, researchers, have figured out how to leverage that into an attack, to arrange to flip the write permission bit on the memory that governs the memory manager to give their process write access to the kernel, for example, where it should only ever have read access.



So anyway, so what happened a couple weeks ago, we talked about this, is the guys who have been doing all this Rowhammer research at VU Amsterdam, Herbert Bos and his team, have come up with a new attack called Throwhammer, which is over the network.  Because it turns out that network adapters have gotten so fast that they've had to be able to do DMA, Direct Memory Access, into their buffer memory because they need super high-bandwidth connectivity.  Well, that turns out to be row hammerable, and so thus Throwhammer.  So first we had network-based Rowhammer attacks.  Now we have network-based Spectre attacks.



It turns out that in this case - shoot.  I had it in my notes, but I don't see it here.  A pair of researchers figured out how to use the network activity on many different systems.  They used desktops and laptops and even cloud-based VM systems to induce Spectre speculation vulnerabilities.  What they said in their abstract was - and a little bit of this is repetitive, but again they sum it so nicely.



They said:  "Speculative execution is a crucial cornerstone to the performance of modern processors.  During speculative execution, the processor may perform operations the program usually would not perform.  While the architectural effects and results of such operations are discarded if the speculative execution is aborted, microarchitectural side effects may remain."  And this is the whole idea of, like, training a branch predictor for a certain prediction and then causing it to deliberately mispredict.  Well, nothing at the architectural level sees that because that's a microarchitectural optimization which all processors have incorporated.



Anyway, they continue:  "The recently published Spectre attacks exploit these side effects to read memory contents of other programs.  However, Spectre attacks require some form of local code execution on the target system."  And in fact, in discussing this on the podcast, it's one of the reasons I've said to our listeners, you know, certainly this is of concern in a cloud environment where multiple VMs might be owned by different organizations.  One of them could be untrustworthy, and they're running on shared hardware.  Because there you've got code from a third party that essentially is sharing the same processor, and that's where the leakage can occur.



On an individual's, like an end user's machine, well, if you've got bad stuff running in your system in order to perform Spectre manipulation, well, you've already got problems because your system has got malware on it.  And, frankly, there are easier ways to break privacy if malware's running on a system probably than relying on Spectre.  So it's less of an issue.



Anyway, so they said:  "Spectre attacks require some form of local code execution on the target system.  Hence, systems where an attacker cannot run any code at all were, until now, thought to be safe."  And that's the key is that, in mitigating these attacks, because mitigation costs performance, that is, you have to - we have speculation in our processors because it is a powerful optimization win.  So if you have to turn off speculation, you want to turn it off selectively to minimize the performance impact.  Thus it's only been turned off so far where it's been clear that it's necessary to turn it off.



What these guys have done by demonstrating that just network activity is able to leak information, that is, believe it or not, the timing of the return packets, when enough of them are looked at over time, allows them to dribble bits, reliably dribble bits at a low rate out of a system to which they have network access.  So that says they're not running any code on the system.  So this changes the universe of mitigation for Spectre attacks significantly.  These guys did report their discovery to Intel in March, and at this point Intel's probably just, like, thinking, when is this nightmare going to end?



So they said:  "In this paper we present NetSpectre, a generic remote Spectre Variant 1 attack.  For this purpose, we demonstrate the first access-driven remote cache attack over network, leaking" - and this doesn't sound like much, but we'll talk about this - "15 bits per hour."  Again, slow.  That's why I called it a "dribble."  But the reason this is significant is that it can be going on as long as it needs to because it's over the network.  So it can be going on behind someone's back, and how would they know? 



They said:  "Beyond retrofitting existing attacks to a network scenario, we also demonstrate the first Spectre attack which does not use a cache covert channel."  In other words, normally they're looking for whether data is in cache or not, and sensing the delay required to fetch from main memory as opposed to the much faster access from cache.  Anyway, they found some instructions in the Intel processor, the AVX instructions, which give them a cache-free Spectre attack and allow them to get substantially more, well, four times, 60, six zero, bits per hour from the target system.  So again, this is just them being a little more academic in seeing what they can do.



They said:  "We verified that our NetSpectre attacks work in local area networks, as well as between virtual machines in the Google Cloud."  And finally they said:  "NetSpectre marks a paradigm shift from local attacks to remote attacks, exposing a much wider range and large number of devices to Spectre attacks.  Spectre attacks now must also be considered on devices which do not run any potential attacker-controlled code at all."  Like can you say IoT?



They say:  "We show that, especially in this remote scenario, attacks based on weaker gadgets" - "gadgets" is the term now in the Spectre literature for some code which they arrange to use to leak the information.  So "Attacks based on weaker gadgets which do not leak actual data are still very powerful to break address space layout randomization remotely."  So that's the other thing they've done.  Address Space Layout Randomization, ASLR, is a mitigation we often talk about because the idea is that one of the more powerful attacks is so-called "Return Oriented Programming," where if you are unable to execute your own code, for example, in a buffer overflow, you can still arrange to execute existing code.  That is, you jump into the tail of a subroutine which does a few things you need and then returns to you.  So it's called "Return Oriented Programming."



And the point is, to do that - oh, and that's like code in the kernel.  So as long as you know where code of the various kernel modules are located, you can jump to little prearranged spots in order to have it do little bits of work for you, come back, jump somewhere else, do a little bit more, and end up building a potent exploit by just using little snippets of code that already exists.  But you have to know where it is because it's a blind attack.  There's no way you can check beforehand.  So the mitigation for that has been to scramble around, that is, address space layout randomization, to randomize where things are loaded.  Every time a modern OS boots, the modules that form the OS are all randomized in their locations.



So the point is that these guys have also demonstrated that, even if they're unable to find an information leak where they actually read out memory contents, they are still able to derandomize the address space layout randomization remotely.  So that could be like the beginning of an attack on a system that is not rebooted frequently since rebooting would rerandomize the address space layout.



Anyway, so that's the nature of what they've done.  The popular press in covering this I think got it wrong because the approach that the press took was, well, 15 bits per hour, that's really not so fast.  It's like, okay, wait a minute.  The fact that it's a slow dribble of information by no means renders this ineffective.  So, for example, imagine the scenario where some malware gets on an employee's machine through a phishing attack inside of an enterprise, and we've seen that before.



Then the question is, what mischief can the malware get up to?  Well, normally Intranet servers are going to be hardened against their own internal network, just as they are hardened against the external network.  So if something gets onto an Intranet and is then able to use NetSpectre to, overnight, I think I figured it takes 17 hours on a LAN to obtain a 256-bit private key from a server.  So that's not that long.  I mean, 15 hours...



JASON:  It's not inconvenient at all, yeah.



STEVE:  Yeah, exactly, running behind the backs of a system.  If it's then able to determine what somebody's private key, or the private key on a corporate Intranet LAN, then that's powerful.  And they did also verify among Google separate instances of a server running in the Google Cloud there is plenty of bandwidth between cloud servers; that they were able to attack another server running on Google Cloud and obtain the same kind of rate of information.  So this is something that definitely needs to be looked at.



They said toward the end of their write-up on leakage, and to give our listeners a sense for how much is necessary, they said:  "Desktop and Laptop Computers.  In contrast to local Spectre attacks, where a single measurement can be sufficient, NetSpectre attacks require a large number of measurements to distinguish bits with sufficient confidence.  Even on a local network, around 100,000 measurements are required to reduce the noise to a level where a difference between bits can be clearly seen.  By repeating the attack, the noise is reduced, making it easier to distinguish the bits."



They said:  "For our local attack, we had a gigabit connection between the victim and the attacker, a typical scenario in local networks, but also for network connections of dedicated servers and virtual servers.  We measured a standard deviation of the network latency of 15.6 microseconds."  And they talked about breaking ASLR.  They said:  "To break ASLR [Address Space Layout Randomization], we require the cache covert channel.  On average, this allows breaking the randomization remotely within two hours."  So that's not bad.



And on a cloud network they said:  "We evaluated the performance in the cloud using two virtual machines instances on the Google Cloud.  These virtual machines have a fast network connection.  We configured the two machines to each use two virtual CPUs, which enabled a 4Gb per second connection.  In this setup, we repeat the measurement 20,000,000 times per bit to get an error-free leakage of bytes.  On average, leaking one byte takes eight hours for the cache covert channel, and three hours for the AVX covert channel."



They said:  "While this is comparatively slow, it shows that remote Spectre attacks are feasible between independent instances in the public cloud.  In particular," they said, "APTs [Advanced Persistent Threats] typically run for several weeks or months.  Such an extended timeframe is clearly sufficient to leak sensitive data, such as encryption keys or passwords, using the NetSpectre attack in a cloud environment."



And then here's where I did the math that I quoted before.  In the notes I said:  "A 256-bit secret key on a local network at the rate of four minutes per bit, or 1,024 minutes, is 17 hours."  So entirely feasible.  However - okay.  So what they're doing is they are sending packets to the server and getting a reply.  Their only instrumentation, that is, the only thing they're able to sense is the timing of the reply.



So what that does say is this is completely infeasible over the Internet.  It is only within a locally contained cloud environment where you've essentially got the equivalent of an Intranet in the cloud among servers, or on a corporate LAN.  I mean, so it's got to be point to point between the network interface which is making the requests, and the network interface that is being probed.  There is absolutely, I mean, given how many packets were required to obtain a bit of information with a LAN or in a virtual network environment between VMs, there is absolutely no way this could possibly produce information over the Internet where you've got multiple jumps with routers all buffering packets and introducing huge amounts of delay.  I mean, they're looking at a standard deviation of 15, what was it, 15.6 microseconds.  And that's the kind of timing they need.  So with a standard deviation of 15.6 microseconds, they're still needing hundreds of thousands of packets per bit.



So as we know, on the Internet we readily see timing of, what, 30, 40, 50 milliseconds, and huge variations, so a much larger, I mean, a huge standard deviation.  So there's just absolutely no way this is feasible for a remote attacker, for example, to attack someone's SOHO router which has a public face.  But this should be a serious concern both in cloud computing scenarios and also on LANs because, as we said, you could spend a day, I mean, who wouldn't?  What attacker wouldn't be happy to pound on a LAN-accessible server for 17 hours if it meant that they could get that server's 256-bit elliptic curve private key?



JASON:  Absolutely.



STEVE:  So yet another mess that speculation brings us.



JASON:  Seems to me like this is the perfect kind of example of the slow and steady approach; right?  Slow and steady wins the race.  It doesn't matter how much you're pulling.  What matters is that the plan works, even over a long period of time at low amounts of data.  Like there's still something to be gained from that.  It's very easy to kind of consider it a theoretical sort of thing and unvaluable to an attacker.  But obviously there's something to be gained there.  And a day's really not that much time in the grand scheme of things.



STEVE:  No, not if you've got something, like, well, not when you consider the value you're getting in return.



JASON:  Absolutely.



STEVE:  I mean, if you get a server's private keys, you're able to spoof its identity.  So, I mean, the private key is the most protected thing that a server has because that's how it's signing all of its outgoing traffic.  So, yeah.  And I really think that what we're going to need to see, as we've said, is short-term, kind of hold-our-breath - it's important to remember, too, all of this, all of this Spectre stuff is speculative.  That is, in terms of it even being - it's theoretical.  There's never been an instance found of this being done in the wild.  We know that it's possible, and we know that what's possible ends up happening.



So essentially what everyone's been doing is to make this as improbable and as low leverage, that is, generally the Spectre flaws, as possible.  Ultimately we're just going to have to learn the lesson and come up with another redesign of chips that recognize that we need to isolate the microarchitecture speculation from the top-level architecture so we can still get the performance that we want from speculation, but that nobody is able to sense that out at the top level.  So we get the isolation that we thought we always had, but it turns out that was always an illusion.



JASON:  Now, you kind of touched a little bit on IoT in there, and it looks like - it's such an ominous name, Death Botnet.  Who's the person that selected this, and why was that not taken already?  Because that seems like an obvious one.



STEVE:  So last week - there's a security researcher with NewSky Security, Ankit, and I always trip over his name.  It looks like Anubhav, Ankit Anubhav.  And I'll just call him Ankit to...



JASON:  Ankit A.



STEVE:  So that I'm not mangling his last name.  So last week he brought us the notion of what I called a "flash botnet" because it had acquired 18,000 Huawei routers within 24 hours, that is, from the moment it started, a day later it had 18,000 routers in its net.  And it was using a several-year-old vulnerability that had long since been known, that Huawei had issued a patch for.  But these are routers.  These are things in people's closets, or they're just sitting there, minding their own business - well, not so much now.  But the point is that they had not been updated.  And this is the problem that we're really seeing now with routers because it's now become just the play land for hackers because now all hackers know that routers are not being updated, for the most part, with the latest patches.  So they just go and look at a two-year-old patch and go, let's find out who's vulnerable.



Well, Ankit found, for this week's update on This Week in Compromised Routers, he found another botnet which its author calls the Death Botnet, who knows why.  He fired up a dialogue with the hacker who goes by the handle EliteLands, E-L-I-T-E-L-A-N-D-S.  So EliteLands named this new botnet "Death."  So far it has not attacked anybody, so it hasn't done anything, although it's going after AVTECH-based devices, which include DVRs, network video recorders, IP cameras and more, once again using a two-year-old and also long-since-patch-available device flaw.



But this one is a little bizarre, believe it or not.  It turns out that the AVTECH devices expose their password in the clear.  So the password is available in plaintext.  So it's easy for someone to scan them, hack into the device to get the password in order to log in.  Beyond that, it turns out that, believe it or not, the password process has a flaw such that whatever is used in the password ends up being a command sent to the shell.  I mean, it's just hard to believe this.  So, for example, if you set the username to "This is my username with a password of reboot," it reboots the device after having performed that.  So what Ankit told Bleeping Computer, who had their reporting of this, he says:  "If I put 'reboot' as the password, the AVTECH system gets rebooted."  And he said:  "Of course, the Death Botnet is doing much more than just rebooting."



Ankit says the hacker has been experimenting with different payloads for use in the password field.  Meaning that what we're talking about is a shell command, that is, a command, any command executed that you choose in the password field.  So Ankit said that he's recently started using these payloads to build a botnet which he refers to as Death.  So in the latest version of the payload, Ankit says that EliteLands is adding accounts with a lifespan of only five minutes that execute his payload and then disappear from the infected device.



So Ankit said it's like a burner account because usually people don't set up passwords for access of only five minutes.  But in this case that's enough to get the job done.  That essentially commandeers the device and gets it to join into the botnet.  And then the account which he used is transient, and so it expires off of the device and can no longer be used.  So 1,200 AVTECH devices are hijackable in this way, which Ankit determined using a readily available IoT search engine.  I don't know if it's Shodan or one of the other ones.  But anyway, this was patched more than a year and a half ago.  But as we've said, these devices don't actually tend to be patched in the real world.  Their users just, you know, it's an IP camera or a DVR or a net video recorder.  And so, you know, if it's not broken, don't fix it.



JASON:  The person who owns it would have to know, first of all, to jump on there and do some sort of a manual firmware update in order to get that [crosstalk].



STEVE:  Right, right.



JASON:  They're not going to do that.  That's the big problem with IoT at this point.  No one is going to do that.



STEVE:  Exactly.  And as we've said, what we have to - we have to move to a model where these non-maintained devices are able to update themselves.



JASON:  Yes.



STEVE:  We have to have that.  And then it'll just have to be - we'll just have to wait around for the devices which don't know how to update themselves to finally just die off, to age out of the ecosystem and be replaced by solid state network recorders that hopefully will know how to update themselves.  But that's what we have to do is have self-updating, self-healing devices.  I mean, we already have that with our web browsers.  All of our web browsers are now updating themselves.  As we know, our OSes are, albeit with a little more user interaction.  But Windows 10 has been controversial because it's been so heavy-handed in forcing people, ultimately forcing them to update the OS.  So you just don't have a choice at some point, if you're going to be connected to the Internet.  So it's going to have to be that our IoT devices take that same responsibility moving forward.  And then we just wait for the ones that don't to die off.



JASON:  Yeah, because we can't be trusted.  That's what we've learned at this point.  We cannot be trusted to protect ourselves.



STEVE:  Yeah, it's really that we're learning that all of these devices, I mean, unfortunately, all of them have vulnerabilities which emerge after they've been shipped, and that users...



JASON:  Well after, yeah.



STEVE:  Yes, and that users just treat them as appliances.  It's a plug it in and don't think about it again.  Until it breaks, they're happy with it.  I mean, that would be the other thing to do would be to build in a deliberate breakage into the device so that it dies after three years, although then you have all kinds of other legal issues.



JASON:  Yeah, I think people would be upset about that, too.  I mean, but that's just - that's the huge issue with IoT as our homes, you know, I don't think of my home as incredibly smart.  I mean, there are smart devices.  But if I went into my Eero app and added up all the devices that were connecting, there's tons of them.  Even though I don't think that there are.  And I'm pretty tuned into technology.  If somebody has a decent amount of these devices, and they're just not thinking about updating them, it's so untenable to think I'm going to go over to all my devices every month and make sure that they're updated.  So it's got to be on the device side.  It's got to happen automatically.  I think it's more and more complex.



STEVE:  Yup.  So we have a senator who is often in the news because I have to say he's on top of things.  He's Oregon's Senator Ron Wyden.  And he's probably a pain in the government's rear end, but it's a pain that we need him to be.  Two months ago Ron wrote a letter to the Department of Defense.  He addressed it to Dana Deasy, who's the Chief Information Officer, so the CIO, at the U.S. Department of Defense at the Pentagon.  The letter reads:



"Dear Mr. Deasy:  I write to ask that you take immediate action to require the adoption of" - and get this.  I mean, it's hard to believe that this letter had to be written in the first place - "that you take immediate action to require the adoption of cybersecurity best practices on all publicly accessible DOD (Department of Defense) web services.



"In 2015" - right?  So three years ago - "the Office of Management and Budget issued memo M-15-13 requiring all federal agencies take steps to secure their websites and other web services, including interfaces for automated programmatic interaction (APIs) from cyberattacks.  The OMB memo gave agencies" - okay, so this was in 2015 - "gave agencies until the end of 2016 to enable HTTPS encryption and to enforce its use with HTTP Strict Transport Security (HSTS), which ensures web browsers will not use insecure protocols when connecting to HSTS-enabled websites."



So, okay.  In the first place, this is very cool that a senator knows what this is.  Maybe his staff; but at least, you know, but we know from Ron in the past that he's savvy to this stuff.  He then says:  "In 2017," okay, after the OMB letter of 2015 gave federal agencies until the end of 2016 to do this, in 2017 Ron writes:  "The Department of Homeland Security issued Binding Operational Directive (BOD) 18-01, reiterating the OMB requirements and requiring civilian agencies to adopt additional forms of basic cyber hygiene."



He says:  "A small number of DOD websites, including the Army, Air Force, and National Security Agency" - NSA, right, okay, so we would hope - "currently implement HTTPS by default" - a small number - "and use certificates trusted by major web browsers.  Unfortunately, many other sites, including the Navy, Marines, and your own office's website at dodcio.defense.gov, either do not secure connections with encryption" - now, this was two months ago, right, deep into 2018 - "either do not secure connections with encryption or" - get this - "only prove their authenticity using a certificate issued by the DOD Root Certificate Authority" - okay, and then he says, and I won't editorialize, I'll continue reading - which, he says, "many mainstream web browsers do not consider these DOD certificates trustworthy and issue scary security warnings that users are forced to navigate before accessing the website's information.  These challenges do not only impact civilians; service members accessing DOD pages from home regularly encounter security warnings and must click through such errors when accessing public DOD resources."  So in other words, to the degree that they did implement TLS encryption and authentication through HTTPS, they did so with some random certificate signed by the DOD root that isn't publicly trusted.  Unbelievable.



JASON:  So then what's the point?



STEVE:  Yeah.



JASON:  I'm very surprised that attention to this - and I'm curious why Ron, like what has motivated Ron Wyden to get so involved with all aspects like this.  But surprised that attention hasn't been given to this before.  This seems like no-brainer stuff.



STEVE:  I know.  And so what happened was this was prompted by Wyden's awareness of what was going to be happening with Chrome.  The letter continues:  "The DOD cannot continue these insecure practices.  Starting in July" - and we'll be talking about this in a minute because we've been speaking of it before, of course - "the Google Chrome browser" - and I'll editorialize here, noting that it is the majority browser on the Internet - "will begin warning visitors of non-HTTPS sites that the requested site is not secure.  These warnings will erode the public's trust in the department and its ability to defend against sophisticated cyber threats."  Or your mother.  Okay, he didn't say that.  "Moreover, the DOD's refusal to" - I love it, "refusal" - "to implement cybersecurity best practices actively degrades the public's security by teaching users to treat critical security warnings as irrelevant."



JASON:  It's a really good point.



STEVE:  "Normalizing these warnings increases the risk of cybercrime and foreign government hacking as users, both military and civilian, incorporate these dangerous practices reinforced by the DOD into their daily habits."



He finishes:  "DOD has prided itself on cybersecurity leadership, and now is the time to again demonstrate that leadership.  I urge you to direct all DOD agencies and offices to take" - I mean, I was going to interrupt myself to say, you know, everybody else has - "to take the following three concrete steps to improve cybersecurity of their publicly accessible web services.  First, adhere to all guidelines specified in that OMB memo M-15-13 and the subsequent DHS Binding Operational Directive 18-01, including enable HTTPS with HSTS on all public web services.  Facilitate the adoption of HSTS by delivering a list of all public DOD domains, including .mil addresses, to DHS as required by the DHS BOD.  Obtain and deploy certificates" - imagine this - "trusted by major web browsers for all web services accessible to the general public.  And evaluate the use of shorter-lived machine-generated certificates such as those available at no cost from organizations like Let's Encrypt."



There I would argue that, if you're going to do this, get EV certs from DigiCert and go with the best.  If you're going to bite the bullet, then inspire as much confidence as possible because why not?  And he says:  "Please provide me an action plan" - so this was two months ago.  This was in May he sent this - "by July 20, 2018, describing your progress implementing these steps and detailing an estimated date by which all publicly accessible DOD web services will implement these cybersecurity best practices.  If you have any questions regarding this request, please contact Chris Soghoian..."



JASON:  Soghoian, I think.



STEVE:  Soghoian, that's it, "...Chris Soghoian in my office."  So it's significant that Chris is there in Ron's office or accessible through Ron, at least.  So the good news is they did reply.  The DOD did reply last week, so close to if not by the deadline, saying:  "We've been working on it for two years, and we plan to have this done by the end of the year."  So, okay.



JASON:  Celebrate.



STEVE:  In the meantime, Chrome 68 and subsequent did execute on its intention of flagging non-HTTPS sites as "Not Secure."  That is happening as of last week, when 68 emerged from beta and became the version that everyone is using.  And for what it's worth, the DOD is not alone.  According to Cloudflare's telemetry, more than half of the top one million sites, 542,605 of the top one million sites do not use or do not redirect users to, an HTTPS version of the HTTP URL.  In other words, while they may have support for HTTPS, if you enter the site with HTTP, you stay there.



And of course that's a problem.  That's not promoting the user to the secure version.  It means you go in either way, and you get to remain.  So what this means is that, starting with the release of Chrome 68, a large number of users will see, if they happen to HTTP, a "not secure" indicator next to most of the sites they visit after they update to Chrome 68.  And I've said before, and I still don't know why it's not happening, that our browsers really should start attempting an HTTPS connection when not otherwise forced to HTTP.  So, for example, traditionally, if you just put in www.something.com, the browser defaults to HTTP.  They really should start flipping that and defaulting to HTTPS.



JASON:  Is there a reason that they don't?  I use a Chrome plugin called HTTPS Everywhere, and that kind of forces that behavior.  But why don't they do that?  Because that seems like - it seems like the right direction to go, more secure down to less secure if more secure doesn't exist.



STEVE:  Yes, it is.  And it's just inertia.  It's just that there's a concern that it might break some things.  But that's becoming decreasingly probable.  And I did some googling just because I was curious what effect the appearance of this not-secure indicator would have.  I heard Leo on the weekend, I thought I heard him grumbling about it, like he was annoyed that Chrome had done this because I guess it was upsetting people.  Maybe he had some calls on his Tech Guy show over the weekend.  I'm not sure what the back story is, and I'll ask him to elaborate next week.  But in googling I ran across an instance of somebody posting in the Google forum their upset that foxnews.com was now showing not secure.  And that did not make them happy.



So I thought, wow, really?  So I went over, and I put in www.fox - or maybe just foxnews.com.  And sure enough, it took me to http://www.foxnews.com.  And in Chrome, not secure.  And then I was curious.  So I added an "S," HTTPS, and that worked fine, too.  And now Chrome was not saying that it was not secure.  It wasn't super happy, I mean, it wasn't an EV cert, so it didn't light up green.  But I checked the certificate at https://www.foxnews.com, and it was a perfectly valid certificate issued by DigiCert.



And then what was interesting was they had a little "i" to the left of the URL for information.  So I clicked on that.  And even though it was HTTPS, that is, in the HTTPS version, everything was okay, I got a red warning saying "Your connection to this site is not fully secure."  And then in black underneath it said, "You should not enter any sensitive information on this site, for example, passwords or credit cards, because it could be stolen by attackers."  So I thought, oh, what?  So again, dug in some more, and I found some dialogue about this from people that might have been in the same thread because there was some back-and-forth grumbliness about Fox News showing as not secure.



And Google finally stepped in and said, "Hi, all.  The reason why https://www.foxnews.com is getting marked 'not fully secure' is because it includes a search form that sends data to http://www.foxnews.com."  And he says:  "Note the insecure HTTP.  We currently count that as not fully secure and downgrade the security indicator icon to the 'i' symbol rather than the lock.  A secure HTTPS site should load all resources over HTTPS and have all forms submit to HTTPS.  If you have any questions about this policy or our current implementation of it, feel free to follow up in this thread."



And what's interesting is that GRC has been historically accessible both ways.  I did quite a while ago, years ago, switched over to HSTS and preloaded GRC.com and www.GRC.com in Chrome as a known HTTPS-only site so that that would be the only way that Chrome users could connect.  And that list has been propagated since.  But you can, in a site's URLs, you can just leave the http: off and just do //www.foxnews.com.  And then any URLs on that page inherit the security of the containing page.  So that if the page is HTTPS, then all of the nonspecific links get upgraded to that.



JASON:  Right, that makes sense.



STEVE:  Yeah.  Although what you really want to do is just - it's time for, like, to everybody to switch over.  So anyway, I just wanted to give some more detailed coverage because a lot of people are going to be seeing either "not secure" or this "not fully secure."  And, you know, people are squawking.  But this transition cannot have caught anyone by surprise.  Google has deliberately signaled the plans to do this for more than a couple of years, well in advance, to website admins, all of whom had ample time to migrate their sites to fully secure connections.  I mean, maybe they just want to be stubborn, and they don't want to.



But clearly that's not the case for Fox News.  They've got a good DigiCert certificate.  They just haven't gone like the final step of moving HTTP, you know, if they've got the cert, HTTP accesses should simply redirect.  I mean, that's like one line in the Apache server config in order to redirect the user to the same URL, but with "S" at the end of HTTP.  So that's trivial.  Just do it.  And then it's clear that they need to go in and remove some older HTTPs within the content of the page, like that search form, and switch it over to HTTPS.



So anyway, Google tends to lead by having people kicking and screaming, but leading by example and getting people to make these changes.  Nobody's ever happy with it, but this is what it takes.  And so I'm sure there will be people who will, you know, this puts pressure on sites to decide what they want to do.  Maybe somebody wants to be stubborn and just stay HTTP.  It's certainly their right to do so.  But it is true that the site is not secured.  It doesn't really mean it's not secure, but I understand what Google is doing.



JASON:  I mean, not only is the site not secured - and sure, that's up to the site owner to make that determination.  But no matter what going forward, anyone using Google Chrome to navigate to that site is going to be presented with this big scary red screen.  I mean, at some point, I mean, this is what Google's move in this regard was all about; right?  Like yes, they signaled it far in advance.  They gave people ample warning.



But even Google knew that there was going to be an insane number of sites that weren't going to do the work to update to this.  And they know that people are going to squawk and be upset when they try and visit their favorite website and they're presented with this, and that the website owner isn't going to like it.  But they did it anyways because they know that, in the long run, the end game is that more of these sites are going to switch over, even kicking and screaming, but eventually they will because they aren't going to sacrifice the potential of their site losing traffic or whatever because people are scared of it now.



STEVE:  Well, and they would arguably not switch unless there was some pressure on them. 



JASON:  Absolutely.



STEVE:  And as you said, I mean, again, more than half of the top one million sites right now, 542,605, are not pushing people to HTTPS when the site supports both.  They're allowing people to come in with HTTP.  So if they've got bookmarks with HTTP, they just keep using that entry because that's the default, because the site doesn't bounce them over to the secure version.  And we also know that Google's not done.  This is the Chrome 68 step.  Downstream this gets more scary.  It gets red with a slash through it if sites don't take the hint at this point.  So it's not over yet.



JASON:  Yeah, absolutely.  



STEVE:  So there were some other cool features, I won't go into them in detail, that also were dropped with Chrome 68.  Most of them were for developer side enhancements, some CSS enhancements, and WebRTC got some upgrades.  But they did also add the Web Payments API, which has been evolving over time.  The intention of the Web Payments API is to provide - to essentially standardize through the W3C an API for purchasing stuff so that users who purchase through their web browsers, as so many of us do now, will be able to use, where sites support it, a unified experience for purchasing.  So that's one thing that I expect users to see.



The one thing that is still a puzzle for me is that we were supposed to get this strict site isolation feature in Chrome 68.  And I've got Chrome 68 now, and it's still off.  So I don't know why.  I talked about it a couple weeks ago.  Leo brought up Chrome that was supposed to have it.  And if you put in the URL chrome://flags, then you search for "isol" as the first four letters of isolation.  That brings up a selection of instances of settings.  And they're all, like the main site isolation for me yesterday when I double-checked again, was still disabled.



JASON:  Yeah, mine's disabled right now, as well.



STEVE:  Yeah.  And we keep hearing that it's enabled for almost all users.  And I'm thinking, almost all who?  I don't know where that is.  But it's very cool because it deeply sandboxes the rendering, cross-domain rendering.  So it breaks even one page, it's not just a page per process.  Now it's a domain per process to further insulate domains from each other.  Potentially expensive in terms of resources on the client system, on the client machine, but so much stronger that Google is looking at backing out of their Spectre mitigations because they believe that this per-domain isolation, when it's available, if it's ever available, will be really useful.  So we'll see.



JASON:  I just activated it myself and then restarted my - because I'm on Chrome OS, restarted the entire computer and then realized that's a horrible idea during a show.  When I do these things, it usually means I mess something up along the way.  But thankfully everything's all back up, so we're good to go.



STEVE:  Yay.



JASON:  So thank you for that tip.  All right.  So what else - he's been busy.  What else is Ron Wyden up to, and why are we still talking about Flash?  It amazes me that this is still a topic in 2018, but it is.  Here we are.  



STEVE:  Unbelievable.  It really is.  So we've talked about how glad we are that Adobe is finally going to shoot it and just put it out of its misery.



JASON:  Yes.



STEVE:  By the end of - but not, like, tomorrow.  They're dragging it out to the end of 2020.  So Ron Wyden has written another one of his letters, this time to - get this - the NIST (our U.S. National Institute of Standards and Technology), the NSA (the National Security Agency), and the DHS, (the U.S. Department of Homeland Security).  The letter asks those government officials in those departments to find solutions and procedures to mandate the removal of Adobe Flash content from all U.S. government websites by August 1st of 2019.  Okay, so one year from today.  So that makes it about a year and a half before its official end of life.  And of course we know that, as I said, Adobe is due to formally end-of-life Flash in 2020, after which they have said they would cease to provide any technical support for the software.  Okay.  Lord help us.  It just needs to go away.



JASON:  Yeah.



STEVE:  In his letter, well, and, I mean, it can because HTML5 - basically Flash now is only an old-school media player.  It used to be that games and puzzles and all kinds of stuff were run on Flash because it was - we didn't really have JavaScript as pervasive as we do now.  The web just didn't have the media handling and especially the video playing capabilities.  Well, all that's now available in every browser through HTML5.  So it's only inertia.  Again, it's only because Flash used to be what people were using that it's still there.



JASON:  Well, and what my question in that regard of it being inertia, okay, it was there.  Are they still actively using it and updating it?  Or is it just like legacy Flash pieces?  Because this isn't just about security.  It's also about broken sites because as users we're not using Flash anymore.  So we visit a site that has Flash, we're missing something.  Or even worse, it looks like garbage.



STEVE:  Well, and of course iOS devices have never supported Flash.



JASON:  Right, that's true.  



STEVE:  That was one of Jobs' big issues was he said I refuse to support that on the iPhone or on the iPad.  So it's always had a problem there.  So anyway, so Ron said in his letter, he said:  "The federal government has too often failed to transition away from software that has been decommissioned."  Actually, he referred to Windows XP, which I guess the federal government is still using.  And frankly, I have to say, I would be, too, if my XP machine hadn't died last month.  But now I'm on Windows 7.  I'm happy there.



And he says, he points out Flash's serious, and I love this, "largely unfixable cybersecurity issues" as one of the reasons U.S. government sites should take proactive steps to permanently remove this technology from all of its sites well before the cut-off date, after which, as he says, Adobe won't provide any security fixes at all.  So he asks that government agencies not deploy new Flash-based content on any federal website, effective within 60 days; that federal agencies remove all Flash-based content from their websites by a year from tomorrow, August 1st, 2019; and require agencies to remove Flash from desktop computers by that same date, by August 1st, 2019. 



So anyway, it is in decline.  We know that.  This last February Google's director of engineering said that the daily use by their Chrome users had dropped from 80 percent, that is, I should say Chrome users who encountered at least one page containing Flash content per day had dropped from 80 percent in 2014.  So four years ago, four out of five Chrome users did encounter at least one page containing Flash content.  Now that's 8 percent.  So from 80 percent to 8 percent by the time he made this statement in February of this year.  So way down.  And there's a web technology survey site, W3Techs, said that only 4.9 percent of today's websites utilized Flash, which is down from 28.5 percent at the beginning of 2011.



So that doesn't represent as much of a precipitous decline as Chrome's users have encountered, that is, a factor of 10 drop.  Here it's a factor of, what, like five.  But still, it's clear that it's time for it to go.  So I think what'll end up happening, I mean, it's been so deprecated that it must just be sort of abandoned sites that have not figured out by now that, boy, you know, if we want our videos to play, we've just got to move over to HTML5.  



JASON:  Yeah.  I guess it must be painful to move your Flash content over because, if a government site is still serving it up - well, either that, or there's a theme here.



STEVE:  Well, and if it's not - yeah, exactly.  There is the "we don't care" theme.



JASON:  Yes, exactly.



STEVE:  But if it is active Flash content, then it may be they no longer have the source.  They lost the programmer.  They've got this SWF thing that actually does some business logic in it.  And someone's going to have to recode that in JavaScript.  And that will take some resources.  So, I mean, it's certainly the case that there were other things than video that Flash was able to do.  And so if there are sites that have proprietary business logic written in Flash, well, sorry, folks, you're going to be SOL.  I mean, there's no question that browsers will refuse and OSes will refuse to run Flash in a couple years.  So minimize the pain by getting it done sooner rather than later.  



JASON:  I guess it's also a cautionary tale for anyone building a website or building a service around certain tools like that; right?  You hinge so much of what you're offering to people on a certain piece of technology.  And if and when it's deemed insecure or decides to go away, yeah, you're left footing the bill.  But I guess that's just the world that we live in.



STEVE:  Well, 10 years ago it probably made sense.



JASON:  Yeah, good amount of time.  Yeah, exactly, it was a good amount of time.



STEVE:  Yeah, there was no useful alternative back then.  Now there is.



JASON:  Absolutely is.



STEVE:  I did have in the notes, and I got off on the strict site isolation, I did want to mention that Firefox is also moving toward strict site isolation.  They have a project that they call Fission, which I don't know, seems like the reverse of strict site isolation.  But anyway, that's the name of the project.  And it's intending to provide much stricter site isolation sandboxing, very much like what Chrome is doing, and bringing that to the Mozilla Firefox platform.  So everyone's going to experiment with that.  We'll hope that it doesn't become too burdensome on our systems.  But it does look like giving each domain its own process would be a useful increase in sandboxing security.



JASON:  Yeah, and reading through that what kind of was a little alarming was their example of showing just how much memory overhead they were getting right now, and the number's far too high.  When I think of Firefox as a browser, and it might be an outdated belief because I don't use Firefox on a daily basis necessarily, definitely not on mobile, a little bit on desktop, but I don't think of it as the most memory capable, let's say.  It always seems to kind of slow itself down in my experience with it.  So that seems like a big hurdle for them to overcome here.



STEVE:  Yeah, yeah.  There was an interesting piece in Bleeping Computer that sort of caught me up short, I guess.  And I wanted to make sure I put it on our listeners' radar.  And it's a problem.  And that is that there is a trend now for the registration by third parties of legitimate domain names in other top-level domains.  And Bleeping Computer's article covered specifically French and Spanish language versions of legitimate sites:  keepass.fr, 7zip.fr, inkscape.fr, gparted.fr, clonezilla.fr, audacity.es, clonezilla.es, handbrake.es, gimp.es, thunderbird.es, audacity.fr, I mean, and on and on.



And what's happening is that these are not sites that are put up by the owners of the official dotcom domains.  They normally do contain French and Spanish language translations of the legitimate sites, but they are offering adware-laden downloads of those products, of 7-Zip or KeePass or GParted or Clonezilla.  So of course the elevated danger beyond that is that they could also be offering fraudulent malware.  At this point they're not.  What they're doing is they are wrapping them in adware installers because the adware pays per installation.



So these are legitimate-looking, language-specific websites using the second-level domain of the legitimate site that's normally a dotcom domain, offering altered downloads which install components of software that the original providers don't have anything to do with, and certainly don't want to be supporting.  So just be aware.  I don't know, I mean, the problem is, in this instance, all of these various sites were registered using the same email address.  So it's one entity that has decided this is the way they're going to make money is they're going to adware-enhance legitimate software, create foreign language translations of the original English-language dotcom sites under .fr and .es, which of course are French and Spanish, respectively.  So under those TLDs.



And of course, I mean, some companies will take the time and effort to register the equivalent domain in every other TLD, which, I mean, it's one of the problems, one of the fundamental problems with the way the Internet is structured is what do you do about that?  I mean, if you invest in a major trademark like GRC or like TWiT, well, you don't want some creep registering the same second-level domain in a different TLD and drawing some traffic to their site under the belief that it's related in some way to GRC.com or TWiT.tv or Google.anything else.



So the tendency for a commercial company is to immediately also grab their main dotcom site name in all other TLDs and essentially just squat on them or grab them and then aim them all, redirect them all to their primary authentic domain.  But if you do that, then that kind of kills all the whole point of having individual country-based TLDs.  So the whole thing just sort of didn't evolve right.  And I guess the way it should work is that an English-language dotcom site could create an authentic French-language .fr and Spanish language .es version of their own site, and then do it legitimately.



But that's also a huge amount of work.  And a lot of these are just open source sites, you know, they're .orgs or .nets.  I mean, they're just - they don't have the resources.  And, by the way, as we know, you have to pay annually for domain names.  So if you go and grab your name in every TLD there is, suddenly your annual maintenance cost just jumps up because you've got to maintain the registration of those everywhere, and some of them are not as inexpensive as the dotcom variants.  And there's a bizarre, I mean, there's, like, they're constantly proliferating, with there being more of them all the time.



So it's a mess, and I don't see a good solution.  The problem is, since they're legitimate registrations, and they legitimately own the domain, they can get, you know, they can be using Let's Encrypt and have certificates and HTTPS and have Google Chrome say, yeah, this is a secure site.  And if it looks legitimate, how would a user know that, if it's a French translation of a dotcom with a valid certificate that looks right, how would they know not to download the software and in fact get this extra adware installed on their computer that they don't want to have?  So anyway, I just wanted to sort of put it on everyone's radar because it's a problem.  And I don't see a solution.



JASON:  Yeah, that sounds nasty.  And it's not just the fact that you end up at the site, and you see the TLD, and everything checks out in your head, looks legit or whatever.  



STEVE:  It is.



JASON:  But also the fact that, like, you installed the app that you expected to install, and it's not like it's some wacky other thing.  It's exactly what you wanted.  It just did this other stuff underneath.  So from that point on you still kind of go on living your life, not thinking anything happened because every single step of the way checked out.



STEVE:  Yup.



JASON:  Seemingly, anyways.  The article on Bleeping Computer mentioned something called VirusTotal as a service to upload files that you download from these things.  Are you aware of that service?  Is it like, if you don't have a virus scanner on your computer, that's a way to run the file through to make sure it's legit?



STEVE:  Yeah, it's very cool.  We have talked about it on the podcast.  It's a service owned by Google.  And you're able to, if you have - the way it's mostly used is, if you have a file that you just - for some reason you feel a little sketchy about it, like, I'm not sure where this came from.  I found this on a thumb drive, or a friend gave it to me or something.  It's free.  And what it does is, I don't remember how many there are, it was like 70 or 80.  What it does is it passes the file through every AV solution in the industry and shows you how many of them think that there's something sketchy about it.  And so it's very cool.  You can also do it with URLs, so like is this a sketchy site URL?  It'll evaluate those.



For a while I was, with SQRL, Windows Defender was taking a while and scrutinizing my SQRL client deeply.  And so I kept putting versions of it up on VirusTotal, and it turned out, I mean, I'd just built it fresh.  I knew that there was nothing weird in it.  It turns out it was just the reputation.  It hadn't earned a reputation because it was so new.  And that behavior all went away, and now there's no delay.  But, yeah, VirusTotal is a very cool solution, and available for free.



JASON:  Yeah, there it is.  Nice to know about that.  I didn't realize that existed.  Awesome stuff.  This next one I've been very curious about.  This is about Google's kind of Play Store rules, and that involves cryptocurrency mining now.



STEVE:  Well, yeah.  And for a while, I mean, cryptocurrency mining is on the outs, obviously.



JASON:  Yeah.



STEVE:  The whole idea of mal-mining or cryptojacking, stealing someone's processor time, has been problematical.  Actually it's a bigger problem on smartphones because, although smartphones are balanced to allow the processor to burst when necessary to get some work done, they assume most of the time the processor is going to drop to a more or less idle, low power state.  Of course, that's antithetical to cryptocurrency mining that wants to just go balls to the wall and squeeze as much processing out of any processor and GPU that it can find.  So there have been instances of devices truly being destroyed by malicious mining on them.  Batteries overheat; cases expand; gases are produced; the cases break, are deformed; the batteries leak; all kinds of stuff.



So over time we've been seeing Google sort of incrementally putting the screws to cryptocurrency mining.  For a while, the intermediate step was that, as long as the mining was not deceptive, that is, as long as an app said, "Hi, I'm John's Ethereum miner.  Download me and plug your phone in because otherwise I'm going to drain your battery.  Let's mine some Ethereum."  I mean, when an app was right upfront, Google said, well, okay.  If the user wants to do that, then fine.  Whereas the mining that did it surreptitiously in the background, those had been officially banned and were removed whenever they were found.



Well, what happened just now is Google has changed their policy.  They have now, in their Developer Policy Center, which is a long, comprehensive document about all the things, and it's a growing list of things, that apps are not allowed to do, all kinds of deceptive stuff and pretending to be one thing and being another, you know, basically the list is growing because Google wants to have a basis for rejecting things from the Play Store and say, look, you're in violation of our policy.



So what just appeared under the Developer Policy Center, under a new section that was added called "Financial Instruments," they said:  "We don't allow apps that expose users to deceptive or harmful financial instruments."  And then they said:  "We do not allow apps that provide users with the ability to trade binary options."  And specifically, under "Cryptocurrencies," they said:  "We don't allow apps that mine cryptocurrency on devices, period."  They said:  "We permit apps that remotely manage the mining of cryptocurrency."



So this is a change.  So even a Play Store app that is, as I was mentioning before, is right upfront and telling it like it is, sorry, that's no longer the case.  They did not notify anyone of these changes.  They did not publicly preannounce this.  So there were developers who had apps removed who were complaining on Reddit about this.  But Google said, sorry, we've made a policy change.  We're no longer going to allow any cryptocurrency mining at all to be sold through or offered through the Google Play Store.  So I suppose you could still sideload if you wanted to in order to get things onto your device.  But it's probably a bad idea. 



JASON:  And if you're mining cryptocurrency, there's probably some sort of Venn diagram between, like, cryptocurrency on mobile miners and people who are comfortable sideloading apps.  There's probably a big overlap there; you know what I mean?



STEVE:  Yes.



JASON:  I'm sure it'll be all right.  I'm sure they'll be okay.  But this is good news, I think, for Google to do this.  They're not alone.  Apple did this very recently, as well.



STEVE:  Last month.



JASON:  Google's been blocking this from Chrome extensions in the Web Store.  So, yeah, this makes sense.



STEVE:  Yeah.  It was time.  And I think it was clearly something, I mean, the handwriting was on the wall that this was not going to be allowed to continue.



JASON:  Yeah.



STEVE:  We've talked also about in the past about supply chain attacks.  And this is sort of worrisome.  One that occurred recently was an app that Leo's not a big fan of, but actually I am, CCleaner, got a version of itself infected, like the actual website for CCleaner was attacked, and the download for CCleaner was altered.  I don't remember now what malicious was put into it.  It might have been file encrypting malware, or it might have been a cryptocurrency miner.  But it was found and fixed quickly.



But the point is that there people were legitimately downloading from the CCleaner site an app that was not what it appeared to me in a so-called supply chain attack.  Microsoft in their cloud blogs talked about their discovery of a new, a recent supply chain attack which Bleeping Computer covered.  And I'm just going to share what Bleeping Computer wrote because it's a nice summary.  Microsoft's write-up, if anyone is interested, I have a link in the show notes, and it is much longer.  Bleeping Computer did a nice summary.



They said:  "Microsoft said today that hackers compromised a font package installed by a PDF editor app and used it to deploy a cryptocurrency miner on users' computers.  The OS maker" - meaning Microsoft - "discovered the incident after its staff received alerts via the Windows Defender ATP, the commercial version of the Windows Defender antivirus.  Microsoft employees say they investigated the alerts and determined that attackers breached the cloud server infrastructure of a software company providing font packages as MSI files."  That's the Microsoft installer format.



"These MSI files were offered to other software companies.  One of these downstream companies was using these font packages for its PDF editor app, which would download the MSI files from the original company's cloud servers during the editor's installation routine.  But hackers created a copy of the company's cloud servers.  Microsoft security researchers said that attackers created the first company's infrastructure, duplicating it on a replica server that the attackers owned and controlled.  They copied and hosted all MSI files, including font packages, all clean and digitally signed, in the replica server.  Then the attackers decompiled and modified one MSI file, which was an Asian fonts pack, to add the malicious payload with the bit mining code.



Using an unspecified weakness, which they said did not appear to be a man in the middle or a DNS hijack, the attackers were able to influence the downstream parameters used by the PDF editor app.  That's the one which was then following up and downloading the font packages.  The parameters included a new download link that pointed to the attackers' server.  Users who downloaded and ran the PDF editor app would unknowingly install the font packages, including the malicious one, from the hackers' cloned server.  Because the PDF editor app was installed under system privileges, the malicious coin miner code hidden inside would receive full access to a user's system.  So actually it was sort of fortuitous and lucky that all it wanted to do was to mine cryptocurrency and not get up to more mischief.



"The malicious miner would create its own process named xbox-service.exe under which it would mine for cryptocurrencies using victims' computers."  And I was thinking about that.  I mean, if you looked into why your system suddenly was running slower, and then checked with process monitor and saw that xbox-service.exe was using up a lot of processor, well, maybe you'd think, oh, well, okay.  Maybe it's supposed to.  You know, I mean, a lot of people aren't going to know.



Microsoft said Windows Defender ATP detected mining-specific behavior from this process.  Investigators then tracked down the origin of the process to the PDF editor app installer and the MSI font packages.  Security researchers said it was easy to identify which MSI font package was the malicious one because all the other MSI files were digitally signed by the original software company except that one file, which lost its authenticity when the crooks injected their coin miner code into it.



So anyway, this is another problem that we're seeing in various forms, the supply chain attack where users are being as responsible and cautious as possible, where they are visiting a site whose security looks good.  It's got its certificates intact.  Everything looks copacetic.  They download an installer which looks like it's doing a legitimate job.  It's signed.  It's working correctly.  But once the PDF, in this case, PDF viewer or the app that was installed by the installer runs, it then reaches out and flushes itself out.  At that point, it's going to a different, you know, to this cloned network.  And we don't know how it was pointed to a different service, but it was, and downloaded this altered font package, among all the font packages, which installed the cryptocurrency miner.



So the problem is everything was done right by the end user, yet they still got a cryptocurrency miner in their system.  So again, running as system, where the malware could have done anything it wanted to.  So I don't know how someone would protect themselves from this.  It's certainly beyond the expectation of an end user to, well, actually the app you've installed is just going out and doing stuff on the network, and you trust it because it was secured by the installer, and the installer was secured by the download, and the download was secured by the certificate of the sites you went to.  So an intact chain of trust.  Yet you've still got a cryptocurrency miner in you.  So this is a fruitful attack, and I have no doubt we'll be seeing more of them in the future.



JASON:  And when a computer has been co-opted like that, there's a cryptocurrency miner happening active in the background, like what does the user of the computer actually notice?  Do they notice any sort of slowdown?  I mean, that's not light work that's happening.  Or I don't know; is it?



STEVE:  Right.  It is, yeah, well, no, no, it is normally very heavy.



JASON:  Yeah.  



STEVE:  Because the cryptocurrency miner wants to use as much of the system's resources as possible.  So one of the things - but the cryptocurrency miner also wants to remain undiscovered to whatever degree is possible.  It doesn't want to have itself removed by drawing attention to itself.  So what normally happens is, and we've talked about this in the past, they are beginning to be a little foxier about when they use the system.  Like they'll notice if the system is idle and, for example, use all of the CPU, but use it at the lowest priority possible so that other things that need to run, running at normal priority, will be able to preempt the cryptocurrency miner, yet it'll still suck up all the slack time.



Or it may wait for a period of inactivity so that there actually is an API call that is, like, notify on system inactivity, where some period of time of no mouse and keyboard use has occurred.  So then it figures, oh, I'm still running, but the user has walked away, so now's a good time to do some mining.  And then the second the user touches the mouse or types something, the miner will suspend itself, again not wanting to give away the fact that it's there.



JASON:  Interesting.  Yeah, I definitely see more of that, for sure.



STEVE:  Yeah.  So Titan security keys.  This was big news last week.  Google launched their own USB and Bluetooth-based FIDO (that's the Fast IDentity Online, F-I-D-O) FIDO U2F (which is the Universal 2nd Factor protocol) hardware tokens.  Of course we're all familiar with the equivalent from YubiKey, which is actually the company that I'm probably responsible for discovering.  Stina tells everybody that it was this podcast, which occurred a couple weeks after a significant RSA conference.  I happened to be attending the RSA conference for the podcast with press credentials, and Stina was there standing at the top of an escalator looking around for some press people.  And she asked me if I was interested in authentication, and I said yeah.  And I didn't know how much I was interested; but, I mean, it had been a constant topic of ours for the podcast.  We talked about the little eBay and PayPal football that generated the six-character changing one-time password token.



Anyway, so she explained that she had a little thing that emulated a USB keyboard, and it produced a one-time password.  And every time you touched it, it spit out a different string that was cryptographically secure.  And I gasped because it was such a clever and cool idea to use this little tiny fob as a USB keyboard.  It needed no drivers.  You could just put the cursor in a field and touch it, and it would type this crypto string into a browser form and then off you'd go.  The first chunk of digits was not changing.  That was its ID.  And the second longer string was a one-time password which changed every time.



So, I mean, it was just - it was beautiful.  And three weeks I think or so later I did the entire podcast on the YubiKey, explaining how it worked and what it did.  And Stina, I guess she has - there was some presentation someone pointed me to years ago where she said, yeah, we got put on the map thanks to Security Now!, which was very cool.



JASON:  That's awesome.



STEVE:  So they've been, I mean, they have been the hardware dongle provider of choice.  And I don't know what their relationship to Google is.  I know that they've worked with Google, that Google has had lots of YubiKeys in the past.  And so I don't know if this has been done in conjunction with Google, or if Google just decided they wanted something different or go in a different direction than Yubico.  But so this is something like that.



I did give Yubico a full presentation on my solution, SQRL, last November and sat down with Stina and three other techies, I mean, like THE crypto techies, ran through the whole thing, answered all their questions, and they were very interested.  They're basically just waiting for me to get this thing done.  And if it gets traction, they're very interested in doing a SQRL-based hardware token of the same ilk as what they've done before because SQRL you can absolutely have the user's super secret one private key put into a piece of hardware to protect it from any possibility of theft.  And I've gotten a lot of questions, not surprisingly, as time has gone on with AuthN and FIDO and so forth, about how SQRL and FIDO are different.  And I will certainly address that in a document to put them side by side.



One of the things I will say is that FIDO is by its nature, and even by its name, you know, U2F, Universal 2nd Factor.  What it is not is a first factor.  And that's probably one of the biggest differences is that what SQRL is, is a secure one factor, that is, it identifies you and authenticates that identity.  None of these 2nd factor things are able to identify you.  They have to know who you are claiming to be first because they store your credentials on the server.  So the server has to send the credentials back to you for you to then authenticate against those credentials.  So it's a small thing, but it makes a big difference in the login flow because with SQRL you don't have to identify yourself.  You just click the "Log in with SQRL" button, and you're done.



And there's a lot more, too.  SQRL has what we call "complete lifecycle management," or "identity lifecycle management," where if a bad guy gets your SQRL key, malware gets into your phone where you're storing it, or something happens, or you were to lose it, there is a provision for locking down your identity and securely recovering your identity and managing it over the long term, none of which is built into FIDO.  That's considered all, like, per site.  Each site will handle that individually.



So anyway, when it's finally done, when I get SQRL's authentication working on the SQRL public web forums, then it'll be time to officially announce and launch this.  And there is an Android client, an iOS client, web extension.  There's a native Linux client, a PHP support server side, Java server side code.  So there's a lot of other stuff going on that I'll be talking about in the meantime.  And right now the Google Cloud users are able to get these Titan security keys.  They are said to be available widely through the Google Store at some point in the future.  And I look forward to learning a little more about what's going on behind the scenes with these.  Are they actually being manufactured by Yubico and privately branded, or what?  I don't know.



JASON:  Or completely removed from Yubico.  I guess Google's had its employees, 85,000 of them, using these types of security keys since 2017.  They've said that there have been no account takeovers since they put that requirement in place.  So it'd be interesting if they were using the Yubico keys all that time and then, like, all right, we got what we needed.  Now we have our own.  See you later.  Hopefully there's some sort of a relationship there.



STEVE:  Yeah.  Yeah.



JASON:  Well, that's cool.  I want to get one myself and check it out.



STEVE:  Sounds cool.  So I did get a nice note on the topic of SpinRite, which I try to share with our users every week when I can, from Joshua Montgomery, who's in Springfield, Illinois.  The subject was "SpinRite Saved One of Our ATMs."  He sent this on the 26th of July.  He said:  "Steve, I've been an avid listener since the beginning of Security Now!.  Thanks to you and Leo for all of your hard work."  He says:  "I wanted to thank you for making this splendid product.  It saved my neck.  We had a very high-traffic ATM hard drive fail.  The tech said they could not make it for 48 hours."  He says:  "I pulled the hard drive from the ATM and ran SpinRite on it.  It fixed enough of the drive that I could clone it to an SSD within three hours.  Now we have one the fastest ATM machines in the North.  Keep up the good work.  Joshua."



JASON:  That's got to feel good, to get a message like that.



STEVE:  I get them all the time, and I really do love it, to be able to, just like here, SpinRite just fixes the drive and then off people go again.



JASON:  Yeah, from 48 hours to three hours, no big deal.



STEVE:  Especially on a high-traffic ATM.  Apparently it was, like, really important.



JASON:  Yeah, you don't want any downtime on something like that.  That's people's money.  Including your own.



STEVE:  Yup.



JASON:  So tell me a little bit about Bluetooth because I'm, like, late to the game on Bluetooth, at least when it comes to headphones, anyway.  I mean, I have Bluetooth devices that I use on a regular basis, but I've been reluctant to use Bluetooth over the years just because I find it not as dependable, dropouts.  There's always the kind of potential for security risk depending on what's going over.  And this definitely ties into that.



STEVE:  Well, and you've got to keep them charged.  And so there's that, also.  And actually I'm a little concerned about the health risks.  I mean, you're sticking a radio in your ear canal.



JASON:  That's true.  



STEVE:  And it's like, let other people do that.  I just think I'd rather not do that myself.  But Bluetooth of course has lots of other applications.  It's become, because it is high speed and low power, and it's sort of got a nice range compromise, you know, NFC is deliberately like contact near-field radio.  And of course WiFi is deliberately long-range radio.  Bluetooth sort of fits right there in the middle, at 30 feet or 10 meters as the official distance, and then there are some extended distances.  But it has always been - there's always been a vulnerability.



In fact, the research paper which was published and will be given is titled "Breaking the Bluetooth Pairing Fixed Coordinate Invalid Curve Attack."  And our listeners have heard me say many times in the past that the only way I think it is secure to pair Bluetooth devices is to walk out into the middle of a large parking lot, like on a Sunday morning when it's empty, or maybe go into the middle of a football stadium.  That is, physically isolate yourself from the environment and do the pairing because Bluetooth pairing is the moment of vulnerability.  That's when there is stuff happening that is the devices finding each other and exchanging cryptographic material and deciding, like agreeing upon a secret key which they will then use for everything else.  That's the weakness.  That's the moment of maximum vulnerability.



Now, the protocol works to minimize that.  There's all this, you know, each side displays a six-digit code to the other, or one side is a screen and the other side has a key.  And so you enter the number that one side is showing.  The idea is they're trying to use an out-of-band communication, that is, a visual real-world communication outside of the radio band.  But there are many instances where you don't have that luxury.  You've got like an IoT device.  It's a door lock.  Or, well, a door lock might have a keypad, so that's a bad example.  But like a sliding window lock where it's just a little magnet that is going to use a Bluetooth connection to the home hub, and it doesn't have anything.



So pairing has always been problematical.  And so the spec over time has evolved through Bluetooth, well, up to 4.0 and 4.2, continually working to improve the pairing technology.  So the crypto used in the latest Bluetooth is Elliptic Curve Diffie Hellman, ECDH, which is an evolution of the original RSA-style Diffie Hellman, which is really a cool technology.  We've talked about this before.  This is a key agreement protocol which, even though it's hard to imagine, two parties can exchange data in the clear, that is, can publicly exchange some information with each other.  And even though a third party can capture all of their interaction, those first two parties are able to securely agree upon a secret which the third party, despite seeing all of their conversation, cannot obtain.



So it's just - it's very cool.  And it's one of the fundamental architectures for cryptography.  And elliptic curve is being used today because it allows much shorter keys.  And that ends up being important.  The packet length, for example, for Bluetooth Low Energy is - I think it's 16 bits, or it's very, very small.  So maybe it's 16 bytes.  Anyway, it's very small.  So it's impractical to exchange a large key in a Bluetooth Low Energy environment.  You really want the key to be shorter.  And the computational power of these little devices is really reduced.  So that's why elliptic curve is where everybody is going moving forward.



Okay.  So what's this attack?  The short version is it is nothing to worry about.  What these guys discovered is an active attack where there should not be one.  But it does require that an attacker be involved in the pairing, be able to intercept and modify some of the traffic that is being exchanged.  What the current spec fails to do is to authenticate one of the parameters that is being exchanged.  In Elliptic Curve Diffie Hellman there is the private side of the public and private key pair.  The private side is a large scaler number which is kept secret.  The public side is a coordinate on the elliptic curve, which is exchanged as an X and a Y.  It turns out that the protocol only requires, well, yeah, the protocol only requires that the X coordinate of the X and Y coordinate pair, which form the public side of the public/private key, is validated.



And what these attackers discovered is, if they can arrange to get themselves in a position to somehow zero the Y coordinate of the X/Y coordinate point on the curve with a 50% probability in some cases, or a 25% probability in most cases, they're able to decrypt the private key that these guys - that the two valid endpoints end up negotiating.  Thus the reason that I don't think this is a big deal.  So it's like only in the event of the pairing, only if an attacker can actually intercept and alter by zeroing some of the data being exchanged, can this even be done, and even then with a 25 or maybe 50% chance of determining what the private key is that is negotiated between the parties.  And in the other case their having zeroed the Y coordinate causes the agreement to fail.



So they're either able to get the key that the parties negotiate or the Bluetooth pairing fails, which would cause it to be redone until it succeeds, which would suggest they're ultimately able to obtain the private key.  But I don't know how, I mean, in practical terms, an attacker gets themselves in a position where they're able to prevent the recipient at each end, because it's an exchange, to prevent the recipient from receiving the proper coordinate.  So you'd have to have directional antennas aimed at each of the devices being paired.



Also Bluetooth, remember, is also a frequency-hopping, spread-spectrum technology.  So you have to lock onto the frequency hopping and know which frequency each side is going to be transmitting to the other on.  That's now doable, but still it's more involved.  You then presumably have to flood the recipient with a saturation, a radio saturation attack to prevent it from receiving the valid data from the other side; then stop flooding it and then send it the altered data which you would have received from the other side while it was trying to send it to the side that you were flooding; then alter that by zeroing the Y coordinate; and then forward it on to the side that you had just flooded; and then do the entire same thing immediately to the other side.  So it's like, Holy Toledo. 



JASON:  So Steve, you're saying then that there's a chance that it could happen.



STEVE:  Yes.  This is like, okay, uh, yeah.



JASON:  Yeah, it sounds like it was a long way to go.



STEVE:  Yeah.  And so it is a vulnerability.  It is there.  It is theoretical.  Yes, it could happen if, like, all the planets aligned correctly and somehow a bad guy is desperate to do this and has the technology and is present during the event of pairing.  I mean, maybe there is a - and I can't think of what it would be, like a high-value system which is going to be pairing over Bluetooth at a known time and place, and a bad guy could set up in order to, like, intercept it, that would be high enough value to be worth doing.  But I really can't imagine the scenario.



So the attackers have looked at different systems.  They did find that the Android Bluetooth stack Bluedroid is vulnerable, whereas Windows is not because it does not support the Low Energy SC pairing that is vulnerable.  There is a protocol, SSP, Secure Simple Pairing.  The vulnerability in SSP depends upon the Bluetooth chip firmware implementation, since the handshake is performed in the chip rather than by the host.  They said that during their research they found that the devices of most chip vendors are affected.  Qualcomm's, Broadcom's, and Intel's implementations are vulnerable, which they said together constitute most of the Bluetooth chip market.



So they said:  "We stress that every device - mobile phone, laptop, or car - that uses such a chip is vulnerable."  On the other hand, I would stress that it really doesn't matter because, I mean, think about this.  Our listeners should know that there's this chance.  It is also the case that everybody is scrambling to fix this.  So Android will get an update.



JASON:  I think they already did.  I believe it may have been in the June security patch.  But then again, it's part of the patch.  Who has the patch?  You know, that's the conundrum that Android users are always in.



STEVE:  Yes.



JASON:  Like Google did something about it, and they included it in their security update.  But did you ever get that update?  Probably not.



STEVE:  Yup.  So there's a lot of furor in the press about, oh my god, Bluetooth is vulnerable to hacking during pairing.  It's like, yeah.



JASON:  Technically so.



STEVE:  Theoretically.  And anyway, the fix it turns out is simple.  If the Y coordinate is zeroed, then the public side of the public key pair, which is supposed to be a point on the curve, will be invalid.  So all anybody has to do is what they maybe should have always done, which is verify that the public point they receive from the other side is a valid point on the curve.  They're not doing that.  So that's all they have to do.  So fixing it is not a big problem.  It'll generate a next rev of the spec, which will require it because now we know, oops.  Maybe there's some chance of it being compromised, and we don't want that to be the case because we all want Bluetooth to be secure.  So it'll be even more secure.



I think it's fine just the way it is with this bizarre caveat that, okay, there's a theoretical way that the handshake could be intercepted that would, with some probability of 25 or 50%, allow the bad guy to negotiate in order to watch, to recover the private key that both guys end up with.  But wow.



JASON:  Far cry from BlueBorne, which I believe was the Bluetooth virus that we heard about last year.



STEVE:  Exactly.  Exactly.



JASON:  Steve.  Got any other things that aren't included in this list that you want to kind of finish things off with?  I know obviously GRC.com, but I don't want to start the close of the show yet until I know for sure.



STEVE:  Close it.  Close it.  We're done.



JASON:  You've got it all in.  It's all in there.



STEVE:  My work here is finished.



JASON:  Your memory dump is done.  It's over.  And I really appreciate it.  GRC.com.  I know everybody listening and watching appreciate it, too.  Anybody who wants to catch all the stuff that Steve is up to, go to GRC.com.  Obviously you talked about SpinRite, best hard drive recovery and maintenance tool.  You can get your copy there.  Information about SQRL.  Audio and video of the show, you're hosting that on that site, as well, as well as transcripts; right?



STEVE:  Yup.  Yup, exactly.



JASON:  Right on.  People can find all that there.  TWiT.tv/sn, if you want to go to our site.  And you can of course find all the audio and video there, subscribe to the podcast, Security Now!.  Do a search for Security Now! in any of the podcatchers that you're using, you're going to find it.  Search for TWiT or Security Now! and you'll definitely come across that.  And then of course we have a live chatroom, got a live stream.  We do this show every Tuesday at 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC.  If you go to TWiT.tv/live you can tune in and watch as the show is recorded and participate in chat.  Everybody's always talking about a number of the security topics.  Everybody kind of expands on the topics of the day, and it's a lot of fun to participate in that way, as well.



Steve, I always appreciate having the chance to do a podcast with you, so thank you for allowing me to be your cohort today.  I really appreciate it.



STEVE:  Well, likewise.  And I won't see you next week or for a few weeks, but then you're going to be back for a run of three.



JASON:  That's right.



STEVE:  So that'll be great.  We will have more of the Jason/Steve team in September.



JASON:  Right on.  I'm looking forward to it.  That's right, that's in September.  So Leo will be out then, and I will see you then.  But next week Leo and Steve will be back for another episode, so make sure you don't miss it, of Security Now!.  Take care, everybody.



STEVE:  Thanks, Jason.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#675

DATE:		August 7, 2018

TITLE:		New WiFi Password Attack

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-675.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss yet another new and diabolical router hack and attack, Reddit's discovery of SMS 2FA failure, WannaCry refusing to die, law enforcement's ample unused forensic resources, a new and very clever BGP-based attack, Windows 10 update dissatisfaction, and Google advancing their state-sponsored attack notifications.  We ask, "What is Google's Project Dragonfly?"  We go over a highly effective and highly targeted ransomware campaign, present some closing-the-loop feedback from our listeners, and reveal a breakthrough in hacking/attacking WiFi passwords.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  WannaCry just won't stop crying.  We'll talk about a new router attack and how people hate Windows Update.  Plus Steve will detail a new and a little bit scary WiFi password attack.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 675, recorded Tuesday, August 7th, 2018:  New WiFi Password Attack.



It's time for Security Now!.  Yes, the moment you've all been waiting for.  Once a week, everybody gets...



STEVE GIBSON:  Those of you who are listening may have been waiting, yes.



LEO:  That's Steve Gibson of the GRC Corporation.  Hi, Steve.  Good to see you again.



STEVE:  Leo.



LEO:  Sorry I missed last week.



STEVE:  Great to be with you again.  Jason did a nice job of MC'ing last week, and I guess we're going to have him through most of September.



LEO:  Yeah, that's right, because we're leaving for a vacation at the end of the month.



STEVE:  Yeah, I've been noticing you recording extra content for the Sunday show.



LEO:  The Tech Guy, yeah, yeah.



STEVE:  In order to fill in.  So, cool.



LEO:  Catching up, getting ahead, I guess.  So thank you, Jason.  And here I am.  I'm back.  Did anything happen while I was gone?



STEVE:  Yeah.  Bluetooth got hacked.



LEO:  I heard about that, yeah.



STEVE:  But aside from that.  Oh, and I did want to mention that I had a nice conversation with Stina.



LEO:  I was worried about her.



STEVE:  Yeah, and not without reason.  Our conversation was off the record.  And she had said that she was going to be meeting with Google at the end of last week and then would have an update for me, and they're still sort of trying to determine what Yubico's official stance should be.  And I wasn't ready to assume that this was done without their knowledge.



LEO:  Let me give people the back story.  I know you talked about it last week.  But if people didn't listen, Google - and it was actually really good news - announced that 85,000 of their employees have been provided with YubiKeys, those hardware-based one-time password, used for two-factor authentication on Google accounts.  And in the year and a half or two years they've been using them, zero successful phishing attacks.  It was very effective.  They followed that announcement, good news, Stina was probably jumping up and down for that, with the announcement that they were going to do their own hardware key, the Titan.



STEVE:  Yup.



LEO:  And that's what I was worried about.  Now, I have to point out that Titan is not only a hardware key that you plug in the USB port, but it's got Bluetooth in it.  And with this news about Bluetooth snarfing, I'm wondering if that's a good thing.



STEVE:  Well, the other thing to wonder is whether having the secrets installed in it in China is a good thing.



LEO:  Ooh, I didn't hear that.



STEVE:  Yeah, they - yeah, yeah.



LEO:  Hmm.  They're making it in China.  Hmm.



STEVE:  Yeah, by the low bidder.  So anyway...



LEO:  So Stina is - and you met her at an RSA conference some years ago, just by chance.  She was going up the down escalator, and you ran into her.



STEVE:  Yeah, well, it was interesting.  She said to me when we began our conversation last Monday, she said it had been 10 years.  So it was 10 years ago.  I was there with Security Now! press credentials, just covering the conference.  And on the third day I'd seen everything and looked through all the booths and attended a bunch of the various presentations.  And everyone was tired, and I was just sort of - I remember I was, like, walking toward the escalators on the upper level of the convention center, and there was this attractive blond Swedish woman with badges and stuff.  And I was, what, so this was - oh, and Elaine told me, by the way, I had lost a year in where we are with the podcast.  This is not Year 12 that we're finishing.  This is Year 13 that we're finishing.



LEO:  Right, right, right, right.  We just finished Year 12.



STEVE:  Correct.  No, we're...



LEO:  We're just starting 14.  That's right, yeah.



STEVE:  Yes, will be.  I think it's like mid-August, like in a couple weeks.  Anyway, so we'd been doing the podcast, you and I, for about four years.



LEO:  It was Security Now! 143, I'm told, that you told this story.  In fact, it's called "YubiKey."



STEVE:  Yes.  Well, and so what happened was - you know, so you and I had been doing the podcast for, like, four years.  And we'd been talking about the eBay, the PayPal football, and time-based one-time passwords, and all of this.  I mean, obviously, authentication and multifactor stuff is crucial.



So she says to me, are you interested in authentication?  And, I mean, she had no idea who I was, just I'm some random guy with press credentials hanging around my neck.  And so I said yeah.  And so she says - she, like, pulls this thing out of her pocket, and it was clearly a USB - it had the four USB fingers, and it was clearly a USB plug.  She said, "This is a one-time password token that emulates a keyboard."  And I just went <gasp>.  I mean, because I got it.  And it was like, oh, my god, it was just so clever, the idea that by emulating a keyboard you needed no drivers, and you could just plug this into the computer.  It got power from the computer, so it didn't need to have a battery.  And when you touched the little touch-sensitive metal button on it, it would spit out a string of characters.



LEO:  I think the original one didn't even have - it wasn't a - wrote to anything.  It was just one string, wasn't it?



STEVE:  No, no.



LEO:  It changed every time.



STEVE:  It did have a...



LEO:  Oh, okay.  It does now, anyway.



STEVE:  ...little bit of permanent memory.



LEO:  Right.



STEVE:  The beginning chunk of the string was static, which identified you.  And then the rest of it changed every time.  So you had to stay - and it was counter-based, not time-based.



LEO:  That's right.  That's right.



STEVE:  So it had to stay in sync with an authentication server.  Anyway, so it took her a while to believe that some random press guy instantly understood what they had.  But of course I did because I'd been living in this space for quite a while.  Anyway, and so it was three weeks later - because I had to do a lot of RSA news after the conference.  We did several podcasts on what I saw and what was announced and so forth during the RSA convention.  And then I said, "And now let's talk about something that is really cool."



Anyway, that was 10 years ago.  So I think the sad thing is that Stina and Yubico really have been prime movers of this whole area.  I mean, I know a lot of behind-the-scenes stuff about FIDO and UAF and U2F and what's been going on and what a political mess it has been.  So for 10 years she's been pushing this.  And they moved to Silicon Valley to be in the Valley, and Google has been one of her major accomplishments.  And politics.



So anyway, I looked on Yubico's site for any updated response.  I don't know if there will be one.  I didn't see any yesterday.  But anyway, for what it's worth, it'll be interesting to see how this plays out.  I think Yubico...



LEO:  And it sounds like we're going to continue to support Yubico as the right choice; right?



STEVE:  Yes.  Yes.  And certainly there are many organizations that won't choose Google because they're Google.  I mean, you know, many people will.  But in the same way, for example, that you can't buy a Kindle book on the iOS app for Amazon, it's like, which is so dumb, so you have to go to the browser and browse to the Amazon website to buy a Kindle book because Apple wants to make iBooks, not Kindle books.  And so there are, you know, we see all kinds of places where corporations have their own political issues with each other.  So Yubico is still, I mean, my absolute goto solution for this and for other future things that may be coming down the pike.



So anyway, this week 675 is our episode number for the first Tuesday of the month, which is the latest Tuesday we can have.  So second Tuesday of the month, which will be Patch Tuesday next week, will be the 14th, which is as far back in the month as it's possible to have that happen.  So I'm sure we'll have stuff to talk about then.



In this case, there is a new attack on WiFi passwords which occurred to the author of Hashcat, which is the very cool high-performance hash-cracking tool that is able to enlist the aid of GPUs and gets really high hash-based cracking rates.  He was poking around at WPA3, the forthcoming next version of WPA.  Everyone knows we're at WPA2 now, the beleaguered WiFi security specification.  And we were talking about WPA3 a few weeks ago, that, well, eventually it'll come out.  I was excited about it because I got teased by the fact that it looked like the specs were online, but all they were was like the Table of Contents.  And it's like, na na na.  It's like, okay, fine.  So I don't get to take a look at it yet.



But the problem is that, unless all of the existing devices are retrofit, then it doesn't do us a lot of good, even when WPA3 certified routers eventually occur.  It isn't clear that WPA2 routers will be able to upgrade their firmware to WPA3.  I don't know one way or the other.  But the Wi-Fi Alliance is all about certification and stamps and trademarks and nonsense.  There should be none of that in a protocol as important to the world as wireless Ethernet access.  But it is.  That's where we are.



But even if routers could get updated, or when they're eventually replaced, well, all the other devices need to be updated, too.  And it's not clear when IoT light bulbs are going to get themselves updated.  So we're stuck with WPA2 for the foreseeable future.  What the author of Hashcat tripped over, I mean, like this has been sitting here for years and no one noticed it, is a significant advancement in I guess we'd call it the state of the art in WiFi network password cracking, which makes it enough easier that everyone's going to be doing it.



LEO:  Oh, boy.



STEVE:  So, yeah.  That's the topic of today's podcast.  But we've got a lot more to talk about.  We've got another new and diabolical router hack and attack.  Reddit's discovery that SMS-based two-factor authentication is insufficient.  Oh, who knew?  Well, of course all of our listeners knew.  WannaCry refuses to die and knocked a major, well, actually, the largest company in Taiwan off of its manufacturing cycle for a weekend.  We have an interesting piece of research, a study that was done about law enforcement's use of forensic resources that has some surprises we'll talk about.  And Bruce Schneier weighed in on that, as well.  A new and very clever BGP-based attack.  We were talking about how that Portuguese ISP was finally frozen off the Internet after years of selling BGP chunks of IPv4 space they didn't own to spammers.  This is different and diabolical, again.  These attacks are getting more clever.



An interesting survey was done.  Lawrence Abrams, who's the founder of Bleeping Computer, has a good friend who's a longstanding MVP of Microsoft, who did a survey, took a survey of Windows 10 Update actually sort of dissatisfaction.  A snap from Bleeping Computer's coverage is the Picture of the Week, and we'll talk about that just briefly because, as a consequence of this, it bears on us because people are disabling it, which of course ends up having a security consequence. 



LEO:  Oh, boy.  Yeah.



STEVE:  Google has advanced their state-sponsored attack notifications in an interesting way.  Also The Intercept got wind of Google's new Project Dragonfly, which I'm sure you and your crew will be talking about tomorrow with Jeff and Stacey.  We have a highly targeted and amazingly powerful ransomware campaign, some closing-the-loop feedback from our listeners, and then we're going to talk about essentially something that's always been possible, nobody noticed, in the existing WPA2 protocol, not WPA3.  The Hashcat guy acknowledges that, if it ever happens, they finally really did solve this problem, probably.  But it's going to be a long time coming.  And in the meantime, hacking WPA, you know, WiFi access point passwords just got a lot easier.



LEO:  Oh, boy.



STEVE:  So I think another great podcast.



LEO:  <Growling>  Geez.  Well, I can't wait to hear how much easier.  That's going to be germane to the conversation.



STEVE:  Yeah.  So our Picture of the Week ties into the story we'll get to a little bit later.  So we could come back to it, but it's probably not surprising to any of our listeners.  The numbers are interesting.  It would be nice to imagine that, I mean, Microsoft must be aware of this.  I imagine you'll be talking with Paul and Mary Jo about this tomorrow also, Leo, because it was an interesting story where this Microsoft MVP took a survey of I think 1,800 people, a thousand in enterprise and 800 users.  And nobody was real happy with the outcome of what Microsoft was doing.  For example...



LEO:  This is about Windows Update.



STEVE:  Exactly.



LEO:  And they're just not very satisfied with the experience.



STEVE:  Exactly.  Well, and for example, I guess, and I wasn't aware of this, fortunately, because I'm not in Windows 10 world, I guess last month, and you probably know because of Windows Weekly with Paul and Mary Jo, I guess July was really rough for side effects and problems with Windows 10 updates.



LEO:  Yeah.  They pushed out 1803, and I think that was the biggest problem.



STEVE:  Well, and so, for example, satisfaction with Microsoft patching, very much not - it's ranked from one through five.  Very much not satisfied was almost 32%; not satisfied, 37%; neutral, 13%; somewhat satisfied, 16%; and very satisfied was 2.28%.  So satisfaction with the quality of the updates pretty much follows the same pattern, with very satisfied at 3.54%, everybody else very much not satisfied, not, or neutral, or somewhat.  Useful to my business:  35 not useful at all, 35 rarely useful, 19 neutral, 10% somewhat useful, and only 1.96% said extremely useful.  And the last question was on the issue of feature releases.  40% said once every two years.  Another 40% said once every year.  So there's a total of 80% of the respondents wanted it no more than once a year, and half of them every two years.  8.89% had no opinion.  Two times a year is fine said 11.2%.  And then somewhere there's 1.42% that wants them even more often.



So anyway, we'll loop back around to this a little bit just when I talk about what Larry and Susan found.  But in the meantime we have, you know, I guess we sort of seem to go through themes or phases or periods with the podcast.  We are now in the hacking publicly facing routers phase of the industry as reflected by this podcast.  Trustwave had a blog posting.  SpiderLabs is Trustwave's security side.  A Simon Kenin did a blog posting where, coming back from the Asian RSA conference recently, something jumped on his radar.  He wrote in the first person.



He said:  "On July 31st, just after getting back to the office from my talk at RSA Asia 2018 about how cybercriminals use cryptocurrencies for their malicious activities," he says, "I noticed a huge surge of Coinhive in Brazil."  Okay, now, remember, Coinhive is the server which serves the browser-based, and now we know it's based on the native assembly code for browsers, so it's browser based, relatively high speed, as much as you can get in a browser, cryptocurrency mining.  They mine Monero currency.  And we've been talking about them for months because of course bad guys have been arranging to get unwitting - either apps on Android have been downloading cryptocurrency miners, or they've been injecting them into browsers.



Coinhive, remember, it's a little bit shady because we know that there's no way they don't know their service is being abused.  And also remember they're getting a fair chunk of coin themselves for everything that's being mined because essentially they take a large piece of the action.  So they've sort of barely skated along.  Their domain has been blacklisted.  Third-party anti-mining plug-ins exist now to block them.  So, I mean, I don't think they're even in the gray zone.  They're in the dark gray zone of offering a service.  



So anyway, so he notices a huge surge of Coinhive in Brazil.  He writes:  "After a quick look, I saw that this is not your average garden variety website compromise, but that these were all MikroTik" - and I'm not going to pronounce it "microtic" this time, though I'm a little tempted to, as we'll see here in a minute.  They were all MikroTik network devices.



He writes:  "This could be a bizarre coincidence, but on further inspection I saw that all of these devices were using the same Coinhive site key, meaning that they all ultimately mine into the hands of one entity."  He wrote:  "I looked for the Coinhive site key used on those devices and saw that the attacker indeed mainly focused on Brazil.  My first thought was that on such a large scale that could be a zero-day exploit, possibly in the MikroTik HttpProxy component, so my next step was to check whether anyone else also noticed this, since during the conference I had limited time and Internet access to keep up with daily news."  So he sort of was coming into this, he felt, late and thought, well, maybe this has been going on for a while, and somebody else has mentioned it.



He says:  "Google didn't produce many results, but the few that did come up were actually quite useful in helping me pinpoint the attack vector and what the attacker did.  For example, this result shows injection of Coinhive on a hospital website in Brazil.  However, this web server runs Apache, which contradicted my initial thought of an exploit directly on MikroTik HttpProxy."



He says:  "After doing some querying on Shodan, I actually found the hospital's MikroTik device, so perhaps it is an issue with MikroTik, but not necessarily with the HttpProxy.  So there I was," he writes, "back at square one with a huge surge of Coinhive hits in Brazil, but no idea where and how it originated, and back to my Google results.  I went to see what else they had to offer."  He says:  "I found a post on Reddit from someone who was repeatedly being infected with Coinhive mining."



Anyway, so I'll stop quoting him there and just say that what he found ultimately was that this attack started in Brazil.  It has since spread globally.  More than 200,000 MikroTik routers are infected.  The bad guys are using, not a zero day, as one might expect, but this is what we've been talking about now for quite a while is that what I guess the hacker community has awoken to over the last few months is that routers are not being patched.  And so old exploits which have long since had patches available are virtually never being deployed, never being installed.



And so it is worth looking for instances of publicly facing routers still that are remaining vulnerable years after the exploit was published.  Well, in this case, it isn't years.  But on April 22nd MikroTik was informed of a remote authentication bypass exploit on virtually all of their routers.  And to their credit, one day later they had a patch.  So April 23rd of this year, many months ago, this was fixed.  Did it matter?  No.



LEO:  Of course not, because you can't patch - they're not auto patching.  You have to know.



STEVE:  Right, exactly.



LEO:  That's the problem with these older routers.  You know, when MikroTik had its first problem, we didn't know how to pronounce MikroTik.  We've learned now.



STEVE:  Right, right.



LEO:  Quite a good opportunity to learn.



STEVE:  So today, hundreds of thousands of these MikroTik routers remain vulnerable.  Now, in this case, MikroTik offers a custom UI which they call "Winbox," which is a Windows-based management interface which the MikroTik routers offer.  I only thought of MikroTik routers as little consumer devices because I remember when I was in love with that amazingly inexpensive...



LEO:  The Ubiquiti?  Yeah, the EdgeRouter X?



STEVE:  Thank you, the Ubiquiti EdgeRouter X.  Several of our listeners said, hey, what about MikroTik?  And sure enough, MikroTik has some little routers.  Turns out they also make what I would be tempted to call "big iron" routers.  I mean, they make routers that ISPs use.



LEO:  They're Latvian.



STEVE:  Yes, exactly.



LEO:  They're from Latvia.



STEVE:  They are.



LEO:  Well, there you go.



STEVE:  Okay.  So get this.  Here's the new diabolical bit.  Rather than running the Coinhive miner on the router itself, which is how the exploit was apparently first operating when it was discovered, the attacker has since started using the router's intermediate position on any network where it's located.  After all, I mean, it is going to be on the boundary between the WAN and the LAN.  That's how the routers are being deployed.  It is now dynamically injecting live Coinhive script into every web page that a user visits.



LEO:  Oh, lord.  Oh, my god.



STEVE:  And it's bidirectional.  Not only will it inject Coinhive script into routers behind, like on the LAN behind the router, but in the case of servers running that have public-facing websites, it's injecting live Coinhive script onto the customer's router, that is, onto the customer's browser, I mean, onto the visitor's browser.  So people out on the Internet who are going to a website hosted by someone whose ISP has a MikroTik router on its edge is getting infected with Coinhive.



LEO:  Now, does that happen if it's an HTTPS site?  I mean, wouldn't that prevent that?



STEVE:  No, no, and that's exactly right.  So HTTPS cannot get intercepted.  And this is what was a surprise to me, and again another - that's a perfect question, Leo.  The attacker was apparently worried that it would be too obvious if all pages were being intercepted.  And maybe it's because of the inability to intercept HTTPS.  So it turns out they're intercepting error pages returned to the router's users.  And those error pages are probably not HTTPS.  So this guy observed that, for example, like a 404 Page Not Found, or a DNS lookup error from an ISP, those are probably not HTTPS.  So they are nonencrypted page opportunities to run the script on people's browsers.  I don't think the script's going to run very long.  No one's going to sit there with an error page on their web browser for, like, hours at a time.  But that's how this is happening.



And he finishes his posting.  Simon says:  "Let me emphasize how bad this attack is.  The attacker wisely thought that, instead of infecting small sites with few visitors, or finding sophisticated ways to run malware on end-user computers, they would go straight to the source:  carrier-grade router devices."  In other words, MikroTik is sourcing ISP devices.  He says:  "There are hundreds of thousands of these devices around the globe, in use by ISPs and different organizations and businesses.  Each device serves at least tens, if not hundreds, of users daily."



LEO:  Ooh.



STEVE:  So by being an intermediate Coinhive injector, it's able to get broad visibility for its script.  He says:  "Allegedly" - not quite English - "each user would have initially gotten the Coinhive script regardless which site they visited.  Even if this attack only works on pages that return errors" - and I think that's probably because those are not HTTPS - he says, "we're still talking about potentially millions of daily pages for the attacker."  Although I would argue probably not each running for very long.



So anyway, just once again, here we have, as you said, Leo, as we've been saying, these consumer routers are not self-updating.  And that behavior has to change.



LEO:  Got to change.  Well, I have to say it is mostly changing.  All the good new gear does.



STEVE:  Yes.



LEO:  Thank god.  And you shouldn't even consider one that doesn't because you're not going to update it.  How often do you check?



STEVE:  Yeah.  I mean, how would you know?



LEO:  Right.



STEVE:  I mean, yeah.  You normally just get the thing from Amazon or from your local retailer and plug it in, and you forget about it.



LEO:  Right.



STEVE:  And you don't register it.  You don't want to give anybody else your email address because it's just more opportunity for spam.  And so it's like, okay, fine.  So they can't contact you.  The router itself needs to phone home.  And part of the agreement needs to be that it may take itself offline for 10 minutes at 3:00 a.m., when nothing is going on, in order to update itself.



LEO:  It's also why - and this is bothering people, but I don't mind it - you're starting to see companies like Eero and Plume ask in the former case and demand in the latter case a subscription fee.  You're paying an ongoing $90 a year subscription fee.  And part of what you get for that is updated services and patches.  And I don't think that's unfair, I really don't.  I think that that's part of the guarantee.  Although, oh, and also remember when we were talking about HTTPS Everywhere, and I was a little miffed at Google because they were pushing this so hard?  Well, now, that's the answer to the question, why do you have to do this?  That's exactly the answer.  HTTPS Everywhere would prevent this.  At least this one, anyway.



STEVE:  Yeah.  So Reddit got hacked, despite having two-factor authentication in place.  They posted:  "We had a security incident.  Here's what you need to know."  Which was their posting.  And they said:  "What happened?  On June 19th we learned that, between June 14 and June 18, an attacker compromised a few of our employees' accounts with our cloud and source code hosting providers.  Already having our primary access points for code and infrastructure behind strong authentication requiring two-factor authentication, we learned that SMS-based authentication is not nearly as secure as we would hope, and the main attack was via SMS intercept.  We point this out to encourage everyone here to move to token-based two-factor authentication."  And of course this is something that we've been talking about for some time.



I'll just finish a little bit more, saying:  "Although this was a serious attack, the attacker did not gain write access to Reddit systems; they gained read-only access to some systems that contained backup data, source code, and other logs.  They were not able to alter Reddit information, and we have taken steps since the event to further lock down and rotate all production secrets and API keys, and to enhance our logging and monitoring systems."



So anyway, I heard you, I guess it was - I don't think it was last weekend, but maybe the weekend before, or during the week, you were talking a lot about Yubico and YubiKey and Google's Titan stuff.  And I thought you made the very, very good point, which is that the problem even - first of all, we completely agree that SMS-based two-factor authentication is a problem.  I think it was when I was setting up my Hover account.  Several years ago I was wanting to migrate away from Network Solutions finally because they had become so evil.  I chose Hover as my next registrar.  They offer two-factor authentication.  And they gave me a choice:  SMS or time-based.



And we had been talking about how insecure the cell phone system, the cellular network fundamentally is.  And I knew that the advantage of a time-based token is that only one time I needed to have a secret pass between them and me.  And this is where I was then promoting the idea of printing the QR code right there and then, when it's shown to me, just print it.  So I have it on paper, and that way I'm able to easily clone my time-based one-time password to other devices.



Anyway, so that was years ago.  And we talked then about the reason that you want time-based six-digit changing tokens, not per-use.  And so that's what bit these guys.  Anyway, when you were talking about it, Leo, you observed, and I appreciated that you had, the fact that, unfortunately, even, I mean, every single time, everywhere you see the opportunity to use multifactor authentication...



LEO:  It's a fallback.



STEVE:  ...is still, yes, oh, I don't have it with me.  Or, oh, I left it at home.  Or my dog ate it.  Or whatever.  And it's like, okay.  So we're back to square one.



LEO:  It's the lowest common denominator.  Whatever the, you know.



STEVE:  Exactly.



LEO:  I respect Google because Google does have a fallback if you're not going to use your YubiKey. But it's the authenticator.  And that may be a setting.  I can't remember.  Maybe you could say don't use SMS.  But, yeah, whenever possible, try to avoid that.  And my bank, every bank, SMS is always an option.  And, I mean, to be fair, it is a little more secure; right?  I mean a lot more secure because it's not - it's a nontrivial thing to steal somebody's SMS number.



STEVE:  Absolutely.  Well, and the advantage of it is it requires zero user preparation.



LEO:  Right.



STEVE:  So it's like, oh, my bank just sent me this code.  Fine.  And so you just enter it in.  And so, I mean, the problem is that usernames and passwords also require zero user preparation.  And look at what a disaster that is.  So the problem with time-based is you then need - you need to have an app running on something that's able...



LEO:  You've got to prepare, yeah.  You've got to set it up.



STEVE:  Exactly, requires some preparation.  And I'll just add a little plug for SQRL here that we'll be talking about in the future.  SQRL already in the protocol has two settings in it where you can, once you become comfortable with how the system works, and you understand what's going on, you can turn on its request to every site you visit to disable all recovery options.  And you can also turn on its request to disable all non-SQRL authentication.  So there are two different settings.



LEO:  Hmm, I like that one.



STEVE:  And there are flags that it just - it just sends the flags out whenever you use it.  And so it's up to the web server, it's up to the site to decide if they wish to honor it.  But it indicates the user's request that essentially says, "I understand what I'm doing.  I don't want recourse.  No recourse."  And so, as we know, it's the only way you can really have security.  And those flags are off by default because I don't want anyone to hurt themselves.  But it's built into the protocol.  You can say, you know, "I'm serious about my security.  I'm taking responsibility for it.  Cancel all recourse."  And that's safe to do because the SQRL system itself provides lots of recourse to prevent you from getting yourselves in trouble.



LEO:  Very nice.  Very nice.



STEVE:  Yeah.  Anyway, we talked about Stina.  We talked about Google's Titan keys.  Stina had said she was going to get back to me prior to this podcast, but I didn't bug her.  She was in New York when I talked to her last Monday, and then she was going to be - this came as a surprise to them.  So I think they're going to be fine.  I think that there's a huge non-Google world, and Yubico is the company you want to use.  They're really staying with it.  There are some things I can't talk about yet, but they've got a bright future.



LEO:  Shhh.  Good, good.



STEVE:  Yeah.



LEO:  On we go.



STEVE:  So WannaCry...



LEO:  Oh, man.



STEVE:  ...refuses to die.



LEO:  This is the last year version, too.



STEVE:  Gosh, yeah.  So as we'll remember, WannaCry was the weaponization of a flaw in Version 2 of Windows SMB, the Server Message Block, also known as Windows file-and-printer sharing, that was believed - well, we know that it was leaked by the Shadow Brokers.  We believe it was an internal NSA exploit originally named EternalBlue.  So EternalBlue was taken and weaponized as WannaCry, which was a very, very destructive crypto malware.



LEO:  I think EternalBlue was the thing that let it spread over the network; right?  That was the...



STEVE:  The SMB, exactly.  It was the propagation mechanism.



LEO:  Here's my T-shirt.  I tell you, if you go up to a girl, and you say "ITPro.TV and Chill?" and she doesn't say yes, she's not the right one.  I'm just saying.  No, all right, okay.



STEVE:  The good news is that, with that T-shirt, she can really see you coming.



LEO:  It's a little red.  All right.  Sorry.



STEVE:  So it struck again over the weekend.  It's expected to shave a quarter billion dollars from a major Taiwanese semiconductor manufacturer's revenue as a consequence of it bringing down, bringing to a halt, multiple semiconductor manufacturing fabrication facilities across Taiwan.  The company is cleverly named Taiwan Semiconductor Manufacturing Company.



LEO:  We just call it TSMC.



STEVE:  TSMC.  Not only is it a major supplier of parts for Apple, AMD, Nvidia, and Qualcomm, but it's Taiwan's largest company.  So big guys.



LEO:  I didn't know that.  Wow.  They make the A11 chip for the iPhones and iPads.



STEVE:  Right, right.  And so there was some concern that this would be a supply chain glitch for maybe Apple and others.  It's unclear.  I mean, it was relatively short.  I mean, they were able to get things back up.  But they wrote:  "This virus outbreak occurred due to misoperation during the software installation process for a new tool, which caused a virus to spread once the tool was connected to the company's computer network." 



So, yeah.  So it's taken a chunk out of their revenue, although they're a major company.  They're forecasting revenue in the quarter to be around - their original forecast was $8.45 billion.  No.  It was 8.55.  So it's expected to drop by at least 200 million to 8.45.  So anyway, so they're saying the revenue hit could be as high as 256 million, in other words a quarter billion, but still they're 8.5 billion, so they'll be okay.



But what's interesting is it's not clear how this happened because it didn't just spread.  It destroyed their systems.  It encrypted their systems.  And we'll remember that it was our friend of the podcast Marcus Hutchins, a.k.a. MalwareTech, who was looking at this immediately upon its appearance on the Internet and saw in the code that for some reason this WannaCry malware was checking for - it was making a DNS query, but the DNS domain that it was querying was unregistered.  So all of these queries were coming back as no such domain name.



So he thought, huh.  Wonder what that's about?  And he registered the domain name.  And immediately WannaCry stopped.  And so what we surmise is that this was a deliberate kill switch which was put into the malware by its authors as a means for them to shut it down if for some reason they wanted to.  Well, it's still in place.  And as we know, I coined the term Internet Background Radiation (IBR) to talk about the fact that many of these worms, Code Red and Nimda, still live on some systems; and they're out there pinging around, looking for new victims.  Similarly, WannaCry is still out looking for other systems to encrypt.  But its check for that DNS domain is succeeding, which prevents it from doing so.



So we don't know about the details of this Taiwanese manufacturer TSMC's internal network.  But it might be that they've got strict controls over what DNS queries are allowed to be made to public DNS servers, or they might have some sort of border protection that's preventing all but some explicitly whitelisted domains.  We don't know.  But what we do know based on the information we have is that, if this was WannaCry, it would not have done anything if it had been able to get resolution of that DNS.  So maybe it was a variation of WannaCry that has had the kill switch neutered so as not to use it any longer.  No one knows really what happened.  But anyway, that's the story is that this thing, WannaCry, is still delivering its destructive payload wherever it can, in at least this instance.



There was an interesting report from the Center for Strategic and International Studies titled "Low-Hanging Fruit:  Evidence-Based Solutions to the Digital Evidence Challenge."  And the reactions to this report have been many.  Essentially it turns out that, not surprisingly, James Comey's replacement, Christopher Wray, who's now the head of the FBI, is continuing to threaten legislative action if Silicon Valley companies do not offer up some means for responding to a search warrant.



He recently said:  "I think there should be room for compromise.  I don't want to characterize private conversations we're having with people in the industry.  We're not there yet for sure.  And if we can't get there, there may be other remedies, like legislation, that would have to come to bear."  He says:  "We're a country that has unbelievable innovation.  We put a man on the moon.  We have the power of flight.  We have autonomous vehicles.  The idea that we can't solve this problem as a society," he says, "I just don't buy it."



So Bruce Schneier weighed in on this.  Bruce's blog was titled:  "New Report on Police Digital Forensics Techniques."  And he refers to this report from the CSIS, the Center for Strategic & International Studies.  I've got a link to it.  He's got a link to it in his blog posting.  I have a link to his blog posting and also to the study.  But I'll just share the top of this because it gives it some context, and sort of it's enough.  So the study reads - this is the low-hanging fruit.



It says:  "Over the past year, we conducted a series of interviews with federal, state, and local law enforcement officials, attorneys, service providers, and civil society groups.  We also commissioned a survey of law enforcement officers from across the country to better understand the full range of difficulties they are facing in accessing and using digital evidence in their cases.  Survey results indicate that accessing data from service providers, much of which is not encrypted, is the biggest problem that law enforcement currently faces in leveraging digital evidence.



"This is a problem," they write, "that has not received adequate attention or resources to date.  An array of federal and state training centers, crime labs, and other efforts have arisen to help fill the gaps; but they are able to fill only a fraction of the need.  And there is no central entity responsible for monitoring these efforts, taking stock of the demand, and providing the assistance needed.  The key federal entity with an explicit mission to assist state and local law enforcement with their digital evidence needs is called the National Domestic Communications Assistance Center (NDCAC).  It has a budget of $11.4 million, spread among several different programs designed to distribute knowledge about service providers' policies and products, develop and share technical tools, and train law enforcement on new services and technologies, among other initiatives.



"In addition to bemoaning the lack of guidance and help from tech companies - a quarter of survey respondents said their top issue was convincing companies to hand over suspects' data - law enforcement officials also reported receiving barely any digital evidence training.  Local police said they'd received only 10 hours of training in the past 12 months, state police received 13, and federal officials received 16.  A plurality of respondents said they only received annual training.  Only 16% said their organizations scheduled training sessions at least twice per year."



So anyway, all of this essentially reduces to the FBI and other law enforcement agencies are annoyed that they are unable to get everything that they want.  That's what they're complaining about, this whole Internet going dark problem; while at the same time, based on this survey, they're not nearly making use of everything they can already have access to.  So I thought this was useful and important.  And it is generating a lot of attention.  And it's surprising people that essentially what we are continuing hearing is this drumbeat of we need absolute decryption of all information.  Yet it turns out that there is in fact a large body of evidence and digital information which is readily available, not encrypted, and not being taken advantage of.



So anyway, I just thought that was - Bruce essentially weighed in with the same position, that what's happening is law enforcement is not nearly taking advantage of what's available.  So maybe we need to worry a little bit less about, or they need to worry a little bit less about having decrypted access to everything until such point as they're taking advantage of what they do have access to.



This is, oh, very, very clever on the topic of BGP, the Border Gateway Protocol, and hijacking of routing.  What we're seeing is evidence now of border gateway protocol hijacking becoming targeted and weaponized.  We talked a couple weeks ago about Bitcanal, which was the Portuguese ISP who had for years been poisoning the BGP propagation system by advertising routes to a large number of little tiny /24, that is, 256 IP networks.  Just basically sort of nibbling off the edges of other people's larger IP allocations and commandeering the routes for those small bits of networks.



This works because routers will always route to the largest networks possible.  And so if you claim to own a little piece of network, then you end up being able to capture that traffic, even if somebody else is advertising a route that includes yours.  And this allowed them to then resell or lease those IP blocks to spammers, who were able to thus avoid spam blacklists.



Oracle did some coverage of this, almost a chilling, for its cleverness, new attack, a new style of attack.  They said:  "In April of 2018 we detailed a brazen BGP hijack of Amazon's authoritative DNS service in order to redirect users of a cryptocurrency wallet service to a fraudulent website ready to steal their money."  Okay.  So that's sort of the classic attack, where you attack BGP.



In this case, though, what's interesting is that they attacked the DNS, that is, they rerouted Amazon's authoritative DNS service to a spoofed DNS server, which would then offer up the wrong IP to people wanting to go to the cryptocurrency wallet service.  So that's sort of an indirection; that is, rather than directly rerouting the traffic to the wallet service, they commandeered the DNS that served the wallet service and rerouted that.  Well, that has some interesting implications that I'll explain in a second.



Then they said:  "In the past month we've observed additional BGP hijacks of authoritative DNS servers with a technique similar to what was used in April.  This time the targets included U.S. payment processing companies."  Specifically they found three different payment companies.  And I'll just finish with one last paragraph.  They said:  "As in the Amazon case, these more recent BGP hijacks enabled imposter DNS servers to return forged DNS responses, misleading unsuspecting users to malicious sites."  Here's where it gets very diabolical.  "By using long TTL values in the forged responses, recursive DNS servers held these bogus DNS entries in their caches long after the BGP hijack had disappeared, maximizing the duration of the attack."



Okay.  So that is what's so chillingly clever.  So, okay.  The authoritative DNS server is the actual anchor DNS for the service.  So, for example, in GRC's case I use Level 3's network.  And there are a pair of Level 3 servers that are authoritative for GRC.com, meaning that anyone out on the Internet asks their - well, first they ask their own computer, their own local workstation, for the name.  If it doesn't have it, then it asks the DNS server that it's configured to query.



Well, that is a recursive DNS server, meaning that the user asks it, and then that DNS server, which is what we're normally just used to referring to as like our ISP's DNS server, or Google, or OpenDNS, you know, one of those.  It's given the job to do the full resolution task, which is with DNS it can be a recursive process.  So when it ultimately gets the answer, since DNS is a caching system, part of the answer it gets - and it will ultimately contact the authoritative DNS server to obtain the DNS record.



In the example here, it would contact one of the two Level 3 servers and get the record that answers its question about GRC.com.  Part of the record is the caching information, how long I, as the admin of GRC.com, want the Internet to remember GRC's IP.  This is handy because longer TTLs, longer times to live for the cached information, lessens the burden on the authoritative DNS server.  But it means that, for example, if I wanted some agility of my IP, I wouldn't be able to change IPs quickly.  So one of the things that could be done, if I were planning to change my IP, is to reduce the DNS cache duration so that queries start coming more often.  And that way, when I do change my IP, the rest of the cached Internet will update with that new information more quickly.



Anyway, so what these guys have done is they've weaponized DNS caching by creating a spoofed DNS server which has the wrong, malicious IPs for a given, in this case, three different payment processing companies.  And super long cache times.  I can't remember now what the longest DNS cache is.  But, I mean, it can be many, many, many days.  So they set up these spoofed DNS servers; put very long cache times in the DNS.  And DNS, the recursive servers, will sometimes not obey very short caching.  As far as I know, they do obey very long caching.



So that means that just a comparatively brief BGP hijack that is redirecting traffic from the true authoritative DNS server to their spoofed authoritative DNS server, during the period of that hijack, any existing caches which expire out on the Internet in recursive DNS servers will ask for an update of the IP for the now spoofed payment service.  They will get the wrong IP with a super long cache value.



And notice it's not just the user who made that request.  But what's happened is it's poisoned the cache for the recursive DNS server that might be used by all the customers of a given ISP.  So it can have long-lasting, very broad and sweeping implications.  We're talking about a very clever weaponized attack that uses BGP as one of its parts, and then a long cached DNS record to put the cherry on top, or the icing on the cake.



So anyway, Oracle concluded their posting of this security news by saying:  "If previous attacks were shots across the bow" - that is, BGP attacks - "these incidents show the Internet infrastructure is now taking direct hits."  And they write:  "Unfortunately, there is no reason not to expect to see more of these types of attacks against the Internet in the future."  And this is an example of where DNS is being exploited where the only solution that we have for this exploitation is and will be the use of DNSSEC.  DNSSEC would securely sign the records, and there would not be a way in the instance of this kind of rerouting spoofing, for the spoofer to spoof the records, as long as the recipient knew that the site records were signed by DNSSEC.  And in fact the authoritative record for that domain would specify that in this case GRC.com, for example, that the GRC.com domain was supporting DNSSEC.



So as we also talked about recently, the progress toward DNSSEC stalled about five years ago and is not moving forward slowly.  I don't know how this ultimately happens.  It's just going to take time.  But until that happens, we're vulnerable to this kind of exploitation.



And I had here in the notes this comment about the issue of poor Windows 10 Update experiences.  I think we've pretty much covered it.  Lawrence Abrams of Bleeping Computer posted his note about Susan Bradley, who's a well-known Microsoft MVP.  She's been an MVP for 18 years, polled a whole bunch of people.  She put together an open letter to Microsoft executives Satya Nadella, Carlos Picoto, and Scott Guthrie about the frustration that Windows 10 users have when dealing with installing new updates and all of the problems that are known to happen.



Unfortunately, despite the fact that she's apparently well known, it doesn't look like her open letter got to anybody.  Lawrence posted two days later.  The first posting was on August 1st, last Wednesday.  And then two days later on Friday, on the 3rd, he posted a follow-up, essentially sharing the very disappointing reply.  Basically it looks like just a Microsoft email-reading functionary responded with boilerplate, saying, you know, thank you very much for your reply.  Windows 10 is unlike other Windows operating systems, blah blah blah.  I mean, it wasn't responsive in any way.  Doesn't look like it came to anyone's attention.



So anyway, I just thought it was interesting from a security standpoint because she said in her letter:  "It's due to increasing frustration with patching and patch management issues."  She says:  "I see consultants turning off updates completely as they see that as the only way they can have stable systems."  She wrote to them:  "I see server admins saying that Server 2016 is fine except when you want a system that doesn't reboot."



She says:  "I see Surface users get their machines cratered with 1803 side effects" - exactly as you were saying, Leo - "with SSD drives."  He wrote:  "Susan told me via email, 'I see more and more people say that waiting to install updates is what one must do.  All of these are unacceptable.  We can't continue on with the status quo.'"  So anyway, we'll keep an eye out, I'm sure, to see whether Microsoft responds to this because, as we know, it is burdensome for users; and, unfortunately, as a consequence of mistakes it's causing problems.



Back in 2012, so six years ago, Google began warning its Gmail users - and I wasn't aware of this until they made this change to G Suite, which is their enterprise level offering.  I wasn't aware that they had been alerting users of Gmail to state-sponsored attacks upon their account.



LEO:  Oh, yeah.  I've gotten them, yeah.



STEVE:  No kidding.  So like a red warning bar saying...



LEO:  Yeah.  Let me see if I can...



STEVE:  ...that your account is under attack, we believe from a state sponsor? 



LEO:  Yeah.



STEVE:  Wow.  Anyway, that's very cool.  Last week they announced that administrators of G Suite accounts would now be able to enable and configure a special alert when government-backed state espionage groups are trying to hack into one of their company's user's accounts.  So I just wanted to let our users know, any of our listeners who are G Suite admins or maybe employees of companies who are using G Suite, to consider turning that on.  I don't know if it's enabled by default.



But under the Admin Console > Reports > Manage Alerts there is a new option, Government-Based Attack, which can be enabled by the super admin of the account.  It allows the admin to be alerted.  Apparently the user's account under attack can automatically be locked and have the password account reset process initiated.  And the system is also able to alert the end user as well as the admin, optionally.  So it's very cool that Google is doing that, and not something that I was aware of.



And, well, this doesn't directly bear on security for us, although we've recently been talking a lot about the consequences of nation-level censorship:  Telegram fighting Russia we've talked about a lot; Apple moving its Chinese users' data to China and handing over control of the data center to China's primary government-owned telecom company; and we've also been talking about Chinese censorship.  What came to light recently from documents leaked to The Intercept was that Google is planning to make a comeback in China with a fully censored search engine offering through a project known as Dragonfly.



The Intercept said:  "Google is planning to launch a censored version of its search engine in China that will blacklist websites and search terms about human rights, democracy, religion, and peaceful protest," The Intercept wrote.  They said:  "The project, code-named Dragonfly, has been underway since spring of last year, and accelerated following a December 2017 meeting between Google's CEO Sundar Pichai and a top Chinese government official, according to internal Google documents and people familiar with the plans."



So I won't go any further, but just to say that, as we know, Google was censoring, what, from - for the last eight years they've been dark.  And for the two years prior they were censoring.  So from 2008 to 2010 they were doing some censoring.  Then they just decided to pull up roots altogether.  And now they're deciding to go back.  And again, as we know, ultimately I think, I mean, I understand people are upset about this.  People feel that it's Google capitulating to authoritarian dictator government.  But if you want to operate within a country, you need to abide by the country's requirements.  And I think that's what we're going to see as the Internet matures and sort of moves from being a startup experimental network to a mature global network forever.



And lastly, before we do our last break and then talk about this new way to hack WiFi, an interesting new approach for ransomware which has netted its hackers or attackers $6 million, which is a little bit surprising.  It is targeting attacks on large companies.



Sophos has been following these guys for quite some time.  They said:  "As the year 2016 began, a ransomware threat appeared that attacked its victims unlike any previous ransomware attack.  It's named SamSam after the filename of the earliest sample that we," they write, Sophos, "uncovered."  And they say:  "It uses a brutally minimalist manual approach to target and compromise victims.  The attacker or attackers use a variety of built-in Windows tools to escalate their own privileges, then scan the network for valuable targets.  They want credentials whose privileges will let them copy their ransomware payload to every machine - servers, endpoints, or whatever else they can get their hands on.



"Once in, the attackers spread a payload laterally through the network, a sleeper cell that lays in wait," they write, "for instructions to begin encrypting."  They say:  "Ever a predator, the attacker waits until late at night, when the target organization is least well equipped to deal with it, before the final blow is struck.  In a sneak attack while the target literally sleeps, SamSam encrypts a prioritized list of files and directories first, then everything else."  So, okay.  We know that in the past ransomware has been sort of a scattershot, opportunistic attack.  This is different.  This is establishing essentially an advanced persistent threat, but using ransomware as the attack vector.



They said:  "Unlike virtually every other ransomware attack, the entire attack process is manual, no badly worded spam email with an attachment.  The attacker breaks in the old-fashioned way, using tools that attempt as many logins as quickly as the Remote Desktop Protocol will permit" - so this leverages exposed RDP, which is something that I've also encountered in other reading recently - "and exploits operating system vulnerabilities, though not as many as you'd think.  SamSam usually succeeds when the victim chooses a weak, easily guessed password."



And they say:  "In this report" - and I've got a link, by the way, to the full report because I'm just going to cover that and their key findings - "we'll cover the anatomy of a SamSam attack, and why it isn't necessarily hard to defend against.  We also took a deep dive into the ransomware payload, tracing its evolution from an early beta through its (so far) third major revision, with no sign of a slowdown in sight, and an ever-increasing ransom demand with each subsequent attack.  Partnering with the crypto monitoring firm Neutrino, we traced the money trail and discovered far more victims and funds than had been previously reported."



Okay.  So key findings of this.  SamSam has earned its creators more than $5.9 million since 2015.  74% of the known victims are based in the U.S.  Other regions that are known to have suffered attacks are Canada and the U.K. and the Middle East.  The largest ransom paid by an individual victim so far was $64,000.  So this is not your one bitcoin attack, or your fractional bitcoin attack, because they are, by the way, they are using cryptocurrency in order to extract ransom.  This is $64,000 for an enterprise.



So what's interesting about this new model is that they're getting in and doing devastating damage to a significant enterprise.  And in return for that, they are getting major payouts.  And again, if the enterprise is fully protected against every nook and cranny being lost, then they're able to recover from their backups.  But if they're not, then you can well imagine that they're going to arrange to first get proof of decryptability, and then these bad guys are only asking for a single large payout.  So that changes the problem of small people not having data that's valuable, or having a backup for their own workstation, or being unable to deal with the hurdle of making a cryptocurrency payment.  So anyway, I think this is interestingly and diabolically clever.



Sophos also said that:  "Medium- to large-scale public organizations in healthcare, education, and government have been targeted by SamSam."  They said:  "Our research discovered that these only make up for about 50% of the total number of identified victims, with the rest comprising a private sector that has remained uncharacteristically quiet about the attacks."  Yeah, because no company wants to admit that they've had any kind of a penetration of their network.



"The attacker uses care in target selection, and attack preparation is meticulous.  SamSam waits for an opportune moment, typically launching the encryption commands in the middle of the night or the early hours of the morning of the victim's local time zone, when most users and admins would be asleep.  Unlike most other ransomware, SamSam encrypts not only document files, images, and other personal or work data, but also configuration and data files required to run applications, for example, Microsoft Office.  Victims whose backup strategy only protects the user's documents and files won't be able to recover a machine without reimaging it first."



They said:  "Every subsequent attack shows a progression in sophistication and an increasing awareness by the entity controlling SamSam of operational security.  The cost victims are charged in ransom has increased dramatically, and the tempo of attacks shows no sign of slowdown."  So this is ransomware attacks taken to the next level.  And again, sort of foreseeable if you had, okay, if we get into a network of a major corporation, and our goal is to get paid, how do we generate the largest possible incentive for cash?  In the past it's been doing damage like was done with Sony, with an advanced persistent threat.



But these guys want money.  And so doing reversible critical damage on a massive scale to an organization and then using cryptocurrency payment is the way you do that.  And that's exactly what Sophos has found with this SamSam attack.  Yikes.  You don't want to be the target of that.



LEO:  Unh-unh.



STEVE:  So I ran across a couple of, I thought, interesting closing-the-loop bits, feedback from our listeners.  Stuart in Seattle, his subject was "A Flash use case."  And he said:  "Long time listener/SpinRite user, et cetera."  Now, Flash, we're not talking about flash memory.  We're talking about the evil Adobe Flash interpreter that's been the source of so many problems.  He says:  "I keep hearing you slam Flash.  At work we have a use case for Flash, and as of now there is no workaround.  We use it to provide a dynamic UI in a PDF file" - oh, that's what you want is Flash in a PDF.  When does that ever cause anybody trouble?  Anyway, sorry.



LEO:  Double trouble.



STEVE:  Yeah.  "We use it to provide a dynamic UI in a PDF file that contains 3D work instructions.  It is simply not possible to do this using PDF forms constructs.  I agree that, for HTML content, Flash should go away.  Unfortunately, with a PDF file, there isn't an alternative at this time.  We are in a tricky spot of needing to come up with a different way of presenting the work instructions, then going through the expensive process of getting the new way certified for production."  So I'm not sure what certification Stuart's talking about.  But that's interesting.  Unfortunately, Flash embedded in PDFs is like the worst attack vector there is.  I mean, it's like one place Flash can still live and PDFs carry something malicious.



So, yikes.  It'll be interesting to see what happens.  I mean, I guess PDF support for Flash will go away.  Wow, yeah.  I don't know, Stuart.  Anyway, I just wanted to share that, that there are places where it's not easy to say no.  But ultimately, in a couple years, "no" is going to get said.



Blitz in Phoenix wrote:  "You can kill HTTP when you pry it from my cold dead hands."  He said:  "Hey, Steve."  And then he said in brackets:  "<insert Love the show, you guys are great, lots of praise/>."  He says:  "I want to bring up one specific scenario where I am actively uncomfortable with HTTPS."  And I kind of agree with him here.



"When you join a network, and it wants you to accept a terms of service or otherwise authenticate me via a web form.  When you go to a website, a captive portal intercepts the connection and returns a page for you to login/agree/whatever.  That's fine when the connection is straight up HTTP because you can 'man in the middle' an HTTP.  But when the connection is HTTPS, you get a certificate error.  Okay, so I could create an exception for that site and NOT store it permanently, but that feels wrong.  In addition" - and as you mentioned, Leo - "if everyone goes full bore HSTS" - that is, HTTPS Everywhere, essentially, HTTP Strict Transport Security is HSTS - "doesn't that mean you can no longer bypass a certificate error?  Also, as someone who has to do a lot of network-related troubleshooting, I greatly appreciate the ability to turn off SSL during packet captures as it makes it much easier to analyze."



He says:  "Oh, and for anyone who finds themselves hitting a captive portal, and they don't want to accept the certificate being presented, try http://neverssl.com."  He says:  "That site is so simple - pronounced 'elegant' - and straightforward in its design and explanation that I immediately suspected it could be a Steve Gibson creation."  Well, that's flattering, thank you.



He says:  "I've come to the conclusion that it is not one of yours" - no, it's not - "but primarily because I would have expected you to mention it at some point if you had created it."  He says:  "<insert Thanks for the show/SpinRite/Vitamin D info/>."  He says:  "Looking forward to how you handle the four-digit episode issue.  Instead of Y2K, would that be the E1K issue?"  Oh, Episode 1K.  Yeah, that's right, when we cross out of 999.  Anyway, signed off Blitz from Phoenix. 



LEO:  The E1K.



STEVE:  I'll just say that, yes, it is a problem when, I mean, this is like a problem more with Chrome.  Firefox is still making it a little bit easier to accept an exception.  But, boy, Chrome is getting really intolerant of non-HTTPS.  And there are places where just doing web management, when you're talking to your local router or you have a reason for needing to go to something that is not HTTPS, and they remain present, it is in fact becoming increasingly difficult to do that as everything moves to HTTPS.  So maybe we'll end up needing to have some access to routers that give us a way to do that, that we only use for network management purposes.



And then this was interesting, Leo.  Cory Bitney in Kalispell, Montana said:  "Rogue admin!  Help!"  And he said:  "Hey, Steve.  First of all, I'm a huge fan of Security Now! and SpinRite; so thank you for providing both.  The reason I am writing is I'm in a bit of a conundrum.  I just took over as systems administrator for a new company, replacing a sole admin that initially left the company, after seven years, on good terms."  Okay.  So I guess the company's not new.  He's now newly there, a new employee, just took over as systems admin for a new company, meaning, yes, his new employment, replacing a sole administrator that initially left the company after seven years on good terms.



"At the end of week two I found that he installed crypto mining software, MinerGate, on all of the workstations on the domain, as well as a few servers.  I told the owners, and they are concerned, but are hesitant to file criminal charges because they've known him for so long.  They at least finally gave me permission to do a full lockdown a few days ago.  And the more I dig, the more I find - backdoors, monitoring software, et cetera."



LEO:  Oh, boy.



STEVE:  "Basically everything in a sysadmin's nightmare."  And Cory says:  "He has definitely abused his power.  My question is, if the owners don't press charges, do you know of anything I can do to report him?  The company he went to work for is an ISP."



LEO:  Oh, that's really bad.  Oh.



STEVE:  "So it scares me to death what he might do with them.  To me, someone like this should never be trusted/have a career in IT again.  As Uncle Ben said, 'With great power comes great responsibility.'  Any advice would be greatly appreciated.  I know you're a busy person, and if I don't hear back it's understandable.  Either way, thanks again for all your contributions to our industry."  It is a conundrum.



LEO:  Wow, yeah.  How do you let everybody know?



STEVE:  Cory knows who this is.  The company where he is doesn't want to press charges.  First of all, I don't know how you secure his new employment, that is, his new employer.  I mean, as we know, Leo, when you mistakenly install something, you really can never be sure you got it out, and there aren't rootkits and things still hiding.  Imagine an admin who had full access to the infrastructure and clearly, I mean, that's just unconscionable what he did.  So first of all, he should, I mean, charges should be pressed.  But I also agree that Cory probably has an ethical responsibility to inform this person's new employer of what he found when he stepped in in this guy's wake.



So, yikes.  I mean, I'm not an attorney.  I don't have any advice.  But I'm glad he's giving it some hard thought.  So, wow.  And, I mean, there's no way that the ISP who has just hired this clown would not desperately want to know what this person did at his previous network of employment.  I mean, there's just no way.  So Cory, consider that.  There is no way his new employer should not know what you found that he had done, if you're really sure that he did this maliciously and deliberately.  There's just no way.  And, I mean, okay.  Even if it wasn't malicious and deliberate, he's a horrible IT admin.



LEO:  If nothing else.



STEVE:  If this was all done by mistake, then he still shouldn't be employed as an IT admin anywhere.  So anyway, thank you for sharing that with us.  And I'm glad to reshare it with our audience.



Okay.  So what happens when you are a capable white hat, well, maybe gray hat, no, okay, light gray hat hacker who designs the industry state-of-the-art high-speed hashing system known as Hashcat - which is able to, I mean, essentially it's like leveraging crypto mining hardware for the purpose of cracking passwords, that is, brute forcing the hash used to protect passwords - when this person decides he wants to look at WPA3, and in the process discovers that the protocol everybody is using today, WPA2, can be much more easily attacked than has been believed?



Okay.  And the only thing this needs is a good name, and it would be all over the place.  The one thing he didn't do was give it a good name.  So if you Google PMKID, that will show you that this attack is blowing up on the Internet, because the PMKID is the name given to the part of the protocol of WPA2 that's attacked.  So, okay.  So what this is is a new way of obtaining a small bit of information from the access point, an access point, which then allows you to attack its secret.



The traditional attack, the attack everybody knew about before last week, required an attacker to - and we've talked about this in the past - an attacker to monitor all of the traffic on an access point they wish to attack and capture a successful negotiation with a client to the access point.  So there is normally a four-packet handshake where somebody who knows the password is authenticating themselves to the access point.  So that's not a super high bar, but it requires something of the attacker.  They have to be there.  They have to be monitoring the traffic.  And while monitoring, somebody has to successfully authenticate a client to the access point using the secret, the password for the access point.



Thanks to this update, it is now none of that is necessary.  So what has happened is that's all gone.  It is now possible and now widely known that it is possible to attack the WPA2 protocol on any access point which supports a widely available technology for roaming.  What happens is, as part of the - there's something known as RSN IE, Robust Security Network Information Element, in the EAPoL frame.  That's a frame which is sent from the access point when it wants to explore reassociating.



Okay.  So the idea is that state-of-the-art access points, that is, 802.11i/p/q/r support roaming, the idea being that you're able to roam in and out of the influence of the access point.  When you come into that network, there is a silent negotiation between your client and the access point to see whether you have previously authenticated.  And, if so, your agreed-upon secret is cached at both ends.  So you are able to essentially silently reassociate yourself with the access point.  And we are all familiar with that happening all the time.



Well, it turns out the way that's done is by the access point sending a secret, which this guy who figured out how that could be abused could take advantage of it.  So anyway, what he wrote was:  "This attack was discovered accidentally while looking for new ways to attack the new WPA3 security standard.  WPA3," he writes, "will be much harder to attack because of its modern key establishment protocol called 'Simultaneous Authentication of Equals' (SAE)."  And that's what we talked about on this podcast a couple weeks ago.



He says:  "The main difference from existing attacks" - that is, compared to what he is describing - "is that in this attack capture of a full four-way handshake is not required.  The new attack is performed on the" - and here he describes the RSN IE - "the Robust Security Network Information Element of a single frame.  At this time, we do not know which vendors or for how many routers this technique will work.  But we think it will work against all 802.11i/p/q/r networks with roaming functions enabled," which he says is most modern routers.  And in the research I've done independently, they're all over the place.



There's a neat link here in the show notes, Leo, to that Medium.com.  Adam Toscher has a write-up where he's using readily off-the-shelf equipment and has a complete step-by-step how-to on implementing this attack.  And then the Hashcat guy goes on to explain in detail all the advantages of this.  It reduces to the following:  There is this PMKID is what is contained in this EAPoL packet.  It consists of an HMAC-SHA1 128.  So just an HMAC, as we know, uses two hashes.  But it's an SHA1, which has been highly accelerated by our current GPUs.



So the pre-master key, the PMK - in podcasts past we've described all of this in great detail.  The PMK, the pre-master key, is the secret which both ends of the connection have.  That is used to key the HMAC.  And then what is hashed by the HMAC is the access point's name, concatenated with the MAC address of the access point, concatenated with the MAC address of the station, that is, the client.  So of course we know the name of the access point.  We know the MAC address of the access point because that's in the Ethernet frame that it sends us.  And we know our own station MAC address.



So the point is that those three pieces of information are concatenated and hashed through this SHA1 HMAC with the pre-master key.  So the attack is time-consuming, but trivial, and essentially allows the reversal of or the determination of the pre-master key that the access point has cached and is exploring the use of with a client.  So it still means that we need to have - essentially you still need brute-force decrypting.  But I've seen quotes from a few hours to a day or so in order to perpetrate this crack.



So unfortunately there are access points that do a poor job of rotating the key that they're offered.  There are some that do a good job; many that do a bad job.  And there are attacker tools available that know which are which.  So you really, really, really need to use a secure password for your router.  This guy in some of his - I don't remember whether it was tweets or in his blog post.  He mentioned that what he uses to secure his own access points that he's in charge of is a pseudorandom string containing, I mean, like a high-quality password generated by a password generator, nothing memorable or in any way short-circuitable by brute-force cracking, something long and super high entropy so that it will minimize the chance of brute forcing.  But essentially this means that it is possible, for example, to do a drive-by attack where you capture any of the traffic from the access point as you're passing by and then can perform a relatively high-speed offline attack.



Again, the only protection here is a really high-entropy password for the access point.  And I would argue that, given the amount of attention this is getting, the fact that we are a long way away from WPA3, the fact that we've got password managers, all of the systems we use remember the passwords for our access points, there just is no excuse now not to use a password for a WiFi system that it is not possible to remember.  Generate it with a password manager.  Get it to your various devices once, tell them to save it, and then you never have to worry about it again.  It's increasingly important.  Well, and maybe not for random end users off in the woods somewhere, but especially for corporations, anyone where you have high-value access.  Because imagine somebody gets in, then they go and encrypt all of the files and servers and everything that your entire corporation has, and then ransom you for $65,000.  Not good.



LEO:  No.  No.  Not at all.



STEVE:  So WiFi cracking just got easier.  Someone determined could have still done this by camping out and watching authentication happen.  But given the amount of attention this is getting, I think we will see shortly some turnkey tools that make this, you know, someone's going to automate this and then turn lots of people loose to be just - and it's an offline attack.  So after you capture the traffic, you go home and turn off your bitcoin miner and have it do hash cracking instead.



LEO:  Wow.  So you do need to have physical access for a little while.



STEVE:  Well, you need to be in presence of - you have to have radio access.



LEO:  Right.



STEVE:  So, yeah.



LEO:  Sitting in a coffee shop, for instance.  How long does it typically take to crack the hash, though?



STEVE:  It is a function of the password.  So if the password is poor...



LEO:  Right.  Good password's a good idea.



STEVE:  Yes.  That's the only way to protect yourself is just use a crazy 64-character garbage, like something from GRC.com/passwords.  Take that, dump it in.  It's a pain to get everything converted once.  But once you do, I mean, how often are you really having to enter your WiFi password?  Shouldn't be that often.  Normally devices are able just to remember it.



LEO:  Right.  All right.  Guess I'm going to upgrade my passwords.



STEVE:  Worth doing because this is going to end up being turned into a turnkey attack, and people are just going to be doing it to play with it because they can.



LEO:  Yeah.  Well, thank you, Steve.  We do the show every Tuesday afternoon, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Every Thursday, did I say?  Every Tuesday.  I hope I said Tuesday.  Tuesday, 20:30 UTC.  Come by and watch at TWiT.tv/live.  Or you can even have a seat in the studio.  Just email tickets@twit.tv.  We'll put a place out for you.  If you watch live, or you're even in the studio live, you should join us in the chatroom, irc.twit.tv.



You need a download?  Well, we've got those, too.  If you cannot join us live, a download will be provided at GRC.com.  That's Steve's site.  While you're there, pick up his bread and butter, SpinRite, the world's best hard drive recovery and maintenance utility.  You can get a copy at GRC.com.  And then there's a lot of free stuff there, too, you've got to peruse.  It's one of those places you're just going to sink into it.  And three hours you'll come up for air and go, "What time is it?" because there's so much good stuff there, including the audio versions of this show and transcripts, too, by Elaine Farris, so you can read along as you listen.



We have audio and video at our website, TWiT.tv/sn.  And of course we've been around, as Steve says.  We're on our 14th year now.  That's really kind of amazing.  Wow.  Because of that, though, we're in every podcast application.  Just search for Security Now!, you'll find it, or search for TWiT.  Thank you, Steve.  Have a great week.  See you next week.



STEVE:  My friend, talk to you next week.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#676

DATE:		August 14, 2018

TITLE:		The Mega FaxSploit

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-676.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we cover lots of discoveries revealed during last week's Black Hat 2018 and DEF CON 26 Las Vegas security conferences, among them 47 vulnerabilities across 25 Android smartphones, Android "Disk-in-the-Middle" attacks, Google tracking when asked not to, more Brazilian D-Link router hijack hijinks, a backdoor found in VIA C3 processors, a trusted-client attack on WhatsApp, a macOS zero-day, a tasty new feature for Win10 Enterprise, a new Signal-based secure email service, Facebook's Fizz TLS v1.3 library, another Let's Encrypt milestone, and then "FaxSploit," the most significant nightmare in recent history - FAR worse, I think, than any of the theoretical Spectre and Meltdown attacks.



SHOW TEASE:  It's time for Security Now!, and it is our annual post-Black Hat/DEF CON episode.  That means there are exploits galore.  Macintosh, Android, and, yes, even your Hewlett-Packard printer.  Steve will cut through the chaff, tell you what exploits to worry about, what ones aren't so bad, and give you all the security news, too, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 676, recorded Tuesday, August 14th, 2018:  The Mega FaxSploit.



It's time for Security Now!, the show where we cover your security, privacy, safety, and well-being - that's the new word everybody's using, "well-being" - online.  Here's the Master of Well-Being, or MWB, Mr. Steven Gibson.  Hello, Steve.



STEVE GIBSON:  Yes, it used to be things needed to be user-friendly.



LEO:  Not anymore.



STEVE:  Now we want well-being.



LEO:  Yes.  That's what Google calls the things that make your phone less attractive in the middle of the night, well-being.



STEVE:  Really?



LEO:  Yeah.  Digital well-being.  And Apple and Google both are kind of all over that.  Because you know why, because people don't sleep as much as they used to because they're Snapchatting all night, or something.



STEVE:  Huh.



LEO:  It's not a problem you have, I'm sure.  



STEVE:  No, it's not.  So Leo.  We have arrived.



LEO:  What?  We're here?



STEVE:  We are here.  And this is not the penultimate, because I learned my lesson about that word.  This is the ultimate episode of our 13th year.



LEO:  Oh, you scared me.  I thought we were at 999.



STEVE:  No, no.



LEO:  Oh, we are completing our 13th year.



STEVE:  Yes.  The first podcast we did was 18 minutes long, in 2005, on August 19th.  And next Tuesday will be August 20th.  So we will lap ourselves by one day, meaning that that's the beginning of Year 14, and this is the last podcast, no tears to shed, of Year 13 of Security Now!.



LEO:  It did feel like an unlucky year.



STEVE:  It was, yeah, that's a very good point.  And I have to say, too, that, I mean, it's been a rough year.  And I know that you sort of laughed at this because you don't think people - I guess you don't assume that these fax combo scanner devices are in heavy use, or that people don't have fax machines.



LEO:  Oh, I saw this story about people getting hacked on their HP printers through the fax.  And I thought, well, who the heck has a fax machine hooked up to the phone?



STEVE:  So they're still very popular.  I mean, like I know of five or six businesses.  I have two myself.  And 100% of businesses in Japan; 45% of households in Japan.  Google lists 300 million fax numbers.



LEO:  Well, you know why it's big in Japan.  First of all, they invented the fax because they don't have Roman characters.  They have to handwrite their characters.  So it just makes sense to send faxes of handwritten notes.



STEVE:  Do you remember the Qwip?  That was the name of that first machine?  It was the Qwip.



LEO:  Yes, I do.



STEVE:  And it was a cylinder that you wrap - it, like, grabbed the paper along its long edge.



LEO:  Oh, there's a blast, yeah.



STEVE:  And it spun.  And then this head very slowly moved, sort of like - anyone who's used a fishing reel where the guide moves back and forth, well, this was the head very slowly moving down. And this image, it was amazing.  You could sit there and watch it, and it would be spinning, and very slowly.  I think it would have to have been a 300-baud modem because that's the technology we had at the time.  And you got, after about, I don't know, 10 minutes or so, I mean, it was not fast, kind of an image.



LEO:  This is how newspapers got pictures from the wire services was one of these, yeah.



STEVE:  Yes.  So we are the week after the, I mean, well, I'm at a loss for words.  This year's Black Hat 2018 and DEF CON 26 Las Vegas security conferences occurred last week.  Cornucopia, there we go, that's the word I was looking for.  It's just, whoa.  We've got 47 vulnerabilities across 25 Android smartphones, an Android "disk-in-the-middle" attack, Google tracking when asked not to, more Brazilian D-Link router hijack hijinks, a backdoor found in the VIA C3 processors, a trusted client attack on WhatsApp, a macOS zero day, a tasty new promised feature for Windows 10 Enterprise, a new Signal-based, Signal protocol-based secure email service, Facebook's release of their Fizz TLS v1.3 library into the open source community, and another Let's Encrypt milestone met.



And then we're going to talk about FaxSploit, which I think is the most significant nightmare in recent history - far worse, I would argue, than any of the theoretical Spectre and Meltdown attacks that we have talked about because of the ability for it to target specific organizations with a very potent attack over the phone.



LEO:  Wow.



STEVE:  So a good podcast.



LEO:  Over the phone.



STEVE:  Over the phone.



LEO:  Who would have thunk it?



STEVE:  Huh, what?  Yes.



LEO:  Do you want to do your Picture of the Week?  I don't want you to forget now.  Sometimes you forget, you know.



STEVE:  No, I'm not going to forget.  I know.  You're exactly right.  So this is a screenshot I took myself this morning when I did this because I have a combo scanner/fax/printer, a black-and-white one, and I also have a separate HP color printer that I sort of use depending upon what I'm printing.  And it was vulnerable, meaning that, had somebody known my personal fax number, and this will be what we cover in detail at the end of this podcast, it is possible for all HP devices, and probably other manufacturers, too.  But the attackers, the hackers who developed this only reverse engineered an HP printer.  It is possible to send a specially formatted fax which will take over the printer, install the NSA exploits, and to then take over the network to which that device is attached.



So if our listeners do nothing else - and I understand, maybe people have moved beyond fax altogether.  Maybe they've moved beyond a physical fax device.  I know that HP is selling them, and there's a bazillion models of them.  The point is it strikes me as a perfectly targetable attack.  Businesses have fax numbers like on their web pages.  And if they have an un-updated HP device answering that phone, they're vulnerable unless the device has been updated or has updated itself.



Interesting, after I posted the show notes, I got a tweet from Joe Levine who said: "Interestingly, my printer, an Office Jet Pro 6970, would not update from the downloaded software.  It turns out it had auto-updated itself sometime back and is now maximally up to date without my doing anything at all."  So it may be the case that HP devices, as all such unattended things should, have already taken care of themselves.  But you're going to want to make sure.



So the good news is, when I first encountered this I'm thinking, oh, how does one update some random device's firmware?  It could not be simpler.  And I've got the links in the notes.  And in fact I also tweeted the link earlier this morning, along with an urgent request for people to consider this.  You download an executable.  You run the EXE.  It finds the device on your network.  You say yes, and it updates it.  So you don't have to - there's not a log of rigamarole associated with it.  I know of a number of businesses I work with who also have fax machines, and this is the first time all year, for example, or for years, and in some cases ever, that I have written them an email saying "Please forward this to your IT department."



If it affects you, it's serious.  Because this is so - first of all, hackers are not going to be able to, like, not have fun with this.  All the details were presented during DEF CON last week, necessary for bad guys to retrace these guys' steps.  It was responsibly disclosed.  HP has dealt with it.  But we know, unless the printers are taking care of this themselves, and clearly some do, the devices are going to remain vulnerable.  So if nothing else, if you're affected by this, take it seriously.



We will come back around, and I'll talk about it in detail at the end of the podcast.  But this is the dialogue box, our Picture of the Week, that I saw this morning when my system updated.  So I was happy to have that done.  And it turns out my office device had been - it was an HP that got kind of flaky after many, many years of service.  So we switched to a non-HP device, and this morning that was removed from our network.  It actually hadn't been used.  Sue was still using the HP for the printer function, I think.  But anyway, it's now off of that network because at this point, as far as I know, all non-HP devices are vulnerable, as well.  And I'll talk about more of that.



LEO:  Interesting.  So if you have a printer with a phone line plugged in, and you don't use it for faxing, unplug the phone line.  That would eliminate this problem.



STEVE:  Yes.  Yes, yes, yes.



LEO:  And then just plug it in when you need to fax.



STEVE:  Yes.  And of course this is, again, this is our              constant refrain.  It is a massive interpreter in the fax machine which interprets the fax command system.  And as we know, interpreters are notoriously problematic.  And this is a very, very cool hack, so we'll talk about that at the end.



So in the meantime, during last week's Las Vegas DEF CON security conference, researchers with the U.S. mobile and IoT security firm Kryptowire, spelled with a "K," revealed findings from their research which was conducted as part of a grant awarded by our U.S. Department of Homeland Security.  They presented the details of 47 vulnerabilities in the firmware and the default installed apps of 25 different Android smartphone models, 11 of which have a presence in the U.S. market.  And this is one of those distressing cases where the vulnerabilities are literally too numerous to tick off one by one.  So I've included a link to Kryptowire's announcement in the show notes for anyone who's interested.



Anyway, the vulnerabilities discovered on the devices, which were offered by major U.S. carriers, include arbitrary command execution as the system, that is, as root, obtaining the modem logs and Logcat logs, which is a feature in Android, this Logcat, wiping all user data from the device, which is essentially performing a factory reset, reading and modifying a user's text messages, sending arbitrary text messages, getting the phone numbers of the user's contacts, and more.  And of course all of these fall outside of the formal Android permission model.  The major brands affected were Alcatel, Asus, LG, Nokia, Sony, and ZTE.  And then there are lesser brands, some I've never heard of, but Coolpad, Doogee, Essential, Leagoo, MXQ, Oppo, Orbic, Plum, SKY, and Vivo.



And so rather than getting into the details - oh, you'll notice that Google is not present in the list.  And looking at this range of vulnerabilities, many in the firmware, which we know means they will probably not be fixed, especially by lesser brands that are sort of off the map and are happy to take your money, and then you never hear from them again, I was sort of asking myself what's best?  Is it better to have a monoculture or a heterogeneous spread of devices?  We know that the danger of a monoculture approach is that, when there's a problem, it affects everything.  Well, like HP; okay?  Clearly, HP had one core of code to handle their fax interpretation, and it spread.  You know, they used it in all of their devices.  And they're a major longstanding provider of that technology.



So this one bit of research suddenly affects all HP combo fax/scanner/printer devices everywhere.  Which you could argue, whoops, I mean, that's a problem.  And we've seen various, you know, like Heartbleed sort of had this problem because it was a problem in the most common web server family that shared a common code base.  So there's a danger with the monoculture approach that you don't have with heterogeneous spreads.  For example, of course it's incredibly impractical, but if everyone wrote their own operating system, like individually, then there wouldn't be - first of all, then they would all be insecure.  But they'd all be insecure in different ways.  So you wouldn't have the concentration effect of finding a vulnerability, for example, in all of the D-Link routers in Brazil, which allows them all to be commandeered, that we'll be talking about later.



So I guess the question sort of is what's your individual profile?  That is, are you likely ever to be targeted or not?  The reason I think that comes up is that, if you were somebody, a political dissident or somebody who for whatever reason anyone might be focusing on, then the danger of using an off-brand device, and unfortunately by "off" I mean other than Apple or Google, is that the entity targeting you could probably figure out what device you have and then, as a consequence of this rich background of attacks, figure out how to get into that particular device in order to compromise you in a targeted attack.  That sort of person, I would argue, is safest and probably only truly safe against that kind of focused target attack if you stay with the major provider for the platform.



Even Google is not safe.  We're going to be covering a little bit later a story in which Google's own devices or apps on Android have some problems.  They respond immediately and fix them; so that's, you know, as we know, from a security standpoint, that's the best we can really expect.  Anybody can make a mistake.  HP made a big mistake.  They immediately fixed it.  This was all disclosed with HP responsibly, and patches have been out for weeks now, before this went public.  So again, we're familiar with that cycle.



But so when somebody is deciding whether it's worth the money to purchase a more expensive smartphone, which is what you pay if you buy a Google brand device or an Apple brand device, I think the question is who are you because you really want, I mean, this is, as we know, these are pocket computers.  And they are imperfect.  And problems are being found.  But what you want is, to the degree you need to be truly concerned about security, I just think staying with the platform's parent where there is a proven fast response to security problems is the only way to be safe.



LEO:  I only use Pixel 2 phones because of that.  I mean, LG is a major brand.  Sony is a major brand.  Essential is a major brand.  These are all supposedly Android OEMs who do regular updates, who work hard at security.  And I always say never buy an Android phone if the manufacturer hasn't agreed to push out the monthly security updates that Google offers in a [crosstalk] fashion.  But honestly, these guys, some of these guys are on that list.  So I would say at this point you're right, get a - maybe Samsung.  I notice Samsung's not on there.



STEVE:  No.  Very good point, yes.



LEO:  But I would say Google.  I mean, and the goodness is that Google Pixel phones are as good as anything else out there; the best cameras out there.  So wow.  Yeah, I pretty much stick with the Google phones, and this is a good reason.



STEVE:  Yeah.  So here's another reason.  Check Point Research, we're going to be hearing their name a lot in the next hour and a half.  They have been really busy.  In this case, they examined an interesting shortcoming in the way Android apps used storage resources.  It turns out that sort of the careless use of the external storage by applications on Android can open the door to attacks resulting in any number of undesired outcomes:  silent installation of unrequested, potentially malicious apps to the user's phone; denial of service for legitimate apps; and even causing applications to crash, which can open the door to, as we know, the beginning of an exploit as it crashes.  As it matures, it no longer crashes, it runs the attackers' code for them.



So the problem is the nature of the way Android is currently handling the storage resources on its devices.  There are two types of storage, loosely:  internal, with each application using its own internal space which is segregated and enforced by the Android sandbox; and then there's external storage, which is often an SD card.  I know on my Android devices, I mean, I've used it this way.  It's an SD card stuck in that can be big, and it's exchangeable and so forth.  And it creates a logical partition within the Android OS.  And that region, and here's the crux of the problem, is shared by all applications.  That external storage is often used to deliberately share files between apps or, for example, with a PC.  So it is a sandbox bypass by design.  It gives you some flexibility.



What Check Point researchers asked and then answered is how can we use this?  Can we use that to effect exploitation of the Android trust, essentially, the trust boundaries?  And in their coverage they explained that there are other reasons for an app developer to choose to use external storage, rather than the sandboxed internal store.  It might be lack of sufficient capacity in the internal storage.  Maybe backward compatibility considerations with older devices.  Or not wanting the app to appear to use too much space inside  the phone.  Or just lack of care on the developer's part.



Anyway, so whatever the reasons, when using the external storage, certain precautions are necessary.  And Google does understand this, even if they don't yet enforce it.  According to Google's Android documentation, application developers are advised about their use of external storage in their apps with guidelines which include "perform input validation when handling data from external storage; do not store executables or class files on external storage; and external storage files should be signed and cryptographically verified prior to dynamic loading."  Right?  I mean, all those things make sense.



Well, it turns out that what Check Point has called "man-in-the-disk" attacks are made possible when applications are careless about their use of this Android external storage, such that failing to employ pretty much any of those security precautions can leave applications vulnerable to the risks of deliberate data manipulation.  And apps from major OEMs, including Google, do not always follow these guidelines.  During their research, they found instances where an app was downloaded, updated, or received data from the app provider's server, which was passed through the external storage before being sent on to the app itself.



So the app used the external storage as a temporary staging store for a blob of update code, or app modification code, before incorporating it into the internal store.  And of course doing that violates Google's guidelines and offers, I was going to say provides, an opportunity for an adversary to manipulate the data held in that store during the interval before the app reads it again.  So they actually developed exploits for all these things.  Applications that were tested for this new attack surface included Google Translate, Yandex's Translate, Google Voice Typing, Google's Text-to-Speech, the Xiaomi browser, and various other apps.



After referring to the advice given within Google's guidelines, Check Point compared the advice to what was actually the case.  The Xiaomi browser was found to be using external storage as a staging resource for application updates.  Check Point was able to execute a successful attack by which the application's update code was replaced, resulting in the installation of an alternative undesired application instead of a legitimate update.



So literally in this case they downloaded an app that looked innocent.  I think it was a flashlight app.  And the flashlight app asked for access to external storage.  Since that's not regarded as a security issue or problem, and apparently lots of Android apps that you download and install ask for that permission, it's often given.  So that's all an app needs in order to exploit this kind of attack is, no matter what kind of app it is, access to external storage.  It then monitors the external store, waits for an opportunity, replaces the staged download, and its modification of what was downloaded gets installed instead, and nobody is the wiser.



In the case of Google Translate, Yandex Translate, and Google Voice Typing, Check Point found that the developers were failing to validate the integrity of data read from external storage.  So those apps were also using external storage, but assuming it was pristine, that is, what they had loaded there hadn't been changed.  So they were taking it as gospel when they brought it in and did whatever they do with it, interpreted it in some fashion.  So that Check Point was able to compromise certain files required by those apps and get them to crash, which as we know could lead to eventual takeover.



After the discovery and verification of these problems, Check Point did responsibly contact Google, Xiaomi, and vendors of other vulnerable apps to update them and request their response.  Google immediately responded and fixed the problems.  Xiaomi chose not to address it at all.  And as I understand it, the Xiaomi browser is a very popular browser on Android, which is now known to be vulnerable to this sort of an external storage staging attack.  And apparently it's going to remain so.



So to conclude, Check Point summarized the problems and what they view as shortcomings of Android by saying:  "An Android device's external storage is a public area which can be observed or modified by any other application on the same device.  Android does not provide built-in protections for the data held in the external storage.  It only offers developers guidelines on proper use of this resource."  But again, no enforcement.



"Developers anywhere," as we know, "are not always versed in the need for security and the potential risks, nor do they always follow these guidelines.  Some of the pre-installed and popularly used apps ignore these guidelines and hold sensitive data in the unprotected external storage."  And again, that is subject to manipulation.  And they say:  "This can lead to a man-in-the-disk attack, resulting in the manipulation and/or abuse of unprotected sensitive data."  And, as they found and proved:  "Modification to the data can lead to," as they put it, "unwelcome results on the user's device."



So this probably doesn't come as a huge surprise, but it's a warning.  And I would argue that Google needs to go further than just tell developers, you know, be careful about what you read from external storage.  Don't assume it hasn't been modified.  Certainly anybody, I mean, the only way, in the case of the Xiaomi browser, could be caught out is if it is not verifying a signature on the thing it downloaded and then is incorporating into itself from external storage.  I mean, that's unconscionable, but it is happening.  And who knows what other apps that may be happening to.  It's very simple for somebody to install an app that says, oh, I'm going to need to have access to external storage; okay?  And most users are going to say yes.



So hopefully, maybe what we need is for Google to extend the sandboxing of external storage out into it so that there is per app ties.  And then where apps explicitly need to share through external storage, that permission is granted because either that or they have to somehow - I don't think an app can be forced to parse the data it reads.  Maybe it could be forced to have executable content signed.



But again, I mean, we have a huge ecosystem of Android apps, and this is a glaring vulnerability which I'm really happy that Check Point brought to light.  Google is aware of it.  Maybe they'll respond.  But until then, I guess all I can do is caution our listeners who are Android users that this is a potential vulnerability that's just hanging out there right now.



And while we're on the topic of Google and Android, it turns out that the Hacker News reported on some research that the AP, the Associated Press did, which is probably, I mean, this is maybe a little inflammatory clickbait, but it's worth pointing out to our listeners.  The statement was "Google tracks you everywhere, even if you explicitly tell it not to."  And there is some truth to it.  What the AP found is that preventing Google from acquiring long-term tracking data requires more than turning off the obvious location history function.  What the Hacker News wrote was:  "Every time a service like Google Maps wants to use your location, Google asks your permission to allow access to your location if you want to use it for navigating."  They say:  "But a new investigation shows that the company does track you anyway."



The AP revealed that many Google services on Android and iPhone devices store records of your location data, even when you have paused the location history on your mobile device.  Disabling location history in the privacy settings of Google applications should prevent, it is argued, Google from keeping track of your every movement as its own support page states, quote:  "You can turn off location history at any time.  With location history off, the places you go are no longer stored."  That's not true.



As the AP explains, quote from the AP:  "For example, Google stores a snapshot of where you are when you merely open its Maps app.  Automatic daily weather updates on Android phones pinpoint roughly where you are.  And some searches that have nothing to do with location, like 'chocolate chip cookies' and 'kids science kits,' pinpoint your precise latitude and longitude," they write, "accurate to the square foot, and save it to your Google account."



During their coverage, to demonstrate the threat of this, what they consider - I mean, again, clickbait, but worth understanding - of this Google practice, the AP created a visual map of the movements of a Princeton postdoctoral researcher, a Dr. Acar, who carried an Android smartphone with location history switched off to prevent location data collection.  However, the researcher discovered that the map includes records of Dr. Acar's train commute on two trips to New York and visits to High Line Park, Chelsea Market, Hell's Kitchen, Central Park, and Harlem.  To protect the privacy of Dr. Acar, the publication did not plot the most telling and frequent marker on the map, his home address.  According to the researchers, this privacy issue affects around two billion Android users and hundreds of millions of iPhone users across the world who rely on Google Maps or search.



In response to the AP's investigation, Google issued the following statement:  "There are a number of different ways that Google may use location to improve people's experience, including location history, web, and app activity, and through device-level location services."  They say:  "We provide clear descriptions of these tools and robust controls so people can turn them on or off and delete their histories at any time."



Jonathan Mayer, whom we have spoken of years past often, a Princeton researcher and former chief technologist for the FCC's enforcement bureau, argued:  "If you're going to allow users to turn off something called 'location history,' then all the places where you maintain location history should be turned off."  He says:  "That seems like a pretty straightforward position to have."



So for what it's worth, to stop Google from saving timestamped location markers, users need to turn off another setting, and that's that web and app activity, which is enabled by default and stores a variety of information from Google apps and sites to your Google account.  Once disabled, it will not only stop Google from storing location markers, but also prevents the company from storing information generated by searches and other activities.  So anyway, I just wanted to point this out.  It seemed a little over the top in the way it was being described.  But it's certainly reasonable, if somebody wasn't really paying attention and thought that just turning off location history shut that down completely, you also need to go into your Google account and turn off web and app activity, which does not just record activity, but where that activity occurred.



So, okay.  We've been talking about D-Link routers in Brazil.  Let's see.  Last time we talked about them, they were being commandeered into a botnet in short order, within 24 hours.  I coined the term, although it wasn't particular, didn't require much imagination, a flash botnet because it was just flashed into existence within a day.  As we know, these D-Link routers have - many of them, unfortunately, tens of thousands - have a known old vulnerability which allows remote access to essentially an authentication bypass that gives somebody on the outside access to running their own code and commandeering the router remotely.



Well, Radware's Threat Research Center has just identified another hijacking campaign also aimed at Brazilian D-Link routers against two Brazilian banks.  This is an attack which hijacks - these are DSL modems, D-Link DSL modems.  It hijacks their DNS settings.  And we've talked about DNS hijacking in the past.  In this case the DNS servers on these vulnerable routers are pointed to a malicious DNS server which replaces the proper ID for two domains:  www.bb.com.br, which is Banco de Brasil; and another one is Itau Unibanco, and that's www.itau.com.br.  In both cases, the IP address redirects to a fake website which then attempts to spoof the user and to obtain their bank credentials.



Now, okay.  We know there are limitations to this sort of attack; right?  We know that, unless the attackers are able somehow to obtain valid TLS certs for those domains, it won't be possible for users to avoid seeing a warning of some sort, and in often cases just like hard to bypass warning.  I mean, the good news is - we were complaining last week about how difficult it is to bypass these warnings when you legitimately know you want to, like you're wanting to talk to your own LAN router over HTTP, and your browser is becoming increasingly resistant to allowing you to do that.  That's a pain.  In this case, you don't want to be fooled into making an HTTP contact to your bank.



So the problem is that, if banks also allowed HTTP and didn't force a redirect - well, in fact, even if they did force a redirect, if they allow an HTTP connection, which then bounces you over to a secure connection, it might be that people's bookmarks for their banks would be HTTP.  That would allow these bad guys to establish a fake bank website which does not redirect users to HTTPS and of course doesn't have any other HTTPS links.  If people are not paying attention, they wouldn't notice the difference and would interact with this cloned site without knowing that it wasn't where they thought they were and could get themselves in trouble.



We've talked about, in fact you offered it up last week, Leo, HSTS, HTTP Strict Transport Security.  What should be done is that all companies that are intending only to be accessed over HTTPS add the HSTS header to every response from their server.  That allows browsers to learn and permanently or semi-permanently, because you can set an expiration, but essentially the idea is forever, and it's refreshed every time you revisit, to learn that the bank only wants HTTPS connections such that the browser is given permission to automatically upgrade HTTP connections to HTTPS.



So the point is, you know, we're sort of in this crossroads period.  We've got Chrome raising the alarm increasingly when any site, even a benign blog site, isn't HTTPS.  We have all the mechanisms in place, but not yet completely deployed.  Unfortunately, we have browsers that are not yet also trying HTTPS when they try HTTP.  And I would argue it's about time for them to be making parallel lookups and connections to see if they can autonomously move over to HTTPS.  There are some problems with doing that.  So I can understand the need for the permission from the site in the form of an HSTS header.  And GRC is on Google's and many other permanent HSTS lists.  We got on the list very early when we first talked about this years ago, and so we will always be accessible over HTTPS.



So it's worth reminding people that this is a problem, that is, that the thing that can be done to avoid this is not, if you are worried that your router could be compromised, is not to use DHCP, the Dynamic Host Configuration Protocol, which is the obtain IP address automatically.  You can typically override your computer's DNS settings so that it doesn't get your DNS servers automatically from the router.  You provide them yourself, in which case you would be immune from this sort of takeover.  Certainly better to take responsibility for your router not being updated.



But maybe someone has no opportunity to do that.  There is no updated firmware.  The router's no longer being maintained.  They're worried that there could be some exploit.  As we know, even closing down WAN access, WAN management on that side doesn't provide bulletproof protection.  So I could see, sort of as a belt-and-suspenders approach, just change your own machine's or any important machine's DNS over to something that you explicitly and deliberately specify so that, if your router had its DNS changed, you'd still be safe.  But better to have a good router.  And they're cheap these days, if you have any choice.



But notice that things like multiple NAT servers, network segregation, other things we've talked about to protect users in the past wouldn't protect you from this.  Making sure, however, that you have an HTTPS connection to your bank and that you verify that's where you are would go a long way to keep you safe.



Also at DEF CON - it's pretty much a DEF CON podcast we have.



LEO:  Yeah, it usually is after DEF CON.



STEVE:  Exactly, once a year, right around our anniversary.  A  repeat DEF CON presenter, Christopher Domas, who is an annual contributor, I mean, he's a hardware hacker, specializes in x86 stuff.  His presentation this year was titled:  "God Mode Unlocked:  Hardware Backdoors in x86 CPUs."  He revealed the presence of a hidden, as he called it, "god mode," which was undocumented in the VIA 3C x86-based CPUs, at least back from 2001 through 2003.  There have been people that have argued, well, it's not really a backdoor because they disclosed it in 2004.  This feature was present in the VIA C3, it looks like, what, Nehemiah, N-E-H-E-M-I-A-H, chips.



But Chris said that all other C3 chipsets are bound to feature a similar mechanism.  He actually named this god mode the "Rosenbridge backdoor," which allows an attacker to elevate the execution level of malicious code from ring 3, where security safeguards would constrain it to kernel ring 0, where it's unrestrained.  And if Rosenbridge sounds familiar to any of our science-oriented listeners, that's because the official name for a space-time wormhole is an Einstein-Rosen bridge.



LEO:  Aha.



STEVE:  Which suggests the possibility of short-circuiting space time if a nontrivial solution to Einstein's field equations can be shown to hold.  So Einstein presented the possibility of such a thing, although we sci-fi buffs, I mean, it's a constant, you know, for example, our favorite author, Peter Hamilton, his whole Commonwealth...



LEO:  Is based on that, isn't it, yeah, yeah.



STEVE:  ...is knit together very cleverly with wormholes which short-circuit space time.  Anyway, Domas says that this backdoor mechanism is a RISC, R-I-S-C, Reduced Instruction Set Computer, coprocessor that sits alongside the main C3 processor and is part of it.  By using a launch instruction, which is a 0f3f, a register control bit can be flipped which enables this additional coprocessor.  And Chris says that coprocessor does not enforce the same security protections as the main C3 chipset.  And nobody disputes that.



This alternate instruction set says the documentation, which was subsequently published, this alternate instruction set includes an extended set of integer, MMX, floating point, and 3DNow! instructions, along with additional registers and some more powerful instruction forms over the x86 instruction architecture.  So it's sort of a switch you can flip on which extends the chip's instruction set with formally defined instructions and then a bunch of apparently VIA's own instructions.



However, the VIA document also mentions that the additional instruction set is specifically meant for testing, debugging, or other special conditions.  Hence the reason it is not documented for general usage.  Which seems curious to me because certainly MMX, floating point, and the 3DNow! instructions, those are all general use instructions.  They're not debugging and testing instructions.  But it is nevertheless too potent to be left available because any instructions sent to this additional coprocessor is run under ring 0, bypassing all security and privilege barriers, rather than under the normal ring 3.



The good news is that this controversial backdoor, as Chris himself explains, should require kernel-level access to activate.  So there was the intention of some protection.  However, he points out that this Rosenbridge backdoor mechanism, as he named it, has been observed to be enabled by default on some systems, allowing any unprivileged code to modify the kernel without prior exploitation.  In these scenarios, the attacker only needs to send that specially crafted instruction to the additional RISC processor which was then standing by, ready to read and execute them with no ring 3 privilege restriction.



So I'm glad he brought this to life.  The problem is these chips, I mean, they are - it's a very popular sort of low-end Intel x86 clone present in all kinds of stuff.  So if it happened to be on by default, this is a potentially serious execution opportunity.  For example, it might well be that some of the printers and fax machines use these.  Actually, the HP is a 32-bit ARM processor.  We know from the reverse engineering research.  But still, these little VIA C3s have been long used in lower end applications.  I've got a link for additional information in the show notes if anyone's interested.  Chris put out, I think it's a 110-page slides PDF at DEF CON.



Okay.  So there was a bunch of coverage of an attack on WhatsApp.  And I thought, uh-oh, that's not good.  WhatsApp, as we know, was purchased by Facebook.  It's arguably now, as a consequence of that purchase, what is it, the number one messaging app in use?  And so the idea of somebody being able to spoof users and spoof what they send and inject malicious content into the conversation seems like a problem.  Turns out, eh, useful research, interesting research, nothing to worry about.  And the WhatsApp people say, yeah, we know, and we're not changing it.



Once again, Check Point researchers, who as I have mentioned have apparently been working overtime recently, revealed that someone who is already participating in a two or more way WhatsApp chat can alter messages to spoof their content and sender.  Okay.  Once again, somebody who's already trusted and has the keys required to chat in a two or more way messaging group can spoof the content and sender.  So, okay.  The press coverage suggests that this is a flaw in WhatsApp which takes advantage of a loophole in WhatsApp security protocols to change the content of the messages, allowing malicious users to create and spread misinformation or fake news from what appear to be trusted sources.  Certainly I agree that that's the case.  It turns out it's by design.  It's not something that was overlooked.



Essentially what Check Point did was to reverse engineer some undocumented protocol.  After being trusted, that gave them the access that they wanted.  So by decrypting the protocol received by somebody who is in the group, the Check Point researchers were able to develop three attacks.  And I've put "attacks" in quotes in the show notes because it's like, okay, first, they can change a correspondent's reply to essentially put words in their mouth, that is, change what they said; change the identity of a sender in a group chat, even if they're not a member, so like fake somebody breaking into the chat; and send a private message in a chat group, but when the recipient replies, the whole group sees it.  So, that is, to unprivate a message in the chat group.



So basically they're screwing around with the internal messaging protocol in some ways which it probably should not surprise anyone is possible if you are participating in the group.  So when the researchers reported the flaws to the WhatsApp security team, WhatsApp argued that, since these messages do not break the fundamental functionality of the end-to-end encryption, users always have the option of blocking a sender who tries to spoof messages, and they can report problematic content to WhatsApp.  In other words, yeah, okay.



WhatsApp replied:  "These are known design tradeoffs that have been previously raised in public, including by Signal in a 2014 blog post" - I think we covered it at the time, four years ago.  And, they said:  "We do not intend to make any change to WhatsApp as a result."  So I guess WhatsApp users might be usefully cautioned against this possibility, that is, that it's possible for somebody in a WhatsApp messaging group with enough technical skills to mess with the content of that messaging of those people in the pool, in the group.  So that, but nothing more.



LEO:  The only reason it might be a cause for concern is that WhatsApp is a big culprit in the spread of phony news and disinformation in some countries like India.



STEVE:  Ah, okay.



LEO:  I don't know how it works.  But I would guess that WhatsApp, you'd be in a group, a large group of news.  And that, in that case, you could be, for instance, somebody could post some misinformation and then fake a lot of other people in that group going, oh, you know, that's true.  I saw that, you know, or that kind of thing.



STEVE:  Yeah, very good point.



LEO:  So I wonder if that's what they're concerned about.  I'm not sure.  But that's a real problem in India and other places where WhatsApp has been used to incite mob killings and things.



STEVE:  Okay.  I didn't realize that WhatsApp groups got that big.  Because, you're right, if it's three people you know, then it's, yeah, eh.  But if you're joining a massive messaging collective...



LEO:  Well, that's the thing I don't know.  I'm not sure how - I don't know if it's being - so this is a story from The New York Times from a couple of weeks ago, how WhatsApp leads mobs to murder in India.  And these fake stories get spread.  But I'm not sure if they're spread in messages.  It says "Local authorities have struggled to contain false messages which have circulated throughout India for months."  So what I don't know is if it's a large newsgroup.  If that were the case, though, that would be one reason that this would be something to be worried about; right?



STEVE:  Yeah, because, for example, a legitimate news provider could...



LEO:  Be spoofed.



STEVE:  Could be spoofed completely, their identity and the content of what they're sending.  So, yeah.



LEO:  Yeah, that's something to pay attention to.  I just - it says:  "WhatsApp's design" - this is the Times again - "makes it easy to spread false information.  Many messages" - oh, here it is - "are shared in groups.  And when they are forwarded, there is no indication of their origin.  The kidnap warnings have often appeared to come from family and friends."  So it's the spoofing of messages that is part of the problem here.



STEVE:  Yeah.



LEO:  Anyway, I don't - just more information on that story, I guess.



STEVE:  So I heard you on the tail end of MacBreak Weekly talking about last week's security conference relative to macOS hacks.  



LEO:  Yeah.



STEVE:  And I don't know whether one of the things you talked about was this synthetic mouse clicks problem?



LEO:  Yes, we did, yeah.



STEVE:  Okay.  So High Sierra is vulnerable, but Mojave will not be.  Patrick Wardle, who is an ex-NSA hacker who is now the Chief Research Officer of Digita Security, discovered by accident and revealed a - and actually it's a zero-day, technically, because it came as a surprise to Apple.  He did not responsibly disclose this.



LEO:  He didn't tell them, yeah, yeah.



STEVE:  He revealed a critical zero-day vulnerability - which, by the way, is zero-day because of him - in the High Sierra macOS operating system that could allow a malicious application installed in the targeted system to essentially virtually click objects without any user interaction or consent.  Okay, now, the reason that's a problem, I mean, so Mac has this technology built in for their accessibility features to help disabled people interact with the system interface.  But they explicitly hold security-critical events out of that.  That is, for example, you can't use a synthetic mouse click to click on the "Okay" for a crucial security permission like access to the keychain or access to install something.



Normally events in an OS are typically discrete.  And so, for example, there's a down click event that the mouse button makes when you press it down.  Then there might be a series of mouse move events while you drag something somewhere.  Then there would be a mouse up event when you release the mouse button.  So these events are all discrete.  What Patrick accidentally discovered was that in High Sierra, two consecutive synthetic, that is, non-physical mouse generated, but software generated, mouse down events in a row are treated as a legitimate click, which then allows attackers to programmatically interact with security warnings which ask users to choose between "Allow" or "Deny" access to sensitive data or features.



So I don't understand why Patrick chose not to disclose this responsibly.  I mean, this is not good.  The good news is Mojave, as a consequence of a redesign of this whole system, already has this mitigated by blocking all synthetic events.  But for the time being, I mean, I imagine Apple will quickly move to address this.  So in the meantime, I guess, be careful.



LEO:  I've used this, and I wonder why, I mean, one of the reasons it's in there is for people with accessibility issues to be able to do stuff.  And the way I've used it, this is - I didn't think this was a bug.  But at one point, probably because of something I was using like text expander on my Mac, I wasn't able to click - you know when you install something very low level, a kernel extension on macOS, you have to then go into - this is really a nice, I think a nice feature.



It's not merely a warning, oh, are you sure you want to do this?  You actually have to go into the system preferences.  You have to go to security.  It's where Gatekeeper lives.  And you'll see a new thing that says, oh, this XYZ program is trying to install a kernel extension.  Do you want to allow this?  And there's a button that says "Allow."  And you actually, I mean, this is a bit of a, and I think a good thing, a rigamarole...



STEVE:  Yes.



LEO:  ...in order to allow something to modify your system.  But for some reason, because of something I had installed, the ability to click on that "Allow" button was disabled.  I clicked, click click, nothing happens, click click click.  So I wrote an AppleScript.  I figured out where the button is on the screen and wrote an AppleScript to click on that location on the screen.  This is that exact facility we're talking about.



STEVE:  Yes, yes, yes.



LEO:  And it's the only way I could click that "Allow."  But if somebody wrote a malicious AppleScript to do that, you could see what the problem would be.



STEVE:  Yeah, to automate the whole process.



LEO:  Right.



STEVE:  Of getting something into your kernel.



LEO:  We talked about AppleScript and security on Friday on this Triangulation episode with Sal Soghoian, who was the king of AppleScript at Apple.  And he even talked about the negotiation between the Automation Division, his division, and Apple Security, who's really good, about all this because it's really great from the point of view of automation to have access to Apple events and to be able to trigger events in a scriptable fashion.  And so what they decided, apparently, was, well, the AppleScript would only run at the user level, the permissions level, the privilege level of the user running the script. 



STEVE:  Right.



LEO:  Which was somewhat mitigated because it prevented it from doing anything an administrator access was required for.  It sounds like now they've even gone a step further.  It's a tricky thing because you do want to give people automation capability, and you want to give people who have mobility issues...



STEVE:  A legitimate need.



LEO:  Accessibility issues, the ability to do this, as well.  That's what mouse keys is all about.  So I understand the - it's always a balancing act, isn't it.  It's a fascinating story.



STEVE:  Yeah, yeah.  So our friend Lawrence Abrams, who founded Bleeping Computer, published on his site that Windows 10 Enterprise would apparently soon be getting a new InPrivate Desktop feature.  This feature would allow administrators to run untrusted executables in a secure sandbox without fear that that executable might or could make any changes to the operating system or the system's files.  Lawrence quotes the Windows 10 Insider Feedback Hub as saying - and I should mention it's since disappeared.  It said:  "InPrivate Desktop (Preview) provides admins a way to launch a throwaway sandbox for secure, one-time execution of untrusted software.  This is basically," it writes, "an in-box, speedy VM [Virtual Machine] that is recycled when you close the app."



He writes:  "The quoted quest is no longer available in the Feedback Hub.  But according to its description, this feature is being targeted at Windows 10 Enterprise and requires at least 4GB of RAM, 5GB of free disk space, two CPU cores, and CPU virtualization enabled in the BIOS.  It does not indicate if Hyper-V needs to be installed or not.  But as the app requires admin privileges to install some features, it could be," he writes, "that Hyper-V will be enabled."



He wrote:  "When the quest was live, I had attempted to install the InPrivate Desktop (Preview) app, but it was not accessible from the Microsoft Store as described.  Furthermore," he says, "a wiki link in the quest description brought me to a page asking me to log into my Microsoft account.  When I logged in with my account, I received a message that indicates I need to be part of the Azure Active Directory tenant for Microsoft."  He says:  "It's too bad that I was unable to test this feature as it looks to be an interesting way to execute untrusted software without fear of permanent file modification, program installation, or configuration changes.  This will also provide a new security boundary that Microsoft will need to protect and that researchers will be hammering on for bug bounties."



And I would add that it would be nice if this were to migrate down to other non-enterprise builds in the future, since I could see, I mean, for myself I'm often firing up a VM in order to run something whose behavior I want to constrain until I'm able to vet it a little bit.  So that would be cool if that were built in.



I know this is going to be of interest to people, and I didn't know if it would hook you, Leo.  But there is a new email service using the Signal protocol.  It's called Criptext, or Cript, well, there's only one "T."  Okay.  So here it is.  You probably want to bring this up:  www.criptext.com.  C-R-I-P-T-E-X-T dotcom.  Criptext.com.  And they say:  "Criptext is an encrypted email service that guarantees security, privacy, and control over all your email communications.  We don't have access to your emails, nor do we store them in our servers."  They say:  "You're in control now." 



So under their bullet points they offer "End-to-end encryption:  All your emails are locked with a unique key that's stored on your device alone, which means only you and your intended recipient can read the emails you send.  Signal protocol:  The Criptext email service utilizes the open source Signal protocol library, which protects your privacy and security throughout your entire Criptext experience.  Open source:  Criptext's source code is open to the entire privacy community to see.  We actively work with our open source community to improve on the software in order to provide the best email experience.  No cloud storage:  Criptext doesn't store any emails in its servers.  All your emails are stored on your device alone, which means you're in control of your data at all times."



Then they say:  "We know that privacy isn't just about encryption, which is why Criptext doesn't store any emails in our servers."  It's available on web and mobile:  Mac, Windows, Linux, iOS, and Android.  Tip of the hat to Bleeping Computer for pointing this out.  And I just wanted to bring it to our users' attention.  We know Signal protocol is arguably the best piece of work that exists.  This was Moxie Marlinspike and his group put this together.  I initially, our listeners will remember, thought that Signal protocol was a little overdesigned.  Then I understood how it works.  I'm not exactly sure how they say they don't store any emails on their servers.  Email is essentially asynchronous, and it has traditionally been a store and forward.  Maybe they just mean they don't permanently store it.  So it's there in transit through them, kept encrypted until your recipient checks in or polls...



LEO:  It's basically Signal.  I mean...



STEVE:  It is.  It is, yes.



LEO:  It's just Signal.  I mean, it's a messaging service.  It's not an email service.  It's Signal.  Right?



STEVE:  Well, it's an offline Signal.



LEO:  Yeah, but Signal's offline.  If I'm not online, and somebody sends me a message...



STEVE:  That's true.  In fact, that's why there's that whole collect a whole bunch of extra crypto keys for somebody who's offline that allows you to encrypt for them until they're online.  So, yeah.  I don't how it differs from Signal.  Maybe it's just larger documents?



LEO:  Yeah.



STEVE:  Maybe it's like more of an email UI than a short signaling protocol.



LEO:  Is it web interface only?  Or can you use an email client?



STEVE:  No.  Web, well, good question.  They say "web and mobile."  But maybe they've implemented [crosstalk].  So I don't know...



LEO:  But you can download something.



STEVE:  Yeah, then maybe they have an iOS and an Android app for it.



LEO:  Right now it's Download for Mac is the only button.  Oh, but you can side - open criptext.com/download on your phone to install.  Apple doesn't allow sideloading, so I'm confused.  Well, I'm going to download it on the Mac and see.  But it sounds like it's just Signal, basically.



STEVE:  Well, it might be Signal oriented towards a larger format of transaction.



LEO:  Yeah, exactly, yeah.



STEVE:  So what I would say is, given that they haven't made any mistakes, that is, given that it is Signal...



LEO:  It is open source, anyway, so...



STEVE:  It is open source.  Signal is as robust as it gets.  It was really done well.  If somebody wanted - and again, each end needs to have it.  So you've got to have, in your own encrypted email a la Signal network, who you're sending it to needs to set themselves up, too.  But this looks like a good way to send larger bulk stuff in a truly, as I said, as good as it can get sort of fashion.



LEO:  Yeah.



STEVE:  C-R-I-P-T-E-X-T dotcom.



LEO:  When I go to Criptext.com/download on my phone, it says 404.



STEVE:  Okay.



LEO:  But I was able to download a DMG package for...



STEVE:  Oh, for the Mac.



LEO:  ...Macintosh, yeah.



STEVE:  So there is some...



LEO:  It looks like the only version right now.



STEVE:  Okay.  It's very fresh.



LEO:  Very fresh.  Still fresh.



STEVE:  So in a very nice move, I think, Facebook has open-sourced their Fizz, F-I-Z-Z, TLS v1.3 Library.  And I just give props to them.  As we know, TLS 1.3 is the recently ratified latest version of TLS, which incorporates a bunch of new features for encrypting the handshake messages to keep certificates private, which otherwise aren't; redesigns the way secret keys are derived to make that more robust; offers a zero roundtrip connection setup which is able to make some requests faster than any previous technology.  It was written in C++ 14.  This is what Facebook is using.  They say it's reliable and a highly performant TLS library that supports all major handshake modes, robust encryption algorithms, and performance optimizations which aim to transfer data securely over 10% higher speed than TLS 1.2.



They write:  "With zero copy encryption and decryption" - meaning in place - "tight integration with other parts of our infrastructure, and other optimizations, we see a reduced usage of memory and CPU with Fizz.  In addition to the enhancements that come with TLS 1.3, Fizz offers an improved solution for middlebox handshake failures, supports asynchronous I/O by default, and can handle scatter/gather I/O to eliminate the need for extra copies of data."  And then they say:  "Facebook has already replaced its older custom protocol, Zero, with Fizz, which is now responsible" - get this - "for securing trillions of connections every day at Facebook."  In other words, I can't think of a stronger endorsement for something than it's what Facebook is already using, and it works.



The social media giant Facebook says it has deployed Fizz and TLS 1.3 globally in their mobile apps, Proxygen, their load balancers, their internal services, and even their QUIC, that's Q-U-I-C, the DNS-based library, which is MVFST.  More than 50% of their Internet traffic is now secured with TLS v1.3.  And it's on GitHub, GitHub.com/facebookincubator/fizz.  So big props to Facebook for putting that out there.  It's just very, very cool to have a robust and proven TLS v1.3 library now freely available for the Internet community to use.



And lastly, Let's Encrypt.  I said that they had another milestone to report.  They just reported that they are now officially trusted by all major root programs.  Remember that when they started out they had their own root certificate, but nobody else knew about them.  I mean, it's only by having the root store or the root certificate in everybody's root store that we are then able to verify the signature of any certificates that the Let's Encrypt system signs.  So to solve that problem, the root was cross-signed by InTrust.  InTrust?  I think it was InTrust.  Oh, IdenTrust.  So, and IdenTrust already had established a longstanding trust relationship across all platforms.



So as a consequence of the cross-signing, Let's Encrypt was trusted, not because they had signed it, but because IdenTrust had signed it.  Now they report last week that they are in all the major root stores.  So they will continue to be cross-signed for those devices which are not being kept current for whatever reason.  But for all current devices and future devices, Let's Encrypt's root is in the store everywhere.  And I imagine way downstream at some point, when their metrics demonstrate that nobody is any longer needing IdenTrust's cross-signature, they'll remove it.  But another nice milestone for Let's Encrypt.



And speaking of nice milestones, this morning, or I guess it was yesterday morning, Monday, I found a nice tweet from Jim Berry that started off "Testimonial, so we know what that's going to be about.  He says:  "I have an HP Z200 used in a high-priority security context at my job that was experiencing the 'click of death.'"  Okay, now, in this case this is a hard drive doing the click of death, and that's not something you...



LEO:  There's a name from the past.



STEVE:  Yes, exactly.  And that's not something you want.  He says:  "It was experiencing the click of death, and it wasn't booting at all."  So this is an HP Z200 in a high-priority security context that was experiencing click of death and not booting.  He says:  "I was able to see the drive in Linux, but Windows was a no-go."  He says:  "I ran SpinRite on Level 4 over the weekend with the 500GB drive."  And he says:  "It still clicks," he says, "getting it replaced ASAP, but now it boots up.  So the drive is being imaged onto a replacement, and the machine will be redeployed with the hearty thanks from my customer."



So that was interesting.  So that's probably SpinRite bringing the drive back from just about as far over the line and the brink of death as is possible.  What that suggests is that there is some serious, serious damage on the drive, probably caused by past power failures while the drive was writing.  That's the kind of damage that can occur because what it suggests is that the servoing information, the low-level data on the drive, which we can't get at from the data level typically, is damaged.  SpinRite nevertheless brought it back to the point where it was able to at least boot and get reimaged.



And you definitely - this here's one you don't want to say, oh, I wonder how much life left the drive has.  Should I replace it after running SpinRite or not?  No.  You do a Hail Mary and reimage the drive.  And the good news is SpinRite was able to get it to the point where it could be imaged and so the device experiences minimal downtime.  So, yay, and thanks for sharing that, Jim.



LEO:  Yay, yay, yay.  All right.  Steve, let's find out why we should disconnect the old fax printer.



STEVE:  So I do feel like I've pretty thoroughly already stepped on the lede here because - and frankly, I've done so only in the best interest of our listeners because I will be surprised if - I don't know even if this is going to get attention because it just might be used in targeted attacks.  But it's been a long time since I've seen a vulnerability that has such exploit potential for attack targeting.  We often talk about how interpreters are so difficult to secure.  It turns out that, once again, Check Point Research, as I said, working overtime, did some amazing reverse engineering of an HP combo fax/printer/scanner device.



The device was not easy to get into.  I mean, not only physically.  Physically they destroyed it.  They had to cut off the plastic back in order to access the brain board inside of this combo device.  They found the JTAG interface, which is the programming interface for connecting a debugger to an embedded processor.  They were able to begin to see what was going on inside.  But they needed to get more debugging capability in.  So they found a means for bootstrapping themselves using a network-based large packet which they were then able to use to attack the printer from the network, all of this just for the research.  This is just the reverse engineering part.  This is not the attack.  This is gaining a foothold for reverse engineering and understanding what was going on.



That allowed them, using first the JTAG to then figure out how to get a buffer overflow from the network, allowed them to get some debugging capability uploaded into the printer that was otherwise locked down.  Once they were in there, they were able to more conveniently look around and start to understand what their environment was.  They found an ARM 32-bit CPU which was running in Big Endian mode, meaning that the sequence of bytes is not least significant byte first, it's most significant byte first for multibyte values.  They found that a shared memory region is used to communicate with the microcontroller and the LCD.



The operating system was ThreadX based, which is a real-time OS made by Green Hills.  It uses a flat memory model, where there are many tasks running in kernel mode, all sharing the same flat virtual address space.  And they determined that there was no ASLR mechanism.  It was all fixed-position memory, which is probably reasonable for a turnkey device like that.  They then proceeded to look for vulnerabilities from the phone line.  I mean, this was their entire intent was, is there a way to attack this device by phoning into it from the outside?  So they took a disassembly of it using their debugging tools, started reverse engineering it, figured out what was going on.



The normal faxes are two-tone black-and-white.  They contain essentially a big TIFF, T-I-F-F, format image, although during the handshaking phase the various metadata for the following TIFF image is exchanged.  They looked there.  They couldn't really see any way of compromising it.  And the TIFF interpreter itself looked solid.  Again, no obvious means of exploit.  But they noticed in the initial handshake there are many, you know, fax protocol, I think that's T.30, has been around for decades.  And it has evolved.  This machine, as do the HP machines, also advertises that it can accept a JPEG image.  It says it supports color, maybe just half-tones in order to be accommodating.  But there is a capabilities bit saying can accept a JPEG.



Well, it turns out that in the standard, if you're sending a JPEG, then the metadata for the JPEG is included in the JPEG, not in the handshaking.  They looked there, and they found, I mean, like it was a - I'm speechless.  I mean, there were so many problems with what they found.  They immediately found a number of obvious buffer overruns.  They were able to basically pick and choose which ones they wanted for the ease of their own use.  And they found one that gave them maximum flexibility and freedom, where basically they worked out how they could dial up the fax machine unmodified; right?



So, I mean, all of that hacking that I talked about was information gathering to reverse engineer what was there.  And this is what HP has reused universally, that is, that core across all of their combo devices.  And I don't know if I could say there are hundreds of them.  But, I mean, the list is endless, over time, of all the devices which are vulnerable.  So what they designed was a "fax" which, when the fax machine answers, they do the little handshake and agree, and then they upload this JPEG image which compromises the JPEG interpreter, allows them to execute a buffer overrun.  And because they wanted them, I mean, it wasn't just enough to get in the fax machine.  They wanted to demonstrate true capability.  It then loads the EternalBlue and DoublePulsar NSA weaponized attacks to turn around and go out into the network to which this device is attached and take over all the machines there.



So my point is that today this is all public knowledge.  Their write-up, I mean, it stops short of providing the binary code to do this.  But it's just - and it took good hacking.  I mean, people needed to know how to do this.  But when you consider how target-rich the environment is, as I mentioned earlier, Google lists the fax numbers of 300 million machines.  All Japanese businesses apparently have fax numbers; 45% of Japanese households do.  Again, we don't know that they're HP.  They may be Canon and probably are, other Japanese brands.  On the other hand, we don't know that they're not equally affected.  It was only HP that these guys attacked.  They responsibly disclosed.  HP said, you know, I'm sure there are people not sleeping.  It was like, OMG, immediately produced, across the board, like every single device has firmware updates.



I updated my own machine this morning, thus the Picture of the Week is that picture.  I tweeted it immediately, a link to the HP site.  If you just go to printers, there's drivers and downloads.  There's a firmware tab.  You tell it the device you have.  It'll point you at an EXE.  You download it and run it.  It'll find the device on your network and patch it.  Interestingly, I wanted to know whether it would patch it if it had already been patched, and so I ran it twice.  The second time took longer, but finally finished, but it didn't tell me that it had already been patched.



So somebody who's cautious may want to work through the devices, either their built-in network monitor to get the current firmware version, or maybe it shows if you dig around in the UI.  If you have an LCD screen on yours, find out what the firmware is first, then run the patch, then confirm.  Essentially, this is the first occurrence of a through-the-phone attack, that is, that turns around and attacks a state-of-the-art network to which this phone-connected fax printer is attached in order to do its job.  HP has responded.  It's very likely that other manufacturers are vulnerable.  And anyway, so that's really the gist of this.



They had sort of a fun Q&A in their coverage, and I've got the links for all of this in the show notes.  They asked themselves the question:  "What is this research about?  Check Point Research has uncovered critical vulnerabilities in the fax protocol."  Now, that's not true.  It's HP's implementation of the fax protocol.  They say:  "These vulnerabilities allow an attacker with mere access to a phone line and a fax number to attack its victim's all-in-one printer" - in this case HP that hasn't been updated - "allowing him full control over the all-in-one printer and possibly the entire network it is connected to."



They then ask:  "Does this apply to all all-in-one printers?  No.  We conducted our research on all-in-one fax printers.  However, similar vulnerabilities are likely to be found in other fax implementations such as fax-to-mail services, standalone fax machines, et cetera."  Then they ask:  "Who uses fax anyway?"  Much as you did, Leo.  "Surprisingly," they answer, "fax is still used by many industries, governments, and individuals around the world.  These include the healthcare industry, legal, banking, and commercial, some of which are governed by regulations, and others simply for legacy reasons."



They ask:  "What is the severity level of this vulnerability?  HP classified this vulnerability as 'Critical.'"  In fact, yes, 9.8 out of 10.  So as bad as it gets.  "How does this affect organizations and consumers?"  They answer:  "Once an all-in-one printer has been compromised, anything is possible.  It could be used to infiltrate an organization's or consumer's internal network, steal printed documents, mine bitcoin, or practically anything."



They ask:  "Does this apply to all fax machines?  Our research was done on HP Officejet all-in-one printers, though this was merely a test case.  We strongly believe that similar vulnerabilities apply to other fax vendors, too, as this research concerns the fax communication protocols in general."  Again, no.  It concerns the implementation of the fax communication protocols.  There's nothing that the fax communication protocol itself does that allows this to happen.  It just wasn't written right.  But it's difficult to write it right.



"Is it widespread?" they ask.  "By our estimates," they say, "there are currently hundreds of millions of fax machines still in use around the world.  Financial reports from Wall Street indicate that tens of millions of all-in-one printers are sold worldwide each year."  So it hasn't gone away.  Still very popular.  "Has it been fixed?  We worked closely with HP to fix the vulnerability; and, following the process of responsible disclosure, they managed to release a patch before this publication.  In fact, if your device is already configured to auto-update," as Joel's was, I mentioned before, "then the patch has likely already been applied," as he found.



"This patch, however, only applies to HP all-in-one printers, and the vulnerability may well still apply to devices from other manufacturers, as well."  So just again, props to HP for beginning to have auto-updating devices.  We need anything that is autonomous like a printer to either alert people that it needs patching or just to do it itself in the middle of the night when no one is using it, like our routers should be.



"What should I do to protect myself?  If you own an HP Officejet all-in-one printer, then follow the instructions from HP."  They provide a link in their Q&A.  I have a link in the show notes.  I tweeted it this morning.  "In addition, you should implement segmentation policies, software patching, and proper IT hygiene.  Please see our 'Recommendations' section," they say, "in the blog post above.  If you are no longer actually using the fax functionality in your all-in-one printer, then we recommend you to disconnect the phone line."



Perfect advice.  You know?  Yes.  Just unplug the phone line.  Or as you said, Leo, only plug it in when you need to send or you're expecting to receive a fax.  That would work, too, as an interim.  And again, non-HP combo, like connected fax devices, hopefully all those companies are working on this, too.  So do keep a lookout for your non-HP device updates.  And in the meantime you may want to take it off the network or disconnect it from the phone, rather than just leave it sitting there.



And then they ask, finally:  "Has it been seen it the wild?"  And they answer:  "Not yet.  Our research was intended to highlight a potential security risk."  And again, this isn't the sort of thing that I would expect to see.  You know, you're not going to get a botnet created from it.  It's not going to be high visibility.  It's going to be an attack in the dead of night by somebody who develops this, who weaponizes this, and then uses it to get into a company whose device is old, doesn't auto-update, isn't aware of this.  Their IT team, maybe they don't even have one, or they just aren't fixing this.  But I just think this is probably the most significant vulnerability we've seen so far this year.  Spectre and Meltdown, they were bad.  They were theoretical, difficult to do.



LEO:  Still not in the wild; right?  I don't...



STEVE:  Exactly.  And no practical attack has yet been seen.  This one, boy.  I mean, I...



LEO:  It's already in use, frankly.



STEVE:  Yeah.  There is just - it is too tempting and too easy using, you know, off-the-shelf standardized tools for bad guys to do this, to follow in the footsteps of Check Point who, in their blog post, I mean, they explained it all.  They've got screenshots and code, and they show what they found.  I mean, it's complete documentation of what HP had that was vulnerable in their machines.  And devices that don't get updated are just going to be sitting there, waiting for the phone call.



LEO:  Wow.  My friend, we have come to the end of this fine rendezvous.  Once again, you've set our minds at ease - not.  Not.  Actually, for a post-DEF CON/Black Hat episode, this wasn't too bad.  Too bad.  You didn't talk at all about voting machines.  Now, that's scaring me.



STEVE:  Yeah.



LEO:  Ladies and gentlemen, we invite you to join us for the live edition of this show.  We actually stream all the shows as we make them.  So it's a little different than the final product.  But it's a chance for you to kind of watch along with the other folks in the chatroom and comment.  We do the show every Tuesday, 1:30 Pacific, that's 4:30 Eastern, 20:30 UTC.  And you can watch it stream at TWiT.tv/live.  If you do that, join the chatroom, irc.twit.tv.  They're all watching the show, too, and talking about it, commenting on it.  If you can't watch live or listen live, you can always get on-demand versions.



Now, Steve hosts the audio at his site, GRC.com.  He also has - he's the one-and-only source for transcripts, which usually take a few days for Elaine to do, but they come out shortly after the show.  And a lot of people like to read along.  It's also a great way to search.  You can search the transcripts on his site and find exactly what you want from any one of our 676 episodes, 13 years of Security Now!, GRC.com.  While you're there, pick up a copy of SpinRite, the world's best hard drive maintenance and recovery utility.  You might also want to check out some of the other free stuff Steve does, catch up on SQRL, Perfect Paper Passwords, and more.  GRC.com.  Steve himself is on the Twitter at @SGgrc and takes questions and comments and tips there.  You can DM him.



I have audio and video of the show in case you wish to see Steve's moustache in person.  Yeah, no, it's good.  It's a good moustache.  Very high-quality moustache.  Now everybody's going to want to download the video to see it.  Or get the mug.  You can actually get the moustache mug.  So our audio and video versions are at TWiT.tv/sn for Security Now!.  The mug is at TWiT.tv/store.  Have your very own Steve Gibson mug.  Let the world know you're secure.  And let's see what else.  Oh, if you don't yet subscribe to the show, my recommendation is subscribe, you know, go to your favorite podcast app and search for Security Now!, and then press the Subscribe button.  That way you will get it automatically.  And that's...



STEVE:  And also I just will say I love hearing people's SpinRite success stories.  We've been sharing them for 13 years now.



LEO:  We need more, yeah.  



STEVE:  So GRC.com/feedback, or you can tweet me at @SGgrc.  Share your success, and I will share it with our listeners.



LEO:  Yeah.



STEVE:  Since every one just really - I'm so happy that we're able to help people.



LEO:  Okay.



STEVE:  And next week Year 14 begins.  Woohoo!



LEO:  Wow.  That's really kind of mind-boggling.  A kid who was born when we started the show would be a teenager now.  That's mindboggling, Steve.  All right, Steve.  Have a great week.  See you next time.



STEVE:  Talk to you next week.  Bye.



LEO:  Very nice.  Very nice.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#677

DATE:		August 21, 2018

TITLE:		The Foreshadow Flaw

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-677.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, as we head into our 14th year of Security Now!, we look at some of the research released during last week's USENIX Security Symposium.  We also take a peek at last week's Patch Tuesday details, Skype's newly released implementation of Open Whisper Systems' Signal privacy protocol, Google's Chrome browser's increasing pushback against being injected into, news following last week's observation about Google's user tracking, Microsoft's announcement of more spoofed domain takedowns, another page table sharing vulnerability, believe it or not "malicious regular expressions," some numbers on how much money Coinhive is raking in, flaws in browsers and their add-ons that allow tracking-block bypasses, two closing-the-loop bits of feedback, and then a look at the details of the latest Intel speculation disaster known as the "Foreshadow Flaw."



SHOW TEASE:  It's time for Security Now! as we begin our 14th year.  Wow.  Episode 677.  And it seems like for 14 years we've been talking about speculation flaws in Intel processors.  There's a new one.  Plus why regex can get you into trouble, and a retrospective look at this month's Patch Tuesday.  It's all coming up next on Security Now!.  



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 677, recorded Tuesday, August 21st, 2018:  The Foreshadow Flaw.



It's time for Security Now!.  Yes, it is.  Oh, boy.  We got cake, let me just put it that way.  Security Now!, the show where we protect you and your loved ones online, and your privacy, and let you know how it all works with this guy right here, the Explainer in Chief, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you.



LEO:  And Happy Anniversary.



STEVE:  As planned, we announced the end of Year 13 last week, so we get to note that it's the beginning of our 14th year today.



LEO:  Wow.  Wow.



STEVE:  And so this is Episode 677 for August 21st, 2018.  And there was no question that this one had to be titled "The Foreshadow Flaw."



LEO:  Oh, boy.



STEVE:  Yeah.



LEO:  It's the gift that keeps on giving.  Hey, before - we did have cake.  The funny thing is the cake is because this is the second anniversary of the day we moved into this studio, so it's also that, as well as the anniversary.



STEVE:  Interesting.



LEO:  And technically it was August 18th, 2005 that we began this show.  And in the studio with me is Theodore from Sacramento.  He is a computer science student at Sac State.  He's here for his birthday.



STEVE:  Oh.



LEO:  So there's another reason for cake.  Theodore asked his dad, he said - actually Dad asked him, "What do you want to do for your birthday?"  And Theodore said, "For my 18th birthday I want to watch Security Now!."  But what we didn't calculate, and you did, is that means he was four years old when the show started.



THEODORE:  Five.



LEO:  Five.



STEVE:  Yes, and I had hair.



LEO:  You weren't quite five yet.  You were five the day after the show started.  Four and 364 days.  And Steve, you did not.  I'm sorry, but I have to beg to differ.  I don't believe you had hair when we started the show.



STEVE:  Oh, maybe not.



LEO:  Did you?



STEVE:  Maybe my moustache was darker, something.  There has to have been some deterioration over the last 13 years.



LEO:  No.  We look exactly the same.  This show is unchanged.  And I can say that with certainty because the first 100 shows were all audio only.  So there's no proof.



STEVE:  We do know that the length has grown from 18 minutes to 120.



LEO:  Honey Monkeys was only 18 minutes.  



STEVE:  More than 10 times longer.



LEO:  It's longer because in the original show I feel like it was just news; right?  And maybe even just one story.



STEVE:  I was sort of sitting around thinking, okay, what are we going to talk about this week?  Yeah.



LEO:  Now you, like, work all week preparing this.



STEVE:  It's become a labor of love.



LEO:  Yeah.  A labor, for sure.



STEVE:  Yeah, well, so for example we're going to talk about the Foreshadow Flaw which is, as you said, the gift that keeps on giving.  As I said at the beginning of the year when we saw the first crack of this happen, the concept that speculation was a problem, I thought, oh, goodness, this is really going to be bad.  Well, I learned something that stunned me about what Intel was doing that I'm still - I still can't get over it, which we'll get to at the end of the show, which is the consequence of this flaw that just I'm gobsmacked, as they say in the U.K., over.  But we're going to also look at some additional research that was released during last week's USENIX Security Symposium, which took place in Baltimore, Maryland.



Of course last week we were talking about the Black Hat and DEF CON conferences.  These security conferences are always fabulous for interesting topics.  We're going to take a look at last week's Patch Tuesday details.  It's always happening, like during the podcast, so there's no chance to really do anything comprehensive about it.  This one was a little more frightening than usual.  We've got Skype's newly released implementation of Open Whisper Systems' Signal privacy protocol, which is welcome and interesting.  Google's Chrome browser has taken its next step with its release number 68, doing something we talked about them announcing late last year, which is pushing back against being injected into.



LEO:  Okay.



STEVE:  So nobody wants that.



LEO:  Nobody wants that.



STEVE:  Well, I guess some people.  Anyway...



LEO:  Stop right there, Steve.



STEVE:  Yes.  We have some follow-up news to last week's observation from the research by the Associated Press about Google's user tracking.  Microsoft's announcement of - just this morning I watched an interview, it happened by Brad Smith at Microsoft talking about taking down six more web domains.  We've got another page table sharing vulnerability which is not the foreshadow flaw, but this harkens back to a variation on the Rowhammer attacks that's not Rowhammer.  So we'll talk about that.  And believe it or not, Leo, the emergence of malicious regular expressions.  Yes.



LEO:  What?



STEVE:  The ReDoS, Regular Expression Denial of Service.  So we will talk about when regular expressions go bad.  I also have some numbers, thanks to some research, actually it was one of these USENIX guys, on how much money Coinhive is raking in with their kind of questionable tactics.  Some flaws in browsers and their add-ons, which are allowing attempts to block tracking to be bypassed.  A couple of closing-the-loop bits of feedback with our listeners.  And then, not in 18 minutes certainly, hopefully we'll squeak this into 120.  We'll look at some details of the latest Intel speculation disaster known as the Foreshadow Flaw.



LEO:  At least the name is creative.



STEVE:  Oh, it's got a good logo.  You can download - you can get the logo...



LEO:  Oh, I'm looking for that in any flaw.  It's got to have a good logo.



STEVE:  You can get the logo in three different sizes, as a PNG or an SVG, whatever you want.  So, yeah, it's got some good marketing behind it, and its own web domain, of course, TheForeshadowFlaw.eu.  So it's like...



LEO:  Oh, lord.



STEVE:  ...okay, got to have all that these days.



LEO:  Registered trademark.



STEVE:  So I have a bunch of these pictures which they're lovingly sent to me when they're discovered in the wild by our listeners.  And I just get, I mean, they're simple to parse visually.  Just a constant source of bemusement because you think, okay, I mean, I don't get it.  So just for those who are listening and not seeing, we have a very nice-looking, modern, high-end, push-button entry combination door lock with the 12-key pad, star and pound sign and one through zero.  And prominently displayed on the top, which anyone except maybe a toddler would be able to see, it says:  "Passcode:  4143#."



LEO:  It's good they put the pound on there.  I might forget to press that button.



STEVE:  Yeah, wouldn't want to forget that.



LEO:  It's obviously not a secure facility, I hope.



STEVE:  I just really don't - I don't really get, you know, somebody must have had ambition for security at some point, but it was just too pesky.  You know?  It was like, okay, what's that code?  You know, and the cleaning people couldn't get in, and the fire department was complaining because they need to have entry.  It's like, okay, what, you know, forget about this, let's just - but, you know, they didn't remove it.  They just said, okay.  Well, I guess that gives them the option of peeling the tape off and changing the code, or just changing the code.  I guess if you left the wrong code on, that would be even more security.  I don't know.  Anyway...



LEO:  Yeah.  You don't know that that's the right code.



STEVE:  That's true.



LEO:  I mean, maybe they're smart.



STEVE:  That could really be tricky.  Or the secret could be do it backwards.  It's actually #3414.



LEO:  Or, you know, ROT-13 or something.  You know, do a little transformation on that.



STEVE:  Anyway, I'm tempted to do a weekly series of these for the foreseeable future because I have a bunch of them.  And they're all different.



LEO:  Everybody sends them to you.



STEVE:  They're all different.  And they're just like, okay.  You know, really, what is the story?  I just don't get it.



LEO:  I love it.



STEVE:  That's like, okay, you know...



LEO:  I wish we'd had you with us yesterday.  We shot a piece for The New Screen Savers in an escape room.  Have you ever done escape rooms?



STEVE:  No.



LEO:  It's basically a giant puzzle.  So this one is a team building escape room in San Francisco called Reason.  And the idea is - we were just five people.  It's built for 10 people, but we were five people.  You go in.  They lock the door.  And you have an hour and a half to get out.  And in this case the scenario was a nuclear reactor is scrammed, and it's going to explode in 90 minutes unless you can figure out how to stop it.



STEVE:  Ah, not good.



LEO:  And it's all these - and there's Arduinos.  There's VR.  There's drones.  There's all these technologies in this small couple of rooms.  Actually, there's one room, and then once you figure out some puzzles another door opens, and you get another room.  And it's puzzles like, you know, there's code.  And it's really fun because it's mathematical.  It's visual.  And we could have used your brain, frankly.  Because we got out...



STEVE:  And communication among all the participants.



LEO:  That's why it's team building; right.  



STEVE:  Right.



LEO:  Because, oh, I got it over here.  And some people are better at some things, and so you have to figure out who's good at this.  The best team got out in 48 minutes.  We got out at 98 minutes and four seconds, 90 minutes and four seconds.  It had blown.



STEVE:  Ooh.  



LEO:  We got out, literally, it blew, and the door opened.  We were that close.  So it was fun.  Next time you're in town - I know you want to come up for SQRL - we'll take you.



STEVE:  Yeah, yeah.



LEO:  Because you would have probably got us out in 48 minutes.



STEVE:  Would have been fun to interact with everybody.



LEO:  Yeah.



STEVE:  So we have last week's second Tuesday of the month, famously now forever branded Patch Tuesday.  And again, as I said at the top of the show, it's happening as we're doing the podcast.  So there's never really time to go in-depth.  However, last week we had Microsoft releasing patches for 60 - that's not 16, that's six zero - flaws, two which were actively being attacked at the time of release.  I say this because we have a grab bag of problems here.



And I think probably - I know that there are people who are annoyed by the updates, and they're like, I don't want to reboot my computer now.  Certainly enterprises, as we've been talking about, there was that survey that was taken recently by the gal who is an MVP for Microsoft who asked people, how are you feeling here about all of these updates?  And people are grumbly about them.  And we know that enterprises have been hurt by rapidly applying them without giving them sufficient testing.  They feel like they want to test them themselves.  Anyway, so sometimes the months are a little more important than, you know, some months of patches are more important than others.



In this case, of the 60 flaws, 19 were rated critical by Microsoft and encompassed Windows, Edge, IE, Office, the ChakraCore, .NET Framework, Exchange Server, Microsoft SQL Server, and even Visual Studio.  All of them - and I've got to ask how Visual Studio is in there.  But all of them allowed remote code execution when successfully exploited.  So 19 remote code execution vulnerabilities.  Those two that were known to be publicly exploited in the wild at the time, and that is to say also still workable on any still unpatched machines. The first one was an IE memory corruption problem which affected IE 9, 10, and 11 on all versions of Windows - probably, you know, they don't talk about XP anymore, but 7, 8.1, and 10, and all of the server versions, which allows remote attackers to take control of any unpatched as of last Tuesday or subsequently, vulnerable system by convincing users just to view a specially crafted website through IE.  So not good.



In its advisory, Microsoft noted that an attacker could also embed an ActiveX control marked as safe for initialization in an application or a Microsoft Office document which is able to carry those, that hosts the IE rendering engine.  And so that would be a way of not directly, but rather indirectly, getting IE involved, even if it's an Office document or something else.  So anyway, so that allows a bad guy to run code on your machine.



The second publicly known and actively exploited flaw resides in the Windows shell as a consequence of improper validation of file paths.  And, boy, again, we're always talking about file path exploits and validations.  We talked about the famous ..\..\..\ as a means of backing up, like above the level where you're starting in the directory hierarchy in order to get up to mischief.  And that's been a problem for web browsers and for operating systems for a long time.  In this case, this flaw, which they don't go into any further detail, but it is now being exploited, allows once again arbitrary code to be executed on targeted systems by convincing victims to open a specially crafted file which they receive via email or on a web page.  So those two are known to be being used right now.



That leaves 17 other known remote code execution flaws which they fixed and, as far as they know, nobody is currently exploiting.  SQL Server, both 2016 and 2017 versions, are both vulnerable to a buffer overflow vulnerability that can be exploited remotely by an attacker.  However, they have to be able to somehow submit a query to an affected server.  So that seems, I mean, if you've got your SQL Server query port wide open on the Internet, accepting unauthenticated queries, you've got bigger problems than that.



LEO:  Seems slightly dangerous.



STEVE:  They'd have to somehow figure out how to get access to that.  But still, we know that hackers are devilishly clever, so you don't want that to be left unpatched.  Windows 10 has a PDF-based remote code execution vulnerability when Edge is left as its default browser, which again allows Windows 10 to be compromised by a user merely visiting a website that hosts a malicious PDF.  And Microsoft notes that malicious ads can be hosts of content that can trigger this vulnerability.  So they're not saying that they know of this happening, but they're glad that they got this patched, and they would like everyone to fix it.



Exchange Server has a vulnerability, remote code execution, which is worrisome since it would support attacks targeting specific enterprises known to be using Exchange Server 2010, 2013, or 2016, all of which were vulnerable until last week.  It's a memory corruption vulnerability which allows a bad guy just to send email to the Exchange Server.  So it's not a question of a port being open in this case.  Everybody's advertising their port with their MX record, and DNS saying please send us email here.  And if a bad guy does that with a cleverly formatted piece of email, they can obtain control of the enterprise's Exchange Server.  So if anybody is listening and has been deferring updating, this is CVE-2018-8302.  You've going to want to make sure you're patched for that one because it's particularly useful for targeted attacks.



It turns out that all of Microsoft's currently supported versions of Windows, meaning 7, 8.1, and 10, as well as Servers 2012 and 2016, all share the same GDI, the Graphics Device Interface subsystem, which has a vulnerability in its font processing library due to improper handling of specially crafted embedded fonts.  Websites are able to ask browsers to download and render fonts on the fly.  That's a nice feature for websites.  But when your font renderer cannot be trusted, then it creates a vulnerability which can be exploited.  And that's the case here.  It's possible for a malicious font to be invoked to cause your browser to download this malicious font which a malicious website hosts.  Or maybe bad guys could like sneak the font into wherever the website is pointing to, and so it wouldn't be the site's fault, technically.  It would be a bad font got in.  And then it could compromise your system.  And lastly, believe it or not, what is it, 2018?  How old is Theodore?  He's 18?



LEO:  He's 18, yeah.



STEVE:  When he was five, we were having problems with link files, with Windows shortcut link files.  He's now 18, and we're still having problems with those pesky .lnk shortcuts.



LEO:  You'd better get on this, Theodore.  See if you can fix that, yeah.



STEVE:  I just wish we would stop screwing around with these operating systems.  Just leave them alone.  Because we don't seem to be making any real progress here.  A remote code execution vulnerability exists.  This is one of those 19 serious top-level problems in all versions of Windows that allows remote code execution if a .lnk, a link shortcut file, is processed.  An attacker could present the user a removable drive or a remote share that contains a malicious .lnk file.  Or, you know, maybe a thumb drive that's got something on it.  It's like, oh.  And again, we're still having problems.  As a consequence, the link file can, just opening the drive, when the operating system looks at the .lnk file, just enumerates it, that causes it to be able to run an associated malicious binary which will execute naturally that binary of the attacker's choice on the target system.  So even before a week ago, it was still the case on all versions of Windows and Windows Server that plugging a thumb drive into your system that would allow the system to look at it could cause a takeover.  So again, throughout Theodore's entire life this has been a problem, and still is.



LEO:  He's weeping.  He's weeping right now.



STEVE:  Well, no.  He's in computer science, so he's got a great future ahead of him.



LEO:  Yeah, exactly, yeah.



STEVE:  That's right.  He might have worried when he was five that, oh, by the time I'm old enough...



LEO:  There'll be nothing to do.



STEVE:  There'll be nothing left.



LEO:  Nothing left.



STEVE:  Not to worry, Theodore.



LEO:  It'll all be solved.



STEVE:  That's right.  The podcast just keeps getting longer and longer as we struggle to deal with what happened just in the previous week.  Speaking of which, not to be left out of any good Patch Tuesday, we have Adobe released security patches for 11 vulnerabilities, two of which were rated as critical, for Acrobat and Reader.  Of course we've got Flash Player was among them.  Creative Cloud Desktop and Experience Manager were the other apps that were affected.  Good news is in this case none of the 11 things that were patched were either publicly known nor known to be actively exploited in the wild.  So these were responsibly disclosed.  However, I would argue that the real responsible thing to do would be just to stop, just like say goodbye to Flash Player because it's just so optional now.  Although we did have the listener who said, you know, our company needs it to be embedded in PDFs.  Ouch.  Okay.  What could possibly go wrong?



So there was a bit of nice news, which is that Skype - Microsoft of course now owns Skype.  And I was a little nervous, Leo, when I turned my Skype machine on this morning because...



LEO:  You never know, do you.



STEVE:  You never know.  And it would really mess up my Tuesday and yours.  Because this time I got, for no reason that I could tell, the big dark screen UAC, like click this to approve changes.  And I went, like, what?  Just turning the machine on, which I've never seen before just turning the machine on.



LEO:  Yeah?



STEVE:  So I looked at it closely, and it said Microsoft's Skype.  And I'm like, oh, no, no, please.



LEO:  Nooooo.



STEVE:  I mean, what could I do?  I gulped, and I said okay.  And it kind of rummaged around for a while and made a lot of noise, and then everything settled down, and here I am.  So once again.



LEO:  Whew.



STEVE:  However, what they announced last week was their support, which had been announced at the beginning of the year, and at the beginning of the year they announced their relationship with Open Whisper Systems.  In fact, I have a link in the show notes to the Signal.org blog about the Skype partnership with Signal.  And, you know, this surprised people at the time because it was like, wow, really?  Microsoft's going to add end-to-end encryption to Skype.  And it must be that it's because everybody else has.  You know, WhatsApp now supports Signal.  Google does.  Facebook does.  There's even Signal now on apps in our various mobile devices and a desktop version.



So anyway, the good news is, in Skype, you click on the Chat + button.  And I did yesterday because I was like, oh, really?  The third item in the menu is Start Private Conversation.  And this allows you to, as you would expect now that Skype supports so-called private conversations, although Microsoft apparently didn't really announce it, it got noticed in the wild that it was now there.  It is the Signal protocol which Skype now supports.  In Android, the Skype Private Conversations appeared at the beginning of the month, on August 2nd, for Android, as I mentioned, in 8.15.0.306.  And Windows Skype received it just at some point in the last few days, 8.28.0.41.  And when I looked, I think I was like .6 something.  So I missed a few incrementals.  But whatever it was that I clicked on this morning when I fired up my Skype machine, who knows where I am today.  But it is present.



And I'll just say that I'm always a little uncomfortable when explicit key management is taken away from the user and is performed by the system.  I mean, I think it is a rational tradeoff given that no typical user wants to be bothered with key management.  We've talked about how Apple's iMessage system is cryptographically strong, but it inherently supports multiway messaging, and the user is not managing the keys for the recipients to the chat themselves.  Which does allow Apple, if they were compelled to, to insert a key for an additional party that would then be participating silently in messaging.



So that's, I mean, again, is it an absolute problem?  Well, it's a theoretical problem.  But in reality, I mean, staying a little bit grounded, it's so trivial, as we know, to capture the keystrokes before or after the encryption, that is, like outside of the encrypted channel, that what anybody using these systems should consider is that what they are providing is really, really strong proof against a network-located attacker.  That is, when we began talking about, really, when all of the issue for privacy ramped up with the Snowden revelations and the idea that the NSA had big data sucking node taps all over the Internet, and it was funneling virtually all of the traffic off to some farm somewhere for permanent storage.



So that's where the real awareness of the need for HTTPS began to happen, and then we learned even that's not safe because of the lack of perfect forward secrecy that allows a key that is subsequently revealed to be used to go back and decrypt what the NSA had been storing all these years.  So now we're getting better about that.  But in the case of Signal, we have a really robust, on-the-wire system that protects against that.  But users should remember that the protection is not absolute.  That as we talked about with Android last week, we sort of pounded on Android a bit with all of the various - the man-in-the-disk attack and such, where it is still very possible for the endpoint to be exploited.  Crypto technology, we've got that nailed now.  We can do bulletproof crypto.



But the NSA and law enforcement and other agencies realized, well, fine.  Let them encrypt as it goes from point to point.  We can still stick a tap in outside of that encrypted tunnel.  So somebody who's really interested in seriously having untappable communications needs to think about having a dedicated device where they rigorously refrain from any other use of a dedicated device except that.  I would get an iPhone with a known history, maybe brand new, and just not use it for anything else.  Install Signal.



LEO:  You would use Signal?  You wouldn't use Apple Messages, obviously.



STEVE:  Yes.  I would use Signal.  I think, you know, exactly.  Because if you use the Signal app from Signal, then that's as good as you're going to get.  They're taking advantage of the security in iOS.  Again, you don't want to just, like, resist all temptation.  I mean, it would be your secure comm platform if you really absolutely care about security.  Because as far as we know, Signal is open source.  It's open protocol.  Thoroughly vetted.  It's really been scrutinized.  And so you want to put it on a platform that hasn't been contaminated.  I noted that Signal is available for Windows, macOS, and Linux.  And right now...



LEO:  There's also a Chrome extension you can use, which is nice.



STEVE:  Yeah, I think, exactly.  Although where I'm headed with this is that at some point it will become part of the Linux distro.  It's probably not quite yet.  It's hosted on something that's a little worrisome, which makes it only available for two of the different types of Linuxes.  But my point is once it became bound into the turnkey distro, you could create a bootable CD.  And that would be a very strong security way of doing messaging, if you needed to, to boot a CD that you trusted on a machine that you had some reason to believe, I mean, as we know, there are even ways to infect the boot process if somebody was really, really being targeted.  But that would be a way of getting a clean OS boot that already had a really state-of-the-art secure messaging technology built right into it.  So I can't think of any real practical way of being more safe and secure than that.  And it's got to be safe enough.  And speaking of Google's Chrome browser....



LEO:  Yes?



STEVE:  They are becoming - as I said, no one wants to be injected into, Leo - becoming more proactive against code injection instabilities.  Last year we talked about Google's announcement to developers that they were going to start moving toward making Chrome increasingly intolerant of having code and hooks injected into its browser processes.  As we know, software is named.  First there was hardware.  And then it was like, oh, look, software.  It's malleable.  It can be changed.  So hooking is a longstanding practice when using some sort of add-on like antivirus facilities which need to access some of a system's internals that the system is not explicitly exposing via an API.  Back in the early days of Windows, the early firewalls, the personal firewalls, Windows was firewall hostile.  Like ZoneAlarm and other early firewalls had to go in and hook the OS.  They installed drivers that went in and modified entry points to the OS itself that gave them the access that they needed to do their function and still let the OS operate correctly.



Now, of course, the problem is rootkits do the same thing.  They hook the OS in order to hide themselves and do nefarious things.  So you could have well-meaning hookers and malicious hookers of API entry points.  The question is does the system want to tolerate them?  And increasingly, we're seeing them being abused.  And also there are inadvertent problems that are causing crashes.  And this is what Google is arguing.  This is the way Google is arguing is that not all companies are equally adept at going in and essentially modifying the Chrome internals in such a way that they don't cause side effects.  And then there's the problem of Google trying to update Chrome, which may change the nature of some of the things that, for example, an antimalware program is hooking, thus causing a crash.



So, okay.  So at the beginning, in April of this year, Google, as they said they would, so they gave developers plenty of notice, they began alerting users after a browser crashed which was they believed caused by a third party who had injected code, that the user would get a notice saying to update or remove problem applications.  And then it said - that was the title of a dialogue:  "The following application could be preventing Chrome from working properly."  And then it would name the application that it had determined had caused Chrome to crash.  So they know that they're being hooked into, and they're tolerating it at the moment.  But they're moving away from that posture.  And so developers have been given notice.



Now, last week, with the release of Chrome 68, actually it was last month, the next phase of this gradual tightening has continued.  Now the advisory nature of the previous warning has been elevated to a commandment.  They said "update or remove incompatible applications."  And in some of the notices that I've seen samples of, Malwarebytes was mentioned, and BitDefender Total Security.  So there are a couple user-added add-ons which have caused Chrome problems, which should be put on notice that their behavior is not going to be tolerated much longer.



The next phase of tightening is slated for January of 2019, when Chrome 72 will be released, and it will begin automatically blocking third-party code injection.  So these things will become incompatible with Chrome.  In their coverage of this, Bleeping Computer wrote - I think it was Lawrence who posted this article.  He wrote:  "Since this feature was enabled in July" - that is, the next level - "there have been an increasing number of reports about antivirus software being listed as incompatible applications by Chrome.  Some of the antivirus applications," he says, "that we have seen reported as incompatible include Malwarebytes, BitDefender, Eset, Emsisoft, AVG, IObit, and Avast."



He said:  "Strangely, there are many other programs that are also being listed as incompatible applications such as TortoiseGit, TortoiseSVN, Stardock, Acronis True Image, Dropbox, FileZilla, Acer Power Manager software, and RocketDock.  While antivirus software," he says, "I can understand being listed, some of these programs are a bit surprising."



On the flipside, I would argue that to me they're not so surprising because things like Stardock and RocketDock, they're like adding browser enhancements, in a way, which goes beyond just being an add-on.  They're obviously hooking inside.  According to a Google developer who posted to a Google Help Forum regarding these alerts, he said they have no way of determining which programs are innocently injecting code or which might be malicious, as I mentioned.  Rootkits do this, too.



So in his posting he said:  "Chrome dev here.  This is related to a new feature that aims to prevent third-party software from injecting into Chrome's processes and interfering with its code.  This type of software injection is rampant," he writes, "on the Windows platform and causes significant stability issues," he says, parens, "(crashes)."  He says:  "The Microsoft Edge browser already does this kind of blocking, and we are in the process of making Chrome behave similarly.  What you are seeing is the warning phase of a year-long effort to enable blocking, originally announced in November of 2017."



He says:  "Since it is effectively impossible for Chrome to automatically determine whether any particular piece of software is innocently injecting or purposefully" - that is to say maliciously - "injecting and interfering with Chrome code."  He says:  "To keep things simple we warn about all injected software without making value judgments.  Note that soon we will actually start blocking software from injecting, at which point this warning will cease."  Well, because no injections will be possible.  He said:  "Note you should only be seeing these crashes if you manually navigate to the" - and then he has in his posting "chrome://settings/incompatibleApplications" is the special URL under Settings.  "Or on a startup after the Chrome browser has crashed.  Additionally, this feature is currently considered experimental; so not all users will see these warnings."



So I'm now on Windows 7, and I'm glad to once again have Windows Defender's built-in real-time protection watching my back.  In my opinion, Chrome should block modifications to its internals.  This is where we're headed in the future.  And we're moving past the point where programs will and ought to allow themselves to be injected into for third-party modification.  But it also suggests that those apps which don't offer features to allow third-party AV tools, for example, to obtain the needed access that they believe they should have, are going to have to reciprocate.  Like, for example, if Chrome won't allow itself to be modified in a way that AV has been, then Chrome needs to publish APIs that allow AV tools the access that they need.  Otherwise Chrome becomes unsafe if there isn't a way to provide that kind of antiviral scanning of things you download.  I have to think, though, that, for example, Windows Defender's built-in real-time protection is providing protection against everything coming through, including what Chrome is doing.



So anyway, for the time being we're going to see an escalation.  And eventually these tools, at least as they are today, will be banned from getting in and mucking around with Chrome as they are now with Edge.  And as I think all apps need to be in tomorrow's future structure.



LEO:  Yeah.  I'm almost surprised you can get to the internals.  But I guess that's the nature of a browser?



STEVE:  Well, it's the nature of Windows.  For example, any time you allow a debugger, you're allowing something to put debugging code in your process and go in and stop you and single step.  So, for example, Visual Studio you're able to attach to a process and debug it.  Which has always struck me as, like, oh, okay.  What that means is bad guys can, too.



LEO:  Right.



STEVE:  So I did want to follow up.  This is sort of predictable, I suppose.  And that is that somebody decided, someone being opportunistic has filed a class-action lawsuit against Google over the...



LEO:  I saw that.  Oh, shoot.



STEVE:  I know.



LEO:  I spilled my coffee.



STEVE:  Oh, okay, I'll keep going.



LEO:  I got so excited about this.



STEVE:  So last week of course we talked about how Google made it less obvious than you could argue it could be to disable tracking.  I noticed you brought that up the following day on this week in Google with Jeff and Stacey.  And they sort of concurred with this and thought, yeah, this is not...



LEO:  They didn't think it was malicious, but they probably didn't - weren't as forthright as they could have been.



STEVE:  Yes, yes.



LEO:  As engineers they thought, well, everybody knows what this means.



STEVE:  Right.  



LEO:  Probably not.



STEVE:  Right.  And of course it turns out that the Electronic Privacy and Information Center, who are sort of watchdogs, EPIC, they sent a three-page letter to the U.S. Federal Trade Commission following up on AP's research, noting that Google's what they called a "deceptive trade practice" is in clear violation of the 2011 settlement with the FTC.  EPIC wrote:  "Google is not permitted to track users after they have made clear in their privacy settings that they don't want to be tracked.  The FTC's failure to enforce its Consent Orders places American consumers at risk.  The Commission's inactions have made the Internet less safe and less secure for users and consumers."



Okay.  Well, so, I mean, I'm happy that AP did the research.  I would agree that if you say turn off location tracking history, that ought to be global and not for part of what Google is doing but not for other things.  And as I mentioned, some guy in San Diego has filed a class-action lawsuit arguing that we should all hurt Google as a result.  It's like, well, okay.  I'm sure Google will address this and fix it.



LEO:  They did.  They already changed their statement.  So the privacy statement is now clear.



STEVE:  Yes, I did see that they had amended their statement.



LEO:  The issue comes down to, in my opinion, tracking kind of implies that we're going to do consistent tracking of your location. But if you're using the map, and you want the map to work, they're going to have location information.



STEVE:  But they don't have to have location history.



LEO:  Right.  But it does have to send back to the servers your location.



STEVE:  But not store it in perpetuity.



LEO:  Yeah.  Right.



STEVE:  And that was the problem is that it was creating a log of every - for example, in the research the AP did, there was a multi-week record of everywhere...



LEO:  Right.  But in order for that to happen, you had to launch the map app or some app that required location information.  If you didn't launch this app...



STEVE:  Well, he turned off "I don't want location history."  And so the argument is that's a clear expression of intent.



LEO:  There is another setting, though, that says "turn off location information."



STEVE:  Well, it's web apps and sites or something.  Which isn't clear that it's what it's doing.



LEO:  Right.  It needs to be clearer.  But if you use a map, and the map has to get data, it's going to send back to Google where you are.  And that's just logs.



STEVE:  But it's not - it doesn't need to store it, and that's what it was doing.  It was creating a perpetual log of everywhere he'd been.



LEO:  Only when he launched those particular apps, though.  That was the key.  You have to launch the app that takes the location information.



STEVE:  And why does it need to store it forever?



LEO:  Right.  Well, I don't know the internals of it.  Maybe it doesn't.  Maybe it does.  I mean, it's going to log that.



STEVE:  It was.  It was storing it.  And so against his preference to not store location history.  Location history is where you've been.



LEO:  Well, we'll see what the court says.  My suspicion is they'll say, well, then you shouldn't use anything that requires location.



STEVE:  Leo, location now and location yesterday are different things.



LEO:  If they have server logs, that's going to log it.  I mean...



STEVE:  No.



LEO:  No?



STEVE:  Not if you're able to turn off a setting, and then it doesn't. 



LEO:  Right.



STEVE:  You're able to turn off a setting, and then it doesn't.  That should have been inclusive in turning off location.  I'm not letting you off the hook on this one.



LEO:  That's all right.  No, no, no, no.  What I don't know is, and I'd love to hear Google's explanation of why they need that information after the location requests.



STEVE:  But they want it.  They want it.  We know they want it.  And I wouldn't argue that it doesn't improve your experience.  It probably does.  They probably offer you additional, oh, look, you're over here, or you're back here again, I mean, I'm sure they're leveraging it in some useful way.  But the fact that you can turn off that setting and maps still works demonstrates that you can turn off long-term storage.  And the argument was one setting, turn off location history, that should have done both, essentially.



LEO:  Right, right.  It should just be dumb about your location.



STEVE:  Yeah.  Know where you are now, not remember where you were yesterday.  If you said I don't want you to remember where I was yesterday, please, they'll go, oh, ouch, but okay.



LEO:  Right.  The issue is really there's two different settings for location.



STEVE:  Right.



LEO:  I agree with you it needs to be clearer what they're storing.  And of course Google has that consent decree, so I think that they have a legal responsibility to be clearer. 



STEVE:  Yeah, I think they're probably going to - they're going to change it.



LEO:  Yeah.  Well, they did change - I'm trying to find the new - we've been updating the explanatory language about location history.  I'm trying to find the actual language, but I don't see it here. 



STEVE:  I did encounter exactly what you're talking about when I was researching it, and it made it a little clearer.  I think what's going to end up happening is someone says turn off location history, that'll just be - it'll be a blanket removal of that history.



LEO:  Right.  As it should be.  I agree.



STEVE:  Yeah.  And they could pop up something that said, oh, wait, wait, hold on.  Do you really want this because blah blah blah, and the user says yes, that's why I'm turning it off.  And that's like, oh, okay, fine.



Anyway, Microsoft is I guess getting really proactive.  They're up to, I have it in the show notes here somewhere, 84 fake websites that have been created by that APT28 group.  It happened that I just caught an interview this morning with Brad Smith, who was talking to Andrea Mitchell.  It was his first interview about this because this news was just a few hours ago.  He's of course Microsoft's President and Chief Legal Counsel.  They announced that they had taken down six Russian-sponsored fake websites intended to spoof the U.S. Senate and conservative organizations.  There's my-iri.org, hudsonorg-my-sharepoint.com, senate.group, adfs-senate.services, adfs-senate.email, and then also one Microsoft site or spoof site, office365-onedrive.com.



So Microsoft has the DCU, the Digital Crimes Unit.  And it's interesting to me in the news that it took as long as it did.  They disabled the fake websites last week after obtaining a court approval last year.  So I don't know, you know, I guess it's a function of where the domains were registered and working through diplomatic channels and international treaties and who knows what, depending upon where these sites were registered.  But Microsoft believes that they were able to seize the fake domains which they were able to verify were created by APT28.  And this is that multi-named hacking group Strontium, Fancy Bear, Sofacy, Sednit, and Pawn Storm.  Various security firms have named them different things as they've tracked these guys down.  They're all believed to be this single APT28 group.  And Microsoft has used the courts about 12 times now in the last couple years since 2016 to remove a total of 84 fake websites.



So, you know, and we talked about this last month during the Aspen Security Forum.  Microsoft's VP Tom Burt said that the company had taken down that other fake domain registered to APT28, or registered by APT28, which was targeting three congressional candidates.  So, you know, it really looks like Russia is involved in messing with our country, and it's good that Microsoft is being vigilant.  And I'm sure that the attention has been turned up.



Okay.  So a couple years ago we discussed a very clever Rowhammer attack which leveraged the fact that, especially in virtual machine environments, but also sort of in Windows in general, where memory managers found identical pages of memory, they would coalesce the memory so that different processes, or in this case a couple years ago different virtual machines, were reusing memory that was identical.



So, for example, you have a virtual machine environment hosting a bunch of different OSes.  And the OSes, if they're the same version, or even, well, if the OSes are the same version, they're going to have huge amounts of their own code which is identical, that is, between virtual machines.  So it makes a huge amount of sense for the virtual machine manager, as they do, to in the background sift through the memory of the various virtual machines they're hosting, looking for identical pages, that is, where separate virtual machines have each allocated memory and filled it with the same content.



If found, there's no reason for the page tables - which are basically tables of pointers from the logical addressing to the physical addressing, and we'll be talking about this a lot here at the end of the podcast because that's the nature of this new Intel nightmare - the page tables where they're pointing to different physical memory containing the same contents, as long as those are marked "read-only," there's no reason not to alter one of the tables' pointers to point to the same physical memory as the other virtual machine, which then leaves one of the two copies of identical memory with no one pointing to it.  So it could be released back to the virtual memory pool.  And it turns out that this strategy is very prevalent because it results in huge physical memory savings allowing the same server to host a great many more instances of virtual machines where there's a high incidence of code reuse.



Well, back in the early days of Windows, memory was expensive and scarce.  So Microsoft came up with this whole DLL concept, Dynamically Linked Libraries.  And Leo, how many man years have been spent in DLL hell, as it's called, because even though you'd have ole32.dll, but then Microsoft would revise that and create a new version, but one of the apps that you had was assuming the old version, and suddenly now you had two versions, it wasn't compatible with the new one, I mean, it just ended up being a mess.  The concept was identical, that is, if an application in Windows was composed of a whole bunch of these DLLs, and they came from a common source, then just point to the same DLL.  Don't load the DLL again into new memory because the DLL is a library which is read-only by its nature.  It's providing a bunch of functions that can be called.  So let's reuse it.



So it turns out that that's today's world in Windows, which is sort of a microcosm of what's going on in the virtual machine environment.  Well, there's another security conference in Vegas which just wrapped up last week called BSides, just the letter B-S-I-D-E-S, which is a - it's an interesting privacy and security conference.  There's no entry charge.  They don't want to, like, create any barriers.  They ask for volunteers.  There's voluntary corporate support for it, and it's all free.  There's a limit to how many people that they're able to get in, so you have to sign up early, and they'll give you some preference if you are with someone who's sponsoring the show or if you volunteer your services and so forth.  Anyway, it's another security and privacy conference, much like Black Hat and DEF CON, called BSides.  It's been going on for a long time.  



So the researchers with enSilo Cybersecurity revealed and responsibly disclosed to Microsoft a new hack that is effective against the Windows virtual memory page table management.  So it's very similar because they came up with a new way of abusing the fact that multiple processes in Windows are sharing the same code.  That is, pointing essentially at a single instance of physical memory.  Given that that's the case, and it typically is for all of the highly used DLLs, ole32, user32, what they look for is a flaw in the code, and unfortunately we know that many exist, where the code is instanced into multiple process images, as happens with Windows DLLs by design, even though you could argue these days memory is, you know, like DLLs are of little use and more problems than they are a solution.  And memory is no longer scarce the way it used to be.  Still, it's a system that is just legacy.



So what they found was that by finding a vulnerability in one of these shared DLLs, they're able to use the DLL itself to attack, not surprisingly, another process which is also sharing the DLL.  And you could argue that probably, I mean, like everything running in the system is sharing user32.dll and probably ole32.dll because those are so highly used.  Anyway, their talk demonstrated the problem.  They had responsibly disclosed it to Microsoft.  It's not clear, however, what Microsoft does about this.  I mean, it does require that a vulnerability be found in those shared DLLs.  On the other hand, Microsoft just fixed 60 vulnerabilities.  Many of them were not remotely exploitable, but 19 of them were critical vulnerabilities.



The only thing that would clearly remediate this would be Microsoft sort of doing what Intel may end up being forced to do, which is changing the architecture so that there is less sharing than, I mean, this is sort of Spectre-esque in that it comes from sharing resources across security boundaries.  That's exactly what is happening here with this enSilo.  They called it "Turning Tables," as in turning the page tables.  And it does create a new way of attacking Windows.  I don't know how Microsoft mitigates it because, again, it requires a vulnerability, but it allows you an awful lot of power if you can find one.



And Leo, I knew when I was writing this up, this next piece, that you were going to get a kick out of it because how can regular expressions cause problems?



LEO:  Well, you know I'm a great grep freak; right?



STEVE:  Yes, yes.  So during last week's 27th USENIX Security Symposium, which I mentioned had been held in Baltimore, Maryland, a group of researchers updated the world on the state of the art in what they named, actually it wasn't them who named it, it was named in 2012, the so-called ReDoS attacks against servers.  So although the potential for these attacks has been known and appreciated since 2012, so for the last six years, the significant increase in server-side JavaScript on websites, for example Node.js is a major culprit, over the past six years has made their impact much more significant.  So ReDoS stands for Regex Denial of Service.



LEO:  Oh, man.



STEVE:  I know, I know.  It involves remote attackers deliberately submitting super hairy strings.



LEO:  Oh, yeah.



STEVE:  Which present worst-case complexity for regular expression parsers.  Here's what Wikipedia has to say about ReDoS attacks.  Wikipedia writes:  "The regular expression denial of service is an algorithmic complexity attack that produces a denial of service by providing a regular expression that takes a very long time to evaluate.  The attack exploits the fact that most regular expression implementations have exponential time worst case complexity," which is to say that the time required can grow exponentially relative to the length of the string that's provided.  So an attacker, consequently, can "cause a program to spend an unbounded amount of time processing by providing such a regular expression, either slowing down or becoming unresponsive."



So welcome to the world of malicious regular expressions.  We ought to stop for a second and synchronize our listeners.  I remember, I think I first encountered regular expressions, well, obviously a long time ago.  But I think it was Perl where, of course...



LEO:  They are the best grep tool ever, yeah.



STEVE:  Yes, with great abandon.  A regular expression, just it curls the toes of a programmer who loves code because they are so powerful.  Essentially it's an ASCII shorthand for specifying patterns and pattern matching and substring replacement, I mean, it's very much like Perl.  It's just like sort of a Swiss army knife thing.  But the idea is you can do things like you can say find an expression, find a substring in a longer string where the first character is A.  Then there's one or more non-A's.  Then between two and three B's, but not followed by a C.  I mean, believe it or not, you can express that.



What I just said can be expressed with this funky regex, regular expression, language.  And every time I've been using regular expressions, I just think to myself, boy, I'm sure glad nobody ever asked me to create an implementation of a full regex parser because that's just got to be a nightmare.  I mean, imagine the code which is able to accept that, an alphabetic with special characters and things statement that understands it, and then on the fly essentially builds a little machine that then processes the string you give it in order to find it.  Oh, and you can say, once you've done that, find the longest substring in the string for which that is true.



LEO:  That's called "greedy."  That's a greedy regex.



STEVE:  A greedy match, exactly.



LEO:  Here's an example of a very common regex that's just for phone numbers.  This is how you would match a 10-digit phone number.  Actually, more than 10 digits because it does the country code, as well.



STEVE:  Okay.  Read that for our listeners, Leo.  Just to give them a sense.



LEO:  I think I can.  I'll do a dramatic read.



STEVE:  Oh, please.



LEO:  Caret parenthesis backslash plus backslash D bracket one comma two bracket backslash S close parenthesis.  Oh, I'm just beginning.



STEVE:  Oh, yeah.



LEO:  Question mark?  Backslash parenthesis question mark backslash D.  Brace three brace backslash paren close paren question mark.  Open bracket backslash S dot minus bracket backslash D.  Open brace three close brace.  Open bracket backslash S dot minus close bracket.  Backslash D open brace four close brace dollar sign.



STEVE:  Now, that actually, I mean, I can look at that...



LEO:  [Crosstalk] pretty much; right?



STEVE:  Yes.  I know that that handles country codes on the front end.



LEO:  Yeah, because you have one or two - a plus sign with one or two digits.



STEVE:  Yes.  So you and I can look at that and read it.  I mean, that's what...



LEO:  This isn't too bad.



STEVE:  Yeah.  Once you understand regex, you can look at them and read them.  But again, imagine writing a piece of code, a generic piece of code which is able to understand that and everything else.



LEO:  My guess is the reason it's so hard for a human is because this is designed to be easier for a machine to parse, rather than vice versa.  You know what I'm saying?



STEVE:  Right.



LEO:  That the harder it is for a human to understand, the more likely it was designed by an engineer to be easy for a computer to understand.



STEVE:  Well, can you say Forth?



LEO:  I like Forth.  You know, I've never - I should look at a parser.  But, you know, I'll tell you what.  I don't think it can be that hard because there's so many of them.  Regex is everywhere.  Most people copy the Perl standards.



STEVE:  Yeah.  It was only written once, and then everybody else went, oh, thank god.



LEO:  Everybody just pasted.  They just pasted it in.  Jerry Wohl, he wrote it once, and then he just pasted it in, yeah.



STEVE:  Okay.  So these guys in their abstract, they said:  "Regular expression denial of service is a class of algorithmic complexity attacks where matching a regular expression against an attacker-provided input takes unexpectedly long.  The single-threaded execution model of JavaScript makes JavaScript-based web servers particularly susceptible to ReDoS attacks.  Despite this risk and the increased popularity of the server-side Node.js platform, there is currently little reported knowledge about the severity of the ReDoS problem in practice.  This paper presents a large-scale study of ReDoS vulnerabilities in real-world websites.  Underlying our study is a novel methodology for analyzing the exploitability of deployed servers.  The basic idea is to search for previously unknown vulnerabilities in popular libraries, hypothesize how these libraries may be used by servers, and then to craft targeted exploits.



"In the course of the study, we identify 25 previously unknown vulnerabilities in popular modules" - like Node.js - "and test 2,846 of the most popular websites against them.  We find that 339 web sites, 11% of the ones that use Express, a popular server-side JavaScript framework, suffer from at least one ReDoS vulnerability, and some even suffer from multiples.  A single request can block a vulnerable site for several seconds, and sometimes much longer, enabling a denial of service attack that poses a serious threat to the availability of these sites.  We also show that the fact whether a website is vulnerable is independent of its popularity, indicating that the problem requires attention across a wide spectrum of web providers.  Our results are a call-to-arms for developing technologies to detect and mitigate ReDoS vulnerabilities in JavaScript."



And I have some other stuff here in my show notes.  I won't go further than to just note that somewhere - oh, here it is.  "Some of the vulnerabilities we identify are more serious than others.  For one of them, 50 characters of carefully created input" - okay, now, remember, this poor regex is like just going to go, okay, what have I got?  And it's going to start following its little rules and accepting input; right?  For one of the things they found, a 50-character input, carefully created, can block the server for 10 minutes.  So it could take a website offline for 10 minutes.  And frankly, I'm not surprised because what's happening is, I mean, like the way we got into this is that web developers - and parsing that phone number is a perfect example, Leo.  We've all gone to forms where we've filled out a form.  And some of them don't want parens and hyphens in the phone number.  They just want the digits.



LEO:  Yeah, I always think of that as lazy.  You're right.  But it's just lazy.  They didn't want to write the regular expression to parse it.



STEVE:  Right.  And hopefully that's JavaScript running on the user's browser, in which case it's just going to lock up their browser, and they're going to go, what the hell?  And then, like, close it and start over.  But what can happen is, and that's the case with these Express-based servers, where very much like Active Server Pages, which is a server-side scripting, you can do JavaScript-side scripting where you're producing pages on the fly.  And certainly PHP is similar.  You could imagine that the person developing the script, they're not thinking in terms of worst-case strings being given to that maliciously.  I mean, even if they even know about what would be a problem for the server to parse, they're just wanting to solve their problem.



And essentially what this has cleverly demonstrated is this awesome power of a regex can be used to maliciously feed a server 50 characters that will bring it to its knees for 10 minutes.  And then, when it comes back, give it another 50.  So a very low-bandwidth attack that can keep a website offline.  And I guess, after that had been happening enough, somebody would scratch their head and say, what the heck is going on here?  Like why is the processor pinned at 100% for receiving a string?  And then they'd figure it out.  Just incredibly clever.



LEO:  It makes sense, though, because it's kind of like dividing by zero.  It probably isn't too hard to come up with a string that would just break the parser.  It's just like...



STEVE:  Yes.  Almost put it into an infinite loop, trying to, like, wait, okay, now I did this.  Now what about that?



LEO:  Backtrack, backtrack, backtrack, backtrack, backtrack, backtrack.



STEVE:  Exactly.  Exactly.  Some German researchers also presented at USENIX.  Four German researchers published a paper titled "Digging Into Browser-Based Crypto Mining."  What they found was interesting because they were able to put some numbers to what we've been talking about in various forms for the past year.  And we're running a little short of time, so I'm just going to - I'll keep this short.



Their abstract says:  "Mining is the foundation of blockchain-based cryptocurrencies such as bitcoin rewarding the miner for finding blocks for new transactions.  Monero" - which of course we've been talking about recently - "is a recent alternative currency which enables mining with standard hardware in contrast to special hardware (ASICs) as often used in bitcoin, which paves the way for browser-based mining as a new revenue model for website operators.  In this work," they write, "we study the prevalence of this new phenomenon.  We identify and classify mining websites on a large corpus of websites and present a new fingerprinting method which finds up to 5.7 times more miners than publicly available blocking lists.  Our work identifies and dissects Coinhive" - not surprisingly because it's the one we're always talking about, and it is the most popular - "as the major browser-mining stakeholder.  Further, we present a new method to associate mined blocks in the Monero blockchain to mining pools and uncover that Coinhive currently contributes 1.18%" - okay?  "Coinhive current contributes 1.18% of mined blocks turning over Moneros worth a quarter million dollars per month."



LEO:  Whoa.



STEVE:  Per month.  Coinhive is making a quarter million dollars.  Anyway, they go on to explain about the reason bitcoin isn't used is that ASICS, the nature of the bitcoin hard problem deliberately submits itself for exploitation or solution by ASICs; whereas Monero is deliberately designed to be ASIC-hostile and CPU friendly, thus it has been the chosen coin for the realm of browsers.  Anyway, they go on to explain the work that they did tracking this down, finding way more Coinhive presence than was widely known to be out there, find the presence in the mining pools, and then track it down in order to arrive at the position that Coinhive is making a tidy profit of a quarter million dollars per month.  So it's paying off, yes.



LEO:  Wow.  Wish I'd thought of that.



STEVE:  Yeah.  And as we've said, it's a little bit of like the gray market.  I mean, they have that link shortener now which is trying to formally commercialize following a link so you get - and we talked about this a few months ago.  You use a Coinhive link shortener service.  And so if somebody wants to follow a link that you have shortened, then while they're doing that their browser is mining Coinhive, and you get a small piece of it.  I mean, that was the other controversial thing.  It's not, I mean, Coinhive is taking a big chunk of the mining revenue in return for their service.  So okay.



Okay.  So two closing-the-loop pieces from our listeners.  Kurt in Singapore once again questions one of my favorite topics.  His subject was "Lithium-Ion: Slow charge vs. fast charge."  He said:  "Steve, I've been following your advice for years to always plug in my devices and keep them charged up.  One thing I couldn't get a consensus online on is whether slow charging, regular charging, or fast charging has different effects on the battery.  There are conflicting statements.  What would be your answer?"



Okay.  So lithium-ion does not like heat.  So you really don't want them to get hot.  It is, for example, not good to recharge a phone or an iPad that's been in the sun and is like just hot as a consequence of external thermal radiation.  It's much better to wait till it cools off, take it inside, let it cool off, then plug it in.  So they don't like heat.  As long as they don't overheat, lithium-ion can be charged as fast as its resistance will allow.  So the battery inherently presents a resistance to charging.  That resistance causes it to generate heat.  So you don't want its own fast charging to overheat it.  That can cause problems.  But lithium-ion can take a fast charge as long as - and this is what's critical - it isn't overcharged.  What lithium-ion batteries absolutely cannot tolerate is overcharging.



And so the reason there's this controversy about whether to leave them plugged in or not is you absolutely want to charge them with a charger which is smart.  I have never seen Apple's charging technology fail.  We have seen other non-Apple lithium-ion chargers fail.  That is, they overcharge and can cause problems in the long term.  So back with that controversy over the iPhone slowing down when we went to, what is it, the very last version of v10 of iOS.  Suddenly all of our phones got slow because Apple decided if the battery's old it's probably bad, so we're going to bring the clock rate down because we don't want to ask for more from the processor than the battery can deliver.



That annoyed me because I had kept my iPhone 6s in really great shape.  It is still in great shape.  And when I look now at that battery status on iOS whatever version of 11 we're on now, it shows 100% on my old iPhone 6 because it lives on the charger.  And it runs all day if I take it off.  So I've kept it in really good shape.  So anyway, the answer is charging rate doesn't matter, as long as the battery doesn't get overheated.  What matters is stopping once the battery is charged.  Trickle charging is not something you want to do with lithium-ion, either.  That's the equivalent of not stopping the charge.  You want a full stop once the battery reaches the charge point to stop it.  And as we know, in terms of long term, the best way is to keep the batteries half charged.



I noted with pleasure that my most recent Lenovo noted to me after, I don't know, a couple of weeks, it brought up a popup that said, hey, I notice you're always on the charger with me.  If you're not going to be using the batteryness of this laptop, it would be better if we ran the battery down halfway.  And then we'll just keep it there.  And I thought, wow.  I mean, yes, that's what I want.  So that's the way I run that little laptop is it's always sitting at 49%, I notice.  And if I ever go on the road, I'll switch it into full charge, it'll bring it up to 100, and then I get full life in the battery.  So again, charge rate doesn't matter.  You just need an intelligent charger.  And if you have one, I mean, I also see people saying, oh, as soon as your device is fully charged, unplug it.  It's like, if the charger - all lithium-ion systems have a smart charger.  I mean, they have to be monitored. So if it's smart enough, it already internally unplugged it.  There's no need to physically unplug it.  It did that for you.



So anyway, second piece of feedback from Sheldon T. Prodanuk in Canada.  The subject was "CrystalDisk and SpinRite," an interesting case.  He said:  "Hi, Steve and Leo.  Been a long-time fan of Security Now!.  Been listening for 10 years and have listened to every podcast from day one Honey Monkeys."  He said:  "I had an old laptop that was in the process of dying, so decided to take the hard drives out, put them in my PC, and use them for backup storage."  Interesting, plural hard drives.  He says: "Dual 500GB 7200" - so that's RPM - "hard drives."



He says:  "I use CrystalDisk info to monitor the health of my drives since some of them are very old.  CrystalDisk gives me a caution about relocated sectors count being 83, and the threshold is 36."  He says:  "I thought now is a good time to run SpinRite on Level 4 and see what it does.  So I ran a Level 4 overnight and reran CrystalDisk, which still throws a caution with the same warning.  Is this normal, and would it be wise to raid the two disks at RAID 1?"  Meaning to run mirroring.



Okay.  So he's talking about using backup storage, using them for backup storage.  If he used them separately, like a RAID 0, where they're just being connected together, then he would get  a terabyte, 1000GB.  If he mirrors them, he's going to get 500GB, and he gets 100% redundancy.  SpinRite is interesting here because it didn't make the drive worse.  That is, nothing changed.  And the drives were in a laptop, apparently, he says, an old laptop, for a long time.  So it's a bit of a judgment call.



But maybe because they're being used for backup storage they're less critical, for one thing.  And the fact that SpinRite didn't worsen their condition, to me that's the most significant indicator because, if the drives were really not doing well, SpinRite would have been expected to push the relocated sector count up and the health down, which Sheldon says did not happen.  The fact that they're in a laptop means that over time they could have accumulated damage, I mean, actual damage due to just physical bouncing around and their presence in a laptop over time.  Laptops are a rough environment for hard drives.  And so the drive could be in absolutely good shape.



And, frankly, if you run SpinRite on Level 4, as we know, that's a workout.  And it does increase relocated sector counts, and it does push the SMART health indicators down all the time in drives which are becoming soft.  It doesn't feel like these are becoming soft.  It feels like they actually have had sectors taken out of service, probably more to laptop-based accrued damage than to the drives getting old.  So based on the evidence, I'd let them keep running.  I think they're probably in good shape.  Run SpinRite on them occasionally as you have been, Sheldon.  And if this changes, then that would be reason to take them out of service, or maybe double them up and run them in a mirror configuration.  But at this point there's no reason to believe they're about to die.  I think they're probably happier no longer being in that laptop and being bounced around.



Okay.  So this one, probably deservedly, got a huge amount of attention from the press.  Tenable wrote:  "A flaw in Intel's Software Guard Extensions implementation" - okay, well, that's not quite true - "allows an attacker to access data stored in memory of other applications running on the same host, without the need for privilege escalation."  I'll explain.



Trend Micro:  "Foreshadow/L1TF Intel Processor Vulnerabilities:  What You Need to Know."  L1TF is Microsoft's designation, L1 as in the cache, that Level 1 cache.  And TF stands for Terminal Fault, which is, again, their name for something I'll explain.  SonicWALL wrote:  "Foreshadow Vulnerability (L1TF) Introduces New Risks to Intel Processors."  The Hacker News wrote:  "Foreshadow Attacks - Three New Intel CPU Side-Channel Flaws Discovered."  PCWorld:  "Foreshadow attacks Intel CPUs with Spectre-like tactics."  And then PCWorld said, parens:  "(but you're probably safe)."  Wired, even Wired magazine:  "Spectre-Like Flaw Undermines Intel Processors' Most Secure Element."  And that's true.



Okay.  So what happened?  As we said at the top of the show, this is more speculation.  And it's funny, too, because, I mean, it's speculation in the sense of there's never been an in-the-wild instance of this, leading some people to believe that we're speculating about speculating, or speculation.  Like is this really all just a bunch of worry about nothing?  Except to the industry's credit, all of our experience says no, we're not taking this too seriously.  There's no such thing as taking a publicly known vulnerability too seriously because hackers are clever.



Okay.  So let's revisit virtual memory management.  I've often talked about how I cannot believe it even works, that is, processors have gotten incredibly fast.  DRAM, the nature, the physical physics of DRAM makes it slow.  There is not a way to make it go fast.  That's why this Optane memory, the idea of getting very, very fast read access from high density, like DRAM-class density is really interesting.  Remember that the reason DRAM is a problem is that it is a destructive read process.  You read RAM a row at a time.  And memory is essentially a row, a long row of capacitors.  And in order to read the state of the capacitors you have to dump them into sense amplifiers at the end of the column of the row.



Well, that means that you've lost the charge.  You've dumped the capacitors into the sense amps to determine what they were.  But "were" is the operative word.  So now you need to rewrite that row so that it doesn't forget what it was you just read.  And back in the old days when we had core there was a remodify write thing where maybe you were going to be changing the words you just read, so that was an optimization that core-based systems used rather than always rewriting what it was that you just read.  They would take advantage of the fact that core memory was also a destructive read.  And if you were incrementing or changing a value you had just read, that's what you would write back.  Okay, not the case with DRAM.



So the point is that the physical nature of DRAM prevents it from going fast.  It can't.  And believe me, like everybody would love it to be a lot faster.  Because it can't go any faster, we have caching.  We have typically now three levels of caching.  Modern Intel processors have tens of megabytes.  I saw one that's 24 or 36MB of Level 3 cache on the chip, which is shared by all the cores, that themselves have smaller, like 2MB Level 2 caches and Level 1 caches.  So all of the caching is meant to decouple the sluggishness of DRAM from how fast the actual processors have become.  And if Optane ever happens and gets cheap and comes down in price and gets proven not to have a wear problem, which apparently it still does have, which is what Microsoft's been fighting, then architectures could change.  For now we have what we have.



At the same time, we have this concept of virtual memory, where processes have, due to history mostly, they have addressing of main memory which is not physical.  That is, there is this memory management layer between the logical and the physical memory.  And what boggles my mind is that in Intel we have four levels of indirection so that, in this long chunk of logical addressing, and it varies depending upon the mode, but typically pages are 4K.  So that's 12 bits.



So the lower 12 bits of the logical address actually point to the byte offset in a 4K page.  But the other bits in this logical addressing actually are pointers to offsets of indexes in other pages.  And there are three layers of those.  So that the most significant bits select a page, also in main memory, which then is used by the next lesser significant set to select an item from, and it points to another page in main memory.  And then there's a pointer taken into that page that points to another page in main memory where you finally get the pointer to the 4K page that the least significant 12 bits point to.  So you can understand why I can't believe this actually even gets off the ground.  But thanks to caching, it does.



I learned something between now and last podcast, which is, believe it or not, Intel even speculates here.  That is, we've talked about the idea of guessing which way a branch will go, and having the processor remember past branch directions that a particular branch instruction has followed and gone, oh, look.  Based on recent history, the processor tends not to jump at this branch opportunity.  So we're going to just charge ahead, assuming that it's going to not jump again.  Okay, that's sort of sane speculation.  I can get my head around that.



Okay, get a load of this.  Somehow Microsoft speculates about the probable content of pages in this virtual memory management page table architecture which are missing.  Which are not in the cache.  Now, okay.  First of all, the only chance any of this has of working is if you have big caching because you've got to keep - you can't go out to main memory to be fetching these tables.  The master copies of them are in main memory.  That's where they come from.  But thank goodness we've got caching to keep them around so that the system is able to relook them up.



There's also these things called TLBs, Translation Lookaside Buffers.  That's the highest level of caching where the system is able to look at these TLBs, the Translation Lookaside Buffers, in order to just immediately breathe a sigh of relief and say, oh, thank goodness, I know where this instruction, I know what these most significant bits point to is this immediate page that's probably still in cache.  So it doesn't have to do any of that pointer following through multiple levels of page tables that have to be pulled from main memory if they're not still in one of the local caches, which luckily most of the time they are.



Anyway, Intel, if the Translation Lookaside Buffer has been forced out of cache because it aged out, Intel speculates on its contents, which I just - I would not want to be a hardware design engineer at Intel.  And I understand now why the chips have gotten to contain so many billions of transistors is that, I mean, to say they have done everything possible to make the systems go as fast as they can and to need water cooling is not an exaggeration.  It boggles the mind.  And so I guess it should not surprise us that, yes, Intel was speculating here, and it can be abused.  It turns out this is not actually news to Intel.  Two different groups of researchers, once the news - remember we started talking about this the first podcast of the year.  Whoo.  Okay, speculation.  Get ready.  Buckle up.



LEO:  Whoo.



STEVE:  Buckle up.  So in January Intel was informed, and it was only last week that the wraps came off of this.  So this was responsibly disclosed by researchers who, as soon as the idea of speculation hit, I mean, it's been a bonanza for researchers all year because we've been constantly talking about this as like it's been - clearly the theme of this year's podcasts has been all of the problems caused by - and notice also, I'm preempting myself finishing that sentence.  Notice also that this is not new.  What's new is somebody said, hey, look over there, and the whole research community said, what?  And it's like, oh, my god, and off they went.  And this is the riches that have resulted.  I mean, this has always been what we've had.  And we've just sort of been skipping along, not worrying about it, until someone said, you know, speculation might be a problem.



And then it turns out Intel has speculated everywhere.  I mean, again, hats off to them for giving us systems that run as fast as they do, despite being as insanely complex as they are.  This is where all those transistors went is keeping the systems fast by coming up with ways to solve the problem that DRAM refuses to get sped up.  And so here is yet one more instance.  Remember also that there were some CVEs that we talked about months back which were being deliberately non-disclosed.  They've been allocated to Intel.  There was a block of them.  And we said, well, okay.  We only know about two of these eight because the other six are being kept secret.



Well, this is an example of those secrets finally coming out.  So it was last week with the publication by Intel of, yes, more microcode patches because this needs microcode patching again, much as Spectre and Meltdown did.  These particular problems bypass, well, because of the completely different nature of speculation, bypass the previous mitigations.  The big concern was that the so-called Software Guard Extensions, this SGS, this is Intel's built-in Secure Enclave technology which all of the recent processors have.  It cuts through it like butter.  It just absolutely - and the researchers demonstrated obtaining all the protected keys and signing and attestation features in a Secure Enclave.  So sorry about that.  It also requires OS support to update, not only to update the microcode, but to be aware of this problem and mitigate.



So the various OSes now know.  Linux went public with their updates.  There is in the Microsoft Update catalog, not yet part of Windows Update, so you've got to go get it yourself once again, and only for the most recent 1803 build of Windows 10 are the very small, they're like 1.3MB, so they're just the little microcode patches in order to add this to the processor and just enough to inform Windows of them, are available.



People have asked whether my own InSpectre tool will be updated.  It doesn't have my focus at the moment, but I will keep an eye out for whether Microsoft publishes as they did some means for determining what's going on and, if not, if Intel does, because InSpectre has the ability to go down to the chip level and find out, even if the OS doesn't support it, what's going on down below.  So this just happened.  Details are still scarce.  So I don't know whether InSpectre will be or when it will be updated.  But I will keep an eye on it.



So in the meantime, I agree with PCWorld.  We probably don't have anything that we need to worry about.  We still don't, even as far as we know, from the original InSpectre flaws.  I don't know when, like if we're going to finish out the year without more revelations.  This is, again, for me it's mindboggling because I'm amazed that Intel can speculate based on missing pieces of the virtual memory management mapping.  Somehow they were able to do that.  But unfortunately it also meant that it was subject to abuse.  And it has been - the proof of concept exists.  This has been taken from theory to practice.



So this, at least, cannot be ignored because this absolutely allows cross-process, cross-VM, cross-OS, cross-SGX boundaries to be penetrated.  Until now, remember we've talked about gadgets.  A gadget was some code that was found in where you wanted to get into, which could be leveraged against its desire to disclose the secrets you wanted to get.  This Foreshadow Flaw is gadget-free.  If the code that was protected by Software Guard Extensions previously had an exploitable gadget, it could be used to leak information from the SGS Enclave.  This is worse.  This requires no vulnerable code in the protected enclave.  It can just suck the secrets right out.



So again, Intel was having a much worse, well, day or month in January, but probably previous to that, than we even knew.  I mean, somewhere, I'm surprised there just isn't a smoking crater in Silicon Valley where Intel was because for them to realize what speculation flaws meant, they knew more than we did, certainly more than I did about how pervasively speculation was in use.  And then researchers began lining up, saying, oh, and over here, and over here.  And oh, by the way, and this and this and this and this and this.



So apparently deploying this mitigation does have a performance hit.  That's the other piece of this is, again, the problem is we're not even really sure we need it.  I'm really glad we have it.  So there's been some talk about turning on foreshadowing protection, resulting in a clear drop in system performance.  And again, certainly an end user, this is a - oh, and you have to be sharing the same core.  So turning off hyperthreading, for example, would stop sharing a core with another thread.  And that is a short-term mitigation.  Turning off hyper...



LEO:  That's got a big slowdown, too, though; right?



STEVE:  Yes, it does.



LEO:  If you have an app that uses it.



STEVE:  Yes.  Yes, it is.  But if somebody's really concerned, typically you can turn off hyperthreading in the BIOS, and then you're okay.  But again, exactly as you say, Leo, at a significant cost in performance.  So it's not something - end users, I don't think we're in trouble.  If something is in your system that's able to do this, then you're already in worse trouble than speculation is going to bring to you.  The real problem is where you could have an untrusted entity sharing a server that's got secrets it's trying to keep.  And this lets that entity cut right through them.  So that's where you really want to be current and make sure you've got mitigations.  And by the way, Microsoft did say, you know, don't worry.  All of our Azure systems are mitigated.  And I'm sure that that one, like people who needed to know about this, knew about this some time ago.



LEO:  If I eschewed the Intel processor and went AMD, would I be okay?



STEVE:  Let's see.  I've got it here.  Foreshadow is similar...



LEO:  It's kind of unclear.  I mean, all of this is about Intel, but at the same time they use speculative execution on all processors.



STEVE:  So what I have here in my show notes, Foreshadow is similar to the Spectre security vulnerability discovered earlier to affect Intel and AMD chips, and the Meltdown vulnerability that also affected Intel.  However, AMD products, according to AMD, are not affected by the Foreshadow security flaws.



LEO:  Okay.  That's according to AMD, but...



STEVE:  According to AMD, exactly.  And they all...



LEO:  Remember they said that they weren't suspect.



STEVE:  Exactly.



LEO:  Spectre didn't affect them, either.



STEVE:  They said the same thing, and then it turned out that was not true.



LEO:  And ARM also uses speculative execution.



STEVE:  Again, Leo, this is so far out in the weeds, to be able to be using speculation on Translation Lookaside Buffer failure, I'm just...



LEO:  That's a weird thing to do, yeah.



STEVE:  I'm just stunned by that.



LEO:  Yeah.  That might have been overreaching.



STEVE:  Well, yeah.  Wow.  Hats off.  But now I know where a billion transistors went.  Whoo.



LEO:  Yeah.  Yeah.  I'm just thinking, if there's a prudent thing, if I were in the market for a computer today, would it be smart to get an AMD, or do we even know?



STEVE:  Well, again, I don't think there's any reason to believe an end user cares.  Ever.



LEO:  They're not going to run into this.



STEVE:  Yeah, probably ever.



LEO:  It's servers mostly.



STEVE:  I'm still buying Intel chips.  I've always been an Intel, you know, I have a couple AMDs on some laptops because they just came with them.  But other than that, you know, when I buy, I buy Intel.



LEO:  I kind of think the folks at Apple are trying to get those ARM chips into PCs, into their desktops as fast as they possibly can.  Intel has really proven to be a disappointment for a number of reasons.



STEVE:  Well, and the problem is, I mean, to their credit, they're taking a creaky old x86 architecture that I started coding to on an 8008 processor running at 4.77 MHz.  And I can still run the same code today.  So the problem is it was a CISC.  It was a Complex Instruction Set that was, at the time, that was the way you did it.  Memory was expensive.  So you wanted to put the complexity in the processor architecture so that the individual instructions did a lot.  That all changed.  And so that's why RISC makes more sense today is that memory is no longer expensive.  And what you want is you want to keep the complexity of the processor low.  And that's what ARM did.  So ARM had the advantage of coming along later and using an architecture which has turned out to be where you want to be in the future.  Intel is managing to keep this legacy architecture alive by hook and by crook.  And, boy, sometimes it's biting them.



LEO:  Mm-hmm.  Wow.  Okay.  Another day, another flaw.  Another week, another Security Now!.  Another 13 years in the making.  And here we go into Year 14 with Steve Gibson.  Thank you, Steve.  If you want to follow along, there's a couple ways you can do this.  Of course you can watch us do the show live every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30.  And if it's your birthday, and you want to celebrate by visiting, you can just email tickets at TWiT.tv, and we'll bring you in here.  I'd love to have, if we could only get a 13 year old in here who was born the day this show was started, that would really make me feel old.  I guess they'd be 14 today, wouldn't they.  Happy Birthday, whoever you are.



If you want to watch it live, go to TWiT.tv/live.  There are a number of streams, audio and video.  You can watch live.  You can also, if you're watching live, you might as well join the chatroom.  Great bunch of people in there.  They are awesome, including Bill in Michigan who reminds us that the Honey Monkeys episode was actually our second episode.  The first episode I think was just us talking about doing the show, probably [As the Worm Turns].  Because I remember Honey Monkeys as our first episode, as well.  Maybe it was our first, like, where we did something, all those years ago.



If you want a downloadable version of the show, there are several ways to get it.  Steve's got audio at his website, GRC.com.  He's also got transcriptions, written not by machine, but by Elaine Farris, who is far from a machine.  Well, she's a machine in the sense she can crank them out, but she's a human, yeah.  And a farrier [incorrect].  And that should tell you something.  What, I don't know.  That'll be GRC.com.  That's also where you get SpinRite, the world's finest hard drive recovery and maintenance utility.  That's his bread and butter.  But Steve has lots of freebies on the website, too, lots of stuff to read.  It really is, it's worth just checking out every once in a while, including ShieldsUP!, probably the best-known router test out there, which gets better all the time, and many, many other things.  GRC.com.



We have our version of the show.  It's the same audio, but we have a video version you can watch at TWiT.tv/sn.  And of course the best way to do it is probably just subscribe in your favorite podcast application.  That way you'll get an episode the minute it's done on Tuesdays - Pocket Casts, Overcast, Google, Apple, everybody's got a podcast app.  Just pick your app, download your show, subscribe so you don't miss anything.  Thank you, Steve.  We'll see you next week on Security Now!.



STEVE:  Thanks, buddy.  Talk to you next week.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#678

DATE:		August 28, 2018

TITLE:		Never a Dull Moment

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-678.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  It's been another busy week.  We look at Firefox's changing certificate policies, the danger of grabbing a second-hand domain, the Fortnite mess on Android, another patch-it-now Apache Struts RCE, a frightening jump in Mirai Botnet capability, an unpatched Windows zero-day privilege elevation, and malware with a tricky new C&C channel.  We find that A/V companies are predictably unhappy with Chrome, Tavis has found more serious problems in Ghostscript, and there's been a breakthrough in contactless RSA key extraction.  As if that weren't enough, we discuss a worrisome flaw that has always been present in OpenSSH, and problems with never-dying Hayes AT commands in Android devices.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with a potpourri of security stories:  why Ghostscript is dangerous; the return of the Mirai worm (it's really on the upswing); and why AT commands, the Hayes command set, could be a danger to your Android phone.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 678, recorded Tuesday, August 28th, 2018:  Never a Dull Moment.



It's time for Security Now!, the show where we cover your security and privacy online with this cat right here, this man, the myth, the legend, Steve Gibson.



STEVE GIBSON:  The myth.  The myth, I'm always a little surprised by that, Leo.



LEO:  The mythic.  How about that?



STEVE:  The mythological.  The mythological.



LEO:  Not mythological, mythic.  Which would mean as if you were a Greek god, but you're not.



STEVE:  That's not like penultimate; right?  Because it's not like second best, it's like...



LEO:  Mythic:  Relating to or - ah, here it is.  Exaggerated or idealized.



STEVE:  Now, see, exaggerated...



LEO:  That's not quite right either.



STEVE:  Neither of those really work for me, no.



LEO:  The marvelous Steve Gibson, @SGgrc on Twitter.  He's the Gibson Research Corporation, the man behind the Security Now! program for the last 13 years, now going on 14.



STEVE:  Into.  Now, this is podcast number two of year 14.



LEO:  Nice.



STEVE:  Counting down to the last 999 podcast.  It's funny, too, how many people are upset by that.  It's like, well, I'm sorry, but I still have a little hair.



LEO:  And you reserve the right to change your mind if you feel like you want to keep going.



STEVE:  We could switch into alphanumerics, I suppose, so yeah.



LEO:  Yeah.  We could do hexadecimal.



STEVE:  Oh, and there have been lots of suggestions about how to fix this.  So thank you, everybody.  We are final podcast of August, and no one thing stood out in the past week.  Surprisingly, all of the major disasters which were revealed by Black Hat and DEF CON and USENIX have pretty much been covered over the last three weeks.  We do have two more USENIX reports which are interesting, and one DEF CON deal.



Anyway, so I titled this "Never a Dull Moment" because that is certainly true and of course always is.  So we're going to catch up on yet another busy week.  We look at Firefox's changing certificate policies.  The danger - this is something I had never thought of before, and I'll bet if we listen carefully, Leo, when we explain this, we could hear a collective gasp from our audience around the world - the danger of grabbing a second-hand domain.



We also have the Fortnite mess on Android.  Another "patch it now" Apache Struts remote code execution vulnerability.  A frightening jump in Mirai botnet capability.  An unpatched Windows zero-day privilege elevation that just happened a couple of ours ago, and a whole bunch of people tweeted to sort of hope I had seen it and to get it into this podcast, and we have.  A malware using a tricky new command-and-control channel we've never seen before.  Antivirus companies predictably being unhappy with Chrome's decision to start pushing against them being, remember, all that "injected into" fun we had last week.



Tavis, who doesn't even need a last name, has found more serious problems in Ghostscript.  He hasn't missed a year for the last few, and so yet again.  We've also got a breakthrough in contactless RSA key extraction from devices.  A worrisome flaw that's always been present in OpenSSH that only just came to light after two decades.  And believe it or not, AT, the Hayes AT commands, Leo, that you and I grew up listening to, the sound of modems mating - actually, they did sound like unhappy geese - they're still present, and they're surprisingly dangerous.  So lots to talk about this week.  And a fun Picture of the Week courtesy of a good friend of mine.  So I think a lot to do this week.



LEO:  I do enjoy this one.  It's funny because it's true.



STEVE:  It's a little, yes, you transient biological creatures.  Our Picture of the Week is fun.  It's a four-frame cartoon.  The first frame shows a boxy-looking robot staring at a screen.  And of course instead of saying "I am not a robot," the screen reads "Prove you are not a human."  And then there's a blank to be filled in.  Then the next frame of the cartoon shows us a side angle where we can see the robot's sort of arms are typing, and it enters the phrase, "There are no more humans."  And then the third frame screen shows "Correct."  And then the fourth frame, both of them, the screen and the robot from side view, are laughing, ha-ha-ha-ha.



LEO:  Oh, man.



STEVE:  Okay, yes.  I just thought it was fun and a little apropos of, as I said, oh, you transient humans.  You just think you're all everything, while we come up with new AI chips and features and just, you know, we're so clever.  We'll see what happens.



Firefox has begun the expected, well, not really a rollback, the expected untrusting of Symantec's root certificate and all certificates chained up to Symantec's root, with some surprising findings.  It's not yet out in the stable release, but the nightly release for Nightly 63 now is no longer trusting any sites whose certificates were signed by Symantec directly or by any of their partners, including GeoTrust, Thawte, and RapidSSL, all whom Symantec acquired over the years and as a consequence are also no longer being trusted.



In the beta which is early next month this will appear, and then we're all going to see it, those of us who are using Firefox, by the end of October.  October 23rd it will enter the stable release.  And of course, as we know, Google has been leading a little bit.  They took the same actions with Chrome 70's nightly build at the end of last month, on July 20th.  Then it'll be in beta of Chrome on September 13th, and then it will land in Chrome stable on October 16th.



So at that point - and I don't know where Microsoft is on this.  I've not seen any reporting, and I haven't gone digging for it.  But what's interesting is that even, I mean, we've been talking about this, what, six months?  Oh, and of course we also talked about how DigiCert has acquired Symantec's certificate business, and I'll follow up a little bit on that in a second.  But as a consequence of that acquisition, everybody's happy because DigiCert offered to recertify under their root any affected customers.  Yet even so, people are just apparently asleep at the switch, and some significant people.



Mozilla on their tracking page has noted that, for example, the Sony PlayStation Store, the Navy Federal Credit Union's online banking page, First National Bank of Pennsylvania's online  banking, Estonia's LHV Bank, Canadian telecom Freedom, a couple of French banks, and the First National Bank in South Africa.  Oh, and Intel's Japanese website.  So some of them are obscure, but still large.  And the point is that within a month anyone going to those sites with the majority browser on the 'Net, which is now Chrome, and certainly very popular, or Firefox - and again, I don't know what Microsoft is doing, but presumably Microsoft will be in here somewhere - is going to start getting very worrisome notes.  And as we know, if you go somewhere with an untrusted root, it's either difficult or impossible these days to get around that.



So I hope that these companies get off, I mean, it's not like it's hard to remint yourself a certificate which is trusted by DigiCert, which is where Symantec has been aiming everybody.  And as we know, DigiCert is my chosen certificate authority.  Years ago, when I was leaving, who was it, VeriSign that I was no longer happy with because it just seemed like lots of nonsense dealing with them, so I switched to DigiCert, I mean, after doing a lot of research and deciding they were the guys for me.



DigiCert acquired Symantec's certificate authority business.  That act jumped DigiCert from the position as sixth largest certificate authority by market share into the number three slot.  So before the deal - and I'm surprised that Comodo is as big as they are.  I don't really understand that unless it's because of their free certificate offering maybe.  But as we know, we're not a fan of them.  They've had all kinds of mistakes in the past relative to issuing certs.  I'm surprised that they're still trusted, frankly.  But the top six rankings before this were Comodo, IdenTrust, Symantec, GoDaddy, GlobalSign, then DigiCert.



After this deal, interestingly, IdenTrust is number one.  But remember that they're the people who cross-sign Let's Encrypt's free and automated domain validation certs.  So it's not surprising that any instrumentation that looks to see who is the root, since Let's Encrypt only just recently finally got into all of the last of the root stores, it's not surprising that IdenTrust is showing high because they're the people who have been anchoring and are arguably still anchoring Let's Encrypt's free certs.  So second is Comodo, and in third place by market share now is DigiCert, and then followed by in fourth place GoDaddy and fifth place GlobalSign.



Anyway, this is, as we know, and covered at the time, Symantec made a series of mistakes.  And the CAB, the CA Browser Forum, is understanding, and the browser vendors are understanding, of mistakes being made.  What matters is how they are remediated when pointed out.  And Symantec unfortunately did not impress a lot of people.  It turned out that the number I remember was 30,000 certificates through time did not have proper management, and it was felt that this is not something - because the entire browser trust system is built on the integrity of the CA and their management of the process.  Basically Symantec had third parties who they were allowing to do irresponsible certificate issuance.  And that's not okay.



So the good news is I'm sure that the industry, the rest of the CAs have taken note of this and certainly don't want the same thing to happen to them.  So anyway, this has been planned for a long time.  This month and next month it actually happens.  And again, it's like you've got to be scratching your heads why anybody is still using certificates that are going to be distrusted a month from now.  But I imagine as soon as that actually happens, that'll get fixed overnight.



And speaking of certificates - and this, Leo, is where I said I could almost imagine a collective gasp being heard from our listeners.  There is a site for this, as all good problems these days have.  Insecure.design is the site.  And this was - I think it was another USENIX presentation.  It was some research that was done, and I'll catch up to it in my notes.  But they called it "BygoneSSL."  Or perhaps, I was thinking, it should be "Gone, but unfortunately not forgotten."



And here's the problem, which in all of our discussions of certificates had never really occurred to me.  As we know, certificates when issued can have a life of up to three years.  Some of them are shorter.  Some can only be two, some can be three, based on how tight the management of them is based on what they are asserting to be true.  But domains can be abandoned.  And, once abandoned, they can be reacquired by new owners.  But the certificate that was issued to a previous owner of a domain is not automatically rendered invalid because the certificate, as we know, is tied to the domain name.  So anyone picking up a secondhand domain needs to be aware of and cognizant of the fact that there may be, for as many as three years, a previously valid issued certificate for that domain in someone else's hands.



Of course we have certificate transparency now, and we have revocation.  Except that we've extensively covered the fact that revocation is completely broken.  I mean, it doesn't work at all.  And so we're basically trusting the fact that certificates eventually expire themselves because they have bound into them, contained in the envelope which the CA has signed, is a "not valid after" date.  So that's really the only thing that keeps this system working is that eventually the certificates will die.  But until then, yeah, you have OCSP and some CRLSet stuff.  But that only affects extended validation certificates on Chrome.  And, I mean, it's a broken system.



So the question is, so how bad a problem is this?  The research was done by these guys.  And their abstract from their paper says:  "When purchasing a new domain name you would expect that you are the only one who can obtain a valid SSL certificate for it.  However, that's not always the case.  When the domain had a prior owner" - and they said in parens "(or owners)" - "even several years prior, they may still possess a valid SSL certificate for it, and there is very little you can do.  Using," they said, "Certificate Transparency, we examined millions of domains and certificates and found thousands of examples where the previous owner for a domain still possessed a valid SSL certificate for the domain long after that domain had changed ownership."



I'm reading from the abstract still.  They said:  "We will review the results from our ongoing large-scale quantitative analysis over past and current domains and certificates.  We'll explore the massive scale of the problem, what we can do about it, how you can protect yourself, and a proposed process change to make this less of a problem going forward.  We end by introducing BygoneSSL, a new tool and dashboard" - and they've got the link to it on their site - "that shows an up-to-date view of affected domains and certificates by using publicly available DNS data and Certificate Transparency logs.  BygoneSSL will demonstrate how widespread the issue is, let domain owners determine if they could be affected, and can be used to track the number of affected domains over time."



So here's the numbers at the moment.  The researchers took a sample set of 3 million domains and 7.7 million certificates to find how many certificates predated the registration of a domain while still being valid after the registration had expired.  1.5 million entries fit these parameters, and 25% of them had not expired at the time of the investigation.  Meaning that, at the moment, 25% of 1.5 million entries - so, what, about a third of a million? - were still valid at the time of the research, meaning there were duplicate certificates, certificates that had been issued for a domain that its new owner wanted to secure.  And somebody, somewhere, still had a non-expired valid certificate for the same domain.



Which, again, is like, in all of our discussions of this, somehow the issue of domain ownership change had never really occurred to me.  Yes, certificates should be revoked. But as we know, that system doesn't actually work, nice as it would be if it did.  So as a consequence, it is really essentially up to the new owners of the domain to figure out, like to be aware of the problem, to check for the presence of previous owners' still valid certificates and, I would argue, to proactively look at the validity of those certificates.  Has it been revoked?  Where are the revocation lists?  Are they current?  How will the browsers currently handle that certificate if they're shown it?  So anyway, real interesting piece of research, and something that just, like, whoops, you know, slipped through the gaps.



And speaking of slipping through the gaps, this Fortnite mess.  Okay.  So first of all, last week we talked about the so-called "man-in-the-disk" attacks.  Maybe it was last week or the week before.  Anyway, in the last couple weeks.  This was the vulnerability which had come to light where many apps were staging themselves in external storage, which many Android devices have.  It's a shared resource.  And, I mean it's that sharing of the resource that creates the problem because many apps have access, overlapping access to that shared resource, creating the so-called "man-in-the-disk" attacks.  So it turns out that the extremely popular first-person - is it a first-person shooter, Leo?  I've never looked at what it actually is.  I just know it's...



LEO:  Yeah.  It's Battle Royale.  You go in, and a hundred other people go in, and last man standing wins.  Yeah, it's kind of a first-person shooter, special kind, yeah.



STEVE:  So Google takes a 30% cut of all revenue generated by apps downloaded through the Google Play Store.  And as you have often observed on the TWiT Network, Leo, I've heard you talking about Fortnite.  Actually it's about Fortnite itself is free, but they make money, like a ton of money...



LEO:  Yeah.



STEVE:  ...selling other stuff in the game.  So over on iOS, where Fortnite has a player base of it's estimated between 125 and 150 million users, I think that's maybe across both platforms, during the first 10 days of Season 5 the iOS release netted Epic Games, the publisher of Fortnite, $2 million per day.  $2 million per day.  So the total mobile revenue on iOS between its March 15th release and the end of July was $150 million.  Since there's no other method of distributing apps in the Apple ecosystem other than the Apple Store, Epic Games had no choice but to hand over a piece of their action to Apple.



But not so on Android.  Epic Games has elected to bypass Google and the Play Store and to teach how to and encourage people, because there's no other way to obtain this latest Fortnite, how to sideload Fortnite by downloading an APK from Fortnite's own website.  So this of course requires instructing people to push past all the sideloading warnings present on Android devices.  And the big worry among the security community is that this could have the effect of encouraging and normalizing the sideloading process and behavior and pave the way to additional Fortnite clone malware.  And of course we've previously talked about how there were Fortnite cheat add-ons that have been malicious and have hurt people.



So it turns out that on top of this, that is, that Fortnite has decided to sideload in order to not give Google a piece of their substantial action on the Android platform because it's possible, Fortnite is one of the apps vulnerable to this man-in-the-disk attack that we've been talking about, which the researchers at Check Point recently made clear.  Google, maybe being a little miffed at Fortnite for encouraging sideloading, did not give Epic a 90-day window to fix after it came to light that the Fortnite installer was vulnerable to hijacking through man-in-the-disk attacks.



In Google's posting, they wrote:  "The Fortnite APK [com.epicgames.fortnite] is downloaded by the Fortnite Installer to external storage."  And then in the show notes I just clipped out a chunk of directory listings showing all of the entries there.  And then Google continues, saying:  "Any app with the WRITE_EXTERNAL_STORAGE permission can substitute the APK immediately after the download is completed and the fingerprint is verified."



Google writes:  "This is easily done using a FileObserver.  The Fortnite Installer will proceed to install the substituted fake APK.  On Samsung devices, the Fortnite Installer performs the APK install silently via a private Galaxy Apps API.  This API checks that the APK being installed has the package name com.epicgames.fortnite, but does nothing else.  Consequently, the fake APK with a matching package name can be silently installed.  If the fake APK has a targetSdkVersion of 22 or lower, it will be granted" - that is, this potentially malicious fake APK - "will be granted all permissions it requests at install time.  This vulnerability allows an app on the device to hijack the Fortnite Installer to instead install a fake APK with any permissions that would normally require user disclosure."



Okay.  So in response, unhappily, but immediately, Epic released v2.1.  And as I said, Epic is clearly unhappy with Google's short notice.  They said, actually it was Tim Sweeney at Epic who tweeted:  "We asked Google to hold the disclosure until the update was more widely installed."



LEO:  You can see why that's not going to happen, though.



STEVE:  Uh-huh.  Yes.



LEO:  Because it's out there.  It's happening.



STEVE:  Exactly.  Exactly.  So Tim said:  "Google refused, creating an unnecessary risk for Android users in order to score cheap PR points" is what Tim at Epic tweeted.



LEO:  Really?



STEVE:  Google defended their timing, saying that their own instrumentation had shown that most Fortnite users had already updated to v2.1.  So anyway, much has been made of this.  And a lot of coverage in the press has been unhappy with Epic's decision.  I mean, it's difficult to be, when you consider the amount of money that Epic is talking about.  On the other hand, they're only making the money because Google has created the platform.



LEO:  And I think you're risking your customers' safety to make some more money.



STEVE:  You are, yeah.



LEO:  Now, was there already an exploit in the wild?  I mean, it seems to me that the reason Google had to go public, well, I don't know, I mean, is to tell people to stop sideloading.



STEVE:  Right.



LEO:  Or maybe there were no exploits going on?



STEVE:  Well, this doesn't prevent sideloading.  It was that Fortnite's loader was...



LEO:  No, I understand.  But, I mean, the reason they exposed it is because people were actively at risk right now; right?



STEVE:  Yes, yes, yes, exactly, yes.



LEO:  But were there people taking advantage?  My sense was that it had already happened, that there were some...



STEVE:  It was essentially a zero-day, yes.



LEO:  So you've got to tell people.



STEVE:  Yes.



LEO:  So they stop doing it until it's fixed.



STEVE:  Yes, yes, yes.  And immediately get the word out.



LEO:  I mean, imagine if Google didn't tell anybody, and people were getting infected right and left.  Oh, well, we wanted responsible disclosure so that Epic could fix it.  Well, that's not responsible.  That's irresponsible, I think.



STEVE:  Right, right, right.



LEO:  Epic's just cheesed because they did the wrong thing, and they got bit.



STEVE:  Well, and as we talked about when we were talking about this whole man-in-the-disk problem, Google does clearly state that, for example, apps should do what Epic 2.1 now does.



LEO:  Now.



STEVE:  Yeah, exactly.  Now does.  But they've always said this, that if you're going to use external storage, make very sure that you're actually installing the APK or whatever it is; that you can trust what it is that is in that store.  And Epic just wasn't.  They were like, oh, just download some more Fortnite stuff, and let's just suck it right in.  And this really did open them to the hack.  And of course the security community is upset because the problem is sideloading is dangerous because you are training people to push past all of the warnings that Google has saying, you know, we haven't been able to check this.  We don't know what this is you're loading.  You need to be really sure.  And here Epic is widely distributing to tens of millions, if not more, users their app.  Not only was it insecure, but they're encouraging the circumvention of the Google Play Store.



LEO:  Yeah, yeah.



STEVE:  So be interesting to see how this plays out.



LEO:  That's exactly why we thought it was a bad idea to do the sideload.



STEVE:  Right, right.  So Apache Struts once again...



LEO:  Oh, no.



STEVE:  ...in the dog house, yes.  Well, first of all, let's recall that it has not yet been a year since Equifax's massive failure to patch a previously well-known, long-known bug in Apache Struts was leveraged to expose the personal details of its 147 million consumers at an eventual cost to them, that is, Equifax, of $600 million.  So that was an expensive patch to skip.



Well, we're back again.  As we know, Apache Struts is widely used by enterprises globally.  Last year it was estimated to be in use by 65% of the U.S. Fortune 100 companies who use it - it's a Java-based framework - who use it to build their web applications.  So today we have a new remote-code execution flaw which allows attackers to remotely commander web servers.  I've got the two links in the notes.  A group named Semmle, S-E-M-M-L-E, Security posted - this is on the 22nd, so last Wednesday.



They said:  "Today the Apache Software Foundation announced a critical remote code execution vulnerability in Apache Struts," they say, "a popular open source framework for developing web applications in the Java programming language.  Applications developed using Apache Struts are potentially vulnerable.  The vulnerability" - and I had to do a double-take because here we are at a CVE of 2018-11776, so into the five digits of the common vulnerabilities and exploits database - "was identified and reported from the Semmle Security Research Team, which works to find and report security vulnerabilities in widely used open source software.



"Organizations and developers who use Struts are urgently advised" - please, everyone, heed this this time - "urgently advised to upgrade their Struts components immediately.  Previous disclosures of similar critical vulnerabilities" - they're just saying sort of rhetorically - "have resulted in exploits being published within a day."  Remember, that's what happened last time.  Within a day there was proof of concept, and then scanning began within a week of this thing going public.  So that started last Wednesday, putting critical infrastructure and customer data at risk.



So under mitigation, switching now to the Apache disclosure and mitigation:  "This new remote code execution vulnerability affects all supported versions of Apache Struts 2.  A patched version has been released today.  Users of Struts 2.3 are strongly advised to upgrade to 2.3.35; users of Struts 2.5 need to upgrade to 2.5.17.  The vulnerability is located in the core of Apache Struts.  All applications that use Struts are potentially vulnerable, even when no additional plugins have been enabled."  So this doesn't require anything more than just the core Struts package.



They say:  "Struts applications are often facing the public Internet" - yeah, web apps - "and in most situations an attacker does not require any privileges to a vulnerable Struts application to launch an attack against it.  To make matters worse, it is very easy for an attacker to assess whether an application is vulnerable," i.e., scanners are going to appear.  "And it is likely that dedicated scanning tools will be available soon.  Such tools will enable a malicious actor to quickly and automatically identify vulnerable applications."



So they said:  "Whether or not a Struts application is vulnerable to remote code execution largely depends on the exact configuration and architecture of the application."  Then they referred people back to their disclosure.  "For more details, please see the section 'Was I vulnerable?' below."  And they said:  "Note that even if an application is currently not vulnerable, an inadvertent change to a Struts configuration may render the application vulnerable in the future.  You are therefore strongly advised to upgrade your Struts components, even if you believe your configuration not to be vulnerable now."



So anyway, if you know, if you are a Struts-using organization, large or small, and you have any application publicly facing the Internet, to absolutely go to 2.3.35 or 2.5.17.  Oh, and there's no issue with backward compatibility.  This is a security fix only, nothing more.  So it just changes that.  So you don't need to worry about anything else being broken.  Yikes.



Leo, I'm sad.



LEO:  What?



STEVE:  I just, I don't know, maybe you already know about this.  But the budget for the next fiscal year of NIST...



LEO:  Yes, I know about this.  I'm sad, too.  And my clock's really sad.



STEVE:  Yes.  I just saw this.  That's so...



LEO:  Yeah, WWV's discontinuing in 2019.



STEVE:  And, you know, it's the oldest, one of the oldest radio stations in the U.S.  It was established in 1920.  So almost a hundred years.  I mean, I know that sound so well.  I mean, I guess it's aging us a little bit.



LEO:  Yeah, but that's not - is that the digital NTP server, or just the radio station?



STEVE:  Oh, just the radio station.  But I also have "atomic clocks," as they're called.



LEO:  Will they not work anymore?  Will they not set themselves?



STEVE:  No.  They won't set themselves anymore.



LEO:  Oh.  Thanks, NIST.



STEVE:  Well, with any luck, this'll get turned around.  Well, it's the proposed cuts for fiscal year 2019 of the NIST from the White House, which is looking to cut back on things.  So sad.  Anyway...



LEO:  NIST stands for National Institutes of Standards and Technology.



STEVE:  And technology, yup.



LEO:  They run the WWV, what is it, out of Fort Collins, Colorado, the time?



STEVE:  Yes, exactly.  In fact, I kind of point my clocks in that direction because they have a long wave antenna.



LEO:  Sometimes I just put them outside because, yeah, inside they don't always update.



STEVE:  Yeah, yeah.  Okay.  So the Mirai botnet that we have spoken of often has gained a worrisome new trick.  It has started leveraging, or a version of it, the Sora, S-O-R-A, variant has started leveraging Aboriginal Linux, which has made it suddenly much more dangerous.  Aboriginal's slogan is a little prophetic.  It says:  "We cross compile so you don't have to."  This new Sora variant of Mirai is compiled with Aboriginal Linux, which offers a toolchain utility which allows its single base source to generate binaries for a large number of different platforms, many more than previous variants of Mirai could infect.  So in other words, by writing Mirai to be compatible with Aboriginal's toolchain, they're suddenly able to produce a vast array of Mirai binaries.



So what's happened is, and it's been spotted by researchers, once Mirai, that is, the Sora version or variant of Mirai, accesses a device by guessing its SSH password, its infection routine now successively downloads and tries binaries from a long list of Sora binaries, one by one, until it finds the one which is appropriate for the infected device's platform.  And as a consequence, Mirai has made a cross-species jump to Android and Debian, where Mirai has never before been present.  And I have here at the bottom of this page of the show notes a chart showing the rise in Mirai infections from near zero in June - or is that July? I don't know how the dates are organized.  But, I mean, it's just gone... 



LEO:  2018-06-03, 06-17...



STEVE:  Yeah, so it looks like June 3rd to August 12th.  So in a couple months.



LEO:  Boy, it seems precipitous, wow.



STEVE:  Yes.  It has really, really jumped.



LEO:  What is the left scale, though?  I'm always suspicious when I see graphs with no left-hand scale.



STEVE:  Yeah.  And as you know, it's a pet peeve of mine when people do non-zero-based charts.



LEO:  This looks like it might be zero based, it was so flat at the beginning.



STEVE:  It really does.  Yes, it doesn't just show it like already starting up.  So it does look like it's zero-based.  So anyway, just a heads-up.  We're not out of the woods yet with Mirai.  And the bad guys are getting more and more clever.  I mean, again, it's a broken record on this podcast.  But we absolutely have to have autonomous devices that have an Internet face able to update themselves.  It has to happen.  And we know that some HP printers can now be configured to do so, thank goodness.  Our routers have to do that, as well.



Okay, now, I can't read the following tweet in its entirety, or without abbreviating...



LEO:  What is it, from the President?



STEVE:  In this case, no.  It's SandboxEscaper.  He tweeted a GitHub link.  But the tweet reads:  "Here is the ALPC" - that stands for Advanced Local Procedure Call - "ALPC bug as a zero day."  And then the GitHub.com/sandboxescaper link.  And he says:  "I don't effing" - and he didn't abbreviate - "care about life anymore.  Neither do I ever again want to submit to MSFT" - that's of course the abbreviation of Microsoft - "anyway.  Eff all this stuff."  And he didn't say "stuff."  So that was his tweet, surprising the world with that tweet to a never-before-seen, since validated, local privilege elevation that is effective on fully patched 64-bit Windows 10 systems.



Shortly after this SandboxEscaper's tweet, CERT, the vulnerability analyst Will Dormann, whom we've quoted before, verified the authenticity of this zero-day bug, tweeting:  "I've confirmed that this works well in a fully patched 64-bit Win10 system."  He says:  "LPE right to SYSTEM," all caps, meaning that any restricted process can immediately elevate itself to full system, that is to say, root or kernel privilege, using this today.  And frankly, Leo, I was wondering whether it would still be up on GitHub because of course Microsoft now owns GitHub, and it's not good for them for this to be there.  But it's there.  And so props to them for not starting to edit GitHub for things that they like and don't like.



Anyway, according to the short advisory which has since been published by CERT, of course, this zero-day flaw, if exploited, could allow local users to obtain elevated system privileges.  But of course, since local procedure calls are, as their name suggests, local, the impact of the vulnerability is at least limited so that it received a CVSS rating of between 6.4 and 6.8.  But as we know, privilege elevation bugs are still highly sought after since they allow anything that gets into the machine to make much deeper changes, that is, to operate with full root system kernel-level privileges.



So SandboxEscaper did not notify Microsoft, which leaves Windows users vulnerable until a patch is released to address the issue.  I mean, if this were a zero-day remote code execution against all fully patched Win10, Microsoft could be expected perhaps to release an emergency update, an out of cycle.  I don't think they're going to.  We've got the second Tuesday of the month.  You'll be cruising some river somewhere.



LEO:  It's called the Mediterranean Ocean, Steve.



STEVE:  Oh, you'll be on the Mediterranean while we're busily patching our Windows 10 systems.



LEO:  I'm not actually bringing a Windows system with me, so I won't have to worry.



STEVE:  Well, when you get home, do not delay, yes.  And we expect that second Tuesday of the month.  This is probably not going to be difficult for Microsoft to fix.  And so I will be very surprised if it's not bundled into what we get on September 11th.  Ooh, September 11th.



LEO:  Yeah, hmm.



STEVE:  So malware has been found leveraging a tricky new command-and-control channel discovered in the wild, actively exploited, affecting, interestingly, not only MS Outlook, but The Bat!.  I don't know why they chose The Bat!, but that's like this...



LEO:  I like The Bat!, actually.



STEVE:  I was going to say, it's a well-regarded, very complete email client.  I looked at it at one point when I was considering leaving Eudora because it's like, hey, it looks like a good alternative.  So this thing's called Turla, T-U-R-L-A, is the name given to it.  It was first seen in 2013, so five years ago, though it's largely been quiet.  In that code there are some timestamps dating back to 2009, but their authenticity is questionable.  Maybe they were some other stuff that was just brought in in 2013, not recently compiled.  ESET Security did a nice report with all the details about this Turla Outlook Backdoor, as they called it.  And what prompted them was they found a much updated and far more advanced version which transacts with its masters, wherever they are - get this, Leo - using email as the comm channel and PDFs as the carrier.



LEO:  Wow.



STEVE:  So, yeah.  So the bad guys can email a PDF to a user who is known to have been infected with Turla.  The actual backdoor is contained in a single self-registering DLL.  So all something has to somehow arrange is to be able to execute this DLL, either use regsvr32 or it's got a self-registration capability.  Get it to register.  It then becomes persistent and lives in the system.  When an Outlook or a Bat! user receives email, the backdoor is scanning their inbox, spots the PDF, grabs it and deletes it.  And apparently it actually creates a little transient appearance in the inbox, which might cause someone to do a double take.  It's like, what?  Wait.  Wasn't there something just there?  But then they just chalk it up to, you know, ghosts.



Anyway, the PDFs are weaponized that contain instrumentation.  The ESET guys reverse-engineered the technology, found a whole list of commands that can be encoded in PDFs to allow them to do things to the infected user.  So while the infection is present, all outgoing emails are forwarded to the attackers.  So it's a full spy on anything that user sends.  Metadata, so email addresses, subject, and attachment names of incoming emails, are logged.



And among the commands possible, the attackers can request that any file on the system be sent out via the backdoor.  The backdoor can execute additional programs and/or commands; can download additional files, exfiltrate anything, receive commands, as I mentioned, via PDF email attachments; and, ESET said, is highly resistant and resilient against takedown.  So an interesting and sort of unsuspected command-and-control channel.  Basically your MS Outlook or your The Bat! client are compromised; and, when someone sends that person a compromised DLL, that gives them command and control.  And then a PDF is packaged up and sent back out as by way of response.  So it uses PDFs as the transaction mechanism and has access to anybody it manages to get infected.  Wow.



Okay.  We talked about last week, we had fun with the idea of Chrome not wanting to be injected into.  And that coverage was  brought to us by Bleeping Computer.  Of course, and Bleeping Computer, as we know, they first got onto our radar because they were really the go-to website.  Lawrence Abrams is the founder of Bleeping Computer.  They were the go-to website for cryptomalware and were the first really good information about that.  So they reached out to a bunch of AV companies with whom they maintain of course good relations because that's their business.



BitDefender's Bogdan, looks like Botezatu, B-O-T-E-Z-A-T-U, who is a senior e-threat analyst for BitDefender, told Bleeping Computer's reporters that as of last Monday, August 20, BitDefender would no longer be monitoring Chrome 66 and subsequent versions with their anti-exploit technology.  Bogdan said:  "Starting with the Chrome browser v66, Google has gradually rolled out a new feature that prevents third-party software from monitoring the application's processes."  Well, we know that's not quite true.  It's that it's beginning to notify users if the browser crashes and then has a list of things that might be culprits.



Anyway, he says:  "While this measure ensures that rogue applications do not interfere with the Google product, it also prevents security solutions from inspecting the browser's memory in search of potentially dangerous exploit code.  With v66, Google Chrome displays" - okay, now he's explaining it correctly - "a post-crash warning asking users to remove the security solution if it monitors the browser's processes, even if the security solution is not responsible for the respective crash."  And, you know, frankly, as a developer, it's not clear to me how to ascribe responsibility.  If the browser just crashes, Google has watcher processes that will notice that there was a crash.  But it can't necessarily detect how, exactly, or who was responsible.  It's crashed.



Anyway, he says:  "In order to prevent this message from occurring and having users unwarily uninstall the security solution, which would leave them exposed to a vast array of online threats, BitDefender has issued an update to stop the Anti-Exploit technology from monitoring the Chrome browser."  In other words, BitDefender doesn't want you to remove it completely.  So they've just removed that one tentacle of BitDefender from Chrome so that you won't remove - so that their users won't be induced to remove all of BitDefender.  "The update was delivered to customers on August 20th at 7:00 a.m. Eastern time."



He finishes:  "As a leading global cybersecurity technology company, BitDefender is committed to providing cutting-edge, end-to-end cybersecurity solutions."  This is just a commercial.  "We regret being forced into removing protection for one of the world's most popular browsers, and we urge users to not uninstall their security solution they have installed on their computers."  So they were probably either already or correctly concerned that this would cause people to remove BitDefender if Chrome popped up and said BitDefender may be causing Chrome to crash.  I mean, why wouldn't you?  So they're stopping that.



Kaspersky said:  "Kaspersky Lab is aware of Google Chrome showing alerts that the company's applications are incompatible with the browser.  We have contacted Google to find a solution, and we are continuing to look for possible workarounds to resolve this issue."



LEO:  Yeah, but read the next paragraph.  Read the next paragraph.



STEVE:  Yes.  "Having our code injected into the Chrome browser is an important part of the overall Internet security approach implemented by security vendors to provide users with safe web surfing."



LEO:  I rest my case.  Oh, lord.



STEVE:   Yeah.  "For example, it is critical for a feature of Secure Input" - their name - "that blocks attempts of stealing sensitive data like credit card number, login, password, with malware (keyloggers) installed on user's devices."  Yeah.



And Malwarebytes, similar sort of thing.  Avast AVG, they were also quoted:  "We have fixed this issue, and our products are not reported by Chrome." 



LEO:  How do you fix it?



STEVE:  Well, you stop.  Basically all these guys are abandoning their real-time in-browser monitoring, which up until now they've all been doing in order to protect their customers.



LEO:  What do you think?  I mean, is that a legitimate useful thing?



STEVE:  I don't know what Google is thinking.  Google could offer documented hooks which would allow programs to do this.  They could protect them with "are you sure" and "are you really, really sure."  I mean, and so the idea being that the question is, or my assertion is, it could be done in a way which is compatible.  Microsoft has done this.  Microsoft - we went through the same process with Windows in the early days where AV stuff was getting its hooks in.  It was modifying API calls and causing instability for Windows.  Microsoft said no, but in return gave firewalls and AV sanctioned ways of filtering the same stuff.



And apparently, I mean, so also it's noted, remember that Chrome also said that Edge is not allowing, is intolerant of people sticking their hooks into Edge, not allowing injection into the Edge processes.  So Google is arguing we're doing the same thing with Chrome.  We're not wanting our browser to be crashed by other people's code which introduces instability.  I think that's valid.  What I don't have a read on is whether this is a "me too" checkbox so that everybody is saying, oh, yes, we hook your browser and are able to rummage around in its memory, or do we already have that protection from outside of Chrome?  I'm not on the inner side enough to know.  But I do know that Chrome could make it possible and sanctioned and safe if they chose to.  And apparently they're choosing not to at this point.  Maybe that'll change.



And what'll happen is users will just lose that protection.  They'll still keep their AV.  Chrome won't crash.  But there will be maybe additional vulnerability as a consequence.  And in fact Lawrence, who I think did the reporting for the story, is not happy at the idea that AV is being kicked out of Chrome.  His feeling is that's not a good thing for users.



Okay.  And heard of Ghostscript?  We all have, those of us who pay attention to what libraries get installed in our machines and what various things are using.  It is the number one most popular and very widespread interpreter for Adobe's PDF and Postscript that is not Adobe's.  And brought to us by none other than Tavis Ormandy of Google's Project Zero is the revelation of, and for which there is no current patch, a serious set of vulnerabilities in Ghostscript.



It is, as I said, it is the industry's number one open source interpreter for PS and PDF page description.  It's embedded in hundreds of applications and used by code libraries which allow desktop software and web servers to handle Postscript and PDF-based documents.  ImageMagick is one of the more popular apps which embeds Ghostscript.  Evince and GIMP also do so.  And pretty much any non-Adobe software which supports PDF creation, editing, viewing, or printing has Ghostscript in it somewhere.   Exploiting the vulnerabilities which Tavis has enumerated allows an attacker to take over applications and servers that use Ghostscript.  And I'll say again, no patch is available.



Tavis tweeted:  "This is your annual reminder to disable all the Ghostscript coders in policy.xml."  I should note policy.xml is a component of ImageMagick, and it allows you to control what the codecs, essentially, the image codecs which ImageMagick uses.  So you are able to disable ImageMagick's use of Ghostscript, which is what Tavis is strongly recommending.  And I note that Tavis's tweet reads as it does, that is, this is your annual reminder, because he's previously identified problems in 2016 and 2017.  He posted to the Chromium bugs page under the title "Ghostscript:  multiple critical vulnerabilities including remote code execution."  Then he said:  "I sent the following mail to the OSS security mailing list."  And of course that's seclists.org.  He said:  "These are critical and trivial remote code execution bugs in things like ImageMagick, Evince, and GIMP, as well as most other PDF and PS tools."



And so his posting reads - and I'm going to skip most of it, but it starts:  "Hello, this was discussed on the distros list, but it was suggested to move discussion to oss-security.  You might recall I posted a bunch of sandbox escapes in Ghostscript a few years ago."  Then he links to it.  He says:  "I found a few file disclosure, shell command execution, memory corruption, and type confusion bugs.  There was also one that was found exploited in the wild."  So this is under active use.



"There was also a similar widely exploited issue that could be exploited identically."  Then he says:  "TL;DR:  I strongly suggest that distributions start disabling PS, EPS, PDF, and XPS coders in policy.xml by default."  So the idea would be, if distributions did that, then things like ImageMagick would be safe for their users pending Ghostscript being fixed.  But Tavis is not suggesting that.  He thinks Ghostscript is beyond repair.  And of course I use the "I" word.  It is an interpreter.  And it's a huge, massive, hairy interpreter, and we know what a problem interpreters are.



Anyway, so I skipped a whole bunch of stuff in his posting.  And he says:  "For example of what these exploits [he enumerates] do," he says, "This means you can create a file in any directory."  And he says:  "I don't think you can prevent the random suffix," which refers to his exploit.  He says:  "Additionally, I have a trick to let you read and unlink any file you have permission to."  And he says later:  "This can be used to steal arbitrary files from web servers that use ImageMagick by encoding file contents into the image output."  And he says:  "See my previous proof of concept here for an example, i.e., you can convert malicious.jpg thumbnail.jpg, produce an image with the contents of a file visible" to the ImageMagick app, or the DLL which is where Ghostscript is located.



He says:  "These bugs were found manually," meaning just by his inspection of the source.  He says:  "I also wrote a fuzzer, and I'm working on minimizing a very large number of test cases that I'm planning to report over the next few days."  So more may be coming.  He says:  "I'll just file those issues upstream and not post each individual one here.  You can monitor," he says, it's "https://bugs.ghostscript.com if you want to.  I expect there to be several dozen unique bugs.



"In the meantime," he finishes, "I really *strongly* suggest that distribution start disabling PS, EPS, PDF, and XPS coders in policy.xml by default.  I think this is the number one unexpected Ghostscript vector.  In my humble opinion," he says, "this should happen ASAP."  So anyway, he feels that the whole issue of it is a fragile security boundary that should not be tested.  So I imagine we will be seeing Ghostscript updates.  On the other hand, it being as popular as it is means that it is spread all over systems that use Ghostscript for PDF rendering applications and will be difficult to root out completely.



And under the category, quoting Bruce Schneier, of "attacks never get weaker, they only get stronger," a presentation at that recent USENIX conference two weeks ago in Baltimore detailed a new technique for retrieving the public key encryption keys, meaning also the private key, from electronic devices.  And that's been done, you know, the issue of Tempest-style attacks using RF emanations from things has been known for decades.  So that's not news.  However, these guys have figured out how to do it in a single shot, which makes it vastly more effective than any previously known attacks.  In other words, attacks never get weaker, they only get stronger.



So two weeks ago in Baltimore a team from Georgia State University suddenly made this style of attack practical in the real world.  And it's worth noting this is a good lesson in why it's worth pursuing even weak vulnerabilities like Meltdown and Spectre arguably are, since we've never seen them in the wild, and as far as we know they've never been weaponized.  But you have to say "yet" because, similarly, an attack which used RF, which you could only get to by opening the case of devices and putting a probe on the backs of the chips, which is pretty much what was needed until now - oh, and doing over and over and over, re-use those keys you're trying to extract.



You could argue, okay, yeah, so chips are going to leak some RF.  Okay.  But you've got to open the device, you've got to stick an antenna on the back of the chip, and then you've got to make it do its thing for hours while you collect all the data and process it.  You could say, eh, not that exploitable.  Well, in one jump we go to a single operation no longer requiring that the device be opened.  You can get enough signal from the outside.  Now this thing suddenly goes practical.  The point is, impractical things can suddenly become practical, and we may very well see this with all these microcode problems of this incredible install base of Intel chips that we have now.



Their paper was chillingly titled "One & Done:  A Single-Decryption EM [electromagnetic] Based Attack on OpenSSL Constant-Time Blinded RSA."  In other words, this was a good implementation of RSA using constant time in order to prevent a side-channel attack based on how long the operation took.  Their abstract reads:  "This paper presents the first side-channel attack approach that, without relying on the cache organization and/or timing, retrieves the secret exponent from a single decryption on arbitrary ciphertext in a modern, that is, a current version of OpenSSL, fixed-window constant-time implementation of RSA.



"Specifically, the attack recovers the exponent's bits during modular exponentiation from analog signals that are unintentionally produced by the processor as it executes the constant-time code that constructs the value of each 'window' in the exponent, rather than the signals that correspond to squaring/multiplication operations and/or cache behavior during multiplicand table lookup operations."  So they basically used different information that allows them to do it in a single shot.  They said:  "This approach is demonstrated using EM emanations on two mobile phones and an embedded system; and after only one decryption in a fixed-window RSA implementation, it recovers enough bits of the secret exponents to enable very efficient - within seconds - reconstruction of the full private RSA key."



They said:  "Since the value of the ciphertext is irrelevant to our attack" - meaning all they want is the key - "the attack succeeds even when the ciphertext is unknown and/or when message randomization, that is, 'blinding' is used."  They don't care.  "Our evaluation uses signals obtained by demodulating the signal from a relatively narrow band [40 MHz band] around the processor's clock frequency of around a gigahertz, which is within the capabilities of compact sub-$1,000 software-defined radio receivers."



They say:  "Finally, we propose a mitigation where the bits of the exponent are only obtained from an exponent in integer-sized groups, for example, tens of bits, rather than, as is currently being done, obtaining them one bit at a time."  They said:  "This mitigation is effective because it forces the attacker to attempt recovery of tens of bits from a brief snippet of signal, rather than having a separate signal snippet for each individual bit.  This mitigation has been submitted to OpenSSL and was merged into its master source code prior to the publication of the paper."  And finally they said:  "In our experiments we place probes very close, but without physical contact with the unopened case of the phone, while for the embedded system board we position the probes 20 centimeters away from the board, so we consider the demonstrated attacks to be close proximity and non-intrusive."



So anyway, I liked this as a perfect example of why something that has not been a problem suddenly jumps to becoming a problem.  The good news is OpenSSL will have an update in its code as a consequence to prevent this.  And to the degree that devices ever get updated - and again, lots of devices are never going to.  So now what we have is a practical attack without contact, meaning just proximity.  And certainly, for example, any router that is using OpenSSL's library for its work, it's in a plastic box, probably from the outside is going to have its emanations detectable.  



And in their detailed presentation, which I won't and haven't gone into, they talk about how laying the phone on the counter at Starbucks or in any environment where the phone is in contact with a surface, what could be behind that surface is something close enough to, if the phone does any pre-patched OpenSSL work - and again, OpenSSL is the library that underpins certainly all of the open source crypto that we see.  And even non-OpenSSL, yeah, non-OpenSSL, which is using the same bit-at-a-time approach that OpenSSL was previously using would also be vulnerable.  So I just - we have to, I'll say it again, we have to continue to allow security researchers to poke at our systems because all they are doing as a consequence is making them stronger.



LEO:  Oh, boy.



STEVE:  So this is not - I guess it could be worrisome depending upon your situation.  OpenSSH, Open Secure Shell, which is super widely used...



LEO:  Oh, I use it every day to log into my server, yeah.



STEVE:  Yes, yes.  Since day one, since its birth several decades ago, for the past two decades, every version of OpenSSH server has had a worrisome security flaw.



LEO:  Oh, no.



STEVE:  Yeah.  Now, again, not probably terminal.  But the title of the problem posted to seclists.org is "OpenSSH Username Enumeration."  In examining an unrelated source code fix and commit, researchers at Securitum.pl realized that a potentially bigger problem had also been inadvertently fixed.  So again, the reason it's so valuable to have open source and eyeballs looking at it.  They wrote:  "We realize that, without this patch, a remote attacker can easily test whether a certain user exists or not, thus enabling username enumeration on a target OpenSSH server."  In other words, username without password.  And as we've talked about often, what you really don't want is a website that says, first of all, who are you, and then says we know you or we don't, before asking you for your password.  The proper design is give me your username and your password, and then we're not going to tell you, if we're not happy, why we're not happy.  We're just going to say one of those is wrong.



And in fact we saw sort of something just like this with the failure of the WPS protocol on WiFi.  Remember that it was supposed to be eight digits.  It was really seven because the last digit is a check digit which could be computed from the other seven.  But even seven would be pretty secure.  Except that it turns out you could chop it in half.  You could separately check the first and then check the second.  Well, this is a little bit like that.  You can separately check for usernames.  And then, once you've got the username, then you're able to pound on the password.  So it does arguably reduce the security of an OpenSSH server.



LEO:  I use a public key login.



STEVE:  And I would never do anything else.  I do the same thing.  Oh, absolutely.



LEO:  Okay.  So this isn't vulnerable if you're using public/private key logins.



STEVE:  Correct.



LEO:  Okay.



STEVE:  So what they said is the attacker can try to authenticate a user with a malformed packet.  And they literally - essentially they truncate the packet.  What they discovered is the return from a malformed packet is different if the username is known versus if it's not.  And so based on that differential error that is returned, that allows the server to be probed.  They said:  "We believe that this issue warrants a CVE.  It affects all operating systems, all OpenSSH versions."  They said:  "We went back as far as OpenSSH 2.3.0, released in November 2000," but as far as they know it's always been there.  And they said:  "...and is easier to exploit than previous OpenSSH username enumerations, which were all timing attacks."  Previous ways of doing this, you know, you would get a slightly different timing, so you'd have to repeat it in order to disambiguate network-based timing.  And this is much easier.



So they said:  "We also believe that this should be posted to oss-security right away.  The issue, that is, the commit, is already public.  And if we spotted it, then others not so well intentioned did, too.  We are at your disposal for questions, comments, and further discussions."  So anyway, again, not super worrisome.  And I'm like you, Leo, both with OpenSSH and also, for what it's worth, with OpenVPN.  I'm only using certificates in order to do the authentication because that's much stronger.



And get a load of this.  We old farts know about AT commands.  You know, you type ATA and hit Enter, and then out on the terminal says okay.  And then you proceed from there, you know, ATDT and so forth, for dial touch tone.  Yeah.  Turns out a final gift from the Baltimore USENIX conference was a paper titled "Attention Spanned:  Comprehensive Vulnerability Analysis of AT Commands Within" - believe it or not - "the Android Ecosystem."  AT commands are not gone.  In fact, they have quietly been hugely expanded.



A multisource team of 11 researchers from Samsung Research America, Stony Brook University, and U of Florida took a close look into the undocumented AT commands currently supported on contemporary Android devices, and what they found was surprising and a little worrisome.  The team analyzed and reverse-engineered the firmware images of more than 2,000 Android devices from 11 major Android OEMs:  ASUS, Google, HTC, Huawei, Lenovo, LG, LineageOS, Motorola, Samsung, Sony, ZTE, and others.  They discovered that these devices support over 3,500 different types of AT commands, many of which grant access to very dangerous functions.



Now, of course you've got to have a web domain; right?  ATcommands.org, Leo.  Nice-looking graphics and presentation.  ATcommands.org.  The abstract of their paper reads:  "AT commands, originally designed in the early '80s for controlling modems" - which is what we were all using them for - "are still in use in most modern smartphones to support telephony functions."  Remember how we've talked about how there's a baseband processor which is really obscure and, it is believed, as a consequence of lack of lots of oversight, probably full of problems.  The processor that we interact with, with a smartphone, is not the baseband processor.  It's a separate subsystem.  And AT commands are often used for them to talk to each other.



So their abstract says:  "The role of AT commands in these devices has vastly expanded through vendor-specific customizations."  Because, like, okay, there's the channel, it's established, and developers are just adding to what's already there.  Anyway, they said:  "Yet the extent of their functionality is unclear and poorly documented."  And then I put in here parenthetically:  In other words, deliberately undocumented.



They said:  "In this paper, we systematically retrieve and extract 3,500 AT commands from over 2,000 Android smartphone firmware images across 11 vendors.  We methodically test our corpus of AT commands against eight Android devices from four different vendors through their USB interface and characterize the powerful functionality exposed, including the ability to rewrite device firmware" - get that, yes, an AT command can rewrite device firmware through the USB port - "bypass Android security mechanisms, exfiltrate sensitive device information, perform screen unlocks, and inject touch events solely through the use of AT commands."  And who wants to bet that that's not the way the companies like Cellebrite are getting up to some of their games, is they already know this, yet they're not exposing it to daylight the way security researchers are.



They say:  "We demonstrate that the AT command interface contains an alarming amount of unconstrained functionality and presents a broad attack surface on Android devices, and all available through most devices' USB port."



So I've got here in the show notes ATcommands.org is the site, for anyone who's interested.  And ATcommands.org/atdb/vendors will take you to a page where you can check your own phone, if you've got an Android device, your own phone and what they found about your vendor and phone model.



LEO:  I'm baffled that you would support AT commands.



STEVE:  I know.



LEO:  It's bizarre.



STEVE:  I know.  And it's still in use today because it's always been there; and they just said, oh, well, let's just add some more proprietary AT commands.  And so, like, the screen, when you do touches on the screen.  And maybe it's part of production testing, like you plug the device into a special USB...



LEO:  Oh, yeah, because you can have a serial port kind of controlling the testing, yeah.



STEVE:  Right.  And in order to simulate touch events.  But they ship it that way, Leo.



LEO:  That's nuts.



STEVE:  So then they have a little Q&A.  "How does this affect me?"  They say:  "On some Android smartphones, an AT command interface is exposed over USB without USB debugging enabled."  Meaning generic.  "Unfortunately, some devices do not authenticate this interface or allow it to be used from the lock screen.  We found that in some cases the 'charge-only' USB mode may also fail to block AT commands."  So even when you have it set to only allow charging, still happily chatting away over the AT commands.  "This means unsuspecting users who plug in their phones to a USB port for charging..."



LEO:  Oh, my god.



STEVE:  Uh-huh.



LEO:  So even if I have that condom you gave me, I could still get - no, because that's only power.



STEVE:  No, no, no, that's only charging, yes.



LEO:  You'd still have to have data lines, okay.



STEVE:  Yes, yes.  But if you plug it in just because you want a charge, it can be malicious and reach into your phone and do a full takeover.  "This means," they said, "unsuspecting users who plug in their phones to a USB port for charging or data transfer may have their devices locally compromised by a possibly pre-recorded sequence of AT commands.  Furthermore," they write, "many commands, such as those for exfiltrating sensitive data, have no visible side-effects."  In other words, the phone looks just fine.  You don't know what's going on underneath.



LEO:  Oh, my goodness.



STEVE:  So they said - I know.  They said:  "Have you found any vulnerabilities?  Yes.  We have notified each vendor of any relevant findings and have worked with their security teams to remediate the issues."  So not only are their devices wide open, but they're not even secure.  They've got vulnerabilities in their handling of the AT commands.



And then they finally said:  "Did you find any remotely exploitable vulnerabilities?"  They said no, but listen to the caveat.  "All of our investigation centered on the device's USB connection.  We did not investigate remote AT attack surface.  But the first places we would look would be the Bluetooth interface and the baseband."  Meaning over the air.



LEO:  Oh, man.



STEVE:  Yikes.



LEO:  I mean, you're doing keystrokes.  You're doing...



STEVE:  Yes.



LEO:  Basically you can interact with the interface in any way possible.



STEVE:  Complete remote control of the phone.



LEO:  Wow.



STEVE:  By plugging it in.



LEO:  And even where it says "Do you allow this?" you can click the "Allow" button.



STEVE:  Yes.



LEO:  So the fact that the UI says, "Are you sure?"  No, it's like UAC, you can just click it.



STEVE:  That's right.



LEO:  Holy cow.  Oh, man, that's terrible.



STEVE:  Yes.  So probably worth checking.  And again, for phones that are current, this comes back to what you and I were talking about when we were talking about the man in the disk and the similar things.  If you're going to go Android, go big.  Go with somebody like a Samsung or a Google who are actively staying on top of the security of their devices because they're neat; they're very capable; they're more open than Apple phones on the iOS platform.  But there is a serious tradeoff for vulnerability.  The good news is these guys talk to the vendors, and the vendors will likely respond.  But wow.  Nice piece of research.  And, whoo, worrisome.



I've a bit of miscellany.  Ryan Dodds, he wrote on the 17th:  "@SGgrc Steve, did you ever find the perfect coffee travel cup?"  He said "cup."  I'll go with "container."  He says:  "I'm struggling.  All travel cups are usually plastic and make the coffee taste awful."  And so Ryan, and any other listeners, there's only one word that I have, and that's Contigo.  And that seems to be the universal answer to that question.  Contigo is a company, C-O-N-T-I-G-O.  That's what you want. 



They do make some plastic travel containers for cold beverages, but they make absolutely, end of story, the best hot coffee transport ever.  They've got a really good closing mechanism.  You're able to squeeze it, and it opens.  I don't even know how it works.  It's so good.  It just absolutely doesn't drip, but it opens an air hole and the coffee drinking hole at the same time.  Contigo, C-O-N-T-I-G-O.  They're also of course double-lined vacuum containers so they keep the coffee hot for hours.  Anyway, that's what you want, Ryan, and anybody else.



LEO:  What happened to the Temperfect Mug?



STEVE:  Ah, it was a gimmick.  You know.  In fact, did it ever come?  I guess it did.



LEO:  Yeah, we both got them.  I use it once in a while.  You can't dishwash it.  So it's kind of...



STEVE:  Ah, that's a problem.



LEO:  I mean, that's the problem for me, yeah.  I use a Contigo, as well.  I like the Contigos.



STEVE:  Yeah.  And Temperfect had all this problem with that shutter.  That's a real problem.  That shutter thing, you know, it just doesn't work very well.  So it's like, eh, no.  Contigo is what everybody wants.  And the other thing everybody wants, not surprisingly, is a copy of SpinRite.



LEO:  As they should.



STEVE:  As they should.  Floris, who tweeted a little earlier this month, I just saw his tweet, said:  "Just had to help a company figure out moving their super low RPM old half-broken 100-plus hard drives on slow USB1 and USB2 to a new Thunderbolt RAID system."  He says:  "I have a feeling I will be using @SGgrc's SpinRite a lot next week."  He says:  "Time to push that company to buying a proper license."  And Floris, in this instance, yeah.  If you've got more than a hundred hard drives, that kind of pushes my extremely tolerant, give it a try, fix your mother's or your friend's hard drive, and I'm not going to care.  A company that's got more than a hundred drives, and you're moving them, you're resurrecting them and checking them out and moving them over to Thunderbolt RAID, I think they can afford four copies.  That's the deal.



An individual buys one copy.  A company, we like them to have four in a so-called "site license."  That seems fair since we're letting them use SpinRite forever on all the systems that they have, either spinning or solid-state, because we know that it repairs both.  And I kind of came up with that because that way if someone can buy one, if it works, then they can buy just three more.  They don't have to say, well, you know, I already got one, so how do I buy the corporate license, blah blah blah.  As long as you maintain four current licenses, then you're good to go with a corporate site license.



And whenever I heard three yabba dabbas, I'm happy because that says, hey, somebody got one, they tried it, and then they said, hey, we want to be licensed to use it as a company.  So I always appreciate hearing that.  So thank you, everybody, for doing that.  And Floris, thanks for helping this company.  And do give them a little push because a couple hundred drives, or more than a hundred, that's probably worth a license.



LEO:  Time to buy one, yeah.



STEVE:  And that's our podcast, my friend.



LEO:  What?  What?  How could it be over?  Wait a minute.  Wait a minute.  What about never a dull moment?  I guess there's no big story.



STEVE:  There was just all of these things.



LEO:  All of the little ones added up to one big one.



STEVE:  Well, and frankly, as I was thinking about this, I probably should have moved the AT commands down to the end and made that - because it's really a biggie.  But what the heck.



LEO:  That's amazing.  That one's terrible.  I was just looking.  The Pixel 2 phones are not, according to that site, vulnerable.



STEVE:  Correct.  I did notice that, too.



LEO:  Yeah, yeah.  Whew.



STEVE:  And some of them are only vulnerable if rooted.  So rooting is again a reason not to.



LEO:  Steve.  There's never a dull moment with Steve Gibson.  You find him at GRC.com.  That's where SpinRite lives, but also all sorts of other great stuff.  It's really kind of a must-visit site for your geek travels:  GRC.com.  He's @SGgrc on Twitter.  You can tweet him.  You can direct message him if you've got a tip or question, a comment.  You can also go to GRC.com/feedback to leave the question there.  But I encourage you to go and get SpinRite, whatever else you do, because that is the ultimate disk recovery and maintenance tool.  He also has podcast copies there, audio, and it's the only source for the transcriptions.  So that's a good thing to have if you like to read along while you listen.



We have audio and video at our site, TWiT.tv/sn.  We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to watch, you can watch us do it live:  TWiT.tv/live.  You can also chat in the chatroom, irc.twit.tv with all the other people watching live.  But on-demand copies are also available for you at TWiT.tv/sn or Steve's site.  Or better yet, maybe even subscribe, audio or video.  That way you'll have it the minute it's available, and you can keep a collection, have an entire archival system of all the episodes going back to Honey Monkeys.



STEVE:  And I'll note, Leo, that we do this even when you are on the Mediterranean because I am never on the Mediterranean.



LEO:  You never take a break.  We actually have to force Steve to take breaks for holidays.  Yeah, it'll be Jason Howell filling in for the next three weeks on Security Now!, and I will be back on the 25th.  So I won't see you for about a month.



STEVE:  That's going to be weird, but we'll still be here.



LEO:  Nothing's going to happen.  It'll be quiet.



STEVE:  That's right.  The industry's going to take a vacation when you do.  So that'll be just - that will be good.



LEO:  No reason to have any security flaws while I'm on the ocean.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#679

DATE:		September 4, 2018

TITLE:		SonarSnoop

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.GRC.com/sn/SN-679.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we cover the expected exploitation of the most recent Apache Struts vulnerability, a temporary interim patch for the Windows zero-day privilege elevation, an information disclosure vulnerability in all Android devices, Instagram's moves to tighten things up, another OpenSSH information disclosure problem, an unexpected outcome of the GDPR legislation and sky-high fines, the return of the Misfortune Cookie, many thousands of Magneto commerce sites being exploited, a fundamental design flaw in the TPM v2.0 spec, trouble with MITRE's CVE service, Mozilla's welcome plans to further control tracking, a gratuitous round of Win10 patches from Microsoft - and a working sonar system which tracks smartphone finger movements!



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm here in place of Leo Laporte, so I'll do my best to keep up.  We're going to be talking all about some updates on the Apache Struts vulnerability from last week.  There's a new vulnerability in all Android devices that aren't running Pie which is basically all Android devices.  A very unexpected outcome of the GDPR legislation in the EU.  And we're going to talk about something called SonarSnoop, and it's really kind of strange and interesting.  You won't want to miss it.  Security Now! is next.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 679, recorded on Tuesday, September 4th, 2018:  SonarSnoop.



It's time for Security Now!, the show where we talk about all the latest security news, everything that's happening on the security front.  Leo is out for the next couple of weeks, actually.



STEVE GIBSON:  Three.



JASON:  This week and the next two weeks; right?



STEVE:  Yup.



JASON:  So three solid weeks you're going to have to listen to me going, "What did Steve just say?"  That's basically what my role is on Security Now!.  Steve Gibson, you are the man with all the knowledge.  How are you doing, Steve?



STEVE:  Hey, Jason.  Great to be with you for the first of three.  And you did this, what, about a month and a half ago, I guess.



JASON:  Yeah.  



STEVE:  And it went well.



JASON:  Yeah.



STEVE:  And so we sort of have our routine down.  I think it's going to be great.



JASON:  I think it's going to be good.  I will almost undoubtedly play the role of person asking you to - maybe not asking you to repeat yourself, but having a very large question mark above my head at the beginning of the story, and maybe a checkmark at the end of the story.  It'll be like, ah.



STEVE:  Well, I guess since yesterday was Labor Day, this marks the official beginning of, well, is it winter?  At least it's end of summer.



JASON:  It feels, yeah, it feels like fall-ish.



STEVE:  As we plow into September; right?  So off we go.  So the most interesting piece of, well, the most interesting thing that happened in the last week, I think.  Sometimes, as the last time you and I were together, it was just - there wasn't anything that really stood out.  It was just like lots of news.  This time there was, I mean, it was easy to pick one because some researchers also showed a paper, now it's been about three weeks ago, the USENIX security symposium in Baltimore, where they surprised everybody by figuring out how to use the colocated microphones and speakers on, in this case it was a Samsung S4 smartphone, to essentially disambiguate the unlock strokes that the phone user was using in what they called "SonarSnoop."



So it actually turns the microphone and speakers, or microphones and speakers because there's two of each, into working sonar in order to allow them to track the person's finger movements on the screen.  So a really interesting piece of research there.  And to me it feels like, because I read the PDF, and I think it was 18 pages of rather detailed research, I think this is something that, if anyone is interested, can be much further refined.  But anyway, we'll get to that.



We did have a bunch of stuff.  As expected, and as happened previously with an Apache Struts vulnerability, we have now seen lots of action on it in the past week, which we'll talk about.  We've got an interim patch for last week's announced zero-day privilege elevation surprise.  An information disclosure vulnerability present in all - I wrote here in my notes "all Android," but actually it's all before Android 9, a.k.a. Android P.  Did that ever get an official name, by the way?



JASON:  Yeah, it's Pie.  It's Android Pie.



STEVE:  Okay, Pie.  Okay, good.



JASON:  And, I mean, really, those two are distinctions without much of a difference.  How many people really have Pie right now?  It's a pretty small percentage.



STEVE:  Yeah, because I had in my notes "pre-P."  And I thought, what?  No.  So now it's pre-Pie.



JASON:  Right.



STEVE:  We also have Instagram, after a bunch of high-profile hacks, has finally, like, okay, reluctantly it seems, moved to tighten things up.  We'll talk about that.  Another OpenSSH information disclosure problem which is being handled differently than the last one in an interesting fashion.  We've got an unexpected outcome of the GDPR legislation that took effect three months ago in the EU, not anything I would have predicted, but we'll talk about that.  Also the return of something we discussed in 2014 called the Misfortune Cookie that affects medical devices.  Also many thousands, I mean many thousands of Magento commerce sites are being exploited with a skimming script.  We also have, believe it or not, a fundamental design flaw in the Trusted Platform Module v2 spec, which is what all of us now have, certainly in corporate servers and desktops.  And many standard desktops have TPM protection in them in order to secure the boot process.  Turns out that can be circumvented, so we'll talk about that.



Also we often, well, we're always referring to CVE numbers.  It turns out that MITRE is the contractor, kind of of the U.S. government, that is running the system.  And they've been having problems lately which maybe the government is going to come to their aid.  Mozilla has announced plans to improve tracking control and some other aspects of Firefox.  And we've got a gratuitous round of Win10 patches from Microsoft which actually my machine, my Win10 machine which I run Skype on, managed to get through this morning.  I always deliberately start it hours in advance in order to let it get this out of its system beforehand.  So that's done.  And then we're going to wrap up with a discussion of a new side channel attack, bouncing sound waves off of the user's finger as they unlock their phone.  So I think another great podcast for our listeners.



JASON:  Absolutely.  It's jam-packed and full.  I've got to say real quick you're a brave man to do your Windows updates before the show.  Even if it's hours in advance.  Maybe that's different now.  But I almost always feel like I have to push those off until after the show because you just never know.



STEVE:  Well, if you get behind it could be really bad.



JASON:  Oh, for sure.



STEVE:  Like I went to a conference a year ago where someone had a laptop that I kind of got the sense they're normally deskbound, and so they're using their normal probably desk system.  But they grabbed their laptop, and off they went to the conference.  They plugged it in and turned it on.  And it was like half a day.  I mean, they were, like, sitting there not able to do anything while it caught up with everything that had gone on while it hadn't been alive.  So if you bring Windows up every week or two, you're probably not going to fall too far behind.



JASON:  Yeah, right.  And don't get caught using conference WiFi to make all those updates.  That's going to take you even longer.



STEVE:  Yeah, that's true.  That's true. 



JASON:  That's like nightmare on top of nightmare.



STEVE:  That, too.  That, too.



JASON:  Well, we've got a lot of that stuff to talk about.  Obviously a bunch of security news to kick off with.  All right.  So we've got a stack of security news here.  And apparently you made a prediction.  Did you make this prediction last week, as far as Apache Struts is concerned?



STEVE:  No.  Well, we do need to first look at our Picture of the Week.



JASON:  Okay.  Let's do Picture of the Week.



STEVE:  It's one of those where you really can't make this up.  I just got a kick out of this.  Someone sent this to me, and I've had it in my list of pictures that we would get to when we had an opening, when there wasn't something else happening.  So this is a screenshot that appears authentic, I have no reason to doubt it, which is the Windows error reporting dialogue, which has been presented to someone announcing that Windows Error Reporting has stopped working.



JASON:  Oh, boy.  How is it able to do that, if it's stopped working?



STEVE:  I guess it maybe squeezed this in just before it died.



JASON:  It was its last breath.



STEVE:  No, that wouldn't work because it would say that it was stopped working, and then it was going to try to show you the dialogue.  So I don't know.  This is very fancy coding on Microsoft's part.  Apparently it's able to anticipate the fact that it is about to die, and so let's show them a dialogue saying goodbye and goodnight as our last act.  I don't know.



JASON:  Oh, boy.  Yeah.  Seems a little fishy to me.



STEVE:  This is just so good.  It's - yeah.



JASON:  Or maybe once it crashed - because you can't anticipate a crash necessarily.  But you can look backwards once it relaunches and say a crash happened.  So maybe it pops it up when it relaunches somehow.  Or I don't know how it's able to do that.  That's very strange.



STEVE:  Yeah, well, and it says in the fine print a problem caused the program, in this case Windows Error Reporting, to stop working correctly.  So I guess that means to start working incorrectly.  Windows will close the program and notify you if a solution is available.



JASON:  Oh, so in that case it hasn't actually closed it yet.  So that's how it was able to put it up.  Because if that was the program, then it's saying here's the error message, we're going to close it now.  And so when you...



STEVE:  Okay.  Or I guess maybe technically it could be reworded.  Windows Error Reporting is about to stop working.



JASON:  Right, exactly.



STEVE:  And so it's like, oh, here we go, this last chance.



JASON:  And there's no button for "No, don't stop working."



STEVE:  There is always the power button.



JASON:  That's true.



STEVE:  So, okay.  Prediction.  Yes.  I did predict it, only because of what we recently saw.  So Apache Struts is a very popular Java server-side application framework which is notably used by, I think it's 70-some, I don't remember the number now, but like a lot of the Fortune 100 major corporations in the U.S.  It's just very popular.  And it's had problems.  It is the reason that Equifax had that breach that ended up costing them $600 million last year.  What happened was there had been like six months before a notice of a serious problem with the Apache Struts framework which allowed unauthenticated remote access.  Equifax, one of those big corporations that uses Apache Struts, they just never got around to updating their Apache Struts, which allowed bad guys in, and their entire database got exfiltrated.  So the industry hopefully took this as a wakeup call.



We did have, about three months ago I'm guessing, a different problem found.  And, oh, I remember the way it was.  It was on a Wednesday of whatever week it was.  The announcement was, from the Apache Struts guys, a supercritical vulnerability is going to be fixed, patched a week from now, at some exactly certain time of the morning.  So everybody, I mean, this is bad.  So if you are affected by this, get ready.  You're going to want to update your system immediately because, the developers wrote, we don't think it's going to take anybody long to reverse-engineer what we've fixed and then start scanning the Internet for vulnerable systems.  It took hours before the vulnerability was understood, scanning began, and I think it was like three days before we began to see malicious attacks based on it.



So based on that, when we last week covered the fact that there was, whoops, another problem, not quite so bad, because the default, the vanilla installation of Apache Struts was not vulnerable.  Now we know more about it.  And there's a package known as Struts Convention which is a plugin that by default flips some switches to true, which enables this vulnerability.  So the guy who found it last week explained that all Struts were vulnerable unless patched.  And that's from versions 2.3 to 2.3.34, and on the 2.5 channel from 2.5 to 2.5.16, which are the ones that are actively in use and supported now.  They're all vulnerable.  So the discoverer of this explained that, even if you weren't now vulnerable, a configuration change downstream could make you vulnerable.



Now we know, we have all the details about that.  The guys from Palo Alto Networks - I think that was Palo Alto Networks.  I'm looking for it.  I had it in my notes.  Anyway, yeah, Unit 42 at Palo Alto Networks.  They explained in detail what was necessary and also connected this to this Struts Convention plugin which would by default make any site using it vulnerable.  So the guy who found it last week said, even if you're not vulnerable today, really update.  And as we said last week, boy, given what happened to Equifax, unless you've got an extra 600 billion, I mean million, lying around, you probably wouldn't have that much exposure, but still worth updating yourself.



And so the prediction part is that, sure enough, within, well, actually what Palo Alto Networks said in their note was they said:  "Some have noted that a previous critical Struts vulnerability was actively attacked last year only three days after the release of the security update and vulnerability information."  And they continued, saying:  "There are no known active attacks at this time."  And, by the way, this was being written two days after the vulnerability disclosure.  "And the current requirement that two non-default conditions need to be met for the vulnerability to be exploitable makes for a different threat environment."  In other words, less concern, to some degree.



However, the vulnerability was disclosed on August 22nd.  They wrote that on the 24th.  Active scanning for vulnerable systems was observed on the 25th.  Two cybersecurity firms, GreyNoise Intelligence and Volexity, have detected threat actors scanning for Struts servers last week, but they did not identify any attempts of exploitation.  And as we know - and again, this pattern is repeating.  So scanning precedes attacks.  Attacks are expected.  And there has been now, since then, some observation of coin mining being installed on vulnerable servers.



However, the most fun about this whole thing, and I'm hoping you're going to be able to put this on the screen, is that, for all time, the bar has been raised on vulnerability disclosure and demo sites.  Our listeners will certainly remember the Heartbleed problem from last year.  It had a great logo, this notion of, oh, vulnerabilities need good names, and they need good logos.  And of course we have Spectre, the little ghost holding the stick and so forth.  Anyway, I have a link in the show notes to the Apache Struts CVE, and this is CVE-2018-11776.



JASON:  Oh, yeah.



STEVE:  You're now showing it in the video stream.  And it's like, okay.  This has, for all time, upped the ante on a page which discloses the vulnerability and details it.  This actually has a complete tutorial on the exploitation of the vulnerability.  So anyway, it's becoming a little bit of a PR competition now when people come up with new vulnerabilities. 



JASON:  I think the next step is that they're going to embed a full 22-minute cartoon that explains.  That's the obvious next step.



STEVE:  Wow.  So also last week we had a person who - I just used the pronoun "he," not knowing any better.  SandboxEscaper was the person's Twitter handle.  And I just said "he" without knowing.  I later picked up some feedback saying that this was actually a woman.  I don't know - a female.  I don't know how I would be expected to know that, but somebody did.



So for what it's worth, she tweeted, it was sad, to the effect that she really didn't care what happened to her in life any longer.  She had found a vulnerability in Windows, Windows 10 64-bit, which allowed an attacker with no privilege to escalate or elevate to full system privilege.  And she didn't feel like reporting it for whatever reason to Microsoft, so she just told the whole world about it, posted the proof of concept on GitHub and said here you go.



So, now, this is not super critical.  It's not a remote code execution zero-day, where suddenly every Windows 10 system on the Internet would be vulnerable.  But still, a privilege elevation, I mean, the reason we have privilege controls on all modern operating systems is we create this notion of a root user who can do anything and a typical user who administratively and deliberately limits what they can do so that, if a program there running on their account happens to try to get up to some mischief, it won't be able to go rewrite the OS files and so forth.  So the point is that, while by itself it isn't super crucial, it is absolutely something that any malware would want in its toolkit because, until fixed, it allows such a program to do whatever it wants such that even the guest account, even a program running as guest is able to do anything it wants to on the system.



So now we know much more detail about it.  It's been vetted by a number of other researchers.  We discussed one last week that had already confirmed that this thing was legitimate.  Since then a small tweak has been found that widens its scope to 32-bit systems, as well.  And due to a hardcoded filename, it was Win10 only.  Changing a digit 3 to a digit 1 allows it to also attack Windows 7.  So its scope has broadened.



The good news is it's such a simple oversight in one API.  This actually is in the Task Scheduler.  The Windows Task Scheduler API has a function, Schedule RPC, which is Remote Procedure Call Set Security [SchRpcSetSecurity].  That function fails to check permissions.  So anybody, even a guest who has deliberately restricted access rights, can call it and use it to set file permissions on any local file.  The way the exploit works is it allows a hard link to be created and then calls a print job using the XPS printer, which ever since Windows XP Service Pack 2 has been present by default in Windows systems.  That allows the spooler process to invoke a hijack DLL which has been given full system privileges.  And then you're off to the races, and this thing can do anything it wants to on your system.



So it's expected that Microsoft will fix this easily.  There is a company, Acros Security, which has been previously publishing a series of what they call "micropatches," which are basically little code tweaks to existing programs which may not be updated by their publishers because the publishers don't care.  The software has gone out of support or whatever.  They have produced one of these micropatches for this problem.  On the other hand we're now - it's September 4th, the first Tuesday of the month, Patch Tuesday being next week.  So within a week this is almost certainly going to get fixed by Microsoft.



And again, it doesn't expose you to badness from the outside.  It's not network exploitable.  It's purely a local API call requiring code running on your system.  Again, it's not nothing.  I mean, code running on your system which is malicious would love to have this.  So it's not good that this SandboxEscaper person chose not to responsibly disclose this to Microsoft.  But that's the way it is.



So there is a micropatch.  It's just 0patch.com, if anyone is concerned and interested.  I don't like the idea of applying third-party things to Windows.  It turns out it was a very simple thing to fix.  They moved a couple function calls around.  Three instructions needed to be changed in order to fix this.  So as patches go, it's probably about as benign as possible.  But you're better off keeping bad stuff out of your system to start with.  So anyway, that sort of wraps up - we'll wrap it up for sure next week when we verify that this has been fixed in next Tuesday's Patch Tuesday.  But again, that sort of provides some more details to what we were talking about last week.



JASON:  And how often does that happen where someone finds a vulnerability like this and doesn't go through the responsible disclosure of it and just drops it like a bomb onto the Internet in this way? 



STEVE:  It's true.  We're seeing this sort of responsible disclosure now has become the regular routine.  Microsoft acknowledges the people who found it.  All of the security papers that we talk about are being released only after the problems they describe have been fixed.  So it's certainly more dramatic if you disclose some horror that still exists.  But, I mean, you really then become an outcast within the security community.  No responsible researcher will do that because you're just opening the floodgates for instantaneous exploitation.



I mean, even a perfect example is this Apache Struts problem.  Here, it was responsibly disclosed.  No one knew about it until the patch was made available.  What we now know is that even things that are patched, often, I mean, even things for which patches are available still end up with systems for a long time remaining unpatched.  So this is really the only way to go.  It's only fringe researchers who just don't really care about their own reputation who do this.  And that's sort of what she said in her tweet was "I don't care about life anymore."  It's like, uh, okay.  Sorry about that.  But, you know.  Anyway.  And speaking of things that will never be patched...



JASON:  You're talking about Android, aren't you.



STEVE:  I am.  I'm sorry, Jason.  I know it's your beloved platform.



JASON:  You know what, for years I've been involved with Android.  I've just gotten used to it at this point.  It's just the way it is.  Another vulnerability.



STEVE:  Just wear your armor.  So all versions of Android running on all devices are believed to be affected because what we're going to describe here is a core Android function which Google has fixed in the most recent Android Pie release, but has said they have no plans to fix in older versions.  And they're saying you're encouraged to upgrade to Android Pie or later.  Well, I don't have to tell you, Jason, the likelihood of that happening.  I mean, like, in many places you can't.  I mean, it's just not an option.  You're stuck with the Android that you have.  And the Android that you have, if it predates Pie, is subject to a CVE - this is the Common Vulnerabilities and Exploits database - 2018-9489, which has been assigned to this problem.  And all Android devices prior to Pie, until they're updated, or maybe there'll be a patch, we'll see.



So what's going on?  The OS broadcasts on purpose system messages which are global and which any application can indicate - they're called "intents."  And any application can raise its hand and say, I'd like to receive those, please.  And it then receives them without any user permission oversight.  So these are not things like permission to use the camera or other major aspects of the system.  These are sort of regarded as internal stuff that's going on.



Unfortunately, the messages include the WiFi network name to which the device is currently associated, the BSSID, the local IP address, the DNS server information, and the device's own static MAC address.  Some of this information such as MAC addresses has long since been recognized as sensitive, and it has no longer been made available through standard Android APIs since Android 6.  But now we're at 9, and even before 6, 7, and 8, that MAC address is available via the broadcast.  So this argues that Google masking the MAC address wasn't fully done.  The API to request it, it was removed from.  But you can still raise your hand and say broadcast it to me, and then you receive it.  So by listening to these broadcasts which are continuous, any application on the device can capture this information which in turn has the effect of bypassing all permission checks and existing mitigations.



So this leakage does undermine Android's system of permissions.  Of course we know that MAC addresses do not change and are tied to the hardware.  So this can be used to uniquely identify and track any Android device, even when MAC address randomization is used because, as we've discussed here, MAC address randomization is the pre-access point association MAC.  It was realized some time ago, and Android and iOS both fixed this, and I guess Windows did, too, that wandering around just anywhere, WiFi is so present that it's just you're bathed in WiFi now.  So it turns out that any access point could see the MAC addresses of all, does see the MAC addresses of every WiFi-enabled thing within its reach, even those that don't associate with it, that never have and don't.  So what we realized was, okay, that's not so good.



So MAC address randomization causes the client to just generate an arbitrary random MAC address during the time that it is not associated.  But when it associates, only then do you get the true MAC address.  So that's sort of a nice privacy tradeoff that works, and which everybody is doing.  The problem is that any app running in Android prior to 9 is able to determine the physical unrandomized MAC address, the network name, and the BSSID.  And of course there are databases.  There's WiGLE, W-I-G-L-E, and Skyhook, both which are comprehensive databases of exactly that information, which would allow software to geolocate the user using that information.  Even when that software has not been granted any location permission, when location is completely shut off on the device, presumably because the user wants privacy, this is still disclosing it.  So unless some sort of mitigation is made available, this is the way it's going to be on all Android prior to 9.  There's a cool website, I thought I had it here, the website.



JASON:  Are you talking about the app or...



STEVE:  Yeah, the Android broadcast monitor logger.



JASON:  Yeah, broadcast logger or - let's see here.  Internal Broadcasts Monitor?



STEVE:  Yes.  It was, yes.  Internal Broadcasts Monitor is the application, developed by a developer who also put the source on GitHub.  Internal Broadcasts Monitor is available through Google Play.  And so you can install it from the Play Store.  You tap Start, and you start observing all of the traffic which is available to it, not having obtained any permissions whatsoever.  You look for android.net.wifi.STATE_CHANGE and android.net.wifi.p2p.THIS_DEVICE_CHANGED messages, and the data they carry, and you will see all this.



So not a huge problem.  But if a user is interested in not being located, the only way to do that before 9 and until this is maybe fixed by a third-party patch, or maybe Google will reconsider their position - but on the other hand there's all these phones that are not Google and that are not being patched - would be to turn off WiFi because basically the software is still going to know your MAC address.  Well, okay.  You installed it on your phone, so that's where it is.



But these days WiFi is essentially leaking your position by virtue of the fact that we have databases, just comprehensive databases of network names and BSSIDs.  And software is able through these messages to know what you're connected to and what device you are and so produce long-term tracking.  And of course it's able to send the information out wherever it wants to.  So we may be talking more about this in the future.



JASON:  It's kind of disappointing when they stop at the current version with nothing prior.  This Pie is on so few devices that it doesn't even make Android's developer platform or the dashboard for showing how many percentages all of the reporting devices of what OS - like Pie's not even on there yet, which basically means it's less than .1% distribution at this point in time, at least at the time that this report was pulled.  So it's not even appearing on their report that tracks this stuff, let alone those are the only devices that actually get any of this protection.



STEVE:  So it's not inaccurate to say kind of at this point virtually all Android devices are currently enabling this broadcasting.  And any apps which will, I imagine, will start appearing are able to - well, because it's meant to be system global broadcasts.  So the fact that an app is saying I want to watch these doesn't, I mean, it's not going to make the app stand out.  It's not something where Google can say, oh, you shouldn't be asking for this.  No.  It's a broadcast.



JASON:  Yeah.  And Google obviously knows this is not a good thing.  They've patched it.



STEVE:  That's why they fixed it.



JASON:  That's a bummer.



STEVE:  So, okay.  Instagram was victim to some high-profile attacks.  And this, I think, is really interesting because they have formally switched from SMS-based text messages to confirm your identity as an additional factor to, yes, third-party authenticator apps, meaning that they've switched to the time-based one-time passwords.  So yay.  What's interesting is that these high-profile attacks on Instagram users occurred even though they were protected by second-factor SMS-based multifactor authentication, which strongly suggests that the weakness of texting six-digit codes is not just theoretical, it's now actual.  It's now proven.  So anyway, it's good that they're doing that.



They have also moved to combat what they've termed "influence campaigns."  They're showing, first of all, they are making available, the same way Twitter does, verified accounts.  You need to prove your identity to them, your real-world physical identity, by for example sending them a picture of a government-issued ID proving who you are in order to become a verified Instagram account.  And they've added the ability to view account information, which shows the date the account was created, the country the account is centered in, all ads that the account is currently running, any former usernames that the account has gone under, as well as any other public accounts which share with it a large common set of followers.  So bravo to Instagram.  It's a little disheartening that it's taken them this long to implement these measures.  But it really feels like, yes, we are getting there.  And it's better than not getting there.



And I think what we're doing is we're, as a consequence of really pretty much what's gone on in 2018, and probably I guess in reaction ultimately to all of the press that the involvement of foreign parties or alleged involvement of foreign parties in the 2016 presidential election two years ago had, that we're now sort of setting a new standard for the way this can be done.  And it really feels also as though SMS text messages, I mean, they were used because they were the lowest common denominator.  Everybody who wanted to get authenticated who had a smartphone was able to receive a text message.  You didn't need to install an app to do that.  And so everyone's nontechnical relative was able to say, oh, I just got a six-digit code, I need to put that in.  It's like, okay.  That's easily done.  Unfortunately, it wasn't secure enough.



So we've moved as an industry, I think, past that now.  And I imagine it won't be long before authenticator apps won't be third-party authenticator apps.  They'll just be part of the underlying OS.  It's like, clearly it's time for that to happen.



JASON:  That would be really nice, actually.  I hadn't considered that as a solution.  But as you were kind of talking about that, it's apparent to me that SMS is infinitely easier for the broad spectrum of people to do because their phone already does SMS versus having the app installed.  That's one extra hurdle.  People might not understand why they need to install this app to do it when it already works on SMS.  Build it into the OS, that's a really great solution.  That makes a lot of sense.



STEVE:  Yeah.  And in fact I saw something, and I didn't take the time to track it down, but it looked like there was some provision for automatically registering the secret that drives the one-time password token within an interapp API.  So if it were built in, and if there was a secure means for doing that, it might be possible for you to say I want two-factor authentication, and for the site to right then auto register your secret with the built-in authentication app.  And so you're not even having to manually move that from one place to another.  Which again, I mean, then that really makes it much more easy to use.



JASON:  Well, and you even see that right now on SMS; right?  At least I've noticed it before where SMS was the only choice for authenticating.  That message comes through, and it automatically populates that into the app.  The app that I'm using, trying to get into, recognizes that came from - I guess it came from the number it expected to get it, plugs the number in, you don't have to do anything.  It's just like, great, we got it.  Move on.



STEVE:  Yup.  Unfortunately, the bad guys got it, too.



JASON:  Yeah, right.  Fair enough.  Well, Google also has their other authentication where, when I log into their account, I will get just a popup on my phone that says is that you logging in, yes or no, so instead of doing a code or whatever.  Where does that fall on kind of the security spectrum, as far as that's concerned?  Better than using SMS, I assume.



STEVE:  It is, but it's proprietary.  And so the advantage of the generic one-time password is that you get to choose which app you want.  You're able to store them yourselves.  There's a tool on iOS called OTP Auth, which I have started to use and like.  I think I was using LastPass's for a while.  And Leo and I talked about this.  OTP Auth does use iOS Cloud in order to synchronize, which provides additional convenience.  So that as long as you set it up that way, if you add another account to it on one device, all your other iOS devices automatically know about it.  So it ends up further improving the experience.



JASON:  All right.  So we talked about Instagram.  I'm happy about that.  I'm buttoning up my Instagram account, hopefully once that update comes through.  It hasn't come through yet.  Tell us a little bit about this OpenSSH disclosure vulnerability.



STEVE:  So we talked about a different one last week.  In looking at some code associated with a recent security-related fix, a developer said, hey, wait a minute, there's another problem here.  The problem was that there was a way that it was possible for someone to probe an SSH server to determine whether the username was valid or not.  So separate from the password, it was regarded as an information disclosure vulnerability.  And generally you want to only validate the username and the password together and say you got it or you didn't, rather than allowing someone to separately probe the username.  So that happened, and we discussed it in detail last week.



Since then, the guys at Qualys found another somewhat related problem.  But as a consequence of the way the OpenSSH developers responded to the first one, the Qualys guys were a bit put off and not really sure what they should do about it.  So I want to share what Damien Miller, who is the OpenSSH dev, said with regard to the first problem, which we talked about last week.



He wrote:  "Hi.  Regarding CVE-2018-15473, a few people have asked why we just committed a fix for this without any secrecy or treating it as a security problem. The reason is that I and the other OpenSSH developers don't consider this class of bug a significant vulnerability.  It's a partial disclosure of non-sensitive information."



He says:  "We have and will continue to fix bugs like this when we are made aware of them and when the costs of doing so aren't too high.  But we aren't going to get excited about them enough to apply for CVEs or do security releases to fix them.  The following explains our reasoning."



He says:  "First, this isn't 'user enumeration' [he has in quotes] because it doesn't yield the ability to enumerate or list accounts."  He says:  "It's an oracle" - that's the cryptographic technology term - "allowing an attacker to make brute-force guesses of account names and verify whether they exist on the target system.  Each guess is moderately expensive, requiring one TCP connection and a cryptographic key exchange, limited in concurrency by SSHD's MaxStartups limit."



He says:  "Second, very little else in the Unix ecosystem tries to prevent this style of information disclosure.  Many network daemons will still happily return 'user not found' style messages; but, more importantly, system libraries are simply not designed to consider this a threat.  They don't consider it a threat because usernames have long been considered the non-secret part of user identity, of limited use without actual authentication credentials."



He says:  "In the absence of the underlying system stack being designed with this in mind, the best applications like SSHD can do is try to paper over the most obvious differences by avoiding behavior divergences in our own code and adding some prophylactic timing delays.  But it's a losing battle."



And he goes on with some other details that I'll skip over because they don't really matter.  Anyway, that gives you a sense for the philosophical position that he has.  But he does say this.  He says:  "Finally" - and I think this is very salient.  He says:  "Finally, and perhaps most importantly, there's a fundamental tradeoff between attack surface and this class of bug.  As a concrete example, fixing this one added about 150 lines of code to our pre-authentication attack surface."  So that is to say this is the code that anyone comes into contact to before they authenticate, meaning it is critical that they not introduce new vulnerabilities; that is, that there's nothing that they do wrong there.



So he says:  "This one added about 150 lines of code to our preauthentication attack surface.  In this case, we were willing to do this because we had confidence in the additional parsing, mostly because it's been reviewed several times, and we've conducted a decent amount of fuzzing on it.  But given the choice between leaving a known account validity oracle or exposing something we don't trust, we'll choose the former every time."  In other words, if it meant that fixing it might introduce a new vulnerability, they're just going to leave the oracle where it is, meaning that there's something probeable, technically.



So having read that, the guys at Qualys Security, in looking over that region of code, then spotted yet another somewhat similar problem.  So there's a second similar oracle vulnerability.  But due to the grumpy reaction of the previous similar issue, Qualys was somewhat put off.  They wrote:  "We understand that the OpenSSH developers do not want to treat such a username enumeration [and they have in parens] (or 'oracle') as a vulnerability."  And of course, as we know, the OpenSSH developers even argue against the term "username enumeration." They say:  "Although," Qualys says, "it is still quite useful in an attacker's toolbox."  So they're asking:  "But how should we coordinate this disclosure, then?  OpenSSH developers, distros, please advise."



So anyway, our listeners know that I am constantly saying to Microsoft, would you please just leave it alone?  Leave Windows be.  Give it some hope of stabilizing someday.  Don't keep messing with it.  And this is exactly what the OpenSSH developers understand and are saying.  I mean, OpenSSH is all about security.  That's its sole benefit.  You don't use it to play videogames.  You use it for Secure Shell connections.  So thank goodness they're being this security conscious.  Last week we acknowledged that it wasn't such a big problem.  They have certainly stated that clearly and for the record.  And it's hard to argue with that.  It's a little daunting that it was 150 lines of code required in order to fix this problem.  I guess I'm glad they fixed it.  It's not clear whether - I guess what they're saying is they will fix it if they can do so with confidence.  But even so, they're not going to be issuing CVEs and considering this a big security problem, which I can certainly concur with.  Interesting.



JASON:  Yeah.  So GDPR.  I know that this is next up, and I've got lots of questions about this.  I've got a few, anyway.  This is interesting to me because I just assumed that the GDPR happening in the EU, all those rules were going to make sweeping changes everywhere.  But apparently that's not happening.



STEVE:  Yeah.  And so we'll explain what you're talking about and then talk about it because it is sort of an unexpected consequence.  Nearly, get this, 1,200 U.S.-based news sites are deliberately remaining inaccessible, that is, they are blocking visitors from the EU as a consequence of the EU's adoption of the high-fine GDPR regulations, which has just freaked everyone out.  And these deliberately blocked sites are not all obscure since they include, get this, the Los Angeles Times.  Yup.  Cannot bring up the L.A. Times from Europe.  The Chicago Tribune.  The New York Daily News.  Dallas News.  The Baltimore Sun.  The Sun Chronicle.  The St. Louis Post Dispatch.  And Newsday.  None of them are currently available to people in the EU.  And as I mentioned, nearly 1,200 U.S.-based news sites, that leaves well more than a thousand smaller regional news sites which provide the bulk of news reporting overall and certainly are serving their communities.



So as we know, the European Union's GDPR regulations require websites to disclose their data collection practices in much more depth and detail than ever before, and also requires websites to obtain an explicit permission to collect this data from its visitors.  The regulation also forces websites to provide a portal where users can see what data the website has collected about them and provide a way for users to delete this data.  Now, that's easy to say and easy to request.  It's hard to do.  I mean, it requires every single website on the Internet which can be visited by someone from the European Union to comply.



And so what have all of these nearly 1,200 sites done?  They've said we don't need visitors from the EU.  We'd rather block them than switch or invest in, right now, maybe forever, in abiding by these regulations imposed by another country on us.  And except for these big sites, you could argue, like some Des Moines Dispatch or something, that no one in the EU probably wants to go there anyway.  So blocking the ranges of IPs that are outside the U.S. or inside the EU, it's like, well, okay, you could argue they've saved themselves the exposure of being in breach of the GDPR, and it's not costing anybody any problem for them having to do that.



Remember that companies who do not adhere to the GDPR risk facing massive fines of as much as 4% of their annual revenue, which for major ongoing operations is significant.  And again, I think it's rational for them to just say, you know, we didn't do this on purpose.  Some other country has just decided that we're liable for behavior that nobody else in the world has a problem with.  So, fine, we're just going to block you.



There is an interesting script and site monitoring this.  Joseph O'Connor grabbed the domain verifiedjoseph.com, so it's data.verifiedjoseph.com which maintains a list on the fly of all the websites not available as a consequence of the EU GDPR.  The script ran this morning because it was fresh.  It shows up at the top the last time it was run.  And as of this morning, when I put the notes together, there were 1,149 unavailable websites that he is monitoring and 147 that are.  So more than a thousand.  Actually, 1,002 more unavailable than are available.  And some big names among them.  So an interesting consequence, a side effect of one nation saying, one group of countries saying we're going to fine you unless you do this because our visitors might go to your website.  It's like, oh, okay, fine.  Block them.



JASON:  Well, they're missing out on one way that they could make extra money because the Washington Post, apparently, back in May, their response was to put up an additional paywall targeted at the EU, a premium EU paywall that would remove ads...



STEVE:  Interesting.



JASON:  ...and make extra off of that.  So it's like, yeah, sure you can get it in the EU.  We'll remove all the ads for you.  You've just got to pay us more than a basic subscription fee in order to do it.



STEVE:  Actually, that makes a lot of sense.



JASON:  They can make more money.



STEVE:  Yeah.  There was, as a consequence of the legislation, this is a related matter, I think it was a 22% drop in cookies on websites just because of the GDPR.  There were too many violations; and sites said, okay, we're not going to host third parties that are in violation of the GDPR because we can no longer afford to do that.  So as an indirect consequence - because as we know third-party cookies are coming from ads and other unrelated sources typically.  Those are down by about 22%.  So that's been nice.



Speaking of cookies, four years ago, in 2014, Check Point's Malware and Vulnerability Research Group made a discovery.  When a client connects to a web server, they ask for some resource from the server.  That's what the URL is.  And images and the text and all the stuff that the site has are requested.  With those requests go any cookies that the browser has previously received from that domain as a consequence of previous queries.  So as we know, cookies are the way, they're a mechanism that Netscape added in the early days of the web to allow this notion of logging into a website.



The actual act of obtaining a web page is stateless.  The browser says give me a bunch of stuff, and it gets it.  And then, if you click on a link within that site to another page, the browser asks for that page, and all of that page's stuff.  But nothing links you and those two pages together.  There is no tracking in the original web.  That was created by Netscape where, in response to the first request, which would not have had a cookie from your browser because your browser has never been there before, the web server goes, oh, let's give him a cookie.



And so a nonce, a one-time - a nonce is a number once.  Just a random, pseudorandom blob of noise is handed back to the browser in the first reply.  And now the browser has a cookie, and so it sends it back on all subsequent requests, not only for the duration of the other things on that page, but when you click a link, it sends it back with the link you're clicking.  And as the browser then obtains the next page and all of its assets, that cookie keeps going back.  So that creates a stateful connection to the site.



Okay.  So it turns out, not surprisingly - so remember, in a query the browser is returning the cookie it received.  Turns out that what Check Point discovered was in a huge number of exposed residential routers, so-called SOHO, Small Office Home Office routers, the web server that was publicly exposed was not correctly checking the length of the cookie it received.  It assumed that the browser was sending back the cookie it had been given.  But you could have malicious cookies, or in this case Misfortune Cookies.  Thus this name.



So back four years ago, researchers from Check Point's Malware and Vulnerability Research Group uncovered this critical vulnerability which was at the time present on millions of residential gateway devices across models and makers.  It got assigned at the time a CVE number, 2014-9222.  It was a severe vulnerability, allowing an attacker to remotely take over the device, obtaining administrative privileges.  At the time, they detected approximately 12 million readily exploitable unique devices on the Internet across 189 countries, making it one of the most widespread vulnerabilities that had been seen at the time.  And research suggested that the true number of affected devices that were not detectable might have even been greater.



So anyway, the point is that anyone making a query to the website could, along with the query, essentially inject a malicious cookie into the query which would induce a buffer overflow and allow that cookie to contain code which the web server would then execute, making this an incredible potent remote code execution vulnerability.



So now move forward four years.  It's 2018.  The Industrial Control Systems Cyber Emergency Response Team - there's a mouthful, that's the ICS-CERT - has identified this same vulnerability, currently widespread in medical device systems.  There's a very commonly used piece of equipment or technology known as the DataCaptor Terminal Server (DTS), which is a medical device gateway developed by Qualcomm Life subsidiary Capsule Technologies SAS, which is widely present in medical management systems.  The gateway is used in hospitals to connect medical devices into the larger network infrastructure.



The cybersecurity firm CyberMDX discovered the presence of the flaw, which can be exploited by attackers to conduct remote arbitrary memory writes, just like four years ago, which could lead to unauthorized login and code execution.  The vulnerability in the device is present in a software component called the RomPager from Allegrosoft, which is used by the DTS web interface.  And according to CyberMDX, the version of RomPager in use is an older version, earlier than 4.07, where this problem was fixed, and the older version is susceptible to the Misfortune Cookie attack.  More up-to-date versions are not affected because they were patched.



Turns out that even more worrisome is the fact, given where these things are being deployed, that the vulnerable version has remained in use through the last four years only because the vendor would need to pay to receive updated firmware and has elected not to do so.  The only silver lining in this cloud is that the web server component, which is where the vulnerability is, is only utilized and required during the initial configuration of the device.  So the embedded vulnerable web server can be disabled once the device is set up and configured.  On the other hand, we all know the likelihood of that happening in the real world.  It's almost not going to.  Maybe a few responsible admins who become aware of this, who are listening to this podcast, will say, oh, that affects me, and they'll turn this off.  That would be great.



We've often talked about how devices like this which qualify as Internet of Things devices have to take responsibility for keeping themselves up to date.  These things have to periodically check with the mothership to see if there's an update for them and fix themselves autonomously.  We've moved away from SMS text messages for second-factor authentication to time-based.  We have to adopt similarly autonomous updating of IoT stuff.  Our routers have to do it.  Our light bulbs have to do it.  If it's on the Internet, if it's accessible, it's got to update itself.  Again, we're in the early days yet.  And we know that because we see these things changing as we slowly and reluctantly, kicking and screaming, mature.  But, boy, it's not happening quickly.



JASON:  Yeah.  I'm filled with a little bit of doubt, but I'm at least enlightened by the fact that you say that it's early days and that it worked for SMS, it could work for this, too.  But, man, it's an uphill battle, I think, with IoT.



STEVE:  It is a big ask.  And you are right, Jason, to be filled with plenty of doubt.  So Magneto is an Adobe company that boasts it's powering twice as many top retailers as any other provider.  What they are is an eCommerce provider.  And, for example, Coca-Cola and Burger King and many other users leverage the eCommerce platform which Magneto offers.



JASON:  Is it Magneto or Magento?



STEVE:  Ah, you're right.  Well, M-A-G-N-E-T-O.  I don't know how you pronounce it.



JASON:  No, if you click, yeah, I think if you click through it's Magento.



STEVE:  Oh, you're right.  It's M-A-G-E-N-T-O.  Yeah, yeah, yeah.



JASON:  I mean, I think it's a Marvel Comics character, and he's pretty awesome, but Adobe is even awesomer.  I don't know.  No, I don't think that works.  Yeah, I think it's Magento.



STEVE:  That's interesting because throughout the notes I have Magneto Core.  Magento.  Anyway, Magento, sorry.



JASON:  It's all good.



STEVE:  Magento, looks like Magento.  Good, I'm glad you caught that, Jason.



JASON:  Yeah, no worries.  



STEVE:  Over the past six months Dutch security researcher, whom we've often referred to on the podcast, Willem de Groot has found a malicious credit card skimming script which, now, I've got in here Magneto Core, but it's magentocore.net.  So sure enough, I'm checking the actual URL here.  So, yes, which he named Magento Core.  And get this.  He has found this on 7,339 Magento-hosted storefronts.  So that's just the places where end users go and say, oh, I want to buy something, and put in all of their name, their address, their whatever information it asks for, email, phone number, and credit card information.  On 7,339 storefronts there has been, and in - get this - 5,172 domains right now today there still is, skimmer script running which grabs all of that information and sends it back to bad guys, apparently in Russia.



He explains on his posting of this that victims of Magento Core who use the storefronts have their credit card and identities stolen.  He writes that the group that is behind this has not slowed down; that new brands, that is, new storefronts, are hijacked at a pace of 50 to 60 stores per day over the last two weeks because he's got now a scanner running, looking for this illegal scripting.  And apparently you can just do a worldwide web search for the appropriate pieces of the script, and Google will find it for you.



The Magento Core skimmers gain illicit access to the control panel of an eCommerce site, often through brute-force techniques, automatically trying lots of passwords, sometimes for months.  So nothing is preventing them from doing long-term brute-force cracking of the login to the eCommerce site control panel, which suggests that, unless not in use, that control panel should be taken offline.  Unless you actively need to get to it from the web, turn off the control panel.  That's clearly the crux of the vulnerability here.



Once they succeed in gaining access, they embed one line of JavaScript into the HTML template.  And it's just a standard JavaScript invocation to https://magentocore.net/mage/mage.js.  That script gets added to the pages and sucks off everything that the user does.  It records keystrokes from unsuspecting customers and sends everything, he writes, in real-time to the magentocore.net server registered in Moscow.  Also the malware includes a recovery mechanism.  In the case of the Magento software, it adds a backdoor to the cron.php file which will periodically run to download malicious code and make sure that it comes back after it gets removed.  And there's even a clean.json backup, which is PHP code, which removes any competing malware from the site.



So, wow.  Again, this feels to me like something that Magento needs to address by doing something like removing long-term global access to the control panel by default.  So like requiring internal access only.  Or if you make it available on the public interface, then have it automatically shut down after some minutes of non-use.  Not just log you out, but become unavailable so that some other action is required in order to bring it back up publicly.  This is a big problem that apparently, you know, this has been going on for six months.  Certainly they're aware of it, and apparently nobody's doing anything about it.  Seems wrong.



Okay.  So our Trusted Platform Module, the TPM, is present now - I remember, what, 10 years ago, because it's been around for a long time, it wasn't clear whether you were going to have it or not.  My previous motherboard, which was about 15 years old, had a socket where - it was a, can't remember now, a gigabyte board, I think.  It had a socket where you could plug in an optional TPM module, but it wasn't just built into the motherboard.  They are now.  You can verify that just by going to your BIOS.  Go the security tab in the BIOS, and you will see you can turn it on or off.  You can initialize it and so forth.  So pretty much everybody's got TPM onboard.



There's a problem.  And it's not with some random manufacturer's backdoor or bad implementation.  It's actually in the spec.  Researchers with the South Korean National Security Research Institute identified a flaw in the Trusted Platform Module spec relating to the way power modes are handled.  They presented their findings, again, at that recent USENIX conference.  ACPI is the longstanding Advanced Configuration and Power Interface spec and API which defines the power states for a system and the hardware registers for supporting power management.



There are global states known as G0 through 3, which are respectively working, sleeping, soft-off, and mechanical-off.  And then there are local per-device states which are the ones we're more familiar with.  There's S0 and S1, which are working and power on suspended.  There's S2, which is the same as S1, that is, power on suspended, but the CPU is also powered off.  There's S3 which is the well-known sleep, where all devices are powered down except RAM.  It keeps RAM alive.  And then S4 is the formal name for hibernation, where the RAM is written out to static storage, and then it, too, is powered off.  So sometimes you see those in a BIOS.  And essentially what ACPI does is it provides per-device power management.  So in order on a system that's got all kinds of stuff all over it, it's possible to power down selectively different pieces of the entire system in order to get overall improved power management.



Well, it turns out there's a glitch in the way the TPM interfaces with ACPI such that it's possible for the TPM's boot-time hash validation values to be intercepted and replaced.  So the TPM is used when we boot our systems to implement so-called "secure boot."  The idea with secure boot is you start with something you absolutely trust.  You have to have a trust anchor, and that's the TPM.  It's in hardware.  You cannot read stuff out of it.  You can only ask it to do work for you.



It is like a Secure Enclave on our PC-based systems.  It verifies the signature of the first thing that is before it's run.  Basically it oversees the entire boot-up sequence where, stage-by-stage, the signature is verified of the software by taking its hash and checking it against the valid hashes that the TPM has, the idea being that, if you start from absolute secure, and you load and check each module in turn, there's no way for bad stuff to get in until you finally load the OS and make sure it hasn't been tampered with and then turn over control to it.



Well, turns out by being tricky with power states these researchers have found a way to subvert that secure chain boot-up process, and they demonstrate it.  They have a proof of concept, and it works.  So ultimately what we're going to look at is another round of firmware updates.  In this case it's not the firmware for the processor, it's the firmware for the TPM.  It is a standalone microcontroller device that has firmware that runs instructions, and they need to be updated to fix this.  If you're in a high-threat environment, there is a short-term mitigation which anyone can employ if your BIOS allows you to disable the system's S3 sleeping state.  And many BIOSes do that.



Historically there have been systems that did not awaken from S3, from sleep, correctly.  And so an option in the BIOSes for years has been to disable S3.  The hack these guys have come up with absolutely requires that S3 be enabled.  And many people don't use S3.  They just don't have an occasion to sleep their systems.  Certainly enterprise servers, probably lots of enterprise desktops don't.  Mostly laptops.  But remember that when you sleep your laptop, there's still a power drain because RAM is being kept alive.



So if you hibernate your laptop when you close the lid, or it does a full shutdown, even a laptop may not be needing the S3 state.  So you can probably disable it without any inconvenience to you, and in doing so you are protecting yourself until Intel, probably, gets around - or I guess Infineon is the actual maker of most of these TPM chips.  So maybe it'll be from Infineon, then maybe Microsoft will push this, or Intel will.  We'll see how we get this.  But essentially the entire industry has been put on notice that it is not safe to trust TPM at the moment.  It can be subverted.



JASON:  What else have we got here?



STEVE:  Well, so I've mentioned throughout the podcast already many CVE numbers, which is the database that we've really become quite dependent upon, and not just in the U.S., but globally.  In the show notes I have a chart courtesy of Bleeping Computer, who covered this story, showing the black bars as the funding from 2012 through 2016 versus the white striped bars are the number of CVEs published.  And so for our listeners who are only getting audio, the funding was in 2012 and moving forward each year:  6.7 million, then dropped to 4.8, then dropped to 2.8, then dropped to 1.7 in 2015, to rise to 4.0 in 2016.  But still, in 2012 it was at 6.7.  Across that same period of time, the number of CVEs published has about doubled.  In 2012 it was 7,370; and in 2016, 14,472 reported vulnerabilities.



So I'll share what Bleeping Computer had to say since they summarized this nicely.  They said, and I've got the link to the Bleeping Computer story for anyone who wants it:  The CVE, they said, "was created in 1999 by the MITRE Corporation using U.S. government funding.  It's a database that contains identifiers - tracking numbers - for security vulnerabilities.  Since its creation, the CVE system has been adopted by the public and private sectors.  Most modern cybersecurity software use CVE numbers to identify and track cyberattacks exploiting particular software bugs.  Despite being a U.S. creation, the system has been widely adopted in countries all over the globe, which use and recognize the CVE identifiers issued by MITRE's staff and industry partners.



"But in recent years," writes Bleeping Computer, "the CVE system has been under stress.  Its problems became evident in late 2015 and early 2016 when a large number of security researchers reported long delays in receiving CVE numbers for the vulnerabilities they were reporting.  At one point, a few of them united to create an alternative vulnerabilities database known as the Distributed Weakness Filing (DWF).



"At the time, MITRE said the CVE number assignment delays were caused by the increased number of software vendors compared to the late '90s and early 2000s, but also because of the proliferation of software-driven SCADA industrial control equipment and Internet of Things devices."  In other words, more companies creating equipment, more security researchers reporting, and more things going wrong to report.



"Both factors," writes Bleeping Computer, "contributed to a huge rise in vulnerability reports, with which the CVE staff wasn't managing to keep up to date."  And of course at the same time their budget was being cut every year.  "A late 2016 report found that MITRE's CVE failed to assign CVE numbers to over 6,000 vulnerabilities discovered in 2015."  So there were 14,000 numbers assigned; 6,000 weren't.  So actually more than 20,000 vulnerabilities.



"So in March of last year the U.S. Senate got involved to investigate the problems and concluded:  'From 2012 to 2015, the program has received on average 37% less year-over-year funding.'"  Also, they said:  "'The documentation produced by DHS and MITRE shows that the CVE contract vehicle is both unstable and prone to acute fluctuations in scheduling and funding.'



"To solve this issue, the Senate's Committee proposed that DHS officials move CVE's funding from a contract-based funding scheme into the Department of Homeland Security itself, as what's known as a PPA (Program, Project, or Activity) funding line item.  The Committee believes this would provide a constant stream of funding, reducing huge budget fluctuations, and keep MITRE focused on running the CVE database instead of always worrying about its future funds."



So that all sounds good to me.  I hope that that happens and that they're given enough money because it's very clear that this is providing a vital clearinghouse database for managing all of these cybersecurity threats, which, as we know, can be significant.



Mozilla has announced a very welcome change of anti-tracking approach for future Firefoxes, which is available in the Firefox Nightly builds for anyone who wants to enable it.  In their posting they said:  "Anyone who isn't an expert on the Internet would be hard-pressed to explain how tracking on the Internet actually works."  And of course we've been left breathless on this podcast doing just that.



"Some of the negative effects," they write, "of unchecked tracking are easy to notice, namely eerily specific targeted advertising and a loss of performance on the web.  However, many of the harms of unchecked data collection are completely opaque to users and experts alike, only to be revealed piecemeal by major data breaches when they occur.  In the near future Firefox will, by default" - and that's the key, you don't have to go anywhere and flip anything on - "by default, protect users by blocking tracking while also offering a clear set of controls to give our users more choice over what information they share with sites.



"Over the next few months," they write, "we plan to release a series of features that will put this new approach into practice through three key initiatives."  Get this.  Under improved page load performance they wrote:  "Tracking slows down the web.  In a study by Ghostery, 55.4%" - okay, 55.4, more than half - "of the total load time required to load an average website was spent loading third-party trackers."  Right?  Third-party trackers are more than doubling the amount of time required to load our pages.



They say:  "For users on slower networks" - I would just say for anybody - "the effects can be even worse.  Long page loads are detrimental to every user's experience on the web.  For that reason, we've added a new feature in Firefox Nightly that blocks trackers that slow down page loads.  We will be testing this feature using a shield study in September.  If we find that our approach performs well, we will start blocking slow-loading trackers by default in Firefox 63."



So that's very cool.  What that means is they will instrument Firefox - five seconds is what I saw referenced elsewhere.  So if a tracker takes more than five seconds to get itself loaded, it will be blacklisted, and it will no longer be loaded by Firefox.  To which I say bravo.  And it's interesting, too, because think about it.  Trackers don't have any particular incentive to be speedy.  Websites do.  But websites don't have control over the trackers that their ads load or that other third-party assets load.  So websites are being hurt indirectly by the trackers that the assets they're invoking are loading.  So this really is great.  This says to trackers, get your servers sped up so that you are able to respond to the requests you are creating on web pages; otherwise, be blocked.  So I think this is great.



JASON:  Fantastic news.  And you have to imagine that Firefox, the work on Firefox Focus, which came out I think in 2015...



STEVE:  Ah, right.



JASON:  Initially for iOS.  Now it's on Android.  And the focus, no pun intended, when they first launched was to basically block the tracking.  And then it's kind of expanded into other privacy kind of emphasized sort of uses.  But, yeah, that's great.  Any way that it can speed things up and clean up the experience, just remove the muck.



STEVE:  And Google is in, of course, as we know, a bit of a dicey problem because their revenue all comes essentially from web-based advertising which is enhanced by tracking.



JASON:  Sure.



STEVE:  So it'll be interesting to see what they do, if they follow suit.  Also, that was the first of three.  Second of the three is removing cross-site tracking.  Mozilla writes:  "In the physical world, users would not expect hundreds of vendors to follow them from store to store, spying on the products they look at or purchase.  Users have the same expectations of privacy on the web; and yet, in reality, they are tracked wherever they go.  Most web browsers fail to help users get the level of privacy they expect and deserve.



"In order to help give users the private web browsing experience they expect and deserve, Firefox will strip cookies and block storage access from third-party tracking content.  We've already made this available for our Firefox Nightly users to try, and will be running a shield study to test the experience with some of our beta testers in September.  We aim to bring this protection to all users in Firefox 65 and will continue to refine our approach to provide the strongest possible protection while preserving a smooth user experience."  So that's big.  That's not the DNT, you know, please don't track me flag that you can set.  This is the browser itself not honoring third-party, that is, cross-origin content.  And again I say yay.



And then the third of three, mitigating harmful practices.  They write:  "Deceptive practices that invisibly collect identifiable user information or degrade user experience are becoming more common.  For example, some trackers fingerprint users, a technique that allows them to invisibly identify users by their device properties, and which users are unable to control.  Other sites have deployed cryptomining scripts that silently mine cryptocurrencies on the user's device.  Practices like these make the web a more hostile place to be.  Future versions of Firefox will block these practices by default."



And they say:  "Note that these features are currently available in the Firefox Nightly builds.  Users wishing to experiment with them may manually enable them."  And in the link that I have at the top of the story to Mozilla's blog, there are the details spelled out for how to do the Firefox Nightly enablement of these if you're curious.  And again, Firefox is still my go-to browser, mostly because it handles side-located tabs so nicely, and I desperately need tabs, horizontal tabs running vertically down the side of my browser window.  So I'm sticking with it for now.  And I'm sure happy with these privacy-related features that they're supporting.



JASON:  Yeah, very nice.



STEVE:  I'll just note that Microsoft did release a non-Patch Tuesday substantial update which, fortunately, my Windows 10 machine that I'm talking to everybody and you over, Jason, was able to digest in time for the podcast.



JASON:  Right.



STEVE:  It wasn't security related.  It was a whole host of non-security-related problems.  I won't enumerate them because literally there are too many.  But reading the list as I did of the things that they fixed makes you glad that Windows 10 was working beforehand because, wow, I guess there's all kinds of little fringe edge conditions and things that they are working on.  And as I've said, if they would only just leave it alone, then they could fix these things and not be breaking more things at the same time.  But that's clearly falling on deaf ears because here we are at Windows 10, even though earlier Windows worked just fine.  Anyway, grumble, grumble.



I found a really nice note from a Rick Zich - he actually helped me pronounce his name, it's spelled Z-I-C-H but pronounced "Zeke" - in Tucson.  He said:  "Steve, I built this machine for my father's business, and it had been running good for a few years.  Over those years we replaced the motherboard and also the power supply and added a high-end graphics card as he now pushed the computer to a large 50-inch" - wow - "4K TV screen and wanted the best picture he could get.



"Recently he started to get some pink screens of death."  And Rick says, "Yes, a pink screen, not a blue screen.  Evidently, when you are running Nvidia 1070 or other high-end video cards, the blue will change colors to pink.  As strange as I found that, the point was that the machine was rebooting randomly.  I also tested that concept.  I took out the video card and did indeed continue to get BLUE screens with the card out.  I could not for the life of me figure it out.  Lots of help on the Internet for things like make sure to update the Intel video drive even though it's not active, run your Windows updates, or reset your RAM."  I guess that means pulling the DIMMs out and pushing them back in again, which I certainly know well.  That's kept a few of my machines going.



He said:  "I had individually tested all the pieces of hardware in a separate machine, and all would run fine with the other machine.  He was running a strange version of Windows 8.1, so I even upgraded him to 10 and even put it on a 500GB SSD, which was brand new.  However, I had cloned the disk over to the new one and then ran the Windows upgrade.  I finally thought, could I have cloned over corrupt data?  So I pulled out my SpinRite on my bootable USB stick and ran it on Level 2.  I had no expectations of it working, as I thought SpinRite would only fix damaged disks and not damaged data.  But what did I have to lose?  Well, it's been a week, and no reboots yet from pink screens.  Yeah!!!" he says with three exclamation points.



And I'll just note that it is true that SpinRite's focus is on recovering sectors which the drive says it cannot read.  But there is, as we've often talked about, a disturbingly large gray area.  And as defects get large, it is possible for the drive to miscorrect those sectors so that it believes it has performed an effective error correction, because it's actually statistics, when in fact it has not.  It's a little bit like a parity error which can correct even or odd parity.  And so it will detect a single bit flipping because that will change the parity of the entire block.  But two bits flipping it won't detect because that maintains the same parity.  ECC is the same way.  SpinRite won't be fooled.



So what happened was SpinRite realized there were sectors which were being miscorrected by the original hard drive.  It fixed them correctly and rewrote them so the drive then worked properly.  So it's always surprising people how much there is under the covers of SpinRite that is not revealed by the fact that you just run it, and it fixes everything.  But that's what it does.



JASON:  Does it ever surprise you at what it's capable?  Like at this point you've seen it all.



STEVE:  Oh, yes.  Oh, yeah, yeah.  And in fact I've talked about how my bookkeeper operations gal, I had set her up with a mirrored RAID, and the first drive died.  And she didn't want to bother me.  And I've since explained to her, "Sue, that's when you bother me.  That's why we have a RAID, to be redundant."  And in fact I've talked about this on the podcast before.  It annoyed me that the RAID continued to say, I'm damaged, but okay.  A consumer RAID should say, "Okay, one drive is down.  Fix me now while you can still clone the good drive to another  good drive."  But it didn't.  It just kept on going, for months, until the second one died.  And that's when I got the call from Sue saying,  "My computer won't boot."  And I said, really.  So I made a house call.  And I said, "Wow, both drives are dead."  And that's when she confessed that one had died a long time ago.  And I just said, "Oh, okay.  Next time, when the first one goes, believe me, you're not inconveniencing me, you're saving me time."



JASON:  Right.



STEVE:  So the point is the system was completely down.  I ran SpinRite on it.  I don't remember which of the two drives came back.  One of them did.  Maybe they both did, I don't remember.  And then I was able to - there was zero data loss, and we brought the system back up, and I think I probably put new drives on it because why not?  And she's off again and hasn't  had any problems since.  Although she is complaining that it's getting a little old.  So it's time to get her fixed up.  But anyway, SpinRite does surprise me, often.



JASON:  Awesome.



STEVE:  Okay.  SonarScoop.



JASON:  Snoop, Snoop.



STEVE:  Oh, Snoop, yes.  Why do I do that?  Yeah, you and I both did that.



JASON:  No, I totally did it before the show.  And I was, what is a sooner - I think it corrected to SoonerScoop, for the Sooners.  It was a sports site for the Oklahoma Sooners football team.  And I was like, this is not right.



STEVE:  So I'll tell you, this is the best time of year for the podcast because we've got the USENIX conference, and we've got Black Hat and DEF CON.  I mean, it is just security conference heaven.  So this is another piece of research that was shown at the three weeks ago now Baltimore USENIX security conference.  In the show notes I have a picture from their research PDF which they published as part of the conference proceedings.  It shows the back of a Samsung Galaxy S4 with the microphone at the top of the phone and the speaker on the back.  Oh, I'm sorry, the microphone at the bottom that you normally talk to and a speaker at the bottom.  And then on the other picture is a top-located microphone and a top-located speaker.  So the top one is where your ear normally is, and obviously the bottom mic is where your mouth normally is.  But to increase noise cancellation and the utility of the phone, they put both the microphone and a speaker also on the opposite ends.  So you've got a top microphone, a top speaker, a bottom microphone, and a bottom speaker.



Well, the next page of the show notes I show their simplified diagram of what that means.  That means you can ping from the top speaker an ultrasonic sound that the user will not hear, which will be received after some length of time based on how far away the person is and the rate at which sound travels, will be heard by the upper microphone.  Similarly, you can ping from the bottom speaker, and it will bounce off the underside of the user's finger and back into the bottom microphone.  In other words, classic sonar.



And, now, to triangulate you really would want the sensors to be mounted on adjacent corners of the area.  That way, if you take the two distances, it's called "triangulate" because the two sensors lie on one edge of a triangle, and then the distance that they're each sensing informs it of the length of the other two lengths of the triangle, allowing it to uniquely determine the apex of the triangle not containing the sensors, thus triangulation.  We don't have that here.  We've got kind of arbitrarily located sensors.  So it's less than ideal.  But their research demonstrates that they are able to, not perfectly, but to significantly determine the instantaneous position of a user's finger which is in contact with the screen.



So reading from their abstract at the top of their research, they said:  "We report the first active acoustic side-channel attack.  Speakers are used to emit human-inaudible acoustic signals" - you know, like bats - "and the echo is recorded via microphones, turning the acoustic system into a smartphone sonar system.  The echo signal can be used to profile user interaction with the device.  For example, a victim's finger movements can be inferred to steal Android unlock patterns.  In our empirical study, the number of candidate unlock patterns that an attacker must try to authenticate herself to a Samsung S4 phone can be reduced by up to 70% using this novel acoustic side-channel."  So, note, not reduced to zero, but reduced by more than half.



"The attack is entirely unnoticeable to victims.  Our approach," they write, "can be easily applied to other application scenarios and device types.  Overall, our work highlights a new family of security threats."  So in terms of details, they used a dictionary of 12 unlock patterns.  Okay, so not the universe of possible unlock patterns.  They just, you know, they were looking for does this kind of work at all.  So they used a dictionary of 12 unlock patterns in their tests which contained 15 unique strokes.



The data collected from 12 volunteers was fed into an AI learning machine model for classification of each stroke; and, as expected, the classification accuracy was significantly higher when simultaneous input from both microphones was considered.  The researchers reduced the average number of correct candidates from the 12 unlock patterns to 3.6.  That's average.  In some instances, the analysis eliminated all guesses and revealed the single correct pattern uniquely.



I looked at the research, and I won't go into any more detail here because everybody gets it.  But I have a strong gut sense that we're going to see this evolve.  It uses sensors available in our devices.  It could certainly be used for benign purpose,  like moving your hand around in front of an app in order to do something, and having the app respond.  It's not super high fidelity.  Again, the sensors are not located exactly where you want them.  Was it the Apple, I mean, it was the Amazon phone.  They had like some funky sensor in each corner of the screen for a while; didn't they, Jason?



JASON:  Yeah.  And that was meant to kind of change the parallax of what you were looking at.  It was meant to kind of, like, from the four points be able to determine your perspective and shift accordingly.



STEVE:  So to see your face so that they could change the screen in order to make it look more depth-y.



JASON:  It was a lot of hardware for a very minimal application purpose.



STEVE:  Yeah.



JASON:  But, no, what this reminds me of is, if you remember, I think it was like three or four years ago at Google I/O, Google had shown off something called Project Soli, which was also...



STEVE:  Oh, yes, yes, yes.  And I am so jazzed by that, where you could do, like, push buttons and turn knobs and things.  Oh, I really want to see that happen.



JASON:  I haven't really heard much about it since then.  And I think their idea was that it would possibly be integrated into wearables, like smartwatches of some sort, so that you wouldn't have to touch it, but you could still do the controls.  And then there's also something else that I read about called, what was it, it was called FingerIO, which there's a demonstration.  And that seems a little bit more aligned with what SonarSnoop is all about.



STEVE:  Yeah.  And Soli was EM radiation.  So it was very low-level electromagnetic radiation that gave it extremely high resolution in return for having to have a lot of processing behind it.



JASON:  Right.



STEVE:  But anyway, I thought this was very cool, very clever, something no one had thought of before.  And I wouldn't be surprised if we see this evolving in the future.  This feels like something that could be taken from this initial research out to its next level.  So props to these guys, and a nice piece of research.



JASON:  Yeah.



STEVE:  No matter what you call it.



JASON:  I'm curious on a device-specific perspective because, like you were saying, every phone has a microphone and a speaker in a different location.  And so for this to work on a broad spectrum of Android devices, I imagine those locations create different hurdles or different requirements for where exactly on the screen your finger is at any given moment.  If the speakers and microphones are over here versus over there, does that then narrow down the accuracy differently?  



STEVE:  Well, and so, for example, one thing that's - if somebody wanted to be clever, they could create a game which inherently requires you to move your finger around and touch different areas of the screen.



JASON:  Oh, to learn, yeah.



STEVE:  While, yes, while you're playing the game it's doing sonar on you, and you don't know.



JASON:  It's calibrating.



STEVE:  Yes.



JASON:  Yup.  Dang it, that's too wise.  That's too smart.  And you know what, that'll probably happen.  But this only really, from a security standpoint it seems like this really only works if you're doing a swipe pattern; right?  Would there be any indication that it would work with tapping in a PIN or something along those lines?



STEVE:  And, see, that's exactly what I mean when I say this feels like first gen.



JASON:  Yeah.



STEVE:  And so they demonstrated the concept is viable.  And so I wouldn't be surprised if, before long, they could turn this into read keystrokes on a keyboard.



JASON:  Totally.  Well, that's another thing this kind of reminded me of, the whole thread from years ago about people being able to listen to someone typing on a keyboard and being able to reconstruct what they're typing based on the sound of those keys clacking.



STEVE:  Yeah, yeah.  That's just bizarre, but it's true.



JASON:  Yeah.  Oh, this stuff is crazy.  Very interesting.  Well, we'll certainly find out more about that as we go along.  But have we missed anything?



STEVE:  We're done, baby.



JASON:  We hit it.  We hit the end.  Our outro has been SonarSnoop.



STEVE:  Oh, my god, and look at that.  My clock says one hour, 50 - oh, there it is, two hours.  Two zero zero zero zero.



JASON:  We made it.  Steve, thank you so much for all the news and diving in deep on all this stuff.  It's always awesome to get the chance to do this with you, and I get another two weeks, so I'm looking forward to that.  For folks who don't already know - you already know, but as a reminder, GRC.com if you want to find all the cool stuff that Steve is up to.  You can get SpinRite there, which you heard him talk about a little bit ago, the best hard drive recovery and maintenance tool.  Get your copy there.  I'm sure information on SQRL, all sorts of stuff found at your site; right?



STEVE:  Yup, yup.



JASON:  So make sure and do that, GRC.com.  And if you want to find this show, I think you're also posting audio and video over there, as well; right?



STEVE:  We have audio, video, and the transcripts are uniquely available over at GRC.com/securitynow.



JASON:  Yes.  And you'll want those transcripts, as well.  So check it out over there.  You can also come to our site, of course.  We have the show, every single episode cataloged at TWiT.tv/sn.  You can find audio and video there.  Subscribe to the podcast.  Of course you can find this on YouTube and anywhere.  You know, if you have a smart TV, just do a search in the console there, you're going to find it.  And, yeah, I think that's about it.  We record live every Tuesday, starting at 1:30 p.m. Pacific, that's 4:30 p.m. Eastern, 20:30 UTC.  And TWiT.tv/live if you want to watch us live.  Thanks to the folks in the chatroom.  They were throwing out some valuable information to us, as well, that kind of made its way into the show.  So that's what I love about the live experience.  People get in there, they're chatting about it, and sometimes it kind of makes its way in.  So we appreciate their participation, as well.



That is it.  Steve, thank you so much, man.  And we will see you next week.



STEVE:  Thanks, buddy.  I'll talk to you next week.



JASON:  Take care.  Another episode of Security Now!.  See you later.



STEVE:  Bye.  



JASON:  Bye, everybody.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#680

DATE:		September 11, 2018

TITLE:		Exploits & Updates

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.GRC.com/sn/SN-680.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss Windows 7's additional three years of support life, MikroTik routers back in the news (and not in a good way), Google Chrome 69's new features, the hack of MEGA's cloud storage extension for Chrome, Week 3 of the Windows Task Scheduler zero-day, a new consequence of using "1234" as your password, Tesla making their white hat hacking policies clear (just in time for a big new hack!), our PCs as the new malware battlefield, a dangerous OpenVPN feature spotted, and Trend Micro, caught spying, getting kicked out of the macOS store.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo on the second of three weeks in a row.  Steve's going to talk about a whole bunch of updates.  Security news is flowing even as the show is going on.  We're going to talk a little bit about Google Chrome 69's URL hiding feature and whether you want to keep that or maybe toggle it off.  There's a hack of MEGA's cloud storage extension for Chrome.  We'll dive into that.  Tesla is making its white hat hacking policies very clear, and it really just happened at just the right time because there's basically breaking news involving Tesla, and a whole lot more coming up next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 680, recorded on Tuesday, September 11th, 2018:  Exploits & Updates.



It's time for Security Now!.  This is the show where we talk about all things security.  And boy, today do we have a ton of security stories to check in on.  Obviously Leo is out.  I, Jason Howell, am in, in Leo's place.  Not in his comfortable studio digs.  I'm more in like the big studio here, the big open space that allows us to actually have an audience.  Steve Gibson, we have a live audience in the studio today.



STEVE GIBSON:  I think Leo kind of likes to keep the office to himself; right?



JASON:  I think so.



STEVE:  It's sort of like his inner sanctum, and everybody else gets to play out there in the studio.



JASON:  Yeah, totally.



STEVE:  So I think that works.



JASON:  I think it's fair.  He kind of like owns the place.  He runs the place.  So he's - it's fairly done.



STEVE:  Ah, I see somebody's white pants with their legs crossed in the...



JASON:  I know.  We've got not one, not two, not three, but four people.



STEVE:  Now they're uncrossed.  Oh, okay.



JASON:  Four folks in here who all made the trek.  They're all in the Bay Area specifically for no other reason than to watch Security Now!.  That's what I'm to understand, anyway.



STEVE:  And, you know, having driven that goat trail up from San Francisco to Petaluma, I really do appreciate the effort that they went to.



JASON:  Oh, I know.



STEVE:  It's not easy to get there from civilization.



JASON:  It's not easy to get there.  But what they get at the end of the rainbow, they get Steve Gibson and Security Now!.



STEVE:  Eh, well, sorry about that.  So we're Episode 680.  And this is, of course, the 17th anniversary of 9/11.



JASON:  Yes, it is.



STEVE:  The day that lives in infamy for the U.S.



JASON:  Absolutely.



STEVE:  So there's been lots of acknowledgement of that and so forth.  And so it's a perfect day for Security Now!.  We were talking about before we began there wasn't any one big thing that seemed to, like, dominate.  So I just named this episode "Exploits & Updates" because we have a lot of both.



I did have news on Patch Tuesday, which we will get to.  We're going to discuss Windows 7's newly announced additional three years of life support.  MikroTik routers are back in the news yet again, and of course never in a good way.  Google has released Chrome 69 with some controversial new features which are raising some questions.  There was a hack of MEGA's - MEGA is an encrypted cloud storage solution that's very popular.  Their Chrome extension got hacked and was present in the Chrome Store for, I think, about four hours last week.  And we'll talk about the consequences of that.



We've got Week 3, and that's the final week, actually, of the Windows Task Schedule zero-day, that advanced local procedure privilege elevation.  We have an interesting new consequence of using a weak password like 1234.  Tesla has made their white hat hacking policies very clear, and just in the nick of time because there's also a big new hack of the original security key that the Tesla Model 2s have been using.  Our PCs are becoming a battlefield for malware, which we'll talk about.



We've got a dangerous OpenVPN feature was spotted, and looks like the door was closed before it got abused.  And then Trend Micro was caught spying and has gotten itself kicked out of the macOS store.  And we of course have a fun Picture of the Week that we'll talk about.



JASON:  All right.  Picture of the Week to begin things.  Tell us what we're looking at here.



STEVE:  So this will not convey well in words, unfortunately.  So I think our listeners, if they're interested, are going to have to grab the show notes and take a look at it.  But I'll describe it.  It's the ever-wonderful and often brilliant xkcd brings us this.  So we sort of start in the upper left.  There's four frames.  And the first frame shows sort of a network diagram with some things connected, but they're like floating around.  And the caption is:  "I wish these parts could communicate more easily."



And then there's an arrow taking us to the next frame that shows sort of the same parts that have a bunch of green connections, interconnections between them, and that seems like a good thing.  The caption says:  "Oh, this new technology makes it easy to create arbitrary connections integrating everything."  And then the third frame shows now these things are all still connected, but now we've got some red lines and circles, and looks like it's not quite so happy.  And the caption is:  "Uh-oh.  There are so many connections it's creating bugs and security holes."



Which leads us to the fourth frame, which shows a bunch of green enclosures around the different areas, breaking them apart.  And now the caption is:  "Oh, this new technology makes it easy to enclose arbitrary things in secure sandboxes."  Which leads us - there's a fourth arrow leading us back to the first one that says:  "I wish these parts could communicate more easily."



JASON:  Wait.  So we never reach the end. There is no actual end.  We've never solved all these problems.  It just keeps getting more and more complicated.



STEVE:  Yes.  And in fact, you just said the word.  As a consequence of our desire to interconnect, then realize, whoops, we've got some side effects that we didn't expect from the interconnections.  So now we've got to isolate them, but now we're not happy that they're isolated because they really need to be talking to each other.  So we're going to interconnect them maybe in a different way.  Whoops, different problems.  Now let's re-isolate them and so forth.  We go around in circles.  We end up dizzy and with lots of complications.  So anyway, xkcd just does it again.



JASON:  Love it.



STEVE:  It's like, you know, brilliant.



JASON:  Yeah.  They are super brilliant.  It just reminds me of how services, I mean, this is such a, you know, as far as like Internet services are concerned, such an indicator of what we're so used to seeing.  Service starts out, it's one single use case.  You're like, oh, I love it for its simplicity.  But then over the years they feel the need to kind of build upon it and do all these other things, which is great from a feature set standpoint, but just complicates things so specifically.  And it's such a great effect that it loses the thing that it once captured.



STEVE:  Well, and probably there are several great examples of that.  But one that is apparent, I think, to everyone is the original iPhone.  The original iPhone, it's funny because it sort of looked incomplete.  If you remember, it did not support applications, that is, third-party apps in the beginning.  It had this screen with nine icons on it, kind of all in the upper.  And they didn't even have a complete last row.  There were, like, two icons out of the four that there was room for.  It just sort of looked like it was unfinished.  It's like, well, where's everything else?  And the point is, no, this is what you're getting.  And of course, as we know, the UI was designed for that.  Everything worked really well.  But people wanted more features; and unfortunately, with more features comes...



JASON:  Comes a blanket.



STEVE:  Yes.  Now we need folders to enclose the icons because otherwise we're just scrolling, like, where did this app, I mean, I often have the problem of, like, I'm sure there's an app in here somewhere that I downloaded once.  And you're just scrolling around, and it's not easy to find them.  So now I just sort of search for it because I can't find it among all this junk that I've got loaded onto my iOS device.  So, and of course this is the problem is that everybody wants more features, yet then with that comes interactions among them, and complexity.  And then it slows down.  And, I mean, also old-timers among us will remember the days when Microsoft was always several chips behind, and you just couldn't get the systems to go fast enough because we were trying to get them to do so much.  And I'm glad that the days of Windows crashing constantly are behind us.  I mean, for most people, Windows seems to be pretty stable now.  So that's good.



And speaking of Windows, this is second Tuesday of the month of September, Patch Tuesday.  Microsoft, I don't have a detailed breakdown, which sometimes it's useful to give.  This time, eh, not so much.  The big news is the one thing that we were hoping for, expecting, and kind of wanting, was this advanced local procedure call zero-day, which was disclosed by that disheartened hacker via Twitter a couple weeks ago.  We have followed it since.  It matured.  It was able to run on 32-bit machines, and then someone figured out how to change a filename, change a 3 to a 1 to get it to run on Windows 7.  And we'll be talking about in a minute the fact that it did end up being exploited.



We now know, thanks to some researchers at ESET, almost immediately it has been patched.  So it wasn't super critical because it wasn't a remote code execution.  But as I said at the time, anything that gets in your system definitely wants to elevate its privileges, some say "escalate," to obtain the rights to write itself into sensitive areas of a system or to modify system files which by design people running with non-admin privilege are unable to do.  So anyway, we got 62 vulnerabilities patched today, this morning, of which 17 were critical.  Most of them were browser-related.  The rest include Windows Office, Hyper-V, the .NET framework, and so forth. so as I said, it's one of those things people should patch as soon as they can because Microsoft is constantly fixing problems.



And speaking of constantly fixing problems, it was announced, on looks like the 6th is the date of this blog post, that Windows 7 support would be available beyond 2020.  Which doesn't really surprise me.  So the title, their title was "Helping Customers Shift to a Modern Desktop."  However, being the author of Never10, I should mention now with nearly three million downloads of this little utility that I wrote, rather than "Helping Customers Shift to a Modern Desktop," I would title the blog posting "Being of Necessity Somewhat More Patient in Our Effort to Force Customers onto a Less Desirable Desktop, That They Really Do Not Want...."  Anyway...



JASON:  I actually think that that fits in with Microsoft's kind of ambition and habit of naming things really long strings of words.  So I think that would actually work for their naming template.



STEVE:  Almost a more appropriate title than their official one.  Yes.  And speaking of which, even the guy's title, this is the "Microsoft Corporate Vice President for Office and Windows Marketing" is the person who wrote this.  Anyway, he says, under the subheading of "Windows 7 Extended Security Updates," they wrote:  "As previously announced, Windows 7 extended support is ending January 14th, 2020.  While many of you are" - love this - "are already well on your way to deploying Windows 10" - yeah - "we understand that everyone is at a different point in the upgrade process."  Yes.  There's, like, barricades in front of some of us.



JASON:  Wishful thinking.



STEVE:  Yes.  And let's note that, despite Microsoft's extensive and highly controversial efforts to actively force everyone over to Windows 10, such that no one would still be on Windows 7 without actively resisting the push to Windows 10 - as we know, I mean, you had to struggle not to have your Windows 7 machine grant you Windows 10 despite your desire to stay where you were - today, okay, today Windows 7 remains the majority desktop in the world, with a greater market share than Windows 10.  Now, not by much, granted.  Windows 7, courtesy of Bleeping Computer, I got this chart from them because they pulled some market research - Windows 7 is at 40.27% to Windows 10 at 37.8.  And then followed by the macOS at 5.86, Windows 8.1 at 5.10, and believe it or not, well, actually I shouldn't say that because I was on XP not long ago, XP at 3.30%.



JASON:  I believe it.  I actually kind of expected XP to be higher than that.  But it was a longstanding version.



STEVE:  It doesn't know about, I guess it was SP2 doesn't know about SHA-256.  It doesn't know about any of the TLS protocol updates.  I mean, it's really - XP was really sort of struggling to stay alive.  But the point is, for Windows 7 to still be more popular, to have a larger market share than 10, I mean, the only way that would happen is if people refused to budge.  So it's worth noting, though, Windows 7 is waning, and 10 is gaining.



And of course one of the reasons is it is no longer possible for consumers to purchase systems with Windows 7, much as they may wish to.  And the latest Intel chipsets are no longer compatible with Windows 7.  Ask me how I know.  Yes, I know.  I'm deliberately now, when I need a new machine, I have to go an older, I think it's i6 or 7; 8 won't run Windows 7 any longer.  So the reason 10 is ultimately going to win is just through attrition.  People will hold onto 7 with, apparently, with their dying gasp.  But they're going to end up ultimately losing because you just can't get it anymore.



Anyway, so Microsoft continues their blog post, saying:  "With that in mind" - that is, that we're all in different stages of adoption - "today we are announcing that we will offer paid Windows 7 Extended Security Updates" - of course we have an acronym, the ESU Program - "through January 2023.  The Windows 7 ESU will be sold on a per-device basis, and the price will increase each year."  Oh, they're going to just force people off this thing one way or the other.



"Windows 7 ESUs will be available to all Windows 7 Professional and Windows 7 Enterprise customers in Volume Licensing" - meaning not end-users - "with a discount to customers with Windows software assurance, Windows 10 Enterprise, or Windows 10 Education subscriptions.  In addition," they said, "Office 365 ProPlus will be supported on devices with active Windows 7 (ESU) through January 2023.  This means that customers who purchase Windows 7 ESU will be able to continue to run Office 365 ProPlus."  Will be able to run it?  You wouldn't be able to run it?  Oh, well, who knows what they're going to do.  I mean, you know, they're switching this whole thing over to OS as a service, rather than an operating system as an operating system, because they can.  So anyway, for what it's worth...



JASON:  When did it initially launch?  This was, what, seven, eight years ago?



STEVE:  Yeah.



JASON:  So talking 2023, I mean, man, that's 13 years old.  Hopefully by that point [crosstalk].



STEVE:  And the problem is people, I mean, the problem is it works just fine.



JASON:  Yeah.



STEVE:  You know?  It's like it's not - you don't have...



JASON:  They made it too good.



STEVE:  Well, actually, I would argue that at the time Windows 2000 worked just well.  Windows XP worked just fine.  I mean, this whole idea of needing 64 bits, 64, come on.  If they had designed the 32-bit OS correctly so that it could use virtual memory - but again, they didn't want to.  Anyway, I guess I'll end up just on FreeBSD Unix at some point and then just say, well, good luck to the rest of you.



JASON:  But you're an individual user.  Would you qualify for it because...



STEVE:  No.



JASON:  So you wouldn't even qualify for it if you wanted to pay and get some of that tech support.



STEVE:  And that will not bother me because, after all, I wasn't getting Windows XP updates for quite some time.  And once again, everything just continued to work fine.



JASON:  Hey, what do you know.



STEVE:  So, well, because the browsers really are our portal to the world.  I mean, yes, you need to download software.  But if you're careful, if you're not hanging out in sketchy areas of the Internet, if you're behaving yourself, and if you've got something watching your back, I mean, I guess now, well, I'm sure that Security Essentials will stop working for me also because it'll be like, agh, you're still here?  You're still using that Windows 7?  What's wrong with you?



Anyway, yes.  I don't think, I mean, I have some Windows 10 machines.  I'm talking to you over a Windows 10 machine.  And there are some places where I have had recently to use updated chipsets and couldn't use 7, so I had to use 10.  I did manage to scrape all of that other junk off of it.  I mean, there's, like, there's HoloLens is built into Windows 10.  It's like, I don't want a HoloLens on my operating system.  Or Xbox anything, live or dead.  So anyway...



JASON:  Yeah, but you will.  HoloLens is going to change the world, Steve.  You're going to be right there with it.  You're going to be podcasting from HoloLens in a couple of years.  You just don't realize it yet.



STEVE:  God help me.  Well, we are talking over Skype, so maybe that's true, I don't know.



JASON:  That's totally possible, actually.



STEVE:  We'll have HoloSkype, with no choice.



JASON:  I wouldn't be surprised.



STEVE:  Okay.  So MikroTik is the way you're supposed to pronounce this.  But I really did like pronouncing it mi-CROT-ic because it sounds so awful.  And I know that they're very popular routers.  There are a lot of people who like them.  I mean, they're also very powerful.  I mean, they've got a nice UI.  They are feature packed.  But here's where features come to bite you.  I mean, this is just exactly like xkcd's cartoon.  The MikroTik routers are back in the news, and not in a good way.



As we know, they're been suffering all year as a consequence of a problem that was first found in their SMB handling, and then their handling of Winbox authentication, which allowed for unauthenticated remote access.  Winbox is a Windows-hosted UI which allows for administration of the router.  The problem is, and for the life of me I don't understand how this could happen, the Winbox access is by default available on the WAN interface in addition to the LAN interface.  In other words, exposed to the public.  And it's a router.  So of course it's going to be on the edge of your network, half of it looking out to the rest of the Internet, and the other half looking into your modest little network.



Anyway, back on April 23rd, MikroTik explained in their vulnerability disclosure, they said:  "We have discovered a new RouterOS vulnerability affecting all RouterOS versions since v6.29.  The vulnerability," they wrote, "allowed a special tool to connect to the Winbox port and request the system user database file."  Of course the Winbox port should have never been available to connect to from the Internet.  But they didn't mention that.  They said the attacker would then use the user details found in the exfiltrated user database file to log into the MikroTik router remotely.  And of course then the party begins.



Okay.  So the vulnerability in question was titled "Winbox Any Directory File Read," and it's got a CVE of 2018-14847.  And it turns out that it was found exploited by the CIA Vault 7 hacking tool called "Chimay Red," along with another MikroTik WebFig remote code execution vulnerability.  So they're feature rich.  These routers are popular.  And to their credit, I absolutely give them credit for patching that vulnerability within hours of hearing of it.



But we know that without the ability to push the vulnerability fix out to devices somehow, or probably by devices automatically periodically phoning home to check for an auto update, and then autonomously doing it because I'm sure that those of us with routers who are security conscious, we've looked.  You log into your router, and you tell it to check for updates.  And it's like, oh, yes, I got a flashing exclamation point.  There's an update.  Well, how long as it been available?  Is it crucial?  Is it critical?  You know, the router doesn't tell you.  It just needs to do it.  You need to - part of setting it up has to, you know, like you give it permission to maintain itself, and then it just does this for you.  But as we've said often, probably too often, we're not there yet.



Okay.  So now, today, this exploit code which affects all not-yet-patched MikroTik routers, is available from at least three different online sources.  So it is hardly surprising when, at the beginning of last month, we reported, as we have been, that somewhere around 200,000 - 200,000 - MikroTik routers were enlisted in a massive Coinhive mal-mining operation.  And since the beginning of these various attacks on consumer and corporate routers, of course, I've been observing how fortunate it is that the bad guys seem uninterested in the details of the networks lying behind these infected and infested routers.  I've often commented that it's like let's just hope that's the way things remain, that all they want to do is stick mining malware on the routers and not care about the fact that they've established presence on the device through which all of your local network traffic transits.



Well, it turns out that's changed.  Last week researchers at 360 Netlab posted their article titled "7,500+ MikroTik Routers Are Forwarding Owners' Traffic to Attackers."  And they ask then rhetorically:  "How Is Yours?"  They wrote:  "We understand that user devices come and go on the Internet all the time, so the data used in this blog reflects what we saw between August 23rd and August 24th.  From our own scan result, we logged more than 5,000,000 devices with open port TCP 8291."  That's this web API that the MikroTik routers support.  This Winbox port is 8291.  They said more than five million devices with that port open, and 1.2 million of them were identified as MikroTik devices, within which 370,000, which is to say 30%, are vulnerable to this CVE-2018-14847.  In other words, 370,000 MikroTik routers can be remotely accessed today over this Winbox port and are vulnerable to this non-authenticated access.



Now, you're showing on the screen the chart from the show notes, which shows the distribution in the first column of the top 20 nations.  Brazil, for example, has 42,376 vulnerable routers.  Russia, in number two, at 40,742.  Indonesia, 22,441.  And onwards, downwards, all the way through the top 20 nations.  This middle column shows the top attackers who have been observed attacking these routers by IP address.  And the list of ports being eavesdropped on.  I'll get to how that's happening next.



So on these routers is still Coinhive mining code injection, so they're generating revenue at some pace using the coin mining script.  And remember we talked about this before where they use the opportunity of displaying a nonencrypted error page to inject the mining script into the user's browser.  All the browsers that are being serviced behind that router will receive the error page unencrypted with Coinhive mining.  So that's how they're getting around the HTTPS and TLS encryption, which otherwise would not allow them to inject their code into an encrypted connection.



Okay.  Additionally, at the moment, right now, a total of 239,000 MikroTik routers are confirmed to have a Socks4 proxy enabled maliciously.  The Socks4 port is mostly configured to 4153.  And, interestingly, it only allows access from one single netblock, which is 95.154.216.128/25.  So it looks like there's one perp behind all of this.  In order for the attacker to gain control after device reboot, and the possibility that the public IP might change, the device is configured to run a scheduled task to periodically report its current IP by accessing a specific attacker URL.  And the attacker is continuing to scan the Internet because of course it's a rich source of vulnerable MikroTik routers.



Okay.  So, finally the MikroTik RouterOS device allows, as I said, it's very feature rich.  It allows its users to configure the router to capture and forward packets using a protocol known as TZSP, which is the TaZmen Sniffer Protocol, which we've never talked about before, but it's an encapsulation protocol used for sniffing and forwarding packets.  Intrusion detection systems use it, and it's supported by Wireshark that knows how to look inside that protocol and look at the packets inside.  So it's well known.  So they did a standards-based sniffer.  Unfortunately, and again, I guess this is a feature, but the forwarding of the packet capture and sniffing is not restricted to the LAN.  It can be set over the WAN.



So using first the fact that we have this Winbox vulnerability on 370,000 MikroTik routers, more than 7,500 of those have been configured with this sniffer actively capturing the traffic that is transiting the router.  And you might say, okay, fine, but it's encrypted; right?  Well, no.  The number one port being captured is port 21, which is one of the two ports used for FTP, which is typically itself not encrypted.  Second up is 143, which is IMAP, which of course is email, also, as we've often said, unencrypted.  Next up, 110, which is the POP3 protocol.  Not encrypted.  Fifth up is port 25, SMTP, which is used for submitting mail as opposed to getting mail.  Unencrypted.



So what these guys are doing is they've taken advantage of the Winbox vulnerability, which is widespread on MikroTik routers, and have used a feature in the router to essentially forward all of the FTP traffic, of which there's probably not that much, but maybe FTP uploads would be of some interest to them, you know, like people putting stuff up on a server somewhere.  But essentially intercepting all of the email, unencrypted, in and out of that network.



So that's, again, as I said, MikroTik in the news.  In their disclosure of this, at the end of their explanation of the counts they saw, the ports, posting that chart of everything, they said, that is, the 360 Netlab folks said:  "We recommend that MikroTik RouterOS users update the software system in a timely manner and check whether the HTTP proxy, Socks4 proxy, and network traffic capture functions are being maliciously exploited by attackers."  Yes.  I think that's good advice.



They said:  "We recommend that MikroTik denies inbound access to the WebFig and Winbox ports from the Internet [uh-huh] and improve the software security update mechanism."  Oh, I completely concur.  So, you know, we know that anyone can make a mistake.  And I do congratulate the MikroTik folks for quickly creating a patch for their mistake.  It was within hours of finding out about it.  I mean, yeah, anyone responsible would do that.  But this disaster - and this is what you'd have to call this.  This is a disaster.  For there to be currently 370,000 routers more than six months after a critical vulnerability was publicly disclosed, I mean, it's not like nobody knows about this.  Everybody knows about this.  And people are not fixing their routers, leaving them open to people who want to exploit them.



So the point is this disaster is not the result of a mistake.  It's the consequence of a deliberately and misguided policy of enabling WAN-side remote access and management by default.  These routers default to making this access available.  They should default to it being off.  And then only if a given user actually needs or wants remote admin over this originally insecure, now secured service, should they turn it on with cautions about strong passwords and so forth in the router.



So I know everybody likes the routers.  They're popular.  They're feature-rich.  But there is a serious policy problem.  And if you happen to own a MikroTik router, absolutely make sure that you've got this WAN-side admin stuff off.  There's no way there could be 370,000 of these exposed publicly if it weren't for the fact that they were exposed by default.  So I don't know whether the firmware change closed it by default.  They're probably worried that they're losing a feature.  Well, it ought to be manually enabled, rather than automatic.  There's just no way around that.



JASON:  Absolutely.  And maybe it'd take a scenario like this to teach them that lesson.  One thing I'm curious about as far as the cryptomining aspect of this, which it sounds like is something that you've talked about in weeks past.  But when you're talking about this number of routers, all focused on cryptomining to this degree, what does that actually translate to in crypto terms on the other end?



STEVE:  We did talk about this.  And what they're doing is they're using a script that runs in the browser.  They're mining Monero currency because that's CPU-friendly and GPU-hostile.  So it gives computers a fairer chance, for example, as opposed to bitcoin, which is all just gone to ASICs now.  So you just can't mine bitcoin with a regular CPU.  So they're mining them in browsers.  They're using a script hosted by Coinhive, which is the "service," unquote, I mean, they're definitely gray hat, tending toward black.  So Coinhive is the service that the JavaScript comes from.  Coinhive gets a significant piece of the action and is currently making a quarter million dollars, boy, was it a week or a month?  At least a month.  So I think it was a quarter million dollars a month in their piece of the action from this malicious use of their script on people's browsers.  So there's money being made.



JASON:  Absolutely.  Wow.  Big-time.



STEVE:  So we're now at Chrome, Google Chrome 69.



JASON:  Yes.  I upgraded on my desktop in the other room just this morning, actually.



STEVE:  Cool.  And so I went to www.grc.com, which is the official name of GRC's website.  And I was greeted by the nice EV Gibson Research Corporation with [US].  But the URL didn't have http:// nor www.  They were missing.  All I saw was GRC.com.  And I had to click twice in the URL field to see the full URL because I thought, what?  Okay, what's more, you put blog.google.com in, and it turns it into blog.google.  So I thought, what?  Okay.  So then I put blog.grc.com in because that's valid also, but it did not turn it into blog.grc, despite the fact that blog.grc.com is also backed by a highly trusted DigiCert extended validation certificate.  So what's going on?



Well, first of all, I figured out the blog.google.com mystery, how were they turning themselves into just blog.google, yet blog.grc.com was not turning to blog.grc.  I saw that Google, as I had mentioned to Leo some weeks back, now owns the "Google" top level domain, that is, .google.  So blog.google.com turns into www.blog.google, which now in Chrome 69 appears as blog.google because they're getting rid of the www. in front of everybody's domain.  They've decided, Google has, to call these subdomains "trivial," unquote, I'm not kidding.  And apparently on the mobile platforms m. is also removed as superfluous.



So this has ruffled a lot of feathers, I mean, among the old fogies on the Internet, who are not happy about this.  And it's funny because, like, oh, it was at least 10 years ago, maybe more, I had a decision to make with GRC, and that was originally you could get to GRC either with or without www.  And as a consequence, some people put it in, some people didn't.  But the Google was indexing us both ways.  And I thought, okay, this is dumb.  We ought to have, like, one official way of being a website.  And so the question is, should it be with or without the www?



So there was a big discussion in GRC's newsgroups, and I decided to go with www, since I had other subdomains of GRC, like news.grc.com, for example, is for the newsgroups and so forth.  So that seemed to be the right thing to do.  And I look at, like, Amazon.com is www.amazon.com.  Well, no more.  Not if you're in Chrome.  Now it's just Amazon.com, even though the www. is still there.  And I guess there are some really weird - I did some poking around among all of the people who are unhappy about this.  I guess things like example.www.example.com, the www just gets stripped out of the middle.  And so Chrome is doing like these weird things to domain names, which has the purists up in arms.



So it'll be fun to experiment with this.  I'm not, I mean, I have long said, and often said, that the whole concept of the URL is user hostile.  You know, my mom, she said, "Honey, what is http://?  What do I need to do that for?"  And I thought, yeah, I know, Mom.  But, I mean, you know.  And I don't disagree that www is, well, can be superfluous.  If you put GRC.com in, I do have that domain, I have a certificate for it, and I redirect people to www.grc.com.  And now all the links for search engines to GRC are unified with www.  Now Chrome has decided it's not going to show us the www.



So it'll be interesting to see how the controversy fares because they have created some.  They've also done some other things in 69.  It is the 10th anniversary of Chrome, by the way, so this is like the birthday party, the 10th year birthday party for Chrome.  Discussing Chrome turning 10, they said:  "Getting things done faster."  And I'm quoting from Google.  "You get a lot done online these days - booking travel and appointments, shopping, and working through your to-do lists across multiple sites at once.  And we want to make sure that you can do all these things easily and safely.  Now, Chrome can more accurately fill in your passwords, addresses, and credit card numbers, so that you can breeze through online checkout forms.  All this information is saved to your Google account and can also now be accessed directly from the Chrome toolbar.



"We've also significantly improved the way Chrome handles passwords.  Staying secure on the web means using strong and unique passwords for every different site.  When it's time to create a new password, Chrome will now generate one for you," they say, parens, "(so you're not using your puppy's name for all your passwords anymore).  Chrome will save it; and next time you sign in, it'll be there, on both your laptop and phone."



Okay, so I was curious about this.  I did a bit more digging and found some discussion on a Google blog about Chrome's new password manager.  They say:  "In a time when most people don't remember more than a handful of phone numbers, can you really be expected to remember a strong, unique password for every online service?"  And of course not.  That's why we're all using password managers today; right?



They say:  "It's no wonder most of us end up using an easy-to-remember password over and over again.  But if it gets stolen, as were 3.3 billion credentials last year alone, you're exposed to a much greater risk because now the thieves have a key that works across several sites.  So what are you supposed to do?  Write them all down?  Do the forget/reset shuffle every few weeks?  There has to be a better way, and now there is."



They said:  "As part of this week's update, we're rolling out significant improvements to Chrome's password manager.  Across desktop platforms, and coming soon to mobile apps, we're rolling out unique password generation.  Chrome will now recognize a sign-up field, offer you a unique and secure password for that site, and save it.  Every password follows these guidelines: at least one lowercase character, at least one uppercase character, and at least one number.  If a site requires symbols, we'll include those, too.  We'll also avoid certain characters for readability issues" - okay, I'm not sure why that matters, but okay - like a lowercase "l" or uppercase "I," which can be confused visually.



"You can view all your passwords, credit cards, addresses, and other stored information from the main desktop Chrome toolbar.  You can also export all your saved passwords into a .csv file at any time."  And they finish:  "We've also made password autofill even more reliable."  Of course we've talked about lots of hacks of autofill where forms are offscreen, and you don't realize your browser has filled them, and then JavaScript running on the page sucks them off the form, and you've just been hacked.  Whoops.



Anyway:  "Now, Chrome can be used to save or fill in the appropriate password on any site you need.  This update would not be possible without significant improvements to the underlying autofill capabilities.  When Chrome fills in your passwords, credit cards, addresses, emails, and other types of information, it's backed up by Chrome's multiple security layers and web standards."  Well, we're about to hear about a breach of those, but hold on.  "And if you're signed into Chrome across your devices, syncing your credentials to your Google Account will allow you to access them wherever you have Chrome installed, laptop or mobile.  And if you're using the Android app of your favorite site, your passwords and other information will be there, too."



Okay.  Well, of course that's a solution.  It doesn't solve the problem of services being compromised and losing their passwords, nor of password recovery hacks, which they talk about the reset and receive a new password problem.  Nor local browser extension malware, which did hit Chrome last week.  And of course it's not browser agnostic, so it's creating a so-called "walled garden" and requiring that people stay with Chrome.  But I would, I mean, I'm not saying it's not a useful step.  On the other hand, the MEGA cloud storage extension for Chrome was hacked to do exactly this, to steal login credentials.  And Jason, we're at a good point to take a break.  I will explain what happened with MEGA cloud storage extension when we come back.



JASON:  Yeah.  And I should say, like I said, I installed that Chrome update earlier today.  And now it's confusing because I have LastPass running as my password manager.  So when I go to log into a Google account, it's like the LastPass is behind the autofill suggestions that appear above it.  So if my account that I'm trying to log into is the topmost account in my LastPass list, I can't verify which account it is because it's covered up by Google's version of it.  And so there's no way around it.  And I just don't know, like being so ingrained in LastPass as a password manager, do you feel confident enough in Google's kind of password management within Chrome by comparison?  Or do you think there's still a gap?



STEVE:  Well, it's single browser; right?  So with LastPass we have it under Firefox and across our browsers, across our platforms.  You know, Google is multiplatform, so Chrome is all there.  The problem is that the browser is the attack surface of the future.  I mean, it's the way we reach out onto the Internet.  And we've been covering stories about all the malware in the Android Play Store, which Google, despite their best efforts, is always chasing.



We've got problems also with the Chrome browser extensions, and if, for example, this MEGA cloud storage extension was attempting to steal your Google login credentials.  So if your Google login credentials get stolen, and they can work around, and there are ways to work around, the various second-factor approaches that Google has used like identifying whether this system or this IP has been seen logging in before, then they have access to this massive repository of everything, your credit card numbers and all of the passwords that have been generated that are being stored in Chrome.  So, I mean, there are benefits to a monoculture, and there are risks to a monoculture.  And what I'm going to talk about when I get through talking about MEGA is the fundamental problem with this whole model that we'll have fun talking about in a second.



JASON:  Absolutely.  All right.  So that's kind of a teaser forward, before we get there.



STEVE:  So MEGA is a very popular multiplatform, multi-client, encrypted cloud storage provider service.  Last Tuesday, on the 4th of September, MEGA's Chrome Web Store account was compromised and was used, that compromise was used to successfully host a malicious variant of the MEGA Chrome extension.  A hacker who goes by the handle Ser, S-E-R, Hack, SerHack, discovered the breach, and he wrote a long posting.  The TLDR is:  "On 4 September, 14:30 UTC, an unknown attacker managed to hack into MEGA's Google Chrome Web Store account and upload a malicious version 3.39.4 of MEGA's Chrome extension to the Web Store, according to a blog post published by the company.



"Upon installation or auto-update, the malicious extension asked for elevated permissions to access personal information, allowing it to steal login/register credentials from ANY websites like Amazon, Github, and Google, along with online wallets such as MyEtherWallet and MyMonero, and Idex.market cryptocurrency trading platform.  The trojanized MEGA extension then sent all the stolen information back to an attacker's server located at megaopac[.]host in Ukraine, which is then used by the attackers to log into the victims' accounts, and also to extract the cryptocurrency private keys to steal users' digital currencies."



MEGA themselves quoted essentially the same and then added, with a slightly more flattering tone, of course, since it was their system, they said:  "Four hours after the breach occurred, the trojaned extension was updated by MEGA with a clean version (3.39.5), auto-updating affected installations.  Google removed the extension from the Chrome Web Store five hours after the breach."  They said:  "You are only affected if you had the MEGA Chrome extension installed at the time of the incident, auto-update enabled and you accepted the additional permission request" - and of course users would because they don't know - "or if you freshly installed version 3.39.4," which was the trojanized one.



"Please note that if you visited," they wrote, "any site or made use of another extension that sends plaintext credentials through POST requests" - now, okay, note, that's what everybody does.  The connection is encrypted over TLS.  But the contents of the post is typically plaintext.  No one bothers to encrypt it in the browser before encrypting it under TLS.  So which is to say, I mean, again, they're trying to say, oh, well, there are ways in which you might not get bitten by this.  But no.  Everybody's going to get bitten by this if they logged in to any site.  Anyway, they say:  "...either by direct form submission or through a background XMLHttpRequest process," which is a standard XHR Ajax request process.  And they say:  "While the trojaned extension was active, consider that your credentials were compromised on these sites and/or applications."



So a four-hour Window during which time, if you were using Chrome and had the MEGA Chrome Extension, which would auto-update itself, and that happened, then until that was removed, that trojan was in your browser sending everything that you did back to this server in Ukraine.  Also at the time of the attack Monero's Twitter account posted:  "The official MEGA extension has been compromised and now includes functionality to steal your Monero," as indeed it did.



Also, Riccardo Spagni, who tweeted from his handle @Fluffypony on Twitter, said the previous owner of MyMonero - oh, he is the previous owner of MyMonero.  He tweeted:  "Confirmed that it also extracts private keys if you log into MyMonero and/or MyEtherWallet in a browser with this MEGA extension installed."  And then further security research confirmed that it could log any POST request where the URL contained specific strings including "login," "register," "sign in," and so forth.



Okay.  So we have the Chrome adding its own management of passwords.  And here we have an instance where that system collapsed during the time that this malicious extension was in place.  So of course this nicely highlights why, while Chrome's autofilling of user passwords with strong passwords is better than not encouraging that, it's still using the existing old model - and for that matter, all password managers do, too - of static one-way passwords which everything demonstrates is no long sufficient protection.



And I don't mean to make this a commercial about SQRL, although the future clearly belongs to a solution like SQRL, if not SQRL.  Doesn't necessarily have to be SQRL.  But what makes a system like SQRL different is that it assumes the presence of some computational capability at the user's end.  When you think about it, that's the difference.  Traditional username and password systems only assume some memory from the user, no computation.  And what Google has done with this, or what password managers do, they're not changing that.  They're not improving that.  They're enhancing the memory at the user end, but it's still just memory.



And of course the reason we're where we are is that this all began in a world where we were using terminals connected to mainframes; and the user at a terminal, a so-called "dumb terminal," they were called "dumb" for a reason, they would enter their username and their password, which they had memorized.  And then they would log into some mainframe at the other end of the wire.  Or later, when we had modems and we were dialing in, same thing.  There was just memory on the user's end.  Okay.  So everything changes, everything changes when you can provide a little bit, doesn't even need to be much, a little bit of computational power, when you can assume that at the user end.  And now we all have computational power because we're logging in with a computer to another computer.



So, for example, with SQRL or - and again, I don't want to make this a SQRL commercial, although SQRL has solved all of these problems in a way that is far superior to what anything else has done.  With SQRL, the server to which you're authenticating your identity knows your SQRL public key.  It has that on file.  And it doesn't even need to keep it secret, by the way.  That's what it has.  It has your public key.  And that's all it really needs.  When you wish to authenticate your identity to a website, that site sends you a challenge, that is, just some random junk, some gibberish, which it has never sent to anyone before, and it will never send to anyone again.  And that's easy to do these days with crypto stuff.  So just it sends you this stuff.



That bit of computational capacity at the user's end, which we all now have, not memory, computation, it responds to that challenge by signing it using the matching SQRL private key, and it returns the signed result.  The website then uses your public key to verify your signature, and it knows you're you, and no one else.  That's all there is to it.  And it changes everything.  It means that websites no longer have secrets to keep.  They can lose their database, and it doesn't matter.  And what essentially is a replay attack, if you just have memory, and the browser has stored your username and password, then the bad guy gets it.  And they replay that same memory, and they're able to authenticate as you.



What this does, by using a unique challenge and signing it, is you solved the replay attack problem.  Nobody who doesn't have your private key, which is very well protected, is able to impersonate you.  And even, and think about this, even somebody who can sniff your traffic, who sees what's going on, is unable to gain anything because the website you're authenticating to will never use the same challenge again, so the same signature can never be reused.  Anyway, we're like holding on for dear life to a system which is really broken, and which we know now how to fix.  And so I guess this is sort of a last gasp on Chrome's part.  I mean, to their credit, they are moving forward with other solutions.  But it's really time for us to get past this and recognize that, as long as users have some computation, we can completely solve this password problem in a way which is really robust.



So in any event, if you're a MEGA user, you know who you are.  Think about what was going on last Tuesday, a week ago.  That's when this happened.  And the advice from MEGA themselves is any site that you believed you touched and authenticated to during this time when you may have had the malicious extension installed in Chrome, you should absolutely change your username and password immediately.  Who knows how much data they collected during this period of time?  But I'm sure the folks in Ukraine are taking advantage of it.



JASON:  Do you think in MEGA kind of placing blame on Google's shoulders, do you think that's valid?



STEVE:  Well, yeah.  They were complaining that they're no longer able to sign their own code; that Google now does the signing of the code, and they're not able to.  It's unfortunate.  I don't know why they don't both sign it because there's nothing to prevent double signing of code, which could also work.  And we don't also know how this breach occurred.  So, you know, was it a weak password?  Was it some malware got into MEGA?  I mean, unanswered is how this, you know, what the underlying cause of this breach was because we certainly don't want to be in a position where Chrome browser extensions are, like happened with MEGA, are able to get subverted, or we'll be in real trouble.



JASON:  Absolutely.



STEVE:  So we're at Week 3 of the Advanced Local Procedure Call, the ALPC, zero-day.  Remember as we talked about two weeks ago, SandboxEscaper tweeted that she was sort of disenchanted with the world.  I don't remember her exact words, but she wasn't happy, and she didn't have an interest in submitting another vulnerability to Microsoft, so she just posted it to the world.



We now know, well, and then last week we continued the coverage because the micropatch had been created for somebody who wanted to deal with this immediately, although at the time I said with you, Jason, that I didn't think it was really that big a problem.  I mean, it was a problem if malware got in your computer because it could do more with it than if it didn't have a privilege elevation or escalation.  But still, I don't know that it would merit like a third-party patch to Windows.  That just seemed, eh, a little bit of a problem.  And we talked about how some other researchers had looked into the problem, noticed that it was easy to make it run under the 32-bit version of Win10, and also under Windows 7, without any effort at all.



Well, now we know, thanks to researchers at ESET, that within two days of the original disclosure tweet, this flaw was being used and actively exploited by a group they call the "PowerPool developers" because they use PowerShell-based malware.  These guys, they did not use the proof-of-concept code directly.  They understood what it was.  They made their own.  And they attacked, here again, Google Update, which is the official  Google Updater app which runs in Windows systems under admin privileges so that it's able to read or write any files on the system.



So what the privilege elevation allowed them was the ability to replace GoogleUpdate.exe with their own malicious version, which the Windows Task Scheduler would then start up as it has been programmed to by the Google Installer, and that would then run their malware, which would install a backdoor which they've been using as the second stage of their attacks.  And thanks to what ESET found, this privilege elevation, they were able to do this with full system privileges in order to modify something they shouldn't have been able to modify.



So this is what we're seeing now.  It's sort of an interesting evolution in the world, that is, the speed with which this moves.  We get an Apache Struts vulnerability.  Even if it's been patched, that patch delay, if it's more than a few days, if it's a public vulnerability, as we've seen, it's immediately attacked and jumped on.  And in fact we'll be talking about that, too, in a second.  Here, within two days, a zero-day that Microsoft has had no opportunity to patch is in use by malware.  If it gets into someone's machine, it's then able to get up to much more mischief than it would otherwise be able to.  So we're patching now monthly.



I remember thinking at the time, I mean, our listeners remember, before this notion of Patch Tuesday, for a while they were just ad hoc.  They were happening whenever Microsoft felt like doing it.  That caused the enterprise IT guys a lot of heartburn because these would happen anytime.  They needed to make sure that the patches weren't going to upset their enterprise networks and specific software that they were using.  So finally Microsoft said, okay, we'll just do these patches on the second Tuesday of every month.



Now we're in a world where, if a zero-day comes out a few days after a Patch Tuesday, unless Microsoft does, and they have in some cases where it's a really bad vulnerability, an out-of-cycle update, you could have a month go by before this gets fixed.  And there are situations like we talked about with MikroTik where, unless there's some means for these updates happening essentially autonomously, which is now what the whole world is getting used to with Google automatic updating itself, and Firefox is doing that, and of course Windows and all the OSes are moving in that direction, if they're not already there, that's the world we're in today is an automatic update cycle.



But what that means is that we also have very rapid communication of the presence of flaws.  When something is patched, it's reverse-engineered to figure out what the change was, even when it's kept secret; and the bad guys can now figure that out and develop an exploit before the patch, even though it exists, gets widely deployed.  So, wow, crazy world that we're in.



JASON:  Yeah.  They're going to have to turn to neural networks to solve this in a more rapid fashion.



STEVE:  I don't know what they're going to do.  I mean, of course that sort of sounds a little bit like heuristic approaches.  And we've seen that, like, AV tries to use behavior-based filters in order to stop things that they don't recognize based on their behavior.  But then those tend to false-positive.  You know, you end up with things saying, oh, you've got some malware in your system.  And it's like, no, I don't.  I know what this program is that you're saying is malware.  So not.



I'll just take a break here to bring up two things.  I got a tweet or saw a tweet from someone named Daniel, who tweeted as looks like @Gisleburt is his Twitter handle.  He was replying to @smolrobots.  And because he referred to @SGgrc, he said:  "Check out @SGgrc's SpinRite."  He says:  "It has brought many drives back from the dead, although you should probably replace it immediately even if you get your data back."  He says:  "It's not free, but there is a 100% money back guarantee."  Which of course is true.



And we've talked a lot about whether it's necessary to replace the drive after SpinRite repairs it or not.  If you want to err in the direction of caution, if it's easy to replace the drive, then yes.  But there are many cases where SpinRite is fixing a specific problem that is really fixed, rather than the drive itself indicating that it's dying, and SpinRite pulling it back from the grave for some length of time before it tries to die again.  So as we've said, unfortunately drives are way more analog than we wish they were.  So again, erring in the direction of caution, by all means replace the drive after SpinRite makes it readable again.  But it's not necessarily the case.  If you've been listening to this podcast long enough, you probably have enough tools in your arsenal to know.



And secondly, Jim Sanders, who's apparently in Irvine, California, he wrote, I saw an email from him, on the subject of "Degauss or Not Degauss."  And he said:  "Steve, I've managed to collect a stack of old spinning hard drives.  Normally, I would have DBAN on the drive prior to removing it from my computer.  But I'm now looking at a couple dozen needing the treatment."  He says:  "I've found my old handheld degausser and wondering, if I give the drive 30 seconds or so of treatment back and front, is this as good as DBAN?"  He says:  "I realize the magnetic field may render the controller useless, but that's fine by me.  What think ye?"



Okay.  So my answer is I don't think that's sufficiently good.  The problem is that magnetic fields fall off surprisingly fast.  I mean, you need to have a magnet virtually in contact with the surface of the drive in order for it to have any effect.  So if you're on the outside of the drive with a powerful degausser, although you would never want to do this to a drive whose data you cared about, I don't think it's sufficiently good to do it to a drive whose data you absolutely want to make sure you never get back.



So I really - I would take a drill, and I would just drill right through the drive casing several times.  DBAN is good.  Our listeners know that after I get 6.1, maybe .2 or .3, but at least 6.1 has to happen first, then it's my intention to create something specifically for this purpose, which will be extremely fast and extremely good, which will be known as "Beyond Recall," and it will do what we expect it would do, which is very quickly and securely wipe all the data from a drive.  Failing that, or until then, DBAN is a good system, although it is very slow.



I would just drill a hole through the drive a number of times.  That's really going to prevent the head from successfully flying over the drive.  But I think physically, I think you pretty much have to physically destroy the thing in order to be safe.  Being a few inches away, being on the other side of the drive case, I just don't think that's good enough.  You really, really need to have intimate contact with the surface of the disk.



JASON:  Take advantage of the reason or the fact that you can actually put a drill through it because that's got to be kind of satisfying; right?  Take a drill to your hard drive?



STEVE:  Yeah, well, and depending upon the age, some platters are glass now.  And if you smack it hard, if you then hear it, then, you know, smack it hard and then shake it.  And if you hear, like, "crinkle crinkle crinkle," you've done your job.



JASON:  Right.



STEVE:  You know that you had a glass platter, and it's no longer going to get any data back.



JASON:  It's no longer a platter.



STEVE:  But if you smack it hard and then don't hear that you've shattered a glass platter, then I think you need to drill some holes through it.



JASON:  Lucky you.  All right.  We've got a few stories to round this out.  And this is a really interesting one because it seems to bring, like, a certain payload to people who aren't very creative with their password management.



STEVE:  Well, yeah.  And I'm not sure what this means.  So here's the story.  A Czechoslovakian court recently sentenced two hackers to three years in prison for accessing Vodafone's customers' mobile accounts and using them to purchase about  $27,000 USD worth of gambling services.  According to Czech news reporting, the hackers accessed mobile customers' accounts by using the password, get this, "1234."  Once they gained access, they ordered new SIM cards that they picked up from various retail branches.  Since they knew the phone number and the simple password, 1234, they were able to install the SIM card in their phones without any other verification.  This in turn allowed attackers to charge over - and this $27,000 U.S. is 600,000 Czech Koruna, or approximately, as I said, $27,000 U.S. for gambling services.



Okay.  So, however, the plot thickens, since Vodafone is saying that it's the customer's fault for having weak passwords, which allowed their accounts to be taken over.  Vodafone is claiming that the fault is not theirs, but the fault lies with the customers who chose such weak passwords, and that the hacked customers with easy passwords should have to pay the stolen money back.  Some victims of the theft have reported that Vodafone has sent debt collectors to recover the money stolen by the hackers.  The victims, on the other hand, have stated, first of all, that they have no idea how their passwords were set to 1234 in the first place, and they also didn't know there was an online market that could be used to buy services that would be charged against their accounts.



Vodafone has stated, get this, that it may have been possible that one of their employees configured this password for the customer when the phone was purchased.  But even if so, the user should still have changed it to a stronger password.  Okay.  Except it turns out that the My Vodafone portal itself only allows four- to six-digit passwords.  So Vodafone doesn't have great security.  It's not allowing people to set a strong password, four to six digits, which we all know can easily be brute-forced.



So my take on this is that $27,000 cannot be a big deal compared to the reputation damage that Vodafone will suffer in blaming their own customers for hacks of their accounts, when by Vodafone's own admission Vodafone's employee may have set the password to 1234, and it was never changed by their customers.  So Vodafone should bite the loss, you know, say, "Okay, fine, it's on us," and then audit their customers' passwords, which they apparently can do, to make sure they're strong, and inform them that they need to change them to something else.



And the portal should disallow 1234 as a password, where if you try to put that in, it's trivial to have some JavaScript running on the page, even if you're going to hash it, and I would imagine they're not even bothering to do that.  They could just disallow it, like, right off the bat and say, no, you cannot use 1234, and check for, like, 4321 and 1111 and so forth.  But mostly fix it so that you're not allowing short decimal passwords.  That's just nuts.  Wow.



JASON:  Yeah, that really is crazy.  That's exactly what I was thinking before.  And it's like, if you have these rules, like build it into the tool because we already know people are not very adept at passwords.  And path of least resistance is commonly the approach that's taken with a number of people and passwords.  So don't allow them to take the easy way, and put those barriers in place, especially if you're going to just charge them if something bad happens.  That's just bad business.



STEVE:  Well, and also probably the focus is the phone; right?  So you buy a phone, it comes with a My Vodafone portal, which you may never even use.  You may know that it's there.  So the employee sets it up as a password 1234, and may have even said, okay, I've set the password up on your My Vodafone portal to 1234.  Make sure you change it.  But they have the phone.  The phone is what they want.  They probably maybe never even logged in.  And the point of this is, when we're using a web service, like online banking, it's the service that we're trying to secure.  So there, your authentication to the portal makes sense that you would want that to be secure.  Whereas someone getting a phone, they're just thinking about their phone.  They want the phone to work.  They may never even log into their portal.



So,  yeah.  I mean, although the interesting precedent and the worry that this sort of suggests is are we going to see more of this kind of behavior from other people besides Vodafone?  Traditionally, when a user has lost money due to their account being hacked because they used "monkey" as their password, and their debit card was grabbed, and their banking account was emptied, it's like, oh, too bad, it's on you; right?  So here we're seeing exactly the same model, but the user is running up a bill on their account that they're refusing to pay, and Vodafone is saying, no, your account got hacked, you're responsible for the fact that bad guys got a SIM which they hooked to your phone number, thus to your account, and then ran up gambling charges.  You don't like the bill that you received, but we're going to send debt collectors after you in order to collect it.



So I don't know.  We've often talked about how weird it is that in software the user accepts all the blame for whatever happens, even if the system hurts them.  It'll be interesting to see whether legislation begins to take the position of corporations that are trying to hold users responsible.



And speaking of which, Tesla gets hacking exactly right.  They updated their formal product security page and eliminated all doubt.  I've said on the podcast, our listeners have heard me say it over and over, that we absolutely have to hold valid, legitimate, non-hacking, white hat researchers unresponsible, I mean, not legally responsible.  Do not sue the guys who are trying to improve your product.  We've talked about a lot of stories in the past where that was done.  So Tesla updated their pages.



They said, under product security:  "Tesla values the work done by security researchers in improving the security of our products and service offerings.  We are committed to working with this community to verify, reproduce, and respond to legitimate reported vulnerabilities.  We encourage the community to participate in our responsible reporting process."  Bravo.  "To register as a pre-approved, good-faith security researcher and register a vehicle as a research-registered vehicle, please submit requests to VulnerabilityReporting@tesla.com."



They said, for vehicle or product related services:  "While we use Bugcrowd as a platform for rewarding all issues, please report vehicle and product-related issues directly to vulnerability@teslamotors.com, using our GPG key to encrypt reports containing sensitive information."



They said, under third-party bugs:  "If issues reported to our bug bounty program affect a third-party library, external project, or another vendor, Tesla reserves the right to forward details of the issue to that party without further discussion with the researcher.  We will do our best to coordinate and communicate with researchers through this process."



And finally, and here it is.  Actually, there's this and, yeah, their responsible disclosure guidelines:  "We will investigate legitimate reports and make every effort to quickly correct any vulnerability.  To encourage responsible reporting, we will not take legal action against you, nor ask law enforcement to investigate you, provided you comply with the following Responsible Disclosure Guidelines."



Here they are:  "Provide details of the vulnerability, including information needed to reproduce and validate the vulnerability and a Proof of Concept.  Make a good faith effort to avoid privacy violations, destruction of data, and interruption or degradation of our services.  Do not modify or access data that does not belong to you.  Give Tesla a reasonable time to correct the issue before making any information public.  Alter only vehicles that you own or have permission to access.  Do not compromise the safety of the vehicle or expose others to an unsafe condition.  Security research is limited to the security mechanisms of the Infotainment binaries, Gateway binaries, and Autopilot ECU binaries."



And then they said, for the avoidance of doubt, and I really love that, I mean, they like absolutely want to clarify this.  "If, through your good-faith security research, you (a pre-approved, good-faith security researcher) cause a software issue that requires your research-registered vehicle to be updated or reflashed, as an act of goodwill Tesla shall make reasonable efforts to update or reflash Tesla software on the research-registered vehicle by over-the-air update, offering assistance at a service center to restore the vehicle's software using our standard service tools, or other actions we deem appropriate.



"Tesla has complete discretion as to the software or other assistance that we will provide, and it may be only a limited number of times.  Tesla's support does not extend to any out-of-pocket expenses, for example towing, incurred by you.  Tesla reserves the right to limit the number of service requests per pre-approved, good-faith researcher, and unregister a research-registered vehicle at any time."



So in other words, if you brick your Tesla, and you're behaving well, you demonstrate good faith, you're registered, some of your research caused your car to die, they'll fix it for you.  They're not saying they're going to always fix it for you.  They're not going to fix it a hundred times if you keep doing it.  On the other hand, if you keep finding problems, I imagine they're going to say, oh, okay, find some more.



But still, they said:  "Tesla considers that a pre-approved, good-faith security researcher who complies with this policy to access a computer on a research-registered vehicle has not accessed a computer without authorization or exceeded authorized access under the Computer Fraud and Abuse Act (CFAA).  Tesla will not bring a copyright infringement claim under the Digital Millennium Copyright Act" - the infamous DMCA - "against a pre-approved, good-faith security researcher who circumvents security mechanisms, so long as the researcher does not access any other code or binaries.  Tesla will not consider software changes, as a result of good-faith security research performed by a good-faith security researcher, to a security-registered vehicle to void the vehicle warranty of the security-registered vehicle, notwithstanding that any damage to the car resulting from any software modifications will not be covered by Tesla under the vehicle warranty."



So bravo to Tesla.  They clearly explained that researchers will be held harmless when they respond in a responsible disclosure fashion.  And I can't imagine anybody could have a complaint with that.  And it turns out this is in the nick of time because today Wired magazine reported that yesterday hackers revealed at the Cryptographic Hardware and Embedded Systems conference in Amsterdam that they had found a way around the original encryption used in the key fobs of Tesla Model S cars, using a few hundred dollars in radio and computing equipment and some one-time preparation, which I'll explain in a second.



They are now able to wirelessly read signals from a nearby Tesla owner's fob, spend less than two seconds - actually it's 1.6 seconds - of computation to yield the fob's cryptographic key and essentially completely clone the security; to defeat the security, unlock the doors, and drive away.  Lennert Wouters, who is one of the KU Leuven researchers, at the KU Leuven University in Belgium, yesterday said:  "Today it's very easy for us to clone these key fobs in a matter of seconds.  We can completely impersonate the key fob and open and drive the vehicle."



So:  "Two weeks ago Tesla rolled out new antitheft features for the Model S that include the ability to set a PIN code that someone must enter on the dashboard display to drive the car.  Tesla also says that Model S units sold after June of this year [so new ones] are not vulnerable to the attack, due to upgraded key fob encryption that has already been implemented in response," so in response to the disclosed responsibly to Tesla ahead of time KU Leuven research.  "But if owners of a Model S manufactured before then" - and that's Leo, I think; right?  I think Leo has a Model S; right?



JASON:  I think he had the Model X.



STEVE:  Oh, the X, okay.  If the "owners of a Model S manufactured before don't turn on that PIN, or don't pay to replace their key fob with a more strongly encrypted version," and that's certainly what I would do, "the researchers say they're still vulnerable to their key-cloning method."  In other words, everybody needs to know you can put a PIN on, or you can pay to get upgraded encrypted key fobs.  But if you don't do that on a Model S purchased before June of 2018, you're vulnerable.



So what do we know?  Like most automotive keyless entry systems, Tesla Model S key fobs send an encrypted code, based on a secret cryptographic key, to a car's radios to trigger it to unlock and disable its immobilizer, allowing the car's engine to start.  After nine months of on-and-off reverse engineering work - apparently they didn't work on it full-time, they just worked on it when they had time - this research team discovered last summer, that is, the summer of 2017, that the Model S keyless entry system, built by a manufacturer named Pektron, P-E-K-T-R-O-N, used only a weak 40-bit cipher to encrypt the key fob codes.



And the problem is a 40-bit key just doesn't have enough combinations.  It's just too easy to brute-force it.  The researchers found that once they gained two codes from any given key fob, they could brute-force every possible candidate cryptographic key until they found the one that unlocked the car.  So two codes from the fob, and then computation.  That narrowed down the universe of possible codes because there are only 2^40 of them, that they were able to unlock the car.  Then, having accomplished that first piece of research, that taught them exactly what was going on.



So they then precomputed all possible keys for any combination of code pairs, which created a massive 6TB lookup table of pre-computed keys.  Armed with that table, any two codes received from any new key allowed them within 1.6 seconds, a little bit of computation, and then a lookup using their 6TB lookup table, which of course these days 6TB is portable, allowed them to determine the key of the fob in 1.6 seconds, and it allowed them to clone it.



So responsibly disclosed to Tesla.  Tesla made the changes, began selling them, updated the firmware in the cars to add a PIN, which would defeat this attack for non-upgraded systems, and then the researchers disclosed it all yesterday at the conference in Amsterdam.  So that's the way security is supposed to work.  What of course this also says is that users are responsible for setting a PIN, or paying to get an updated fob, or recognizing that if they happen to be a target of an attack, again, this is one of those things where, once we know it can be done, it's probably - and I haven't looked at the report to see how much detail they went into.  But I imagine, given the typical security report of what they did, all the details are available now publicly for someone to easily retrace their footsteps.  So Model S owners definitely want to take heed.



JASON:  I've always been a little nervous about key fobs just in general, but I'm sure they're a lot more safe than...



STEVE:  I don't have one.  I don't have one on my car.  I'm old school.  So, yeah.



JASON:  I can understand that.  I guess there's always the, what is it, the clothes hanger, you know.  The analog solution still exists, too.



STEVE:  If you have buttons that you can hook around.



JASON:  That's true.



STEVE:  Fortunately, my car has smooth buttons.  In fact, I don't even know if they work.  They're just sort of a visual indicator of whether the door is locked or unlocked.  I don't think I've ever tried pushing down on it, come to think of it.  It's amazing.



Okay.  So we talked about PCs becoming a battleground.  We also talked about this latest Apache Struts 2 vulnerability, which allowed a single-URL query to a vulnerable Apache Struts web server to compromise and take over the system.  Thanks to research by F5 Labs, we now know they have caught this being exploited in the wild, and in at least one case have a complete breakdown of what's going on.  There is a Monero cryptomining campaign which is attacking Apache Struts 2 vulnerable systems. They named this the CroniX, C-R-O-N-I-X, because it uses the cron service in Linux for persistence and the Xhide capability for launching executables with fake process names.  So because of cron and Xhide they named it CroniX.



The main target appears to be Linux systems, but there is some involvement of Windows because it also has a Visual Basic component.  So even though they are largely seeing Linux being attacked, it looks like the bad guys gave some thought to also attacking Windows.  It uses a single URL which is able to embed something known as OGNL, which is the Object-Graph Navigation Language, which is supported by these servers.



And I have in the show notes a screenshot of the query.  It's a GET query, which is to say it's a standard URL which is crazy, but you can see in there cmd= and then 'wget and then a user-agent, looks like it's just "l," and then a few other parameters, and then the URL.  And it's using wget to grab update.sh and then piping it into bash.  So it's grabbing this update.sh shell code and then executing it on the system in order to take it to the next level.  And then bash gets routed to dev/null, so it just goes into hyperspace.  And then in the screen shot you can see a lot of other stuff going on.



So the attacker sets the number of so-called "huge pages" in memory to 128.  And the F5 guys say this is the first clue that the attacker's intention could be a mining operation, as this step is probably related to improving mining performance.  It then sets up a number of cron jobs for malware persistence.  And again, it primarily looks like it's targeting Linux servers, although there is a Windows component.



So what they found most interesting is that it is the most aggressive anti-competition malware that they have seen.  I mean, it's not uncommon for malware to boot other malware out, and especially mining malware, because of course they want all of the system's CPU resources for their own mining.  They don't want to share the CPU with some other guy who's also trying to mine cryptocurrency on the same machine.  So they go in, and they kill a bunch of processes by name of known malware.  But then, because some mining malware is known to use legitimate process names, they watch the system's processes run and selectively kill off processes utilizing 60% or more CPU resources.



Specifically crond, sshd, and syslogs are the processes which they've seen other malware masquerading under.  But when you see something like the cron daemon or ssh daemon using 60% of the system, that's going to be unusual.  So very likely other malware has gotten in ahead of it.  So it goes and kills them and then deletes the binaries from the system in order to set up shop and essentially take over.  And once it's all settled in, it mines Monero using a mining pool at eu.minerpool.pw and uses as much of the resources of the system as it can get, staying in the system as long as possible.  And remember, unlike the first Apache Struts problem earlier this year, which affected a huge number of servers, this one, because it requires some non-default settings, it's probably going to be less prevalent, but it has been found actively pursuing systems in the wild.



So it's certainly not without concern.  And as we said, it's good at this point that all these guys are doing is wanting to set up cryptomining operations rather than wondering whose network they are in and turning around and taking a look at it.  Unfortunately, as we said earlier in this podcast, it's not the case that the MikroTik router folks are so lucky.



JASON:  And that's why it's probably only a matter of time anyways.



STEVE:  Yes, exactly.  And I'm a big believer or a big fan of OpenVPN.  It is open source.  It is a robust solution.  I just think it is, rather than using some third-party solution, it's like these guys are on it.  They are being very careful as they evolve it.  And this problem is solved.  You want to use UDP as the transport because TCP doesn't like being used as the carrier for TCP as the protocol.  You want a non-reliable protocol to carry the reliable protocol because most of us are going to be using TCP conversations.  That wants to be encapsulated in a UDP tunnel.  OpenVPN does both.  UDP will give you better performance.



The problem is that - and I don't know what the OpenVPN guys will do.  I hope they step up to address this.  There are instances where you can add too many features.  And I would argue OpenVPN probably did.  In the OpenVPN docs, under "Using Alternative Authentication Methods," they said:  "OpenVPN 2.0 and later include a feature that allows the OpenVPN server to securely obtain a username and password from a connecting client, and to use that information as a basis for authenticating the client."



And I should mention Leo and I have talked about this before.  We use certificates because that's belt and suspenders.  That's better than just username and passwords.  But they go on, saying:  "To use this authentication method, first add the auth-user-pass directive to the client configuration.  It will direct the OpenVPN client [at the user end] to query the user for a username/password, passing it on to the server over the secure TLS channel."  So that's cool.  They're establishing a secure channel.  The config file prompts the user for the username and password if auth-user-pass directive is present in the client configuration.



Then they said:  "Next, configure the server to use an authentication plugin, which may be a script, a shared object, or DLL.  The OpenVPN server will call the plugin every time a VPN client tries to connect, passing it the username/password entered on the client.  The authentication plugin can control whether or not the OpenVPN server allows the client to connect by returning a failure or success value."



So here's the problem.  The plugin is specified by a user-writeable configuration file; and the OpenVPN service, which runs under the system account, loads and executes the optional  authentication plugin within its own process and privileges.  In other words, we have a breach of security boundary as a consequence of the fact that the config file can be changed by someone with non-system privilege, but you can use it to specify a DLL which will get loaded and run by the server with system privileges.



So we know that that's a recipe for disaster.  It turns out that both the ProtonVPN and the NordVPN both based on OpenVPN, for which I salute them, were found to be vulnerable to exploitation of this trick.  They quickly patched it.  But then security researchers looked more closely and realized that, other than just this authentication plugin, also the other commands which were valid for the OpenVPN config, "script-security," "up," and "down," could also be used as command vectors.  So they did another round of patches and updated.



So the good news is I don't know where other OpenVPN systems stand, but they're going to want to make sure that they're patched and updated with the revelation from this.  It is the case that ProtonVPN and NordVPN have been notified, fixed it, were notified that wasn't a full fix.  They've updated again.  So if you are, to our listeners, if you're a ProtonVPN or NordVPN user, you're going to want to make sure that you have updated yourself to the latest version of those systems so that you're safe.



And one last little piece of sort of fun here.  Trend Micro got themselves in trouble.  They're in the doghouse.  Yesterday, in response to a firestorm of controversy which erupted over the weekend, Trend Micro posted under - their posting was "Answers to Your Questions on Our Apps in the Mac App Store."  They began:  "Reports that Trend Micro is 'stealing user data' and selling them to an unidentified server in China are absolutely false."  Then ZDNet, in covering this, their headline starts with:  "Trend Micro says sorry after apps grabbed Mac browser history.  The company has now removed a browser history data collection feature from its macOS products."



Okay.  So what happened?  We know Trend Micro.  They're a well-known security firm.  They've apologized after it turns out, and they've admitted, that several of their consumer macOS antimalware products and non-antimalware utilities were discovered to be capturing the user's browser history data and sending it to a remote server.  Turns out the remote server, it's not exactly clear where this China idea came from.  Apparently it's an AWS server based in the U.S.  The Trend Micro apps which have been removed include Dr. Cleaner, Dr. Cleaner Pro, Dr. Antivirus, and Dr. Unarchiver.  So Dr. Unarchiver, why does that have anything to do with snapping browser history?



So what Trend Micro explains is, they said:  "Trend Micro has completed an initial investigation of a privacy concern related to some of its macOS consumer products.  The results confirm that Dr. Cleaner, Dr. Cleaner Pro, Dr. Antivirus, Dr. Unarchiver, Dr. Battery, and Dr." - oh, I'm sorry, not Dr. Duplicate Finder, just Duplicate Finder.  I guess they didn't get their doctorate on duplicate finding - "collected and uploaded a small snapshot" - this is Trend Micro saying "a small snapshot of the browser history on a one-time basis, covering the 24 hours prior to installation.



"This was a one-time data collection, done for security purposes, to analyze whether a user had recently encountered adware or other threats, and to improve the product and service."  Huh.  Like maybe trying to determine why you were installing Trend?  I don't know.  One of their doctors?  Anyway:  "The potential collection and use of browser history data was explicitly [I love this] was explicitly disclosed in the applicable EULAs and data collection disclosures accepted by users for each product at installation."  And then they say:  "See, for example, the Dr. Cleaner data collection disclosure here," and then a link, which I have in the show notes if anyone's interested.



"The browser history data was uploaded to a U.S.-based server hosted by AWS and managed and controlled by Trend Micro.  Trend Micro is taking customer concerns seriously and has decided to remove this browser collection capability from the products at issue."  And of course why were Dr. Unarchiver and Dr. Battery collecting browser history?  And what I really wonder is if those EULAs also made this data disclosure, which they claim was being accepted by users.



Anyway, what they've said was:  "We have learned that browser collection functionality was designed in, common across a few of our applications, and then deployed the same way for both security-oriented as well as non-security-oriented apps such as the ones in discussion.  This," they say, "has been corrected."  So, okay.  They use some sort of common library.  Maybe it was part of the install process.  Who knows?  Which is why the Dr. Unarchiver and the Dr. Battery were also collecting browser history.



Anyway, they've all been booted from the macOS store.  I imagine they'll get this fixed and then probably come creeping back in and hopefully do a better job in the future.  So for what it's worth, if you're wondering where they went, that's where and why.  Not a big deal, probably.  But still it's good that we've got, you know, thank goodness we've got people looking, watching this, catching this behavior.  I love that we have an industry full of researchers who are being third-parties who are looking at this stuff, and that we have an environment that doesn't prosecute third parties for saying, I don't think you meant to be doing this on your battery app.



JASON:  Yeah, that's true.  Everybody in chat is pointing out maybe if the software has "Dr." in the name, maybe that's a good indication to look elsewhere.  Maybe it's making promises that it can't quite hold up to.  And like that, it's also like Cleaner, I know on Android there's a bunch of Cleaner apps.  And oftentimes with those there's other stuff happening.



STEVE:  They're kind of low end, yeah.



JASON:  Yeah.  Yeah, totally.  So anyways, I've never actually known if Trend Micro was like a brand that consumers could actually trust.  They were just a brand that was always kind of on my radar, I think, personally.



STEVE:  Yeah, yeah.  They don't seem like a high-end brand.



JASON:  I wouldn't say so.  Steve, you've done it again.  I think you've covered, well, you've covered a good chunk of the security news.  I'm sure there were other stories out there, but they obviously did not rise to the top this week.



STEVE:  Well, and there were some that were breaking as we were going to press.  So I've already got a few lined up for next week's coverage.  So fear not.  If there's something you think I missed, I'll probably be talking about it next week.



JASON:  GRC.com, if you want to check in on all the things Steve offers and is keeping track of.  Obviously you talked a little bit earlier about SpinRite, the best hard drive recovery and maintenance tool.  You can go to GRC.com and get your copy there.



You also talked about SQRL today, which by the way, it's your show.  You can talk about SQRL anytime you ever want, seriously.  And it's important to know about.  You can find information on SQRL by going to GRC.com as well, along with audio and video of the show.  There's transcripts.  That's the only place you can go to find transcripts of this show.  You can find it there at GRC.com.  Steve, really appreciate you letting me tag along this week.  Thank you.



STEVE:  Hey, a pleasure.  And we will do our third and final show together next week.



JASON:  That's right.



STEVE:  Looking forward to it, Jason.



JASON:  That's right.  And in the meantime, TWiT.tv/sn for all of the previous episodes to subscribe everything.  And of course next week you're talking about Tuesday, that's 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC.  And that's it for this week.  I'm Jason Howell.  Steve Gibson, thank you again.  We'll see you all next week on another episode of Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#681

DATE:		September 18, 2018

TITLE:		The Browser Extension Ecosystem

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.GRC.com/sn/SN-681.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we prepare for the first-ever Presidential Alert unblockable nationwide text message.  We examine Chrome's temporary "www" removal reversal, check out Comodo's somewhat unsavory marketing, discuss a forthcoming solution to BGP hijacking, examine California's forthcoming IoT legislation, deal with the return of Cold Boot attacks, choose not to click on a link that promptly crashes any Safari OS, congratulate Twitter on adding some auditing, check in on the Mirai Botnet's steady evolution, look at the past year's explosion in DDoS number and size, and note another new annoyance brought to us by Windows 10.  Then we take a look at the state of the quietly evolving web browser extension ecosystem.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  This week we talk about a lot of things.  I'm Jason Howell filling in for Leo.  We talk about the Presidential Alert that's coming up.  It's been postponed, but it's right around the corner.  Also Chrome's temporary www removal reversal.  There's a link that promptly crashes Safari OS, so you don't want to click that link.  And Steve talks about the state of the web browser extension ecosystem and some of the bad habits that we've gotten into over the years.  All that and more, coming up next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 681, recorded Tuesday, September 18th, 2018:  The Browser Extension Ecosystem.



It's time for Security Now!.  This is the show where we talk about all the latest security news, the happenings, everything security related on this very show.  I'm Jason Howell, filling in for Leo Laporte, joined by none other than Steve Gibson.  How you doing, Steve?



STEVE GIBSON:  And you are at this point, Jason, a fully trained-up co-host for Security Now!. 



JASON:  Oh, yeah.  In fact, if you want to take the week off, I got this.  I will just interpret your notes as best as I possibly can.



STEVE:  Well, so this is your third and final week co-hosting while Leo's on vacation.



JASON:  That's right.



STEVE:  And I think we'll end up with a bang here.



JASON:  Excellent.



STEVE:  So this is #681, which I titled "The Web Browser Extension Ecosystem" after some news of a Firefox change in two major releases from now sort of got me thinking about, like, we've talked about - you and I have in the last couple weeks talked about browser extension security issues where malicious extensions get into people's machines.  So I did a little bit of poking around.  And I was a little, I mean, even more than a little surprised by the number of extensions the majority of people have.



And in fact this is the chart that is this week's Picture of the Week.  It shows a histogram by number of browser extensions.  And oh, my god, even at the tail, it's like, you're kidding me.  Somebody, like there's a non-zero number of people who have 69 browser extensions.  I don't know how that thing even gets off the ground with 69 extensions.  But believe it or not, the peak of this histogram looks like, what is it, that thick bar, it's like 13 to 14...



JASON:  You have 15, somewhere around there.



STEVE:  Yeah, it looks like, yeah.  So that is to say the most people have 13, 14, 15 individual browser extensions.  And it doesn't drop by - it's not like it's a super sharp peak.  And we're talking from I think they surveyed several hundred thousand.  Anyway, we will get to it at the end of the podcast.  But I wanted to sort of step back and take a look at what has happened to the whole idea of extending the browser and what it means because it's funny, too, because in the research I ran across references to that old Conduit extension that was so nasty and was infecting people's machines.  And just, I mean, they've been a problem.



And in fact even now, what is it, I guess it's when I'm updating Java on a machine where I need to have Java because of other client stuff that I'm doing, they're still trying to slip an Ask toolbar or something in, you know, the little "install this because it'll improve your life."  It's like, ugh, thank goodness I saw that it was still checked, so I was able to turn it off.  And I've missed it a few times and had to go back and take it out.



JASON:  Yeah, you've got to be really careful with those.  It's super easy for those to just kind of slip right through.



STEVE:  Yes.  And I'm sure that some of us who are in charge of keeping other people's machines alive have sometimes gone to a machine we haven't seen for a while, and half of the vertical height of the browser's page space is bars.  It's like toolbars running.  It's like, how do you even see the Internet through the little...



JASON:  But it's useful, Steve.  Just think of how productive you are when all of those features are always in front of you.



STEVE:  Oh.  And always running, always having to get going whenever your browser loads and so forth.  Anyway, we'll have fun talking about that at the end of the podcast, which is titled "The Browser Extension Ecosystem" because it has become that.  And we'll talk about even extensions fighting each other for dominance and kicking each other out, which also happens.  But anyway, we have a lot of other stuff to talk about.  We need to prepare.  It has been postponed.  It was going to be day after tomorrow.  But the recent hurricane caused FEMA to decide, let's bump this back.  But we are still going to have the first-ever Presidential Alert, which is an unblockable, nationwide - if you have a phone, and it's on, you will get a Presidential Alert a couple of weeks from now, a text message.



It's funny, too, because the last time I was up there it was - I think it was Christmas before last.  It was when Leo's plan at the time was to have a couple hosts join him for an end-of-the-year kind of a holiday, I don't know if it was TWiT.  But so I think it was two years ago, so that we knew that President Trump was - he had been elected.  He was going to become our President.  And somehow this topic, I mean, the idea that there was going to be the ability for the President to simultaneously send everybody in the U.S. a text message that they could not block was discussed during that holiday event.  And so anyway, it's happening.  It's two weeks and one day from now.  But anyway, I think I pretty much already covered that topic, so we may skip that when we get to it.



But we've also got to talk about Chrome's temporary www double removal reversal that we talked about last week, that suddenly www was disappearing.  And now it turns out some people said no, it isn't.  Well, it turns out they had changed their mind very quickly, but only temporarily.  We'll cover that.  We've got Comodo's somewhat unsavory marketing, which was brought to my attention by one of our listeners who tweeted a note to me.  We're going to discuss a forthcoming solution to the problem of BGP, the Border Gateway Protocol, hijacking, which we've discussed extensively previously, that is, the hijacking problem, not the solution.



California recently passed some legislation waiting for the governor's approval or signature for IoT security, which is kind of a mixed blessing because of course it's government.  We'll deal with the return of cold boot attacks.  Oh, and also take a look at this, click on it, and your iOS device crashes.  I tried it, and it works.



JASON:  Oh, another one of those.



STEVE:  Both in 11 and in my freshly updated iOS 12.  So Apple didn't get it fixed.  We also have - I want to congratulate Twitter on adding some very welcome auditing.  It's sort of basically catching up with what other people have been doing.  We're going to check in with the Mirai botnet's steady evolution, look at the past year's explosion in DDoS number and size.  Also note a new annoyance brought to us by Windows 10.  And then we actually have an interesting bit of errata and some miscellany. Then we're going to take a look at the state of the quietly evolving, but boy, it's really become something, web browser extension ecosystem.  So I think you'll have a good sendoff, Jason.



JASON:  Absolutely, and I've already counted the number of extensions, at least on my Chrome OS device here, my Pixelbook.  And it's, like, right there at the peak of that graph.  It's like 14.



STEVE:  Interesting.



JASON:  So I'm right there with everybody else.  And I even tried to trim that down a couple of weeks ago.



STEVE:  In fact, it's interesting that basically what you have is a battery-powered browser.  I mean, essentially Chrome OS is, as we know, it's just basically a browser.  Because, I mean, the browser has become, as we've often said on this podcast, our portal to the 'Net.  And with web-based apps and the increasing efficiency of code running within the browser's control, that only looks to expand in the future.  So, interesting.  I'm down at, like, five, I think.  But on the other hand, this system has recently been set up.  So I haven't had a chance to accrue any barnacles yet.



JASON:  Yeah, yeah.  Barnacles, it's a good way to put it.  Maybe we should change it to web barnacles.  I think that's probably more appropriate.  All right.  So, yes, we've got the Presidential Alert that was supposed to happen, I think two days from now; right?  But that's been pushed back.



STEVE:  Correct.  Correct.  It was going to be Thursday, but what was it, Hurricane Florence, I think that was her name, we're still recovering from that.  FEMA's busy, didn't want to, like - well, and I think they probably worried that, if somebody got something that says "Presidential Alert" on their phone while a lot of people were literally under water, that could be a problem.  So anyway, so it's been pushed back to October 3rd.



Okay.  So we have two things.  There's the national EAS, which is the Emergency Alert System, which I don't know about you, Jason, but it just drives me nuts when I get this [mimicking loud alert sound], oh my god, on my cable, like switches over and everything stops, and it's just like, okay, thank you.  And they say it's monthly, but it seems like it's a lot more often than that.  So that's existed for a long time.  What hasn't - oh, and it's been tested, I had it in the notes, three times previously.  Oh, yeah, there it is, on November 2011, September 2016, and September 2017, an emergency alert system different from what we're getting on our cable boxes.  What's new is called WEA, which is Wireless Emergency Alerts, which will be tested for the first time beginning at 2:18.  How they chose that time I have no idea.  Not 17, not 19.  No, we're going to do this at 2:18 p.m. Eastern Daylight Time.



JASON:  There must have been a reason.



STEVE:  On October 3rd.  So who knows?  So what does that make it?  That's Pacific Time, that's 11:18 a.m. for us on the West Coast, 2:18 p.m. on the East Coast in the U.S.  And so what happens is, starting at that time, for approximately 30 minutes, every cell tower in the United States that's hosted by participating wireless carriers, and more than 100 carriers are participating, I mean, they didn't say every one.  But probably, I mean, it has to be all of them.  And so what will happen is our phones will give us a text message with the header reading "Presidential Alert" and text that reads, first four words, all caps, "THIS IS A TEST," which is a good way to start, "of the National Wireless Emergency Alert System.  No action is needed."  And we are told that the phones, first of all, all of our phones have this capability built in, which has never been exercised.



So the assumption is that we'll get, during the 30-minute continuous broadcast, the phone will only give us one text message, let's hope, and that all phones within range will receive that one text message.  So anyway, I'm sure we'll talk about it in two weeks because two weeks from now it will be the day following that podcast, being a Wednesday.  So anyway, it was supposed to be in two days.  I had it in my notes.  I double-checked.  They moved it.  It's like, oh, well, okay, we'll talk about it anyway and then remind everybody in two weeks.



JASON:  I know these things are important, and I know that it's important to set it up.  I will point out that on - this is Android Pie.  And there is a - I don't know if there's any sort of over-the-shoulder camera, so I'll just kind of show you from a distance.  There is an Emergency Alerts section in Settings where you can toggle on or off the different types of settings, the different types of alerts.  So Amber alerts, extreme threats, severe threats.  I have an entry there for required monthly test, and I have that off, so I don't get the monthly test ones.  So maybe check on your phone and see if there's a way to, like, switch that one off.



STEVE:  Wow.



JASON:  But there is nothing in here where you could turn this particular warning off.  The presidential one guaranteed comes through, from what I understand.



STEVE:  Correct.  And, I mean, really, in our wired world today, if there were something that was, like...



JASON:  Major.



STEVE:  ...of absolute national consequence, we want something like this.  And given that everybody is walking around with cell phones, that's the way to get a message out to everyone.  I mean, you have to be very, very, very careful with how you handle it.  I mean, we know what happened in Hawaii.



JASON:  Yeah, that's what I was going to mention.



STEVE:  When the guy pressed the wrong button, and Hawaii thought they were under attack.  So it's like, okay, let's...



JASON:  I mean, could you even imagine what that must have been like, to get that message and not get any sort of an update to it for, like, 30 minutes, an hour.  I can't remember how long it took, but people for a very considerable amount of time assumed that it was real.



STEVE:  They thought missiles were in the air.  And it's like, oh, goodness.



JASON:  Yeah.  Craziness.



STEVE:  Okay.  So Chrome 69.  To www or not to www?  Well, so it's back, kinda.  The Chromium blog wrote:  "In Chrome M69 we rolled out a change to hide special-case subdomains" - okay.  First of all, okay, special-case subdomains?  So www is a special-case subdomain?  Okay, notice they're no longer calling it "trivial."  I think they got slapped pretty hard because they originally said, oh, these are trivial.  Okay.  So now they're special case.  Many people argue that.  But anyway, so what they said was "...to hide special case subdomains 'www' and 'm' in the Chrome Omnibox," which is what they call their multifunction URL field.  "After receiving community feedback about these changes, we have decided to roll back these changes in M69 on Chrome for Desktop and Android."



Now, I'll just pause here to say that I absolutely verified, as I mentioned last week, that this was all working for me prior to the podcast.  Several people subsequently said, "What are you talking about?"  And now for me indeed it is no longer working.  So they reached out and tweaked my Chrome.  And I don't know if the other people had already been tweaked or never got tweaked or untweaked, I don't know.  But anyway, so it's gone for now.  



However, they continue:  "In M70" - so that's the next major release - "we plan to reship an adjusted version.  We will elide [as they put it] 'www' but not 'm.'  We are not going to elide 'm' in M70 because we found large sites that have [what they call] a user-controlled 'm' subdomain."  Okay, and I thought, what?  Okay, so the point being that the user can decide if they want the mobile version or the desktop version by choosing or not choosing to prefix the domain with an "m dot."  And so they learned the hard way - because of course they immediately just, first of all, they just got rid of it, it's like, ooh, ouch, okay - that that should be user controlled.  Then they continue, saying:  "There is more community consensus that sites should not allow the 'www' subdomain to be user controlled."  And it's like, okay.  They're on the peninsula.  What are they smoking?  I don't get this.  But believe me, a lot of people don't, either.



They continue, saying:  "We plan to initiate a public standardization discussion with the appropriate standards bodies to explicitly reserve 'www' or 'm' as special case subdomains under the hood.  We do not plan to standardize how browsers should treat these special cases in their UI."  Huh?  Okay.  "We plan to revisit the 'm' subdomain at a later date, after having an opportunity to discuss further with the community.  Please leave feedback" - oh, did they get some - "or let us know about sites that have user-controlled subdomains at 'www' or 'm' in this bug."  Because, you know, this is their Chromium bug list is where this all lives.



So, okay.  And on top of this, remember that I briefly noted last week that Chrome 69's initial implementation not only was controversial by doing this unilaterally, but also buggy.  And it would match and remove multiple www's which were embedded within a single domain name, not just the leading www - this in a code patch they have since fixed.  So, however, users who are voicing opinions remain unhappy with the Chromium team's stated intention to return to removing a leading www from the URL in Chrome 70.  The Chromium team solicited comments, and they received many.  I pulled three out.  Actually, I got these - these were pulled by Lawrence over at Bleeping Computer, and I like his taste in comments.  So I'll just - I'll quote those.



First one, someone posts:  "What is the reasoning behind the decision not to wait for the standardization discussions?  As the majority market shareholder, it's incumbent on Chrome and Chromium to be cautious when making arbitrary decisions about how connection information should be displayed or, in this case, deliberately transformed and obscured.  The urgency with which this change is being pushed is baffling."



He continues:  "I strongly disagree with the idea that we should be connecting to one host name while displaying another in the address bar.  Absent legitimate discussions with the appropriate standards bodies, this decision feels myopic at best, and wrong-headed at worst."



Comment 2:  "Standardization discussions aside, no changes to the look and function of subdomains should happen unless it is an opt-in setting.  SaaS, ESP, and ISP providers take great care to manage their subdomains and use 301 redirects to bring users to the right location."  It's interesting he should say that because that's exactly what I had mentioned GRC does deliberately.



He continues:  "A browser should not determine this function unless it is completely a user choice.  Perhaps Google/Chromium would like to respond as to the exact end game and reason for such a change."  It does feel like they ran out of other stuff that was more important to fix and so, oh, let's screw up the domain that we're displaying.  Anyway:  "Dictating a change which is controlled through DNS, but shows up differently on a browser, is just plain confusing and wrong.  I don't agree with what Safari and Windows have done, either."



And the third comment, a shorty, says:  "At least make it a toggleable option.  I don't mind if URLs are elided by default, but please have an option to disable this."



So anyway, I think these are appropriate responses.  I mean, first of all, I have always and long said that it's unfortunate that the Internet and the web has evolved at all the way it has.  I mean, http://?  Really?  You know, we're trying to explain that to our moms?  And then, you know, should I use www or not?  I mean, it's a mess.



But I completely agree that this is not something that a single browser, despite being the majority browser on the Internet, should decide.  That is, this should be, I mean, the only reason this exists at all, the only reason there is a Google and that there is a Chrome browser is that there are standards which preexisted their existence.  And it's the compliance to those standards and the interoperability of those standards which allowed Google to get created and allowed them to create a browser.  They've been standing on the shoulders of others who set these standards and then abided by them.  And now they're just unilaterally saying, "We're just going to change what the user sees."



So I'm in 100% agreement with the first two comments that say, okay, yes, let's talk about this.  But if we're going to do this, this needs to be a global decision, and all the browsers need to decide how they're going to handle this.  And that allows then websites to similarly decide, first of all, to participate in this, which didn't happen.  And as a consequence, Google realized, ooh, crap, this "m dot," that was important, and we took it away.



So again, I just don't think - I think they've overstepped.  They've been having fun.  They've been saying, oh, well, we're going to expire our recognition of certs early.  We're going to do this.  We're going to do that.  We're going to stop supporting SHA-1.  And they've tweaked some feathers by cutting their own path.



I think maybe they've stepped a little too far this time.  They immediately backed away, said they're going to go forward again.  Let's hope that doesn't happen.  Again, I would love to see http://www. all go away.  Just kill it.  But we have to do that universally, by agreement, with lots of discussion and deadlines to make sure that things are ready for it.  And then all the browsers have to be in sync with this change.  It would be great to fix this, but I don't think simply blinding users of one browser to this arbitrary change, I mean, I don't think that makes any sense.  It's just - it's wrong.



JASON:  I think Google has many times before had big ambitions or big ideas kind of similar to this.  And I think they believe that because Google is Google, and Google/Alphabet is so large and influencing and all that, that many times they can just decide to do a certain thing, and that's going to create critical mass that moves everyone there.  It kind of falls into Google's kind of ethos and a lot of Silicon Valley ethos of move fast, break things; that whole mantra of just, like, do it, and then ask for forgiveness later and see what happens.  Doesn't always work out, obviously.



STEVE:  Yeah, they definitely broke something this time.  Okay. So I got an interesting tweet from Sean Nelson that I saw just before launching last week's podcast, and I shot back a note thanking him for the information, telling him that there wasn't time to get it into last week's podcast.  But I told him that I thought it was worth sharing.  Which was sort of - this evidences an interesting, I would think a little slimy and questionable marketing practice.



He tweeted to me:  "I manage a couple hundred school district websites that use Let's Encrypt certs for HTTPS."  And first of all, let me just stop and say that's a perfect example of an application where it makes sense to save school districts, a couple hundred school districts, each needing an expensive domain cert only, I mean, not because they have to have it, but because they want to have encryption.  That's a beautiful application of Let's Encrypt.  So probably before Let's Encrypt this wasn't being done.  These things were - they weren't able to offer HTTPS protection; or, if so, only at substantial cost.  So bravo for the change that we've seen over the last couple years.  A couple hundred school districts, he says.



Let's Encrypt, as we know, "renews automatically," he writes, "using a script on my servers.  I just got a breathless phone call from Comodo warning me that one of my hundreds of domains has a cert that will expire soon, like," he says, "70 days from now, and I'd better buy a three-year certificate or I would be sorry.  When I told her that I know what I'm doing, and it renews automatically, she made it sound like I was being irresponsible by risking the certificate renewal every three months rather than every three years."  He finishes, saying:  "I wonder how many other LE users are getting these phone calls from Comodo desperately trying to scare them into buying a certificate instead of using a free LE cert."



Anyway, I got a kick out of that.  I thought that was an - so it's an interesting marketing approach since, when you think about it, every website certificate clearly displays its expiration date.  So desperate sales agents with nothing better to do could just attempt to roust users whose certs are nearing retirement and renewal and say, hey, just wanted to make sure you knew, and why don't you buy ours?



And of course I'm sure that Sean was being a little bit rhetorical because, unless somebody receives the marketing call who is not the admin, who as Sean says knows what they are doing, anybody using Let's Encrypt is well aware that the system just takes of itself, and that it starts early so that it's sure that it's able to get a renewal in time and that it's a constantly rolling renewal system which actually also solves the problem of long-life certificates that need to be revoked because that's of course famously another thing that Chrome - you were talking about Chrome going off on their own direction.



And I of course brought a lot of attention to the fact that Chrome's certificate revocation system is completely broken.  I mean, like the most broken of any browser there is.  So constantly rolling short-life certificates is a solution to having revoked certificates otherwise living for a long time.  So anyway, I just got a kick out of Sean's note.  And I'm sure that anybody using Let's Encrypt knows not to pay attention to such a somewhat questionable marketing call.



Now, we've talked about Border Gateway Protocol a number of times.  In fact, a couple months ago there was one large ISP, I think in Portugal, maybe Brazil, I don't remember where, who was finally shut down by the collective decision of all other ISPs on the Internet because this one group was causing their routers to advertise that they handled small networks of IP space, IPv4 space, that they had never been allocated.  And this had been going on for years.  So if nothing else, this is a testament to the fact that the people in charge take their responses seriously, don't make decisions on a whim, and you've got to really demonstrate that you're determined to break the rules for years before the hammer finally falls.  But when it does, as for example happened with Symantec and their misissuance of certificates, you're out of the game.  Sorry.  Go find something else to do.



So in this particular case this sort of demonstrates the problem.  This one large ISP kept claiming to adjacent routers that it was the owner of small blocks of IP space.  Border Gateway Protocol is that communications protocol by which routers talk to each other and update their routing tables.  And routing works by finding the most specific path to a network.  So advertising small routes, that is, a fewer number of host IPs within a network, that's a very specific path.  So those specific paths would get propagated automatically from one router to the next, out across the Internet.



And what ended up happening then was that essentially they were stealing, successfully stealing IPv4 networks, small subnets from their actual owners.  The actual owners would stop receiving that traffic, and it would go to this hijacker instead.  And BGP attacks or misconfigurations have also happened where a well-meaning ISP, someone just types the wrong /subnet limiter or specifier into a routing table and suddenly ends up receiving traffic they don't want because their connections can't handle it.  But as a consequence of the mistake, they're saying, yeah, we have pretty much all of the IPv4 space.  And local routers go, oh, we'll just send our stuff to you, and off they go.



So it can be inadvertent.  It can be deliberate.  But what's been clear for years is it is a problem.  So what we have now is the emergence of a solution.  The final piece of the standard to protect against Border Gateway Protocol hijack attacks and also misconfigurations, which can happen, they don't last for long because it pretty much comes to everybody's attention.  But we have the first official draft of the final of three pieces.  The effort is termed SIDR, standing for Secure Interdomain Routing, and that's what BGP is about.  It's these border gateways are the routing between domains. 



So we have three interrelated protocols.  There are two that were ratified and finalized about a year ago.  It was in September of 2017.  RFC 8206, which is titled BGPsec, as in, you know, we're all used to talking about DNSSEC.  That's DNS Security.  This is BGP Security, Considerations for Autonomous System Migration.  And then RFC 8210, which is known as the Resource Public Key Infrastructure, RPKI, to Router Protocol Version 1.  And it actually updates a previous RFC, which was Version 0.



And so, for example, there they said:  "In order to verifiably validate the origin Autonomous Systems and Autonomous System Paths of BGP announcements, routers need a simple but reliable mechanism to receive Resource Public Key Infrastructure, prefix origin data, and router keys from a trusted cache.  This document describes a protocol to deliver them."  And so our savvy listeners can see that this is very much modeled on the DNSSEC approach, the idea being that routers will in some way arrange to sign and provide signatures which can be verified of the routes which they authoritatively control.  So that's what the system is going to be providing.



This third document, a year later, which was recently finished - and I should mention that this is the NIST and DHS are the two U.S. governmental organizations behind this, although this has been a huge multi-participant effort because they're working to get it right the first time.  They just released a mind-numbing 264-page document which is - do I have the title of it?  Oh, yeah.  They called it the BGP Route Origin Validation (ROV) standard which, when coupled with the other two existing protocols, promises to help ISPs and cloud providers protect against BGP hijack attacks.  And of course, as we know, it'll do more than that.  It'll also, by creating a means for validating what a router is claiming, the addresses that a router is claiming to be responsible for, it will solve this problem of both inadvertent mistakes and attacks.



So I have in my show notes here a bunch of bullet points from it.  But we already understand what this is about.  It is intended to strengthen BGP to provide using the public key infrastructure, much as DNS is planning to, or actually, I mean, has.  We now have the root DNS server signed, and the effort is stalling because it just requires people doing things, and people don't want to do things unless they have to.



In this case, we at least have standards.  And over time, routers will probably be updated to support them.  And we will eventually move towards a world where it's no longer possible for someone like this small ISP to deliberately commandeer chunks of the Internet for the purpose - actually in this case, as we talked about before, they were reselling this IP space to spammers, stealing legitimate IPv4 space which had not been blacklisted as spam sources and essentially reselling it to these spammers.  So it's still early days, as we've talked about.  And you can tell because, when you look at the URL, it often says http://www for no obviously good reason.



JASON:  Again and again and again.  So California has this IoT legislation, and I'm wondering how much actual teeth it has, or whether it's even satisfactory to begin with, because it's a pretty light touch, I feel.



STEVE:  Oh, goodness.  Well...



JASON:  There's some good stuff in there, though.



STEVE:  Well, actually there's, yes, a little bit of goodness.  So a little bit of background.  The California State Legislature recently approved "SB-327 Information Privacy: Connected Devices," that was the bill, and handed it to our Governor, Jerry Brown, to sign into law.  It purports to introduce security requirements for connected devices sold in the U.S., and defines these devices as any device which connects directly or indirectly to the Internet and has an IP or Bluetooth address.  Okay, which is kind of everything.  Unfortunately, legislators.  This is really why anything like this needs to get done by technologists and then sort of just like, get out of our way, please.



JASON:  Right, right.



STEVE:  Anyway, so quoting from this:  "This bill, taking effect on January 1st, 2020, will require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features" - oh, so you can have more than one, how nice - "that are appropriate to the nature and function of the device; appropriate to the information it may collect, contain, or transmit; and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure, as specified."  Okay.  Which basically said nothing, but used a lot of words to say it.  So this, I guess, we can say, well, this is evidence of intention.  I don't know what this means.



But there was a little bit, there was a little tiny sliver of silver lining here because at one point the bill says:  "If a connected device is equipped with a means for authentication outside a local area network, the authentication system must meet one of two criteria.  First, if the device uses a default password, the password must be unique to each device; or, two, the device must prompt users to set up their own password whenever the user sets up the device for the first time."  So that's a breakthrough in, like, we don't have anything like that now.  And that's very clear.  That first paragraph could not be less clear.  This one says no default passwords if there's authentication outside a local area network. 



Which is to say, all of these situations where there have been routers with a default password exposed to the WAN, that is, to the Internet, the Wide Area Network, those can't be sold.  And I don't know, I mean, they're saying in the U.S.  I guess they can't be sold in California.  California is a huge market.  So that gives this legislation some teeth.  So it hasn't turned into law yet, but it certainly does represent a nice step forward.



Now, any legislation is going to have detractors.  And last Monday, in reaction to this, Errata Security's Robert Graham, who is the - he was the original guy behind the Black Ice Firewall many moons ago, back when personal firewalls were a thing, and they were added to operating systems like Windows 98 that never knew what a firewall was.  He generated a long posting from which I'll just share the first three paragraphs.  He wrote:  "California has passed an IoT security bill awaiting the governor's signature or veto.  It's a typically bad bill based on a superficial understanding of cybersecurity/hacking that will do little to improve security, while doing a lot to impose costs and harm innovation."



He writes:  "It's based on the misconception of adding security features."  He says:  "It's like dieting, where people insist you should eat more kale, which does little to address the problem if you're pigging out on potato chips.  The key to dieting is not eating more but eating less.  The same is true of cybersecurity, where the point is not to add 'security features,' but to remove 'insecure features.'"  Okay.  No one's arguing that.



"For IoT devices, that means removing listening ports and cross-site/injection issues in web management.  Adding features is typical 'magic pill' or 'silver bullet' thinking that we spend much of our time in info security fighting against.  We don't want arbitrary features like firewall and antivirus added to these products.  It'll just increase the attack surface and make things worse.  The one possible exception to this is 'patchability.'"



He says:  "Some IoT devices can't be patched, and that is a problem.  But even here, it's complicated.  Even if IoT devices are patchable in theory, there's no guarantee vendors will supply such patches or, worse, that users will apply them.  Users overwhelmingly forget about devices once they are installed.  These devices aren't like phones and laptops which notify users about patching."  And of course he's echoing many of the concepts and suggestions of this podcast.  So anyway, it'll be interesting to see where this goes, whether it gets signed.  It feels to me like Jerry Brown is probably going to sign it.  I don't know, I mean, that first intent paragraph sort of says, I mean, you can reasonably...



JASON:  What does "reasonably" mean?  Yeah.



STEVE:  Yeah, yeah.  Exactly.  You could just say, oh, we have port closed, you know.  I mean...



JASON:  We put the icon of a lock on the packaging.



STEVE:  Yeah.



JASON:  That's enough.



STEVE:  Exactly.  Yeah, we said it's secure.  So be happy.  Okay, fine.  Anyway, who knows what's going to happen.  I mean, this is the beginning.  We know we have a problem.  When we talk a little here before long about what has happened with DDoS attacks, it is entirely a function and a cause of IoT devices.  And, boy, is it sobering.  So this has to get changed.  And of course we talked last week, Jason, you and I, about what's gone on with routers and the number of routers which are now being hijacked and enlisted for DDoSing.  Okay, so...



JASON:  Yeah.  I mean, I suppose it has to start somewhere.  So I'm happy to see that.  I like the password suggestions.  Those are very useful, at least.



STEVE:  Yeah.  And in fact it's, absent that, it has been possible for manufacturers who absolutely could care less to set, I mean, all of our routers have admin/123456 when you install them.  We all know to change that.  So it will, on the other hand, that's local access.  And that doesn't have to change.  It's the WAN side access that does have to change.  But many routers you can turn on global access.  This law says, if you do that, it cannot be with the same password.



So there will have to be some firmware updated.  And I would argue against Rob's position that doing that is a bad thing.  If you turn on WAN access and the law states you must then prompt for a non-default password, I don't see how that's anything but good.  But I certainly, you know, all of the other points he makes are points we have long made on the podcast, which I naturally agree with.



Okay.  So 10 years ago, in 2008, we talked about the problem of sleeping PCs and laptops which go into that ACPI S2, I think it is, it's either S2 or S3.  That's where everything is shut down, even the processor, except RAM is kept powered up.  That's the so-called "sleep" that we're used to using on our machines.  And we all see it.  Typically on laptops the power light throbs very slowly to tell us that, oh, it's asleep.  But when you open the lid, it blinks back on.  It doesn't take long for the device to come back.  It's not rebooting, it's basically powering up all the systems that have been idled, and then it continues where it left off.



So that was 10 years ago.  What we talked about then was so-called "cold boot attacks" because what was recognized a decade ago was that the RAM was still - RAM still had its contents.  And if you could arrange to reboot the system without the RAM powering down, or even not powering down for long, if the RAM wasn't wiped proactively, and you could boot into a different OS, you had access to the state of RAM at the time that system slept.  And if, for example, BitLocker was in use and had been unlocked, or for that matter TrueCrypt, which was a big deal 10 years ago, or any other important crypto keys, I mean, basically RAM is there.  And you can rifle through it as a third-party OS that gets control.



And as we know, there's also literally freezing cold boot, where you spray Freon on the chips.  And we've talked about how, as a consequence, the fact that DRAM are actually a large bank of tiny capacitors which are slowly losing their charge, thus the need to refresh the contents before they lose so much charge that you can't tell whether they used to be charged or not.  If you sprayed them with Freon, you could actually remove them from one machine and stick them in another and power them up in time to preserve their contents.



So what had happened since then is that the BIOSes began, like from 10 years ago, when the so-called "cold boot attacks," where you don't take the RAM out and physically freeze it, you simply keep the RAM alive and reboot into a hostile OS environment, which then looks at the RAM and says, hey hey hey, we've got some goodies here.  What BIOSes had started doing is, first of all, being less willing to boot from, for example, an external drive, a little more resistant to that, and also proactively wiping the contents of RAM.



So last Thursday two researchers from F-Secure gave a presentation at the Sec-T Conference titled:  "An ice-cold boot to break BitLocker."  And the teaser for their presentation wrote:  "A decade ago, academic researchers demonstrated how computer memory remanence could be used to defeat popular disk encryption schemes.  Not much has happened since, and most seem to believe that these attacks are too impractical for real world use.  Even Microsoft has started to play down the threat of memory remanence attacks against BitLocker, using words such as:  'They are not possible using published techniques.'  Well, we will publish techniques that allow recovery of BitLocker encryption keys from RAM on most, if not all, currently available devices.  While BitLocker is called out in the title, the same attacks are also valid and fully effective against other platforms and operating systems."



Okay.  So what did these guys do?  It was pretty simple.  Keeping the target machine - in this case a laptop, but it could be a desktop - powered up, they opened the back and hooked onto the readily available and exposed flash memory with a programmer and tweaked the machine's BIOS setting storage chip where it does the nonvolatile BIOS settings in order to cause it to [do] what they wanted.  They tweaked the settings to disable the RAM memory overwrite, which it turns out is not exposed to the UI, but does exist; and they enabled booting from external devices.  And that's it.



They then carried out a traditional cold boot attack by rebooting that powered up system into a special program on the USB stick.  It's special because it needs to minimize its RAM footprint so as not to overwrite anything valuable in main memory.  But that's easily done.  And they were then able to obtain the BitLocker or other disk encryption keys in order to have access to the drive.



And remember that, since this defeats BitLocker or whatever other whole disk encryption might be there, they're not only getting the contents of RAM, but are also able then to decrypt the entire contents of everything stored on disk, which is then accessible externally without the layers of protection provided by the OS, which it is assumed is the only entity able to perform that decryption.  Which means you know you have root, I mean, absolute full root unrestricted access to the file system on that system on which you have performed this attack.



So what do we do?  We can expect that, in reaction to this, machines will get further hardened, as they should be, against this.  Microsoft maybe will be a little less glib about the fact that these memory remanence attacks are no problem for BitLocker when in fact these guys last week demonstrated that they in fact are today by pulling off exactly this attack.  What it probably means is that, until then, or for existing systems, if you're going to have systems which sleep, that is, that do this, you need to maintain physical security.  This is a physical security attack.  Not possible to perform this without digging into the machine itself.  But you can imagine, if law enforcement were to grab a machine that were sleeping, they would now keep it powered up and take it back to the lab and probably be able to gain full access to it.  I mean, sleeping is dangerous, even if you're prompted for a password when you open the screen.



So as with the recent TPM bypass that we talked about, the culprit is this sleep state.  Remember that the TPM specification itself had an error in the way sleep is managed with the Trusted Platform Module.  So if it's possible, as I suggested for anyone concerned about this, disable the sleep option in the BIOS.  Most BIOSes today still let you disable sleep so that it's just not available.  But the other thing you can do, at least in Windows, is you can use the so-called "Group Policy Editor."  And I have on the page in the show notes here, I've detailed it completely.  You type at the Run bar gpedit.msc, which brings up the Group Policy Editor.  And I won't go into details because it is here in the show notes for anyone who's interested.



Using Group Policy Editor you're able to disallow Windows to use the standby states S1 through S3 when sleeping, both on battery or when plugged in.  You're able to disable that.  And it simply removes the option from the menus.  It will no longer sleep.  And you are then able, for example, to enable hibernation if you would prefer to do that.  Hibernation still makes me a little uncomfortable, but at least it's not going to be - you're not going to have the data in memory, and presumably Windows is a lot safer coming out of hibernation than it is waking up from sleep because sleep has a running instance of everything sitting there in RAM.



So if anyone is really concerned, if their system is sleeping, and they believe there's some possibility that a system that is sleeping could be actively physically attacked, then disabling sleeping certainly makes sense.  And of course you are able to more easily do that in a UI just by disabling the automatic move to sleep and instead have the system hibernate or just shut itself down.



JASON:  Sometimes I come on this show, and it makes me realize I have bad habits because I do put my computer to sleep.  But I do that actively.  So it's like I'm making that choice, and that doesn't make it any better.



STEVE:  Yeah.  And it certainly is a convenience to have it just wake up out of sleep.  So, yeah.



JASON:  Sure.



STEVE:  And I would argue, too, that it increases the system's longevity because you're turning off, you're powering down the processor and all of the peripherals and only keeping the RAM alive.  So it minimizes the heat in the system.



JASON:  Sure.



STEVE:  Okay.  So as they say, don't try this at home.  I couldn't resist, mostly because I had a laptop that had not yet been updated to v12.  And I thought, okay, if they're going to fix this after - I don't mean a laptop, an iPad.  If they're going to fix this in v12, which just became available, I want to know that it wasn't fixed in 11 point whatever I had before.  So this is known as Safari Ripper.  It is an Apple Safari vulnerability that quickly crashes iOS and Mac devices, where links are handled by Safari.  And it works.



Sure enough, I clicked the link, pow.  It just completely collapsed the iPad.  Then I updated to v12, did it again, whoops, screen went black, and it rebooted and had to come back to life.  So Apple has a problem that they did not fix in the most recent update to iOS.  It exists before, and it exists today.  It leverages a relatively new and processor-intensive feature of CSS, you know, Cascading Style Sheets, which is known as the "backdrop filter."  That uses 3D acceleration to process the image background underlying CSS elements.



The backdrop filter is able to blur or color shift regions behind an element.  And you can imagine fancy, I mean, we see it on web pages often, fancy pages where you move over something, or you scroll up, and something that you're sort of not supposed to be paying attention to blurs.  Or you have a full-color picture, and it goes to black and white.  Or it goes to, sort of like a chroma keyed effect.  You can do all kinds of things with it.  But this is actually happening, not with code, but, well, with code driven by static CSS elements and, in this case, this implementation of a backdrop filter.  So the point is it's image processing happening merely by a declaration of a line of CSS.



So security researcher Sabri Haddouche was poking around while looking for reliable denial of service bugs against various browsers when he discovered this, a simple way to essentially bring any Safari-based system to its knees.  All he used was nested divs.  The "div" is short for "division."  And so you can have a division in a division in a division in a division.  And in fact I looked at his code, and he's got, I didn't count them, but he's, like, hundreds.



And the problem is, if you were to apply the backdrop filter to the CSS for the division, then you're going to cascade the need to perform this rendering, that is, essentially you're layering hundreds of these filters on top of each other.  And so the result that you're going to see on the top layer is the result of its filtering through the CSS for it, which it doesn't get until it knows what the CSS behind it is going to resolve to, which it doesn't know until the CSS behind it is going to resolve to, which you're able to force with this crazy stacked set of divs in CSS, in the HTML.



So it turns out, boy, is it potent.  For anyone who's interested, I have the link in the show notes.  You might be able just to google "Safari Ripper" right now and find it.  It was not easy for me to discover it.  I mean, not difficult for me to discover it.  It was easy.  And, I mean, it doesn't do any harm, but it does immediately crash your Safari-based rendering, whether on iOS or Mac.  And I wouldn't be surprised if the link ends up getting retweeted and so forth, people playing games with this.  As I mentioned, v12 doesn't fix this.  So I would imagine that it won't be long before we get a 12.0.0.1 that does somehow arrange to resolve this.



JASON:  And that only works in the Safari browser?  Is that right?  We've got some folks in the chatroom who are saying 403 Forbidden on that link.  I know I tried it earlier, but I wasn't on a Safari browser, and I got that 403 Forbidden link.  But does that mean that they've bottled it up?  Or is that [crosstalk]?



STEVE:  Okay.  I just tried it, and it came up.  Although it came up on, let's see, it came up on Firefox, which is my default renderer on Windows.  And it looks just fine.  And it says "Triggered," which is I know what the image is supposed to look like.



JASON:  Yeah, I get a Forbidden, yeah.  Strange.



STEVE:  Interesting.



JASON:  Get a 403, yeah.



STEVE:  Anyway, so maybe it's been taken down.  It's certainly on the 'Net and around.  And in fact I've got it on my browser.  So who knows?



JASON:  Interesting.



STEVE:  Okay.  So as I mentioned at the top of the show, Twitter has permitted apps and devices management, which makes them late to the game, but better late than never.  In Twitter, if you go to Settings and Privacy, under there you'll see Apps and Devices.  You can also just go to the URL Twitter.com/settings/sessions, which will take you to the same page.  First, at the top of the page, I did it yesterday, they're promoting connecting to Facebook.  No, thank you.  But then there's a section of "Apps connected to your Twitter account," followed by, and this is what's new, "Recently used devices to access Twitter," which is nice to see.



And we've seen this with other, you know, Google has been doing this for a while.  We've talked about Facebook and the Facebook API.  Broadly, these various services have APIs that allows us to give applications or other services access under our permission.  Very much like the browser extensions - or the browser barnacles - that we'll be talking about at the end, these sort of tend to just accrue over time.  You're doing something.  Something says, hey, I need access to Facebook, or I need access to your Twitter account.  And you go, okay, fine.  And so you use this new widget for maybe five minutes, and then it's like, okay, and you get distracted.  You see something else that's shiny, and off you go.  But that thing still has access because they don't ever give it up as a consequence of disuse.



It's also very useful in the case of something where a lot of you is present, like on Google.  It's worth looking, auditing where the devices have been that have logged into your Google account.  So that we didn't have before now.  Now we do with Twitter.  So on the first hand, in the case of apps, it's always useful from time to time to just scrape the barnacles.  Just go through the list.  And I did yesterday.  And it's like, oh.  There were, like, maybe half of the things that I had given permission to previously I was no longer using.  And so they no longer have permission.  They're gone.  And then the second half is where have logins occurred recently?  And maybe it's a little buggy, or maybe it's just because it's new.  But I had several devices showing that I had last logged onto Twitter on New Year's Eve 1969.



JASON:  At 4:00 p.m.  I've got one on my list.



STEVE:  No kidding.



JASON:  Who knows what it is?  It's an Android [crosstalk].



STEVE:  Were you alive, Jason, in 1969?  I guess you were.



JASON:  Not that I know of.  I don't believe that I was.  But I can neither confirm nor deny.



STEVE:  Okay.  In that case, I can tell you that in '69 I was, what, I was a freshman in high school, and I had not yet encountered the PDP-8 from DEC which would change my life as I learned to program in its machine language and its assembly language.  But there was definitely no Internet, and there was definitely no Twitter.  And so this was definitely a mistake on when did your app last use Twitter.  Okay.  But anyway, definitely worth, again, auditing these things, whether they be, in these barnacles, whether they be permissions that we've given to services or extensions that we've installed in our browsers over time.  Definitely worth doing for the sake of privacy and security from time to time. 



JASON:  For sure.



STEVE:  Just a quickie note that the Mirai botnet is on track to become historic.  Things like Code Red and Nimda are worms that we could argue will go down in history.  WannaCry and, you know, various things have made a big impact over time and have ended up carving out a little bit of a niche for themselves.  I think the Mirai botnet is probably on track.  In its latest evolution, its repertoire of exploits has grown to 16.  And while the majority remain aimed at compromising routers, networks, video recorders, and DVRs, Mirai is also more recently and now targeting unpatched Apache Struts vulnerabilities.



So, I mean, the people behind this are demonstrating an intent to jump on anything that they can go after.  We talked last week or the week before about the fact that there were - we were now seeing botnets which were using a version of Linux which had multiple processor hardware targets in order to significantly broaden the hardware platforms they were able to infect and occupy.  And so when you combine that with a growing vocabulary of exploits, I mean, known exploits which are patched from their manufacturers, but not patched on the endpoint devices, we're ending up looking at a botnet which is growing in size and is becoming incredibly powerful.  And we will talk about, when we talk about the explosion of DDoS next, just how much these things are becoming a threat.



Okay.  So explosion in DDoS network sizes.  There's a company, Nexusguard, N-E-X-U-S-G-U-A-R-D dotcom, that's in a business I want no part of, which is DDoS mitigation.  It is just - it's not a fun job to be in the DDoS mitigation business.  If you're doing it by trying to filter bandwidth, then you're tackling staggering sizes of flooding attacks these days.  And it just seems to me it's sort of a thankless job.  Anyway, they're in the business, and so they have the ability to generate statistics that we don't see about what's going on in the DDoS world.  In their most recent report, they have compared the first two quarters of last year versus the first two quarters of this year.



So during these identical periods, they have seen a 29% upwards jump in the number of attacks and, get this, a staggering 543% increase in the average size, that is, the bandwidth flood size of the attack, where the new average DDoS attack is at 26.37 Gbps of attacking bandwidth - 26 Gbps.  Which says, you know, if you have a gig connection to the Internet, it will be flooded.  The point is that it will be saturated with attack traffic such that no valid traffic has the opportunity to get into your servers.  You will be knocked off the 'Net.



So this Nexusguard report shows the average size of attacks in 2017 during this period was 4.1 Gb, with a maximum observed of 63.7 Gb.  One year later, the average size has grown by more than five times to 26 Gb.  That's average; right?  That's 26.37 Gb.  The maximum size attack they have seen, 359 Gb.  So again, as I said, being the company trying to keep an entity online that is being hit with 359 Gb, good luck.



And of course the attacking sources will come as no surprise to followers of Security Now!.  Nexusguard in their report wrote:  "The increase in attacks and their sizes is attributed to attackers amassing giant botnets using insecure IoT devices.  Attackers are using vulnerabilities in these devices to rapidly build large botnets that can then be used to perform targeted attacks that are increasingly difficult to stop.  For example, at one point the Mirai Satori botnet was seen from over 280,000 IP addresses over a 12-hour period, and the newer Anarchy botnet was able to amass over 18,000 routers in a single day.  These botnets were created by attackers exploiting vulnerabilities in routers such as ones made by Huawei and D-Link."



And of course we've talked about these before.  They are sitting on ever larger bandwidth WAN links, thus allowing them to attack targets with ever larger traffic.  And 280,000 machines in a single botnet is just staggering.



Nexusguard wrote:  "In addition, severe botnet epidemics like last year's Satori continued to threaten cyberspace by exploiting zero-day vulnerabilities.  Since its high-profile attack on Huawei home routers in December 2017, Satori has wreaked havoc over the past few months on various IoT devices, including GPON-capable routers" - those are the ones that we've talked about often in Brazil - "manufactured by South Korea's Dasan, D-Link's DIR-620 routers" - that we've talked about - "and the uc-httpd IoT devices.  Additionally, the quarter saw the emergence of the Anarchy botnet, which exploited zero-day vulnerabilities in a similar fashion as Satori."



The report also said that slightly more than half of attacks, 55.28%, had a duration under an hour and a half.  So slightly more than half of the attacks had a duration of less than 90 minutes, but the average attack lasted 318 minutes long.  And that average was pushed up because some attacks lasted for days, with the longest being six days, five hours, and 22 minutes.  And then they also finally said, although nearly two-thirds of attacks, 64.13%, were under 10 Gbps, the average size, as we mentioned, was 26.37 Gb.  Oh, and lastly, the United States was the largest source of attacks at 20%, followed by China, France, Germany, and Russia successively, sourcing smaller levels of attack.  So we are in a world where DDoS happens with increasing frequency and with literally unstoppable force.



JASON:  I kind of feel like that should be a T-shirt:  DDoS Happens.  Are we at the errata?



STEVE:  Not quite.



JASON:  The strange Windows 10 popup, the screenshot.



STEVE:  Well, okay.  So Windows 10 is starting to warn - actually, yes, warn - users who attempt to install another browser.  Sean Hoffman tweeted this.  His tweet was - he tweeted it to @MicrosoftEdge.  "What kind of slimy marketing cesspool crap is this, Microsoft?  I proceed to launch the Firefox installer, and Windows 10 pops this up.  If I wanted to use your browser, I would."



And he quotes, and I found this elsewhere also, so his tweet includes a screenshot of a happy person with the Edge "e" behind him in a window.  And this dialog from Windows 10 reads:  "You already have Microsoft Edge - the safer, faster browser for Windows 10."  Okay.  And then there's two options:  Open Microsoft Edge or Install Anyway.  And then underneath it says:  "Don't want to be warned in the future?  Open settings."  And I didn't look, and I haven't had this pop up.  This is on the Nightly build, or the Windows 10 Insider at this point.  But what is the settings going to say?  Don't warn when I attempt to install a non-Microsoft browser?  Is that what it's going to say?  Anyway...



JASON:  Just give me a checkbox, a checkbox on that thing that says don't do this again.  It reminds me of when you get subscribed to something like an email thing, and then you go down to the bottom, and instead of the Click Here to Unsubscribe it's Click Here, and then that takes you to another page where you have to, like, work in order to unsubscribe. It's like, just do the thing.



STEVE:  Well, and that's it.  It says:  "Don't want to be warned in the future, open settings."  So what is the setting going to read that you turn off to disable being warned when you're trying to install a browser that Microsoft didn't provide?  I mean, I guess I get it that maybe somebody who would have Windows 10 would not click on a link where Edge immediately comes up and would instead think, oh, first thing I need to do is install somebody else's browser.  But come on, really?  We know that's not the case.  They are deliberately creating some pushback, some back pressure against anybody who wants to install a different browser.



It turns out that this also applies to Firefox, Chrome, Vivaldi, and Opera; that it's in Windows 10 Insider Builds only.  And, yeah, I know I'm annoyed when I'm going to Google's search page, and I get the little box in the upper right.  Google's doing the same thing, you know, promoting their own Chrome browser when I'm using Google Webs without a Google Chrome browser.  But that's what I prefer to do.  At least Google lets me ignore it, and I can just sort of go about my business.  Here you've got to say, yes, I'm sure, I really do want to install the browser that I clicked on to try to install a minute ago.  So anyway, grumble.



JASON:  Grumble, grumble.



STEVE:  I have a bit of errata from Alexey, I'm not kidding, Alexey in Russia.  And if Leo were here, he would probably give us his Russian accent reading.  I will spare our listeners that in this case.



JASON:  I will not follow in his footsteps on the Russian.



STEVE:  Thank you.



JASON:  You're welcome.



STEVE:  The subject was "Couple of thoughts in defense of MikroTik," he says, parens, "(and they *can* auto-update)."  He says:  "Hi, Steve and Leo.  Long-time listener, SpinRite licensee with all the customary blah-blah-blahs to you both.  I've been using MikroTik (MT for short) routers since about 2012.  I have a couple at home and one at work.  I'm a sysadmin in a small company.  I'd like to mention that my oldest router still receives all the updates, generally one or two a month.  It's a very strong point for MikroTik in my book.  I've heard about Ubiquiti long after becoming acquainted with MikroTik, and never had reason to switch nor spare cash to experiment, despite your fascination with the infamous Ubiquiti Edge.



"First of all, I'd like to mention that MT routers can be easily configured to auto-update.  Details are below, if you're interested.  Secondly, I do agree with you MT have to change their policy and default settings.  And they have the capabilities to do so without additional hassle for anyone.  They have several templates for users to choose from on initial setup.  They just have to harden their home router template and make it the default for new devices.  They don't.  But if they did that, users would be reasonably protected, and advanced ones will configure from scratch or choose another template.



"However, in their defense, I have to say that, A, it's absolutely clear from even a cursory look that their routers are not consumer-oriented plug-and-pray or plug-and-forget devices; and, B, that they're targeted, not at casual home users, but at geeks at the very least."  He says:  "I'll leave out the professionals and such here.  In my experience," he says, "configuring MT router is a bit more like configuring Linux server than configuring a traditional consumer router.  Common good practice to use non-standard ports for services, including SSH, Web UI and Winbox if you use them, fully applies here, and to disable ones you don't use, as well.  And you absolutely have to configure your firewall.  Default configuration is far too basic for deployment.



"There's no checkbox to allow/disallow WAN access.  We're grownups here.  You have to configure firewall rules - with netfilter/iptables-like CLI commands or in a GUI - to allow or disallow such connections.  And, yes, with great power comes great responsibility.  MikroTik routers can easily be misconfigured.  I have to say all recently reported vulnerabilities you told us about were mitigated by properly configured firewall, i.e., allow inputs only from trusted IPs in LAN or maybe WAN if you really need it.  Drop all other inputs.  And changing default service ports helps a lot, too."



And then finally he says:  "Regarding auto-update.  As you've said in your coverage, RouterOS, the OS of MT's routers, have extensive scripting capabilities and cron-like scheduler.  Script to check for update, install it automatically, and reboot is about four or five lines long.  Couple more if you want to be notified by email before or instead of updating.  Schedule it to run daily at 3:00 or 4:00 o'clock in the morning, and you're good to go.  Judging from MT forum posts, sysadmins and ISP guys don't think their routers should auto-update.  They prefer to test new releases beforehand and only deploy security fixes, let's say.  But for home users who don't want to do it manually, it's there.  And users don't need to buy a new router.  Their old ones are perfectly capable of doing so.  Me personally, I prefer to do my updates manually.  And now I have an automatic update available notification."



So that's what Alexey has to say.  I will note that, first of all, I first learned of MikroTik when people were comparing it to the $49 Ubiquiti EdgeRouter X.  And so I went over to MikroTik.com, and I found, for example, the hAP ac, which they say is our most universal home or wireless device.  Dual band, 3x3 MIMO with gigabit ports that opens the full advantages of 802.11ac speed while maintaining compatibility with legacy devices in 2 and 5 GHz.  So there's the hEX Lite at $39.95, the hEX at $59.95, hEX PoE Lite at $59.95, the hEX S at $69.00, and so forth.  Now, yes, they absolutely have enterprise-oriented products because, for example, the CCR1072 1G 8S comes in at $3,050, a 1U rack-mount device with lots of features.  And they clearly have a sense of humor because sort of in the middle is the RB1100AH Dude Edition, which comes in at $349.  So a range of routers.



I understand and respect what Alexey said.  But I disagree.  I mean, I understand what he said.  But, for example, if there is a template, and a home user can say give me the home router template, and that doesn't protect them from incoming service access from the WAN, which is what Alexey just said, that is, the template, the default home router template does not provide protection from a user who does not want to set up iptables cron scripts, then MikroTik is doing it wrong.  Given the numbers that we have seen, those are not enterprise - those are not $3,000 routers.  Those are $39.95 routers, and they've been set up as home devices by people who assume that means they're done.  And what Alexey has said is the default template for the home user does not protect them the way they would expect to be protected.  So I get it that it is a cool, feature-packed, powerful router.  But they've got their defaults set wrong, and they should fix that.



And I'm on the fence about updates.  I get it that there are at the high-end level the enterprise guys are saying, we don't want our routers auto-updating.  We want to take responsibility.  But if you're, I mean, especially if you're going to default to having service ports for sensitive services by default exposed to the WAN by design, as I said, that's a policy problem.  That's not a bug.  I just think it's unconscionable in this day and age to do that.  And so there's no way to let them out of the hot seat.  I get it that they have a lot of power in their devices.  People like them for that reason.  But if you select "I'm a dummy, please protect me," and then it doesn't, that's wrong.



Also, in Miscellany, Jeff Root in San Diego, California, he said of our last podcast:  "OpenVPN Running with Privilege?"  He said:  "All the reporting I saw on this bug said it was exploitable on Windows only."  And I stand corrected.  I should have mentioned that.  He said:  "Checking my Linux system, I see that OpenVPN is installed," he says, "by default on Debian, at least, so that it runs as user 'nobody' and group 'nogroup.'  The config file for OpenVPN clearly states that it will drop privileges after initialization except on Windows.  As far as I'm concerned, the question that should be asked is:  Why can't it drop privileges on Windows?  Is that not supported by the Windows API?  Which should, actually, be the fix for this anyway.  This violates the fundamental security principle of 'least privilege,' which has been known and implemented since the '60s."



And I will say that having built a custom installer for my SQRL client, I learned exactly what Windows can and cannot do.  And in fact it cannot, there is no privilege de-escalation availability or capability in Windows.  So in fact there is no way, I mean, I'm sure you can come up with some rigamarole where you then run with some sort of limited special user.  Unfortunately, they're running with system privileges, rather than creating a special user that does exactly what it is they want.



So I guess more could be done.  But it is the case that you are unable to give up privileges once you're running.  And it makes for some interesting gyrations when you're running an installer that needs to install as a limited user, but install in places where you need to be an admin in order to do so.  But those problems can be solved.



An interesting instance of SpinRite repairing an SSD.  As we've said, I don't know what I'm going to in the future with the name because I really like SpinRite, but it fixes things that don't spin.  And somehow PlugRite doesn't really work for me.  I've not figured out what I want to do.  But Justin in Olympia...



JASON:  AllRite.



STEVE:  AllRite.  Ooh.



JASON:  AllRite.  See, it's kind of...



STEVE:  Oh, Jason.  That's pretty good, yeah.  Justin in Olympia, Washington, his subject was "SpinRite Catches the Bad Guy."  He sent this on September 16th.  He said:  "Steve, long-time Security Now! listener, blah blah blah.  Wanted to let you know how SpinRite helped to catch a criminal.  Granted, it was in a roundabout way.  But still it probably would not have happened without SpinRite.  My security camera server was having issues.  The normal rock-solid software started crashing on a frequent basis.  Every time it happened, until I happened to notice it and was able to reboot the server, my system was down.  It was really getting annoying."



So the point he's making is it was crashing frequently, and therefore being offline until he happened to note that it was crashed, then he would reboot, and it would be up for a while until it crashed again.  So it's spending the bulk of its time down, apparently.  



He said:  "I'd tried a lot of things to fix it, but I was getting the sneaking suspicion that the SSD startup drive might be the problem.  Earlier this week, I ran SpinRite on Level 2. While it did not report any bad sectors, it did the trick anyway.  The camera software has not crashed since.  This is a very good thing, as today someone tried to steal some things I had in front of my house.  I was *praying,*" he has in asterisks, "that the camera software had not crashed and was thus able to record the attempted theft.  And thanks to SpinRite, the entire incident was recorded.



"I was able to provide the clips of the guy to our Sheriff's Office, and within an hour he was in custody.  Oh, yeah, he fought with a deputy, and when he was being arrested got introduced to the business end of the K-9 unit.  He also had several felony warrants outstanding - a real upstanding member of society.  Thanks to SpinRite, a really bad dude is behind bars.  Thanks again."  And, yikes.  Thank you, Justin.



JASON:  That's awesome.



STEVE:  For sharing that.  And, yes, a very cool story.



JASON:  Yeah.



STEVE:  Okay.  So we wrap up by talking about the web browser extension ecosystem.  And, boy, you know, I'm going to stick with barnacles.  I think that's, you know, that would really - we had a problem because the text, the web browser extension ecosystem wouldn't fit all on the lower third line for the podcast.



JASON:  Yup.



STEVE:  And so it could have been just Web Barnacles.  That would have been good, actually.



JASON:  That works for me, Web Browser Barnacles.



STEVE:  Web browser, yeah.  So as I mentioned at the top of the show, I was taking a closer look at the current state of our web browser extension ecosystem, driven by the news that two major versions from now - right now we're at Firefox 62.  In Firefox 64 it will be adding a welcome new feature to the UI.  Right now in the popup menu, the dropdown menu, from its web browser extensions icons, there is a list of commands such as Manage the Extension, Remove the Icon from the Toolbar, Pin to the Overflow Menu.  And then there are also, lower than that, some non-extension-related display options to Hide or Display the Menu Bar and Hide or Display the Bookmarks Bar, and also to further customize the browser's UI.



Firefox 64 will add a prominent, actually at the top of the list, Remove Extension - or scrape barnacle - at the top of the popup menu list.  The team at Mozilla wants to make the process of removing unwanted extensions easier and very, very clear.  And at some point in the future, although apparently not yet with Firefox 64, they further plan to build in an option to submit an abuse report, on the spot, to solicit feedback about the reason for an extension's removal.



So this news, along with our recent coverage of malicious extensions creeping into Chrome and the many times I have encountered weird and apparently unwanted browser toolbars installed in other people's machines led me to look a little bit into the current browser ecosystem for web browser extensions.  And that led me to this stunning graphic.  Oh, my god.  I'm just, well, and Jason, you're at the top of the peak, baby.



JASON:  I'm right there in the middle at the very, very top, absolutely.



STEVE:  You are representative of the majority of users.  So to describe the shape of the curve, for those who can't see it, it's a histogram where the bar height is the number of people with that many extensions installed.  So it looks like people with one extension is just a little less than halfway between 5,000 and 10,000 browsers in the sample size.  And I'm trying to - I had it here in the show notes somewhere.  Oh, okay.



So they examined 900,000 PCs, okay, so .9 million.  They looked at a big cross-section of PCs.  So it looks like about a little shy of 7,500 users had one extension.  Looks like it goes by single units.  So then it looks like maybe 8,000 had two extensions.  Almost 10,000 had three extensions.  Maybe about 14,000 had five extensions.  18,000 had six.  Maybe 22,000 had seven.  More than 25,000 users had eight.  And we keep going up till we get to about 14 or 15.  Anyway, so 12, 13, 14, 15, 16, 17 numbers of extensions were all above 35,000 users.



So essentially what we're saying is that many, I mean, okay, and, okay, so that's the shape on the leading edge of the histogram.  The trailing edge is more slopey, meaning that the number of extensions keeps growing, although the number of users with that many extensions is falling off more slowly than on the leading edge such that more than 5,000 machines had 41 extensions.  More than about 15,000 systems had 30.  And believe it or not, there's a non-zero blip out here at looks like 75 extensions installed in some non-zero number of browsers.  And actually there's tiny little blips all the way out to 90.  Out to a hundred.  Again, so, okay, holy crap.



JASON:  You know, but they use every single one of them very regularly.  Every single one gets used on a daily - no, probably not.



STEVE:  Just like a good barnacle.



JASON:  Yes.



STEVE:  One of the reasons I love the analogy is that they do slow the boat down.  The reason you have to bring the boat into dry dock every so often and scrape the barnacles off the hull is they represent substantial resistance to the water flowing in what's supposed to be a laminar flow across the hull.  They  mess that up.  And in the same way barnacles, browser barnacles, slow down your browser.  Because they need to be loaded.  They need to be initialized.  And they're doing something with your traffic.



So what are they doing with your traffic?  I found, okay, this chart came from a study that was conducted titled "Quantifying the Web Browser Ecosystem."  It's a research paper from the summer of last year, 2017.  And I'll just share the abstract and a bit of the introduction.



They said in the abstract:  "Contrary to the assumption that web browsers are designed to support the user, an examination of 900,000 distinct PCs shows that web browsers comprise a complex ecosystem with millions of add-ons collaborating and competing with each other.  It is possible for add-ons to 'sneak in' through third-party installations" - as we've all experienced where you're installing something that wants to bring along some toolbar, thank you anyway - "or to get 'kicked out' by their competitors without the user's involvement.  This study examines that ecosystem quantitatively by constructing a large-scale graph with nodes corresponding to users, add-ons, and words, the terms that self-describe add-on functionality.



"Analyzing add-on interactions at user level using the Personalized PageRank random walk measure shows that the graph demonstrates ecological resilience.  Adapting the PPR [Personalized PageRank] model to analyzing the browser ecosystem at the level of add-on manufacturer, the study shows that some add-on companies are in symbiosis, whereas others clash with each other as shown by analyzing the behavior of 18 prominent add-on manufacturers.  Results may herald insight on how other evolving Internet ecosystems may behave, and suggest a methodology for measuring this behavior.  Specifically, applying such methodology could transform the add-on market."



Okay.  So they're getting into the world of sort of statistical research.  But they did have some interesting things to say in their introduction.  They said:  "Web browsers have become a major component of the routine human-computer interaction" - yeah, no kidding - "with some browser operating systems" - and Jason, you have it in front of you - "some browser operating systems based entirely on browsers, e.g., ChromeOS by Google."  And there it is.



"Browser extensions, also known as add-ons" - and now also known as barnacles, okay, they didn't say that, I did, just to be clear - "are computer programs that, as the name suggests, extend, improve, and personalize browser capabilities."  Okay.  Hold your breath.  "More than 750 million add-ons were downloaded and installed by Google Chrome browser users as of June [not recently] 2012."  So that, lord knows, in the last six years that's only gone up.



"Some examples of add-ons include an extension that allows visually impaired users to access the content of bar charts on the web."  Hey, that's very cool.  "An extension that addresses user security concerns by seamlessly producing a unique password for each website the user accesses."  And of course we've talked about Ghostery.  NoScript was one I was promoting for a long time until you just couldn't use the web without having scripting enabled.  And then more recently Gorhill's uBlock Origin is one that I promote.  And we've talked about iOS allowing the content filtering, and that immediate upsurge in adblocking that then became available to Safari on iOS.



"Internet software companies," they write, "are very interested in installing their add-ons, and particularly toolbars, on users' machines."  And of course that pressure is why end users end up with these stacked toolbars that provide hardly any room to actually see the web page.  They write:  "Toolbars are GUI widgets that typically reside in the upper part of the browser's window, extending the browser's functionality.  Toolbars can collect information about the browsing history of the user, for example Yahoo Toolbar, and can redirect user search activity to a specific search portal, for example, MyWebSearch.com.  Crucially, the company that owns the search portal, and typically also the toolbar, receives payments from ad providers per user click on the ads it displays."  And then they say, "Primary ad providers are Google and Yahoo.  This revenue generation model is used extensively by software companies that distribute free products."



Now, get a load of this.  There's a stat I did not know.  "For example, 45% of AVG's Antivirus Technologies sales in 2012 were generated by its browser toolbar."  Okay.  Nearly half, 45%, of AVG's AV sales came from the installation of its toolbar.  "It was estimated that Google, the biggest web advertising firm, might have lost $1.3 billion in revenue in 2013 because of changes to its policy with respect to toolbars and a resulting shift of some add-on distributors to Google's competitors."  In other words, there's gold in them thar barnacles.



"Consequently, add-ons compete with each other over resources such as battery, memory, disk space, and computing power."  And of course we know that now browser coin mining is a thing.  And so, yes, competing for computing power and user attention.  "Regardless of how intelligent they are, they may be aware of each other and may piggyback on each other, or uninstall each other.  Add-on behavior within the web browser is characterized by add-ons making their own decisions independently and often unbeknown to the user, which comprises a complex ecosystem with the user being just one of the participants."  In other words, we're not in control.  "A key issue in understanding that ecosystem, responding to it, regulating it, and transforming it into a mature market is the current inability to show that it is inherently stable and measurable.  This study addresses that issue."



Okay.  So I think our takeaway from this is the recognition that the end user and also apparently the browser's vendor are less in control over this browser extension world that has evolved than we and they may think, and that those browser extensions we may have installed some time ago and then promptly forgotten are still alive and installed and having access to everything we do on the web - which, by the way, is unencrypted to them, and also slowing down every launch of the browser.



So I welcome tools, which I mentioned in Firefox 64 we're going to get.  We're going to make it easier for people to just right-click and remove something from Firefox.  And this is another of those things that is absolutely, I would argue, you know, sailors, as I said, pull the boat out of the water and scrape the barnacles.  We as browser users should take a check from time to time into our browser's add-on list and scrape those barnacles off the browser because they're slowing it down.



So, as I said, I welcome tools which profile our extensions and alert us when an extension is slowing down the browser's launch on page loads.  And I'll note that I've seen Edge do that.  It'll show me when a certain extension is representing a problem.  And it's like, ooh, is it worth it to me to have it slowing me down?  And anything that our browsers can do to provide us with additional decision-making tools and empowerment, I think we should all welcome.



The problem, of course, is that we're savvy system users, and web browsers are being used predominantly by people who don't know what they need and don't need.  These are the people who for years have been downloading updated versions of malicious Flash players because the web page they visited told them that they needed to do so.  By comparison, our operating systems have pretty much finally locked down the management of what gets installed into our machines and when.  But the browser extension ecosystem really hasn't yet matured to that level.



So anyway, I just think that in the same way that it's good to occasionally curate the apps that Google, Facebook, and Twitter now have been given permission to access our accounts against, it's worth taking an occasional survey of what extensions our browsers have accumulated and deciding whether we're still needing them or using them or not.  So anyway, I was stunned by the graphic of the number of extensions in browsers.  And when hearing, for example, about one to aid a visually impaired person to see something like a pie chart that they wouldn't otherwise be able to appreciate, I think that's very cool, and clearly super useful.



The other problem is the whole thing is about reputation.  Mozilla and Chrome and Microsoft have earned, have hard won reputation.  We believe that the browsers themselves are operating in our best interests because it is in their best interests for them to do so.  The problem with extension makers is we don't, for the most part, we don't know who they are.  I mean, I know LastPass, and I've got the LastPass extension installed in all my browsers because I want the services it offers.



But, boy, you look at some of these extensions that just come from an unknown author offering something that seems at the moment like something we want, but this person has no reputation.  How do you make a decision to install this in your computer?  I mean, the browser is in a position to see a huge percentage of what we do.  And what I think users don't appreciate is this is global.  Extensions have global visibility into what we're doing, not only when we call them up or only when we use them, but they're sitting there in the catbird seat, loving the fact that they're able to see everything going on everywhere in the browser.  And I really think we just do have at this point a very immature system for dealing with this whole issue.



JASON:  Well, it ends up being out of sight, out of mind, too.  Like in order to actually see these things running, like sure you can look up in the corner of your browser, and you might see those icons.  After a while that almost becomes something that you're so used to seeing that you don't really scrutinize it to any degree.  In order to actually see your entire list, you have to go into the settings.  And how often do you go into the settings and check your extensions?  I've been scraping the barnacles while we're been having this conversation here.  What I'm realizing when I look at it is about half of those extensions are actually Google extensions - Google Hangouts, or Google has this password alert extension that protects you against noticing phishing attacks, Google Docs offline.  So some of these are Google.



STEVE:  Right.



JASON:  And when you're talking about trust...



STEVE:  I agree.



JASON:  You know, Google already knows everything about me.  So I would leave that on there.



STEVE:  Well, it is their browser that we're adding the extension to.  And I like that.  There I would consider Chrome to be modular in that you're able to add Google modules to Google.  They certainly know how to make a module that won't upset the browser and hurt the user's experience and so forth.



JASON:  Hope so.



STEVE:  And so I like the idea of, for example, I don't have to have those Google things added if I don't want them.  You're able to.  And so you've extended Chrome.  And there's the perfect example of the fact that Google has a reputation.  I wouldn't hesitate, if I wanted those features, to add them to Chrome because I know where they're coming from.  But, boy, you look at some of the shady characters that are, I mean, we just don't know who they are.



JASON:  Right.



STEVE:  And it's like, yeah, come on in and watch everything I do with my browser.  No.



JASON:  Well, and I think what you said earlier about impulse, too.  Like so often over the years we've become accustomed to the fact that there is literally an app or an extension or a website or anything that will appeal to that momentary impulse that we have to need something very specific.  And at that moment in time it seems like it's so important.  And you find the extension, like, oh, yeah, sure, I'll add that.  And then you forget about it because it was useful then.  And then months or years down the line, like you just don't even know that it's there anymore.  And it's still, like you say, surveying the landscape.  So it's important to go in there and clean it up.



STEVE:  I have so much crap on my iPhone, I look at it, and I just, it's like, okay, someday I'll need to deal with this.  But, I mean, I can't think of a better example than, you know, the barnacles that I have on my iPhone and my iPad.  But it's like, okay, fine, I still have lots of room for more barnacles.



JASON:  I mean, this experience matches up to going into, like you were saying, social media accounts, going into Twitter and taking a look at all the accounts that you've connected through Twitter.  I did this with Facebook.  Like I hardly ever use Facebook anymore, but probably like a half year ago I went there and checked the apps connected to Facebook list.  And, I mean, it was huge.  It was like a hundred different things dating all the way back to like 2008 - apps, you know, services that don't even exist anymore, yet they're still connected somehow.  And so I just - I basically just crossed my fingers and said disconnect everything, even the things that I think I might need now.  Like, whatever, I'll figure it out.  Just get rid of it all.



STEVE:  Right.  If something breaks, then you can put it back.



JASON:  Exactly.  And you know what?  I've been fine.  I haven't even run into a single speed bump, so I'm happy I did it.



STEVE:  Yup.



JASON:  Steve Gibson, this was a lot of fun.  We've reached the end of Security Now! for this week.



STEVE:  And for you.



JASON:  And for me, until the next time Leo gallivants, as I like to say, around the world, as he is wont to do and as he deserves to do, take vacations from here from time to time.  And I would be happy to sit in in his place when he does so next time.



STEVE:  I know I can speak for our listeners when I say we will be happy to have you back, Jason.



JASON:  Thank you, Steve.  GRC.com if you want to check out everything that Steve's been up to.  Of course he talked a lot about SpinRite today.  It is the best hard drive recovery and maintenance tool, so you can find your copy there.  Also information about SQRL.  Audio and video of this show.  The only place you can go to find transcripts of this show can be found there, as well, GRC.com.



And then of course our website is TWiT.tv/sn for Security Now!.  You can find our audio and video there.  Podcast links if you want to subscribe, if you haven't already.  I'm sure you're already subscribed.  But if you haven't, go there and subscribe.  Everything you need to know is there.  And the show records live every Tuesday, 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC.  And if you want to watch it live, you can:  TWiT.tv/live.  Like I said, if you're subscribed, you'll get it automatically each and every week.  And maybe just do both.  Then you're covered.  Thank you, Steve.  Really appreciate it.



STEVE:  A pleasure, Jason.  Until we meet again.



JASON:  That's right.  Until next time.  And we'll see you all next week, Leo will see you next week with Steve on another episode of Security Now!.  Bye, everybody.



STEVE:  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#682

DATE:		September 25, 2018

TITLE:		SNI Encryption

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-682.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we look at additional changes coming from Google's Chromium team, another powerful instance of newer cross-platform malware, the publication of a zero-day exploit after Microsoft missed its deadline, the return of Sabri Haddouche with browser crash attacks, the reasoning behind Matthew Green's decision to abandon Chrome after a change in release 69 - and an "Ungoogled Chromium" alternative that Matthew might approve of - Western Digital's pathetic response to a very serious vulnerability, a cool device exploit collection website, a question about the future of the Internet, a sobering example of the aftermarket in unwiped hard drives, Mirai Botnet creators working with and helping the FBI, another fine levied against Equifax, and a look at Cloudflare's quick move to encrypt a remaining piece of web metadata. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  I'm back.  And we're going to talk about a lot of things, including some things about the new Chrome that are making people upset, the bifurcation of the Internet, and a hack that takes advantage of a massive flaw in a NAS from Western Digital.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 682, recorded Tuesday, September 25th, 2018:  SNI Encryption.



It's time for Security Now!, the show where we cover your security and privacy online.  I'm Leo Laporte.  Yes, I'm back.  He's never left, Steve Gibson.  Do you ever go on a vacation ever?  I don't remember you ever going on vacation.



STEVE GIBSON:  I never have, no.  There's nowhere I would rather be than right here, working on code and...



LEO:  Oh, come on.  Come on.



STEVE:  No, it's true.  It's sad.  It's sad, but true.



LEO:  Well, it's not sad.  It's actually great.  It's certainly more economical.



STEVE:  Well, I'm just happy where I am.  Lorrie's figured out...



LEO:  But, yeah, doesn't your - yeah.  Doesn't Lorrie want to go somewhere?



STEVE:  She would love to, and she's figured out, well, okay, I'll just find some girlfriends and go with them.



LEO:  Go by herself, yeah.



STEVE:  I said, "Great, honey.  We'll FaceTime."



LEO:  Where does Lorrie want to go?



STEVE:  Everywhere.  Anywhere.



LEO:  See, I love to travel.



STEVE:  She's normal.  She's normal.



LEO:  Where should we send Steve?  You should go to Russia.  You know where you should go is Estonia, the digital capital of Central Europe.



STEVE:  They're really ahead of everybody else.



LEO:  Yeah, yeah.  You'd be welcome there.  People would gather 'round.  Hasn't anybody ever invited you to a conference or anything like that?  Of course they have.



STEVE:  No, I get that all the time, yeah.



LEO:  And you just don't go.



STEVE:  Yeah, it just doesn't make any sense.  It's like, you know, I think the last thing I went to is RSA when I discovered Stina and Yubico.  So that was good.  But anyway...



LEO:  This is the guy who's lived on ham sandwiches for 20 years.



STEVE:  We're at Episode 682, the last episode of September.  And Leo, you missed it last week because the Presidential Alert was originally scheduled for last week, but it's been postponed until next Wednesday.



LEO:  But wait a minute.  What do you mean?  What Presidential Alert?



STEVE:  We talked about this on the TWiT that I did with you two years ago.



LEO:  Well, I know the capability exists.



STEVE:  It's happening.  Trump is going to tweet.



LEO:  Why?



STEVE:  It's actually, well...



LEO:  Is it a test?



STEVE:  Well, it's a FEMA test.  And every cell phone which is turned on.  And apparently the title is Presidential Alert.  And it starts out...



LEO:  It's the one alert you can't turn off on any phone.



STEVE:  Correct, correct.



LEO:  Supposedly only to be used in grave, dire national emergencies.



STEVE:  Well, but of course you've got to push the test button every so often.



LEO:  We've never - has it ever been tested?  Never.



STEVE:  Never been tested.  It's the first time it's ever going to be done.  And it was the weather back in the Southeast caused FEMA to say, you know, we don't want to confuse people with a test alert during a crisis.  So they're bumping it to October 3rd, which is Wednesday, a week from tomorrow.



LEO:  Is it during - what time is it?



STEVE:  Oh, it will be.  Yes, it'll be during your Wednesday podcast.  I think it's some bizarre time like 2:18 p.m. Eastern.



LEO:  Oh, great.  It'll be right in the middle of Windows Weekly.  Oh, boy.  Well, that's good.  Now, there's extreme, extreme penalties for rebroadcasting an EANS alert on the radio.  And I know this because some DJ in New Orleans did it on - he worked for the company, the radio company I work for.  And as a result, every three months I have to take a test, which mostly consists of "Is it ever okay to rebroadcast a sound that sounds like the EANS signal?"  And the answer is no.  "Thank you, you passed the test, and then we'll talk to you again in six months."  But this is going to be coming over the phone.  I presume I could just hold it up to the microphone; right?



STEVE:  I don't know if it makes any sound.  It's a text message.  So, you know, it's an SMS.



LEO:  Oh, no, it makes a sound.  Oh, you haven't - you must have turned off the Amber alerts.



STEVE:  Oh, yeah.



LEO:  It makes a loud, annoying sound, particularly if every cell phone in the room goes off at the same time.  It's extremely loud.



STEVE:  Either we don't get them, or everybody I know has turned them off because I've never had that...



LEO:  Yeah, everybody's turned them off.  But you can't turn this one off.



STEVE:  Well, there's an effective alert system.



LEO:  You will hear a loud noise.  It will go [mimicking loud buzzer].  And then it doesn't say anything, it just pops up.



STEVE:  It's like our cable TV provider.  It's like they're saying it's a monthly test, but I swear it's like every other week.



LEO:  Oh, that test I hate.



STEVE:  It just shuts everything down and [mimicking loud buzzer].  It's, like, so annoying.



LEO:  So just out of curiosity, does the President get to write the announcement?  Or is FEMA going to do it?



STEVE:  No, no.  This one, this test, is prewritten.  We know what it's going to say.  I had it in our show notes because...



LEO:  Because you know if President Trump was given the right to write that, he would do something wild.



STEVE:  Yeah, he would have fun with it.



LEO:  So to speak.



STEVE:  Yeah.  And it was almost three years ago that you and I and - who was our co-host?  Because I came up for the holiday TWiT.



LEO:  Yes, right.



STEVE:  Or tweet or whatever.  TWiT, yeah.  And that was when we got the news that this was a new thing that was going to happen.  And of course we'd had the election already, so we knew that it was going to be President Trump who would then have his finger on the button.



LEO:  Yes, we talked about it, that's right.  That's right.



STEVE:  Yeah, exactly.  So anyway, the first one happens a week from Wednesday.



LEO:  Well, that'll be interesting.



STEVE:  In the meantime - yes, it will.  And I'll remind everybody next week, of course, too. 



LEO:  Yeah, please do because I have to be prepared for it on Wednesday.



STEVE:  Yes, you do.  So we've got - you missed a little bit of news.  I realize you were sort of offline probably.



LEO:  I did not follow what was going on at all.



STEVE:  One of the things that Google did that was quite controversial...



LEO:  Oh, yeah, I did follow this.



STEVE:  ...was they decided to stop displaying the www dot in front of domains that actually have that.  And they also decided - they called them "trivial domains," that one and "m dot."  Well, there was a huge outcry because they just sort of unilaterally surprised the world with Chrome 69 that sort of just stopped showing it.  



LEO:  Well, Safari's done that for a long time, but you can turn that feature off.  Safari doesn't even show - they don't even show the domain name at all.  And I remember I interviewed Tim Berners-Lee, who created this whole mess.  And he said, "I never expected any of this stuff to be visible to the end user.  This was for computers."



STEVE:  Yes.  And we've often talked about how hostile https://www dot, it's just like, our moms should not have to be dealing with that.  So we've got additional changes coming from Google's Chromium team that we're going to talk about.  We have another powerful instance of newer cross-platform malware.  Now it's all being written in Python so they can have multiplatform binaries in order to have a longer reach.  We've got the publication of a zero-day exploit after Microsoft missed its deadline a couple weeks ago for September's Patch Tuesday.  We're all expecting maybe they're going to make it for October.  We've got the return of Sabri Haddouche, who you also wouldn't have heard about, probably, Leo, while you were on vacation.



LEO:  No.



STEVE:  But he's the guy who was, while looking for DDoS attacks on web browsers, figured out how to crash iOS.  And I tried it last week, both pre- and post- version 12.  And sure enough, it crashed Safari and took iOS down with it.  So he's back with some DDoS browser attacks, which is what our Picture of the Week refers to.  Also, we have the reasoning behind Matthew Green's decision to abandon Chrome after a change that they also made in release 69 which has also let loose a firestorm on the 'Net.



LEO:  Yeah.  This is the one I thought you were talking about, yeah.



STEVE:  Yeah.  We've also got something called the "Ungoogled Chromium," which is an alternative to Chrome that Matthew might approve of.  We have Western Digital's pathetic response to a very serious vulnerability, a cool device exploit collection website that I just discovered when I was pursuing that previous story.  A question about the possible fracturing of the Internet in the future that Eric Schmidt brought up at a conference last week.  A sobering example of the aftermarket in unwiped hard drives.  The Mirai botnet creators have been hired by the FBI.  Another fine has been leveled at Equifax.



And believe it or not, if we have any time, we'll actually get around to the topic of this week, which is SNI Encryption.  That's the Server Name Identification which is still being sent in the clear, even with the latest TLS v1.3 encryption, which is sort of metadata.  You know, we've often talked about how, yeah, stuff is encrypted, but you can still see who you're sending it to.  And so although you can't see into it, you can still see like the envelope.  So Cloudflare has quickly moved to adopt an interesting addition to TLS v1.3.  And Cloudflare just continues to do really neat stuff.  So I think another jam-packed episode of Security Now!, with lots of interesting stuff for our listeners.



LEO:  You've got us all champing at the bit.  I can't wait to hear all of that stuff.  Before we do, though, have you installed iOS 12?  I guess you have, gather you have.



STEVE:  Oh, yeah, yeah, everywhere, yup.  And I agree with you...



LEO:  And, now, I remember you hated 11.



STEVE:  Well, what I hated was that 10, the last version of 10 where they crippled the performance, preemptively fearing that the battery wasn't strong enough.



LEO:  Right.



STEVE:  And on that 6S or that 6 Plus or whatever it is, the big boat that I have, even now, now we have that beta of the battery test, and it says 100%.  Your battery's in perfect condition.  Even so, they forced me to go to an iPhone 10.  However, I'm quite happy where I am now.  But yeah, so iOS 11 kind of began to get it better, and it runs just fine on 12.  And I'm with you and Megan.  I really like 12.



LEO:  I do, too.



STEVE:  I like the things that they've done.



LEO:  I keep getting calls, Steve, from Apple.  I don't know if you heard me mention this on Windows Weekly yesterday.



STEVE:  I did, I did.



LEO:  Got this this morning.  I could play it back for you.  It's the funniest thing.  I just got another one while I was doing that ad.  It basically says this is Olivia calling from Apple Support.  Your iCloud's been breached.  Don't, whatever you do, log into your iCloud.  Don't use your Apple device.  Call us at this toll-free number.  Which keeps changing.  So I just thought, well, I guess it could be true.  So I logged into my - because I figured, well, if it is true, I'll certainly have, you know, I'm using two-factor.  I doubt anybody's compromised my iCloud.  But if it is true, I'll certainly have an email or something from Apple.  So I log onto iCloud, and of course everything's fine.  It's just a scam.



STEVE:  Yeah.  I wonder if they've, like, used up all the Microsoft support people.



LEO:  I think that's probably it; right?



STEVE:  Because they've switched over to Apple.



LEO:  Unbelievable.  Unbelievable.  And they've called me now five times.



STEVE:  And it is interesting that, I mean, it does plant that seed of doubt.



LEO:  Yes.  Yes.



STEVE:  It's like, well, I don't think this is real, but, you know?



LEO:  It's scary, yeah. 



STEVE:  And I get spoofy things sometimes, too.  And you do just, like, your Google account has been compromised.  And it drives a savvy person to go just make sure that isn't true.  So at least we're not clicking on the link and just saying, oh, yeah.  Give me a call.



LEO:  And the reason I bring it up is I know anybody who listens to this show is going to go, yeah, right.  But tell your friends and family they're doing it because Apple just announced new products.  Everybody's getting new iPhones.  And so there's some credibility to it.  So tell your less savvy friends and family don't believe this thing.  It's bogus.  It's not real.



STEVE:  Yeah.  So our Picture of the Week relates to a story that we'll be getting to in a minute.  When we talked about the iOS crash last week, Sabri had not put all of this together.  Now there is a site where you, too, can go to crash your browser.  It's ReaperBugs.com, R-E-A-P-E-R-B-U-G-S dotcom.  And if you go to - it doesn't crash the moment you get there.  You get to choose which browser you want it to stress.  And so there's Reap Chrome, Reap Safari, and Reap Firefox.



LEO:  I love it.



STEVE:  And we will get to this in more detail later on.  But it is just sort of a - it's sort of a kick that he created a website to host his various browser crashes.  And it probably isn't such a big problem.  I mean, again, we'll talk about that.  But for the title on the picture I said, "First we crash; then we burrow inside."  And we do know that there are some classes of crashes which can be the beginnings of the development of an exploit.  But in a world where our browsers are running code with ever-increasing performance, it's not necessarily the case that we're doing anything other than just abusing the power that code from some specific source, typically untrusted, is providing.



LEO:  Is it safe to do this?  Should I, I mean, this is a live picture from my Safari.  Should I...



STEVE:  Well, if you don't mind crashing it, it does crash it.  I mean, it'll just - and we know why.  It's a WebKit problem where one of the newer features of CSS allows the blending of the background.  And the guy created some, I don't know how many hundreds of nested divs, each which invokes this blend operation and brings iOS, or brings Safari and in turn iOS to its knees.



LEO:  Oh, I clicked the wrong button.  I'm in Chrome.



STEVE:  Ah.



LEO:  That's why nothing happened.  Let me refresh and do it in Chrome.  There we go.  Okay.  Are you sure, he says.  Yes, crash me.  Now, I am on Mojave now.  I don't know if that will protect me.



STEVE:  Ah, that's interesting.  And they just updated that.  Because I did check it, and it crashed 11, whatever the last version of 11 was.  And then also I updated to 12, and it crashed that, too.



LEO:  Well, it looks like Google might have put in some protections because it doesn't seem to be crashing anything.  So, huh.



STEVE:  Okay.  So speaking of Google, they're continuing to mess with Chrome's UI.  So first they took away www and "m" as the prefixes.  Then they came back.  Then they announced that they were going to still take away www, but "m" is remaining.  What they learned kind of the hard way, and this is why many people who were really upset with this a couple weeks ago argued that this should not just be something that Google can do unilaterally.  You know, messing with the address is messing with Internet protocol.  And, for example...



LEO:  They're not messing with the address.  They're just messing with what the Chrome displays; right?  I mean, the...



STEVE:  Well, except, for example, the "m" signifies you want the mobile version of a site.  So the user has some control over whether they want "m dot" or not.  So Google sort of missed this.  They were just assuming that www was superfluous.  And they learned that, whereas www may be, "m" can actually have some important meaning.  And so, yeah, I mean, how many times have we talked about that this is not the way this should have all evolved, where we've got all this cruft on the beginning of our domain names.  And I talked about how GRC used to accept either URL and always took you to the same place.



But then we ended up with Google search results being a mixture of www and non-www.  And so, like, links were all diluted and confused.  And so I began redirecting everyone to www.  Now I'm sort of regretting that decision.  It was maybe 15 years ago, and I maybe should have just said, okay, if you put in www I'm going to redirect you just to GRC.com.  But anyway, Google has now decided that the file:// is also - that doesn't make any sense to put in the URL.



So with Chrome 70, the next release of Chrome, they're going to be adding what they call a "chip," which is the visual flag to the left of the URL, which will just say "file."  And then they're going to reformat it so it just says, for example, c:\, you know, the normal path that you see in your OS.  So that's another thing that they're going to be changing with Chrome 70.



LEO:  But to be clear, this is merely what is displayed to the end user; right?  I mean, there's still internally file://.  That's the protocol.  And there's still internally "m dot" whatever, or www dot whatever; right?  It's just what's showing up in that browser URL bar; right?



STEVE:  Correct.  But the idea was...



LEO:  It's an indicator, I understand, to the end user that this is what you're getting.



STEVE:  Right.  Except if you do remove something like the "m dot," which is semantically significant, that is, users may need to know are they at the mobile version of the site or not.  So is it prefixed by "m dot"?  And so in hiding this, Google was preventing users from seeing something relevant to where they were.



LEO:  Yeah.  You know why they don't, because it's not a standard.  It's a practice. 



STEVE:  Right.



LEO:  And some people write "mobile."  Some people write "m."  There's no standard on what that prefix should be; right?



STEVE:  Well, except that so the purists, you know, the Richard Stallmans and so forth of the Internet, I mean, really got upset with the idea that Google was deciding that www dot was so-called a "trivial subdomain" and could simply be removed from the URL.  I mean, the rest is still being displayed, but they  just decided, well, this is not - it's just superfluous.  And so as a consequence of the outcry, it came back.  And so but they're still saying that they're going to talk to standards bodies and see about coming up with some sort of uniform representation that everyone agrees upon.  I mean...



LEO:  Apple did this a long time ago.  They stopped showing the fully qualified domain name in the browser bar.  They just showed you, if you're at Yahoo, you're at Yahoo.  It doesn't say Yahoo.com, even.



STEVE:  Right.



LEO:  And then there's a setting.  You can turn that off.



STEVE:  Yes.



LEO:  But I think Apple's attitude was, well, users don't need to see all that extra cruft.  Especially since it's not a standard.



STEVE:  Right.  And I think where we are is just sort of, you know, in sort of this awkward place where we're moving into a different place where the standards that we've had that built the Internet are being, like, questioned, exactly like this.  Is there a need for www on the front of the URL?  And at this point we don't have a consensus.  And so people got upset because Google just decided, well, we're going to change this.  And they are the major browser on the Internet.  They are the majority browser.  And what was I going to say?



LEO:  While you're thinking, let me just show you what Apple has decided in this regard.  So I have it checked right now, "Show full website address."  But I can uncheck that.  And then instead of seeing up here the fully qualified address, I'm just going to see Twitter.



STEVE:  Yes.



LEO:  Or Twitter.com.  If I turn that on, I'll see this long URL.  And you know what's missing is the actual URL for the site.  It's just showing you're on Twitter.  You can, if you copy it, you will get the fully qualified URL and everything after it.



STEVE:  Yeah.  Well, and you know, Leo, I think partly this is also a reaction to how incredibly ugly URLs have become.



LEO:  Yeah, yeah.  Look at the difference.  Do you really want to see this?  But you could make the argument, I could make the argument, well, yeah but I need to know this is the Verge's account, and this is a particular status from the Verge, instead of just, oh, generic, bland, I'm on Twitter.com.  At least Apple gives you the choice here.  But I don't think...



STEVE:  And so giving the user choice, yes.



LEO:  But the default is, by the way, the simplified version.  And I bet you 99% of Mac users don't even know that.  They don't.  They're not aware of it.



STEVE:  Right.  So what's interesting is that apparently Lawrence Abrams of Bleeping Computer picked up on this.  Apparently Google is experimenting with the same thing on their search results.  In his posting, Lawrence wrote:  "Google really wants to get rid of the www subdomain."  He says:  "First we had Google removing www in Chrome 69 address bar, and now there is some test underway to remove it from search results, as well."  He wrote:  "I was first alerted to this when one of BleepingComputer's reporters noticed that the BleepingComputer domain was showing up in Google search results as just https://bleepingcomputer.com," although officially it is www.



He said:  "When I checked from my end, though, it was showing it listed as normal with www.bleepingcomputer.  While researching this behavior, I found many domains where Google was removing the www subdomain in the search results.  Once I performed a refresh of the page" - meaning the search results page though - "the normal www subdomain would be listed again.  In some cases, I could refresh over and over," he wrote, "and the results would switch back and forth between www and non-www.



"Ultimately," he says, "I could not get BleepingComputer.com to show the non-www version, so I found another site that was also performing this behavior.  When I searched for Palo Alto Networks, it showed the domain listing without the www subdomain.  If you clicked on the search result, the site would perform" - and this is interesting to me - "a 301 redirect to www," which means they weren't just changing the visual, they were choosing not to - the URL itself did not have the www in it, so that Palo Alto Networks was doing a 301 redirect over to www.  He says:  "...which is the site's desired behavior."



He said:  "On a refresh of the search result page, the normal www version of the URL appeared again in the search results.  This time, though, the site links have been changed," he said, "to a smaller display under the domain description."  And he goes on.  But my guess is, as we know, Google has a massive server farm, and he was probably getting different servers picking up his query when he refreshed; and some of them for whatever reason were not showing www, and others were.  And he did ask Google.  He got no response from Google one way or the other to what they were doing and what was going on.  But we do look like we're in some sort of flux here with whether or not or how Google is going to treat the www prefix on domain names.



LEO:  It's an interesting question because - and I would hope, and this isn't the time to do it, but maybe at some point we could talk about how this all works.  Because I don't need - www used to be the thing before the domain name.



STEVE:  Right.



LEO:  Mail.www.whatever was the name of an actual machine; right?  Is that...



STEVE:  Correct.  Yes.



LEO:  I think for a long time that's not been the case.  And in most cases a site doesn't need www dot anything.



STEVE:  That is correct, yes.



LEO:  That was to indicate that you're going to go to the machine that has the web server Apache's running on that machine.  If you want to go to my email server, Postfix is running on mail dot.



STEVE:  Correct.



LEO:  But nowadays that's all handled with DNS in an invisible way; is it not?



STEVE:  Well, actually it's more likely handled by port numbers because we have well-known ports.  And so, for example, GRC's news server is at news.grc.com.  It doesn't need to be because the news server uses port 119.  And so it could just be GRC.com, and the news client knows to connect to port 119, in the same way that the web client knows to connect to 443 or 60.  So you're right, Leo.  It's sort of an old-school, you know, you have the domain name, and then you have the machines that are accessible on that domain name.  And there are supposed to be prefixes of the domain name.  But it's not necessary.



LEO:  No.  It's not handled that way anymore.



STEVE:  And in fact I can't even think of, I mean, now, okay, the "m dot," there's a place where, as you said, by convention rather than by standard, by convention the "m dot" has been a prefix which takes you to the mobile version of a website.  But even there it could be the same IP.



LEO:  Right.



STEVE:  But when the URL comes up with an "m dot," it gives you different content, which is optimized for mobile screen sizes.



LEO:  Well, the perfect example is Twitter.  If you go to m.twitter.com, you get a progressive web app for Twitter which is intended for mobile.  But you know what, I just did it, and Twitter rewrites it as mobile.twitter.com.  So it's not in fact the canonical name of that server.



STEVE:  Right.



LEO:  And it's rewriting the name anyway.  And there is no standard for what "m dot" means.  So I think Google's attitude was, well, why should the end user be concerned about this?  My only concern - I think it's probably yours, too - is from a security standpoint you kind of do want to know the fully qualified name that you're going to or that you're at or that that link points to.  Right?



STEVE:  Yeah.  I would say you at least want to be able to get to it if you want it.  When Google was doing this - and I did experience it briefly.  It was funny, too, because, Leo, I put in www.GRC.com, and sure enough it showed me GRC.com.  Then I put in blog.google.com, and it converted it to blog.google.



LEO:  Right.



STEVE:  And I thought, wait, what?



LEO:  They own dot google.



STEVE:  I thought - exactly.  First I thought they were taking away the dot com.  And so then I put in blog.grc.com, and it didn't do anything.  So I thought, wait a minute, what's going on?  But it then, as you and I had talked about once before, they have dot google as their top-level domain.  So, yeah.  But when it was doing that, I could click on the URL twice.  I think once did something, but it didn't show it to me.  And when I clicked again, then it showed me the unredacted full URL.  And it was like, okay.  So, I mean, so it was available if you wanted to have it.  And even then there was a setting deep down in the Chrome://flags that allowed you to get to that and turn that behavior off.  So purists could still have it their way.



Speaking of Palo Alto Networks, which was the domain that Lawrence at Bleeping Computer thought to try next, they've found this malware that I was referring to.  They named it XBash.  It's written in Python.  It's like the Swiss Army knife, unfortunately.  It's a self-propagating worm which targets both Windows and Linux systems.  It's a botnet.  It's a cryptocurrency miner.  It's a ransomware spoof that deletes a victim's databases and demands payment while being unable to restore the destroyed data, thus the spoof of ransomware.



It was found in the wild by the guys at Palo Alto Networks, and it's been tracked back to a Chinese-speaking APT, an Advanced Persistent Threat actor known for previous cyberattacks using similar attack mechanisms.  They said it has nascent functionality, without specifying what, that would allow it to spread quickly within an organization's network, but they hadn't - but that was, like, still not activated in the code that they had seen.



What's a little disturbing about this, I mean, you know, you don't want any of this.  Nobody wants anything like this on their network, on their servers.  But it hunts for vulnerable, unprotected web services using well-known ports and deletes the data from many different popular databases, including MySQL, the Maria database, Couch, and Mongo running on Linux servers.  It scans a targeted IP, both over TCP and UDP, for the well-known ports HTTP, VNC, the well-known database ports, Telnet, FTP, RDP, ElasticSearch, Rlogin, and others.



So it's, like, scouring for any way in.  If it finds a listening service, it uses a brute-forcing, assuming the use of weak usernames and passwords in a dictionary attack, and apparently it contains a large dictionary, which it's finding effective in getting in.  And then, if it is in, it deletes the databases that it's able to gain access to, completely just deletes them.  Doesn't encrypt them, like misbehaving ransomware.  I was going to say "well-behaving ransomware," but it's misbehaving.  But in this case it just deletes them and displays a ransom note asking for payment, even though it does not have the capability of restoring it.



LEO:  It's deleted.



STEVE:  Like, why bother with that functionality?  We've already got their money.  Good luck.



LEO:  Yeah, of course.



STEVE:  So it's known to have infected at least 48 victims who have apparently, says Palo Alto Networks, it's not clear how they know this, but who have paid the ransom, totaling about $6,000, and never received their data back.  There's no indication that any data recovery happened afterwards.  Anyway, so the...



LEO:  I paid my money.



STEVE:  Exactly.  Can we please have our SQL database back?  Oh, goodness.  Okay.  So it's written in Python.  And then the PyInstaller system is used to convert the malware into multiplatform, standalone, executable binaries which can infect Windows, Apple macOS, and Linux.  Which of course allows it to run cross-platform everywhere.



Okay.  So I was thinking about this.  Okay.  So here's the news.  Multiplatform, really bad malware - oh.  On Windows it only runs a cryptocurrency miner.  It does not install its botnet functionality.  But it does that under Linux.  And it wasn't clear whether it deletes Windows databases or only Linux databases.  But still, so what's our takeaway?  And I was thinking about this.  First of all, we know no services should ever be publicly exposed unless they need to be publicly exposed.  Like, for example, there have been so many instances where somebody's database is installed on a server.  And what is the MySQL port, 3369 or something?  I can't remember now what it is.  But it just - it opens the database service port on the WAN interface.  It's like, it's crazy.



And, I mean, it is the default when you install MySQL from Oracle.  It's like, oh, yes, we're going to make these services available.  But hopefully any admin who knows what they're doing is saying, okay, we don't want to expose - we don't want to put that on the Internet.  So first of all, no service that doesn't have to be publicly present should ever be.



And the other thing is, think about it, we're well past the point where users are logging in; right?  Other client systems are connecting to a database backend.  Other client systems, even if it's a news reader, you know, your news reader, not you, is connecting to the news server.  So all of these usernames and passwords should be, as long as they can be, absolutely random gobbledygook.  It's like go to GRC.com or wherever, like our passwords page, and just get some gibberish.  That should be what you use if you have to have a service publicly exposed.  No person ever is going to be typing that in.  Some other client is going to be connecting to the service.



So the idea that usernames and passwords still need to be user memorable, like that ship as sailed.  And yet, obviously, for this thing to be doing a dictionary attack and for it to be successful says that people are still setting up servers imagining that a user is going to be manually logging into something, which is almost always a backend for some other client, which then can be given the responsibility of remembering how to log in.



So just something for our listeners to think about is, you know, as you were just saying, now that we have a world where we have LastPass to do the logging in for us, I was explaining last week or the week before that to sort of put my own work with SQRL into context, we're still in a mindset of a dumb terminal where back in the day, when you and I were at college, Leo, there was some CDC mainframe, and we were using a Hazeltine or a Lear Sigler or some terminal.  And we were, like, logging in with our username and password because this was a dumb terminal.



Now we're all using computers to log in.  So on one hand we can have password managers that remember for us; or, in the case of SQRL, the work I'm doing, we have computation.  There's some computability at our end, so it's feasible for the thing we're logging into to send us a challenge which requires some computation on our end.  Which now everybody has.  In every device that we're logging in with, it's not just a memory machine, it's a computer.  So anyway, I just sort of want to further plant this notion in people's minds that the era in which it's necessary to remember a username, and especially a password, is really - that's just long gone.  Yet obviously people are still doing that.



LEO:  Not really long gone.  It just should be long gone.



STEVE:  It's not as long gone as we wish, yes.  Okay.  One more, and then we'll take our second break.  Microsoft missed a vulnerability deadline.  So there is now in the wild, with a proof of concept posted on GitHub, a zero-day vulnerability for the Windows Jet database.



And it must be that Microsoft wasn't that worried about it because Trend Micro responsibly disclosed on May 8th to Microsoft that they had found a way in which, if the Jet database engine - which is now integral into all versions of Windows.  It's down there and used for various things in Office and Access and so forth.  If it opens a malformed database, there's a buffer overrun which has been found which can cause the Jet database engine, and we know where this is going, to overflow a buffer and run code that is also contained in that database file for a remote code execution vulnerability.  On the other hand, you have to somehow arrange to get the Jet database engine to do that.  And hopefully, once again, it's not publicly exposed.



But so what happened was, on May 8th, Trend Micro's Zero-Day Initiative reported the vulnerability to Microsoft, and Microsoft acknowledged.  Let's see.  On the 14th of May, they successfully reproduced the issue and confirmed.  Then nearly four months goes by.  On September 9th they reported an issue with the fix, whatever that means, meaning like, oops, maybe they weren't going to have it out in time.  I don't know what the issue was.



Trend Micro said, well, you know, time is up on this, and we're going to hold you to it.  On September 11th they confirmed that the fix did not make it into the build for September's Patch Tuesday.  And on September 12th Trend Micro confirmed their intention to release it, pursuant to their zero-day schedule, which they did on the 20th, which, what, was last Monday?  So, wait, no, the 20th.  Today's the 25th.  So middle of last week.



So at this moment on GitHub is a full exploit proof-of-concept for a Windows zero-day which is effective across all versions of Windows.  We can presume that this has Microsoft's attention.  I mean, Microsoft knows about it.  And it felt like they were trying to get it into the September Patch Tuesday.  We can presume it'll be there in the October Patch Tuesday and also that Microsoft's not that worried about it.  They're saying that user interaction is required to exploit the vulnerability inasmuch as the target must visit a malicious page or open a malicious file.  On the other hand, visiting a malicious page is not a high bar to reach at this point.



So I imagine probably next week, which will still be before Patch Tuesday, we may be talking about some in-the-wild exploitation of this Windows zero-day because what we're seeing now, the trend is bad guys know they don't have a large Window of opportunity before these get closed.  And so they're just jumping on them immediately.  This is worse than a privilege escalation or elevation.  This is a bad guy running their code on your computer.  And if you can actually do that with a web page, then I think that's pretty serious.



LEO:  Yeah.  And you're right, I mean, many web pages that are even responsible, well-run web pages sometimes have malware on them.



STEVE:  Because they host malicious ads.



LEO:  Yeah, right.



STEVE:  Because they just have URLs to some ad service; and it's like, oh, show anything you want to here.



LEO:  Right, right.



STEVE:  So, okay.  I don't know how much we care about this, but if nothing else it's a fun hack.  We talked last week about Sabri Haddouche, who, while looking for browser DoS, browser Denial of Service things, he encountered this iOS hack which, when I tried it, as I said, brought my iOS, my Safari WebKit, because basically it's a WebKit vulnerability, brought it to its knees.  I don't know if it's a vulnerability.  So, for example, in that case, there's a feature of CSS which allows a CSS item to ask to have its background, that is, the stuff behind it, blurred or color skewed or made into monochrome.  It's fancy.  So it requires image processing to display this object.



And what occurred to him is that, since you can nest divisions, "divs" as they're called in CSS or HTML, you could set the CSS, the cascading stylesheet, the style of the div element, to require blurring of its background.  And if you then nested them,  you would be nesting these blurrings or whatever image processing you required.  And to faithfully do that, you'd have to go to the farthest back division and have it perform its blurring, and then have the resulting image given to the next  higher in the hierarchy nested division and have it do its blurring.



Anyway, you could see that this becomes very processor intensive.  So unfortunately it crashed.  And I guess I would argue that it's bad if somebody can design something where you could just go to a website, and it crashes you out.  I mean, it doesn't just freeze your browser, it reboots iOS.  At least it did then.  And here on this web page at ReaperBugs.com, he's got three buttons:  Reap Chrome, Reap Safari, and Reap Firefox.  And it looks like, on Chrome, it looks like it'll bring 69, both Chrome and Chrome OS 69.0 and earlier, which is where we are now at Chrome 69, it'll crash it.  And for Safari he shows iOS and macOS from 9 to 12.0, it'll crash those.  And also there's one for Firefox, 62.02 and earlier.  And I don't remember where Firefox is right now in its - let's see.  About Firefox, yeah, 62.0.2.  That's the latest version that is in the main channel.



So as I said before, if this was a glitch in JavaScript processing, which could potentially be leveraged into a buffer overrun that could allow a bad guy to run code on your machine when you visit a web page, much like maybe that Microsoft Windows Jet database problem that zero-day we mentioned is, then that would certainly be a problem.  But if it's just, I mean, we understand that we're running code now, I mean, for a long time we were promoting the idea of running NoScript, and that you would have NoScript on by default, and you would only allow scripting where necessary.  And of course our listeners know that I gave that up.



LEO:  I gave it up after three minutes.



STEVE:  Yeah, I know.  It was just like, okay.



LEO:  No site works.



STEVE:  Nothing worked anymore.



LEO:  Congratulations, you've broken the Internet.



STEVE:  So we had to back off of that policy because scripting is just now a reality, which means when you go and browse around the Internet, you are running JavaScript of the site that you're visiting.  Now, if the site abuses that privilege, you close the page.  You say "Ow," and "I'm not going back there again."  And so that's sort of a self-limiting effect.  And we've talked about the idea of cryptomining.  You go to a site, and it pins your CPU.  Same thing.  Ouch.  And you close your tab.  So it makes sense, I mean, I'm thinking this is nice work that Sabri is doing.  And presumably Safari could put a - or WebKit could put a special case in where, if it's like you're asking for a stack of this blurring effect, more than 10 deep, it's like, no.  It's like prevent itself from doing that.



The problem is there are probably an infinite number of ways of abusing code.  I mean, this is code.  And in fact I was reading down through some of the conversation about this, and there was someone else who posted some JavaScript that just used the forward and backward ability.  Like in JavaScript you're able to script moving to the previous page and moving to the page you just left.  So you could certainly put that in a loop and just cause the browser to jump back and forth.



LEO:  Yeah, right.



STEVE:  It's like, okay.  How is that useful?  I mean, and the point is we don't want to bog our browsers down with special casing every possible way because there's an infinite number, I mean, it's code.  There's an infinite number of ways you can get up to mischief.  So it would be nice if there was maybe some resource control, or maybe if the browser just closed the page that was abusing it itself so it's just like, okay, go away, bad page.  I don't know.  Or maybe puts up a sign that says, okay, this site is misbehaving.  Who knows?  So maybe there is some sort of prophylactic measure generically that could be taken to protect browsers.



So I think what Sabri is doing is great.  Either he's going to succeed in driving some useful change, or he's just going to have a page that is always able to annoy people who click on the, yeah, crash my Chrome.



LEO:  Please.



STEVE:  Yeah, it's like, okay.  But the good thing that could come out of this is if remote code exploits are found.  But if it's just a matter of making the browser unresponsive, it's like, well, okay.  I'm not sure that's - maybe it's worth showing people.  Who knows?  And if the browsers can do something, sort of like all-encompassing, to say we don't like this website you're at.



LEO:  Don't ever come back here.



STEVE:  Yeah.  Would you like us to remember not to come back here again for you or something.  I don't know.  But it was sort of interesting, and I wanted to follow up on the work that he had done before.  And just to mention that now you can also, if you're interested, you can crash your Chrome.



LEO:  Hurt yourself?



STEVE:  And you could crash your Firefox.  Okay.  So Matthew Green, our professor of cryptography at Johns Hopkins University, whose work we often cite and enjoy, his blog is titled "Why I'm Done With Chrome."  And I'll just say he's not alone.  Now, he wrote lengthy, and we're not going to go through it all.  It was 2,236 words, believe it or not.  But I will share the beginning of it because it sets it for us and explains why he's upset and gives us our context.



He starts off with:  "This blog is mainly reserved for cryptography, and I try to avoid filling it with random 'someone is wrong on the Internet' posts.  After all, that's what Twitter is for," he says.  "But from time to time something bothers me enough that I have to make an exception.  Today I wanted to write specifically about Google Chrome" - and I should just mention I'm not onboard with this completely.  It's like, come on, really?  But okay.  So "...specifically about Google Chrome, how much I've loved it in the past, and why, due to Chrome's new user-unfriendly forced login policy, I won't be using it going forward."



Okay.  So we'll take a break here for a minute.  It's like, what?  After the release of Chrome 69, users discovered that anytime they logged into their Google account, or any Google service, they would also be automatically logged into Chrome, which is different than that, whether they wanted that or not.  And I'll just mention that, too, can be overridden.  The underlying feature here is something that the Chromium folks call "identity consistency" between the web browser and the cookie jar.  And it's mostly the fact that, as you said, Leo, Google has the dot Google domain.



And we know how cookies work.  Whenever your browser is making a query to a domain for which it has previously received a cookie which is still valid in the context that it's being used, because there's secure and a number of additional flags that could be added, and it hasn't expired and so forth, it offers the cookie up.  And it is the cookie that creates this notion of continuity and logging in-ness and this abstraction of having a session, a login session.  So this is sort of tied in, this identity consistency, with the web browser and cookie jar.



But they did change the behavior when they went to Chrome 69.  And so the Chromium people say, when this is enabled, as it is by default, the browser manages sign-in and out of Google accounts under Mac, Windows, Linux, ChromeOS, and Android.  So Matt Green and many others feel that this is a big deal since it associates a browser with a Google account, which he argues should not happen unless the user explicitly chooses to log into Chrome.  Even if browsing data is not uploaded, and sync is not enabled, there's data that could be gathered simply by the authentication process alone.



And it's true that, pursuant to Google's own privacy policy, logging into the browser does involve a different set of privacy expectations.  In their privacy policy, Google says, "When you sign into the Chrome browser or a Chromebook with your Google account, your personal browsing data is saved on Google's servers and synced with your account.  This type of information can include browsing history, bookmarks, tabs, passwords and autofill information, other browser settings like installed extensions."  And they say: "These settings are automatically loaded for you anytime you sign into Chrome on other computers and devices.  To customize the specific information that you synchronize, use the Settings menu."



So the point is that logging in with Chrome to your browser is like a separate thing.  And so what has Matt all bent, and a lot of other people, is that this is something they changed without mentioning it.  He says:  "The fact that Google has decided to sign users into their browser without their permission causes" - oh, I'm sorry.  It's me in my notes.  The fact that Google has decided to sign users causes Matthew to worry that Google may decide to start synchronizing user data whenever they choose.  He wrote to Google: "If you didn't respect my lack of consent on the biggest user-facing privacy option in Chrome, and didn't even notify me that you had stopped respecting it, why should I trust any other consent option you give me?  What stops you [Google] from changing your mind on that option in a few months, when we've all stopped paying attention?"



LEO:  Yeah.  The roots of this go back a little way because you may remember a couple of years ago Google unified all of their services under one Google account.



STEVE:  Right.



LEO:  And there was a big change in the privacy policy.  And this they did make a big deal about.  They put an announcement on and stuff.  So it used to be, when you logged into YouTube, that was different than logging into Gmail.



STEVE:  Right.



LEO:  But now it's all the same.  And for instance, you have a Google Plus account - oh, Apple's calling again.  Oh.  You have a Google Plus account whether you want one or not.  You have, if you create a Gmail account, you have Google accounts across all their services.  And all this is, is the last shoe dropping that now that includes, among all those services, that includes Chrome.  And I completely understand what Matthew's saying is that the browser's a special case.  But the answer is easy.  Just don't use Chrome.  I think, now, he says that the Google team hasn't given any reasonable reason for this.  I disagree.  I think it's apparent what the reason is, is this is what users expect.  For instance, on the iPhone, when you log into Google once on the iPhone, that's the only time you log into Google.  Every time you use something else, including Chrome, including YouTube, you're already logged in.



STEVE:  It knows who you are.



LEO:  It knows who you are.  Apple allows Google, in fact encourages Google, to do that.  And I would submit that that's kind of what users expect.  Well, look, I logged in.  I shouldn't have to keep logging in.  I completely agree with Matthew that this is kind of overreaching.  But the good news is you don't have to use a Google browser.



STEVE:  Or, well, and there is a way to turn it off.  And so I think one of the issues, probably I think if we could argue that Google made a mistake, it was in not notifying.



LEO:  Right.



STEVE:  And so he's upset because there is a lot going on behind the scenes with Google.  I mean, they're already sort of shrouded in, like, okay, well, where is our data going and what's being done with it and so forth.  And so the idea that he takes objection to is that it was this just happened, and people discovered, hey, wait a minute, I'm seeing my face, my Google avatar, in the upper right-hand corner of Chrome, and I did not log into Chrome.  And suddenly, whoops.



Now, the fact is, however, that in this mode synchronization is not enabled.  So the Chromium people did take that into account.  That is, if you explicitly log into your Chrome browser, then you're enabling sync.  If you sort of autonomously get logged in, sync is not enabled.  And so a number of people sort of missed that.  And to me it suggests that they clearly understood that that would be going too far, if they just made it exactly the same as if you had enabled cross-browser sync. 



LEO:  It would also be annoying because a lot of people don't enable it because they don't want the same bookmarks on all the instances.



STEVE:  Yes, yes, yes.



LEO:  And they don't want the same user interface.  They don't want the same tabs.  They don't want the same extensions.  I use Chrome synchronization.  I like it.  But I think there are a lot of people say, well, no, but I don't want those bookmarks on this as well as that.  And so it would be wrong for them to log you into their synchronization automatically.



STEVE:  Because then it would really mess up your own manually curated, yeah, setup.



LEO:  Exactly.  This is why I think they're doing this out of response to what users expect as opposed - but this is sometimes hard for people like Matthew to understand because he's not a normal user. 



STEVE:  Yeah.



LEO:  Normal users are - try to explain this to a normal user.



STEVE:  Right.  Well, and...



LEO:  [Crosstalk], well, of course I'm logged in.



STEVE:  They could have - yeah, right.



LEO:  I wanted to be.



STEVE:  They could have, like, presented a "We would like to automatically log you into your browser..."



LEO:  They should have, yes.



STEVE:  "...since we see you're logged into other Google things.  Is that okay?"



LEO:  Right.  That would have been good, yeah.



STEVE:  And people would go, what?  Yeah.  



LEO:  Yeah, yeah.



STEVE:  And then end of drama.



LEO:  People would be maybe confused by that.  And remember Google's probably stinging a little bit from a couple of years ago when they announced they were going to unify all the accounts.  They got a lot of heat for that.



STEVE:  Yeah.



LEO:  And now we've all gotten used to it.  I think normal users expect the same behavior that they get on the iPhone, which is "I logged into Google.  Why are you asking me to log in again for?"  You know?  It should all be part of the same, I mean, I don't like it that I have a Google Plus account.  There's no way to not have a Google Plus account.



STEVE:  Yeah.



LEO:  That's nuts; right?



STEVE:  Yeah.  It's worth noting that, I mean, he was really bullish on Chrome.  I mean, he's upset.  But he says, "When Google launched Chrome 10 years ago, it seemed like one of those rare cases where everyone wins.  In '08" - which is 10 years ago - "the browser market was dominated by Microsoft, a company with an ugly history of using browser dominance to crush their competitors.  Worse, Microsoft was making noises about getting into the search business.  This posed an existential threat to Google's Internet properties.



"In this setting, Chrome was a beautiful solution.  Even if the browser never produced a scrap of revenue for Google, it served its purpose just by keeping the Internet open to Google's other products.  As a benefit, the Internet community would receive a terrific open source browser with the best development team money could buy.  This might be kind of sad for Mozilla," he writes, "who have paid a high price due to Chrome.  But overall it would be a good thing for Internet standards."  And of course we've often talked about how Google may be sometimes viewed as a bit heavy-handed, but they really do have the users' interest almost always at heart.



LEO:  Exactly.  We may disagree with what they do, but I don't think they do it out of evil intent.  They do it because they think this is how users work.  And I think that they're right.  Although your next story is exactly what Matthew was talking about.  The upshot, the result of creating Chrome is actually very positive.



STEVE:  Yup.  Okay.  So before we get that, there is a flag.  Google calls it "identity consistency," that is, this thing we've just been talking about.  So if you go to chrome://flags, and then into the "search flags" field, you type "account-con."  That's enough.  That will select out just this one, which is account consistency.  And if you don't like this, you just disable it.  Set it to disabled, restart your browser, and your Chrome will no longer auto login for you.  So again, they did make it tweakable for those who don't want the default for whatever reason.



But as you were just saying, Leo, we now have Ungoogled Chromium.  And the tagline is "Bringing back the 'Don't' in 'Don't be evil.'"  And this is on GitHub.  There are binary builds for a bunch of different popular OSes I'll get to in a second.  But Ungoogled Chromium describes itself as it's Google Chromium sans integration with Google.  It also features some changes to enhanced privacy control and transparency.



For their motivation and description, they said:  "A number of features or background services communicate with Google servers despite the absence of an associated Google account or compiled in Google API keys.  Furthermore, the normal build process for Chromium involves running Google's own high-level commands that invoke many scripts and utilities, some of which download and use prebuilt binaries" - meaning that they're just saying there's some lack of full transparency there, you don't know really what they are - "provided by Google.  Even the final build output includes some prebuilt binaries.  Fortunately, the source code is available for everything.



"Ungoogled Chromium is a set of configuration flags, patches, and custom scripts.  These components altogether strive to accomplish three things:  Disable or weaken offending services and features that communicate with Google or weaken privacy; two, strip binaries from the source tree and use those provided by the system or build them from source; three, disable features that inhibit control and transparency, and add or modify features that promote them.  These changes are minor and do not have significant impacts on the general user experience."



And then they say that Ungoogled Chromium should not be considered a fork of Chromium.  "The main reason for this is that a fork is associated with more significant deviations from the Chromium, such as branding, configuration formats, file locations, and other interface changes.  Ungoogled Chromium will not modify the Chromium browser outside of the project's" - and they didn't say "limited," but I'm adding "limited" - "goals.  Since these goals and requirements are not precise, unclear situations are discussed and decided on a case-by-case basis."  And I've got a link to it, Ungoogled Chromium.  I'm sure if you just Google it you'll find Ungoogled Chromium.



LEO:  It's on GitHub.



STEVE:  Yes, exactly.  And I have a link to their prebuilt binaries.  They are - most of them are very current.  Debian 9.0 has a 69.0, so that's a current development binary, and 68 in release.  Portable Linux has about the same thing.  There's also one for, oh, Mac is at 68.  Ubuntu is at also 67 for release or 69 for development.  And Windows 64-bit has a binary at 67.  So almost completely current.  And again, they're not providing those.  Other people have built those for those platforms.  So caveat emptor.  But still, it means you're not stuck having to build it yourself, and you can use a version of Chrome that has those things done to it.  So as you said, Leo, it's a consequence of...



LEO:  I think it is, yeah.



STEVE:  Yes, of it being open source and Google saying, well, we're going to do this.  If you don't like it, you can take what we've done and do something different.



LEO:  I like that.



STEVE:  Which I think is neat.  Okay.  So Western Digital.  They have a very popular NAS known as My Cloud.  That's one of the more popular Network Attached Storage devices because it is turnkey.  It's more user-friendly than some of the more techie NASes.  And it's able to offer a cloud-based access; right?  So here's an example of something you attach to your network.  It's able to poke a hole through your NAT router and make itself available globally so that you're able to access your NAS when you're out roaming around on the public Internet.  For that, you want really good security, obviously.  Otherwise, bad guys can get into your NAS, which nobody wants.  That's why there is a username and password.



So security researchers at Securify discovered an authentication bypass vulnerability actually across the Western Digital My Cloud NAS boxes that could allow, actually very trivially, unauthenticated attackers with network access, meaning Internet access in the case of this thing being out on the cloud, to escalate their privileges to admin level without needing to provide a password.  This allows attackers to run commands that would normally require admin-level privileges and gain complete control over the affected NAS device, giving them the ability to view, copy, delete, and overwrite any files on the device.  So all that's horrible; right?



Okay.  So what Securify wrote in their disclosure was:  "Whenever an admin authenticates, a server-side session is created that is bound to the user's IP."  Okay.  So you imagine somebody out on the Internet.  They're logging into the NAS.  And so this creates a session tied to their public IP.  That's how sessions work.  That's fine.  They say:  "After the session is created, it is possible to call authenticated CGI modules" - you know, executable modules - "by sending the cookie username=admin in the HTTP request."  Just a cookie.  But that's after you've been authenticated.  So, they say, "The invoked script, the CGI, will check if a valid session is present and bound to the user's IP address."  So that's fine.



However, what they discovered was it's possible for an unauthenticated or pre-authenticated attacker to create a valid session without being required to authenticate.  Whoops.  An authentication bypass.  The network_mgr.cgi module contains a command called cgi_get_ipv6 that starts an admin session that is tied to the IP address of the user making the request when invoked with the parameter flag=1.  Meaning that's all you have to give it.  Subsequent invocation of commands that would normally then require admin privileges are now authorized if the attacker sets the username=admin cookie.  And they show a proof of concept which is just, I mean, it's a standard post submission, an http post, to this manager, this network manager CGI, giving the cookie username=admin with a content length of 23 and just a single line, cmd=cgi_get and so forth, with a flag=1.



You send that to any Western Digital My Cloud NAS box, and you're authenticated as an administrator.  And you can then do anything you want to from the Internet.  And in their disclosure they say:  "Next, call an endpoint" - meaning a command like cgi_get_ssh_pw - "that requires admin privileges and authenticate as admin by adding the cookie," and it works.



Okay.  So in Western Digital's blog just recently, I got a kick out of this, they said:  "Recently, security researcher Securify published an authentication bypass vulnerability for our My Cloud products."  They said, "We are in the process of finalizing a scheduled firmware update that will resolve the reported issue.  We expect to post the update on our technical support site at support.wdc.com within a few weeks."



Okay.  Now, but get this.  What I didn't say is that on April 9th of 2017, okay, 2017, Securify discovered the vulnerability.  And the next day, on April 10th of 2017, they notified and reported this to Western Digital customer support and were ignored.  So they waited until September 17th of 2018, I mean, maybe they forgot.  Like, what, 17 months?  No, 19 months later?  On September 17th of 2018, this month, they requested a CVE designation for this.  They received it the next day, on the 18th, and later that day - on the 18th, so what's that, that's last Tuesday - published the details of this.  Then they got, finally, Western Digital's attention.



After this vulnerability was known to Western Digital, these guys kept their mouth shut about it for 19 months.  And within a few days, actually, three days it took Western Digital to update the firmware once this went public.  So the firmware is now available.  So the first takeaway is, because as we know there are bots, and there are bad guys out there scanning for this - so this became public knowledge a week ago, and there's no authentication on any of these Western Digital My Cloud products.  So if you have one, and if it is publicly exposed, you absolutely definitely want to update the firmware on your device posthaste.



And I'm at a bit of a loss about what to do about, I mean, what they should have done.  It's a vulnerability that they did not disclose, to their credit.  But on the other hand it wasn't until they did disclose it that Western Digital then patched it in three days.  So maybe they should have held Western Digital's feet to the fire, given them a 90-day deadline and said, look, as other companies do, as Google does with the things it finds, fix this in three months because we're going public with it whether you do or not.  Then if Western Digital blew them off, well, once they went public, three days later this would have been fixed.  To me, that would have been way better than leaving this unpatched for 19 months, during which time it could have certainly been possible that other people would have found this.



These guys found it by reverse-engineering the code in the My Cloud firmware, looking for vulnerabilities, and they found a big one, complete login authentication bypass, and told Western Digital, who then did nothing about it.  So I'm unimpressed with Western Digital, except I'm kind of impressed with their response once they decided, ooh, we'd better fix this.  They were certainly able to immediately.  So I don't think they should have been given this much time to fix something that is this important.  And anybody who does have a My Cloud system should absolutely get themselves updated immediately.



LEO:  On we go, Steverino.



STEVE:  So in researching the previous story about the NAS, there were some links that I was following.  And I ran across a site I just wanted to put on our listeners' radar because I thought they would get a kick out of it.  It's Exploiteers.  So it's E-X-P-L-O-I-T-E-E dot R-S.  So it's a clever use of the .rs TLD.  And it's a site dedicated towards just sort of old-school hacking of all kinds of gadgets and IoT devices.  For example, I just pulled one out because it was a Sony Blu-ray player, for example.  They have there:  "A bug exists in the MTK-supplied SDK which affects many Blu-ray players, including the BDP-S5100.  The main binary, which controls all aspects of the player, has leftover debug instructions for the VUDU app.  When the VUDU app is run, if a file exists named vudu.txt in a directory labeled vudu on a FAT-formatted flash..."



LEO:  This is awesome.



STEVE:  Isn't it?  It's so cool.  "It will attempt to execute vudu/vudu.sh and deletes vudu.txt.  It runs this sh as root.  Using the commands below, you can spawn a root telnet shell, allowing access into the device."  And then they just show you.  It's a simple shell script, like five lines, that ends up running the telnet daemon.  And so they explain:  Put this on a file.  Stick it on a FAT-formatted flash drive.  Restart the device.  Go to vudu on the Sony menu, and now this thing then runs a telnet server.  So it's going to be listening on port 23.  You then telnet to the Blu-ray, and you're in.  But, I mean, the list is just like LG, Panasonic...



LEO:  Well, it's a wiki, so people are adding to it; right?



STEVE:  Yes, yes, yes, yes.  And so, I mean, it's just like the Who's Who of - here they've got Google.  There's a Belkin network [crosstalk]...



LEO:  I have a lot of this old, obsolete hardware that's just sitting in a box.  I can do something with it.  That's nice.



STEVE:  Yeah, yeah, yeah.  And so they have pictures and videos.  There's people hooking wires onto the motherboards of things and doing stuff.  So just sort of fun hacking stuff.



LEO:  You remember the Pogoplug?  Got an old Pogoplug?  Well, look at this.  You can gain root, turn it into a little mini server.  Wow, that's awesome.  What a great site.



STEVE:  Yeah, I just...



LEO:  What is .rs?  Is that Russia?  Where is .rs?



STEVE:  No, Russia's .ru, so I don't know what .rs is [Serbia].



LEO:  I like it.  Exploiteers.  It's Exploitee.rs.



STEVE:  Yeah, I just thought our listeners would get a kick out of playing with that.  Okay.  So this was weird.  And I would argue against what Eric Schmidt believes.  But last Wednesday, in San Francisco, at an event hosted by the venture capital firm Village Global, the economist Tyler Cowen asked about the possibility of the Internet fragmenting into different sub-Internets with different regulations and limited access between them in coming years.  He said:  "What's the chance, say, 10 to 15 years, we have just three to four separate Internets?"  Eric Schmidt, who is of course the past CEO of Google and executive chairman of its parent company, Alphabet, predicted right there on the spot that within the next decade there will be two distinct Internets, one led by the U.S., and the other led by China.



LEO:  Wow.



STEVE:  Yeah.  Eric said, quote:  "I think the most likely scenario now is not a splintering, but rather a bifurcation into a Chinese-led Internet and a non-Chinese Internet led by America.  If you look at China," he says, "and I was just there, the scale of the companies that are being built, the services being built, the wealth that is being created is phenomenal.  Chinese Internet is a greater percentage of the GDP of China, which is a big number, than the same percentage of the U.S., which is also a big number."



He says:  "If you think of China as, like, 'Oh, yeah, they're good with the Internet,' you're missing the point.  Globalization means that they get to play, too."  He says:  "I think you're going to see fantastic leadership in products and services from China.  There's a real danger that along with those products and services comes a different leadership regime from government, with censorship, controls, et cetera."  He says:  "Look at the way BRI" - that's the Belt and Road Initiative.  He said:  "...the way BRI works, which involves 60-some countries.  It's perfectly possible those countries will begin to take on the infrastructure that China has with some loss of freedom."



And I thought, okay, what?  So I'm skeptical.  I mean, first of all, we've talked a lot about the Great Wall, the firewall.  We talked recently about the project that Google was outed as being working on in secret, which is to come back to China with a censored search in order to operate within the Chinese boundaries.



But, you know, in thinking about this, it occurred to me that we have an existing, much older, and very mature model in the global telephone system.  And as far as I know, someone in China is able to phone someone in the U.S. and vice versa.  I mean, it's not like there's a separate phone system, and they're not interconnected, and there's no way to talk across some artificial boundary.  So I don't know, Leo.  To me it's hard to imagine that economically you wouldn't lose so much by chopping the Internet into two pieces.  It seems hard for me to imagine that that's realistic.



LEO:  That's always been the argument against it, is you don't want to participate in the global economy?  Okay, fine.  But China's so big, in a way it is its own global economy.  And remember that the phone system works that way because there are gateways.  Similarly on the cell network there's a gateway between Verizon and T-Mobile.



STEVE:  Yeah.  Yeah.



LEO:  So it's not inconceivable that there would be gateways.  What China's interest is, of course, is in controlling the information flow from the West.  And so I'm not sure that Eric is wrong.  It's not the first time I've heard this idea.  It's certainly what China would like to do.  Whether China will succeed is another matter because, you're right, there's all this pressure to be global; right?



STEVE:  Yeah.  Yeah.



LEO:  I don't know if it's a bad thing or not.  We always had the Iron Curtain, remember?  We're old enough to remember the Iron Curtain that separated the East from the West.  That was, in effect, an economic block. 



STEVE:  Yeah.  So something happened recently that caught a lot of attention.  PC Mag had the headline:  "Unwiped Servers With Data on Millions Sold on Craigslist."  Another headline:  "Database servers sold at NCIX auction, allegedly without being wiped."  Bleeping Computer's headline:  "Unwiped Drives and Servers from NCIX Retailer for Sale on Craigslist."  And so forth.  Sophos, their Naked Security blog, had the story and, I thought, just very succinct coverage.  They said:  "Bankrupt NCIX customer data resold on Craigslist."  Sophos said:  "For Canadian or U.S. customers of NCIX during the past 15 years, they should assume any personal data or credit card information logged with them is now potentially in the hands of cybercriminals and raise any suspicious transactions with their bank."



Now, for those who are not aware, NCIX was a major Canadian online and brick-and-mortar retailer.  Sophos said:  "What happens to sensitive customer data when a large company that has collected it over many years suddenly goes bust?"  And that's where we're going to get to here in a second.  But they said:  "It's easy to assume that databases are wiped by diligent IT staff just before they turn off the lights and close the door for the last time."



LEO:  Oh, sure they are.



STEVE:  Uh-uh.



LEO:  Absolutely going to do that before they walk out.  You bet.



STEVE:  See ya.  And they say:  "At the very least that data should have been encrypted."  It wasn't.  They say:  "It has now emerged that something entirely different and more troubling took place when Canadian computer and electronics retailer" - and I sort of think of them as like the Fry's of Canada.  They sort of had that profile.  "Electronics retailer Netlink Computer Inc. declared bankruptcy in December of 2017."  So, okay, at the very end of last year.



"According to Privacy Fly researcher Travis Doering, the company simply abandoned" - not surprisingly - "much of its equipment in a hurry, which he discovered when it was offered for sale on Craigslist this August," so month before last.  "After arranging a meeting with the seller to examine the hardware, it turned out to comprise 20 Dell PowerEdge and Supermicro servers, 300 desktop PCs, 109 hard drives, and another 400-500 drives that had been inside those desktops or sent to it for repair.



"Now for the disturbing bit.  It soon became clear that the valuable part of the data was not the drives themselves, but what was on them:  13TB of data all told, including 385,000 database records containing names, email addresses, phone numbers, and account passwords, 258,000 of which included full credit card payment details.  A separate Canadian database contained 3.8 million customer records gathered by NCIX between January 2007 and July 2010.  The seller had got hold of passwords to access the databases, while significant amounts of the data were not encrypted in the first place."  But get this.  The seller understood the value of the data and was offering it, itself, for $15,000.



LEO:  They knew.



STEVE:  Uh-huh.



LEO:  There's good stuff on here, man.



STEVE:  You can have all the data.  Who could possibly take advantage of that?



LEO:  Wow.



STEVE:  Anyway, so this was a bankruptcy.  The landlord foreclosed, got the equipment, and so there we are.  So of course this raises a good point.  The Internet is driving a consolidation of retailers.  And here's a big company that had a ton of data on its servers, and the question is whose responsibility is this when the company decides to walk away.  It seems to me like it's the kind of thing that bankruptcy proceedings should evolve to consider is what about the data contained on your hardware?  Because of course this stuff has to go through the courts in order to get liquidated.  And apparently that's still something that just sort of is not being considered and is just slipping under the radar.



LEO:  Do you remember - this was 2003.  A guy named Simson Garfinkel and Abhi Shelat, they were graduate students at MIT.  They bought on eBay 159 hard drives for $5 to $30.  And this is 2003.  So, you know, this is...



STEVE:  Fifteen years ago.



LEO:  Yeah.  So of these drives - one of them came out of an ATM, by the way, completely unerased.  It was a year's worth of transactions with account numbers.  Another one had been formatted, but of course they used simple unerase utilities.



STEVE:  Yeah, unformat.



LEO:  Yeah, and they got 5,000 credit card numbers.  Of the 150-some hard drives they bought, all but a few had - everything was still on it.



STEVE:  Yup.



LEO:  In many cases they just deleted what was in the My Documents folder.  Isn't that funny?  So this was 15 years ago.  It still happens.



STEVE:  Yeah.



LEO:  Nobody's learned.



STEVE:  No change.



LEO:  No change.  We'll be telling this story in 15 years.  Episode 999 we will have a similar story.  The only thing that's changed is the hard drive capacity.  I mean, these guys weren't that big.



STEVE:  Yeah.  So the U.K. regulator has fined Equifax 500,000 pounds, which is in U.S. dollars at the moment $658,419.



LEO:  Holy cow.



STEVE:  Yes.



LEO:  Wow.  Sharp slap on the wrist.



STEVE:  Well, and it's reputation, it's like further reputation damage.  As we all know, and as we have often talked about because Apache Struts stays in the news because it's continuing to have some problems, although less severe than the one that bit Equifax, they suffered a significant data breach in 2017 after leaving a widely known and long since patched flaw in Apache Struts present on their servers, which were facing the Internet, which allowed bad guys to get in and exfiltrate basically all of the Equifax sensitive data.  And it's always easy to pick on someone after the fact.  But a company such as Equifax, which is collecting and vacuuming up sensitive financial consumer information without our knowledge and permission, I mean, I never told Equifax I wanted them to do this.  They're selling...



LEO:  No, but it's a contingency on getting a loan or renting an apartment.



STEVE:  Yes.



LEO:  In the fine print down there it says we're going to provide this information to Equifax.



STEVE:  Right.  Oh, back to the credit reporting firm.



LEO:  Yeah.



STEVE:  Ah, okay.



LEO:  Part of the deal.



STEVE:  Anyway, so of course it's not unreasonable for us to expect them to be really, really careful with all this data that they have collected.



LEO:  Oh, of course they are.



STEVE:  And, I mean, this fine from the U.K. is like the least of their problems because of course they've been slapped with multiple class-action lawsuits.  And, I mean, this is bad for them.



LEO:  Yeah.  But as of yet, I don't think Equifax has suffered any consequences.  They've made more money than they've lost in this.



STEVE:  Yes, well...



LEO:  There is a story, though, that is very good news.  Last Friday Congress passed a law saying that these companies - Equifax, TransUnion, what's the other one, Experian - couldn't charge for credit freezes anymore because they've been charging for those.



STEVE:  Nice, yes.



LEO:  So you can get one for free now in every state of the union.  And they extended the length of fraud alerts from 90 days to a year.  So, frankly, that's a punishment because that hits them at the bottom line; right?  They can't make money off of you if you've got a credit freeze.



STEVE:  Yes.



LEO:  So I'm telling everybody I know, you can now, if you're worried about this, get a credit freeze or a fraud alert at no cost from all three.



STEVE:  Yeah.  So, okay, just to wrap up this story, almost 20,000 U.K. customers had their names, dates of birth, telephone numbers, and driving license numbers exposed; 637,000 customers had their names, dates of birth, and telephone numbers exposed; 50 million U.K. customers had names and dates of birth exposed; 27,000 people in the U.K. had their Equifax account email addresses swiped; and 15,000 U.K. customers had their names, dates of birth, addresses, account usernames, plaintext passwords, account recovery secret questions and answers, obscured credit card numbers, and spending amounts stolen.



LEO:  Wow.



STEVE:  So that just pissed off the U.K. big-time.



LEO:  Good.



STEVE:  And they lowered the hammer.  That 500,000 pounds is the largest fine that can be levied against a company.  And, yes, it's a slap on the wrist to a $15 billion company like Equifax.  But still, it keeps it in the news.  And, boy, you do not want to be a CIO of some other company that does something like this.  And I have to say, Leo, at this point, look at what we're covering every week.  There ought to be a full-time job in every company of size that has something to protect, where that person's sole job is to scan the update notices of every piece of software that they're using, packages like Apache Struts, looking for important updates to it and then pushing the company to roll out fixes.  I mean, they ought to just...



LEO:  That's something GDPR mandates is a data protection officer.



STEVE:  Yeah, yeah.



LEO:  In fact, GDPR would have killed these guys if this had happened today; right?



STEVE:  Oh, yes, yes.  I found a nice note as I was going through my email bag from Louis Vincent in Ottawa, Ontario, Canada.  The subject caught me:  "SpinRite fixes Task Manager."  And of course that's not actually what happened.  But he said:  "Hi, Steve.  SpinRite owner since 2007.  Security Now! listener yada yada.  This past week my laptop started grinding to a halt.  With no program running, Task Manager would show that the CPU was pinned at 100%."  He said:  "Did I have a crypto miner on my device, I wondered?  The weird thing was that Task Manager was showing that the process taskmgr.exe was itself taking 40-50% of the CPU, with McAfee taking most of the rest."  Actually, I have a theory I'll share in a second.  "That was odd, at least for the taskmgr.exe."



He says:  "I did not find the issue while in Safe Mode.  But every time I logged back in normally, taskmgr.exe was back hogging the CPU.  I decided to SpinRite the 500MB hard drive.  As you might imagine, a couple of hours later SpinRite reports nothing crucial found on the drive.  But a reboot later, and everything is working as it was at the beginning of the week.  Thanks for a great product."



So this is another story, actually we covered one while you were on your vacation, Leo, about some guy whose security camera system was booted from an SSD, and it was freezing all the time, but not notifying him.  So every time he checked on it, it had frozen, and he didn't know when, but all he was doing was rebooting it.  Finally he ran SpinRite, and of course no more freezes, which was fortunate because some time afterwards some bad guy was caught on camera stealing stuff from his front yard, and he was able, thanks to the fact that it hadn't crashed, to provide the video to the local sheriffs, who arrested the guy, and he was now behind bars.



LEO:  Nice.



STEVE:  So again, SpinRite didn't show that it fixed anything, but it did fix whatever the problem was.  And we've talked about how that can often be the case.  My guess is that McAfee was doing some routine background scanning of the system, trying to do its AV stuff, and hitting some spot on the SSD that was giving it some heartburn, or it was reading the data wrong or whatever.  And so even though SpinRite didn't raise any flags, because we know that error correction is not perfect, it's sort of like parity, "parity" meaning that there's an extra bit which forces all the bits to have even parity.  If one bit is wrong, then the parity is incorrect.  If two bits are wrong, then the parity is correct again, even though they're two wrong bits.



Well, it turns out that error correction is good in the same way, but not perfect.  And SpinRite is able to not get fooled that way and essentially able to correct problems that the drive itself doesn't even see.  And so it often does that, doesn't report anything because it and the drive fixed the problem, yet the problem goes away anyway.  So a nice side effect of running SpinRite on a drive where something seems a little flaky, and then the flakes are gone afterwards.



And the last story that I wanted to share is a tip of the hat to Cloudflare.  We've got a bunch of friends over there.  They're doing great work.  We've covered that they did the 1.1.1.1 DNS service.  They're offering DNS, both over HTTPS and over UDP TLS, known as DTLS.  So they're very privacy and security conscious.  So that allows, if you have a DNS client which is able to do DNS over HTTPS, and we're seeing some web browsers that are beginning to offer that option, the point being that, if you're really concerned about privacy, it's one thing, as I was mentioning before in a different context, it's one thing to encrypt your communications with a remote website.  But if somebody is sniffing your traffic and seeing the DNS queries that are going to your DNS server, well, they know where you're going still.  They may not be able to see what you're saying to that person, but as we know it's very powerful when, for example, say that you're a bad guy and trying to keep your network of other bad guys secret.



Well, if law enforcement is able just to see, to build a graph of connectivity of who's talking to who, that can be leveraged into some very powerful use.  And similarly, if you are an oppressive government that is looking at who some known dissidents are talking to, well, even though they're not able to see what you're saying to each other, they're able to uncover a network.  And there are many valid uses for wanting to keep the metadata of your communications private.  DNS naturally, because it's not an inherently encrypted protocol, leaks that.



Okay.  So what do you do?  You encrypt your DNS.  There's still a problem, though.  And this is a problem that has not yet, until tentatively just now, been solved, even with TLS v1.3 that's the latest version of TLS.  And that is the so-called SNI, the Server Name Indication.  The idea is this is what's necessary for shared hosting environments.  Over the years we've talked about how browsers establish an encrypted connection to a remote server.  Normally what happens is the browser will look up the IP address for the service based on its domain name; will then connect to that IP address.  And the way originally SSL and now TLS, the evolution of SSL work is that the server sends to the client its signed certificate, signed by a certificate authority that the browser, the HTTPS client, trusts.  So since it trusts the CA because it's in the root store, it trusts the CA's signature that the CA has done due diligence in verifying the ownership of the certificate.  And so that's how this is established.



The problem is that the certificate is sent to the client immediately after the TCP connection is negotiated as the first thing that the server sends in order for them to establish an encrypted connection.  So how do you handle multi-hosting, where there may be many different domains, all accessible at the same IP?  Well, for a while there were still browsers that did not understand SNI, that were unable to specify the server that they wanted to connect to.  And there were servers that didn't understand it.



Well, we're past that now.  All browsers that are in use are able to specify, as an addition to the information they're initially sending, the host name that they want to connect to.  So the browser sends its so-called Client Hello packet, which in the old days just said hi, here's a nonce that I've just come up with for our communication.  Here's the list of ciphers that I support.  Take a look at that and send me back the Server Hello.  Now that Client Hello, in an extension field, is able to say, "And this is the server I want to talk to at your end."  And that allows then this whatever it is that is answering the connection at the remote IP, it's able to look at the Client Hello, find that extension to the protocol, see the domain that the client is asking for, all among many that live at the same IP, and then select the certificate to use in the Server Hello to send back.  So that's how we handle encrypted communications and the disambiguation of which service we're asking to connect to at a single IP.  Once upon a time it was just by IP.  But in shared hosting environments, that was a problem.



Okay.  But because the client and server at that stage have not yet established encryption, we still have information leakage.  And that has been discussed, I mean, extensively.  And no one has come up with a solution that everybody likes.  And the point is that that Client Hello is not encrypted and can't be encrypted, obviously, in an obvious fashion, because the server needs to be able to decrypt it.  And the server needs to be able to decrypt it without any knowledge of the client in advance because otherwise somebody listening on the wire could also decrypt it, would be able to be in the same position of decryption.  So that argues that the client has to have some information that works with something only the server has.



And what a small group of developers - Eric Rescorla, whose name I often see in Internet RFCs, he's at RTFM, Inc., which is his consultancy; a developer [Kazuho Oku] at Fastly; Chris Wood at Apple, who's also involved in Apple security stuff; and, not surprisingly, Nick Sullivan of Cloudflare - they got together and authored an RFC proposing a means of solving this problem.  And Cloudflare has brought it up, and it's running sort of in a test mode.  I mean, there isn't yet a large base of clients to connect to.  But they're solving the chicken-and-egg problem by saying, well, we're going to put this online and invite people to connect to us.



And what they've done is clever.  They've defined a record for DNS which provides the public key for the encryption of this information by the client that wants to protect the privacy of the name of the host to which it wants to connect in a multihosted environment.  So a client that is aware of this extension to TLS 1.3, in the same way, probably at the same time that it does a lookup for the A or the AAAA record, which is the IP address of a site, could ask for the text records that also match that domain name.  One of the newly defined text records is a means of encoding a public key.



So the multihosted site has placed a public key for which only it has the private key - notice we don't need any CAs in this model because it's using DNS to do this.  And DNS is not yet safe enough to be used in place of CAs for trust.  But in this model it provides the security guarantee that we need.  It just provides a means of posting a public key which a client can then use to encrypt something that it's going to send to its target.  And no man in the middle, no one listening to the traffic is able to crack that because they would not have the matching private key that never leaves the server.



So then the multihosted system sees a new type of extension because, in the same way that there's a Server Name Indication extension to the Client Hello, there's now an encrypted SNI, ESNI extension which these guys have defined, which has been encrypted under the public key which DNS is making available through a text record.  They encrypt under that, send the Client Hello packet to the service, which is then able to decrypt that encrypted SNI extension, see who it is the client wants to connect to, select the proper certificate, and send it back.



So just, again, a tip of the hat to Cloudflare and Apple and Eric and Kazuho Oku at Fastly, the guys who put this RFC together, and to Cloudflare for bringing an implementation up and running.  This was an actual announcement of the availability of this.  Oh, and by the way, this coming Thursday, in two days, is Cloudflare's eighth birthday.  They are eight years old in two days and just going a great job on the Internet for us all.



LEO:  I agree.  Well, and so are you.  And this concludes this edition of Security Now!.  Thank you, Steve.  You can watch us do this show live every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch at TWiT.tv/live.  If you want to download a copy, Steve's got them all at GRC.com.  While you're there, check out all of Steve's great free work and, of course, his bread and butter, SpinRite, the world's best hard drive recovery and maintenance utility.  GRC.com.  He also has transcripts, which is nice.  A lot of people like to read along.



STEVE:  I was joking with Jason over the fact that SpinRite fixes SSDs.



LEO:  Yeah?



STEVE:  And I said, you know, I don't know what I'm going to do because they don't spin.  And he said, "Well, call it AllRite."



LEO:  Ah.



STEVE:  I was like, ooh, my god, that's good.



LEO:  AllRite.  AllRite, AllRite, AllRite.  Get Matthew McConaughey to do the ad.  AllRite, AllRite.  You can get copies of the show, audio and video, from us at TWiT.tv/sn.  Or subscribe on your favorite podcatcher, and you'll get it every week, right after the show, soon as it's available.  We've got to edit it a little bit, put some stuff at the beginning, put some stuff at the end, but then you'll get it.  Steve, have a great week.



STEVE:  Will do.  Glad you're back, my friend.  And we'll talk next week, with the one-day warning for the Presidential Alert coming to all of our phones on Wednesday in the early afternoon East Coast, late morning for us on the West Coast.



LEO:  We've got to get the word out because people are going to freak out.



STEVE:  Actually, you probably want to be somewhere public.



LEO:  Oh, yeah, because you'll hear it all loud and clear.



STEVE:  Yeah, that would be cool.  Neat.



LEO:  Yeah.  All right, Steve.



STEVE:  Okay, buddy.  Talk to you next week.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#683

DATE:		October 2, 2018

TITLE:		The Facebook Breach

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-683.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss yet another treat from Cloudflare, the growing legislative battle over Net Neutrality, the rise of Python malware, Cisco's update report on the VPNFilter malware, still more Chrome controversy and some placating, the rapid exploitation of zero-day vulnerabilities, the first UEFI rootkit found in the wild, another new botnet discovery, the danger of the RDP protocol, a nasty website browser trick and how to thwart it, a quick update on recent nonfiction and science fiction, and then a look into the recent massive 50 million account Facebook security breach. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got lots to talk about.  He's going to dissect the Facebook breach.  Good news.  I think my sense is he's not as worried about it as you might be reading the headlines.  We'll also talk about some much more serious problems infecting routers and how hackers work.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 683, recorded Tuesday, October 2nd, 2018:  The Facebook Breach.



It's time for Security Now!, yes, indeed.  You've been waiting all week.  Your long delay, your long wait is over.  He's here, everybody.  Steve Gibson.



STEVE GIBSON:  He's back.  Ah, yes.



LEO:  Would you ever like to do this live in a big auditorium in front of a studio audience kind of thing?



STEVE:  We could.  I mean, we've sort of done it.  I guess we haven't in front of a studio audience.  I mean, if it were topics, I mean - okay.  So the problem is, as you know, I like to dig into these stories and have, like, well...



LEO:  And he often has his eyes closed during his speaking.  I'll be honest.  Right?  Am I wrong?



STEVE:  Yes, I do, actually.



LEO:  You close your eyes.



STEVE:  So I can sort of see the terrain.



LEO:  It's not really a performance - you never wanted video, for crying out loud.  It's not really a performance.  It is, but it's more of an audio performance.  I still think we could do this in front of a live - I bet you there'd be a large audience if we decided to do that.



STEVE:  Well, like in what venue would that work?



LEO:  I don't know.  I don't know why I'm asking this.  I mean, it's not like it's up on the - it's on the table or anything.



STEVE:  Yeah.  Certainly when you and I and Mark Thompson all congregated at the Gnomedexes, an audience forum...



LEO:  Wasn't that fun?



STEVE:  Yeah.



LEO:  I'll never forget that. 



STEVE:  That was fun.  And I was the keynote the first year, and I don't remember what the second year was.



LEO:  How many years ago?  That was so long ago.



STEVE:  We did it for a couple years, yeah.  So, I mean, that was fun.  And of course I'm just about, I mean, I'm just bursting with SQRL stuff.



LEO:  That doesn't sound good.  He's talking about some software he's been writing, folks.



STEVE:  And it turns out, well, actually a solution to one of the big problems that Facebook just faced because one of the consequences of the breach was that the Login with Facebook, this whole OAuth kludge, which is really what it is, I mean, if you think eInk is obsolete, this whole login with another site is, I mean, it has always been a privacy disaster and a kludge.



LEO:  Well, not only that, besides all of that, it's a problem for me because I deleted my Facebook account, and now all these places I was logged into I no longer am - I don't even have a - because a lot of times they don't even have a password.



STEVE:  Yeah.  Yeah.



LEO:  So I don't know what to do in that case.



STEVE:  Anyway, so...



LEO:  SQRL's going to fix that.



STEVE:  It does fix that.  And it is decentralized completely so there's no single point of failure and blah blah blah.  The point that I was going to make was that I could prance around onstage and talk about SQRL till the cows come home, I mean, till the SQRLs all leave the building.  So there will be opportunities, I'm sure, to...



LEO:  We should do that. That's a good idea.



STEVE:  To address a crowd.  Okay.  So the Facebook breach for Security Now! Episode 683, the day before the much-anticipated Presidential Alert for which there is now a lawsuit, Leo, trying to prevent our President from sending the alert.  And I think they think it's a tweet that he can send from his phone.  It's like, no, that's not what this is, folks.  So anyway, we'll talk about that again just as a final reminder.  But we're going to talk about yet another treat from Cloudflare, which I just couldn't resist making this our Picture of the Week because it's just cool.  I just love these guys.  They keep doing good stuff.



We've got the growing legislative battle over Net Neutrality because now the Department of Justice has sued California.  The rise of Python malware, which we've touched on before, but now we've got numbers and charts and things.  Cisco's third and final update on their VPNFilter malware which they found, and we talked about first in May, and then again in June, and now they're done.  We've got still more Chrome browser controversy and some placating from Google.  The rapid exploitation of zero-day vulnerabilities, an example of.  The first UEFI rootkit found in the wild.  Yet another new botnet has been discovered.



A warning about the danger of the Remote Desktop Protocol.  I wonder if that means that saying RDP Protocol is redundant.  I guess that would be, yeah.  Anyway, a nasty web browser trick - that I've never stepped into, Leo, but it occurred to me it makes a useful public service announcement - and how to thwart it.  A quick update.  We haven't talked about sci-fi for quite a while, but there are some events that have happened in the world of science fiction, but even nonfiction.  I just am hours away from finishing the "Masters of Doom," which was really a fun read.



LEO:  Oh, yeah, what a good book, yeah. 



STEVE:  And then we're going to take a look at this recent massive, well, 50 or 90 or maybe more million account Facebook security breach, how a very clever leveraging of a subtle mistake was turned into a mess for Facebook, and what lessons we can learn from it.  So I think another great podcast for our listeners.



LEO:  A whole bunch of stuff to talk about.  How exciting.



STEVE:  So I just threw this in because I thought it was fun.  We're often talking about Cloudflare because of their leading-edge security implementations.



LEO:  Love them, love them, yeah.



STEVE:  Yes, we do.  They're often trying things before everybody else.  We were just talking about them last week in this context.  They're clearly always asking themselves the question how can we make our services better, rather than just sitting around and not doing anything.  And they often answer with security and privacy improvements.  In this case, my sense is that they were looking around for, like, okay, what more can we offer?  And they had a service that was previously only available to their enterprise customers, which they have now just made available to everybody.



And you can have a Cloudflare presence for free.  They've got millions of customers who are using them as a free service.  What they've just done is to add for their customers at-cost domain registration.  Which I just, you know, at the moment I saw it I thought, yeah.  It's like, that's neat.



Now, I registered GRC.com, I think it was six months after the domain Microsoft.com was registered, so back in the...



LEO:  Wow.



STEVE:  Back in the early days, yeah.  And of course back then Network Solutions was the registrar.  That's who you used.  And inertia and loyalty, I just, you know, I stayed with them for a long time.  And our listeners who have been following along know that I finally thought, okay, no.  Every time I renewed one of my domains, and I have a whole bunch that are not known to the public because they're just like ideas for products, you know, you grab the domain name for an idea that you have in case you're going to develop it in the future.  Like, for example, CryptoLink.  I have CryptoLink I think in every TLD that is major because for a while I thought I was going to do an easy-to-use, really, really secure VPN, until I started worrying about the federal government saying, okay, encryption is - we're not going to let you do that.  But I still have those.



And so I was constantly renewing these things over at Network Solutions for $35.  But what really annoying me was all the upselling.  I had to click through no I don't want this, no I don't want this, no I don't want this, all of these extra things in order just to say, yes, I'll renew for another year.  Which is why I went to Hover, where I'm very happy.  Now it's $15, and I get all this other free stuff.



I note, however, that Cloudflare is also giving all that way.  For example, the WHOIS registration is blinded by default, and so you get the privacy.  And, boy, is that nice because I refuse to pay to have my WHOIS information made private.  That's just annoying.  But I was curious to see if Network Solutions was still doing that.  And they give it to you free for a month, and then it's $2 per month after that.



LEO:  That's ridiculous.



STEVE:  It's ridiculous.  So that's, what, $24 just to have your WHOIS information...



LEO:  Just to have an unlisted number, that's all.  I mean, basically.



STEVE:  Yes.  And let me tell you, you get spam.  I mean, I see incoming stuff because I have a different email account for that.  And it's just it goes into a bit bucket which is overflowing.  Anyway, it's so annoying.  So I just wanted to give a tip of the hat and an acknowledgement to Cloudflare and to any of our listeners who may be Cloudflare users.  I imagine you're probably on an email list somewhere, so you already know this.  But any domains you want to register you can now do so for at cost.  Oh, and by the way, cost.  The actual registrar fee for the .com, get this, $7.85.



LEO:  Okay.



STEVE:  $7.85.



LEO:  That's really good.



STEVE:  ICANN has their administration fee of $0.18.  So you add those together, and so Cloudflare is $8.03.



LEO:  Very good deal.



STEVE:  Exactly the cost.  .Net costs them more.  But the point is they're zero markup, so $9.95 for .net, $11.02 for .info, and $10.11 for .org.  So, I mean, you can't - clearly they can only do this because they don't need to make money there.  They're doing other things.



LEO:  Would you have to be a customer of theirs to do this?



STEVE:  Yes, yes.  Yeah, so you have to be...



LEO:  Even a free customer, because they have a free tier, but you still have to be a customer.



STEVE:  Exactly.  So you could be a free tier user, and now your domain name, you're also saving money on your domain name.



LEO:  And we should mention they're a sponsor.



STEVE:  Ah.  I didn't know if they were.  Good.



LEO:  Yeah, yeah.



STEVE:  So, cool.  So I'm not saying this because they're a sponsor, I'm just saying it because we love them.



LEO:  No, I know that.  But, you know, we always disclaim when we talk about sponsors.



STEVE:  Yeah, good, yeah.



LEO:  Just so people can make that distinction.



STEVE:  So, okay.  The U.S. Department of Justice Sunday, when California Governor Jerry Brown signed the Senate Bill SB-822 into law, immediately filed a lawsuit against California.



LEO:  [Heavy sigh]



STEVE:  I know.



LEO:  They're all for states' rights until a state does something they don't like, notice.



STEVE:  Exactly.  We want decentralized government except where we don't.



LEO:  Right.



STEVE:  So this bill, which we now have in California, as a law in California, prevents Internet Service Providers from blocking, throttling, or discriminating against any Internet content.  And I was thinking, Leo...



LEO:  By the way.  If the Justice Department doesn't like that and is suing them, that's basically the Justice Department saying, oh, no, we want to have ISPs have the right to block, throttle, and zero rate.



STEVE:  Yes, yes.



LEO:  They're putting the lie to the fact, oh, we don't have to have a regulation because they would never do that.  Well, if you're not worried about it, why are you suing?  You want to preserve their right to block, throttle, and zero rate. 



STEVE:  Yes.  In fact, okay.  So upon signing, the DOJ responded.  FCC Chairman Ajit Pai said, quote:  "I am pleased the Department of Justice has filed this suit.  The Internet is an inherently interstate information service.  As such, only the federal government can set policy in this area.  And the U.S. Court of Appeals for the Eighth Circuit recently affirmed that state regulation of information services is preempted by federal law."



LEO:  I think, by the way, they're going to win because that is... 



STEVE:  Except they over - I know.



LEO:  Interstate commerce is protected federal arena.



STEVE:  Yes, yes.  Well, and what they did was they repealed, the FCC repealed the open Internet order legislation which was passed in 2015.  So they un-lawed it.



LEO:  Yeah, and they want to keep it un-lawed.



STEVE:  And now they're unhappy that we're trying to re-law it.



LEO:  Keep that un-lawed.



STEVE:  And Katharine Trendacosta, who's a policy analyst at the Electronic Frontier Foundation, argued that the California law was specifically written to be within state power.  She said, quote...



LEO:  Oh, interesting.



STEVE:  Yes.  "The argument that the FCC preempted states acting to protect the Net Neutrality within their borders doesn't hold a lot of water when the FCC gave up jurisdiction of this area in the so-called 'Restoring Internet Freedom Order.'"  She says:  "You can't throw up your hands, walk away, and say it's not your problem anymore, only to also say no one else can rush in to solve that problem.  You gave up that right when you walked away," she said.



LEO:  It's just appalling.  I mean, it's a total admission that the whole point of this is to protect the rights of ISPs to discriminate.  That's the whole point of it.



STEVE:  Yes.  And the other thing that I thought was interesting was that, as I was assembling this, I thought, remember how once upon a time everybody was confused about what Net Neutrality meant?  Now it's been completely well defined.  I mean, we did go through several years of, okay, well, now, here's what this means.  And it seemed like a really bad term.  In fact, I remember when we first had the iPod.  It was like, what?  iPod, what a horrible name.  But now it's just, you know...



LEO:  You get used to it,  yeah.



STEVE:  And the same thing for iPad.  What?  And then the Mini Pad and the Maxi Pad and all that.  And it's like, no.  It just dropped into our culture.  And so now, yeah, Net Neutrality.  We all know what that is, and we all know that we want it.  Except, well, not all of us, I guess.  So anyway, so - oh, oh, oh.  And we're not alone.  It's worth noting that California is far from alone in taking independent legislative action.



The National Conference of State Legislatures indicates that 30 states, three zero, 30 states have introduced over 72 bills requiring Internet Service Providers to ensure and adhere to various Net Neutrality principles.  And so far governors of six states, including Hawaii, New Jersey, New York, and Vermont, have signed executive orders about Net Neutrality, while three other states, Oregon, Vermont and Washington, have enacted, as has California now, Net Neutrality legislation.  So, I mean, it's...



LEO:  Play devil's advocate, though.  You don't want a patchwork of laws governing something like the Internet; right?



STEVE:  No, no, no.  I completely agree with you.  I mean, this is the worst case that has resulted is every ISP - well, actually all an ISP has to do is be net neutral.



LEO:  Well, that's the other argument is that, well, we're not asking that much.  Just don't be a jerk.



STEVE:  Yes, exactly.



LEO:  Basically.



STEVE:  It's not like in some places you have to do it and some places you don't.



LEO:  Right, right, right.



STEVE:  If you just don't do it anywhere...



LEO:  Don't do it, you'll be okay.



STEVE:  ...then everybody's happy.



LEO:  Yeah.  I love it that the complete and utter corruption of the FCC and their motivation is now blatantly clear because they got a lot of money from the telecommunications companies, and they're now saying to the DOJ, you can't let this happen.  We paid a lot of money for this deregulation.



STEVE:  Well, and isn't Ajit Pai an ex-lobbyist?



LEO:  Of course he is.  Of course, remember, Tom Wheeler also was.  And he, I think thanks in great part to all the comments that were put up on the FCC site, he changed his mind.  Ajit Pai just managed to bring the site down so he didn't have to see those comments.



STEVE:  That's right.  So ThreatList had an interesting article, and I knew you would love this chart on the next page, Leo, because it's language popularity over time.



LEO:  I do love this stuff.



STEVE:  Hackers have turned to Python as the attack coding language of choice.  Now, it's not surprising because it's not like Python is the attack language, it's Python is growing to become the all-purpose coding language of choice.



LEO:  Absolutely, yeah.  It's really interesting.



STEVE:  Yeah, and one of those purposes, for better or mostly for worse in this case, is malware.  Today - and what's interesting is stats from GitHub.  More than one out of every five GitHub repositories which contain an attack tool or a proof of concept has its code authored in Python.  So, well, and not just on GitHub.  This summer it was Imperva who did a study who showed that between June and mid-September Python-based tools were used in up to 77% of attacks against sites seen in their telemetry.



LEO:  Because it's easy to code in.



STEVE:  Well, and, yes, and as we talked about last week, and we'll be talking about a little bit later here this week, you can cross-compile it to multiple platforms.  So we're no longer in a world where everything is x86.  We've got ARM chips in routers, and some are 32 bits, and some are 64.  And some are little endian, and some are big endian.  And there's many, many, you know, we're in a very heterogeneous environment in terms of what the malware wants to infect.



And so the malware said, oh okay.  We're going to write this in a high-level language.  Gee, which one should we choose?  And in fact Go is another one which was recently chosen by something known as the Torii malware that we'll be talking about in a second.  But anyway, I just thought it was interesting that more than three out of four, more than three quarters of the attacks in the summer through the early fall have been Python-based agents that are doing the attacks.



And the chart is really interesting.  For those who are hearing audio, interestingly, Java has, I mean, if you look at the Java line, it echoes the Python line.  I mean, they were both in, well, way back in '98 - the chart goes all the way back to 1988.  Back then pretty much it looks like C was the - was it C?  The super dark one.  Yeah, C.  C naturally is the old dog.  But Java jumped way up from '98 to '03 into first place.  It still was first place in '08 and second place in 2013 and back in first place now in 2018.  Python has always been a little lower down, but shows the same trajectory.  And, let's see, what's the expense?  Oh, yeah, Fortran.



LEO:  And Ada, Fortran, and Lisp seem to be plummeting precipitously.



STEVE:  Yeah.  And it's interesting also that Objective C had a sharp rise five years ago, and then it's like sort of dropped back down.  And Perl's been kind of cruising downward.  "R" is not doing much.  JavaScript's kind of holding steady.  But for what it's worth, anyway, Java is in number one place right now.  C, number two.  C++, number three.  Python, number four.  And that's above C#, Visual Basic .NET, then JavaScript, PHP, Ruby, R, and Perl, counting down.  So just sort of interesting to see how these things change over time.



And it's my intention, I think I've mentioned it before, because I have very ambitious plans for SpinRite 7, I intend for it to be a full rewrite.  It'll use all of the architecture, the low-level stuff that I developed for the 6.1, 6.2, 6.3 series.  But then I'm going to give it a whole new front end.  And I plan to write that in Python because I want to be able to quickly implement awareness of file systems, and there's just no reason to write that stuff in assembly code.  All of the low-level stuff will be that way because that's where I'm still most comfortable, and I want screaming speed without compromise.  And it has to talk to the hardware intimately, which is not to say that you can't in Python, but assembly language is the natural language in which to do that.  But I fully intend to use Python because I want to be able to author quickly and have it much more dynamic, just like the malware.



Okay.  So in May of this year, earlier in May, we first introduced a confusingly titled malware.  You know, it's better when it sounds bad, like Heartbleed or something, where it's like you don't want to have that.  In this case, VPNFilter sounds like, oh, maybe I need to install that.



LEO:  Sounds like a good tool, yeah.



STEVE:  I think I should filter my VPN.  No.  So this was named because it's in the path name of where this malware stores itself.  And it was obviously named that to sort of look innocuous, like if you were you looking through your file system and saw a directory named VPNFilter, you'd go, oh, okay, I guess my VPN is being filtered, not realizing necessarily that it was evil.  So Cisco's Talos Research found this in May and gave us the first report.  And that's when we first talked about it with the surprise that over half a million diverse routers from Linksys, MikroTik, Netgear, and TP-Link have been found to be infected by this one strain, this new strain of malware.  It's an IoT bot that is scanning for itself and is used for attacks and proxying and so forth.



And at the time they said they had not yet completed their research.  But they felt, due to this discovery of how widely present this was - they found it in 54 different countries, though largely concentrated in Ukraine.  They felt they needed to come forward and alert the public to what they had found so far.  A month later we revisited it on the podcast in June with their update, that it was also they had since discovered it in ASUS, D-Link, Huawei, Ubiquiti, UPVEL, and ZTE routers.  And I remember you and I looked down the list because I put it in the show notes:  75 makes and models of routers where this thing was present.  Meaning that it was managing to get into and infecting a large diversity of routers.



Okay.  So at the time we talked about it.  You'll remember this, Leo, because it was downloading its second stage either from Photobucket or from a domain ToKnowAll.com.  And it was hiding the IP address of its command-and-control server in the EXIF image metadata of images stored on Photobucket.  So sort of a steganography kind of approach where, unless you had reverse-engineered this thing and saw it go reach out to Photobucket and then parse the EXIF image metadata and go, oh, it just extracted an IP address, which it then connected to to get instructions, you wouldn't know what it was doing.



So, okay.  So probably one of the most disturbing things about this is that this is not coming from anybody in their mother's basement.  They have tied this back to a major state actor.  And so it's a little unsettling, and maybe that's one of the reasons that routers in Ukraine are the target, but it's disturbing that a major, you would hope a mature state would be making this sort of malicious software.  But that's the world we're in.



Okay.  So finally, last Wednesday, we're back for the final update from Cisco.  They've finished their analysis.  And just to paraphrase from the beginning of their update report, they said VPNFilter  a multi-stage, modular framework that has infected hundreds of thousands of network devices across the globe  is now known, now as of their third final report, to possess even greater capabilities.  Cisco Talos recently discovered seven additional third-stage VPNFilter modules that add significant functionality to the malware, including an expanded ability to exploit endpoint devices from footholds on compromised network devices.  The new functions also include data filtering and multiple encrypted tunneling capabilities to mask command-and-control and data exfiltration traffic.



They've been, as we know, researching VPNFilter for months.  And they said:  "As part of our continued investigation, we developed a technique to examine a key protocol used by MikroTik networking devices" - and we've often talked about this web-based configuration, this Windows-style configuration utility which unfortunately has turned out to be a vulnerability in MikroTik routers - which has allowed them to hunt for possible exploitation methods used by this threat actor.



So they've found, as they mentioned in their opening, additional Netfilter capabilities.  The reason this took so long is that they had binaries for these things, but they then had to reverse-engineer them.  And the binaries are just ones and zeroes, which they then have to decompile.  And then there's no comments and easy clues in a well-written binary.  It's just instructions.  So you really have to spend some time studying this, which is what they've done.



They found these seven additional what they call "third-stage modules" which provide these greatly expanded capabilities which are present in VPNFilter.  There's one they call "htpx" which redirects and inspects the contents of HTTP traffic which transit through their devices; "ndbr," which they describe as a multifunctional SSH utility; "nm," which allows network mapping activities to be conducted from the compromised devices.  Netfilter is a denial of service utility.  There's a port forwarding system that allows network traffic to be bounced off of these devices to other infrastructure.  There's a "socks5proxy" which can be established on compromised devices, and a reverse-TCP VPN.



So their rather sobering conclusion was:  "As a result of the capabilities we previously discovered in VPNFilter, coupled with our new findings, we now confirm that VPNFilter provides attackers all of the functionality required to leverage compromised network and storage devices to further pivot into and attack systems within the network environments that are being targeted."



So this is exactly what I've been talking about being glad we haven't really been seeing yet.  Turns out this is exactly the behavior which is built into VPNFilter, which is I've been talking about how nice it is that, well, mostly these guys just want to run cryptomining, or they just want to use UPnP abuse to bounce packets off of routers in order to conceal their identity for distributed denial of service attacks, and how good it is that they're not really that interested with what's going on inside the network.  Well, everything we just read and what Cisco has concluded is that that's exactly what these capabilities enable.



They continue, saying:  "It also allows attackers to leverage their access to sensitive systems such as gateway and routing devices to perform activities such as network mapping and endpoint exploitation, network communications monitoring and traffic manipulation, among other serious threats."  I mean, so this really does sound like state actor-based.  This is not people who are wanting to rent out their DDoS botnets.  This is organizations that know what MikroTik enterprise router they are on and are now going to turn around and look inside and cause some havoc.



They said:  "Another dangerous capability provided by VPNFilter is the ability to turn compromised devices into proxies that could be leveraged to obfuscate the source of future, unrelated attacks by making it appear as if the attacks originate from networks previously compromised by VPNFilter.  The sophisticated nature," they say, "of this framework further illustrates the advanced capabilities of the threat actors making use of it, as well as the need for organizations to deploy robust defensive architectures to combat threats such as VPNFilter."



And I would say, I mean, our takeaway, our listeners' call to action is absolutely make certain that the point of contact of both enterprise and personal networks which to say is typically some sort of router gear, absolutely make sure that it is secured, that it is running the latest updated firmware from the manufacturer, that it's from a reputable manufacturer that is maintaining the firmware and really carefully look at the Internet-facing attack surface to see what's there.  It's just it's obvious that we're talking here is more than 500,000 instances of this kind of presence on publicly exposed nodes on the Internet.



And this is just this one malware.  We know that in aggregate it's probably tens of millions of different types of malware that is coming and going.  And as we'll see a little bit later, they're becoming increasingly good at being both multiplatform and really deeply persistent on the things that they infect.  It's just crucial.  And I guess it feels to me like it's so easy to, because we don't know otherwise, it's easy to assume that your node is secure.  You have to look because what we always see is it's not until you look that you discover a problem.



That's why the current best advice for enterprise and, to a lesser degree, personal networks is not only having a firewall and/or NAT router that blocks traffic, but keeping an eye on the traffic inside your network.  You know, you just talked about a sponsor, Leo, that is offering exactly that.  You absolutely, you have to keep an eye on what's going on in your network.  I'm continuously looking at the traffic here in my own home office environment, and I see traffic spikes from time to time, and I take a minute to go see what's going on.  And it's like, okay, well, it's one of my many iOS devices decided it's time to update itself or something.  Whatever.  But it's nice to have an idea of what's going on.  And unless you look, you don't know.  And, boy, it's so easy just not to be, you know, just to be oblivious to what's happening.



Okay.  So one more story, then we'll take our second break:  Chrome back in the middle of controversy.  Last week, I think it was Thursday - or, no, it was Monday.  It was early in the week, but then Chrome responded by Thursday.  Someone named Christoph Tavan noticed something about Chrome and tweeted, and set off a firestorm, which in this case I think was unwarranted because this is something that Google clearly knew and deliberately chose to do and even documented, clearly, right at the site of where this happened.  But in this current "everyone is piling on Chrome" environment, it went over less well than it might have normally, and Christoph got a lot more attention from this tweet.  He observed and tweeted that after he had instructed Chrome to delete all of the cookies his Chrome browser had collected, it did that, except for Google's own cookies, which it retained.



Now, under Settings > Advanced > Clear browsing data, and I went there this morning just so I could talk about it authoritatively and make sure that it was still there, we're shown the number of things and the amount of space being consumed by all of our various browsing history, you know, caches of stuff and various site data, how many different sites have cookies.  I noticed that in the screenshot he posted he had 1,157 different sites had left cookies in his browser that he was going to delete by clicking yes, get rid of all that.  So on the detail line for cookies and other site data, Chrome clearly states parenthetically there:  "You won't be signed out of your Google account."



Okay, well, now, maybe the problem was they weren't clear enough, or I don't know.  We know that cookies are the mechanism by which browsers maintain state with the sites they visit.  The web server gives the browser a cookie to return with all subsequent queries.  So clearly what Chrome was doing was assuming users wanted to get rid of all their cookies except their Google account cookies.  That's the assumption that got them in trouble in this case, even though "You won't be signed out of your Google account" means because that cookie gets to stay.  Because otherwise you will be signed out of your Google account.  Google won't know you.  You'll have to sign in again, and then you'll get a cookie in order to be signed in.



So based upon the uproar caused by this misunderstanding, I guess what Google should apparently have offered was an additional checkbox underneath the cookie report which said - like a checkbox that would say "Also delete Google's own cookies and be signed out of all Google accounts and services."  But they didn't do that.



So that leads us to Google's comprehensive posting last Thursday made by Zach Koch, who's the Chrome Product Manager.  He first catches us up in his posting on recent events which I won't cover because we've been doing that the last couple weeks and just now.  Finally he says:  "Over the years we've received feedback" - and to their credit, I mean, again, anyone can make a mistake.  What matters as we know is that Google and Chromium are listening.  And they certainly have been.  Well, except in the case of www, which is still going to get disappeared in the next version of Chrome.



But he says:  "Over the years we've received feedback from users on shared devices that they were confused about Chrome's sign-in state.  We think these UI changes help prevent users from inadvertently performing searches or navigating to websites that could be saved to a different user's synced account."  And actually there was some report of that happening, too.



Anyway, he said:  "We've heard  and appreciate  your feedback.  We're going to make a few updates in the next release of Chrome" - that'll be v70, released in mid-October, so in a couple weeks - "to better communicate our changes and offer more control over the experience.  We think sign-in consistency" - which is what they call this automatically have the Chrome browser sign in when you sign into a Google account.  "We think sign-in consistency will help many of our users.  We're adding a control that allows users to turn off linking web-based sign-in to browser-based sign-in.  That way users have more control over their experience.  For users that disable this feature, signing into a Google website will not sign them into Chrome."



So that was last week's brouhaha that Matt Green weighed in on, and that's why he's given up on Chrome and so forth.  So they're going to give us a switch.  That is, there was one always.  But it's buried deep in that flags infinite list of things you could tweak.  And now it'll just be a little slider that regular people can do.



He says:  "We're also updating our UIs to better communicate a user's sync state.  We want to be clearer about your sign-in state and whether or not you're syncing data to your Google Account.  We're also going to change the way we handle the clearing of auth cookies.  In the current version of Chrome, we keep the Google auth" - and that's short for authentication, of course - "cookies to allow you to stay signed in after cookies are cleared."  Which is to say all but ours.



"We will change this behavior that so all cookies are deleted, and you will be signed out."  So it's like, okay.  You guys didn't like that, we'll take that out.  And he says:  "We deeply appreciate all the passionate users [yeah] who have engaged with us on this."  Although we're ignoring you in the case of www, but anyway.  "Chrome is a diverse, worldwide community, and we're lucky to have users who care as much as you do.  Keep the feedback coming."  So, you know, I congratulate them for being quick to respond, and they are - I would have been happy with a checkbox that said I'd rather stay in Google.  Delete everybody but Google.



LEO:  I think that would have been more sensible.



STEVE:  Yeah.



LEO:  But, you know, maybe there's some fatigue going on.  All right, fine, whatever you want.  Hey, I thought this was interesting, and I don't know if you have heard about this or not.  But I just updated to the latest version of Windows.  The Windows 1809 update came out today as part of Microsoft's update.  And as I'm updating Edge, I get an interesting notice:  "Issue with blocking third-party cookies."  It's something we've talked a lot about, third-party cookies.



STEVE:  Yeah.



LEO:  "The October 2018" - that's this one - "Windows cumulative update" - actually, no, that was one that came out just before, or maybe it's this, I don't know - "fixed an issue where Microsoft Edge's 'block only third-party cookie' setting was not working properly."  And that's a big deal because of course we recommend you turn on that setting, for sure.



STEVE:  Wow, yeah.



LEO:  "Third parties that add content to websites like advertisers can no longer place, see, or use cookies, even those that might have been previously placed in your browser.  As a result of this update, you may see some changes in your browsing experience," et cetera, et cetera.  But I thought that was kind of interesting.  There was apparently a flaw in their third-party cookie blocking, but they fixed it.  So just a little news.



STEVE:  And also, you know, I did that cookie stuff a long time ago, the Cookie Forensics.  And one of the behaviors that we noted - again, this is along the lines of you don't know it's broken.  You don't know if it's happening unless you see it.  I mean, you need tools to observe things.



And so when I created that set of tools that allowed you to very carefully understand the exact cookie-handling behavior of browsers, one of the things we learned is that browsers were doing different things with turning cookies on and off.  Like in some cases the cookie off switch prevented the acquisition of new cookies, but any cookies that were already there were still being sent back.  In other cases the switch did what you just described that Microsoft has fixed, which is even if cookies had previously been accepted and planted in your browser, setting the switch off would henceforth prevent them from being sent back.  So it's not always obvious what the switch does.  And in some cases people are caught out, caught by surprise.  It's like, wait, this doesn't do what I thought it was doing.  And in some cases it doesn't do anything at all.



LEO:  Yeah.



STEVE:  Good that they fixed that.



LEO:  Yeah, yeah, yeah, absolutely.



STEVE:  So we've often talked about the theoretical danger of a zero-day.  And when this couple months ago, I guess it would have had to have been in August, not long before the September, that is, the following month's patches because there was time for Microsoft to patch it.  But we'll remember that a, I learned subsequently, female security researcher was having a bad day, and she tweeted that she didn't really care much about life or something, and she also didn't feel like reporting the zero-day she had just found to Microsoft.  So here it is.



And this was the advanced local procedure call exploit which could be leveraged through Task Manager.  And of course everybody jumped on it.  Some other researchers realized, oh, not only 64 bits, but also 32 bits.  And not only Windows 10, but also Windows 7.  And so it was bad.  And I said at the time that it doesn't let bad guys get into your machine from the outside.  No doubt that's way worse.  But it's still very useful for malware to be able to elevate its privilege to a system level.  And in this case, since Task Scheduler runs as a system-level process, this allowed essentially a means to get Task Scheduler to elevate malware to system level.



Anyway, I wanted to sort of close the loop because, as predicted, malware was indeed quick to jump onto this zero-day privilege elevation bug.  Bleeping Computer reported that a ransomware known as GandCrab v5 has been seen leveraging this vulnerability to gain system privileges to more deeply infect a target PC.  And as we know, it got patched in September, so it's been patched.



But again, it's worth noting, I think, that in this world that we live in today, bad guys know that a disclosed vulnerability like this will have a very brief shelf life, at least in an environment where updates are flowing regularly and being installed continuously.  Which is certainly the case by and for most Windows systems.  I guess enterprise systems, where they're needing to more carefully vet updates so that they don't foul things up, there may be a delay.  But even so, malware authors, on the off chance that they're able to get into a system that has not been patched, are able to get up to additional mischief in a short period of time.



So as we know, there are systems that are not being kept current, such as, of course, Apache Struts, famously.  But even in Windows a zero-day vulnerability was jumped on quickly and is in use in the wild, hoping to find systems that are some number of weeks behind in being patched.  So there.  And also in this environment there's no way to view that public disclosure of this zero-day vulnerability as anything short of irresponsible.  I mean, if she found it and didn't feel like reporting it, well, just saying nothing would have been far more responsible than blabbing it publicly to the world because it has hurt people.  Which is unfortunate.  And I'm sure Microsoft would have fixed it without it going public.  Although we have seen Microsoft, in cases that we've reported recently, like being a little lazy about getting around to fixing things until their feet are held to the fire.  So who knows.



ESET has uncovered the first UEFI rootkit found in the wild.  And it comes, unfortunately, courtesy of this - I wish we could just all agree on one name for these Russians:  Sednit or APT28 or Sofacy or Strontium or Fancy Bear.  I like Fancy Bear.  Let's call...



LEO:  Let's stick with Fancy Bear, yeah.



STEVE:  I really do.  That one, that's got a little ring to it.  So henceforth known as Fancy Bear.  Let's everybody just call them Fancy Bear.  Anyway, this is part of a rootkit malware system known as LoJax, J-A-X, because of some other malware which was previously found to be leveraging that - I can't remember the name of it.  It's that stuff like Lenovo has in their BIOS, that system, it's kind of termed "LoJack," although it's compu - oh, Computrace, that's the name of it.  Computrace had been subverted and turned into malware some time in the past known as "LoJack" malware.  Anyway, so these guys called this LoJax.



And the problem is, because it's from Fancy Bear, it's state sponsored and is raising some concern.  This UEFI rootkit has the responsibility of dropping this LoJax infection module onto the system.  So the point is that you get your system infected.  It uses a signed utility, sort of like repurposes a signed utility to write to the flash ROM on the motherboard in order to install this UEFI rootkit, which links itself into the boot process and is then responsible for essentially re-dropping this LoJax infection module out into the system whenever the system's booted.  Which of course means that it is able to survive complete wipes and reinstallations of the operating system.  So if you thought something was a little weird, or something set off an alarm bell that, oh, wait, there's some bad stuff in my computer, let's reinstall Windows, well, it comes back.  Or let's change the hard drive.  It comes back.  So it can even, of course, survive hard drive swaps.



One of the things that is crucial, however, is that it cannot function in an environment of Secure Boot.  So it has not subverted the Secure Boot process.  And we did a whole podcast on the Secure Boot system and the technology, we've talked about it since, where Secure Boot functions, by starting with an absolutely trusted anchor, and that anchor checks the signatures of everything it loads before it runs it.  And this stuff does not have a valid signature, thank goodness.



So from I think it was Secure Boot began in Windows 8.  I don't think we had it in Windows 7.  So, yeah, I think it first appeared in Windows 8, and of course it's part of Windows 10.  Typically you are able to turn it off in the BIOS.  If you have for some reason turned it off in the BIOS, it's probably worth turning it on.  That is, running with it on, if it doesn't cause some sort of daily problem, because we are beginning to see instances where something, even if it gets into your system transiently, I mean, we're seeing now an instance where something that is briefly able to somehow arrange to run is able to get down into your firmware and then become persistent and be much more difficult to dislodge.



So the advice, as always, is make sure that you are making - like watch your system boot, look at the version number the BIOS reports.  Check the motherboard manufacturer to see whether they've updated usefully since then; and, if so, it's worth updating.  And it's almost certainly the case that updating the BIOS would re-flush and get anything that had written itself in and linked into the UEFI boot chain unlinked and out of there.  So make sure your BIOS is current, but also run with Secure Boot enabled when you're able to do so.



It is the case that booting something else like SpinRite, for example, in the future would require you to turn that off briefly in order to allow something else to boot.  So there are benign reasons for needing to do so.



LEO:  Or installing Linux.



STEVE:  Exactly.  And of course that was our great fear, remember, that we were worried that BIOSes would not allow it to be turned off, and so users would buy a computer where they had absolutely no control over the operating system.  So fortunately...



LEO:  Right, right.  Fortunately, that did not happen.



STEVE:  ...that did not come to pass, yes.  So I mentioned a brand new botnet, Torii, T-O-R-I-I, and also that botnets are becoming increasingly persistent.  The one thing we don't need is another botnet, yet that's what we have.  This was dubbed "Torii" because it attacks devices' Telnet ports through the Tor network.  So it's using Tor to obtain anonymity for its scanner, which is scanning through Tor in order to get to remote devices' Telnet ports.  What that tells us primarily is, first of all, don't have Telnet exposed if you don't need it.  And if you do, do not, just do not use any kind of human compatible password.



I think we need to come up with an acronym for that, Leo.  Are you using human-compatible passwords?  If so, stop.  We need bot-compatible.  We need, like, automation compatible.  Your password should not be human compatible.  It has to be something which is 64 characters long of gibberish that you have no chance of even possibly typing in correctly, especially not when you can't see what you're typing.  So just it's got to be a cut-and-paste password.  Ooh, I like that.  Maybe that's the way to do it, a cut-and-paste password.



LEO:  Cut-and-paste password, I like it.  Or key, you know, public key, that kind of thing.  I'd stay away from Telnet in general, though.



STEVE:  Oh, yeah.  Okay.  Well, so don't have it open if you don't need it.  If you do have it open, give it an impossible to manually enter password, something that you have to cut and paste in order to log in, or hopefully you're using a Telnet client, and you taught it how to log in for you.  But the point is it's using brute-forcing over time to get into the system.  And the problem is, once it gets in, it is really in there.  It is superior to the seemingly endless Mirai botnet variants which come and go because they lack a strong persistence mechanism.  By contrast, Torii appears to be all about persistence.  It has six different means for making sure that it survives reboots.  Avast has a wonderful blog post, I've got the link to it in the show notes if anyone's interested, I mean, it's really detailed and extensive.



They said of Torii, quote:  "The malware's dropper makes sure that the second-stage payload is executed and that it will remain persistent.  It is unique in that it is remarkably thorough in how it achieves persistence.  It uses at least six methods to make sure the file remains on the device and always runs.  And not just one method is executed.  It runs all of them:  automatic execution via injected code into ~\.bashrc; also @reboot clause in crontab; System Daemon service via systemd; etc/init and PATH, once again calling itself 'System Daemon'; modification to the SELinux Policy Management; and it's in etc/inittab."



So, I mean, it is doing everything in Linux that it can think of to get itself to run and not be removed.  And so if someone comes along and goes, "Oh, what's this in my bashrc?  I don't want that."  You take it out, it doesn't matter.  It has five other ways of achieving execution.



So Avast says:  "The infection chain starts with a Telnet attack on weak credentials of targeted devices, followed by execution of an initial shell script.  This script looks quite different, they write, from typical scripts that IoT malware uses in that it is far more sophisticated.  The script initially tries to discover the architecture of the targeted device and then attempts to download the appropriate payload for that device."  And this sounds familiar.  I've not ever talked about this particular botnet, but I've said the same thing about other botnets, which tells us this is what new malware is doing.



"The list of architectures," they write, "that Torii supports is quite impressive, including devices based on x86_64, x86_32, ARM, MIPS, Motorola 68k" - who's using that? - "SuperH and PowerPC, with various bit-width and endian-ness."  So all of that, both in little endian and in big endian.  So, I mean, if you've got a device...



LEO:  Most people use Python, I think.  That must [crosstalk].



STEVE:  Yes, yes, yes.  Oh, actually this one is written in Go.



LEO:  Oh, Go, yeah, yeah, that's a good language, too, yeah.



STEVE:  It says:  "This allows Torii to infect a wide range of devices running on these very common architectures."  Then I had in my show notes:  Torii is written in Go, allowing it to be readily cross-compiled for any supported processor architecture.  It's clear that the developers of the botnet are seeking broad coverage; so they have built binaries for all popular CPU architectures, tailoring the malware for stealth and persistence.  Also, communication with the command-and-control servers is encrypted, and capabilities include exfiltration of data that is behind the device and command execution.



So anyway, Avast concludes, saying:  "Even though our investigation is continuing, it is clear that Torii is an example of the evolution of IoT malware, and that its sophistication is a level above anything we have seen before.  Once it infects a device, not only does it send quite a lot of information about the machine it resides on to the command-and-control, but by communicating with the command-and-control it allows Torii authors to execute any code or deliver any payload to the infected device.  This suggests that Torii could become a modular platform for future use.  Also, because the payload itself is not scanning for other potential targets, it is quite stealthy on the network layer."  And they say:  "Stay tuned for the follow-ups."



So anyway, if you're interested, the Avast posting about this is very detailed and interesting in all that they talk about.  But what this suggests is we're sort of entering - we're sort of going beyond the let's see how many boxes we can commandeer in the afternoon and then blast some sites with DDoS to let's find places we want to set up permanent residence and deliberately behave ourselves so that we don't give away our presence.



And since it's communicating over an encrypted channel through Tor, and it's arranging, I mean, it's really concerned about maintaining its persistence, this suggests that - oh, and since it can download anything and reports all about its architecture and what it's finding, basically the people are investing in brute-force cracking into the system.  The Linux machine that's hosting a Telnet server and a Telnet service, setting up shop there, and then being able to download, knowing what architecture they have, then being able to download whatever things they want to in the future, and essentially becoming a persistent presence on that node.



So I say again, really, the Internet attack surface of networks is their routers.  And we are, interestingly, that is clearly where the next big threat is going to come from is beachheads being established.  And if you don't look, you can't determine whether you might be a host of such badness.



The government posted an interesting alert, just sort of kind of a newsflash.  It's not really news for us, but I thought it's still really, really important.  And it's yet another example of a particular service which is widely exposed and, I would argue, is really difficult to be safely exposed.  I use Remote Desktop Protocol myself for - it's just so convenient, being that I'm a Windows developer and a Windows user, and I've got multiple facilities in different locations.  But not a single instance of it is exposed to the Internet.  It's just crazy to do that.



Microsoft has had authentication problems with RDP in the past.  And we see instances where things like the early versions of SMB protocol, of Windows file-and-print services have had authentication problems.  I mean, it's difficult to get this right.  Anyway, last Thursday the ic3.gov site just posted sort of a public service announcement titled:  "Cyber Actors Increasingly Exploit the Remote Desktop Protocol to Conduct Malicious Activity."  And I won't go into it all because we understand this.  But they gave four concrete examples that I thought were interesting.



They said:  "The CrySiS Ransomware:  CrySiS ransomware primarily targets U.S. businesses through open RDP ports, using both brute-force and dictionary attacks to gain unauthorized remote access.  CrySiS then drops its ransomware onto the device and executes it.  The threat actors demand payment in bitcoin in exchange for a decryption key."  That's the first example.



Also:  "The CryptON Ransomware utilizes brute-force attacks to gain access to RDP sessions, then allows a threat actor to manually execute malicious programs on the compromised machine."  Similarly, these guys ask for bitcoin ransom to decrypt.



"SamSam Ransomware uses a wide range of exploits, including ones attacking RDP-enabled machines, to perform brute-force attacks.  In July of 2018, SamSam," they write, "used a brute-force attack on RDP login credentials to infiltrate a healthcare company.  The ransomware was able to encrypt thousands of machines before detection."



And then, finally, they refer to the Dark Web Exchange:  "Threat actors buy and sell stolen RDP login credentials on the Dark Web.  The value of credentials is determined by the location of the compromised machine, the software utilized in the session, and any additional attributes that increase the usability of the stolen resources."  And I was curious, so I browsed around a little bit.  And for about $11 or $12 you can buy a login credential for some random machine in some random country.  The ones I was browsing through did not have admin rights.  But you can buy them, and you can log into someone else's computer and see what's there, I guess.  It's crazy.  But, I mean, it's the reality of today.



So again, I don't think, I mean, it's very convenient to have Remote Desktop Protocol accessible from the outside.  The only way to do it is the way I do it, which is to put it behind a VPN, in my case OpenVPN, Leo, as you and I both do, and use certificates for authentication, not username and password, or not only username and password.  You have to do that in order to be secure.  And then the advantage of that is, once you have connected with a VPN, you are essentially on that network, and then you have access to all the goodies that are there.  So that is absolutely the way to configure things.



So they finish up, just under vulnerabilities they explain that the number one problem is weak passwords.  They say:  "Passwords using dictionary words or do not include a mixture of upper/lowercase letters, numbers, and special characters are vulnerable to brute-force attacks and dictionary attacks."  And of course the problem is that, unless you put limits on guessing, then someone could just sit there over months, trying and trying and trying until, bingo, they finally get in.  Unless you're monitoring and/or foreclosing on mistakes made trying to log in, you're not going to know that's going on.



So they also said the outdated versions of RDP may use - and here it is again - this flawed SSP, the encryption mechanism, which enables the potential for acquiring credentials during a man-in-the-middle attack; and leaving it on port 3389, which is the default port.  If you do nothing else, change it to some obscure port somewhere else so that you're not showing up in every Shodan scan of "Please give me a list of all the RDP ports that you know about, Shodan," which of course it does, if you ask it.  So by all means, I mean, it's not any substitute for other things.  But at least change the port that it's running on, which is relatively easy to do.



And in the account policies it's also possible to set a number of login attempt failures and lock the user out for some length of time.  Really, we're past the day when you should need more than three or four attempts to log into a system, especially when the system you're logging into probably remembers your credentials and does it for you the first time every time.  So it makes sense to set a policy to just say, you know, lock a user out.  And, yes, it can cause a little bit of admin backlash if someone is unable to log in, given three or four tries.  But it's a worthwhile tradeoff, I think.



And Leo, finally, this is another little tidbit that Bleeping Computer brought to my attention that I was unaware of, frankly.  I, as I'm sure you and our users do, I frequently go to a site that pops up a little dialogue underneath the URL.



LEO:  Yeah.  I hate that.



STEVE:  Wanting to allow notifications.



LEO:  This is really the bane of, when this started - I just hate it.



STEVE:  I know.  I completely agree.  And it's like, what?  No.  I don't want notifications.  I just want to do whatever I'm doing right there right now and then be left alone.  Well, it turns out that this is being used in a social engineering attack so that, for example, a site knows if it's going to be prompting the browser to make this request.  So now what sites are doing is they're putting up their own dialogue on the page to point to this prompt telling you - and I've got a picture in the show notes; yes, it's on the screen, thank you - pointing to it, saying "Click Allow," and they're pretending that they can't play the video or do whatever it is you want until you click "Allow."



LEO:  Oh, man.



STEVE:  Yes.  So they've escalated this to the next level.  Now, what our users need to understand is that, I mean, this is a powerful permission which sites are asking for and almost should never be given.  And that is that, even when you are not using your browser, when you've closed the window, when nothing is onscreen, this allows a website to pop up a notification down in the lower right of your screen in the notification area of your OS anytime they want.  And most users won't connect these events.  They won't realize that yesterday when the site said we need to allow notifications, and they said, oh, okay, and clicked "Allow," that, first of all, this allowance is persistent; and it allows an unassociated, it's not just when you're on the page or just when you're using your browser, it's anytime the browser is running that site now has permission to pop up a notification.



So the good news is you can retroactively rescind any of these allowances you have given in the past.  I've got the link in the show notes.  Bleeping Computer walks you through.  And it's worth doing a little auditing if you're worried that you have ever said, oh, okay, if you say so.  First of all, just say no.  Unless you're somewhere where you really do want to allow a site to pester you for whatever reason.  There are doubtless good reasons, maybe.



But certainly I mean, this is sort of like the number one rule of Internet hygiene, which is never download something because a website you're visiting tells you to.  No.  No.  Don't.  Just no.  Similarly, you probably don't want to allow a site, give a site what turns out to be a broad permission to pester you in the future.  The good news is you can poke around in your web browser.  Bleeping Computer will show you how, if you follow the link in the show notes, to go in and just remove any permissions you may have mistakenly given to sites in the past.



And, finally, get your popcorn ready for tomorrow's first-ever Presidential Alert, which we will all be receiving at a bizarre time, apparently beginning at 2:20 p.m. Eastern, or 11:20 p.m., Leo, during...



LEO:  Right in the middle of Windows Weekly, yeah.



STEVE:  ...Windows Weekly.



LEO:  Can't wait.



STEVE:  With Paul Thurrott and Mary Jo.  We will get the Presidential Alert.  And a class action lawsuit has been filed by three individuals who oppose the idea.  And so just for anyone who's clear, President Trump cannot send one from his phone.  This is not...



LEO:  I don't know if that's clear, but all right.



STEVE:  Okay.  Well, it should be.  This comes...



LEO:  It comes from FEMA.



STEVE:  Yeah, it comes from FEMA.



LEO:  But why do they call it a Presidential Alert, then?



STEVE:  That's a good question.  But it's supposed to be a terrorist attack, a national disaster, I mean a natural disaster, you know, a tsunami or something, I mean, like something really that merits alerting the entire cell phone-owning population of the nation.  So anyway, all that's happening tomorrow is that there's an alert that will be received and so forth.  So anyway, once again, we just will - it'll be interesting to see.  And for anyone who's interested, I do think it would be fun to be somewhere where there's lots of people, like being in a meeting.



LEO:  Yeah, all going off at once.



STEVE:  All going off at once, yeah.



LEO:  That'll be hilarity.



STEVE:  Yeah.  Okay.  So a little bit of miscellany.  As I mentioned, this evening, as I'm winding down, I will be finishing "Masters of Doom," which I really enjoyed.  I subscribe to the Kindle, what is it, Kindle, I forget the name of it.



LEO:  Oh, I know, the monthly, you get free books, yeah.



STEVE:  Kindle Everywhere.



LEO:  Unlimited, Kindle Unlimited I think.



STEVE:  Kindle Unlimited, where for a fixed price lots of books are available for free.  And so something, I don't know why they offered it to me, I'm not a gamer, I never really have been, although I did, I mean, I was so fascinated by the technology of Doom.  And it was a compelling...



LEO:  Oh, yeah, that was amazing, yeah.



STEVE:  It was arguably - it was amazing at the time.  Anyway, this is, for what it's worth, the story of John Carmack and John Romero, who were the original cofounders of id Software.  And it's a great story.  It starts out with both of them in their youth and follows them through the trials and tribulations of creating this series of first-person shooters.  And I just really enjoyed it.  So for what it's worth, I mean, for me - and Leo, you and I are about the same age.  We lived through this with our Apple IIs and our PCs.  And it was just sort of fun to go back through it.



I did want to mention I also finished the last book of the third trilogy of the Rho Agenda.  I talked about it first years ago, and it was very popular among our listeners.  Remember the first one was "The Second Ship."  Then there was "Immune," then "Wormhole."  And that was like three teenagers, a pair of twins and a friend of theirs, discover an alien spaceship, and it's the adventures that ensue from there.  There ended up being a total of nine books, three trilogies.  And they were fun.  I just finished a while ago the third book of the third trilogy.  I also am caught up on the Frontiers Saga, which is planned, Ryk Brown's series, five sets of 15 books.



LEO:  Oh, my god, he's...  



STEVE:  I know, 75 books.  And they really are fun.  Of course the first series of 15 is finished.  Now we're in the second series.  He's written the first eight of those, and I am now current with those.  And still a lot of fun.  And Leo?



LEO:  Yes?



STEVE:  Peter F. Hamilton is back.



LEO:  Oh.



STEVE:  Yes.



LEO:  Yes.



STEVE:  Yes.  I would argue our absolute number one favorite author.  Peter has never written a short book, or at least not recently.  Actually some of his earlier - he's done some short stories in the past.  This is titled "Salvation."  It is the beginning of another new series, so maybe people will want to wait.  We were annoyed when we got through "Pandora's Star," which was fantastic, but it left us hanging until we had "Judas Unchained," which was the second of that pair which told the whole story.



So a little bit about the so-called "Salvation Sequence."  This is a brand new world or brand new environment, so it's not the Common...



LEO:  He's so good, yeah.



STEVE:  Yes.  It's not the Commonwealth Saga that he had so much fun with.  The little synopsis says:  "In the year 2204, humanity is expanding into the wider galaxy in leaps and bounds.  Cutting-edge technology of linked jump gates has rendered most forms of transportation, including starships, virtually obsolete.  Every place on Earth, every distant planet humankind has settled, is now merely a step away from any other.  All seems wonderful until a crashed alien spaceship of unknown origin is found on a newly located world 89 light-years from Earth, carrying a cargo as strange as it is horrifying."



LEO:  Oh, boy.  That sounds fun.



STEVE:  I know.  I'm getting goosebumps just reading this.  Because, I mean, it's Peter.  Who can imagine what it's going to be, but it's going to be incredible.  "To assess the potential of the threat, a high-powered team" - and you can imagine in his world what that means, I mean, you can't imagine, it'll be beyond our imagination - "is dispatched to investigate.  But one of them may not be all they seem."  And then, finally:  "Bursting with tension and big ideas, Peter F. Hamilton's 'Salvation' is the first book of an all-new series that highlights the inventiveness of an author at the top of his game."



LEO:  How exciting.



STEVE:  Oh, Leo.  Okay, I'll see you next month.



LEO:  Now, are we going to get the first one and then wait a year and then the second one?



STEVE:  I'm afraid so.  I mean, these are...



LEO:  Yeah, what are you going to do?  Yeah.



STEVE:  Yeah, but I'm not waiting.  What I end up doing, what I did with "Pandora's Star" and "Judas Unchained," was I reread "Pandora's Star" just to get back in and get rolling again.  Oh, but his stuff is so fresh and so new and so imaginative.  I mean, and that's, frankly, that's what sets it apart.  Like the Frontiers Saga, it is, it is a saga.  It's character driven.  Nathan and Cameron and Jessica and, I mean, there's like a whole bunch of other - Telles and a bunch of people.  And everyone who's been reading them knows who these characters are.  And so it's just sort of you go along, and it's what happens, but nothing much.  With Peter Hamilton, you're stepping into real imagination, and that's what I really love so much about him.



LEO:  Good.  Oh, I can't wait.



STEVE:  So anyway, I had a long - I was going to talk about SpinRite.  I had a long DM, a direct message, from somebody in Germany, Stefan B., but it's kind of involved, and I need to whittle it down because it's so long and detailed.  And I didn't get to it in time.  So we'll probably talk about him next week.



LEO:  Oh, okay.



STEVE:  In the meantime, we'll have our final...



LEO:  Take a little break, yeah.



STEVE:  ...break, and then we'll talk about the Facebook breach.



LEO:  Oh, man.  Have you ever been on Facebook?  I don't remember.



STEVE:  No.



LEO:  I didn't think so.



STEVE:  I have an account because I needed to talk about the various security and privacy features.  So I've had to go in from time to time.  But no.  And I've got all these people, I mean, I think all these friend requests.  But I'm thinking, okay, I just, you know, I just - I can't.  No.



LEO:  No.  You're smart.



STEVE:  No.



LEO:  Honestly, you're smart.



STEVE:  Oh, and I did have a high school friend who hiked the whatever that is, the top of the mountain, the Pacific something or other trail?



LEO:  Pacific Rim Trail, yeah?



STEVE:  Yes, from San Diego up to Canada.  And so he was posting on Facebook, of all places.  And it's like, oh, crap.



LEO:  You can still read Facebook posts without an account.



STEVE:  No, I thought you had to be - back once upon a time, at least, you had to have a Facebook account in order to see Facebook people's stuff.



LEO:  Not if they're public.  If they're public postings, you can read it without logging in.  Of course if they're "friends only" or somehow limited, you'd have to log in just to [crosstalk].



STEVE:  Well, and of course that is a perfect segue into what the nature of the breach is because it was a mistake in the "View As," "View My Page As" that was the cause of the problem.



So I think the thing that has received the least attention, but it is certainly significant about this Facebook breach, when you hear 50 million accounts were compromised, and another 40 one level removed may also have been, you sort of think, oh, look, mass collection breach of some sort.  That's what we're used to, like when a web service loses its database it's well, you know, how many just like randomly chosen records out of total got loose?



What's significant about this is that this particular breach was absolutely targetable by its nature.  And in fact Mark Zuckerberg and Sheryl Sandberg's accounts were both targeted and potentially compromised.  So the point is that the people who discovered this had to do a lot of work.  It's a sophisticated attack.  So somebody was poking at Facebook for some period of time because Facebook is Facebook.  And the ability to perform the attack occurred in July of 2017; right?  So 17, 18, I guess, months ago, 17 months ago.  So quite a while ago.  That's been there for all this time.  The only way the breach was discovered was a side effect of the way it works is people apparently logging in.



And essentially what you're able to do, and I'll explain a little bit what we know of how, you're able to cause an unauthenticated login of someone else in order to bring their login token current.  And it was the sudden increase in the background continuous and certainly high level of that happening.  But it suddenly spiked.  And so the Facebook people said, you know, the engineers said, wait a minute, why are we suddenly seeing a dramatic increase in the rate of people logging in?  And that's what tipped it off.



But we don't know over what period of time this thing occurred.  And the point is that it's completely stealthful unless someone notices by looking at their own account, wait a minute, somebody logged in from somewhere else and left a breadcrumb behind, or something gets changed.  Okay.  So the crux of this is a feature that's sort of scary, but also certainly useful.  I remember, I guess it was when I was playing with trying to get a hold of and get a handle on the way Facebook's viewability stuff works.  As I said, Leo, I do have a Facebook page, and I don't use it.  I don't think there's anything posted on there.  But I created the account in order to see other people's and also when I wanted to explore the Facebook privacy and security settings because I've talked about them a couple times.



And so certainly one of the things that is very useful, because in any environment where you want to curate who can see what, is you want to test that.  You want to view your own page as if you were someone else.  And, well, that's impersonation.  And it turns out that Facebook understood that this was dangerous, but they also understood they needed to offer the feature.  So you've always had this "View As" feature, which would allow you to cause Facebook to show your page as if you were someone else.  And you get to specify whom.  So if you said "I want to see Mark Zuckerberg's view of my Facebook page," on some level there's been the invocation of Mark Zuckerberg's identity into viewing your Facebook page.  So that was part of this, is the way "View As" works.



Then it turns out that there's a video tool, a video upload capability which allows you to send somebody like a birthday greeting, upload a video.  And it's that tool which acquired a bug in July of 2017 which caused it to incorrectly manage the identity tokens that it was having to juggle, the authentication of different people such that the bad guys were able to, in a multifactor attack, to leverage "View As" and the mistake in this video upload tool in order to obtain a current logged-in authentication token for - and here it is - anyone they wanted, which is what makes this so scary, is this thing has been in place since July of 2017.



We don't know that it hasn't been used, that it hasn't been known.  Who knows how many mysterious breaches or people's pages being changed or mischief or what has been gotten up to during that time because, again, it wasn't, you know, Zuckerberg's was compromised.  Sheryl's account was compromised.  So clearly, before they began a much higher speed wholesale rifling, they were going after high-profile targets, it seems.  So that's now been closed.  Facebook, understanding the nature of the breach, is able to go back in their logs, see this being done.



And then what happened on Friday was that 50 million people, actually 90 million people suddenly found themselves logged out of Facebook.  And it was like, you know, everyone's used to Facebook leaving you persistently logged in because it's convenient.  Well, and as we know, what that means is that you have a cookie in the form of an authentication token from Facebook, which your browser presents whenever it pulls any resource from Facebook, which authenticates you as being logged in as you on this browser.  So all of those authentication tokens were immediately invalidated, and consequently people had to log in again.



Now, the reason, sort of the other shoe here to drop, is that it's bad enough that a bad guy could obtain an authenticated session for anyone they wanted.  Now, that's what this means.  I mean, since July of 2017, anybody who knew about this could obtain a valid logged-in session for anyone they wanted.  But what that means by extension is that the now unfortunately very popular "Login with Facebook" can be abused because it means that your browser contains a valid Facebook login token for anyone you have targeted, which means when you log in with your Facebook account, you're logging in with their Facebook account, meaning that you're able to log into any other third-party service where they have an account recognized by Facebook.



So, for example, under single sign-on, or of single sign-on, in Wired magazine's coverage of this, they said:  "The debacle also underscores broader concerns about single sign-on, which Friday turned into the ultimate object lesson in the inherent tradeoffs between security and convenience.  Kenn White, the director of the Open Crypto Audit Project, said:  'Single sign-on schemes are great; but the downside is, if a single sign-on gets breached, you're hosed.'"



So Wired says sticking with a single more secure sign-in does make sense, especially for use on sites that don't have the resources or inclination to invest heavily in security development.  Actually, none of that's true.  Because, well, anyway.  But just like you want your passwords to be unique, so compromising one doesn't expose them all, account diversity is also vital online, no matter how ironclad a particular sign-in scheme is.  Then they say, quote:  "You don't want a situation where there's one breach, and your entire online identity is gone," says the same guy, Kenn White.



So anyway, that's what happened.  It was longstanding.  It was, I mean, and again, it's hard to do what Facebook is doing at the scale they're doing it.  I would argue that View As is necessary and terrifying.  I mean, it is inherently a horrifying, from an implementer's security standpoint, I mean, it is a real pucker factor because, boy, you've really got to get it right.  And it looks like largely they did; but in one place they missed something, and someone found a little way to wedge themselves in and leverage something which is - I mean, and Facebook, obviously, as we all know, is really staggering at the moment under, you know, you just said it, Leo, you deleted your Facebook account.  I mean, they've been staggering under all kinds of privacy issues as a consequence of being, what is it, two billion users?



LEO:  Two and a half, yeah.



STEVE:  Yeah, being so popular.  I would argue against Zuckerberg, who said - he called it an arms race?  It's like, no.  I mean, you know, it's hard to get it right.  I absolutely agree.  And here I empathize with them that, if you're going to have the ability to show different classes of users different views of your content, then you need to be able to audit that, which means you need to be able to say, "What would so-and-so see of my page?"  And it's like, okay, scary.



But it is actually a perfect example of, I mean, I already hate OAuth.  I consider it a horrific kludge because, as we know and we've talked about, users don't understand that when you log in with Google or log in with Facebook or log in with Twitter or whatever, the reason those services are happy to provide this referral service is they're tracking you.  They're knowing everywhere you're logging in and adding it to their compilation of your profile, and you're updating your information about them.  And as we've just now seen, if that service suffers a breach anywhere you have used them, you're now subject to, I mean, they've extended the blast radius beyond their own perimeter to everywhere else.



So anyway, it's one of the reasons why, when I first told us all about SQRL years ago, I said it's a two-party solution.  It's secure single-factor.  It's like the only reason you need multifactor is none of them are secure enough, so you need lots of them in order to increase the security.  If you have secure one-factor, then it's all you need.  And the beauty of it is it's just two-party. The problem with this "log in as" is you're bringing in a third party, and here's an example of it going wrong.



So anyway, that's what happened with Facebook.  And again, I don't think - I'm not saying, oh, this is a glaring problem.  I mean, I would argue the reason they're already in hot water, that has been Facebook's policy problems that have come home to roost.  Here, this was a really obscure bug, and anybody can make a mistake.  They did get it fixed quickly.



And to our listeners I would say take this opportunity to go use the Facebook tool which they provide of where have logins to your account occurred, from which devices and from which locations.  Just make sure you recognize them all because that's worth doing.  And also eliminate any applications that you've given permission to use your Facebook account that you're no longer using.  As we've said, that kind of periodic audit can be a good thing to do.



LEO:  Yeah, maybe this was just a wakeup call.  Is there a safe way to use single sign-on solutions like that, OAuth solutions?



STEVE:  The problem is users are not given enough control.



LEO:  Right.



STEVE:  There ought to be a way to revoke the otherwise persistent relationship of the connection between the authentication token that you give the third party and the site you're visiting.  Facebook did also kill all of those referral tokens, essentially, which was good.  But the problem is it's done behind the scenes.  And so it's mean to make the system easy to use.  But  because users are not given visibility into it, it's not something that's easy to do.  It's better just not to have it in the first place.



LEO:  Used to be you could do your own OAuth, I remember.  You could have your own site.  But I don't know how that would work if a site didn't know about it.



STEVE:  Yeah, remember you were able to put a token on a page on your server.



LEO:  Yeah, put it on your server, yeah.



STEVE:  And bounce you through.  



LEO:  That would be so much better.  I guess it wouldn't help the average Joe.  But, yeah, that's the problem.



STEVE:  Yeah, yeah.  The average Joe is going to have a solution here pretty soon.



LEO:  It's called SQRL.



STEVE:  Whee.



LEO:  SQRL.  Well, we're going to let Steve get back to his SQRL right now.  We do this show Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can come by, watch us live at TWiT.tv/live.  You can also get an after-the-fact, on-demand audio version of this show from Steve's site, GRC.com.  He also has transcripts so you can read along as you listen.  Those are all at GRC.com.  While you're there, pick up a copy of SpinRite.  If you have a license to SpinRite now, you'll have a license to 7.0; right?  Automatically?



STEVE:  Not 7.0, but all the 6.1s. 



LEO:  Sixes up to 7.



STEVE:  Right.



LEO:  That's GRC.com.  And there's also lots of free stuff there.  Everybody should take a look at it.  It's a fun site:  GRC.com.  We have audio and video versions of the show at our site:  TWiT.tv/sn.  You could subscribe in your favorite podcatcher.  You'll get it automatically the minute it's available.  I think that's about it, Steve.  We have concluded.



STEVE:  Okay, my friend.



LEO:  And I'll see you next time on Security Now!.



STEVE:  Thank you, Leo.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#684

DATE:		October 9, 2018

TITLE:		The Supply Chain

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-684.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we examine and explore an October Windows Surprise of a different sort.  A security researcher massively weaponizes the existing MicroTik vulnerability and releases it as a proof of concept.  Israel's National Cybersecurity Authority warns about a clever voicemail WhatsApp OTP bypass.  What DID happen with that recent Google+ breach?  Google tightens up its Chrome Extensions security policies.  WiFi radio protocol designations finally switch to simple version numbering.  Intel unwraps its 9th-generation Core processors.  We've got head-spinning PDF updates from Adobe and Foxit.  This isn't a competition, guys!  And, finally, we take a look at the danger of Supply Chain Attacks, with a possible real-world example.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Great show planned for you.  We're going talk about how hackers use your voicemail box to get into WhatsApp accounts.  We'll talk about the problem with Windows 1809.  Microsoft thinks it knows what went wrong.  And, yes, the big debate over the Bloomberg Business Week story about Supermicro.  Apple denies it.  Bloomberg stands by it.  Who's right?  Steve weighs in.  It's next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 684, recorded Tuesday, October 9th, 2018:  The Supply Chain.



It's time for Security Now!, the show where we cover the latest security news and ways to protect yourself.  It actually is all about understanding technology, too.  And it's all thanks to this guy, who understands it better than anybody, Steve Gibson from GRC.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again, as always.  So you asked if this was the SQRL episode, and I think what we'll do is probably have me up there live for that.



LEO:  Good, good.



STEVE:  Because what I really want to do is I think get a couple of your gang together and do it sort of as an adversarial mode, where I explain what it is, and then you say, "But what about this?"  "But what about this?"



LEO:  But what about that, yeah, yeah.  Because that's what people say when you talk about it.



STEVE:  Exactly.  You and Mike Elgan and Jason probably would be a great board to say, okay, what about when this happens, and what about this.  Because I think there's an answer for every possible "but what about" question.  So anyway, I'm in the process of bringing up a third portal, essentially.  I had to develop an API, a generic API, which I call the SQRL Service Provider API, because I want to bring it up for the SQRL forums, which are written or developed, well, they are XenForo forums.  And while you can sort of add things to it, one of our listeners is a XenForo developer.



And so I thought, boy, you know, rather than having to learn PHP and then figure out all about what XenForo was doing, if I could give him a clean API to a SQRL Service Provider backend, then he could just write to that; and, of course, so could everybody else.  So it's like it was a piece that I didn't appreciate having would really improve the adoption ease and rate.



So that's done.  I've written that.  I defined it and wrote it.  And as far as I know it's finished, but now I need to test it.  So I'm creating a third site because I already have the GRC demo site for SQRL.  We'll have the XenForo forum.  So there'll be a third one which explores, well, it's designed for me to test that backend API.  But it'll also be a neat way for people to play with SQRL.



And the other thing it offers, and the thing that the API has, is something we call the "managed shared access" because one of the things that SQRL fights against is people giving their username and password away.  You know, how many people are like, oh, you need to log in?  Here's my username and password.  So sharing username and passwords is a common practice because, well, because you can.  But SQRL doesn't really support that.  I mean, you have an identity, and you don't want to give somebody else your SQRL identity because that represents you for the world.  So we've solved that problem, and this allows people to understand how that works and to play with it.



So anyway, people say, my god, Gibson, how long is this taking?  Well, yes, it's taking a while because really, really, really solving this problem is something that nobody else has done.  Everybody else has sort of played at solutions or solved part of the problem.  But really solving it all the way is a little more challenging.  But, you know, I'm getting there.  So this is not the SQRL episode.



LEO:  That's a long way of saying no.



STEVE:  This is the Supply Chain episode, our Episode 684 for October 9th, where we're going to talk about the rather, I mean, calling it earthshaking is probably not overstating it.  It's also controversial because the Bloomberg story, which is what begat all of this, lacks evidence.  And whenever that's the case, you can have the affected companies, like Apple and Amazon, screaming their denials out loud.  And we're left with the, I hesitate to say in this day and age, the infamous "he said, she said."  But, you know, its they said this, and the other people said that, and they're diametrically opposed.



But the point is, independent of whether it's true, and in fact just minutes ago I became aware of an additional Bloomberg story from this morning thanks to our listener Simon Zerafa, who when I went to tweet the link to the show notes, which you have probably grabbed from my tweet, Simon had just tweeted the link to this updated story, which has additional corroboration of the problem, different fact set than what Bloomberg reported.  And because the idea that a major motherboard supplier, that is, Supermicro, was having at least some of its subcontracted motherboards infected with a hardware implant, that's big news.



And we got the word "implant" from the Snowden documents originally because this is something that our own NSA is alleged to have done.  In fact, one of the things that they do is intercept shipments and make a subtle alteration of the hardware that cannot be found.  So independent of whether this particular instance is true, talking about the supply chain in general is certainly worthwhile.  So that's how we're going to end the podcast.



We're also going to look at a different sort of October Surprise which dropped for a number of people who went to update Windows 10 with the October feature update and were unhappy with the consequences.  Thus the October Surprise and our Picture of the Week, as you just noted.  We have a security researcher who has massively weaponized an existing pair of MikroTik router vulnerabilities and released it as a proof of concept on GitHub.  A very clever voicemail hack which bypasses WhatsApp's authentication, its texting authentication, which we'll talk about.  And actually we talked about something like this before.  So it'll be interesting to see that it's actually happened.



We have the question of what happened with that recent Google+ breach, which has a lot of people screaming and claiming that Google was keeping this a secret.  We'll look at that.  Also they've tightened up their Chrome extension security policies, which is good.  And a welcome change to the Wi-Fi Alliance's ever-confusing radio protocol designations, those 802.11a, what, asdf or who knows what.  Anyway, that's being all changed.  We've got Intel unwrapping its 9th-generation Core processors, which we look at, of course, from the standpoint of what have they fixed of this year's treasure trove of Security Now! content with the Spectre and Meltdown problems.



We've got some head-spinning PDF updates from Adobe and Foxit, where we have to remind them, guys, this is not a competition to see who can have the most problems to fix all at once.  And then we'll talk about the inherent danger of supply chain attacks.  And, I mean, what a real, I mean, I would argue insurmountable problem they represent if we're going to - if the lowest bidder is going to be the people who build our products, and they're not our friends.  So I think something great to talk about.



LEO:  Lots to talk about.



STEVE:  Lots of great stuff to talk about.  Unfortunately, people last week who manually checked for updates were treated...



LEO:  Seekers.



STEVE:  Oh, seekers, yes.



LEO:  That's what they're called, yes.



STEVE:  Well, seek and destroy.



LEO:  In fact, I installed it on three machines with no problem.  But obviously not everybody did.



STEVE:  Yes.  And certainly not everybody has a problem.  Unfortunately, those who did had a big problem.  So what happened was that the original plan was that the Windows 10 October feature update was scheduled to be part of today's Patch Tuesday, at which point it would have been automatically found and downloaded and installed.  But it was also put up on Windows Update a week before, where people who manually check for updates would find it and could install it.  The thing that, you know, in retrospect, and I know this is a cheap shot at Microsoft, but their page describing it says:  "With the release of the Windows 10 October 2018 Update" - and that'll be version 1809, right now we're at 1802 - Microsoft says, "we're empowering a new era of personal productivity."



LEO:  Clear your desk.



STEVE:  Yes, exactly.  You know, all those files that have been piling up in your documents folder, really, those are just getting in your way.  Those are slowing you down.  We're going to get rid of those for you.  So what happened was, for those who don't know, some who updated, presumably in anticipation of  being empowered by this new era of personal productivity, soon discovered that all of their personal documents had been irretrievably deleted from the computer.  Seriously.  And that was only one of a number of problems that this thing had.



Now, okay.  We know that when - apparently it had something to do with Windows Cloud syncing, when syncing was not in use, although we haven't yet had a full articulation of why some people were hit and others were not.  But a chunk of people were.  And we know that, when files are deleted from a hard drive or, well, any mass medium, the entry in the directory is eliminated, and then the space that the file was occupying on the medium is just marked as available for reuse.



Thus Norton's original claim to fame of "undelete," where he realized that, oh, you could remove the E5, which was the marker for this is an available entry, and guess what the first character was.  Or the user could say, oh, I remember what that filename began with, using the rest of the name.  And then, subject to the drive's fragmentation, which made guessing where the file was a little more tricky, you could unmark those sectors as free, that is, mark them as in use and oftentimes reclaim your file.



Well, that's exactly what happened here.  When people were initially calling Microsoft saying, "Hey, all my documents are gone," the Level 1 support people said, "What?  We don't know anything about that."  Later, when Microsoft realized, oops, we have a problem here, and they started looking at it more closely, the advice changed to step away from your computer, stop using it immediately, and we'll get back to you.



Later down on the new era of productivity page, under the symptom - well, first of all they said "known issues updating to Windows 10 v1809" on that page.  They added under "known issues" only one symptom:  "We have paused the rollout of the Windows 10 October 2018 Update (version 1809) for all users as we investigate isolated reports of users missing some files after updating."  Yeah.  Like the ones that System Restore will not restore.



So under "workaround" they said:  "If you have manually checked for updates and believe you have an issue with missing files after an update, please minimize your use of the affected device and contact us directly at 1-800-MICROSOFT, or find a local number in your area.  Alternatively, use a different device to contact us.  If you have manually downloaded the Windows 10 October 2018 Update installation media, please don't install it, and wait until new media is available.  We will provide an update when we resume rolling out the Windows 10 October 2018 Update to customers."



And as this Picture of the Week shows, some poor guy named Robert Ziko posted on October 3rd, which would have been last Wednesday, under the title "Windows 10 October 2018 Update v1809 deleted all my files."  He posted:  "I have just updated my Windows using the October update (version 1809).  It deleted all my files of 23 years in amount of 220GB.  This is unbelievable. I have been using Microsoft products since 1995, and nothing like that ever happened to me."



And then he says:  "Files were located at C:\Users\rober" - R-O-B-E-R, and of course his name is Robert Ziko - "\Documents."  He says:  "This location is still present, with no files.  All of the files deleted.  I am extremely upset.  Not sure what to do.  Please let me know."



So anyway, as we know, recovery is possible for those who were affected, subject to, you know, reuse of the space and also the degree of fragmentation.  This is why I've still sort of thought, I mean, I don't defrag SSDs, but I do make images of my system.  Like I have got a nightly imaging system, like a nightly imaging program now established, and with incremental images.  And, I mean, I'm not taking anything - and of course that's what everyone says, right, in response to Robert is, well, you did make a backup; didn't you?



So anyway, this happened.  Microsoft stopped it.  It's gone away.  It's not part of today's Patch Tuesday.  In addition, there were reports that Task Manager was showing incorrect CPU usage levels.  Microsoft Word, at least versions 2016 and 2019, would no longer save documents, no matter what anybody did - Save As, Save.  But apparently that had something to do with add-ons, a conflict with Word add-ons such that, if you went into add-ons and disabled them, then Word would again save your documents.  So if there is anybody who's listening to this who's been stuck and can't reboot or shut down or close because they've got unsaved documents, that apparently is a workaround.  And then a number of games - Battlefield 1 and 4, Assassin's Creed, Hitman, Watch Dog, and Rainbow Siege - reportedly stopped working for some people after this.  So Microsoft has some work to do.  I'm sure they'll get it fixed.  And hopefully people did have backups, and this did not represent a catastrophic loss for them.



For those who can, our takeaway is this is probably worth setting that deferral on feature updates to, like, 60 days.  Unfortunately, Windows 10 Home does not give you that option, which is really infuriating, that all other versions of Windows offer it, but Microsoft said we're going to be stingy.  We're not going to let Windows Home users defer the feature updates.  But for those not using Home - Pro, Enterprise, and others - you can, under the Advanced Settings for Windows Update, Advanced Settings, you can elect to defer feature updates for up to 365 days, for up to a year.  I would argue, you know, for old really cranky people, fine, set it for 365 days, and you'll never get a features update.



Sixty is certainly fine because, if a features update is going to cause a major meltdown, it'll be detected within two months, Microsoft will fix it, and your system, having patiently waited a couple months, will have been spared whatever pain that update may cause.  So really I would say at this point definitely defer features updates for 60 days.  I mean, you can always then decide you want to bite the bullet after a week of no problems.  Or maybe set it to a week of deferral, I don't know.  But clearly we're in a world now where this forward-rolling continuing eras of enhanced personal productivity being offered by Microsoft can sometimes backfire on people.  So I would say not.  Or if you have a plan to image your whole system and/or all of the important parts, then, yeah.  Go ahead.  Be a leader.



Okay.  So this is really interesting.  Unfortunately, MikroTik routers are back with us.  An interesting and really good case study of leveraging multiple vulnerabilities unfortunately comes home to roost here.  So we'll remember that the MikroTik routers which did not have a patch applied back in April ever since then have been vulnerable to an exploit which had widely become known because the patch itself was reverse-engineered, as we've often talked about can happen and is increasingly happening because people have like a window of opportunity.  Looks like it's bigger for MikroTik routers than places where systems are updating themselves autonomously.



But back in April MikroTik offered a patch.  It got reverse-engineered, which allowed unauthenticated read-only access to any file on the router.  Well, that's not good.  What's also not good is the fact that the user database file that contains all of the authenticated username and passwords is stored in plaintext.  No hashing of the passwords, no obfuscation, nothing.  It's just so an unauthenticated user, as of April when this became publicly known, can obtain that file, which then allows them to look up the admin username and admin password in the clear.



And the issue then was what mischief could they get up to.  After all, this was read-only access to any file.  So it's not good, but it's also not the end of this router's life.  However, it was possible to leverage that a little bit.  There was a way to use the admin login credentials with that web interface to change a file.  And in this case, as we reported, the error.html file was changed in the routers so that an HTTP error which went through the router's web proxy would return a page containing the Coinhive browser-based cryptomining software.



And remember that, at the time when we first talked about this, whereas a lot of these routers, as we've talked about recently, are $49 plastic boxes for homes and small offices, there are also big iron MikroTik routers for thousands of dollars rack mounting, which serve enterprises.  And in those cases, if that web proxy got infected, then every user who hit an error page behind that router in an enterprise would, while that page was onscreen, be mining Coinhive.  We don't know how much money that they were making.  I don't think it was netting them a lot.



Okay.  So that was the history.  Near the end of August - so, what, six weeks ago, I think it was the 23rd of August - MikroTik security blog posted the following notice.  They said:  "Security issues discovered by Tenable," which is a well-known, reputable firm that we've talked about before.  MikroTik wrote:  "MikroTik was contacted by Tenable, Inc., who had discovered several issues in RouterOS web server.  The issues only affected authenticated users," okay, so that's different.  The previous one was an unauthenticated, nominally read-only attack.  This one authenticated users, meaning, says MikroTik, "to exploit them there must be a known username and password on the device."  Well, you can already see where this is going.  Whoops.



"Your data, access to the system, and configuration are not under risk," writes MikroTik.  "All the below issues" - four of them - "only allow the authenticated user," and they say "even a read-only user, to cause the www service to crash.  Tenable has assigned CVE numbers to these issues."  There are four of them.  They're 1156 through 1159, of course in 2018.  And their titles are "An authenticated user can trigger a stack buffer overflow."  Which we know is not good.  "File upload memory exhaustion.  An authenticated user can cause the www binary to consume all memory."  Or "Recursive JSON parsing stack exhaustion, which would allow an authenticated user to cause crash of the www service."  And, finally, "Www memory corruption.  If connections are initiated and not properly cleaned up, then a heap corruption occurs in www."



Okay.  And then MikroTik finishes their security note, saying:  "All of the above issues are fixed in the following RouterOS releases:  6.42.7, 6.40.9, and 6.43."  So in all of those four potentially serious problems, MikroTik was careful to remind us and reiterate that only an authenticated user could cause these problems.  That was good.  However, in their disclosure they failed to be explicit about the fact that one of those bugs allowed an authenticated attacker to run any program of their choosing by making a web request to the router.  But again, that attacker would need to be authenticated.



Okay.  However, in that previous bug, what it allows, thanks to allowing the passwords file to be obtained by an unauthenticated attacker, that allows them now to become authenticated.  So in other words, by leveraging both of these problems, neither one by themselves being devastating, but used in conjunction they provide - the first one, which is otherwise much less severe, provides enough information for the second one, which would otherwise be much less severe, to essentially allow an attacker to cause the router to run any externally provided file that they choose as root.



So unfortunately Tenable, and while still obeying reasonable disclosure guidelines because, after all, a patch was immediately forthcoming and was made available, they posted a proof of concept on GitHub with full source, showing how to build the source and how to remotely attack any doubly unpatched MikroTik router.  In other words, MikroTik was relying upon the fact that no unauthenticated user could become authenticated since April in their representation of how bad this newer problem is.  But we now know that the Internet is full of many hundreds of thousands of MikroTik routers which are still today vulnerable to the problem from April, meaning that they are now also vulnerable to this proof of concept, which is widely available, and exploitation of it has erupted on the Internet.



So at this point what we have is the news of its exploitation, and without specifics.  We know that Mirai has used it.  We know that it's been used by this cryptominer injection.  And we know that, as we were discussing last week, the VPN filter that Cisco Talos Group spent the summer analyzing, is also another user of the previous exploit.  Or maybe this one.  It wasn't clear whether the attackers may have been leveraging this latest one, as well.  But we now have a situation where, publicly known by leveraging both of these problems disclosed this year which persistently exist in hundreds of thousands of these MikroTik routers, it is possible to simply cause the web server to fetch a remote file and run it as root and gain additional strength.



What I liked about this was this was such a clean and clear example of the concept that we've often discussed, where multiple vulnerabilities by themselves in isolation may not be nearly as devastating as chaining them in order to create something more strong.  And I would also say, again, we know anyone can make a mistake.  Mistakes happen.  MikroTik is responsible, in my opinion, in patching them and responding to them immediately.  Props for that.  But their design policies they are responsible for.



And the policy which we've discussed, as we've looked at this more closely, which is their policy, of requiring a sophisticated understanding of remote access security from an unsophisticated purchaser of a $49 retail plastic box for the home or small office is absolutely wrong.  A lot of people, our listeners, have defended MikroTik, saying, hey, it's got all these bells and whistles.  You can lock it down tight.  And as we found, yes, but it doesn't come locked down tight.  You don't have to turn these things that you want on.  They're on unless you turn them off.  And that is absolutely wrong.



And as a consequence, I would argue that that they have earned and deserve the reputation damage that they are currently suffering within the security community.  Most users are not listening to this podcast, and they'll never know.  Unfortunately, people who purchased those MikroTik routers and haven't updated them since April are inviting all kinds of problems in their networks.  And now we know, as a consequence of what Cisco found, that these things are setting up shop.  They are, what was it, six different ways of making sure they're not removed, and they're turning around and starting to look into the networks to see what's there behind them.  Which is the worst-case nightmare for something getting onto your enterprise network.



Okay.  Here's a clever attack, which is clever because, again, it's sort of multifaceted.  It uses sort of the unintended consequences of the way an overly complex system works.  We've talked about it in the past.  A security expert warned about it in the past, like a year ago.  And it's happened.  So a wave of reports about hijacked WhatsApp accounts in Israel induced the Israeli government's cybersecurity agency to send a nationwide security alert last Tuesday.  Huh.  We got our nationwide security alert on Wednesday.  Anyway.  Of course, that was just a test.



The alert, authored by the Israeli National Cyber Security Authority, warns of an effective method of hijacking WhatsApp accounts using the mobile provider's voicemail systems.  And as I said, it's clever and tricky.  But it's been known of in theory for some time.  It was first noted a year ago by an Israeli web developer at Oath, and we talked about this at the time back then.  The underlying realization was that WhatsApp users who have voicemail accounts for their phone numbers are at risk if they don't change their voicemail PIN default, which in most cases starts off at 0000 or 1234.  And apparently, based on the provider, it could be obtained from the phone number.  You can even - you don't even have to guess what the default is.  You can find out what the default is.



So the opportunity for an account takeover occurs when an attacker tries to add a legitimate user's phone number to a new WhatsApp installation on the attacker's phone.  In that event, the WhatsApp service sends a one-time code via SMS to the phone number being joined, the victim phone number being joined to the attacker's account.  So this would typically alert a user that something fishy was up if they get, like, a confirmation of account connection from someone they don't know, and they're not expecting it; right?



But an attacker could avoid their target being aware of this by timing their attack at night or when for whatever reason they were sure the user was away from their phone.  After several failed attempts to validate the one-time code sent via SMS, that is to say the attacker simply retries the join request several times, and the WhatsApp service notices this, finally it switches to prompting the user to perform a voice verification requiring no prompting at the user's end.



So the WhatsApp service calls the user's phone number and speaks the one-time verification code verbally.  If the attacker has timed all of this properly so the user doesn't answer their phone, voicemail picks up, and the one-time - and everyone can already see where this is going now.  The one-time verbal authentication code goes to voicemail.  The attacker, knowing of course the victim's phone number, or knowing the voicemail pick-up, because remember that also most mobile telco providers allow remote access to a customer's voicemail account.  So they use that service to pick up voicemail from the non-mobile phone; enter the default voicemail PIN, assuming that the user hasn't strengthened it or changed it from 0000 or 1234; recover the spoken one-time code; enter it inside their version of the WhatsApp app; and link the real user's phone number to the hacker's device, effectively hijacking the account from the legitimate owner.



Once the attackers gain access to the WhatsApp account, they can then enable two-step verification to the user's account to prevent the legitimate owner from retaking control of their WhatsApp account because they won't know what the six-digit one-time password is which only the attacker knows.  And while this may seem like an awful lot of rigmarole to go through, the reporting of this upsurge has noted that it requires minimal technical skills, no specialized equipment, and according to Israeli authorities it's been widely used in recent weeks, leading to many reports of hijacked accounts.



In their alert, the Israeli authorities recommend that users use either a strong password for their mobile voicemail account - yeah, I mean, by all means.  And you can imagine someone might think, eh, why bother changing it?  It just doesn't seem like a security issue.  But when you couple that with failed SMS fallback to voicemail one-time password, suddenly now your voicemail account can contain sensitive information.



LEO:  I think most of us just have like a four-digit PIN on our voicemail.



STEVE:  Yeah.



LEO:  That's pretty common.



STEVE:  Yeah, exactly.  And the Israeli authorities also said take the initiative of adding one-time password protection to your WhatsApp account so that you've got, you know, like using a third-party app rather than SMS in order to authenticate.  And then the bad guys will have no way of getting it.  So again, sort of obvious in retrospect.  But wow, an interesting sort of hybrid combo of things.



So we ask the question rhetorically:  Did Google hide a "major" Google+ security flaw that exposed users' personal information?  The headlines about this were breathless.



LEO:  Yeah.



STEVE:  "Google hid major Google+ security flaw that exposed users' personal information."  "Google+ shutting down after bug leaks info of 500k accounts."  "Google to shut down Google+ after failing to disclose user data breach."  "Google is shutting down Google+ after a secret potential data leak."  "Google+ to shut down after cover-up of data-exposing bug."



Okay.  So what happened?  Given what Google's own security research auditors found, which is who found this back in March, and given all of the deliberately privacy constrained information Google had at their disposal, which is not much because they were deleting it on purpose to protect users, I think they've acted responsibly and appropriately.



The controversy here is that the Wall Street Journal caught wind of this discovery of theirs back in March, which Google fixed and didn't say anything about at the time because Google, despite looking for any evidence that anybody else knew of this bug, was unable to find it.  Since their logs were incomplete, they couldn't be absolutely sure it had never been used, but they had no reason to believe it had been.



So yesterday the VP of Engineering posted under Project Strobe.  The posting is:  "Protecting your data, improving our third-party APIs, and sunsetting consumer Google+."  And of course we've talked about Google+.  I know you guys on...



LEO:  I used to love it, yeah.



STEVE:  Yeah.  But kind of just was one of those things that never got off the ground.



LEO:  No.  It was their, you know, they really put all the wood behind the arrows there.  They even gave people bonuses one year based on Google's social success because they were trying to stop Facebook.  But it didn't take off.  And then once it was neglected, as any social network does, it became spam and porno.



STEVE:  Yup, yup.  And very much like MySpace, which is just kind of like, eh.



LEO:  Yeah.



STEVE:  Okay.  So from their announcement, I pulled a few relevant bits.  So they wrote:  "At the beginning of this year, we started an effort called Project Strobe, a root-and-branch" - and I'm thinking, what?  Is that like soup to nuts?  I guess so.  But that's a root-and-branch...



LEO:  It's a corporate term of art.  That's what that is.



STEVE:  "...review of third-party developer access to Google account and Android device data and of our philosophy around apps' data access."  So that's all good.  "This project," they wrote, "looked at the operation of our privacy controls, platforms where users were not engaging with our APIs because of concerns around data privacy, areas where developers may have been granted overly broad access, and other areas in which our policies should be tightened."  So like a very what-we-want-from-Google-focused, self-imposed audit of their stuff.



So they continue:  "Our review showed that our Google+ APIs, and the associated controls for consumers, are challenging to develop and maintain."  I don't really - I didn't dig in enough to know what that means.  That sounds like corporate speak for we didn't do it right or it's hard to do it right.  I don't know.



LEO:  APIs are hard.



STEVE:  Yeah.  Tell me.  Anyway, "Underlining this, as part of our Project Strobe audit, we discovered a bug in one of the Google+ People APIs."  Okay.  So now here's what we know and they wrote:  "Users can grant access to their profile data, and the public profile information of their friends, to Google+ apps via the API.  The bug meant that apps also had access to profile fields that were shared with the user, but not marked 'public.'"  So these were meant to be kept private.  So they were user-provided fields that the user did not intend to share publicly.



LEO:  I'll give you an example what those are because as soon as this came out I went to my Google+ profile, which I set up ages ago.  And there's mostly public stuff - your links, you know, stuff.  And there were two things in there that were private.  One was my personal phone number, and one was my address.  I have no idea why I put my personal phone number and address in there because they weren't publicly viewable, but maybe there was something - I think there was...



STEVE:  May have been for account recovery at the time.



LEO:  Could be.  Or there were layers of sharing.  I think there were.  I can't remember.  So worst case, I mean, nobody's putting a credit card number in there.  Worst case...



STEVE:  Yes, and actually you can't.



LEO:  Right.



STEVE:  Google explains:  "This data is limited to static, optional Google+ profile fields," and they said, "including name, email address, occupation, gender, and age."  Then in their posting they gave a full link.  And it is worth noting that there is some additional stuff that's a little more worrisome.  The full list contains a lot more.  There is birthday, profile photo, relationship status, current and previous employers with dates, places previously lived, and skill sets.  But again, still...



LEO:  Yeah.  But why would you - right.  I don't know why you put those in if they're super secret.



STEVE:  Right.



LEO:  I'm not sure.  Maybe there was some incentive to do that.



STEVE:  So Google says it does not include any other data you may have posted or connected to Google+ or any other service, like Google+ posts, messages, Google account data, phone numbers - well, except in your case.



LEO:  Well, I put them in.  Yeah, I put them in.  I had one public number and one private number in there.



STEVE:  Right.  Or G Suite content.  Then they say:  "We discovered and immediately patched this bug in March of 2018.  We believe it occurred after launch" - and I should mention that's 2015, so it had been present for years - "as a result of the API's interaction with a subsequent Google+ code change."



They said:  "We made Google+ with privacy in mind and therefore keep this API's log data for only two weeks."  Which in this case turns out to have been a double-edged sword.  They say:  "That means we cannot confirm which users may have been impacted by this bug.  However, we ran a detailed analysis over the two weeks prior to patching the bug" - meaning they had those two weeks of previous log data - "and from that analysis, the profiles of up to 500,000 Google+ accounts were potentially affected."  And that's a key word.  "Our analysis showed that up to 438 applications may have used this [so-called people] API."



And then they conclude:  "We found no evidence that any developer was aware of this bug or abusing the API, and we found no evidence that any profile data was misused."  So I think they have behaved responsibly.  Unfortunately, look what happens when an uncritical reading of the details gets loose.  And so given that - and apparently the Wall Street Journal reported that they got a hold of internal memoranda which suggested that Google was concerned that Sundar would be yanked in front of a congressional committee a la Facebook and the abuse of Facebook's private information, and all kind of lumped into one mess, if back in March they had said we found a bug that we have no evidence was misused, but it could have been, and it could have affected up to half a million Google+ users' private data.  Again, an uncritical understanding of this even now led to those headlines.



So I think Google did the right thing.  It would be nice to have had all of the logs of that.  But then that could have been a problem, even a potentially bigger problem.  So I don't think - as I said, it's a double-edged sword.  I come down on the side of thinking that Google, given the situation, did the right thing.  But I also recognize it's a little controversial.



LEO:  I do wonder what their GDPR liability is because, you know, you have 72 hours to reveal a breach.  But this isn't a breach.  This is a bug that could have led to a breach, but as far as they can tell did not.  So I guess they're not liable.



STEVE:  And was GDPR at the beginning of the year, or does it happen...



LEO:  No, it wasn't in effect in March.



STEVE:  Right, right.



LEO:  So they had no - yeah, I think, yeah.  And I don't - it's not like a credit card or Social Security number breach.  It's, I mean, certainly stuff people might have wanted to keep private and reasonably could have expected to be private.



STEVE:  Well, yeah.  And if they marked it private...



LEO:  It should be private.



STEVE:  Yes.  Google is acknowledging that an API failure allowed this data that was not meant to be released to be released.



LEO:  Right.  Whenever I see an article about Google in the Wall Street Journal, I always go to it a little skeptically because the Journal hates Google because they compete with the Journal for advertising revenue, and has always in its editorial, which is odd, been willing to castigate Google, sometimes inappropriately.  So I don't know if this is one of those.  But we've observed this before with the Wall Street Journal.



STEVE:  Well, yes.  And I would argue, too, that the technical press, all of those headlines that I cited were from the technical press.  I mean, you know...



LEO:  Yeah, they should know better.



STEVE:  Exactly.  They should not just go for clickbait, but say...



LEO:  Drives traffic.  That's right.



STEVE:  You know, here's the story.  So in this case I don't think Google did the wrong thing.



LEO:  I'm glad to hear you say that.  Good, yeah.  It's sad they're closing Google+, but they also said in that post that you were referring to that 90% of visitors spent less than five seconds on the site.



STEVE:  Five seconds.  It's like, oh, crap, where did I go?  Hit back.  Hit the back button.



LEO:  Get out, get out, back out.  It's a shame.  Now, everybody who - like Mike Elgan and Trey Ratcliff.  Trey actually wrote a long piece about what Google should have done with Google+.  Those people, Trey Ratcliff was huge there.  It really helped his career.  And I think we're sad to see it go.  It had such potential.



STEVE:  Well, and remember all the work that Gina Trapani did on that other thing that they, you know, I mean, Google does this.  There's a bone yard.



LEO:  Wave.  She wrote a whole book about Google Wave.



STEVE:  Wave.  Wrote a book.  Yup.



LEO:  Aw.



STEVE:  And then got washed up on the beach.



LEO:  Washed ashore, yeah, yeah.



STEVE:  Okay.  So Google has announced some additional information for their next release of Chrome, which will be 70, seven zero.  We already know, because we've been talking about this a lot, all of that controversy in the display.  They're not going to display files in the same way in the URL.  They're going to simplify the URL by not displaying the www-dot at the beginning of those in 70.  M-dot gets to stay for now.  And we've also talked about - last summer we covered this - the problem with drive-by extension installation.  It had been the case that websites could install Chrome extensions, which was a nice concept, I mean, the idea that you could have a website that would have like a site-specific extension that it could offer a Chrome user on the spot in order to enhance their experience with that site.



On the other hand, we know that installing anything on the spot is really fraught.  And in fact standard wisdom now is never, never click something that a website you're visiting says you need for any reason whatsoever.  Even if they're telling the truth, it's just not worth the risk that they might not be.  So back on June 12th, Google announced their timeline for their gradual rollback of inline Chrome extension installation.  They said on the 12th of June, they said:  "Starting today, inline installation will be unavailable to all newly published extensions," meaning that, if you weren't already in, you're not getting in.



"Extensions published," they wrote, "on June 12th, 2018 or later, that attempt to call the chrome.webstore.install function, will automatically redirect the user to the Chrome Web Store in a new tab to complete the installation."  Which seems really, really very reasonable to me.  Then, starting September 12th, so like three or four weeks ago, "inline installation will be disabled for existing extensions, and users will be automatically redirected to the Chrome Web Store to complete the installation."



So, okay.  And developers have plenty of time, you know, what, June 12th to September 12th.  So, what, is that six months?  Or not quite, I guess, but five.  So plenty of time to fix their website so that they're prepared for that redirect.  And now, in early December 2018, the inline install API method itself will be removed from Chrome 71.  Not 70, so you can still do it.  But inline install API going away with the version after the one that we get in a couple weeks.



Okay.  And now, last Monday, in their posting titled "Trustworthy Chrome Extensions, by Default," what they announced was an additional set of steps which will be appearing in this next release, so in a couple weeks.  The thing that I'm - okay, there are several things that I'm excited about.  One is they are giving users control over an extension's access to hosts explicitly.  So in other words, it has been the case that if you put an extension in Chrome, it essentially became an equal participant with Chrome and could see into all of the sites you visit, just as Chrome can.  So, I mean, so it really needed to be trustworthy.



What's changing is users can now change this to select specific sites where the extension will be active, require you to click the extension's icon to activate it, or allow it to be globally active.  So we've never had those permissions.  So underneath the extension, there's "This can read and change site data," and then you choose from three radio buttons:  when you click the extension; on, and then it fills in whichever site you're on, like in this case in the screenshot material.io; or on all sites.  So you can restrict an extension to one particular place, or shut it down completely and manually activate it.  So that's nice.  We get that in two weeks.



And they said:  "Our aim is to improve user transparency and control over when extensions are able to access site data.  In subsequent milestones," they said, "we'll continue to optimize the user experience toward this goal while improving usability."  They're also going to change the extensions review process.  This posting was aimed at developers, so there's some developer-aimed language.  But they said:  "Going forward, extensions that request powerful permissions will be subject to additional compliance review.  We're also looking very closely at extensions that use" - and this makes me shudder - "remotely hosted code."  I mean, the idea that an extension could be allowed to just go get ad hoc code from somewhere is horrifying.



And they said:  "...with ongoing monitoring."  That is, they will be monitoring this.  Extension permissions should be as narrowly scoped as possible, and all code should be included directly in the extension package to minimize review time.  So to me, this feels like another thing any sane developer will read the tea leaves here and realize that in the future that's going away completely.  Your extension will not be allowed to go reach out and get whatever it wants from wherever it wants.  So start reengineering that now so that you're not inconvenienced later because this really feels like it's going to go away.



And then this is really good.  Under new code reliability requirements - and again, it's like hard to believe they've allowed this.  "Starting today, Chrome Web Store will no longer allow extensions with obfuscated code.  This includes code within the extension package as well as any external code or resource fetched from the web.  This policy applies immediately to all new extension submissions."  So turn off code obfuscation for anything new you submit.  They said:  "Existing extensions with obfuscated code can continue to submit updates over the next 90 days, but will be removed from the Chrome Web Store in early January if not compliant."



They said:  "Today, over 70% of malicious and policy violating extensions that we block from Chrome Web Store contain obfuscated code.  At the same time, because obfuscation is mainly used to conceal code functionality, it adds a great deal of complexity to our review process."  Yeah, no kidding.  I mean, they've got to deobfuscate it and then figure out what the heck it's doing.  They said:  "This is no longer acceptable, given the aforementioned review process changes."



And they also said:  "Additionally, since JavaScript code is always running locally on the user's machine, obfuscation is insufficient to protect proprietary code from a truly motivated reverse engineer.  Obfuscation techniques also come with hefty performance costs such as slower execution and increased file and memory footprints."  And they said:  "Ordinary minification, on the other hand, typically speeds up code execution as it reduces code size and is much more straightforward to review.  Thus, minification will still be allowed, including the following techniques:  removal of whitespace, newlines, code comments, and block delimiters; shortening of variable and function names; collapsing the number of JavaScript files."



They said:  "Extensions in the store with obfuscated code must be updated using the recommended minification techniques for Google developers and submitted before January 1st, 2019."  After that, they're being removed, to which I say bravo.  They should have never been allowed in the first place.  I mean, I guess I can understand someone believing that they're protecting some proprietary secrets by encrypting a JavaScript download.  But exactly like with DVDs, where the DVD player has to have the decryption key in order to decrypt it to show it to its owner, the web browser has to decrypt the JavaScript in order to run it.  So it never made any sense.  So goodbye to obfuscation, especially when 70% of malware is obfuscated.  And, I mean, I want Google to be able to understand and audit the extensions that I'm getting from the Chrome Web Store.  I mean, so, yeah.



Also, on the issue of account takeover, because as we know that was how there was that recent problem that we covered, they are going to require two-step verification for developers logging onto their Web Store accounts.  They said:  "In 2019, enrollment in 2-Step Verification will be required for Chrome Web Store developer accounts.  Popular extensions," they write, "can attract attackers who want to steal it by hijacking developer accounts, and 2-Step Verification adds an extra layer of security by requiring a second authentication step via phone or physical security key.  Google strongly recommends enrollment as soon as possible, and it will be required in 2019."  So, yay.  I'm glad that Google is continuing to move forward to strengthen things.



And Leo, oh, this was just too long in coming.  Okay.  So we ask rhetorically, what's the latest version of WiFi?  Well, those of us who've been paying attention can issue the magic incantation, 802.11ac.



LEO:  Or ax.



STEVE:  Or ax.  Or, well, yeah, actually, that's coming.



LEO:  Yup.



STEVE:  Or what about "n"?  What about "g"?  What about "a"?  What about "b"?  Or a combination of.  So the Wi-Fi Alliance, the group that I would say I love to hate them, but no, I just hate them, they manage the implementation of WiFi, much to all of our disservice, by not making any of their specification production process public, as I've often complained about.  We now have WPA3 coming out, but only people who are members, who are paying dues to be, and they're not cheap, have seen these documents.  What they publish is just the Table of Contents, thank you very much.  So we'll have to wait until it leaks, as it of course inevitably does because people have to implement it, like in Linux, which is open source.



Okay.  So big change coming.  The next WiFi standard, correctly, as you noted, Leo, 802.11, wait for it...



LEO:  Ax.



STEVE:  A-X, yes, ax, the ax version, will instead use a simpler naming scheme, simply being called WiFi 6.



LEO:  Yeah.



STEVE:  Yeah.  And so we have backported.  The reason it's 6 and not 1 is that 802.11ac is WiFi 5; "n" is 4; "g" is 3; "a" is 2; and "b" is one.  So gone.  So now, I mean, and this is good.  For the typical consumer, we'll find out like the next - I guess at some point we'll get an update from Google with Android and Apple with iOS that our new devices support WiFi 6, if they don't already.  Maybe they do.  And you'll go, when you're shopping for your next WiFi router, it'll just say, you know, "Supports WiFi 1 through 6," probably.  And it's like, oh, good.



So the Wi-Fi Alliance wrote:  "To help users identify devices that provide the latest WiFi experience, Wi-Fi Alliance has introduced simplified generational names that may appear in device names and product descriptions.  WiFi devices supporting the latest generation of connectivity are based on the 802.11ax standard and are known as WiFi 6 devices."



And then they've also produced a series of icons which they're hoping that our OSes will incorporate into the UI.  And I've got a picture of it here in the show notes.  And I hope people understand that that's a version number and not how many people are connected because...



LEO:  It's not clear at all.



STEVE:  It's not at all clear, exactly.  It's like, wait.  There are six people on this WiFi?  Maybe I should use the one that only has four people.  I don't know.  Yeah, maybe six is better or stronger, who knows.  Anyway.  So for what it's worth, we're entering WiFi 6, which is a - what was it WiFi 6 was supposed to do?  It's mo' betta.  Anyway, that's enough. 



LEO:  I can't remember, actually.



STEVE:  It's higher data rates, increased capacity, better performance even in dense environments such as stadiums or public venues, and increased power efficiency, making it a worthwhile upgrade for smart home and IoT devices.



LEO:  Oh.  Just as you said, it's mo' betta.



STEVE:  Yeah.  Yeah.  Mo' betta.  We want 6.  Get 6.  We also want 9th-generation Core processors from Intel.



LEO:  Yes, we do.



STEVE:  Which they unwrapped yesterday to much fanfare.  They of course did a whole bunch of them.  I was focused on the desktop, where, boy, are these things getting fast.



LEO:  Twenty-eight cores.  Holy cow.



STEVE:  Yeah.  Yeah.



LEO:  Every one of them with speculative execution built right in.



STEVE:  Built right in, yup.  Just speculate all you want.  And also in the case - oh, there is, for example, the i9-9900, the K versions, which are the unlocked speedy guys.  This 9900K can run up to, can burst to 5 GHz speed.  And let's harken back to the 4.77 MHz.  So what is that, that's...



LEO:  It's faster.



STEVE:  A thousand times.  It's fasta, mo' fasta.



LEO:  Mo' fasta.



STEVE:  Yes.  You can also cook your breakfast on it because these things...



LEO:  265W on the Xeon.  That's a lot for a processor.



STEVE:  Yeah.  Maybe, you know, if you had one of the heat sink with fins, you could just use it to dry your hair.  Oh, my goodness.



LEO:  They do make a bitcoin miner that doubles as a room heater.  I think this would be perfect for that.



STEVE:  Yes, they do, actually, yes.  Just blow some cold air over it, and it'll come out hot.  So the good news is they have built, as one would expect, some additional mitigations are built in.  There was a slide during yesterday's presentation read:  "The new desktop processors include protections for the security vulnerabilities commonly referred to as Spectre, Meltdown, and L1TF," which of course we've covered extensively all year.  "These protections include a combination of the hardware design changes we announced earlier this year, as well as software and microcode updates."



Okay.  So in this case there was time to change the hardware; whereas of course until now we've had to settle with just software and microcode.  So there are the five problems.  The Spectre V2 branch target injection, that's not a hardware-based fix.  It is microcode and software.  But of course in this case it's already got the microcode.  And all of the latest Windows and Linux and I assume Apple, I have not been really keeping my eye on them, but the OS knows about the new features which the microcode offered.  So whereas the challenge for existing systems has been you would need to somehow get your microcode updated, which could sometimes be impossible, these of course come with the microcode current.



The second was the Meltdown V3, which was the Rogue Data Cache Load.  That is fixed in hardware.  So the new chips have it.  It's not possible to have that in the old chips.  The third one was the V3a, which was Rogue System Register Read, fixed in microcode and now built into those chips.



The fourth one is the Variant V4 Speculative Store Bypass.  That, as was the case with the first one, it was Spectre V2, not subject to hardware fix.  That's microcode and software.  And we've got the microcode, and we've got the software.  So everybody will be in good shape with that.  And the last one was the L1TF, the L1 Terminal Fault, which was 100% hardware fixable and is fixed with this latest round of chips.  So it's 9th Generation.  They are i9 down through i3.  So I guess they're all 9000 series; right?  Because the i3 is a 9000T with four cores and four threads, so it's not hyperthreaded.  And it runs at 3.2GB.  And then it goes all the way up to i5s and i7s and then the i9 monster.



In this case, this one has eight cores and is hyperthreaded and nominally runs at 3.6GB and can be bursted up to 5.  Yikes.  Of course, it won't run Windows 7.  So too bad for Windows 7 people.  Thus Microsoft ultimately wins the battle of the Windows versions by not supporting the older OSes on the newer hardware.  Which of course they announced some time ago.



Okay.  So Adobe and Foxit need to understand that this is not a competition.  Last Tuesday of this month, Adobe released security updates for Windows and Mac versions of Acrobat and Reader, including 47 critical vulnerabilities and 39 that were merely important.  Forty-seven.  Of the 47 critical vulnerabilities, 46 of those allowed for remote code execution; one allowed for escalation of privileges.  Whereas the 39 important vulnerabilities were information disclosure.  So, and remember that in the case of a PDF document which a reader is reading, there's never been, well, it's probably safe to say there's never been an interpreter as troubled and incredibly complex as a PDF interpreter.  And we know how difficult it is to get an interpreter to work right.  But they haven't yet.



And so 47 critical vulnerabilities, 46 of them remote code.  And this means, if you've got your web browser configured to use Adobe Reader as its reader, which I don't think anyone should do anymore, you know, use the built-in PDF viewers.  But if you did, and somebody sent you a deliberately crafted PDF, they could run code of their choosing on your computer.  So it is a huge, huge attack surface for today's machines.



However, that was not the largest number of PDF problems.  Not to be outdone, Foxit needs to be updated, too.  So if you have left the Adobe train and switched over to Foxit - I'm using Nitro as mine.  When I switched to my - when I rebuilt this new system after my WinXP machine died, as we'll remember, a few months ago, I had Acrobat there, and it was a fully purchased, licensed one.  But there was no way to recover the license, and Adobe wanted, you know, I just didn't want to go with them.  So I ended up choosing Nitro PDF.



LEO:  That's good.  I think it's good.  



STEVE:  I looked at 12.  And many of them are nice, but they do not produce small PDFs.  They produce big, lazy, blobby PDFs.  And so, no, Nitro.  



LEO:  I should have because this morning when Google announced its new Pixel Slate, they announced:  "And it comes with Adobe Acrobat."  And I went [moaning].  Really?  But it is a problem on Chrome OS to do PDFs, so maybe they had to do it.  



STEVE:  Yeah, yeah.  Anyway, Foxit, their update fixes 116 vulnerabilities.



LEO:  Holy cow.



STEVE:  I know.



LEO:  Is there something inherent in PDF rendering?



STEVE:  It's just really, really difficult.



LEO:  It's an interpreter; right?



STEVE:  Yeah, it's an interpreter.  And it's probably, I mean, something like Foxit is probably based on old Ghostscript code, which they grabbed, and then they took in-house.  And Ghostscript was written just with good intentions, but once upon a time to sort of be a free and open source alternative for PDFs.  But programmers who didn't have a commercial interest just said, okay, let's accept this.  I mean, because a PDF is, as we know, it is a fully parameter-driven tokenized description of a page.  It's a page description language.  And it is really extra difficult to, like, check every possible special case.



I was just telling Lorrie the other day that I'm creating this third website web portal for SQRL.  And a lot of it is just plumbing.  It's like, if the user clicks the login button when they've left the username and password blank, because I also explain, I show how you can use it alongside of SQRL, I have to say, you have to fill in your username and password if you're going to log in.  Or if they leave either one of them blank, but not the other, then I have to say, I mean, the point is, a lot of this is just brain-numbing but necessary plumbing that you have to do.  And in my case here it's just UI.



But in the case of a PDF reader, which in this world now where anything you expose to any content externally can be malicious.  And it just, I mean, it's really - we're in a different world than we were decade ago, where it's like, oh, email macros, how cute.  It's like, oh, look.  It's a malicious email macro.  How sweet.



LEO:  Awww.



STEVE:  Yeah, isn't that adorable.  You know?  Now it's just like, oh, my god.  Even our AV software creates an additional attack surface.  So you put that in to protect yourself, but it's buggy because it's having to interpret what's coming in, too.  Anyway, update Foxit.  If you're a Foxit user, make sure you're running an update.  It's probably a lesser large target than Adobe.  But, for example, if a corporation were known to have standardized on Foxit, and somebody wanted to target that corporation, they could send one of their employees a PDF designed to remotely execute code.



We owe Cisco's Talos Group for these vulnerabilities:  18 of the 116 alone were discovered by Cisco's Talos Group.  And all of the 18 vulnerabilities, as well as many of the others fixed by this update, are labeled "critical" because they could lead to code execution if you just visit a website, and you're using the Foxit Viewer on your browser.  Or let alone if you receive a PDF in email, you make sure it's from someone you trust, you open the PDF, and wham.  So be careful.



And I saw a note in my mailbag from Mark Taylor in - I didn't practice pronouncing this ahead of time, Leo - Wausaukee?



LEO:  Wausaukee.



STEVE:  Wausaukee, okay.  I felt like it needed to have another syllable in there.



LEO:  It feels like that, yeah, yeah.



STEVE:  Yeah.  Lot of vowels.  Wausaukee, Wisconsin.  Anyway, his subject was "SpinRite and encrypted drives."  And I see this question a lot, so I just thought I would take a minute to respond to Mark and anybody else who's curious.  He says:  "Steve:  All the normal accolades.  Can SpinRite be used on encrypted drives without making the drive unusable because of moved sectors that were fixed?  Thanks, Mark."  And the answer is yes, SpinRite can.  This version of SpinRite is completely happy if the drive is encrypted.  It looks at it.  It sees that it has no idea what this drive is.  So it just considers it a blob of opaque storage, and it fixes the sectors.



So, I mean, SpinRite never really cares what data is there.  In years past, it got itself much more involved with the file allocation table and following chains and all that.  A much enhanced version of that will be making a reappearance in 7.0.  But all of the 6.0 versions, where we are with 6.0 now and the forthcoming .1, .2, .3, they will continue.  They will be much faster and much more capable, but of only doing the same thing, which is fixing the drive that you have without moving anything around.



So SpinRite never changes the content of any data.  If a sector is unreadable, then it can often, as we know, make it readable again or show the drive that it is dangerously close to becoming uncorrectable, in which case the drive will move the sector.  But what happens is the data is moved into the new sector, and the old sector is taken out of service.  So, and this is happening in the background even without SpinRite.  SpinRite augments that process.  So definitely worth doing, and it works just fine on encrypted drives.



Let's talk about this mess with our supply chain.



LEO:  Yeah, and I should mention that Microsoft has put out an update.  They think they understand now what was going on with  the Windows feature update, and they are pushing out a new version of it, or will be soon.  They're pushing it out to Windows Insiders.  They say:  "We've fully investigated all reports of data loss, identified and fixed all known issues in the update, and conducted internal validation."  And they're going to push it out to the Insiders before rolling it out more broadly.



"It appears" - I'm reading from The Verge - "that the bug that caused file deletion was related to Windows 10 users who had enabled a feature in Windows called Known Folder Redirection, to redirect folders like desktop documents, pictures, and screenshots from the default location."  I've done that.  In the past I've had my documents folder moved to D.  And you can do that.  Microsoft has a facility to do that.  Microsoft introduced code in its latest update to delete the empty and duplicate known folders.  So when you move the folder to D, it's moving all the contents.  But if there was stuff left behind, perhaps intentionally, it would delete that, thinking it was an empty folder.  But it appears it wasn't always empty.  I can understand why they would do that because normally when you use redirection, that's because the data folder, the documents folder is now on D.  It's not on C.



STEVE:  Sure.



LEO:  There shouldn't be anything left.  But I guess that's not always the case.  Microsoft has developed fixes to address a variety of problems related to these folder moves, and these fixes are now being tested with Windows Insiders.  They say, and this is about what I would expect, that they believe that the data loss happened at a rate of one 100th of 1%.  So that's what, one 10,000th?  One 1,000th?  100 times 100.  So that's one 10,000th.  One in 10,000.  But if you have a million people install that, one in 10,000 is quite a few people.  So that's what happens when you have a massive user base.



STEVE:  And Robert Ziko is not happy.



LEO:  Yeah, he's the one 100th of 1%.



STEVE:  Oh, and I did forget to mention that, with the October feature update, there's another change that I just wanted to point out that might catch out our listeners.  And that is that, for those who use the disk cleanup utility, and I'm a big fan of it because it just goes through and gets rid of a whole bunch of crap, they've added the downloads folder.  And so pay attention.  It's not checked by default.  But if your habit is to turn all the checkmarks on, as has mine been, and then say, yeah, get rid of all this junk, if you are a person who knows that you've got downloaded things that you deliberately have left in the downloads folder, make sure that you pay attention if you turn that on because this update to disk cleanup adds a line item that you can turn on, and it will wipe out your downloads folder.



LEO:  Yikes.



STEVE:  Yeah.



LEO:  We thought they were going to take that out, so I'm glad to see that it's not gone because, you're right, I use it all the time.



STEVE:  I still like it, too.  And they're replacing it with their smart something or other.



LEO:  Of course.



STEVE:  Fixer-upper, cleaner, triple-scoop whoop-de-do.  But I don't know.



So I will begin by sharing an update on this Bloomberg story, which was just posted a few hours ago.  A major U.S. telecommunications company, and I'll explain why we don't know who in a minute, discovered manipulated hardware from Super Micro Computer, Inc., in its network and removed it in August, fresh evidence of tampering in China of critical technology components bound for the U.S., according to a security expert working for the telecommunications company.



The security expert, Yossi Appleboum, provided documents, analysis, and other evidence of the discovery following the publication of an investigative report in Bloomberg Business Week that detailed how China's intelligence services had ordered subcontractors to plant malicious chips in Supermicro server motherboards over a two-year period ending in 2015.  Appleboum previously worked in the technology unit of the Israeli Army Intelligence Corps and is now co-Chief Executive officer of Sepio Systems in Gaithersburg, Maryland.  His firm specializes in hardware security and was hired to scan several large datacenters belonging to the telecommunications company.  Bloomberg is not identifying the company due to Appleboum's nondisclosure agreement with the client.



Unusual communications from a Supermicro server and a subsequent physical inspection revealed an implant built into the server's Ethernet connector, a component that's used to attach network cables to the computer, Appleboum said.  The executive said he has seen similar manipulations of different vendors' computer hardware made by contractors in China, not just products from Supermicro.  Quote:  "Supermicro is a victim.  So is everyone else," he said.  Appleboum said his concern is that there are countless points in the supply chain in China where manipulations can be introduced.  And deducing them can in many cases be impossible.  That's the problem with the Chinese supply chain, he said.



So that's a perfect preamble because, as it happens, I didn't title this "Supermicro Attack," I titled our podcast "The Supply Chain" because this is a bigger problem than just this one particular "in the news at the moment" issue.  And I don't know how it has a solution, Leo.  I mean, we covered - what was the chip?  There was another chip that had a backdoor in it.  Oh, it was an Ethernet chip.  Was it RealTek?  



LEO:  I remember that.



STEVE:  It was somebody's - yeah.  I mean, so...



LEO:  We've known this is a problem for a decade.  Brian Krebs said he remembered 10 years ago when he was writing for the Washington Post he found a Chinese printer or a hardware update to a printer that would copy everything you printed and send it to a server controlled by the Chinese military.



STEVE:  Yes.



LEO:  This stuff's been around forever.



STEVE:  So there is a fabulous link, the last link I've got on the last page of the show notes, to LightBlueTouchPaper.org, which was written on the 5th of October, so that's Friday, "Making Sense of the Supermicro Motherboard Attack."  I commend this to our listeners to read.  It's an engineering paper.  And so the guy obtained the baseboard firmware from Supermicro and disassembled it to take a look at it, to better understand what's going on.  It is the case that the world has switched from parallel interfaces to serial interfaces.  All of this PCIE stuff, you know, the x4, the x8, the x12.  Well, that x number is the number of paralleled serial interfaces.  So you could have x1, which is a single connection, which nonetheless runs at crazy high speed.  And in fact USB is a serialized protocol.



And it turns out that, because the interconnection density has become a problem, speed is not.  So it's easier to send bits out faster from a single pin than it is to have 32 or 64 pins.  That's just too expensive in this day and age.  And they're also extremely noisy.  Every time you have signals going up and down you are radiating energy.  And you're also having to supply power to pump the capacitance of that signal up and down.  So the more copper you have, the more electrons you need to pour in and suck out in order to raise the voltage and lower the voltage again.  So it's just a mess.



So serial is the future.  And as a consequence there is - it's called the SPI, the Serial Peripheral Interface, which is a well-defined single pin.  Actually there's a clock line and a data line.  So it's a two-wire interface.  And people who have studied this particular instance with the Supermicro motherboards and have seen the pictures that Bloomberg posted of what looks like little - they describe it as a grain of rice and looking, I think they called it like a "signal conditioner."  Well, I'm sure that's common speak for a capacitor.  And it showed six connections.



And people have said, how can that possibly do anything?  Well, it could be that the motherboard allowed for expansion by having an additional SPI flash ROM, and they can be chained in series.  So if there was a spot on the motherboard that was unpopulated, all somebody would have to do is to move the official chip over into the second place and put the SPI chip in in the first place.  And six pins, you need power and ground and clock and data - clock in, data in, clock out, data out.  That's exactly six pins.  So that could work in order to allow that chip to have its contents downloaded by the processor when it starts up.  That baseband processor is an ARM9.  It's an older version of an ARM9 processor running Linux.



So we just - we see a motherboard that doesn't have the Intel 265W processor and its heat cooling technology on it and everything else.  We sort of look at it, and it just looks like a bunch of sockets.  But there is a Linux OS on that motherboard from the manufacturer, which starts up and brings our system to life.  And we've talked about how the baseband has Ethernet communications capability, how it's able to be on the network by itself, how it can communicate even without an OS on top and loaded.  So it's fully awake and aware and has drivers for Ethernet in addition to all the other peripherals that it needs to set up at boot time.



So from a standpoint of skeptics who doubt the technical veracity, it is very clear from an engineering standpoint that something like this is completely possible.  I would also argue that, I mean, that this is huge.  It is probably understood and is probably enjoying some containment within our intelligence services because we're talking about an economic global partner in China with whom we do a lot of trade, who is apparently, if we believe the facts claimed, where their intelligence services have engineered this technology to infiltrate the hardware of a U.S. major motherboard manufacturer.



Supermicro isn't as well known as Gigabyte and ASUS and some others.  But it is a major player in the server farm business with at least 30 customers, among whom are Apple and Amazon.  And so an infiltration of this sort, I mean, has epic scope and consequence and really does, I mean, I have no reason to disbelieve it.  Apple and Amazon's denials probably, given the fact that Supermicro's stock dropped in half, down 47% in one day after the news...



LEO:  Somebody believes it.



STEVE:  Yes.  And then it dropped another chunk, I don't remember how much.  There wasn't much left for it to go.  But this most recent news of this morning's report of an Ethernet connector having an embedded SPI hardware implant knocked its stock down even further.  So, yikes.



LEO:  You know, I guess for this show and in general our audience, the takeaway, you can debate about whether this happened, whether Supermicro knew, why are Apple and Amazon and others so vigorously denying it, and yet Bloomberg stands by their story.  All of that is a lot of heat, but there's no question that these kinds of supply chain modifications happen.  I was talking to somebody who buys a lot of servers, like a lot of servers.  And he said it's routine.  It's unusual for it to happen at the factory, much more often happens in transit.  It's usually done as a targeted attack.  So you know that we've got these 15 servers going to Sony.  Let's modify those.



Hardware modifications are not unusual, like this Ethernet port modification.  They're very difficult to detect and often go undetected.  And because the supply chain, well, even if the supply chain weren't in China, we don't have a secure way of transiting this stuff.  So, and by the way, the other final point is the U.S. does this just as often as every other country.



STEVE:  As I said, we learned - the first time we used the word "implant" on this show...



LEO:  It was ours.



STEVE:  ...was Edward Snowden because the NSA was implanting and doing exactly this.  They were intercepting packages bound for targeted targets, and making a change.



LEO:  Right, right.  So there really isn't anything in this story besides the specific allegations about Supermicro and the fact that they were ending up in Elementals, which were ending up in the Department of Defense and the Mormon Church and TWiT, by the way.  Those specifics you could debate.  But it doesn't essentially change the point, which is this is happening, and has been happening, and is extraordinarily difficult to stop.  Right?



STEVE:  Yeah, yeah.  And the only, I mean, so I wanted to make it very clear to our listeners that technically there is no problem with this.  That little pea size, that grain of rice size thing could very likely be a serial peripheral interface flash ROM.  That's not in question here.  And the idea that the motherboard, they're typically 32K, the motherboard engineers, the designers, could have just left some pads available for expansion space so they wouldn't have to redesign the motherboard.  They would just populate the six little pads with an additional grain of rice if they needed more than 32K of storage in the future.  



LEO:  Right.



STEVE:  And things like infected Ethernet connectors, that's not science fiction, unfortunately.  We may wish it were, but it isn't.  And I think, Leo, the problem with this kind of hardware, I would say implantation at scale, is that it seems like it was a spray.  What we know from Bloomberg's reporting is that it wasn't Supermicro themselves, nor their Chinese plant, but probably the subcontractors they use for overflow capacity that took the opportunity.  And so somebody somewhere in China took it upon themselves, we don't know from how high up the orders came, if we believe Bloomberg - and again, we have no evidence at this point, and I don't think we're going to.  I mean, again, Apple cannot have it be known that their server farms were infected with malicious hardware from China.  They can't.  Nor can Amazon.  So unfortunately, for the good of their stockholders and their reputation, they have to say no.  I'll bet there are people who know otherwise.



LEO:  If this really happened, there will be more.  There will be motherboards out there that are modified in that way, and they will surface.  There must be thousands.  Alex Lindsay says he has...



STEVE:  I would imagine tens of thousands.



LEO:  Yeah.  Alex Lindsay has an Elemental with the old Supermicro motherboard from 2015.  I mean, it must be hard to detect.  We've got to send it to Chipworks or something for them to figure it out. But this will - I think we've not heard the end of it, I guess.



STEVE:  Yeah, well, and this morning we heard something, another credible source, who says he was asked by a major telecommunications firm to inspect their datacenter.  He found data traffic from the baseband processor that was unauthorized, traced it back, and found a fake Ethernet connector on a Supermicro motherboard that had been affected.



LEO:  How hard is that to do?  Wouldn't that be the first avenue is to see if there were unexpected traffic?  Or is that just very hard to find?



STEVE:  Yes, yes.  And in fact that's how this was found.  It was a ping that was phoning home.  And somebody - but again, Sony, in your example, it was perfect.  Where I was headed was the idea of spraying the world with this is dangerous because someone in the world is going to find it.  Whereas modifying 15 servers that are headed for Sony is much less dangerous, just because of the law of numbers.  And Sony isn't a mainstream, like, datacentering is not their business.  They're just in business to do something else.



And so computers are just arriving, and some IT guy is slapping the server blades into a chassis and firing it up and loading the OSes.  And so they would never be any the wiser.  Nor would they be as likely to catch a little bit, a little ping leaving in order to phone home.  Whereas it was traffic, pursuant to this story, that tipped off somebody that, wait a minute.  What?  And our listeners will remember all the trouble I had when I upgraded my server.  I was having all kinds of weird behavior on an Intel motherboard until I moved the connector away from the primary NIC1 to NIC2, and all the problems went away, because something about the baseband processor, which is only on the primary NIC, was causing these problems.



LEO:  It's a fascinating story for a variety of reasons.  It really does resonate with the political environment.  It is a "they said, they said" story.  It's fake news, according to some.  It's the god's truth according to others.  It's really an interesting story.  I'm sure we'll hear a lot more.



STEVE:  Anyway, for anyone who's interested, this BlueTouchPaper, LightBlueTouchPaper.org piece, "Making Sense of the Supermicro Motherboard Attack."  If you're interested, it's written by a techie and was sent to me by a good friend and has definitely got a nice take on this.



LEO:  Yes.



STEVE:  I mean, like from a, you know, is this reasonable?  And the answer is yeah.



LEO:  Yup.  Oh, I think this is a story that's only just begun.



STEVE:  Yeah.



LEO:  But this show has come to an end.  So there you go.  But we'll be back every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  It's Security Now! time, and a jolly cheer goes up throughout the geek community as Steve takes to the air and explains it all.  I mean, you read a story like that, and I know my only thought was, well, I can't wait till Tuesday.  I can't wait.



Steve Gibson is at GRC.com.  That's where you'll find all of his stuff - including his bread and butter, SpinRite, the world's best hard drive recovery and maintenance utility - plus a lot of free stuff, all the information you'd ever want to know about SQRL, Perfect Paper Passwords, the Healthy Sleep Formula, I mean, that goes on and on.  I was just the other day looking up the Vitamin D stuff that we did, those Vitamin D podcasts, because I was trying to figure out what dosage to get.  It's all at GRC.com.  And this show is, as well.  He has audio versions of Security Now!, and he's the only source for really good transcriptions written by Elaine Farris that make it easy for you to read along while you listen:  GRC.com.



We have audio and video at TWiT.tv/sn.  You can also subscribe if you'd like in your favorite podcast application.  That way you'll get it every episode.  You really want to have it promptly every week, and you want to keep a backlog, too.  The archives are just as valuable on this show.



Steve, have a great week.  Thank you for being here.  We'll see you next time on Security Now!.



STEVE:  Thank you, my friend.  Till then.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#685

DATE:		October 16, 2018

TITLE:		Good Samaritans?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-685.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we observe the untimely death of Microsoft's co-founder Paul Allen, revisit the controversial Bloomberg China supply chain hacking report, catch up on Microsoft's October patching fiasco, follow up on Facebook's privacy breach, look at the end of TLS v1.0 and 1.1, explore Google's addition of control flow integrity to Android 9, look at a GAO report about the state of U.S. DOD weapons cybersecurity, consider the EOL of PHP 5.x chain, take a quick look at an AV comparison test, entertain a few bits of feedback from our listeners, and then consider the implications of grey hat vigilante hacking of others' routers.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of security news.  DuckDuckGo is on the rise.  Details about the Microsoft patch that deleted data.  Yikes.  And which antivirus is the best?  Steve has some information.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 685, recorded Tuesday, October 16th, 2018:  Good Samaritans?



It's time for Security Now!, the show where we cover your privacy and security online.  We also talk about all the latest security news, some tech news, maybe even explain how some of this stuff works, thanks to this guy right here, the Explainer in Chief, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Mr. Laporte, it's good to be back with you again this week for our 685th episode.



LEO:  Wow.



STEVE:  Yeah.  So the thing that caught my interest, and I want to do this more as a discussion, just sort of of the pros and cons, because I think it will be interesting.  My promotion of the idea of auto-updating routers has generated some controversy.  But this particular news will probably generate additional controversy.  So controversy is good.  That's why it's the title of the show:  "Good Samaritans?" with a question mark.  It turns out somewhere, we don't know where - oh, wait, I think he's Russian speaking.



LEO:  Oh, well, there you go.



STEVE:  There's a Russian-speaking Good Samaritan, question mark, who claims to have, and there's been some confirmation, so far patched more than 100,000 of the vulnerable MikroTik routers without their owners' knowledge or permission, just saying, well, I'm going to fix this for you.



LEO:  Taking advantage, I presume, of the exploit in the first place, right, to get in there to fix it.



STEVE:  Yes, yes, exactly.  And then essentially what he's doing, he's not updating firmware, he's installing firewall rules to prevent additional exploitation.  And there's a lot of  sides to this because, well, and we'll discuss it without...



LEO:  Interesting.



STEVE:  ...stepping on our own lede.  We're also going to talk about, just briefly note, as you did at the end of MacBreak Weekly, the death, sad death - I don't know if death is ever good.  In this case it's sad.  Microsoft's cofounder Paul Allen passed away yesterday at the age of 65.  Also we're going to revisit some of the things that we've been talking about recently, just for some follow-ups.  The Bloomberg China supply chain hacking report, which is continuing to be controversial.  There is some additional news on Microsoft's October patching fiasco.  It's hard to characterize it as anything but.



Facebook revised downwards the number of people who were breached and had their private stuff stolen from them.  And oh, my god, Lawrence Abrams over at Bleeping Computer said something that was so funny.  Every time I think about their revision it makes me chuckle, so we'll share that.  We're going to look at the end of TLS versions 1.0 and 1.1; explore Google's addition of something known as "control flow integrity," which was added for the first time to Android 9; take a look at a GAO report about the very sad state of U.S. Department of Defense weapons cybersecurity that actually had some bit of humor in it.  Well, yeah, it did. I was trying to think whether that came from reporting or from the report itself; but, no, it was in the PDF.



We're going to look at the implications of the forthcoming end of life of PHP 5.x.  That's at the end of this year, and that could be a problem.  We'll take a quick look at an AV comparison test, recognizing as we always do that they're always a little suspect because you've got to wonder, well, who contracted for that?  But it was interesting.  We're going to, if we have time, entertain a few bits of feedback from our listeners, including one from John McAfee, who I don't think is a listener because he would be smarter if he was a listener.



And then we're going to consider, as I mentioned, the implications of grey hat vigilante hacking of other people's routers, ostensibly or by intention to fix them.  And then of course we have our typical fun Picture of the Week.  So I think another great podcast to entertain and engage our smart listeners who've been following the podcast for years.



LEO:  Jammed with great stuff.  Steve?



STEVE:  So our Picture of the Week raises a few questions.  First of all, foremost among those questions is, is this progress?  Does this represent progress?  We're looking at a Coca-Cola machine where the screen is explaining that "This Dispenser is temporarily Out of Operation.  Please wait while the dispenser performs required nightly maintenance."  And then in all caps, which makes me wonder who this message is aimed at:  "DO NOT SHUT DOWN THE DISPENSER."



LEO:  Don't unplug it, whatever you do.



STEVE:  And then there's a sort of a clock-y kind of progress thing.  The guy who sent this to me, I responded via Twitter, and I said, "Hey, thanks.  It'll be a great photo for tomorrow's podcast," because this was yesterday.  And he wrote back, and he said, "Just FYI, the progress meter was not moving."  Now, the other worry is that it says "Downloading Updates."  So it's like, okay.  So your Coca-Cola dispenser has to have an Internet connection.  Now, the screen is also glossy, so we can see the reflection, we sort of see the shadowy outline of the guy who took the picture.  But behind him is Windows and daylight, which really makes this look like it's not night.  And there's something that says "01:26:37."  And I'm thinking, well, I hope that's not the remaining time. 



LEO:  I think it is.



STEVE:  Counting down.



LEO:  In an hour and a half you'll get your Coke.



STEVE:  You can have your drink.  And really, "Do not shut down the dispenser."  So is this for the repairman?  Or, I mean, really, a mischief-y scoundrel would be tempted to do just that because, you know, there's a plug back there somewhere.  So I don't know.  This whole thing, I'm skeptical about this kind of progress.  I mean, for example, why is it that it can't be happily doling out Coca-Cola while downloading those updates in the background?  Does it have to stop?  It can't run its Coke dispenser while it's - there's no multitasking here or what?  It just doesn't make any sense to me.  So it's like, okay, fine.  Were this my Coca-Cola machine, it would be downloading in the background.  And then it would say, hold on one second while I reboot.



LEO:  I'm guessing it's one of these newfangled Coke Freestyle machines where they take all the different syrups - see, it's got a big display on the front there.  I bet you that's what this is.



STEVE:  Yeah, actually he did say that you could mix your Cola to your taste.



LEO:  Yeah.  It's a freestyle.  So I guess you need an operating system to mix your Cola to taste?  I don't know.  It's not your average fountain unit.



STEVE:  Probably better than valves, Leo, because I think those would be...



LEO:  Your valves are being updated.



STEVE:  Yes.



LEO:  Yeah.  I was thinking of getting one of those for the office, but maybe not now.



STEVE:  Well, I would love to have some feedback about what it's doing in the middle of the afternoon updating for an hour and a half when you've got thirsty coworkers sitting around here saying, "Hey, boy, Leo, this was a real improvement."



LEO:  I think it's downloading the new Fanta Halloween flavors.  They're scary good.  That must be it.



STEVE:  Oh, my goodness.  Okay, no, I'm not going to get into this any further.



LEO:  You don't want this stuff.  Trust me.  You don't want this stuff.



STEVE:  I'm happy with my single-flavor espresso, thank you very much.



LEO:  This machine does have an app, so I'm thinking, app-enabled, it probably needs updates from time to time.



STEVE:  Yeah.  Again, it should just do it by itself.



LEO:  In the middle of the night.



STEVE:  Well, or it should be able to give you foam and fizz and things while it's downloading to itself and then say, oh, hold on one second.  It kind of makes a burp sound and then reboots.  But no.



LEO:  You can actually use the app to make your own mixes.



STEVE:  I'm sure you can.



LEO:  And there's some way that you can go to the machine and share it with the machine.



STEVE:  What could possibly go wrong?  So okay.  Lots of controversy raised by Bloomberg's report - I notice I spelled their name wrong in my title, but I know that's an E-R-G - about the Chinese supply chain problem that was the topic of last week's podcast.  And I was just curious to see what had happened since and what was the upshot and so forth.  Business Insider reported that "All parties involved have denied the report, including most recently the Secretary of the Department of Homeland Security during a Senate hearing [last Wednesday]."



She said, that's Kirstjen Nielsen said:  "With respect to the article, we at DHS do not have any evidence that supports the article.  We have no reason to doubt what the companies have said."  And of course that's everybody involved denying it.  But shortly after she said that, after saying there's no evidence, she said:  "We can tell you, though, it's a very real and emerging threat that we're worried about."  So we're going to be worried about it, but as far as we know - and I should also mention that Bloomberg, as one would expect, I guess they could retract if they believed that their story had fallen through.  They're sticking by it.



LEO:  Oh, no.  They're not going to retract it.  They worked on this for two years.



STEVE:  Yeah.



LEO:  I talked to Mark Milian, who works at Bloomberg Business Week, on Sunday.  He was on TWiT.  Their editor-in-chief, who's highly respected, was at The Economist for a long time, was very much involved in vetting the story.  I mean, this was a cover story for the Bloomberg Business Week magazine.  They're not going with this unless they have absolutely every confidence it's true.



STEVE:  Yes, yes.  And what I got a kick out of was that during that same hearing Christopher Wray, who's our current FBI director, said something I thought was curious.  He said he couldn't confirm nor deny the existence of any investigation into compromised Supermicro equipment.  And I'm thinking, why couldn't he simply deny it?  You know?  I can neither confirm nor deny.  What is that?



Anyway, so TechCrunch's Zack Whittaker, who as we know has been reporting on security for years, considers this confusion of press releases and statements to be the nature of this kind of reporting.  When the stakes are this high, I mean, and they are, reporting on security vulnerabilities and classified information means that you're more often than not dealing with making your sources anonymous to protect them, which rightfully opens, I mean, and of necessity opens your work up to denials and condemnations from the organizations you cover.



And we have to remember also that here in the U.S., companies who are legally forced to divulge information to law enforcement are also bound to deny that any such disclosure ever took place, you know, the famous FISA warrant mess.  So whatever happened occurred several months ago, as we know, dating at least from back August.  You indicated that they've been working on this for quite a while.



LEO:  This goes back to the Obama administration.  This has been a long investigation.



STEVE:  Yeah, yeah.  So while, yes, we don't have any absolute proof, with something that is this high stakes, we're not going to.  I mean, there isn't going to be a smoking gun.  Everybody had to put their guns away.  They've hidden them all.  And they said, what gun?  No, there's no gun here.  So anyway, I just wanted to follow up that, yes, it generated of necessity some reaction from the world.  But anyway, I'm glad you had him on, and you guys discussed it further.



LEO:  Yeah.  And I trust Mark.  He knows these guys.  He shared a desk with Jordan.  He says no, no, there's no question that these are good sources and that this is a good story.



STEVE:  And I also saw, sort of apropos of this, Kevin Mitnick just demonstrated a malicious USB cable which was entering keystrokes into a fully patched Windows 10 machine to install malware.



LEO:  The NSA had such a thing, according to Snowden.



STEVE:  Yes, yes.  And when President Trump and his entourage went to meet with the leader of North Korea, remember they were handing out little USB fans, and everybody was quite worried about, wait a minute, how hot am I?  Do I really need to...



LEO:  How hot is this fan?



STEVE:  Yes.  Do I really need to cool myself off that much?  Okay.  So also following up on our coverage of Facebook from last week, as we know, they originally acknowledged a sophisticated attack which leveraged a vulnerability in a video uploading tool and their "View As" feature, which allowed essentially a pivot attack where an attacker could obtain the authentication for any other targeted user.  So they have posted an update on the 12th of October titled "An Update on the Security Issue."  I've got a link in the show notes for anyone who wants it in every detail.



But what cracked me up was that Lawrence Abrams covered this for his Bleeping Computer site.  And so quoting from Facebook's update, he first said:  "We now know that fewer people were impacted" - this is Facebook saying, "We now know that fewer people were impacted than we originally thought.  Of the 50 million people whose access tokens we believed were affected, about 30 million actually had their tokens stolen."  To which Lawrence wrote:  "Isn't that great?  Only 30 million."



LEO:  Only 30 million actually stolen.  That's what's amazing.



STEVE:  Yes.



LEO:  That's mindboggling.



STEVE:  Yes.  So Facebook wrote, so that they did provide additional information:  "First, the attackers already controlled a set of accounts."  Which, you know, who doesn't have a Facebook page?  I mean, even I have one, just to go check on security features.  "First, the attackers already controlled a set of accounts, which were connected to Facebook friends.  They used an automated technique to move from account to account so they could steal the access tokens of those friends" - that's what I call "pivoting" from account to account - "and for friends of those friends, and so on" - so like a big network tree that branches out - "totaling about 400,000 people.



"In the process, however" - this is Facebook writing.  "In the process, however, this technique automatically loaded those accounts' Facebook profiles, mirroring what those 400,000 people would have seen when looking at their own profiles.  That includes posts on their timelines, their lists of friends, groups they are members of, and the names of recent Messenger conversations.  Message content was not available to the attackers, with one exception.  If a person in this group was a page admin whose page had received a message from someone on Facebook, the content of that message was available to the attackers."



Second paragraph:  "The attackers used a portion of these 400,000 people's lists of friends to steal access tokens for about 30 million people."  That is another stage of pivot out from all of the 400,000.  So this thing just sort of escalates as it branches out.  For 15 million of those 30 million, attackers accessed two sets of information:  name and contact details, so phone number, email, or both, depending on what people had on their profiles.



For the other 14 million, the attackers accessed the same two sets of information, as well as other details people had on their profiles.  This included username, gender, locale, language, relationship status, religion, hometown, self-reported current city, birth date, device types used to access Facebook, education, work, the last 10 places they checked into or were tagged in, website, people or pages they follow, and the 15 most recent searches.  So a lot of information on 14 million.  And then they finally said that for that remaining one million people the attackers did not access any information.



So I will reiterate something that all of this information has made clear and I mentioned last week, which is - but I haven't seen it noted anywhere else.  And that is that this was also a fully targetable attack.  I worry that the size of the number sort of leaves people to believe that this attack was just sort of like a "you get what you get" sort of attack.  But first of all, we know that both of the top two Facebook executives were compromised.  Maybe they just fell into this 30 million, but also probably - because the idea is, since you could say "view as somebody else would see me," and then in the process you acquire that person's authentication token, you can ask for anybody you want to see what your page looks like and then pivot to their identity.



So to my way of thinking, while yes, 30 million is a big number, albeit down from 50, the fact that these attackers could get this information, although again not total account takeover, just viewable information, but also logged on authentication for anybody they wanted, that makes it in my mind more serious.  And also missing from this Facebook update is they neglected to mention that the then owner of someone's authenticated Facebook identity could then use the widespread Sign-in with Facebook, OAuth, to log into many other Internet websites and services under the identity of that impersonated account, which again is very powerful and potent.  But Facebook decided not to update us on that issue.  So anyway, I just wanted to follow up when I read what Lawrence wrote:  "Isn't that great?  Only 30 million."



LEO:  Oh, what a relief.



STEVE:  Yes, we could all, well, as we know.  Oh, and they concluded their posting, Facebook did, by saying people can check whether they were affected by visiting our Help Center.  In the coming days we'll send customized messages to the 30 million people affected to explain what information the attackers might have accessed, as well as steps they can take to help protect themselves, including from suspicious emails, text messages, or calls.  And it's not a simple URL:  Facebook.com/help/securitynotice, all one word, ?ref=sec.



LEO:  They wouldn't want you to find it too easily.



STEVE:  But that will take you to the page.



LEO:  You could probably go to their security page and navigate from there, I guess.



STEVE:  Yeah, and dig in, yeah.  Okay.  So also an update on the Windows 10 October update mess.  And actually there was so much that has happened in a week, it has been such a debacle that I found it a bit challenging even to organize it into a coherent view for this podcast.  So last Tuesday afternoon, while we were recording last week's podcast, Microsoft posted to the Windows blog "Updated version of Windows 10 October 2018 Update released to Windows Insiders."  Okay?  Not the general public yet.



So they said:  "Last week we paused the rollout of the Windows 10 October 2018 update version 1809" - so that's the feature update - "for all users as we investigated isolated reports of users missing files after updating.  Given the serious nature of any data loss" - okay, now, get a load of this compared to how they end with and something we learned subsequently.  "Given the serious nature of any data loss, we took the added precaution of pulling all 1809 media across all channels, including Windows Server 2019 and IoT equivalents.  We intentionally start each feature update rollout slowly [huh?], closely monitoring feedback [huh?], before offering the update more broadly."  And of course in this case they're really glad.



"In this case," they said, "the update was only available to those who manually clicked on 'check for updates' in Windows settings.  At just two days into the rollout, when we paused, the number of customers taking the October 2018 update was limited.  While the reports of actual data loss are few" - and here they quote the number you mentioned last week, Leo - "one one-hundredth of 1% of version 1809 installs, any data loss is serious."



Okay, so yes.  And what Microsoft failed to note here was the sobering news that there had previously been many reports from Microsoft's own Win10 insiders of exactly this mass data deletion occurring to them which Microsoft had ignored.  So that slipped through the cracks.  Now, to Microsoft's credit - as we know, anybody can make a mistake - they are working to fix this.



They said:  "Prior to re-releasing the October 2018 update, our engineering investigation determined that a very small number of users lost files during the October 2018 update.  This occurred if Known Folder Redirection had been previously enabled."  And Leo, you brought this up while we were covering this last week because that news had just occurred.  "But files remain in the original 'old' folder location versus being moved to the new redirected location.  KFR is the process of redirecting the known folders of Windows including Desktop, Documents, Pictures, Screenshots, Videos, Camera Roll, et cetera, from the default folder location" - which is c:\users\username, then folder name - "to a new folder location.  In the previous feedback from the Windows 10 April 2018 update" - okay, so that's April 2018 update, which was the previous feature release.



"In the previous feedback from the Windows 10 April 2018 update, users with KFR" - that is, the Known Folder Redirection - "reported an extra empty copy of known folders on their device.  Based on feedback from users, we introduced code in the October 2018 Update to remove these empty, duplicate known folders.  That change, combined with another change to the update construction sequence, resulted in the deletion of the original old folder locations and their content, leaving only the new active folder intact."



So now we know in more detail exactly what happened.  It was that in some cases the contents of the original known folders was not moved to the new relocated known folders, leaving that behind.  So at that point, for the last six months, since the April update, users essentially had sort of abandoned orphaned documents in their original folder locations, and then the post-relocation new locations.



And then unfortunately, as a consequence of, you know, Microsoft isn't telling us exactly in what detail, but they didn't apparently look to - they didn't move the stranded orphaned documents then.  They just killed the folders.  Which caused a loss of data for the users who still had and were using the non-relocated known folder contents, which then disappeared.  And they go on to explain how they found and fixed three scenarios where this was seen to occur.  I've got it in the show notes for anyone who's interested.  I won't drag us through it right now.  Then they indirectly addressed the fact that insiders had been screaming about this problem well before the update's release and being ignored.



Microsoft concluded their posting saying:  "To help us better detect issues like this, today we have enabled a new feature in the Windows Insider Feedback Hub.  We've added an ability for users to also provide an indication of impact and severity when filing User Initiated Feedback.  We expect this will allow us to better monitor the most impactful issues, even when feedback volume is low."  And again, feeling a little chastised, I'm sure, they said:  "We will continue to closely monitor the update and all related feedback and diagnostic data from our Windows Insider community with the utmost vigilance.  Once we have confirmation that there is no further impact, we will move towards an official re-release of the Windows 10 October 2018 update.  We apologize for any impact these issues may have had," blah blah blah.



So there were reports of it on the feedback hub as a consequence of, well, aided by the fact that there wasn't the ability before now for people posting feedback to really raise an alarm algorithmically.  I mean, apparently lots of people were saying, "Oh, my god, files have just been deleted."  But that slipped through the cracks.  So now Microsoft has added the ability to specify the severity of the problem.  So that's good.  And hopefully that will help to prevent this moving forward.



Also since this October round of patches, in both the 1803, which is the current pre-feature update, and the 1809, which is the October update, both of those updates, a bad HP keyboard driver had somehow made its way into both 1803 and 1809 updates, which was causing a blue screen of death on boot for HP machines and was triggering a WDF, Windows Device Framework or Driver Framework violation.  And this is of course problematical for users who are not technical because, if you installed the updates and then rebooted, at that point you were never able to get Windows up again.  It would blue screen while it was trying to boot.



So it took intervention in order to back out of that driver.  Microsoft explained that, if you hadn't rebooted, you could get rid of the driver, which was v11.0.3.1, and then you'd be okay for rebooting.  But if you had rebooted, you need to go to additional measures.  And there's Knowledge Base articles and so forth.  I've got the link in the show notes for anyone who's interested.  So Microsoft acknowledged that this was a problem that crept into this month's updates for Windows 10.



Also there is a "what needs your attention" screen which could come up and which might have been a blessing, actually, complaining that an Intel audio display notification problem existed, which was some sort of compatibility issue with a range of Intel display audio device drivers that Microsoft said might result in excessive processor demand and reduced battery life.  As a result, the update process to Windows 10 October 2018, that is, the feature update to 1809, will fail.



If you see a "what needs your attention" notification when you run the October update, you have an Intel display audio device driver, and then they note which one it is, installed on your system that is preventing that update from occurring.  Which, as I said, may have been good because, also and separately, people who did get the install, actually either the regular update, just regular monthly 1803 update, or the feature update, some people were reporting, independent of what their hardware was, that their sounds died.  They were getting an Intel SST Audio Controller problem which was killing audio for those users.  In that case, since this doesn't prevent your system from booting, you can go into Device Manager and delete this - it's called the Intel Smart Sound Technology, driver version 9.21.00.3755 - and that gets your sound back.



Also there was a display brightness resetting problem which is affecting some users such that every time they booted Windows 10 after the October updates, their brightness was set to minimum.  No word on the resolution for that yet.  And with the release of Windows 10 1809, which is the feature update, the Microsoft Edge web browser and Microsoft's UWP Store apps might no longer be able to connect to the Internet, which is a pesky problem for a web browser.  It turns out that Edge and the UWP apps now require, as of 1809, TCP/IPv6 to be enabled, or they will not connect.  And so I'm sure they're enabled by default.  So presumably, if someone had previously manually disabled IPv6 for some reason, those apps would mysteriously no longer function.  So the solution is reenable IPv6.



So anyway, many problems, as we know, for this rather painful round of updates.  And of course it's no wonder, seeing this, that enterprise IT would be reluctant to jump onto updates immediately.  You can imagine if enterprise-wide they had standardized on HP systems that all were blue screening after they restarted, and that caught them by surprise.  So I'm liking the idea more and more of holding off for maybe a week at least on installing new features.  And given the fact that 1803, which is the existing latest build, not the feature update, also suffered as a result of many of these problems in October, maybe just holding off on updates altogether.  Or if nothing else, make an image of your system so that you're able to recover from something that might happen so that you're not in trouble until Microsoft gets around to fixing it.



Also, believe it or not, there's one more.  Remember that on May 8th of this year Trend Micro, through their Zero Day Initiative, we discussed it at the time, notified Microsoft of a potentially serious remote code bug in their age-old and present everywhere JET database engine.  It wasn't clear whether Microsoft didn't think this was that big a problem because it's not remotely exploitable.  On the other hand, it is pervasive on Windows.  Every version of Windows for decades has had the JET database engine in it.  It was one of Microsoft's earliest database implementations, and it's still around for pervasive backward compatibility.



And remember that, as we discussed at the time, if you downloaded a piece of email that had a JET database engine file, essentially this is an interpreter buffer overflow-style exploit so that, if a user were to open that file, that could execute code of the attacker's choice on their system.  And recall that prior to the September Patch Tuesday, Microsoft got back to Trend Micro a few days before and said, oh, we were unable to duplicate that, even though they had originally confirmed four months, 120 days earlier, that they acknowledged the receipt and said that they had verified that they had successfully reproduced the issue.



Well, Trend said sorry about that, but we gave you four months, so we're going to publish.  So they missed the September patch update; right?  So then on the 20th of September Trend went public with the news.  And at the time we discussed it on the podcast, of course, and said that, okay, well, Microsoft barely missed it for September.  Certainly they were going to get it for October.



Now we're at September 20th.  There is that micropatch available from the Acros Security guys, which I think was a 22-byte patch that was an in-RAM patch, which the Acros technology, they have those things they call "micropatches" to fix little things like this quickly.  And I said, you know, it's a third-party patch.  Even though the patch itself is 22 bytes, that's a patch descriptor.  You have to have a patching engine downloaded first.  You have to sign up for an account with them.  And it's like, let's not bother.



Okay, well, the other thing that happened for October is that Microsoft did implement a fix for this problem in the JET database engine, and they did it wrong.  So the problem is still there.  So now we have sort of a different concern, and that is probably for a month we're not going to have this thing properly fixed.  And we know from a binary diff of the DLL that was fixed at the beginning of the month exactly what it is that Microsoft did.  So this is a classic case of reverse engineerability.  The guys at Acros Security who implemented the patch immediately for the original problem, their patch was broken by Microsoft's attempted fix of the JET database engine.  They have a new patch - which is smaller, now it's only 18 bytes - which repatches Microsoft's breakage of their earlier patch.



But the issue is, for people who don't do this - and I still can't recommend a third-party patch, especially for something for which is there is no known public exploit happening.  On the other hand, if it is publicly exploited, we may not know because it would be a targeted attack.  It would be somebody knowing - and understand where we are at this point.  At this point every single version of Windows in use has this remote code execution vulnerability in it today, fully patched.  That is to say, every fully patched version of Windows has this problem such that a malformed file, which happens to be a JET database database, if executed, will run code on the victim's machine.  So that file could be delivered through a web page download, through email, through whatever means; and, if executed, it runs code on that target machine.



And every fully patched Windows and every any patched Windows system now in use is vulnerable as a consequence of the fact that, first of all, Microsoft didn't fix it in four months.  When they attempted to fix it in the fifth month, they did it wrong and left the vulnerability still there and exploitable.  And we now have a month window, whereas before Trend didn't go public until the 20th of September, so it was only going to be a few weeks.  Well, now we have four weeks.  And if any bad guys started working on an exploit back when Trend went public, then they're rubbing their hands together because they just got a four-week extension on the availability of this thing being exploitable.



So we'll keep our eyes out for any sign of exploit.  Microsoft says there's no known at the time.  Microsoft, by the way, fully acknowledges the problem now, thinking that they had already fixed it.  I've got a link in the show notes to their security guidance for the advisory, and this is CVE-2018-8423, where Microsoft says:  "A remote code execution vulnerability exists in the Microsoft JET Database Engine."  Yes, and still exists.  



"An attacker," they write, "who successfully exploited this vulnerability could take control of an affected system.  An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights.  Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with administrative user rights.  To exploit the vulnerability," writes Microsoft, "a user must open/import a specially crafted Microsoft JET database engine file.  In an email attack scenario, an attacker could exploit the vulnerability by sending a specially crafted file to the user, and then convince the user to open the file."



So Microsoft is writing that, believing that they had fixed it, which they didn't.  And it is now every version of Windows in use has this presently available for exploit.  So I will be frankly surprised if we're not discussing in a week or two that people are being attacked with this because it didn't get fixed.



So anyway, if anyone is concerned, 0patch.com.  That is zero, the digit zero, P-A-T-C-H dotcom, is where Acros Security has their so-called "micropatches," which is in this case an out-of-bounds write, and the little patch script is 18 bytes long.  And essentially, every time you boot the system, this reaches in and tweaks a few bytes in Windows to actually fix the problem, rather than Microsoft, that hasn't managed to fix it in five months.  So anyway, maybe Microsoft will introduce an out-of-band patch, if it's found to be exploited.  Again, targeted attacks.  It does require user action and somehow to arrange to execute something.  But it's certainly cause for some concern.



So Abdulrahman Al-Qabandi - I practiced that name.



LEO:  Nicely done.



STEVE:  He's a Kuwaiti security researcher who discovered and published, after he was sure it was patched, a worrisome Microsoft Edge browser remote code execution bug.  And of course this argues against the idea of waiting too long to apply security patches because this is worrisome because it is so trivial to exploit.  So Microsoft did patch it this month.  And it is a potentially potent and trivial-to-exploit, website-deliverable, remote code execution bug.  So all that's necessary is some HTML and a bit of JavaScript, and to somehow induce the user of Edge on Windows 10 to press the Enter key.



So it's not clear, but it seems as though an advertisement could deliver this because it's just HTML and JavaScript, and we know that that's what ads are.  And if something could get you to press Enter, then this runs on your system.  Now, again, patched in October.  So if there's a window of opportunity, it exists between the acknowledgement of this problem.  He did wait to verify that the patch existed, that is, that Microsoft had fixed the thing that he had previously responsibly disclosed to Microsoft before posting, not only the details of the problem, but full working proof-of-concept code.



So the point is that we know that not all of the gremlins who inhabit the Internet are high-end exploit developers.  One doesn't need to be one in order to leverage this particular exploit.  I mean, it's laid out.  So it's a little more worrisome than it would otherwise be that almost anybody could leverage this into a working exploit.  So again, patched already.  But for any systems that are delaying for whatever reason the application of October's patches, this thing is there, and it does allow remote code execution under the default browser, which is Edge, for Windows 10.  So just sort of a heads-up, and another example of this very dynamic world that we're in right now with security updates and patches.



Yesterday, all four major browsers, and of course by implication their minor subsidiaries, announced in a coordinated announcement the planned deprecation of TLS v1.0 and 1.1.  The WebKit blog was titled "Deprecation of Legacy TLS 1.0 and 1.1."  Google called theirs "Modernizing Transport Security."  Microsoft said, "Modernizing TLS Connections in Microsoft Edge and IE11."  And Mozilla said, "Removing Old Versions of TLS."  In the show notes I have a graph from the Mozilla announcement which I will explain in words, but it's just interesting to see it graphically, showing for the period of August through September of this year, so the most recent couple months, under the beta of Firefox 62, the relative ratio of connections by TLS version.  So, okay.  And we'll get to that in a second.



Backing up a little bit, TLS 1.0, which was, as we covered at the time, mostly just a renaming of SSL v3.0 - which is to say there really was no difference.  They were the same protocol.  It just became time to say, okay, we're not going to do Secure Sockets Layer anymore.  We're now going to call this Transport Layer Security.  It will enjoy its 20th birthday this coming January of 2019.



So at the beginning of next year, TLS v1.0 turns 20.  And as such, and by any measure, it has been a spectacularly successful Internet protocol which, even today, nearly 20 years after its introduction, is without any flaws that are known to be serious enough to have forced its early retirement.  But nevertheless, its age is beginning to show through its lack of support for 20 years of subsequent progress, mostly in its lack of support for modern security protocols, that is, the handshaking and encryption and authentication protocols which are negotiated by the client and server.  There's been a lot of progress, not surprisingly, in 20 years that TLS v1.0 doesn't know about.  So it's not that there's anything broken about it.  It's just that the things that it's able to bring along in terms of its cipher suites, they're just not modern anymore.



So consequently, as I said yesterday, all four major browser vendors announced their coordinated deprecation, the end-of-lifing of TLS 1.0 and 1.1.  Where we are today is that TLS v1.0 is down near 1%; 1.11% of all connections today are using TLS v1.0.  And 1.1 never really got off the ground.  It tweaked the protocols a little bit to solve some problems for which there were already other workarounds.  So no one really bothered with it, such that it's currently at 0.09%, I mean, almost none.  And anything that supports 1.1 also supports 1.0, so it didn't ever really need to happen.



The major protocol today is 1.2.  And 93.12%, so better than 93% of all connections that were observed by Firefox during their period of monitoring were using 1.2.  And already we've discussed a few months back the final ratification of TLS v1.3.  That's at 5.68%.  So 1.3 is where we're eventually headed.  1.2 is where we are solidly now, with only 1.11%.  And let's see.  In fact, if we add 1.0 plus 1.1, that brings us to 1.2% of those older protocols.



So when do they die?  Even though they are barely seeing any use, they will not be formally killed until March of 2020.  So, what, 15, 16 months from now, 17 months from now?  So still plenty of time.  But it is the case that the deprecation will be staged and made clear,  I mean, so that everybody who is involved in this should know if they are in charge of websites which are not yet able to offer 1.2.



But it does mean that, when this finally happens in March of 2020, all the major browsers and their subsidiaries that are offshoots of their codebase will absolutely stop acknowledging TLS 1.0 and 1.1.  And again, not really for any clear reason except that it's just time to move on.  And it will allow the existing codebases to get cleaned up by no longer needing to back support a protocol that was originally created 20 years ago.  So Google's Chrome will begin deprecating 1.0 and 1.1 in Chrome 72, such that visitors to sites that are unable to accept 1.2 or 1.3 connections, that is, only understanding 1.0 or 1.1, will begin to see deprecation warnings in the DevTool Consoles from Chrome 72 on.



So that's 72.  Then there's 73, 74, 75, 76, all the way up to 81.  It won't be until Chrome 81 that 1.0 and 1.1 will be completely disabled.  So for what it's worth, you don't want to be anyone in charge of a website that cannot do TLS v1.2, or the Internet's going to seem very lonely after March of 2020.  You won't be having many visitors to your site, certainly not using any contemporary web browser.



In happy news, Google has added a technology known as "Control-Flow Integrity" to beef up the security of their Android kernel, starting with the kernel which is present in Android 9, which is the most recently released Pie version.  CFI, as it's known, Control-Flow Integrity, is an outgrowth of the LLVM project, which has been by all measures very, very successful.  That's where Clang, Objective C, C, and C++ have implementations.  It's a state-of-the-art compiler development project which began as a project in the University of Illinois, LLVM.



It probably originally stood for Low-Level Virtual Machine, I mean, that's what you would think the acronym would stand for.  The project makes it very clear that it is not an acronym; that their stuff has nothing to do with low-level virtual machines, although they did develop and have an intermediate representation.  That is, one of the things that compilers do is they'll often compile from a high-level language, especially if they have multiple front ends, for example, if it can do C and Objective C and C++ and Clang, also known as C Lang, and potentially other languages, those front ends, the high-level end will all compile to a common intermediate representation which is then - and that's sort of like a midpoint stage or staging representation.



And then you have, depending upon which hardware architecture the compiler is targeted for, you then take that common intermediate representation and emit code for ARM or for x86 or X64 or whatever architecture you're targeting.  So my guess is that that's where LLVM came from.  They're distancing themselves from that as an acronym, for what it's worth.



Okay.  So CFI is an outgrowth of that work.  It's been around for a few years and has been already incorporated into other projects.  Microsoft first incorporated it into Windows, and  they called it "Control Flow Guard," CFG, which appeared in the Update 3 of Windows 8.1 back in November of 2014.  And developers are able to add this, if they've got Visual Studio 2015 and later, by adding a /guard:cf flag to linker.  So it's readily available and present.  And as of Windows 10 Creators Update, which was the famous v1703, the Windows kernel itself is compiled with this Control Flow Guard.



So the central idea of both CFI and Windows implementation CFG is it's another approach to dealing with this Return-Oriented Program (ROP) exploitation.  We've talked about this, when you lock the system down such that you can no longer execute your own code which you've provided in a buffer overflow when the buffer is on the stack because the stack has been marked as non-executable.  Attackers, infinitely and always and ever clever, they said, oh, okay, well, we can't execute code we brought, so we're going to execute code that's already there.



And of course any OS kernel has got tons of code in it, waiting to be executed.  And although none of the subroutines or functions that are there may do exactly what the attacker wants, what clever attackers have figured is that the ends of subroutines, like the last chunk of a subroutine, might do something useful, after which, as subroutines do, they return to the caller.  So that's why this was known as Return-Oriented Programming, the idea being that you could knit together a malicious effect by just successively jumping into the tails of the subroutines in existing code, in order not to execute your own, but to execute code that's already there.  But notice that what's happening is you are making a jump into somewhere that you weren't intended to go.  That is, a subroutine is meant to be entered at the top and do its work and then exit at the bottom, not to be jumped into the middle of.



So anyway, the idea is to catch and prevent this malicious code reuse by checking the jump destinations of any data-driven indirect jumps.  An indirect jump is one where existing code is jumping to another location where that location is specified by the data contained in a register.  So it's indirect because the jump instruction itself doesn't say where to go to.  The jump instruction looks at the contents of a register and uses that as the destination for its jump.  That's called an indirect jump.  So since bad guys might be able to change a register's contents, they are then able to execute a jump to the location of their choosing.



So Windows prevents this, believe it or not, by maintaining a bitmap of allowable valid jump destinations.  And when a CFG application is being built, a bit of extra bitmap-checking code is added before every indirect jump instruction to verify that the location it is jumping to is a valid entry point, that is, something the system expects code to be jumping to, not code down in the middle of a subroutine which no valid code would ever jump to indirectly.



Chromium, the Chromium web browser, uses similar technology, the CFI technology under Linux, where the addition of CFI has been benchmarked to have an overhead, an execution overhead of less than 1%, but in this case it had a size overhead of about 15%.  The benchmarker said, well, this hadn't been optimized for size, so don't hold that against us.  But in today's plenty of memory computation world, the protection provided to a large attack surface such as a web browser probably merits the inflation of size by 15% if it really provides substantial protection against exploitation of the browser's own code because the browser, as we know, is the preeminent attack surface today.



So our news is that Google's Pixel 3 is the first Android device to ship with CFI protecting its OS kernel.  To the degree that OEMs don't turn that off and sort of leave it alone and just customize around the OS, other non-Google devices will probably inherit this moving forward.  And I expect it to become increasingly prevalent.



I did some poking around to see whether I could find any evidence of whether iOS was using CFI.  They're of course heavy into Objective C, and they are using the LLVM project's work for iOS stuff.  So I would be surprised if it wasn't in use.  I found a research paper from several years back where they had looked at using a jailbroken iOS and shoehorning this CFI technology into iOS.  But I didn't find any obvious indication that it is in use today.  I'd be surprised if it wasn't.



You know, again, Windows started incorporating it four years ago.  So it does look like it's yet another way of strengthening our software with, I want to say, "minimum overhead," certainly minimum execution time overhead, based on the Chromium browser benchmark.  And certainly useful, if it really does, if it is effective in preventing this class of attacks, as it might well be.



Okay.  So, oh, Leo.  There was a report that was commissioned by Congress to ask the U.S. Government Accountability Office to take a look at the state of the Department of Defense's readiness against cyberattack.  Now, we have to acknowledge that even the term "government accountability" is a bit of an oxymoron itself.  But I have a link to a 50-page PDF.  And I pulled some excerpts from it, one that just caught me by surprise that I thought was kind of fun.



So in their report the GAO says why GAO did this study:  "The DOD plans to spend about $1.66 trillion to develop its current portfolio of major weapon systems.  Potential adversaries," they write, "have developed advanced cyber-espionage and cyber-attack capabilities that target DOD systems."



What GAO found is sad.  It won't surprise anyone.  But, I mean, it's actually a little terrifying.  "The Department of Defense faces," GAO writes, "mounting challenges in protecting its weapon systems from increasingly sophisticated cyber threats.  This state is due to the computerized nature of weapon systems, DOD's late start in prioritizing weapon systems cybersecurity, and DOD's nascent understanding of how to develop more secure weapon systems."



LEO:  Really?  Nascent?  It's just being born.  Just now.



STEVE:  Apparently.  They're just thinking, well, you know, maybe we should start.  Maybe we ought to think about that, yeah.  "DOD weapon systems are more software dependent," they write, "and more networked than ever before.  Automation and connectivity are fundamental enablers of DOD's modern military capabilities.  However, they make weapon systems more vulnerable to cyber attacks.  Although GAO and others have warned of cyber risks for decades, until recently DOD did not prioritize weapon systems cybersecurity."



Let me read that again.  "Until recently, DOD did not prioritize weapon systems cybersecurity.  Finally," they said, "DOD is still determining how to best address weapon systems cybersecurity."  So not only is it nascent, Leo.  We have a committee, and the committee is going to figure out how we should consider going about addressing weapon systems cybersecurity.  But wait, there's some examples here in a minute.



"In operational testing," GAO writes, "DOD routinely found mission-critical cyber vulnerabilities in systems that were under development, yet program officials GAO met with believed their systems were secure and discounted some test results as unrealistic.  Using relatively simple tools and techniques, testers were able to take control of systems and largely operate undetected, due in part to basic issues such as poor password management and unencrypted communications."  Okay.  So this is like 20 years ago, right, in the commercial sector.  And this is today.



"In addition, vulnerabilities that DOD is aware of likely represent a fraction of total vulnerabilities due to testing limitations.  For example, not all programs have been tested, and tests do not reflect the full range of threats."  And I wrote in my show notes here:  "So begins a 50-page report detailing nearly total disregard for cybersecurity within the U.S. Department of Defense."  And then something caught me short, and I grabbed this about halfway down under "Test Teams Easily Took Control," where they did detail a little more specifically.



They wrote:  "Test teams were able to defeat weapon systems cybersecurity controls meant to keep adversaries from gaining unauthorized access to the systems.  In one case, it took a two-person team just one hour to gain initial access to a weapon system, and one day to gain full control of the system they were testing.  Some programs fared better than others.  For example, one assessment found that the weapon system satisfactorily prevented unauthorized access by remote users, but not insiders or near-siders.  Once they gained initial access, test teams were often able to move throughout a system, escalating their privileges until they had taken full or partial control of a system.



"In one case, the test team took control of the operators' terminals.  They could see, in real-time, what the operators were seeing on their screens and could manipulate the system.  They were able to disrupt the system and observe how the operators responded.  Another test team reported that they caused" - get this, Leo - "a pop-up message to appear on users' terminals instructing them to insert two quarters to continue operating."



LEO:  Game over, man.



STEVE:  "Test reports indicated that test teams used simple tools and techniques to disrupt or access and take control of weapon systems."



LEO:  This is terrifying.



STEVE:  "For example, in some cases, simply scanning a system caused parts of the system to shut down.  One test had to be stopped due to safety concerns after the test team scanned the system."  Yes.  In other words, as horribly insecure as our plastic blue-box SOHO routers are, they are arguably more robust than major United States Department of Defense weapons systems.  Our tax dollars hard at work.



Then, finally, I'll just finish with another quote from this report:  "Program offices were aware of some of the weapon system vulnerabilities that test teams exploited because they had been identified in previous cybersecurity assessments.  For example, one test report indicated that only one of 20 known cyber vulnerabilities identified by a previous assessment had been corrected.  The test team exploited the same vulnerabilities to gain control of the system.  When asked why vulnerabilities had not been addressed, program officials said they had identified a solution, but for some reason it had not been implemented."



LEO:  Oh, my god.



STEVE:  What do you know.  "They attributed it to contractor error."  That's right, point your finger.



LEO:  It's a big iceberg.  We only - it's a big iceberg.



STEVE:  Oh.  "Another test report indicated that the test team exploited 10 vulnerabilities that had been identified in previous assessments."  In other words, yes, we have problems.  And we knew it, and we've identified a solution, but for some reason it hasn't been implemented.  Wow.



LEO:  Well, at least this report exists.  I mean, I hope they listen to it and do something about it.



STEVE:  Hope they turn the temperature up, yup.  Our friends at the EFF are warning of the widespread use or, well, increasing use of facial recognition.  And I'm glad they're on the lookout for us.  They said:  "A proposed amendment to the Chicago municipal code would allow businesses to use face surveillance systems that could invade biometric and location privacy and violate a pioneering statewide privacy law adopted by Illinois a decade ago."



So again, this would be a City of Chicago municipal code which essentially is attempting to override a statewide good privacy law that was adopted a decade ago.  The EFF joined in a letter with several allied privacy organizations explaining their concerns, which include issues with both the proposed law and the invasive technology it would irresponsibly expand, so they say.



And so to give a little bit of background they said:  "At its core, facial recognition technology is an extraordinary menace" - this is the EFF writing - "to our digital liberties.  Unchecked, the expanded proliferation of surveillance cameras, coupled with constant improvements in facial recognition technology, can create a surveillance infrastructure that the government and big companies can use to track everywhere we go in public places, including who we're with and what we're doing.  



"This system," they write, "will deter law-abiding people from exercising their First Amendment rights in public places.  Given continued inaccuracies in facial recognition systems, many people will be falsely identified as dangerous or wanted on warrants, which will subject them to unwanted - and often dangerous - interactions with law enforcement."  They say:  "This system will disparately burden people of color, who suffer a higher false positive rate due to additional flaws in these emerging systems.  In short," they write, "police should not be using facial recognition technology at all, nor should businesses that wire their surveillance cameras into police spying networks."



And then they said:  "Moreover, the Chicago ordinance would violate the Illinois Biometric Information Privacy Act (BIPA). This state law adopted by Illinois statewide in 2008 is," they write, "a groundbreaking measure that set a national standard.  It requires companies to gain informed, opt-in consent from any individual before collecting biometric information from that person, or disclosing it to a third party.  It also requires companies to store biometric information securely and sets a three-year limit on retaining information before it must be deleted.  It empowers individuals whose rights are violated to enforce its provisions in court."



So they conclude, well, actually it goes much longer, but I concluded quoting them, saying:  "Having overcome several previous attempts to rescind or water down its requirements at the state level, BIPA now faces a new threat from this recently proposed municipal amendment in Chicago.  The proposal to add a section on 'Face Geometry Data' to the city's municipal code would allow businesses to use controversial and discriminatory face surveillance systems pursuant to licensing agreements with the Chicago Police Department."  Anyway, it goes on, but we get the idea.



And so once again we have a situation where the technological advancement and progress which we all herald is creating new capabilities that our established legal frameworks and assumptions have not been updated yet to address.  And of course we all know because it was just recent when Apple's iPhone X introduced their Face ID, a great deal of concern was raised about its privacy implications, and Apple went out of its way to explain how it was purely a local recognition and unlock capability and that this wasn't giving Apple access to our 3G face geometry.  It was entirely contained within the phone, very much like the finger ID beforehand.  So anyway, I'm very glad that the EFF is watching our backs.  And after I wrote that, I thought, yes, and not our faces.  So, yay.



In my browsing around for things to talk about that I thought were interesting, I ran across in Bleeping Computer a report from DuckDuckGo.  They publish the activity reports just in numbers of how much use they're seeing.  And I put a chart in the show notes showing essentially exponential growth, I mean, a rapidly accelerating growth in the use of DuckDuckGo.  You can see the numbers at DuckDuckGo.com/traffic because they publish this traffic report.  And they proudly claim, I mean, they proudly pronounce themselves as the search engine that doesn't track you.



What annoys me, and I've mentioned this before, is that Google's search links are now all redirecting, which is annoying because I want to be able to right-click and do a "save as" in order to grab the link that Google's search is referring me to, and they're all crazy nonce-burdened Google redirect links.  Which means, as we know, that Google, who knows who I am because there's my face on the browser page, I'm logged in with Google stuff, they know everywhere I go.  Every link I click on Google search is being tracked.



So I was curious, and I brought up a search under DuckDuckGo.com.  And sure enough, every link there is a direct link to the site that you're searching for.  So I know that there's a chunk of our listeners who are interested about their privacy and are annoyed by tracking.  So I just wanted to put DuckDuckGo on everyone's radar, if it wasn't already.  Oh, and I got a kick out of this, Leo.  Under donttrack.us, D-O-N-T-T-R-A-C-K dot U-S, the second slide there is kind of fun.  It shows Google preventing ads which are tracking.  Clearly, DuckDuckGo is presenting themselves as an alternative to Google.



Anyway, it's refreshing to see an alternative.  And in fact I think it was in the Bleeping Computer report, there were a large number of people that were responding to it, saying, yeah, it's all I use.  It's the engine I've chosen and so forth and so on.  So it looks like it is in fact very popular and, as we can see from the chart, growing very rapidly.



Ten weeks from now, at the end of 2018 - wow, 10 weeks from now, whew, 2019 - PHP's 5.x branch support ends.  I'm very glad that I've chosen PHP 7.  I mean, and I kind of had to work at it.  When I set up GRC's SQRL forums, I'm using XenForo, which is a PHP-based web forum solution.  And it works fine under 7, but all the defaults were for 5.  And that's still the case, broadly.  I mean, even though 7 has been around for quite a while now, for example, WordPress has their minimum requirement at 5.2.  Joomla is at 5.3.



And as I said, I had to kind of dig a little deeper and push and work in order to use 7.2 as my own platform moving forward.  I'm glad I did because, I mean, I could change, but it's easier not to.  Which is exactly the point here.  Right now 78.9%, 78.9, so just shy of four out of every five Internet sites, are PHP powered.  That is, to varying degrees they run on PHP.  On the stroke of midnight, when support for the 5.6 branch of PHP ends, at that point 62% of all Internet sites, which is that percentage of the total still running PHP 5.x, will stop receiving security updates for their server and their website's underlying technology, which at that point will, moving forward, expose hundreds of millions of websites, if not more, to potentially serious security risks.



Now, backing away from that, PHP is pretty secure.  It is relatively stable.  And it's been a while since we've seen a major problem with it.  But when you consider, I mean, essentially this increases the target-rich environment that attackers use.  It is open source, so the code can be scrutinized.  Starting with the beginning of 2018, 62% of all Internet sites that don't move will be vulnerable if a problem is found, and that version will not be patched.



So anyway, I just sort of wanted to put this on the "get ready for next year" radar because I wouldn't be surprised if, when it's known that vulnerable sites can't easily update, it may paint a bigger target on those sites that don't.  And for what it's worth, as a heads-up to everyone, 10 weeks from now, no more 5.6 branch updates.  So it might be worth considering, if you have an actively functioning operating PHP-based site, as apparently four out of every five sites on the Internet are, that you consider biting the bullet and making the move over to the 7.2 branch, which is where we are with the v7 series of PHP.  Which is not to say that there'll be a problem.  It's got a good security record now.  It's settled down and is being well run.  Let's hope that there isn't a big problem that appears.



And I mentioned skepticism about AV protection.  Which AV is the best, or the answer to that question, is a consequence of who you ask.  There is a site that to me looked unbiased and comprehensive, known as AV-comparatives.org.  Again, I'm not making any representations for it.  They explain, they said:  "Our Real-World Protection Test is currently the most comprehensive and complex test available, using a large number of test cases.  Currently," they write, "we're running this test under Microsoft Windows 10 Pro RS4 64-bit with up-to-date third-party software such as Adobe Flash, Adobe Acrobat Reader, Java, et cetera.  Due to this, finding in-the-field working exploits and running malware is much more challenging than, for example, under a non-up-to-date system with unpatched vulnerable third-party applications."  So they're testing against the sort of systems that a responsible person who, well, hasn't removed Adobe Flash is using.



So in the chart that I have in the show notes, they said:  "The results are based on the test set of 197 live test cases where malicious URLs were found in the field, consisting of working exploits, for example, drive-by downloads and URLs pointing directly to malware.  Thus exactly the same infection vectors are used as a typical user would encounter and experience in everyday life.  The test cases used cover a wide range of current malicious sites and provide insights into the protection given by the various products, using all their protection features while surfing the web."



So a couple things.  First of all, the chart shows essentially the percentage of among those 197 live test cases, by AV, how many of them were found.  Avast, AVG, BitDefender, F-Secure, Microsoft, Trend Micro, and is it Vipre? - yeah - were the only ones with perfect scores.  Of those, Microsoft had by far the greatest number of false positives, with Trend Micro in second place, F-Secure in third place, and the others lower.  And you do see a correlation here.  There is a tendency for those which were providing the best protection to have the highest false positive rate.  So that suggests that they're more heuristic.  They're cutting, they're slicing this a little further in favor of detection rather than not.



What I like is, if the takeaway is - as you and I, Leo, have been saying now for a while.  You know, given that add-on AV is having a problem under Chrome such that it's being essentially kicked out eventually, and that it represents an attack surface which can itself lower an individual's security, and given that Microsoft is one of the six AVs with a perfect score, maybe just using the system built into Windows is sufficient protection, downside being some possibility of false positives.  But I'd rather that than have things missed.  So anyway, I just thought this chart was interesting, and it does support the contention that running with what Windows now provides us is providing protection that is as robust, to the degree we believe this test, as the other contenders.



Okay.  So a little bit of "closing the loop" feedback.  I saw a tweet from - and I didn't know how to pronounce his name.  I thanked him for the tweet, and he wrote back saying his name is - he's actually Johan.  He said:  "Hi.  Listener of your show for about a year now, maybe longer.  What are your thoughts on OCSP?  Had an issue the other day where I could not access my website via Firefox or Edge; could in Chrome.  Turned off OCSP validation in Firefox, and it worked."  He says:  "Keep the setting off or turn back on?  Thanks."



So as we know, OCSP is the real-time certificate validation which is a way of solving the revocation problem.  That is, on the fly, a web browser can be asked to check for revocation.  One of the problems is that most web browsers have defaulted to defaulting open.  That is, if an OCSP server does not respond, then they'll just say, oh, well, at least we don't know that the certificate we're being asked to trust has been revoked.  We don't have an answer one way or the other.  Because the certificate itself provides the URL for the browser to use to check on its real-time revocation status.



So to answer Johan's question, I would, I mean, certainly running with it enabled as he had it in Firefox is more secure.  There is the problem that OCSP servers are sometimes not up.  So if you fail closed, that is, failed to not trust a certificate that you haven't been able to affirmatively verify is still valid, then you're going to have this problem for some sites.



I would say, if it's not too much trouble, turn it off to go somewhere that you trust.  Be very careful, if you turned it off to go somewhere, that you really are there because, I mean, this would be exactly the attack scenario, right, is that there would be no CSP support for that cert, and it would have been revoked.  And so somehow a bad guy would be blocking your access to it in order that you didn't know that it was broken.  So be very careful while you're there.  And once you're done, turn it back on again.  OCSP is much more robust today than it was 10 years ago.  It's only going to get better moving forward.  And I think ultimately we're going to get to a point where we can trust it.



Stapling is what we really want.  Stapling is the right solution.  I went over to GRC just recently and noted, I mean, I went over to SSL Labs and looked at GRC relative to cipher suite support.  And I smiled when I saw that the server I'm using supports OCSP stapling, and it's in use.  What that means is that the web server itself goes and gets a fresh OCSP attestation and includes it with the certificate that it provides the web browser.  So the web browser doesn't need to go out and separately get the OCSP certificate.  So it's really, I mean, as I've discussed where we've talked about revocation, OCSP stapling is absolutely the right solution.  And as servers evolve to support it, that solution will be increasingly available.  And then OCSP won't be a problem.



Also Timo Gruen tweeted:  "On Bloomberg and Supermicro," he says, "why put the chips ON the boards where they can be found?  Modern motherboards have so many layers, and the chips are so small, you could easily sandwich them INTO the board.  Good luck finding that."  And as a matter of fact, that was noted in the article.  There was some comment among experts who noted that, with motherboards being multilayer, you absolutely could actually bind the chip into the layers of the motherboard because a lot, I mean, the chip itself is vanishingly small.  It's the packaging which supports the pinouts on the chip which gives it any substantial size.  And even then, as we know, that's not much.  So I just wanted to acknowledge that that was something that was mentioned in the Bloomberg article that I failed to mention in last week's podcast.



Someone tweeting as the "Sultan of Saki," whose actual name he signed off "Josh Fenton," he said:  "Hi Steve.  A friend recently asked me how they should go about selling some used hard drives on eBay.  I explained to him the importance of wiping them, and as I was doing so I started thinking about how drives can swap out bad sectors for spares.  It occurred to me that it is possible that, if the drive was ever used unencrypted, that any data in sectors that had been swapped out would therefore be completely inaccessible to the OS or application layers, thus making it possible to wipe the data they contain.  If my analysis of this issue is correct, then this is yet another reason for users to always ensure that they encrypt their drives immediately upon installation; and in no case should they copy data to the drive until the encryption is complete.  Do you think my thinking is reasonable here, or am I missing something?  Thanks, Josh."



And, no, you are correct, Josh.  And this of course applies equally to thumb drives and SSDs also, which have the ability to maintain a reserve pool of storage space and swap it out.  There is in the latest spec, which I will be programming to when I return to SpinRite 6.x work, there is a secure wipe feature.  And I'll be looking at it closely to see whether the manufacturers have uniformly supported that as we would hope they have and allow the secure wipe to also wipe sectors which have been spared out, in which case that might be like I've talked about how I intend to produce a product called Beyond Recall, which will itself take responsibility for doing an extremely fast, but also secure wipe.  And it may incorporate the secure wipe feature as an option, depending upon what we learn about that when we look more closely.



But it is absolutely the case that one way to avoid the problem is by using whole drive encryption, either as an add-on like BitLocker, or down at the BIOS.  As we know, all drives now have the ability to support a password.  When you give the drive a password that the BIOS uses, then that drive is also doing whole drive encryption of itself.  You would then want to make sure you remove the password.  But in the process of removing the drive from the motherboard, then it will be passwordless, and the drive will be secure.  So that is another way to operate is to take the trouble to put a password at the BIOS level.



There are problems with doing that.  You need to be careful about that.  You need to, for example, if you're going to remove the drive for use outside, you need to remove the password from the drive in order to make the drive data accessible without the motherboard BIOS to provide the password.  So you have to use that with some caution, but it can be done.



Gary Napier asked:  "Hey, Steve.  Thanks for all the great info.  Do you know of any way to check your router to see if you are infected with VPNFilter?"  And I would say, first of all, we know that it exists persistently.  That we know.  It was VPNFilter which was found and now has, what, six different means of providing persistence for itself.  So rebooting the router won't help you.  What I would do, if you have any reason to suspect that the router model is one of those which is known to be vulnerable, is I would reflash the BIOS, I mean, reflash its firmware using the latest one from its manufacturer, and then immediately lock it down.



So rather than, I mean, there's probably no good way without really digging in and looking into the file system.  And even then, it's going to try to hide itself.  I would just go back to the factory.  Go back to factory settings, reflash the router, and then lock it down carefully.  And of course reflashing with the latest firmware may solve any known vulnerabilities at that time.  But certainly then, when I say "lock it down," I mean close off any publicly accessible services that you're not actively needing.



Okay.  And I just did want to finish with a tweet from John McAfee, for anyone who might have been worried after seeing this and experiencing the "presidential alert" message.  Our friend John McAfee tweeted "The presidential alerts:  They are capable," he tweets, "of accessing the E911 chip in your phones, giving them full access to your" - you know, "them" - "full access to your location, microphone, camera, and every function of your phone.  This is not a rant.  This is from me, still one of the leading cybersecurity experts.  Wake up, people," tweets John.



LEO:  Wake up.  Wake up, people.



STEVE:  Wake up, people.  Anyway, for anyone who might be concerned, there is no E911 chip.  There is a 911 function which does require that our phones provide location information for the 911 service to locate us.  But that's the limit of it, and it's got nothing to do with presidential alerts.  The presidential alert, as Leo, you said correctly when we were discussing this at the top of the podcast before we began recording, is a broadcast.  And it doesn't suddenly peg the instantaneous location of every U.S. citizen, all 300-plus million of us.



LEO:  That'd be quite the thing, yeah.



STEVE:  With cell phones.  So, yeah.



LEO:  We know where you all are.  Don't move.



STEVE:  Thank you for the public service announcement, John, but not a problem.



LEO:  Yeah, the government wants to know where we all are.  I guess it could be used to identify just where I am.  Right here.



STEVE:  So, Good Samaritans.  So first of all, we know routers are a problem.  Exactly how severe is the problem?  A study was conducted recently by the American Consumer Institute, ACI, a U.S. consumer nonprofit.  They found that five out of six home routers are inadequately updated for security flaws, leaving the devices and indirectly their users vulnerable to hacking.  Not surprisingly, but still five out of six.  The study, and I have a PDF link in the show notes, analyzed a sample - a relatively small sample, so that might make the study a little questionable - but 186 SOHO [Small Office Home Office] WiFi routers from 14 different vendors.  So they weren't looking for population as much as here's 186 different makes and models of routers in use.  What's their status?



So 14 different vendors.  They looked at the firmware version the routers were running and searched public vulnerabilities databases for known security flaws affecting each device's firmware.  So again, we don't know that that's remotely executable.  We just know that there's a security flaw that's known in the firmware which had not been patched for any one of these 186 routers.  They wrote:  "In total, there were a staggering 32,003 known vulnerabilities found in that sample."



So actually I guess the small sample size helps to keep this under control:  186 routers from 14 different vendors found to collectively have 32,003 known vulnerabilities.  They wrote:  "Our analysis shows that, of the 186 sampled routers, 155" - which is 83%, so a little bit more than four out of five - "were found to have vulnerabilities to potential cyberattacks in the router firmware, with an average of 172 vulnerabilities per router, or 186 vulnerabilities per router for the identified 155 routers," that is, of the 83%.  "Of the total 32,003 security flaws, more than a quarter were vulnerabilities that received the two highest severity ratings of 'critical' and 'high-risk.'"



Okay.  So three quarters of them weren't even critical or high-risk.  So they were a technical vulnerability, but nothing you need to worry about critically.  Still, a quarter of 32,000 is, what, 8,000.  So still significant.  They said:  "Our analysis shows that, on average, routers contained 12 critical vulnerabilities" - of course you only need one - "12 critical vulnerabilities and 36 high-risk vulnerabilities across the entire sample."  So a significant problem.



Last week news surfaced of a mysterious, as I said at the top of the show - I recall from my reading into this that he was Russian speaking - a mysterious vigilante grey hat hacker who is patching people's outdated MikroTik routers.  From what I read, he is not performing a remote firmware update, which, okay, well, he's not.  Whether we think that would be better or not is up in the air.



So as I also mentioned, I've noted that my advice has raised some controversy, that is, my advice being any device which a naive user can use that creates an exposure for them, and even for others, for example, in its use as a botnet host which is creating massive floods which are increasingly difficult to deal with, should have manufacturing-managed automatic update somehow.  People have pushed back, saying wait a minute, I hate the idea of my router updating itself.  So, okay.  I get that.  I mean, I understand that.



So how about when you use the router the first time, you are asked if you want automatic updates.  Or you're told that, unless you turn it off, they are on.  I think that's the right way.  I think it needs to default to maintaining itself, to phoning home, checking for an update.  Now, maybe it only notifies rather than performs the update autonomously.  That would be, again, another option, a back step, if it's something that can notify.  A light bulb can't notify unless it refuses to turn on or flickers or does something.  But then the typical user is going to have no idea what's going on.



On the other hand, a light bulb probably can't actually have firmware updates.  But hubs that run IoT devices, or certainly our routers can.  And so they could, for example, redirect their user to a notification page, saying, "There is a known high-risk vulnerability in this version of the router's firmware.  Sorry about that.  Please update the firmware.  Press this button to do so."



So again, there are many compromises that could be made.  I obviously have no problem with having a sophisticated user turning those things off.  And as we've discussed in the case of MikroTik over the last few months, MikroTik kind of has options for allowing the router to be profiled.  But even the least sophisticated profile doesn't protect the user who says, "I don't know what I'm doing, so I'm choosing this profile."  Even that isn't configured by default to protect them.



So these defaults have to change.  And it is entirely because of the default settings that MikroTik is in the trouble that it is today.  So now we have the case of a vigilante breaking the law because it is absolutely, definitely, unequivocally against the law to hack somebody else's router without their knowledge or permission, even if it's with the best of intents and to help them.  And note also that what this guy is doing is, as I understand it, not updating firmware, but bringing their firewall to bear, closing ports which are open, but it's possible that in some cases those are open deliberately.  That is, unfortunately, they are open by default.  They should absolutely not be open by default.  In that case, if they were open, they would be open on purpose, in which case closing them would probably break something.



So the problem we have is that with it being open by default, a vigilante, as well intended as they may be, is unable to determine whether or not in fact they are in use by somebody who needs remote access to a router, such that bringing up firewall rules to close them will break functionality which is needed.  So it's a mess.  Anyway, I just - it got a lot of coverage in the press.  And Leo, what do you think about the idea of somebody coming along and fixing things?



LEO:  It's terrible.  We've seen this before.  The chance of him doing something inadvertently bad are high.  And it is highly illegal.  And, no, it's not okay in any respect.



STEVE:  Yup.



LEO:  No.  How could you justify this at all?



STEVE:  Yup.  You're right.  I mean, there are too many ways that it can go wrong.



LEO:  Now, what if, however, MikroTik pushed out a firmware update?  That's fine.  So Mr. Russian Guy, go to MikroTik and help.  I guess MikroTik doesn't have an automatic update facility, do they.



STEVE:  No.



LEO:  Now, what if they took advantage of the flaw to do it?  No, that would be wrong.



STEVE:  Yeah.  MikroTik can't.  Now, if they're...



LEO:  Okay, I got one.  What if they popped up a message somehow?  What if they used this technique to at least alert people that there was a flaw and that they could go get firmware updates?  How about that?



STEVE:  Wait.  MikroTik or the Russki?



LEO:  Not the Russki.  MikroTik.



STEVE:  Okay, okay.  I guess it's a function of what the fine print of their license says.  I mean, there are MikroTik routers that have auto-update features.  So they're not completely naive to the idea of updating their routers.  They just don't have it on by default.  However, I should mention that I'm beginning to see reports from people who are saying that the updated firmware in various consumer routers is starting to offer auto-update as an option.



LEO:  Oh, I see that all the time, yeah.



STEVE:  Yes.  So yay for that.  And I don't have to tell our users that, yes, turn that on.  You want that.  And the problem is right now none of our routers - I'm not aware of a single router that notifies its user proactively.  You log into the router.  And, you know, for example, in  the case of a Netgear, I have a couple Netgear routers, there's a flashing exclamation point saying check for firmware updates.  And it's like, oh, well, it would have been nice to be notified.



LEO:  Yeah, yeah.



STEVE:  But we're not being notified.  So I agree with you, Leo.  If there was, I mean, if we gave them permission to intercept, to do a browser page intercept and put up a page saying you've got obsolete firmware, and it's obsolete in a bad way.  That would be a service.  I have a hard time imagining somebody being upset by that.  But I should mention there are reports of people being infuriated that some random Russian guy has changed their firewall rules, as you can well imagine.



LEO:  Yeah, yeah.  No, no, this is never okay.  And don't think it is, kids.  Knock it off.  Well, Mr. G., I think we've come the end of this edition of...



STEVE:  So it's Bad Samaritan, not Good Samaritan.



LEO:  Not Good Samaritan.  I don't think there's any conceivable case it's a good idea.



STEVE:  No.



LEO:  No.



STEVE:  No.  At the same time, it's nice to have 100,000 fewer vulnerable MikroTik routers.



LEO:  Maybe.  Maybe they're not vulnerable.  We don't know what the guy did.



STEVE:  True.



LEO:  Maybe they're more vulnerable.



STEVE:  True.



LEO:  Right?



STEVE:  True.



LEO:  Steve, you're always a breath of hot - of fresh air.  I'm just teasing you.  It's always a pleasure.  And I know people listen, wait all week to listen on Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  They wait all week for their Security Now! fix.  And actually, you know, if you don't have time to do it live, and that livestream is at TWiT.tv/live, you can get downloaded editions.  Steve has audio and transcripts you can download and read along as you listen at GRC.com.  That's where SpinRite lives, his bread-and-butter, world's best hard drive maintenance and recovery utility, plus a lot of other free stuff Steve gives away.  Lots of information.  GRC.com.  He's on Twitter, @SGgrc.  That's a good place to keep up with him.  But also, if you wish to communicate with him, you can do it there via direct message.  He accepts direct messages from anybody.



STEVE:  I do, I do.



LEO:  Although that's crazy.  But he does.  You can also go to GRC.com/feedback and leave a message there.  You can get audio and video files from us, TWiT.tv/sn.  That's where the Security Now! files are, TWiT.tv/sn.  Or subscribe in your favorite podcast application.  Details on how to do that, TWiT.tv/subscribe.  You know, if you've got one, you know how to subscribe.  Just search for Security Now!, and that way you'll get it the minute it's available.  Again, audio or video.  Although as Steve says, and I agree, who the heck wants to see us?  But we show your Picture of the Week and stuff like that.  There's stuff to see here.                                                                                                                                        



STEVE:  Yes.



LEO:  Steve, have a great week.



STEVE:  Will do, my friend.



LEO:  See you next time.



STEVE:  Right-o.  Bye.  



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#686

DATE:		October 23, 2018

TITLE:		Libssh's Big Whoopsie!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-686.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week a widely used embedded OS (FreeRTOS) is in the doghouse, as are at least eight D-Link routers which have serious problems, most of which D-Link has stated will never be patched.  We look at five new problems in Drupal 7 and 8, two of which are rated critical; trouble with Live Networks RTSP streaming server; still more trouble with the now-infamous Windows 10 Build 1809 feature update; and a longstanding zero-day in the widely used and most popular plugin for jQuery.  We then discuss what can only be described as an embarrassing mistake in the open source libssh library, concluding by examining a fun recent hack and posing its solution to our audience as our Security Now! Puzzler of the Week.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And as usual it's a roundup of significant flaws, one in a free real-time operating system, another in a well-known router - actually many well-known routers - and an exploit that uses jQuery's File Upload plugin, plus a red alert for Drupal users.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 686, recorded Tuesday, October 23rd, 2018:  Libssh's Big Whoopsie!



It's time for Security Now!, the show where we cover your security and privacy and how things work online with Mr. Steven "Tiberius" Gibson.  Hello, Steve.



STEVE GIBSON:  Leo, great to be with you again.  Trying to get my fingers in the proper Spock Vulcan salute.  But I think it's...



LEO:  It's not easy, is it.



STEVE:  Yeah, I think I'm a little slow on the uptake today.



LEO:  I can do it on my right hand.  And I'm a lefty.  I can't, for some reason, do it on my left hand.



STEVE:  I think I've told the story before, but the best man at my wedding, he had way too much information on me, and I was terrified that he was going to embarrass me horribly...



LEO:  Oh, yeah.  That's his job.



STEVE:  ...when he got up to give a toast.  And I said, "Gary, do not, do not embarrass me."  Anyway, so being the clever person that he is, he got some small red rubber bands because he needed them in order to get his fingers to do the salute unaided.  And all he did was he stood up and he said, "Gibson told me I wasn't allowed to embarrass him.  So I'll just say 'Live long and prosper.'"



LEO:  Perfect, perfect.



STEVE:  I said, "Thank you, thank you."  So we've got a great podcast.  The title is "Lib" - or you say "lib," I say "libe," potato, potahto - "Libssh's Big Whoopsie!"  This was just so fun that it immediately became the topic of the episode because there's a mistake in a fortunately not widely used - but still, I mean, it's around - SSH library, an open source library.  But the mistake is just so, as they would say somewhere in the U.K., "gobsmacking," that we just have to take a look at it because it's just too fun.  Also, in covering another story that we will also conclude with, the hack was so fun that I thought, this would make a great Puzzler of the Week for our audience.



LEO:  Ah.



STEVE:  So we haven't done this as often as we might.



LEO:  I would love to do it every week.  It's fun, yeah.



STEVE:  I know.  It just never occurs to me.



LEO:  Right.



STEVE:  But sometimes the setup is perfect.  And so we will examine a really fun hack, and then I will propose to our audience to, in the intervening week, find a solution in your head.  With everything that we've done on the podcast, all of the tools are available to our listeners who have been following along for a while.  There is a - well, anyway, now I'm stepping on the story.  But there's a super elegant solution.  And it'll be fun to see, just this little self test, who can come up with it, so.



But in the meantime, we're going to talk about a mistake found in a widely used embedded OS, actually it's the number one embedded OS, FreeRTOS.  RTOS stands for "real-time operating system," free as in free.  Anyway, they're in the doghouse.  And interestingly, Amazon has picked up FreeRTOS and decided to run with it and support it and sort of be its benefactor.  But I'm very impressed with the things that I've seen that Amazon has done.  We'll talk about the problems and their solutions.



Also there are at least eight D-Link routers which have serious problems, most of which D-Link has stated will never be patched.  And so there's some interesting takeaway lessons for that.  We're also going to look at new problems in Drupal 7 and 8.  And if you're using Drupal 7 or 8, and you didn't update in the last few days, go do it now.  Stop listening.  Hit pause.



There are five problems.  Two of them are remote code execution, which affect virtually all of Drupal 7 and 8.  So this is another bad security problem that has hit Drupal.  We talked about them not too long ago because I remember I was - you guys use Drupal, Leo, and your guys were already up on the news and had fixed it before we even got to it on the podcast.



LEO:  Oh, yeah.  Oh, yeah.  We have a good team on that, yeah.



STEVE:  There's also trouble with Live Networks RTSP streaming server that I'll just mention as sort of a public service for those who might be using it.  Still more trouble with the now infamous Windows 10 Build 1809 feature update which we need to touch on.  There's a longstanding zero-day in the widely used, most popular plugin for jQuery.  And it looks like it's for eight years this thing has been there.  And it's not even unknown to bad guys.  Somehow the security industry missed it.  Anyway, we'll talk about that.



And then we're going to talk about what can only be described as a consequence, I mean, it's where I got the title "Libssh's Big Whoopsie."  It's just it's too wonderful a mistake.  So I think - oh, and we have a great Picture of the Week.  So I think another great podcast for our listeners.



LEO:  Busy, busy, busy.



STEVE:  This number 686th podcast.



LEO:  And I am not complaining that we don't have puzzlers every week because you put so much - I don't know how much people realize how much work you put in.  If you've ever read the show notes, you know that this is a book Steve writes every week.  So having a puzzler in addition to all the other stuff you do, I don't know, that's asking an awful lot.  So we're happy, Steve.  



STEVE:  Well, this one just dropped into my lap.  And I thought, okay.



LEO:  Yeah, free puzzler. 



STEVE:  This is just too fun.  This week's picture, I got a kick out of it.  It had been in my queue for a while.  I don't know where it came from.  I'm sure somebody tweeted it to me, and I said, oh, that's just too funny.  Sort of a modest prompt for an account creation, this dialogue says:  "Pick a password."  And then it says:  "Don't reuse your bank password.  We didn't spend a lot on security for this app."  So just caveat emptor, you know.  Just be careful.  We wanted to get this out the door.  We asked Moe if he would make sure that it was secure, but we're not too sure about it, so don't use your bank password.



LEO:  That's hysterical.



STEVE:  Okay.  So FreeRTOS is the number one most popular real-time operating system currently.  On the first page, well, the second page of the show notes I've got a graph, or a chart, showing that the result of a questionnaire that was of 568 people asked last year what of all the various operating systems were they looking at.  And, I mean, everything is there.  QNX is down there at 3%.  Wind River Linux, that's another popular one at 5%.  TI has an RTOS, Texas Instruments, at 8%.  The larger OSes, Debian is at 12%, Ubuntu at 11%.



LEO:  Were they asking, though, for real-time operating systems, or just generally what operating system?



STEVE:  Just operating systems because I wouldn't call Debian...



LEO:  Debian is not a real-time...



STEVE:  No.



LEO:  Can you characterize a real-time operating system for people who are wondering what that is?



STEVE:  Yes.  And so that's a great question.  Essentially the idea is it's what you would want if you were literally going to be a light bulb and not a console with a file system and all that.  We know, for example, that our routers have Linux in them because they've got file systems and all kinds of modules and all kinds of stuff.  But imagine if you are an appliance, if you're a device, if you're a glucose meter or a parking meter or a light bulb or something that wants to use software, but wants to be more like an appliance than a general purpose thing.



So you need a few things.  You have to have a processor.  And it doesn't really make sense for the things that are always going to be done again to be done again, like memory management or thread management.  You might want to have like several different execution threads.  We've talked about execution threads a lot.  They're an abstraction of a single-threaded processor where you have a scheduler that jumps the processor around between different things so that everything sort of seems to be going at once, because none of them needs all of the processor, so they can share it.  So a real-time operating system will have a scheduler.



There's also typically a common memory pool.  And so this real-time operating system will have a memory manager which allows the threads to say I need to use some memory for a minute or two, which it then gives back to the operating system.  So the operating system doles out memory and recovers it after it's been freed.  Oh, and you often have interthread communications.  Sometimes you might have a couple threads that all may need access to a shared resource like the LCD on the parking meter.  And so you can't have two threads using it at the same time.  So there's something known as a "mutex," a mutual exclusion event object, where one thread says give me access to the LCD.  And then any other thread that also wants it has to wait until the first one is through.



Well, so the point is that that's sort of a common resource.  And again, the RTOS, the real-time operating system, is the one that manages those things.  So these threads of execution are clients of the operating system; but it's not an OS, as I've said, like we're used to - Debian, Windows, Mac or anything.  It's just it's like the bare necessities to have a processor, the processor hardware, being able to appear to be doing lots of things at once.  Like, for example, the LCD is being refreshed while the buttons on the parking meter are being scanned, while the credit card slot is being checked.  And maybe it's got WiFi into the city's network.  All of those things are sort of - they're not heavyweight processes, but they all have to kind of happen at once.  And so the RTOS does that.



So anyway, the point is that in this chart FreeRTOS, this particular one, is 28%.  It is the number one operating system where this group of people, of engineers, who were asked what they're looking at, this was the one that they were aiming for.  So as a consequence of its popularity, last Thursday's blog posting by Zimperium Labs, also known as zLabs - and we've talked about Zimperium in the past.  They've been a source of interesting discoveries on the security side.  The blog posting was titled "FreeRTOS TCP/IP Stack Vulnerabilities Put a Wide Range of Devices at Risk of Compromise:  From Smart Homes to Critical Infrastructure Systems."  And of course this is not good news to anybody who's using FreeRTOS.  And it's been around for a long time, so like 14 years, and it supports over 40, four zero, different hardware platforms.



LEO:  Wow.



STEVE:  I mean, every processor you could imagine.  Because it's a C library that is easily transportable.  If you've got a C compiler for a piece of hardware, you can port FreeRTOS onto it, and you only need a little bit of assembly language to deal with some things that C can't do because it needs to get right down to the bare metal.  But anyway, so it does have a TCP/IP stack.  And it turns out that there are some problems with it.  It's now at v10.0.1, and everything up to and including 10.0.1 are vulnerable.



LEO:  Oy.



STEVE:  Yeah.



LEO:  And the kinds of systems this is in may not be easily updated.



STEVE:  Oh, that's the problem, exactly.  That is, yes, that is exactly - and we know this is classic IoT stuff that may not be, as you said, subject to update.  Yet if it's using the TCP/IP stack which comes along with FreeRTOS, it's got problems.  So this is, I mean, there's a lot of concern in the community.  When Amazon took it over, they reset the numbering and named their fork, essentially, of it "AWS FreeRTOS."  And it, too, was vulnerable until fixed, up to v1.3.1.  And then there's a commercial version that is sort of like a - it's a functionally identical, so you wouldn't know you weren't using it, but it's not open source.



An outfit called Wittenstein High Integrity Systems (WHIS), they call theirs OpenRTOS and SafeRTOS, although it isn't.  And they're vulnerable, too.  So what Zimperium said in their blog was, they said:  "As part of our ongoing IoT platform research, zLabs recently analyzed some of the leading operating systems in the IoT market, including FreeRTOS.  FreeRTOS," they wrote, "is a market leader in the IoT and embedded platforms market, being ported to over 40 different hardware platforms over the last 14 years.  In November of 2017," so one year ago nearly, "Amazon Web Services took stewardship for the FreeRTOS kernel and its components."



They write:  "AWS FreeRTOS aims to provide a fully enabled IoT platform for microcontrollers, by bundling the FreeRTOS kernel together with the FreeRTOS TCP/IP stack."  However, I was impressed with what I discovered as I dug into this a little bit because these guys said "including modules for secure connectivity, over-the-air updates [yay], code signing, AWS cloud support, and more."  They said:  "With the infrastructure that AWS provides, and the AWS FreeRTOS platform, developers can focus solely on innovation, thus reducing development time and costs."  Sounds a little bit like an AWS ad, but it's not.  This was from the Zimperium guys.



And then they go on to explain what I already did about the commercial version from Wittenstein High Integrity Systems that also has these problems.  Anyway, they said:  "During our research we" - meaning Zimperium Labs - "discovered multiple vulnerabilities within FreeRTOS's TCP/IP stack and the AWS security connectivity modules.  The same vulnerabilities are present in the Wittenstein Connect TCP/IP component for OpenRTOS and SafeRTOS.  These vulnerabilities" - and, yes, they are as bad as they get - "allow an attacker to crash the device, leak information from the device's memory, remotely execute code on it, thus completely compromising it."



They said:  "We disclosed these vulnerabilities to Amazon and collaborated, and continue to do so, with them to produce patches to the vulnerabilities we detected.  The patches were deployed for AWS FreeRTOS versions 1.3.2 and onward.  We also received confirmation from Wittenstein that they were exposed to the same vulnerabilities, and those were patched together with Amazon."



They wrote:  "Since this is an open source project, we will wait 30 days" - so that counter has begun - "before publishing technical details about our findings, to allow smaller vendors to patch the vulnerabilities."



Zimperium's Ori Karliner, who conducted the research, discovered four critical remote code execution vulnerabilities, one denial of service, seven information leaks, and a partridge in - no, no, and one other which was unspecified.  So anyway, we don't have details.  But I just wanted to sort of put the word out.  This is, as you immediately reacted to correctly, Leo, I mean, this is big.  This is, I mean, this is like the microkernel hardware platform of choice.  Not all devices will be affected that use it because they won't be connected.  They won't have TCP/IP.  For example, the blood glucose meter probably doesn't have TCP/IP support.



LEO:  Whew, yeah.



STEVE:  And hopefully the pacemaker doesn't.  It uses something other than a WiFi link.  It's like electromagnetic encoding or something with a magnet placed over the person's chest to talk to it.  But, I mean, FreeRTOS is probably what's in these things because it's what you use.  It's very small, so it doesn't - it leaves maximum space for the application to use most of it and provides the same set of services that the users, the developers, the designers would have to implement anyway.  So why not use it?



The good news is it is getting fixed.  The bad news is not everybody will be as responsible as Amazon has been.  I mean, the fact that, I mean, I immediately dug into this, wondering what Amazon had done.  And they said in their material:  "Amazon FreeRTOS consists of the following components:  a microcontroller operating system based on the FreeRTOS kernel; Amazon FreeRTOS libraries for connectivity, security, and over-the-air updates; a configuration wizard that allows you to download a zip file that contains everything you need to get started with Amazon FreeRTOS; and over-the-air updates."



So they're, like, part of it is that they're offering this as a cloud service, the idea being, though, that they've made it like the default is that your device would check in with the Amazon cloud, probably not for free, but for a reasonable price.  In fact, as I'm saying this, I'm sure we've talked about this recently - well, not too recently, but like a while ago - that Amazon was going to be doing this; and that while, yes, you didn't have to use Amazon's service, you could use your own, if the price is right, why not just fall back on AWS and let them keep your device up to date?  So props to Amazon for doing this.



And the question is what non-AWS-based recent systems, because this only began with Amazon not quite a year ago, do use this always buggy TCP/IP stack with FreeRTOS and WiFi, I mean, you can imagine home connectivity IoT things, webcams and baby cams and baby monitors and security systems, that are using WiFi connectivity.  If they're WiFi, they're TCP/IP; and, more likely than not, they're using a now known buggy version of FreeRTOS.  So this may not be the last time we're talking about this on the podcast.  Anyway...



LEO:  So sad.



STEVE:  Again, props to Amazon.  This is what we need.  It's got to be in there from the beginning so that it's like a no-brainer for the developer who says, "Oh, yeah, I'd like to have the kernel updated if any problems are found in the future, thank you very much."  And why not?  If you have a TCP/IP stack, that means you could be on the 'Net, which means you can ping Amazon to see if they've got any news for you.



Okay.  Now, unfortunately, the flipside of doing it correctly is how not to do IoT deployment correctly.  And for that we have D-Link in the doghouse this week.  Once again, a trio of vulnerabilities can be combined to result in a complete takeover of at least eight D-Link routers.  A Polish researcher at the Silesian University of Technology in Poland discovered and reported responsibly to D-Link that eight routers that he found and tested all had, I mean, like really bad vulnerabilities.  However, D-Link informed him that they would only be fixing two of the eight, that is, the DWR-116 and the DWR-111.



However, I went to D-Link - and this was responsibly disclosed some time ago.  The timeline is really disturbing.  On May 9th he notified D-Link.  On June 6th he asked the vendor, having never heard anything, what was going on.  On the 22nd of June, he received a reply that a patch will be released - okay, June 22nd - that a patch will be released for the DWR-116 and the 111, but the other devices were EOL.  An announcement would be released.  Okay?  Then still nothing happened.  September 9th, still no reply from the vendor about the patches or announcement.  He wrote:  "I have warned the vendor that if I will not get a reply in a month, I will publish the disclosure."  And on October 12th he did so.



So they had plenty of time.  I went to the firmware pages for those two routers out of curiosity, the ones they said they would update, and they were still offering the vulnerable firmware yesterday.  So just something to be aware of.  I would argue that the height of concern, I mean, on one hand, I guess you would have to suggest that, okay, routers have a right to be EOLed, end-of-lifed; and how long should a router vendor be expected to maintain firmware for a really old router?  The problem is this looks like it's all the same firmware - the DWR-116, the DIR-140L, the DIR-640L, DWR-512, 712, 912, 921, and the 111.  All of those eight routers are apparently using the same firmware, which would argue that, if they just fixed it - just fixed it - they could make that firmware available to the whole family, for people who did want to maintain security.



Okay.  So what are the vulnerabilities?  We've got, as I said, a trio of them.  The first one - and this is publicly posted now so everybody knows this.  The first one allows remote attackers - right, remote - to read arbitrary files via the classic forward slash dot dot, which we've talked about before.  Dot dot, as we know, is the go back a level in the directory hierarchy, known as the "directory traversal attack," where you go /.., /.., /.., each one taking you up a level in the directory hierarchy until you get to the root.  Then you move back down to the directory you want.  He discovered that this works after a GET command to a /uir, which was some resource on the router.  So he then posted a proof of concept where he issues a curl to http://routerip/uir// - that's the double forward slash that also performs the backup - and then etc, you know, et cetera, /passwd.  Yes, that's the passwords file.



LEO:  Is it only on the router?  It's not reaching into the network, though.



STEVE:  No, it's only on the router.



LEO:  Any file on the router, okay.



STEVE:  Well, at this point it's only...



LEO:  Oh, because now you have the passwords.



STEVE:  Now you have the password file.  And he writes:  "The vulnerability can be used to retrieve administrative passwords using the other disclosed vulnerability.  This vulnerability was reported previously" - get this, previously reported - "by Patryk Bogdan in a 2017 numbered CVE, but he reported it as fixed in a specific release.  But unfortunately, it is still present in newer releases."  So they had some sort of regression where they apparently briefly patched it, but then it became unpatched in subsequent releases.



He says:  "The vulnerability is also present in other D-Link routers and can be exploited not only, as the original author stated, by double dot, but also absolutely using double slash."  So that suggests that the double slash immediately takes you to the root, and then you do etc/passwd in order to get the password file.



LEO:  Yeah.  And fortunately, that's encrypted; right?



STEVE:  Okay.  Vulnerability.  You've been reading ahead, Leo.



LEO:  No, I haven't.  It's obvious this is worthless if it is.



STEVE:  It's so painful.  Vulnerability number two:  Password stored in plaintext in several series of D-Link routers.  And it's so bad that he even redacted the file from his own public vulnerability disclosure, after waiting half a year.  He says:  "Note:  I have redacted the filename in the description to XXX because the vendor leaves some end-of-life routers" - and even those that aren't, as I verified yesterday - "unpatched, and the attack is too simple."  So in other words, this is so awful that even the researcher was unwilling to disclose it fully.



So he wrote:  "The administrative password is stored in plaintext in the /tmp/" - and here's where he redacted - "XXX/0 file."  Now, of course, anybody can reverse-engineer any of the D-Link router firmware, get the name of that file, and now you know where to find the plaintext admin password.  He says:  "An attacker having a directory traversal can easily obtain full router access."  Right?  Because it's under /tmp/something/0.  Well, the first vulnerability gives you directory traversal.  



And so now he has, again, a proof of concept:  $ curl http://routerip/uir//tmp/XXX/0.  He says:  "This command returns a binary config file which contains admin username and password, as well as many other router configuration settings."  Meaning that it's binary, but they're right there unencrypted in ASCII, standing out.  He says:  "By using the directory traversal vulnerability, it is possible to read the file without authentication."



And, finally, as if that wasn't enough, vulnerability number three:  "Shell command injection in httpd server of several series of D-Link routers."  So "An unauthenticated attacker may execute arbitrary code by injecting the shell command into the chkisg.htm page Sip parameter.  This allows for full control over the device internals."  And proof of concept:  "Log into the router.  Request the following URL after login."  And there he provides - now, remember, logging in is not hard because we can now be admin.  Username and password, we can get that using the first two vulnerabilities.  Now that we have those, we use this - and he provides the full HTML query which gives you a full shell command injection.



And so he finishes, saying exploiting all three together - "taking all three together it is easy to gain full router control including arbitrary code execution."  And then he gives us a link to a description with a video.  So where we are today is that, as far as we know, I mean, every router, every D-Link router he checked was using the same firmware with the same vulnerabilities.  They probably only have the firmware which they have sprayed across who knows how many D-Link routers through time.  We know old ones.  They're not going to fix them.  And even the ones that are still being maintained, they said to him on September 9th that - oh, no, June 22nd was the word back, yeah, we'll fix two of them, but not the other six.  Well, those two still haven't been patched.



So I think the takeaway here is that old routers are bad.  I do accept the idea that a vendor shouldn't be responsible forever.  And I was tempted to go back and figure out like when these various routers were first released.  But I thought, okay, I do want to get SQRL finished someday.  So I don't know how old they are, but I do know that every D-Link user is now in danger, and  there is no recourse currently from D-Link to get yourself out of danger.  This is now public.  As we know, I mean, we've been talking about problems with routers.  It's going to take minutes for Mirai botnet to add this to their collection of things to try as they scan for routers on the Internet.  You don't want to be using a D-Link router is what it comes down to.



And unfortunately, that message will not get - it will not ever get out to most of the users of D-Link routers in the world.  All of our listeners now know and need to be concerned, unfortunately, whether their particular model of D-Link router is vulnerable.  There is absolutely no reason to believe at the moment that it isn't because every D-Link router tested of eight have been, including current ones and especially old ones for which D-Link is never going to provide a firmware update.  So I think at this point we need to consider routers as a commodity which ages out of use for reason of the vendor no longer maintaining it.



And certainly in this case, I mean, as we said, routers are the attack target of the year, maybe of the decade, because they're all sitting on the Internet.  They've got networks behind them that may have juicy tidbits on them.  And even if not, facing outward, botnets are grabbing them up and using them for attacks and reflecting traffic and more.  So I just - any listener, I mean, again, if I had the spare cycles I would add a test to GRC to check for this particular problem.  If you're a D-Link router user, certainly if you have any of the D-Link routers I mentioned and that are written in the show notes, there's nothing you can do, as far as I know, except make sure that nothing is exposed publicly.  Apparently this does require a public server access for someone outside to get to your router.  So if that could be turned off so that there's no WAN-side admin...



LEO:  If you turn off the WAN.  Okay, good.  All right.  That's good.



STEVE:  Yes.  Presumably that will protect you.



LEO:  Unless there's a bug there, too.  I mean, who knows; right?



STEVE:  There was no mention of mitigation in anything that I found.  So I can't say one way or the other.  But I do think that at this point we have to take the position that an unsupported router, obviously one where there's known vulnerabilities the manufacturer has said they have no interest in fixing, even though it appears they could just fix it once and make the same firmware available, they just don't care.  So at that point you just have to say, okay, they're not that expensive.  It's worth saying, you know, it's worth having a garage sale and sticking it out there on the table and say to your neighbor, well, good luck with this.  I've got a new one.



LEO:  Here.  Here's a broken router.  Good luck.



STEVE:  Works great.  And the hackers love it.



LEO:  Yeah, works great for everyone.



STEVE:  That's right.  Okay.  So Drupal.  As I said at the top, if you're using Drupal 7 and 8, US-CERT, the United States Computer Emergency Readiness Team, announcement said a remote attacker could exploit some of these vulnerabilities to take control of an affected system.  So it really needs to be Drupal 7 and 8 have a problem.  There were five problems.  Two were critical, and three Drupal's own security team said were moderately critical.



One of the two critical bugs is an injection vulnerability in the default Drupal email backend, which uses PHP's mail function, which is DefaultMailSystem::mail in both Drupal 7 and 8.  When using this default mailer to send email, some variables were not being sanitized - get this - for shell arguments.  As is common, when untrusted input is not sanitized correctly, remote execution may result.  And in this case it does.



The second of the two remote code execution bugs exists in Drupal 8's Contextual Links module.  In Drupal these modules supply contextual links that allow privileged users to more easily perform tasks related to regions of the page, thus contextual, without having to navigate to the admin dashboard.  However, the Contextual Links module also doesn't sufficiently validate the requested contextual links, which allows an attacker to launch a remote code execution on those links.  That is to say that the links themselves are to code in Drupal which assumes its own variables haven't been tampered with.  But you can tamper with them, use the same link target URLs, and execute your own code on that Drupal service.



So then in addition to those two baddies, the Drupal security team acknowledged that there were three other moderately critical ones.  They said users of any version of 7 should move to at least 7.6.  Users of 8.6.x should move to at least 8.6.2.  And users of 8.5.anything or earlier should move to Drupal 8.5.8.  And then they noted in their security advisory that minor versions of Drupal 8 prior to 8.5.anything are not supported and do not receive security coverage.  So sites running older versions should update to the above 8.5.x release, which is currently 8.5.8.  And those older Drupals, 8.5 series, will receive security coverage until May of 2019.  So if you're going to jump, it's probably worth jumping to 8.6.2.  Just bite the bullet now so that you can continue to get coverage.



Now, although Joomla and Drupal both lag far behind WordPress's nearly 60% domination of the content management system (CMS) market, Joomla and Drupal having 6.6 and 4.6% usage in the CMS market, respectively, even 4.6% of Drupal CMS-driven sites being vulnerable to remote code execution is no laughing matter.  So I hope any admins using Drupal are signed up for security updates and are going to take these problems seriously and get this fixed.



LEO:  I have to say, I mean, as Drupal users here, we love Drupal, and I've used Drupal since the beginning.  You know, it's easy to - any software can have bugs, and Drupal does a good job of keeping it up to date.  And they always have said for years, don't use old versions.  Keep it up to date.  But sometimes the jump is huge.  Sometimes it's a big discontinuity, as we mentioned before, between major versions.



STEVE:  And in fact it's funny you should mention that.  There does look like there's some things that they changed so that you do need to dig around in the code a bit.  So it's not just a completely seamless jump.  They're not happy with some of the functions that they have defined, and they had to change them.



LEO:  They changed - yeah.  So, yeah, that's always been the problem.  I blame PHP.  I really do.  A lot of this - and I think the RTOS, FreeRTOS, as well - goes back to the choice of language.  And I just - people need to use type-safe languages and capture these problems at compile time, not run time.  That's just...



STEVE:  Amen.



LEO:  Yeah.  We know how to do that, you know?  So let's, let's.



STEVE:  Yes.  Declare all your variables and make sure you don't use them until you've declared them.  



LEO:  Yeah.  Things like that, yeah.



STEVE:  Gee, what a concept.



LEO:  Because a lot of this comes to referencing null pointers and things like that, and you can avoid that.  And a compiler should catch it anyway.  All right.  I'll get off my high horse.



STEVE:  Okay.  So this is sort of mostly just a public service announcement for anyone who might be using and have a publicly available real-time streaming protocol (RTSP) media server.  There's a company called Live Networks that has a very popular multi-format RTSP media server known as LIVE555.  It contains, unfortunately, a critical remote execution bug.  Boy, is this becoming a broken record.



LEO:  Heck, yeah.  Yeah, yeah, yeah.



STEVE:  Which affects versions prior, all versions prior to last Wednesday's release of 0.93.  So that just happened on October 17th.  And if you haven't updated, if you're using LIVE555 streaming media server to offer anything publicly, if it's just internal Intranet then you're okay, assuming you can trust all your internal users.  But there is a remote execution bug which would allow any publicly exposed version of this media streaming server to be taken over remotely.  So anyway, I did not get a sense for how widely used it was.  But again, it only takes one, if you're the one who uses it, and somebody's able to scan, find the server, and say, oh, thank you very much, we want to crawl inside your network through this little portal that you've created.



LEO:  We don't use it, but I'm well aware of Live.  They've been around for a long time.  And in fact...



STEVE:  Yeah, Live Networks is like the real deal.



LEO:  Yeah, they're one of the biggies.  The only thing I know, though, is I have a real-time streamer, Facebook streamer, that I bought the Mevo cam from them, which probably uses that protocol.  



STEVE:  Yeah, probably.



LEO:  Yeah, something to be aware of.



STEVE:  So we're unable to stop talking about, for better or for worse, the Windows 10 October 2018 update.  The infamous Build 1809 has another problem.  So as we know - the good news is it's still not rereleased yet.  So they found this in some preview build 18234, also known as 19H1.  I've not been tracking all this cryptic insider...



LEO:  19H1 is the next one.



STEVE:  Okay.  Okay.



LEO:  So 1809 is the one that was supposed to come out now, September 2018.  Now you're talking about 1903, basically, which is 19H1, the first half of 2019.



STEVE:  Ah.



LEO:  So that's in the Insider - the Insiders are getting this now.



STEVE:  Okay.



LEO:  You know, there's something wrong with the process at Microsoft.  This is actually getting to be problematic.  I mean, seriously.



STEVE:  Yeah.  And you know, Leo, we're looking for that new era of enhanced productivity.



LEO:  Oh, lord.



STEVE:  Was that what they were advertising?



LEO:  That's something, and the most secure version of Windows ever.



STEVE:  That's right.  I love that one.



LEO:  As we well remember.



STEVE:  Good old XP.



LEO:  I've seen a number of articles recently saying the problem really is the way Microsoft does this, which is they've got a code base.  They do these short, like six-week sprints to add a feature.  So they spend a long time thinking of the features.  In six to eight weeks they create the feature.  There's no testing at that point.  They lay it into a testing version which they then test for a long period of time.  This is how they did it when they did three-year-releases, but they're still doing this now for these biannual releases.  And it's not an effective testing process.  They need a better way of testing before they get them into these beta releases.  That may be.  There may be a structural problem here.  We'll talk about it tomorrow on Windows Weekly, I'm sure.



STEVE:  So as we know, when the content, those of us who are power Windows users and who understand zip files, when the content of a zip file extraction would cause the overwrite of an existing same named file within the archive, the user - I know, Leo.  Are you sitting down?  The user should be prompted about the pending overwrite.



LEO:  Collisions, yeah.



STEVE:  That's right, a file naming collision, and asked whether to replace or skip the extraction of the colliding file.  It turns out...



LEO:  It doesn't.



STEVE:  ...that Build 1809 is reportedly and reproducibly either overwriting existing zip file content without notification or silently failing and doing nothing.  So the good news is this problem has been caught before, well, what I wrote - now I'm not sure.  So it's been caught before the full formal re-release of Build 1809.  But it was reported as being in 1809.  I assumed it was the pre-release people.



LEO:  Yes, yeah.



STEVE:  A recent tweet by an IT staff engineer at Microsoft on the Windows Insider Program Team indicated that this problem has been resolved back on October 6th with the Windows Insider Preview Build 18234, which is 19H1. 



LEO:  So that means they fix it for the next generation.



STEVE:  Yeah, well, they clearly have to fix it now.



LEO:  Fix it for both, yeah.



STEVE:  And I know that this is just - I'm kicking this dead horse one last time.  But in some recent reporting over on Computer World I noted that Microsoft's forensic analysis revealed that as many as 1,500 instances of Build 1809's pre-release testers had their files deleted and complained without Microsoft noticing.



LEO:  Microsoft said it was 0.01%.



STEVE:  Yeah, well...



LEO:  What did you just say?  What percentage?



STEVE:  1,500 by number.



LEO:  Oh, okay.



STEVE:  1,500.



LEO:  That might fit.  That might fit.  Because a lot of people try these builds.  This is a public build.



STEVE:  They did.  They did.  And but a lot got bit.  And Microsoft said, oh.



LEO:  Yeah, that's a lot of people.



STEVE:  We missed that one.



LEO:  Lot of people with bad [crosstalk].



STEVE:  Also I should just note that, after talking about this for the last few podcasts, in between then and now I updated the machine I'm talking to you on, Leo.  This camera that I'm looking at is running Windows 10.  It was running Windows 10 Home, which is what came preinstalled on the little Windows 10 box that I just grabbed, just a little turnkey box.  Anyway, it's now running Pro because I'm an MSDN subscriber, so it doesn't cost me anything to update.



So I updated to Pro since I definitely decided that I want to begin hanging back from each month's security updates as well as the biannual "feature" update.  There's nothing I need that much each month that's worth being bit like this.  And I'd rather let them, you know, I think that the security release we would know within a week if it was causing problems.  So I've set that to give me two weeks.  And I think I set it to 30 days for the feature update because we would know by then if it's something, if you really should put it off further.



So again, and as I mentioned, Windows 10 Home does not give you the option to delay, to defer these.  You take them when they make them available.  I hope that our listeners consider, after this painful set of October surprises, consider deferring, as I now have, I mean, so much so that I switched to the Pro version just so that I could have that feature.  It just seems wrong that Microsoft is being stingy about that.  No, we're going to make you have Pro if you want to defer.  It's like, my god, okay.



Okay.  So the most popular, second only to the jQuery platform itself, the most popular jQuery plugin, which has been around for 10 years, is vulnerable.  This is the jQuery File Upload plugin which was released, like I think it was within a week, just like five days before the Apache team changed the way the .htaccess file is handled in Apache.  As a consequence, the use of .htaccess, which is, as people know, .htaccess, anyone who's configured Apache, you're able to use that to place that file in a directory to apply access restrictions to that directory.  Well, it turns out that 10 years ago, with Apache v2.3.9, the Apache maintainers deliberately disabled support for the .htaccess file, apparently as a performance improvement, because then the server would not need to check for this file every time it accesses a directory.



LEO:  I remember when this happened because I used .htaccess frequently, and it broke a lot of stuff, yeah.



STEVE:  Yes, yes.  And also the problem is this left some developers - oh, and the other reason was the Apache people didn't want the local application of .htaccess to interfere with the server-wide configuration because they had alternative means for providing that protection.



LEO:  Yeah, you just use sig file now.



STEVE:  Right, exactly.  Okay.  So the story goes that the Messaging Malware Mobile Anti-Abuse Working Group met in Brooklyn, New York two weeks ago, Monday through Thursday.  Attending that meeting was Akamai's Larry Cashdollar.  That's actually his last name.



LEO:  What a great name.



STEVE:  Cashdollar.



LEO:  For a guy running a CDN.  Awesome.



STEVE:  He expected the weather to be nice, so he failed to bring a raincoat, and it rained throughout the week.  So Larry was hotel bound.  Having therefore nothing else to do - he couldn't walk around, sample the local fare - he decided to poke around at the various add-on packages available for Node.js at NPM, which is the packet manager for Node.js, npmjs.com.



Okay.  So I read into the story pretty far, as you can tell.  I'll skip the details of how he arrived at what he found.  Two days later Larry posted an entry on Packet Storm titled "jQuery-File-Upload 9.22.0 Arbitrary File Upload," with the description "jQuery-File-Upload versions 9.22.0 and below" - meaning all previous - "suffer from an unauthenticated arbitrary file upload vulnerability that allows for" - you guessed it - "remote code execution."



Okay.  So first of all, it's always been the case that allowing uploaded files to a server is extremely fraught, I mean, it's inherently fraught with danger.  This is not to say that it's not possible to do so safely.  But few things should instill more fear in the heart of the responsible web designer than enabling file uploads.  How many times have we here on this podcast covered buffer overrun exploits in image renderers?  As we've said, interpreters are very difficult to get correct.  And there have been JPEGs and GIFs and PNGs, I mean, all these image formats.



LEO:  Well, and even JavaScript, to be honest.



STEVE:  Yes, yes.



LEO:  We had that problem.  We were bit with malware.  We had an old Drupal plugin that allowed somebody to hack our code.  And he was able to upload arbitrary code to an executable directory.  And it was JavaScript.  Or maybe it was, probably was PHP, come to think of it.  But, yeah, I mean, [crosstalk] anything you want.



STEVE:  Yeah.  So imagine that you're able to put anything you want anywhere, and then you invoke it from a URL outside...



LEO:  And that's the problem with PHP.  It's a URL-invokable protocol. 



STEVE:  Yes, yes.



LEO:  So stupid.



STEVE:  So what we have here is much worse.



LEO:  Worse?



STEVE:  Than even that, yes.  Due to a presumably, as I mentioned, well-meaning change that the Apache group made back in 2010.



LEO:  Well, that's one of the ways they used .htaccess is to keep people from uploading files to directories; right?



STEVE:  Yes, yes.



LEO:  To block a directory.



STEVE:  So starting with that version of Apache, 2.3.9, the httpd server offered an option that would allow server admins to ignore custom security settings made to individual folders via the .htaccess files.  This setting was made for security reasons, was enabled by default, which means, as I've often said, the tyranny of the default, enabled by default, and remained so for all subsequent Apache httpd server releases.  So in the process this jQuery file upload, which is the most popular plugin, second only to the platform itself on GitHub, its assumption that it could protect its file uploads using a local .htaccess file was rendered, since 2010, invalid.



So on GitHub this plugin says:  "File Upload widget with multiple file selection, drag-and-drop support, progress bar, validation and preview images, audio and video for jQuery.  Supports cross-domain, chunked, and resumable file uploads.  Works with any server-side platform - Google App Engine, PHP, Python, Ruby on Rails, Java, et cetera - that supports standard HTML form file uploads."  In other words, it's been around for years.  Not surprisingly, lots of people use it.  It's very popular.



So our Larry Cashdollar says in his vulnerability disclosure:  "The code in" - and then he cites the URL on GitHub - "doesn't require any validation to upload files to the server.  It also does not exclude file types.  This allows for remote code execution."  As Larry wrote in his description of this discovery, he said:  "I started looking through the package's source" - this is during a rainy day in Brooklyn - "and found myself peering at two PHP" - there's your favorite acronym, Leo, or abbreviation - "files under the directory server/php.  The files are named upload.php and UploadHandler.php.  The upload.php file calls the main file UploadHandler.php where all of the file upload code resides."



He says:  "I also saw that all files were uploaded to the files/directory in the web server's root path.  I wrote a quick command line test with curl, and a simple PHP shell file confirmed that I could upload a web shell and run commands on the server."



LEO:  Yikes.



STEVE:  And it's literally one line, and he used example.com just for the safety of posting, where he provides shell.php.  And shell.php is a simple PHP program that just launches the system command shell into the HTTP response.  He says:  "A browser connection to the test web server with cmd=id returned the userID of the web server's running process."  He said:  "I suspected this vulnerability had not gone unnoticed."  And get this, Leo.  "A quick Google search confirmed that other projects that used this code or possibly code derived from it were vulnerable.  There are a" - get this - "a few YouTube videos demonstrating the attack..."



LEO:  Is there anything YouTube can't do?



STEVE:  "...for similar software packages."  Okay.  So this is of extra concern because the jQuery File Upload bug is not some obscure widget.  It is an extremely capable, as we noted, it is extremely capable and an extremely popular add-on - get this - having been forked on GitHub 7,828 times to create descendant projects of that base package which are widely spread throughout the industry, deployed on websites far and wide.  So that means right now, once again, this is public, and all PHP-based sites which chose to use this jQuery file upload in its 7,828 variations are currently subject to any attacker uploading any file of their choosing, executable, and running it on that hosting server.  Since discovering this critical vulnerability, Larry's been busy.  He's examined 1,000 out of the 7,828 forks of the plugin.  Every one of them was also exploitable.



LEO:  Wow.  Of course, because you just copy the code, yeah.



STEVE:  Exactly.  And still worse, it turns out that at least some of the underground hacker community have been aware of this widespread backdoor for years.  As ZDNet explains in their coverage under the title "Zero-Day in popular jQuery plugin actively exploited for at least three years," they said:  "A fix is out, but the plugin is used in hundreds, if not thousands, of projects."  They say:  "Patching will take ages."



ZD said:  "For at least three years, hackers have abused a zero-day in one of the most popular jQuery plugins to plant web shells and take over vulnerable web servers.  The plugin is the second most starred jQuery project on GitHub, after the jQuery framework itself.  It is immensely popular, has been forked over 7,800 times, and has been integrated into hundreds, if not thousands, of other projects, such as CMSes, CRMs, Intranet solutions, WordPress plugins, Drupal add-ons, Joomla components, and so on.  A vulnerability in this plugin would be devastating, as it could open gaping security holes in a lot of platforms installed in a lot of sensitive places."



They say:  "This worst-case scenario is exactly what happened.  Earlier this year" - and as we know it was a few weeks ago - "Larry Cashdollar," they write, "a security researcher for Akamai's SIRT (Security Intelligence Response Team), has discovered a vulnerability in the plugin's source code that handles file uploads to PHP servers."  And on and on and on.



Cashdollar says that attackers can abuse this vulnerability to upload malicious files on servers such as backdoors and web shells.  He said the vulnerability has been exploited in the wild.  "I've seen stuff as far back as 2016," he told ZDNet in an interview.  And apparently the vulnerability was one of the worst-kept secrets of the hacker scene and appears to have been actively exploited even before 2016.  Larry found several YouTube videos containing tutorials on how one could exploit the jQuery File Upload plugin to take over servers.  One of the three YouTube videos that Larry found and shared with ZDNet was dated August 2015.



So actually I'll note, I've mentioned this before, but it is for exactly this reason that my own forthcoming PHP-based - as I mentioned, I did go to v7.2 because, since I was setting up a server fresh, why not?  The SQRL public forums are hosted on that.  But they are running on their own physically separate and network-isolated machine that has no connection to any of the rest of GRC's network because there's just no way to trust a system like that.  These sorts of things are going to happen.  And while, yes, as long as somebody is wired into security events in the industry, you can keep up with things, I can't have code that I didn't write that was sourced from hundreds of different places in order to glue together a solution.  I can't have that on my network.



So yikes.  I hope that anybody who is aware of what's going on, knows that they used a descendant of this jQuery File Upload, will recognize that the author had the best of intentions.  He worked with Larry.  Initially he could not duplicate what Larry was seeing because the author's PHP test server was not configured to ignore the .htaccess file.  By default, as we know, for eight years, since 2010, Apache has been.  So anyway, the author immediately put file type restrictions on last week's fix of this.  But it needs to be fixed comprehensively.  Wow.



Oh.  And we've talked before about the dangers of lapsed domains.  Since domain ownership is valuable, we have systems in place to rigorously protect that ownership.  As a consequence, over time, trust is created since domains are rarely successfully hijacked.  But what about when a domain that's in use for some purpose is deliberately abandoned, and its name is allowed to lapse?  We've talked about the problems of overlapping security certificates in the past where somebody would have a certificate that was still valid for a domain that was reregistered.  Lapsing domains is something we see all the time since the Internet, as we know, is a constant churn with domains being abandoned and created.



We sometimes find that a link we haven't visited in a long time now takes us to some weird search engine or a marketing page or something.  Advertisers long ago figured that lapsed domains would see some traffic, some non-zero level of traffic.  So they began snatching up any that lapsed to camp out their own nonsense there.  And in fact that happened to me.  I used to - I referred in Podcast 44 to a domain that I had, grcmail.com, which I deliberately allowed to lapse.  And if you go to grcmail.com, my uBlock Origin immediately blocks it because there's some horrible marketing junk.  Some marketer grabbed that domain name when I allowed it to expire because I didn't want to keep paying for it every month, and I decided I wasn't ever going to use it.  And now somebody's camped out there.



But what happens when a supplier of active content, like in this case embedded web page scripting, decides to throw in the towel and no longer host something that they have been providing for years?  The Sucuri blog tells the story of that very nicely.  I've paraphrased from what they wrote.  They said when Twitter announced their new design for Tweet and Follow buttons back in October of 2015, so just about exactly three years ago, marketers across the web developed a mild anxiety, Sucuri wrote.  The new design came with a decision to nuke their beloved Tweet Count feature.  Social signals can be a huge credibility indicator for visitors and site content.  So who doesn't think there's a psychological relationship between the number of social shares and the credibility of the content that's there?  It's social validation, they write, plain and simple.



Naturally, bloggers and website owners with an aversion to change started looking for alternative solutions that offered the same feature.  Marketers breathed a sigh of relief when easy-to-use services started popping up to offer Twitter share counts, and one specific one called "New Share Counts" quickly gained traction.  It even integrated with other existing social share plugins, they write, like SumoMe, AddThis, and Shareaholic.



Setting up New Share Counts on a site was simple:  Navigate to newsharecounts.com, which by the way is gone.  Link your Twitter account and website, then add two lines of code to the bottom of every page you want to track shares from.  And so there's script type text/JavaScript.  And then it says the source for the scripts is //newsharecounts.s3-us-west-2.amazonaws.com/nsc (as abbreviation for new share counts), nsc.js.  So what happens is, naturally, any time a visitor pulls a page from a site that has decided to use New Share Counts, the script at the bottom of the page pulls nsc.js from that AWS bucket and runs it.  Right?



So this summer, after not quite three years, in July of 2018, newsharecounts.com was abandoned, and its service was discontinued.  The service's original provider was about as responsible as one might hope.  He posted a notice on his site referring visitors to opensharecount.com or - and apologies to Leo - twitcount.com.



LEO:  Ay yi yi. 



STEVE:  Uh-huh.



LEO:  I can't win.



STEVE:  Yes.  Okay.  So, however, Sucuri, they did some digging.  They found that more then 800 websites, more than 800 websites did not get the message.  They continued to embed the now discontinued script, which as I mentioned pulls a file, that file nsc.js, from an AWS S3 bucket.  That nsc.js script originally loaded another script from newsharecounts.com, so it was a redirect, which after the domain was discontinued stopped functioning.



On October 3rd of this month, three weeks ago tomorrow, the original AWS S3 bucket was canceled.  Some very clever and nefarious hacker had apparently been waiting, like literally checking it daily, for just that to happen, since the next day they registered a new AWS S3 bucket under the same name and uploaded a malicious version of the nsc.js script.



Sucuri wrote:  "During a recent remediation investigation, our Remediation Team led by Ben Martin noticed malicious redirects being served by websites using the New Share Counts service."  In their report, in their blog posting, they repeat that URL of the newsharecounts.s3-us, the idea being that the original one was abandoned, releasing the registration of that AWS bucket.  A bad guy grabbed it, much as someone would grab a domain name that had been abandoned, and hosted malicious script there under the same name, nsc.js.  I've got a picture of it obfuscated in the show notes.



The Sucuri guys decoded, decrypted this.  They said:  "Once decoded, this script contains absolutely no reference to Twitter or New Share Count.  Instead, this snippet of code adds 10 fake browser history entries for the page that hosts it.  An interesting feature of these history entries is that it prevents the user from choosing the previous page from the back button.  When the user loads the page, a malicious event handler is added.  This handler waits until the user taps the back button on their device or tries to navigate to a previous page.  It then fires an event, which causes the browser to open to the following destination."  And they give the URL of it.  It's a scam page instead of taking them back to the previous page.



They said:  "This behavior is only seen on Android, iOS, and other mobile devices, and only under the condition that the user taps the back button in their browser.  Users on other devices won't notice anything suspicious, except for maybe the lack of Twitter share counts that would exist if the original New Share Count script actually functioned."



They said:  "At the time of writing, 800-plus sites use this script, some of which are fairly popular, who simply wanted to show off their social signals.  Now they are instead maliciously redirecting all of their mobile users to this malicious traffic.  Loading third-party scripts and elements," they write, "on your website always opens up the risk of unwanted content being served on your site without consent, especially when they come from an expired or unmaintained service."  So, yikes.



And Leo, about a month ago I mentioned that I had received a DM from someone whose - the DM I sort of ran out of time in preparing the show.  He provided a lot of information, which was sort of too much, I mean, it was sort of a little bit superfluous.  And I didn't have time to get it into shape.  But I did this time, and I didn't want it just to go unnoticed.  He sent this on Saturday at 8:03 a.m. was the time of his tweet.



The tweet's subject or first line was "SpinRite giving a blast from the past."  He said:  "Hi, Steve.  I've been a long-time listener to Security Now!, and each week someone gets to tell his SpinRite success story.  I've been owning a copy of SpinRite for years and occasionally tried it on hard drives when one of my numerous RAID boxes would drop, and SpinRite would perform its magic.



"Two weeks ago on a Sunday, our Active Directory master server dropped off the network, and when I came in on Monday I immediately checked it.  It had crashed and would no longer boot.  The Active Directory server was using some older SAS" - which is a form of SCSI interface - "drives, in RAID 0, on a controller without BIOS support."  Now, what that means is that, when you boot it, SpinRite wouldn't naturally see it.



He says:  "So I needed to add the old DOS aspi8xx.sys and symdisk.sys drivers" - any of us old DOS hands around here will remember those days - "to SpinRite's boot config.  That allowed SpinRite to see the drive.  Running a Level 2 SpinRite scan on the drive found and repaired two errors on the drive, one which the drive fixed on its own and one where SpinRite's Dynastat recovery kicked in to work at repairing the sector."



He said:  "Since Dynastat took a while, I left it running overnight.  And when I came back in the next morning, SpinRite showed me a green screen, stating that it had successfully completed all its tasks.  Just to be double sure, I reran the Level 2 scan, and SpinRite zipped through the drive in 90 minutes.  Then I reinstalled the drive into its original server.  The SAS controller recognized the drive and reassembled the RAID 0 and virtual drive."  He says:  "Windows 2012 Server booted right up and worked again without issues.  Thanks for a terrific product and for letting me have my very own SpinRite story.  Kind regards, Stephan Budach in Hamburg, Germany."  And Stephan, thanks for sharing.



And I'll just mention that, moving forward, just for exactly this reason, I'm going to continue to support config.sys drivers.  They're not often needed.  Anybody with a motherboard made in the last 10 years will be able to use the AHCI chipset which SpinRite 6.x will then be able to talk natively to.  But for the sake of infinite backward compatibility, 6.x moving forward will continue to support config.sys drivers for instances like this.



LEO:  What a blast from the past.



STEVE:  Yeah.  Libssh's Big Whoopsie.  This was just, as I said, it's just too fun.  Okay.  So I'm sure most of our listeners are familiar with SSH.  For those who are not, it is sort of the logical evolution of the original telnet.  Telnet was a service, a server and client telnet protocol, which gave you basically a remote console.  You could use a telnet client which looked like a console window, connect to a remote server over TCP, and you got a prompt.  Typically you had to log in.  It would give you a challenge of a login and then a password.  And you'd be logged in as if you were a user sitting in front of the actual console of that same machine.



The problem was telnet didn't itself offer any inherent security.  It relied upon a strong username and password.  So which today just seems quaint to us, the idea that you would have a telnet port open.  And this is why, when I've talked about routers that do, I just put, you know, you put your head in our hands.  It's like, really?  It's just an invitation for a bad guy to hook up and start guessing usernames and passwords.  There's often no monitoring, no limit to how many they can guess, no mitigation against that at all.



So what's happened is, over time, responsible OSes have started refusing to even allow it.  I was using telnet in my multilayered security for a much earlier version of FreeBSD.  I think it was v4.  And we're at 10 or 11 now, or more.  And it's funny because, when I recently set up a new Unix machine, I just sort of defaulted to - because, I mean, I've got layered, layered security.  There's no way I would ever allow a port 23 to be exposed.  And it didn't.  But the machine itself, because of the security environment, could have port 23 enabled and other things were connecting to it.



Unix, FreeBSD Unix refused.  It was like, what?  Are you crazy?  No.  No.  We will not let you have telnet exposed on a public interface.  And it's like, yeah, but I know what I'm doing, really.  No.  We don't care.  It's like, okay, fine.  So SSH.  SSH, whereas telnet ran on port 23, SSH runs on 22.  In the acronym SSH is Secure Shell.  And so it is a next-generation, much more secure solution.  I use it, as I'm sure you do, Leo, with certificates.  We've talked about this before.



LEO:  Actually, I use an SSH key, but same idea; right?



STEVE:  Okay, yes, right.  So you're using some...



LEO:  Public key, yeah.



STEVE:  ...high-entropy thing.  Yes, exactly, a public key that no hacker - it's not a matter of guessing usernames and passwords.  You can have those, too.  But you also need to have a private key that no hacker's going to have.



LEO:  I guess that's the same as a certificate.  I use...



STEVE:  It is.  It is a certificate.



LEO:  Yeah.  I use OpenSSH to generate - or maybe I use PGP to generate a key, and then the public key gets stored on the server, and I have access to the private key.  Or is it the other way around?  Yeah, public key [crosstalk].



STEVE:  Yeah, probably the private key - yeah, well...



LEO:  No, the private key is - I can't remember.



STEVE:  Yeah.  And actually you're able to key each end.  So it's able to do mutual authentication.



LEO:  That's right, yes.



STEVE:  The server has a certificate.  Your client has a certificate.  And they check each other in order to validate the connection.



LEO:  Right.



STEVE:  So it sort of goes both ways.  That way you know, well, that protects you from a man-in-the-middle attack because each end is validated with static certificates.  Okay.  So that sort of sets the context.  It's all about security.  I mean, and you need to rely on that; right?  Okay.  So what happened here with libssh?  The design of SSH uses a state machine.  A state machine is sort of a logical abstraction.  They're very handy.  They're a way of simplifying a complex system so that the system is, at any given time, is in a single state.  And so like it might be in connection accept state.  Then it accepts a connection, and so then that event of accepting a connection moves it from connection accept state to connection accepted state.  Okay?  And so from there a number of things are possible.  It might then be able to receive a request for - like bring up the protocol event.



Anyway, so it's a state machine where at any given time it's only in one state, and different events cause it to move from where it is to where it is next.  Okay.  So Peter Winter-Smith of the NCC Group, who we've also spoken of before, or the group, discovered in this libssh what can only be described as a deeply embarrassing flaw.  This flaw was introduced four years ago, back in 2014, with libssh version 0.6.  So 0.6 and above all have an authentication bypass vulnerability in their SSH service.  And as we were just saying, authentication is the whole reason you have SSH.  So if you can bypass authentication you're back to telnet.  And in fact worse because sometimes SSH, because the authentication is so good, it authenticates you to the platform that you're logging into also.



So, okay.  During normal SSH protocol, the client requests a user authentication challenge by sending the server a message that's known in the SSH protocol, SSH2_MSG, for message, USERAUTH_REQUEST.  So that's the client sending that protocol message, SSH2_MSG_USERAUTH_REQUEST.  If all goes well in their back-and-forth handshaking of swapping public keys and private keys, generating nonces, signing nonces with the public key, and then sending it back, and the other end verifies it with their private key, I mean, all this rigamarole to be like ultra-galactically super sure that you've authenticated both endpoints.  Everybody's got privacy and lots of bits in there, entropy, and all that's happening; right?



Finally, if all goes well, the authenticating code at the server end will emit an SSH2_MSG_USERAUTH_SUCCESS message to indicate that the authentication succeeded and the user is now authenticated.  But believe it or not, what Peter Winter-Smith discovered, and I can just imagine the look on his face, this buggy for the last four years libssh state machine does not discriminate between the source of the messages.  He discovered that a remote connecting client can skip all of that hassle and simply send the SSH2_MSG_USERAUTH_SUCCESS to the server and be immediately authenticated and given full remote shell access on the target system.



LEO:  Eliminate the folderol.



STEVE:  So do not request authentication, which you may not be able to provide, after all.  Simply send "We've just successfully authenticated."



LEO:  You know who I am.  Come on.



STEVE:  And the server says, "Oh, okay, welcome."  Okay.  So how widespread are these publicly accessible libssh servers?  Well, to answer that question, everybody immediately goes to Shodan and does a Shodan scan, which turns up 6,000 candidate currently public libssh-based servers.  Closer probing confirms that there are approximately 3,000 servers connected to the Internet that use the library.  And roughly 18 to 1,900 of them, that is, 1,800 to 1,900 are currently vulnerable and are using this vulnerable version of the library.



Who uses libssh?  Well, Red Hat indicated that Red Hat Enterprise Linux 7 Extras uses it.  F5 Networks BIG-IP load balancers use it.  KDE uses libssh for its SFTP file transfers.  Cisco is examining their many devices since they also chose libssh.  GitHub implemented their git ssh server with libssh.  But the particular way GitHub uses it protects them from this exploit.  There's a remote desktop solution for Linux, known as X2Go, as in X server, X2GO.  It uses libssh.  Csync, a bidirectional file synchronizer.  Remmina, I can't pronounce it, Remmina, R-E-M-M-I-N-A, has a GTK+/Gnome Remote Desktop client which uses it.  XBMC is a media player and entertainment hub for digital media, which uses it.  And the GNU Gatekeeper, which is a full-featured H.323 gatekeeper uses it.



On the next page of the show notes, Leo, I've got them broken down by organizations.  And yes, Verizon Wireless has 602 servers currently vulnerable.  Number two is Sprint PCS.  It looks like about 325, maybe 330.  Then we've got Fastweb, University of Maryland, Telstra Internet, Korea Telecom, Telefonica, Amazon.com.  Whoops.  They've got about 75.  Comcast Business, looks like they have about 45.  And another instance of Telefonica with about 40.



And then the green chart is a breakdown by version, showing that 0.6.0 is by far the most used at 1,275, and then it falls off pretty rapidly.  But 0.6.3, 0.7.0, it looks like only - no, actually that's, yeah, 0.6.0 and looks like - I can't tell if that's 0.2 or 6.2.  But that's the second most.  And then so forth.  So anyway, since the announcement, there are now at least four proof-of-concept scripts which have been uploaded to GitHub.  One is a Python script.  There's libssh authentication bypass, that's blacknbunny.  Hackerhouse-opensource has put one up.  And Vulnhub has put up one for the exploit.



Also the guys at Leap Security posted a Python-based scanner to search for vulnerable versions of libssh.  It's there at LeapSecurity.io on their blog.  I've got a link to it in the show notes.  It's got two modes:  a passive mode which determines if systems are vulnerable by banner grabbing, and an active mode which attempts to actually leverage the vulnerability to bypass the server's authentication to confirm its vulnerability.



So needless to say, those 1,800 to 1,900 servers are under attack now.  And they need to get fixed because there is a publicly available scanner.  There is open source, four different open source proof of concepts presented showing anybody how to do it.  And you can have a shell on any of these machines.  So it's not as if this thing were OpenSSH, which everybody uses.  It's a lesser used libssh library.  But it's been used, and boy does it have a mistake.  Yeah, we don't really care who says the authentication succeeded.  Just claim it, and we'll let you go.



LEO:  Wow.



STEVE:  Okay.  And we wrap with this week's Security Now! Puzzler of the Week.  And Leo, you need to be on your best behavior because...



LEO:  I'm not going to guess.  No guessing.



STEVE:  Well, you can guess, but not until after we stop recording.



LEO:  Not out loud, okay.



STEVE:  Because we want to let our listeners - I'm going to pose this as a design problem.  It's just it's perfect.  Okay.  So the back story is vending machines made by Argenta, a successful Italy-based purveyor of coffee services, are also widely used to dispense soft drinks, dry goods, cigarettes.  They're a very popular Italian-based vending machine company.  Some of these machines were located on a university campus - you never want to put your high-end vending machines on a university campus, Leo, that's just asking for trouble - where they came to the attention of Matteo Pisani, an Italian hacker who is also the CTO of Remoria VR.  Matteo was made curious by the fact that these high-end vending machines support both Bluetooth Low Energy (BLE) and Near Field Communications (NFC) technologies to link to a companion Argenta Wallet smartphone app.  You can see where this is headed.



The app was poorly designed, easily reverse engineered, and exploited.  The "Wallet," I put that in air quotes, maintained the user's balance, which the vending machines naturally assumed to be valid.  Presumably, unlike last week's Picture of the Week where remember the coffee machine was offline for I guess an hour and a half while it updated itself during the middle of the day, these high-tech vending machines lacked a connection to the Internet to enable independent verification of the user's balance.  This lack of verification allowed Matteo to easily manipulate the Wallet's database, which he was able to find and decode and reverse engineer and give himself, in the screenshot that was shown, 999 EU of credit, which the machines readily accepted, thus giving him free soft drinks, dry goods, and cigarettes, probably for the rest of his life, except that he chose to responsibly disclose the problem to the company and gave them...



LEO:  Wow, that's nice.



STEVE:  I thought it was, yes, very nice.  Okay.  So he did report and responsibly disclose this to the parent company, who presumably will work out a solution.  So what solution?  We've often talked about the impossibility of encrypting a DVD which a consumer's DVD player must be able to decrypt right there in the living room.  Therefore the keys to decrypt it must be present in the accessible player for it to work.  But the security model here is subtly different.  And using only the concepts we have often discussed here on the podcast, there is a wonderfully simple, robust, and utterly uncrackable solution for the design of this system.  And Leo has assumed his Thinker pose.



LEO:  It sounds a little bit like SQRL.



STEVE:  Well, I've talked about it.  It uses those sorts of technologies.  I want everyone to think about this for the next week.  Next week I'll describe how I would solve this problem, and everyone can check to see whether they came up with the same simple, elegant, and uncrackable solution - or, who knows, perhaps something even better.  So that's the Security Now! Puzzler for the Week.  You have vending machines.  You've got a smartphone app.  The app can be reverse engineered.  You can't protect it.  What technology could we think of that could be used to create something absolutely elegant that would work and be uncrackable?  Think about it for a week.



LEO:  In the sense that the wallet, the integrity of the wallet would be assured.  Is that what we're trying to do?  Or any sense?



STEVE:  In whatever sense is necessary.  



LEO:  Okay.



STEVE:  Just think about crypto things.



LEO:  I've invented this thing called a "coin."  All right.  I like this.  So everybody's going to get to work, think about it, and we'll talk about the answer next week on Security Now!.  That's awesome.



STEVE:  And I think people will like it.  Or at least they will like my solution, and we'll see if they got the same one because it's right there.  And if they didn't, it'll be like, doh.



LEO:  And please don't use the word "blockchain" in your answer.  That's all I'm going to say.  If you can't watch live or be here live, you can always get on-demand versions of the show.  Steve's got audio and transcriptions, so you can read along as you listen, at GRC.com.  While you're there, pick up a copy of SpinRite, the world's finest hard drive maintenance and recovery utility, as you know.



STEVE:  It is.



LEO:  Lots of freebies there, yeah, including SQRL and all the ongoing development conversations around that.  And I recommended - I have a friend who has trouble sleeping, and she's been using the Sleep Formula with great results.



STEVE:  Oh, yay.



LEO:  Yeah.  As do I.  I can go on and on.  But, you know what, it's a great site.  Lots of stuff, a real rat hole.  Prepare to spend an afternoon.



STEVE:  That's right, it's a rat hole.



LEO:  It's a rat - in a good way.  You go fall in, it's a rabbit hole.  Maybe that's better.  You fall in, and you just don't want to leave:  GRC.com.  We have it here at our own particular rat's nest, TWiT.tv/sn.  We actually have audio and video, although god knows why you'd want to watch us.  But if you do - unh-unh.  Steve says unh-unh.  If you do, you can get it there, TWiT.tv/sn.  Or of course, if you subscribe, find your favorite podcast application on your phone or your tablet or whatever and subscribe, you'll get it every Tuesday afternoon, the minute it's ready.  Steve, thanks.  See you next time on Security Now!.



STEVE:  Okay, my friend.  Till then.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#687

DATE:		October 30, 2018

TITLE:		Securing the Vending Machine

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-687.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we follow up on the Win10 ZIP extraction trouble, discuss some welcome Android patching news, look at SandboxEscaper's latest zero-day surprise, examine the Hadoop DemonBot, follow up on U.S. DoD insecurity, look into the consequences of publicly exposed Docker server APIs, look at a DDoS for Hire front end, check out the mid-week non-security Windows 10 bug fix update, look at the just-released Firefox v63, and examine a new privilege escalation vulnerability affecting Linux and OpenBSD.  We also handle a bit of errata, some sci-fi miscellany, and a bit of closing-the-loop feedback from a listener.  Then we answer last week's puzzler by exploring various ways of securing those vending machines.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  More zero-day exploits in Windows 10.  A problem with Docker.  Google's plan to fix Android.  And Steve has the answer to his conundrum from last week, our vending machine problem.  All coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 687, recorded Tuesday, October 30th, 2018:  Securing the Vending Machine.



It's time for Security Now!, time to protect yourself and your loved ones online, as we do each week at this time with this guy right here, Steve Gibson of the Gibson Research Corporation, GRC.com.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you again.



LEO:  Yeah.  You were just up the road apiece.



STEVE:  I was in wine country this morning.



LEO:  Nice.



STEVE:  I awoke among the grapes.



LEO:  Did you have a nice visit?



STEVE:  I did.  I've mentioned to you before the geek group that we were - it was the MRC, the Math Resource Center gang at my high school.  So that was 45 years ago because we were graduating class of '73.  And one of our group is sort of a wandering nomad, and he had taken a picture maybe six months ago of a group that excluded me that was our group.  And I said, okay, I am never going to miss another opportunity, because it's been 45 years since we've all hung out.



And so as it happens, there was an opportunity that came up yesterday.  And so I prepared the podcast, I spent all day Sunday working on the podcast, getting it ready for today, so that I could take yesterday off, which is normally my podcast prep day, and spend it up in Napa.  So I got there by early afternoon, and we just hung out and caught up on 45 years of all of our shared history.  And it was great.  So I spent the night, woke up, and flew back down so I could be here in front of my blinky lights.  And it's good...



LEO:  It's always nice to sleep in your own bed, if you can, I have to say.



STEVE:  I can't wait.  I can't wait.  So, yeah.  Although I do travel with my pillow because that's - Lorrie and I joke that I have three feathers in my pillow.  It's the limpest pillow you have ever, ever seen.  And everybody...



LEO:  Hard to get a limp pillow at a hotel these days.



STEVE:  Yeah, everybody's got these foam things, and your neck is all cricked.



LEO:  I don't like a foam pillow.



STEVE:  And you've kind of got to get on top of it somehow.  It just doesn't work.



LEO:  Oh, I'm with you.  I like feathers.  Give me feathers.



STEVE:  So basically I travel with a pillow.  This is Episode 687, and I titled it the answer to our last week's teaser because, oh, my goodness, has our listener audience had fun with this.  I mean, I was getting feedback from every available channel in every orifice from every venue.  I mean, it was just...



LEO:  But did anybody get it right?



STEVE:  A couple people did.  There was of course the inevitable crazy over-engineering where, yeah, we've got certificates flying in every direction.  You've got to put a catcher's mitt up when one's flying by and then grab it and then go home and come back.  Anyway, so we will wind up with kind of a careful walkthrough of what would be the minimal solution if it weren't for an intriguing problem that the minimal solution has, and then just sort of inch forward a little bit to how to solve that, which then ends up with a really elegant, nice solution.  So this podcast is titled "Securing the Vending Machine."  But of course...



LEO:  And you'll restate the problem before we give the answer.



STEVE:  Yeah, of course.  So of course lots of other stuff happened.  We do have a follow-up from Microsoft on last week's mentioned Win10 zip extraction trouble.  We've got some welcome news from Google on the Android patching front.  Believe it or not, our apparently depressed hacker who goes by the handle SandboxEscaper...



LEO:  She's at it again, yeah.



STEVE:  She's depressed again and has dropped another zero-day surprise on us.  We've got a Hadoop-based bot called DemonBot.  I also want to follow up a little bit on that U.S. GAO report on the DoD lack of security, how distressingly insecure those systems are.  There's been some action - it's a bureaucracy, so good luck.  But still I guess nothing happens without some hope.  We're going to take a look at the consequences of publicly exposed Docker server APIs, which you just have to scratch your head.  It's like, wait, really?  You're saying that the Docker service API, there are public exposures?  Oh, yes.  And so of course hackers have jumped on those.



Also I've got some fun DDoS for Hire news.  We're going to look at the going rate, in terms of dollars per minute, of purchasing DDoS for Hire.  I'm sure none of our listeners would ever blast somebody off the Internet, but now we know exactly what it costs.  And there are six, no, sorry, nine different plans you can choose from, like the copper, the gold, the VIP executive plan.  Anyway, we've got all that.  We're also going to check out a midweek Windows non-security bug fix, which they normally drop a couple days after the second Tuesday of the month where those are all the security fixes.  Then they've got, well, we've also, you know, nonsecurity stuff, and lots of stuff there.  Well, large in terms of number, nothing really in terms of breathtaking.



Also Firefox just jumped up to v63 with some welcome changes that we have forecast previously.  There's an I don't think much to see here new privilege escalation vulnerability which affects Linux and OpenBSD.  We just have to talk about it so that it's covered, but it's not anything that should worry people too much.  Well, based on the profile of your system.  We've got a little bit of errata.  I want to talk a little bit about Peter Hamilton and the Salvation Sequence, just because what he's done - and there's no spoilers in this.  You know I don't do those.  But what he's developed is really fun, in a Peter Hamilton-ish way.  I got a little bit of closing-the-loop feedback from a listener.



And then we're going to wrap by taking a careful look at this interesting security architecture of a smartphone wanting to be used to purchase things from a vending machine.  Of course last week we talked about how in this particular instance the app was easily reverse engineered, and some white hack gave himself 999 euros, which was probably the maximum number that the app would accept, and it worked.  And of course he notified the vendor, who has taken some action.  And so I left it as a puzzler, as our listeners know, for what would be the solution to that?  So I think another great podcast for our listeners.



LEO:  Nice.  Should be fun, securing that vending machine.  Of course, listening to this show, if you get your employees to listen to the show every week, you'd probably be okay.  But most employees are not listening to Security Now!.



STEVE:  No.  So I fact, when I was catching up with my high school gang in this little informal reunion we had yesterday, I mentioned that I've been doing this podcast.  We were in the 13th year.  And one of them said, "So how long is it?"  And I said, oh - and in fact I think that one of them guessed, like, oh, about like 15 minutes?  I said, "Oh, no, it's two hours."  And they said, "Two hours?  Who's going to listen to that?"  And, like, "Nobody does a two-hour podcast."



LEO:  Oh, yes.



STEVE:  I said, "Well, in the beginning it was 20 minutes, and it's sort of gotten out of control."



LEO:  Yeah.



STEVE:  So anyway, we have a fun Picture of the Week.  It's been in my library of things to use for the podcast when nothing is particularly apropos for whatever topics we're talking about.  Anyway, this shows a robot, kind of a classic "robot" robot, android-y kind of robot, sitting in front of a keyboard and screen, looking back over its shoulder at a guy who's just apparently walked in the door holding a cup of coffee.  And the robot is saying to him, "Hey, man.  Would you tick this 'I'm not a robot' box for me?"  So anyway, yes.  Little reversal.



LEO:  Get it?



STEVE:  A little reversal.



LEO:  It is kind of timely.  Google has just updated their - you know they had that reCAPTCHA thing.  You see that everywhere now.



STEVE:  Yup.



LEO:  And they've just put out version 3 that is a little, they say, more effective.  I see it a lot now.



STEVE:  Well, it's nice, too, that where they're able, as we've talked about this before, sometimes all you have to do is just click the "I'm not a robot" button.



LEO:  Right.  Because they know so much about you.



STEVE:  Yes, exactly.  And they go, okay, yeah, well, we thought that, but we just wanted to make sure. It's like, okay, okay.



LEO:  And then there's the ones that I really hate.  Used to be, you know, numbers, letters, and all the things.  The new one, you've seen this, is the click...



STEVE:  Oh, the grid.



LEO:  Yeah.  You have nine images.  Click the ones that have a traffic light in it or a car in it.



STEVE:  Exactly.  Or show a pedestrian or something.



LEO:  And it's so hard.



STEVE:  I get them wrong often.



LEO:  All the time.



STEVE:  I'm not a robot.



LEO:  Because they get the top of the head.  Is that part?  Is that picture good?  Is there a pedestrian in that picture?  It's just a hair.



STEVE:  Yeah.  For a while we were doing those street numbers, like random photos of people's numbers on the side of the house.



LEO:  That was to help Street View because they were actually - those were - they couldn't always read those numbers in their Street View cars.



STEVE:  That's right.



LEO:  Yeah.  They're no fools.



STEVE:  Just abuse your humans.



LEO:  Yeah, that's really what it is.  We know you're not a robot, but could you help out here?



STEVE:  So at 5:27 p.m. last Tuesday afternoon, the day that we were finishing up, I mean, last week we did the podcast in the morning.



LEO:  Right.



STEVE:  In the afternoon Microsoft acknowledged that zip file problem in Windows 10.



LEO:  Oh, boy.



STEVE:  In a posting with a long title.  Believe it or not, this was in answers.microsoft.com.  The title of the posting was "If you copy files from a .zip file without extracting them, they might not be copied or moved correctly, even though it looks like they have been."



LEO:  That's a very long starting headline there.



STEVE:  End of quote, yes.  So they explain.  And it's useful for the exact what it is that they've done here.  They wrote:  "There is a known issue in the Windows 10 October Update where the consent prompt 'Do you want to replace these files'" - which is really handy, okay, that's me saying that, they didn't say that - "is missing when copying contents from a zip file.  With the Windows 10 October 2018 Update, if you copy or move files from a .zip file without first extracting the contents into a new destination folder" - and I guess it really wouldn't be new - "that contains duplicate filenames or is write-protected, you don't get a 'Do you want to replace these files' prompt.  It will appear that the files were overwritten, when in fact the copy action for those files is not executed, and files have not been overwritten."



So, okay, that's a little news over what we understood to be the problem last week.  There were reports on Reddit of overwrites.  It looks like it's just a - it's like it thought it gave us the confirmation, and we said, no, don't replace them.  But it just skipped that step.  So it looked like they were overwritten, but nothing happened.  So then they explained:  "This failure can occur in the following three scenarios:  copying from a compressed file to a regular folder, moving from a compressed folder to a regular folder, or copying from a compressed folder to a protected folder."



And then they said:  "Note:  While the copy action for the duplication file names does not complete and no files are overwritten, the 'move' command will also silently fail and might remove or delete the moved file."  That is, you're moving something, and as we know, moving means essentially copy and then delete from the source.  So in that instance there is a potential file loss problem where, again, it doesn't actually perform the copy, but it thinks it did, and so it removes the source of the file.



So anyway, they said:  "We recommend you fully extract the zip folder before you copy files to a new destination folder to avoid this issue."  They said:  "And if you deleted items at any point in this process, you can recover files that were not copied to the destination folder or were unintentionally recycled by doing the following," and then basically they explain about the recycle bin.  So apparently that's where things go.  Of course, I've never been a recycle bin advocate.  I always - one of the first things I do when I'm setting up a new system is I turn that all off because I just - I've got backups of my backups in any event.  So if I really lost something, I could get it back.  I just like it to go away.



But in any event, so that's the story.  And they said, and this is kind of curious, they said:  "Microsoft is working on a resolution and estimates a solution will be available in early November for this issue."  Okay, well, tomorrow's Halloween in the U.S., so the end of October.  So November starts on Thursday.  But as a consequence of that timing, we don't have our second Tuesday, the infamous - it is infamous now - Patch Tuesday until mid-November, essentially, November 13th.



And this is a potential data loss situation.  So it's unclear whether they might push an out-of-cycle fix for this, or if they're going to just say everybody hold your breath for two weeks, because two weeks from today is when this will be fixed, unless they decide, okay, this is something we have to deal with sooner.  So don't know yet.  And they're not saying.



I mentioned some good news on the Android updates front from Google.  And this is a consequence of The Verge that recently obtained some copies of confidential Google vendor contracts.  So this is not a formal announcement.  All we really had was back in May a statement of Google's intent.  Back then what The Verge coverage was, they said:  "Google says it will require Android phone manufacturers to roll out security patches on a 'regular' basis, though it isn't clear who that requirement will apply to or how rigorous the mandate will be."



Then The Verge says, and this is back then in May:  "On Wednesday, during a talk at Google's annual developer conference that was caught by 9to5Google via XDA Developers, the company announced that many more users would receive regular security patches, thanks to new agreements it's making with partners.  David Kleidermacher, Google's head of Android security, reportedly said:  'When you have billions of users, it's a large target.  [Uh-huh.]  And so it deserves the strongest possible defense.  We've also worked on building security patching into our OEM agreements.  Now this will really lead to a massive increase in the number of devices and users receiving regular security patches.'"



And then The Verge finished the coverage back then, saying:  "Unfortunately, there are no details beyond that.  We reached out to Google to learn how frequent the security updates will be and who they'll apply to," they wrote, "but the company didn't immediately have answers for us.  It sounds," they wrote, "like the requirement will apply only to new phones launching on Oreo or later that take advantage of Google Play services, so likely nothing in China.  Even then, it isn't clear whether it'll apply to all of Google's partners."



So that was then.  Now, today, with the benefit of these leaked confidential documents, The Verge has said:  "Every month, a security team at Google" - as we know, they're sort of giving us some background - "releases a new set of patches for Android.  And every month, carriers and manufacturers," they wrote, "struggle to get them installed on actual phones.  It's a complex, longstanding problem.  But confidential contracts obtained by The Verge show many manufacturers now have explicit obligations about keeping their phones updated written into their contract with Google."



LEO:  Hallelujah.



STEVE:  Yeah.  "A contract obtained by The Verge requires Android device makers to regularly install updates for any popular phone or tablet for at least two years."



LEO:  I wish it were longer, though.  That's...



STEVE:  I know.  I thought the same thing.  It's like, okay.  Because here we were just recently talking about old routers and how they've been abandoned, but they're still in use, and that's a problem.  So The Verge wrote:  "Google's contract with Android partners stipulates that they must provide 'at least four security updates' within one year of the phone's launch.  Security updates are mandated within the second year, as well, though without a specified minimum number of releases.  David Kleidermacher, Google's head of Android security, referred to these terms earlier this year" - and this was what I told you that they had said back in May - "said that Google had added a provision into its agreements with partners to roll out 'regular' security updates.  But it wasn't clear which devices those would apply to, how often those updates would come, or for how long."



The terms cover any device launched after January 31st, 2018, okay, so that's all of this year, after the first month of this year, that's been activated by more than 100,000 users.  Okay, so failed devices, I guess, aren't important enough, or small user bases, or small manufacturers maybe.  Then starting July 31st, so the middle of this year, the patching requirements were applied to 75% of a manufacturer's "security-mandatory models."  Then starting on January 31st, 2019, so again, one month into next year, Google will require that all security mandatory devices receive these updates.



The Verge wrote:  "Manufacturers have to patch flaws identified by Google within a specific timeframe.  By the end of each month, covered devices must be protected against all vulnerabilities identified more than 90 days ago.  That means that, even without an annual update minimum, this rolling window mandates that devices are regularly patched," or at least within 90 days, within three months.  "Additionally, devices must launch with this same level of bug fix coverage.  If manufacturers fail to keep their devices updated, Google says it could withhold approval of future phones, which could prevent them from being released."



So of course our listeners know we've spoken of this often.  And in fact, I mean, we've despaired at the severity of some serious problems that have surfaced on Android which Google's own devices are patched for, responsibly and almost immediately, I mean, as quickly as one can.  But the fact that all the other phones with the same level, the same version of Android, may never be patched, thus leaving huge numbers of users exposed to what then become publicly known vulnerabilities.  And, boy, if this podcast has shown us anything, it's that bad guys are now, I mean, this is like things that are patched are a source for some subset of hackers to jump on, reverse engineer what got changed, what was patched, and then turn that into an exploit and go after all the devices that have not yet received that same patch.



So that's great that we're seeing Google recognizing that their own Android ecosystem is suffering from the fact that they inherently and by design don't have the same kind of grip that Apple does over the iOS ecosystem; but that, frankly, for this kind of product, with the reality of today's vulnerabilities, a tighter grip is necessary in order to protect the user.  So anyway, props to The Verge for getting a hold of that contract.  And now we have some sense for how this actually looks in the real world.



And SandboxEscaper, Leo, she's just still not having a good time.



LEO:  Aw. 



STEVE:  Her tweet read:  "Here's a low-quality bug that is a pain to exploit, still unpatched.  I'm done with all this anyway.  Probably going to get into problems because of being broke now, but whatever."  And, you know, the problem is I took a good look at this because there's a proof of concept with all the source code up on GitHub.  It's DeleteBug1.rar under her GitHub account.  And it's a nice piece of work.  I mean, like I wouldn't hire her, but somebody ought to.  This person should be employed.  I mean, this takes some skills to find these sorts of things.  So, I mean, you'd have to make sure, I don't know, maybe have her talk to somebody about her apparent depression.  But boy, I mean, there is some skill there.



So just to recap, back in late August our listeners will remember when she was apparently having another bad day.  She posted to Twitter to expose and to detail a proof-of-concept exploit for a local privilege vulnerability in Microsoft Windows Task Scheduler, which was due to its handling of an advanced local procedure call, the ALPC service.  And as we know and covered, almost immediately after the proof of concept was released, even though it wasn't a remote execution-style vulnerability, I mean, not the end of the world, as we said at the time, even malware, I mean, this would even be useful for any malware that could get into your system in order to elevate its own privileges to do more damage.



And that is exactly what happened.  It wasn't long after this zero-day dropped that we found malware using exactly this in order to give itself privileges, system-level privileges, and deal with the fact that a user properly using a non-privileged account could still get malware installed into the system.  Which non-privileged users, it's the reason they don't have privilege is it's kind of pain not to, but it's supposed to prevent this.  So we know that it didn't take long for what many in the industry felt was an irresponsible disclosure of a zero-day.



So it's happened again.  What she's describing as a low-quality bug is something she found in a new DLL, something known as the Microsoft Data Sharing Service.  It's dssvc.dll.  It's a new service introduced in Windows 10.  So unlike the previous proof of concept, which went way back into Windows history, so it affected all versions of Windows, this one that she's just dropped now is Windows 10.  And both of the descendant servers, Server 2016 and 2019, are vulnerable to a misuse of what she has found and proven.  So it does not affect 7 and 8.1 or their server versions.



The service, this Microsoft Data Sharing Service, runs with LocalSystem, which is to say root or kernel, full kernel privileges, because it serves as a data brokering agent between applications.  That's what this does.  And so if you are able to abuse this, and she has figured out how, the abuser is essentially a process, this DLL running in the kernel.  So the proof-of-concept exploit code is deletebug.exe, which people can download.  Don't run it, though.  What it demonstrates...



LEO:  Okay.



STEVE:  Yeah, please don't run it.  She uses this to delete a critical system service, a pci.sys file, which you have to have to boot.  So it immediately renders a system unbootable.  So,  yes, again, don't try this at home.  But it convincingly demonstrates the fact that this is doing something it should not be able to do.  And unfortunately, all the source code is there.  Nothing is hidden.  And she just said, here you go, I'm having another bad day, so I just found this, and I'm letting the world know.



So it generated, as her actions have in the past, some feedback.  And I was sort of curious, so I read through the Twitter stream a little bit.  A Mitja Kolsek retweeted a response from Microsoft Security.  And he tweeted:  "Hey, girl.  In case you missed this," and then reiterated the tweet.  And he said:  "Please continue with your work in this area and get paid by Microsoft.  What you're finding are not," he said, "easy-to-find bugs.  And your proof of concepts and write-ups are of high quality."



LEO:  That's awesome.



STEVE:  And I completely agree.  I mean, it's top drawer.  It's really good work.  And then he said:  "There.  You should know this.  Cheers."  And Microsoft Security response was:  "Yes, this vulnerability is in scope for the Windows Insider Preview bounty program."  So...



LEO:  Get your money here.



STEVE:  Get some money.  And then I did get a kick out of, unfortunately, Peter Stevesant also - and he's tweeting from @binaryfraud2017 is his handle.  On October 26th he responded to both of those previous tweets.  And he said:  "Well, when you see how much Microsoft pay for exploit, you should sell them to" - and then he give the URL, and we've talked about them before, http://zerodium.com, which of course is the zero-day reseller.  And he says:  "They will consider your work much better."  And I guess he's meaning you will be better paid for that.  But in any event, dropping zero days like this, I mean, they're beautifully engineered.  And unfortunately what that means is they are immediately snatched up by any malware that wants to be able to bypass the whole Windows privilege system.  That's what this is doing.



So, yes, it doesn't immediately allow bad stuff to get in.  But, boy, I mean, if you click on something in email, like a phishing attack, like for example that you were just talking about from this previous sponsor, Leo, the bad stuff, you've given it a chance to run.  Well, the whole idea of having privileges on accounts is only so that most people don't have them.  And this just cuts through that.  And, yes, Microsoft will fix this immediately.  I'm sure we'll see this fixed two weeks from today on November 13th.  But there's two weeks between now and then, and then there's also the patch delay.  Because, boy, after October's disaster, I don't think anybody's going to be in a big hurry to fix this.



Oh, and I forgot to mention, those little micropatch folks at 0patch.com, they do have a fix for this already.  So if you're in an environment where you think you would be exposed to this kind of problem, you don't have to wait for Microsoft.  And I've talked about these guys before.  It's numeric 0patch.com.  And they're able to respond within hours of these sorts of disclosures to fix these - they call them "micropatches" to fix these little vulnerabilities until Microsoft performs the official update.  So one is available for this, too.



Okay.  So Apache Hadoop.  I just love saying it, "Hadoop."  It's just...



LEO:  So does Mary Jo Foley.  She loves saying it, too.



STEVE:  So it's a collection of open source software utilities which facilitate using a network of many computers to solve problems involving massive amounts of data, huge datasets, and computation.  And you get to say "Hadoop" in the process.  It provides a software framework for distributed software and processing of big data using the MapReduce programming model, whatever that is.  I've never had an occasion to dig into that.



LEO:  That's a Google big data engine.



STEVE:  Ah, okay.  And I'm just quoting from Wikipedia here.  "Originally designed for computer clusters built from commodity hardware, still the common use" - so that means you get to take like bunches of off-the-shelf stuff and just put it all together, and Hadoop somehow corrals it all and manages it with MapReduce, whatever that is.  And Wikipedia says:  "It is also found used on clusters" - and here is the problem - "of high-end hardware.  All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework."  Which is like a dream; right?  You just throw a whole bunch of stuff that's kind of limping, and Hadoop fixes it, works with what it has somehow.  And if something dies, it goes, oh, well, and works around it.



Okay.  So as a consequence of this Hadoop cluster nature, it turns out that Hadoop clusters - and I'm going to say "Hadoop" as many times as I can during this - Hadoop clusters are often very powerful, well connected, and placed into the Internet cloud, where unfortunately they are accessible either deliberately or by mistake.  So it turns out that Hadoop has something called "YARN," which is an abbreviation for Yet Another Resource Negotiator.  And it sounds like something Hadoop would need, right, because it's about resource negotiation.  And apparently there were some before.  This is yet another one.



So YARN.  It provides cluster resource management for enterprise Hadoop deployments, has had a known flaw in the handling of its REST API, which is an HTTP-based API, for which a Metasploit proof of concept was published after a 21-line, which is to say not very long or complicated or hard to understand, Python source proof of concept was posted onto GitHub - get this - seven months ago, last March.  I have a link to this exploit.py file, and it's just like, oh, you mean that works?  And it turns out, yes, unfortunately, seven months later it still works.  There's a Metasploit module which in its description it says:  "This module exploits an unauthenticated command execution vulnerability in Apache Hadoop through ResourceManager REST API."  And so I have a link there, too.



So now what we know is that, unfortunately, there exist on the Internet publicly exposed Hadoop clusters which have not been patched in seven months, since this was known and made public.  So Radware, the security guys, have reported that an entity that actually identifies itself as DemonBot, does not self-propagate in worm style, which is to say that infections do not themselves look for other vulnerable systems.  But they have identified a growing number of servers - they cited the number 70 servers - which use, and I love their term for this, they called it the "spray and pray" tactic to blindly find and infect millions of exposed publicly available and of course unpatched Hadoop instances.



They're using them to run cryptomining ware.  Oh, and also being used to source extremely powerful, thus DemonBot, extremely powerful and debilitating DDoS attacks which unfortunately these Hadoop instances are really well equipped to do because they are typically on high-end hardware and on very well connected to the Internet pipes, so large bandwidth, often on strong servers and able to launch high bandwidth attacks.  So very much as with the millions of compromised routers and other insecure publicly exposed and vulnerable services, attacks such as these will be part of the Internet terrain until we figure out how to build secure systems out of the box, if we ever do.



And it's funny, too, because again, as I was introducing my old friends from high school to the idea that I was in the 13th year of a podcast, they said, one of them said, "You haven't run out of stuff to talk about after 13 years?"  And I said, oh, no.  And in fact I think that may have been the dialogue that got us into, "And how long do you talk every week about this stuff?"  And I said two hours.  And they said, "What?"



LEO:  Somebody's doing the math right now, yeah.



STEVE:  So two weeks after that rather horrifying Government Accountability Office, the GAO report detailing glaring cybersecurity issues in weapons systems at the U.S. Department of Defense, the DoD has announced that it is expanding its existing Hack the Pentagon bug bounty program to include hardware assets.  And Leo, I really don't understand how that's going to work because how do you hack an F-35?



LEO:  Well, god, I hope you can't.  That's all I'm saying.  If you can, they'd like to know.  I think that's fair; right?



STEVE:  Wheel one of those over here, and let me plug my USB stick into it.  But somehow they're saying they're going to do this.  So they're going to be tapping the Synack, the HackerOne, and the Bugcrowd platforms, which are the existing bug reporting bug bounty organizing platforms, to attract more white hats to this effort.  And to me this feels like, okay, I'm not really convinced that this is going to work.  But they said, since the Hack the Pentagon program was kicked off in 2016, the bug hunters have found - and I wasn't aware of this, so that's just two years ago - 5,000 - which is a little distressing, too - 5,000 code vulnerabilities.



And six public-facing bounty challenges have been run, including the most recent, which was Hack the Marine Corps in August.  Other sessions have focused on the Air Force, Army, and the Defense Travel Service.  A three-year, 34 million - and this is what this new contract is, and this is bureaucratic jargon, they called it the "indefinite delivery, indefinite quantity" - contract package covering these three bug bounty managing companies will crowd source vetted hackers to probe the DoD's websites, hardware, and physical systems.  And again, it's like, okay, but, you know...



LEO:  I guess they're going to let them in.



STEVE:  An atom bomb?  That doesn't sound - so according to the contract's performance work statement, the government expects its military contractors to run at least eight limited-time challenges and five continuous challenges during the first year of the three-year contract, and more if an option in the contract is exercised.  Each program will last between three months to a year, and they might overlap.  And I was thinking about this, Leo.  It seems to me that virtually all of the interesting and critical systems are classified; right?



LEO:  Yeah, yeah.



STEVE:  And again, what I was wondering was - because we know that there's inter - I don't know if you'd call it interagency or interdepartmental or division.  We know that there's competition between the Army and the Navy and Marines and so forth.  So I wonder what would happen if, instead of involving external white hats, if you instead opened the systems up to interdivisional hackers.  So the Marine hackers could try to get into the Army's systems and vice versa, if that might be something that would be feasible because, again, I just - it's hard for me to believe that I can sign up to Bugcrowd and be vetted and be given access to hardware which is, you know, "Here, Steve, hack this system.  See if you can get in."  It's like, really?



So, okay.  Anyway, it'll be interesting to see how this pans out.  To me it seems like it would make sense if you kept it within - because, well, the reason I was thinking about this is that we know that for hacking you really have to have an adversarial posture.  You can't have programmers check their own code.  You've got to have programmers who are unfamiliar with the system and who are truly motivated to get into somebody else's system trying to do that in order to find this class of problem.  And so it just seemed to me, if you had a situation where you could set up a true competition, but also sort of keeping it in the family, that is, or within people who have security clearances, then maybe you could get the same effect, but also be able to pull it off.  Because I'll be surprised if people without high-end security clearance really ever get to touch high-end hardware.  So be interesting to see.



And I was a little surprised, Leo, when I heard that Docker Engine APIs were exposed.  And the good news is not a lot of them.  So as we know, Docker creates sort of a universal packaging approach that bundles all of an application's dependency into a container so that you don't - so to solve the whole dependency hell problem you create a Docker container that is just sort of a drop-in solution.  You just put it into an environment that runs containers, and it will run because it brings everything it needs with it.  So these Docker engines are available for Linux on the CentOS, Debian, Fedora, Oracle Linux, Red Hat, SUSE, and Ubuntu.  And also Windows Server has a dockerd service, a Docker daemon that's able to also do runtime Dockers.



So it turns out that, believe it or not, and our regular listeners will believe it, this Docker Engine is a server which interacts with its local environment via an API.  That API exists exposed in some cases on the public Internet.  Or as Trend Micro put it in their security advisory:  "Misconfigured Container Abused to Deliver Cryptocurrency-mining Malware."



They wrote:  "We recently observed cases of abuse of systems running misconfigured Docker Engine-Community" - that is, the community edition, there is a community and an enterprise sort of classes of these - "with Docker Application Program Interface" - that is, API - "ports exposed.  We also noticed that the malicious activities," they wrote, "were focused on scanning for open ports 2375 over TCP and 2376 over TCP, which are used by the Docker Engine daemon, dockerd.  The intrusion attempts to deploy a cryptocurrency-mining malware," which their software, Trend Micro, found as Coinminer on the misconfigured systems.



So this Docker API allows remote users to control Docker images like a local Docker client does.  "Opening the API port for external access," they wrote, "is not recommended" - yeah, no kidding - "as it can allow attackers to abuse this misconfiguration for malicious activities."



Now, unlike routers, where by policy these services are open and vulnerable, which we've been covering really pretty much this year, the Docker Engine itself isn't compromised or abused, and the Docker Enterprise platform is not affected.  The Docker Enterprise platform has very strong authentication that cannot be bypassed.  But Trend Micro wrote that they found "rare instances of abuse" on the Docker Community version.  I mean, and so people have to really work, it turns out, to make this exposure public.  It's not clear whether they have to explicitly open ports to the public, or if they install a Docker Community instance on a public server, if just essentially sheer negligence ends up exposing these systems.



The good news is there are not hundreds of thousands of these instances.  There are hundreds of them.  As in, like, a low number of hundreds.  Trend Micro reported a peak of, looks like from their graph that I have in the show notes, maybe about 425.  It looks like it peaked around early this month, early October, and then it sort of jumped up and down based on their scans.  So anyway, I was just sort of surprised that there were any instances of this.  It does look like, though, since Docker is very popular, and it's certainly in use in hundreds of thousands of systems, it looks like only in a few hundred because this malware is scanning aggressively.  They're seeing a scan rate of 50,000 packets per second, looking for open ports 2375 and 2376.



So it is attempting to move laterally.  If it gets into a network, it scans all networks it's accessible to.  So the problem, of course, is that it'll have, if Docker is there in an enterprise where there is a publicly exposed instance, once that gets infected, it is acting as a worm which is turning around and scanning into the Intranet, looking for additional instances where it may be able to jump.  So they've only seen - it peaked around 425 instances.  But who knows what those instances may have found when they pivoted and then looked inward into organizations that may be using Docker pervasively.



So again, the good news is, unlike routers, the Docker designers clearly understand the importance of not letting this API be exposed, and by default it appears that it's not.  But even then you just can't keep people from misconfiguring systems.  And Leo, you are going to love this next piece.



LEO:  Okay.



STEVE:  What is the going rate for DDoS for Hire?  As a result of Fortinet's threat research, we now know in at least one instance they have screenshots of the front end of a DDoS for Hire service powered by what's known as the Bushido Botnet.  And I have to get a little close to the screen here to read this.  In fact, looks like in the compressed version I can't.  But I know that there was the Bronze, the Silver, and the Gold.  For $20 - oh, yeah.  I have it down below.



The $20 for the Bronze plan, you were able to purchase that, and you got 900 seconds of DDoS, which is 15 minutes.  And the Bronze plan gets you one concurrent attack using what they call their "standard" network.  Then this goes all the way up.  You have the Bronze, the Silver, and the Gold.  But then you can also go from the Standard to the VIP.  So you also have Bronze, Silver, and Gold in the VIP package.  So the highest end plan is the $150 VIP Gold plan.  So again, you have three tiers and three metals, Bronze, Silver, and Gold.  So a total of nine different plans ranging from $20 to $150.



The $150 VIP Gold plan buys you 7200 seconds, which is two hours, offering two concurrent attacks using their VIP network, whatever that is.  And then through the browser UI you're able to select which types of plan you want.  And two pages down in the show notes, just because I thought this was really fun, is the current gigabits per second offered by their DDoS attack tools.  And when this screenshot was taken, it was 424 Gbps.



LEO:  It says "gigabytes."  Capital B.  Isn't that a byte?



STEVE:  Yeah, I think it's probably gigabits.



LEO:  That's an error, yeah.  That may be right.



STEVE:  Yeah, I think it's bits because that's normally what we think of in terms of network speed is in bits.  And but still, that's almost half a terabit per second of flooding bandwidth.  So, I mean, that's goodbye target.  I mean, that's meltdown.  And at the time, again, that this was taken, this site is the 0x-booter site.  And of course these are all couched, not as, oh, we're not selling you attack.  This is to, like, stress test your network.  Okay.



LEO:  Oh, yeah, sure.



STEVE:  424Gb is not a stress test.  It's the center of the sun.  So at the time that this page, this web interface, the snapshot was taken, they had 16,993 bots present in their network waiting for marching orders.  Where would you like us to aim the attack? 



LEO:  Wow.



STEVE:  So Fortinet writes:  "Distributed Denial of Service offerings, often disguised," they said, "as legitimate booter or stressor services," they said, "continue to increase in the cyber underground market.  This relatively new 'crime as a service' trend has created an entry point for novice DDoS attackers offering a simple option to anonymously attack nearly any website, forcing it offline for a small fee."  They wrote:  "Due to the public release of the source code of some popular bots" - and of course Mirai we've spoken of is among them - "building a botnet to provide these services is now simpler than ever.  A quick Google search returns lists of resources for botnet builders, usually with complete step-by-step instructions."  Because why not?



"Being able to reuse and even modify the source code has enabled cybercriminals to create their own versions that implement new functionalities."  Oh, and "Fortinet will be delivering a talk during the upcoming Botconf 2018 being held this coming December 4-8 in Toulouse, France.  During this talk, Fortinet will be detailing the reuse of Mirai source code and the effect it has had upon the development of other botnets in their presentation titled 'Mirai: Beyond the Aftermath.'"



So anyway, they wrote:  "During our regular monitoring, the FortiGuard Labs team recently discovered a new platform" - and this is the one we've been talking about - "offering DDoS for Hire services called 0x-booter.  First appearing on October 17th" - so only two weeks ago, October 17th - "0x-booter is available to anyone who signs up on the website.  This service comes with a simple-to-use interface which enables practically anyone to learn and use the service.  Initiating a DDoS attack is made through a web user interface which eliminates the need for direct contact between the user and the botmaster.  In the attack hub interface, the details of the host or domain, the port, the attack duration, and the type of attack can all be selected before launch."



So, yes, now it's just a matter of going to a website and saying, "I'm annoyed with these people.  Zap them for me, please."  And if you are willing to fork over some cash, ranging $25 for a couple minutes, to sort of evidence your displeasure with someone, more for a couple hours, you can now do that in DDoS for Hire.  The world we live in today.  Wow.



Middle of last week, Microsoft dropped their post-Patch Tuesday, they normally do it a few days later, like on Thursday, bug fix, which are not the security updates, but just fixing their grab bag of stuff.  And this was an opportunity for me to verify that this system that I've been writing about, or rather that I've mentioned, which was Windows 10 Home, which I updated to Pro in order to set a delay.  I went back when I was putting this together on Sunday and checked for anything new.  Got any updates for me?  Nothing.  So I thought, oh, that's interesting.



So I set the delay down to zero and asked again.  And sure enough, I got KB4462933, which was available, but had been held off because that's what I had asked for.  So I knew it was there, so I was able to verify in fact.  And the list of what it fixes is really, I mean, it's really extensive.  But to give you a sense for it, it addresses the redenomination of local currency that the Central Bank of Venezuela implemented to enter the Bolivar Soberano into circulation.  Because of course you'd want that.  Addresses an issue that prevents the clock and date flyout from appearing when the region format is Spanish and the sorting method is traditional.  Because not having that flyout would be a problem.



Addresses an issue that causes the GetCalendarInfo function to return a wrong value for the Japanese era.  Addresses an issue in which applications have handle leaks when using client authentication certificates with the TLS protocol.  This issue occurs when the FreeCredentialsHandle call occurs before the DeleteSecurityContext call in the application code.  And things like that.  Addresses an issue in which Scheduled Tasks configured to run on a specific day of the week don't execute at the expected time, and so on.



Anyway, so, you know, there's things, I mean, and that's like a tenth of all of the stuff that it fixes.  So just a grab bag of things for Windows 10 that they had on their list of things to fix that they did not put into the security update because these are arguably not security-sensitive things.  Just fixing little cruft around the edges of Windows 10.  So I've heard no reports.  That was released on Thursday.  No reports of it causing unexpected problems.  I haven't had any, and I did immediately reset my hold off day counters back to, I think it's two weeks for security and a couple months for the feature update because I definitely want to wait till everybody else has gotten any arrows in their back, if they're going to, before I hurt this system that I get to talk to our Security Now! listeners over every week.  So anyway, that.



Firefox has made its move to v63, which now brings the built-in tracking protection that we were foretelling of.  In their update they said it shouldn't be hard to own your life online.  So they have now built-in tracking controls.  And after you update, and you go to a site which is attempting to track you, up comes this really nice multipage tutorial.  It's interesting because I have - I can't think of it.  I'm blanking on it.  Oh, uBlock Origin.  I have uBlock Origin installed.  And so I was somewhere, and I wasn't seeing anything that happened.  And so I turned off uBlock Origin, and suddenly Firefox jumped in and said, oh, look, there's something to do here.  So uBlock Origin sort of was in there preemptively.  When I turned it off, then Firefox said, oh.



And I got this whole, like, look at this little shield that appears in your URL bar.  And we've got these little, I mean, I got a whole little tutorial.  So Firefox users who update will be told about all this.  However, it is still not enabled by default.  I had gone in and manually enabled it in the earlier build, before they surfaced it out on the UI.  So what v63 does is it brings it out to the UI as "enhanced tracking protection," they call it.  So if you go under Options, you know, go into Menu, choose Options, then select the Privacy and Security tab on the left-hand side, under Content Blocking, at the section at the top is Content Blocking.  There's two checkboxes, which is all detected trackers and also third-party cookies, probably both of which you want to enable.  And this is very fine-grained.



So you should be aware if you turn these things on because they're being careful.  It's there now.  It's in the UI.  But it still defaults to off.  So I think our listeners who are Firefox users should probably flip that on.  But be aware that it might upset some sites that could be dependent upon third-party activities, some forms of tracking, in order to function.  The good news is you have highly granular control to turn that off.  And also - I poked in a little bit and dug around.  Under this tracking protection, where you customize it, there is an option, and I showed a picture of the screen that I got under Block Lists.  And you can currently choose.



They're using the Disconnect.me block list, which is highly regarded.  And so there's two.  There's basic protection and strict protection.  They're recommending basic.  And the description says "Allows some trackers so websites function properly."  In other words, Disconnect.me has found that, if they use absolute strict blocking, there are some known problems with that.  But again, a user who really, if that's what you want, you can choose the non-default strict protection by clicking that link, and then select Disconnect.me strict protection.



But then sort of be aware that, if something doesn't seem to be working, again, you are able to still do per-site overrides in order to allow it when it's necessary.  And as we know, we've seen some benchmarks where blocking content really does produce a startling increase in speed because so many pages have become now so heavy with just stuff that they're dragging along.  So I know that we've talked already about turning on content blocking and how much faster it makes things.  So Firefox is moving forward.



And Leo, I've heard you on other podcasts with your guests talking about sort of this growing distinction that we're sort of presuming will exist as Firefox increasingly profiles itself as the anti-tracking, privacy-focused browser; whereas Google just can't with Chrome.



LEO:  They can't, no.



STEVE:  Because that's their lifeblood.  I mean, it's what's paying for all the free services that we get from Google.  And as you say, yeah, it's their business.  So it hasn't stopped people from making Chrome the number one browser on the 'Net, but it's nice to see that Mozilla and Firefox are finding a niche for themselves because I still like the way they handle tabs.  I imagine I would switch to Chrome if they ever got tabs right, but that doesn't seem to be something that they're focused on at this point.  Oh, and I had it in the show notes, also.  If you put in the URL about:preferences#privacy, that takes you directly to that page.



Oh, and I did mention - I don't think this is worth worrying much about, but it did get some coverage around the Internet, a local privilege escalation vulnerability in Linux and FreeBSD.  It's been fixed.  So watch for updates from your platform's packager.  An Indian security researcher discovered a flaw in the X.Org X Windows Server package which impacts OpenBSD and most Linux distributions, including Debian, Ubuntu, CentOS, Red Hat, and Fedora.  Exploitation of the flaw would allow a lower privileged user to create or override a file or files anywhere on a system, including files owned by privileged users.  And in this example the etc/shadow file for passwords was used as an example.  However, the attacker would require a console session to exploit the issue.  But SSH could be used to get a console session remotely.



So I guess the concern would be probably not an individual user at home.  But if you had a system where you were depending upon the system privileges preventing unprivileged users from getting to sensitive areas of a system, where an unprivileged user could arrange to bring up a console, this could be a problem.  What was found was that the log file option on the X Windows Server config was vulnerable to a format string vulnerability which could be abused by a local attacker.  X.Org immediately fixed the problem and issued updates.



So it doesn't feel like it's a super critical problem for, as I said, for the typical end-user at home, but maybe in an enterprise scenario it would be something that people would want to get onto immediately.  And again, it's fixed.  Apparently the server exists and is invokable, even if it's not in active use throughout those distributions.  So it's there and vulnerable to exploitation and so worth removing it if you don't need it.  Or if you do, then updating it so that you're not in trouble.



Also, just a little bit of errata, two pieces.  An anonymous listener in, wow, Cheektowaga, New York?  Is that how you pronounce it?



LEO:  Yeah, Cheektowaga.



STEVE:  Cheektowaga.  Cheektowaga.  Never said that word before, Cheektowaga, New York.  And apparently I just misspoke.  He said:  "As you said it, slash dot dot doesn't do anything special."  And, yeah, that would be true.  He says:  "Slash would be the root directory, and the parent of the root is the root because it really has no parent."  So, he says:  "You probably meant to say dot dot slash," which is definitely what I meant to say.  So I guess I misspoke.  So I meant to say dot dot slash because, as we know, the dot dot backs you up a level in the directory hierarchy.  So dot dot slash, dot dot slash, dot dot slash moves you successively back up toward the root in the tree.  And I guess I said "slash dot dot" by mistake.  So thank you for that correction.



And a number of people also noted that in my discussion of Real-Time Operating Systems, RTOSes, remember FreeRTOS was the one that I was talking about, I guess it was last week, one of the things I failed to mention was that real-time means real-time.  That is, it's often one of the other characteristics of it, and I failed to talk about it at all, which led some people to say, "Steve, what you really were describing was an embedded OS; wasn't it?"  And so they certainly have a good point.



One of the other things you get in what is a real-time operating system is real-time guarantees of response.  For example, there's no such thing in Windows.  It's like, it's sort of best effort.  If things aren't working right then, or if another process is hogging all the processor, for example mining, if you have a miner, a coin miner running on your system that is misbehaving, the rest of your system is really sluggish because it's sucking up all of your processor resources.  In a real-time OS, one of the things that microkernel, that minimal set of services guarantees is guaranteed response.  Meaning, for example, that you often have hardware interrupts that need real-time service.



And so to do real-time audio playback or whatever, like running a robot arm, so you want to involve the processor in the servo control loop of a robot arm, so what it needs is you absolutely have to provide a guarantee that within X number of microseconds or milliseconds, you guarantee that the interrupt service routine will get run in order to perform the actions that you're requiring in real time.  So that's another set of characteristics unique to a real-time operating system, separate from an embedded OS which may not necessary provide those, and certainly any of our consumer OSes where it's just like, well, gee, things seem slow.  I guess I'd better get a faster machine or give it more memory or something.



And also, Leo, I wanted to take a moment, since I thought we would have time, and we do, to talk a little bit about "Salvation," which during my trip I took an iPad Mini with me, which is now my favorite reading device.  And by the way, for anyone who doesn't know, the Kindle for iPad got the feature I most love on Kindle, which is the ability to smooth scroll.  It's why I use iAnnotate, and they also have an iRead, I guess maybe it's iReader.  That's my PDF reader of choice because it doesn't page the PDFs.  It just lets me smoothly scroll through them, which I really prefer.  And until a recent update, the Kindle app on iOS, well, for the pads, or for the phone, was still a page at a time.  They added the long-sought feature of smooth scrolling, which I really appreciate.



Anyway, Leo, you went through the Commonwealth books, and there have been several of them.  First was "Pandora's Star," and then "Judas Unchained," which was a fabulous pair of books, where we learned about the Commonwealth.  That was Hamilton's reality where what we learned was that, in this Commonwealth, the Commonwealth was basically built on wormhole technology.  Some of the main characters had figured out how to create a portal which could be used to link planets.  And so cleverly, what I like about Hamilton, and I'm now seeing this pattern, is he'll take a fundamental innovation and build a fully realized reality around what if that was true?



And so there were cool things, cool consequences of what if you actually had wormholes that were expensive to maintain because they had to use to vast amounts of energy, and huge complexes had to be built around them in order to maintain this spatial energy warp of some kind, to zero the distance between distant places, the presumption being this is a lot of effort to do, so we're not going to have many of them, and so they're expensive.  So what you do is you bring back trains.  And that's what he had.  He ran train tracks through these wormholes, and so that's the way you efficiently used this expensive yet cool transit system, the idea being that you'd have switchyards on both sides where you'd be staging both passengers and goods, and thus keep these very expensive to create and maintain wormholes busy, by using trains.



Okay.  So that was the Commonwealth.  In this new, and I guess it would be called the "Salvation reality," the similar sort of innovation are quantum entanglement portals.  And so they're like portals which are, unlike wormholes, lots of, like, I mean, you need installations.  You need sort of like mountain-size installations with lots of power and reactors around them and so forth.  These things, these quantum entanglement portals, they're almost free.  They're cheap.  So imagine you have this portal of varying diameters, which is inexpensive.  And you create it, and then you pull them apart.



And so they use quantum entanglement, I mean, we know what that is in theory.  So his extension is, okay, they're just doorways that you're able to separate in physical space, yet it's still a portal that has zero space if you go through it.  So of course they're used on a planetary scale.  But it turns out what he realizes, okay, if you actually had these, we would use them for all kinds of things.  So, for example, in this future, virtually all transportation has gone away.  It's just on foot now, and you have hubs of portals that link you to other hubs after you just cross through a hub room.  And so after crossing through three or four interlinked hub rooms, you're pretty much wherever you want to be.



And the very wealthy have homes where the home is a hub room, and the different rooms of the home are on different areas of the Earth or on different planets.  Like the kids' room is on the moon, and it's a big crystal dome where the kids have their toys, and they're looking up at the Earth.  And when you walk through the door, you're now back on Earth.  Anyway, the one thing that I thought was so clever about him is imagine you  want to bring a large portal somewhere, that is, by foot; but it's a large portal, and so you can't.



Well, again, being so clever, he has this concept of threading where in a backpack or something you can carry, you bring a small portal which is quantum entangled to a portal back wherever, at the headquarters.  And once you get to where you want a big portal to be, you engage a device which pushes a very wide and very narrow portal through the small portal.  Now you have an elongated, a very long and narrow portal, had to be narrow to be able to come through the small diameter portal.  So now essentially you have a portal that's a long slot.  And so the next phase is something, some equipment, a threading, a piece of threading equipment at the other end, now pushes a full-size portal through the slot.  And now you have a large-diameter working portal wherever it is you went.



So again, that gives away none of the plot, but it's just some of the cool technology that he has created in this very richly realized reality.  And props to him.  I'm having a lot of fun with this book, which I had a lot of time to read during my recent trip to visit my high school friends.



And Sunday, as I was pulling all this together, I found a neat note from actually somebody in your town, Leo, in Petaluma, Ben Willerson.  The subject was "It really does fix SSDs!  Thank god!"  And I realized he probably meant that maybe literally.  So on the 23rd he wrote:  "Dear Steve and Leo," he says, "everyone says they love the podcast. I haven't said it before, so add me to that list.  My daily commute is hell,"  he says.  "Since Leo is in Petaluma, he knows.  Although you guys don't fix that, you make it more tolerable."



He said:  "My SpinRite story will not be as important to you as it is to me.  It could not be.  But I hope you find this since others need to know what happened.  Several years ago I switched to an SSD to get more space, but also since I figured a solid-state storage drive would be failure proof.  What could go wrong?  It cannot crash.  There's nothing but reliable solid-state memory inside.  So I was less careful about keeping backups.  I did more at first, somewhat.



"But everything was working until last Monday morning when I turned the PC on after the weekend, and it said 'Missing operating system.'"  He said:  "I had done a huge amount of work using that computer, and all that data was only there since I had stopped backing up."  So I see.  So he'd done a huge amount of work that was only on that computer because he hadn't backed up in a long time.



He said:  "The first thing I tried was moving the SSD to a different computer to see if I could at least get back the data.  But the other computer wanted to format the SSD, and it was not accessible at all.  My next thought was SpinRite.  Like you have said, it can be used for maintenance to keep this from ever happening in the first place.  But it has also recovered other people's SSDs."  And he said:  "I was thinking about that security camera crashing story," which we talked about a couple weeks ago.



He said:  "The subject of this email gives away the ending.  I bought SpinRite from your website, and it was running on that SSD within a few minutes.  I don't know whether you believe in the power of prayer, but I do."  He says:  "I write this letter because of the relief I felt when the entire drive reappeared, with nothing lost, after SpinRite finished with it.  Bless you and bless SpinRite.  Thank you."  And Ben, thank you.  Thank you for sharing your story with us.



LEO:  Nice.



STEVE:  And one little bit of closing the loop from a listener, and then we'll take our final break, and we'll talk about securing vending machines.  And this was a good point.  Andrew Cooper in Sydney, Australia said - his subject was "Inside the Intranet should not be considered safe."  And he said:  "Steve, first let me say that I've been listening to Security Now! almost since it began.  I really enjoy the way you cover topics from the high level right down to the technical nitty-gritty.



"However, something's been niggling at me for a while.  There have been many times when you've been discussing a vulnerability in some protocol or service that you've implied, or explicitly said, that as long as it's not exposed to the public Internet, then it's okay.  I heard it again today in your discussion on the LIVE555 server on SN-686.  This may have been true in the past, but it hasn't been for a while.



"For many years now, the larger security-conscious organizations have known that it's not safe to trust the Intranet.  It started over 10 years ago with talk of APTs (Advanced Persistent Threats) and the 'disappearing perimeter,' and today has developed into a mindset that informs all security discussions and decisions.  Microsoft's phrase is 'assume breach.'



"While this mindset is especially true for medium and large organizations, it should also be a consideration for smaller networks.  Even if your employees or family members are 100% trustworthy, a well-executed social engineering attack" - and again, Leo, what you were talking about at the beginning of the podcast with this new sponsor - "could result in some code running on a PC, or even a phone, that can then use a vulnerability in an internal service to dig its way deeper into the network and pivot to other targets."  And of course we know that was what the Sony attack - exactly was the profile of what bit Sony.



"Of course, this mindset needs to also consider the value of potential targets and the likelihood of attack.  It's probably overkill for most home networks and even some small offices; but I don't think it's helpful to encourage the thinking that if it's" - he wrote "being," but he meant "behind" - "the firewall, it's okay.  That said, thank you for all the hard work you put into the podcast every week.  Keep it going.  Regards, Andrew."



And point taken.  And so I wanted to share that with our listeners because certainly - and I did mention that earlier in this podcast, that for example in the Linux privilege escalation attack, if you had somebody you didn't trust on the Intranet, and you had a Linux machine where you were depending upon its security boundaries, that could potentially create a breach.  So I will be more careful about that in the future.  Thank you, Andrew.  And a good note for our listeners, as well. 



LEO:  Indeed.  Okay, Steve.  Let's hear it.  I've got to know.



STEVE:  Okay.  So first, again, the idea being the minimum solution.



LEO:  Yeah.



STEVE:  So first what looks good, but doesn't work.  So imagine you designed a system with no WAN communication required for spending money from the smartphone.  That sort of seems like what we were talking about.  Remember, basically there was an app on which you could have - where the app could hold the balance.  And apparently it was a negotiation just between you and the vending machine.  You selected what you wanted, and the vending machine communicated over Bluetooth or near field and essentially deducted the money from the wallet that the app contained and said, "Enjoy your candy bar."  And of course the app was reverse engineered.  The guy was able to give himself 999 euros.  And so, whoops, this wasn't very secure.



So fixing that problem is not hard.  And so what we saw was apparently a system where the phone wasn't connected to the Internet, the vending machine wasn't connected to the Internet, and that the sole communication link was between the phone and the vending machine.  The problem was the wallet was not secured.  So securing the wallet is trivial.  We have well-established, simple, and inexpensive hardware which can protect a secret key from being disclosed, the so-called TPM, right, the Trusted Platform Module, which is a version of an HSM, a Hardware Security Module.



And we didn't talk about what is the attack surface.  Do we want to protect the system from the vending machine itself being physically attacked?  And you could argue that, well, if you're going to crack open the vending machine, you can take the candy bar, so you don't have to buy it.  But a larger attack would be to steal some cryptographic secrets which would then allow you essentially to have the equivalent of attacking all the vending machines that shared that secret.  But it's just not difficult to protect a secret.  Our iPhones do it.  Our motherboards that have a TPM chip do it.  And it's no longer expensive.  Those chips are on motherboards, and they're available for pennies.



So let's assume we can keep a secret in the vending machine.  That is, proof against attack.  Well, that means that all of the vending machines could securely share that one secret, and that that one secret doesn't need fancy public key crypto.  It could be just a symmetric key.  It's just a secret key which is either used to encrypt the wallet or to sign the wallet.  But doing either of those things completely prevents the wallet from being tampered with.  That is, it would completely defeat this attack where any reverse engineering of the app could in any way alter the wallet so that the alteration was not detected.



If it was encrypted, the wallet would just be an opaque blob.  It would just look like static that you couldn't do anything with.  If it was signed, then you could see the contents, but you couldn't touch it.  You couldn't alter them, or that would break the signature.  And the point being that when the wallet was handed over Bluetooth or near field to the vending machine, it would just reject it.  It would say sorry.  It would use its secret key to check the signature and say there's something wrong with the wallet.  Go deal with headquarters in order to get your wallet repaired, and then come back if you want your candy bar.



So that problem, that is, this tampering with the wallet problem, is easy to fix.  But it doesn't address the actual security.  There's still a flaw here.  And that is, in this model, even if you've made the wallet tamper-proof, you haven't solved what is known as or could be known as the "double-spending problem" because nothing would prevent an attacker from making a copy of the wallet before they buy the candy bar.  So they take the wallet and make a copy of it; right?  Then they buy the candy bar.  The vending machine deducts the cost of the candy bar, gives them back a re-signed wallet with a lower balance, and they simply replace the wallet that has now less money in it with the copy that they made before they bought the candy bar.  They've essentially reset the wallet's balance, and the whole vending machine system would have no way of knowing that had happened.  So you just buy another candy bar and do that forever.



So while we address the first issue of wallet tampering, we haven't really solved the problem of creating a secure vending machine system.  The existence of the double-spending problem demonstrates that there is no means by which the user's balance can only be stored in the app, in this case in the smartphone.  That is, you absolutely have to somehow involve another party.  There's just no way to prevent double spending if you can copy a wallet that has money in it and reset it after you've taken the money out.



So if every vending machine did have a connection to the Internet - and again, we don't know that that's the case.  We're assuming it's not because, if every vending machine was on the Internet, then this sort of double spending could be trivially prevented by having the vending machine essentially reach around the user to contact headquarters, and the user's balance would actually be maintained by the vending machine operator headquarters.  And so, again, then any kind of fraud of this sort could be prevented.



Essentially, the app would then be reduced to the role of identifying the user whose account and balance is maintained somewhere where the vending machine is able to access it and deduct the amount from the wallet there.  But we're assuming this is not the case, that is, that the vending machines are being kept simple, and they're not connected to the Internet, which is why they were so easily hackable by this first-generation attempt at an app.



So let's assume the vending machines do not have a connection to the Internet.  All they have is a connection to the user who's standing in front of them with a nearby connection, with a Bluetooth or near field connection.  If that's the case, we still need a third party.  Not the vending machine, not the user, but we need to get in contact with headquarters.  And since we cannot trust the app, we have to use the app as an intermediary.  It's the link between the vending machine and headquarters.



So now we have a system that once again works securely.  The user says, hey, I want a candy bar.  So the vending machine prepares a transaction which it signs with its secure key.  It's got to have that because there has to be something that the app doesn't know, and that's the secret stored in secure hardware, in a Trusted Platform Module chip, for a few cents these days.  So the transaction description is signed by the vending machine's network secret.  I mean, it could be the same secret in all the vending machines.  It doesn't have to be anything fancy.  There's no need for a per-vending machine secret.  It could have one if it wanted, but unnecessary.  We're wanting to keep this simple.



So it signed that with its secret and uses the Bluetooth or near field link to give it to the phone and say, if you want the candy bar, you've got to check in with headquarters and give it this signed transaction.  Again, the app can't mess with it, can't fuss with it, because it's a cryptographic signature.  It could see it if it wasn't encrypted.  It could be encrypted if they wanted the additional security.  And when I was thinking this through, I was thinking, and you know, it's a vending machine; right?  It's got limited quantities of candy bars.



So if it's going to be sending a packet back to headquarters, it ought to send its current inventory along with it.  So that way headquarters can keep track of how many candy bars are in the machine and send someone out to refill the vending machine if it's getting dangerously low.  But anyway, extra feature.  So the phone has this transaction.  Oh, I forgot one part, very important.  The vending machine also has a nonce.  It generates a - it doesn't even have to be encrypted, really.  It could just be a counter.  Doesn't really matter because it doesn't have to be secret.  But it absolutely has to have a nonce - that is, as we know, that's short for "number used once" -  a token which it embeds in this transaction, and that's to prevent replay.



Because what happens is this transaction is going to go to headquarters, along with the user's ID who wants the candy bar.  And so it uses the smartphone link.  We know the smartphone is on the Internet.  Vending machine doesn't have to be.  User's smartphone is certainly on the Internet.  So it sends it to headquarters, who looks up the user, checks their balance, looks into the transaction, says, oh, candy bar, 25 cents.  Deducts that from the balance, assuming that there is a balance there.  Authenticates the transaction and re-signs it with, again, this shared secret that headquarters and all the vending machines have.  So this authenticated, approved, and re-signed transaction comes back to the phone.



Of course, the user just - this all happens instantaneously from the user's perspective.  The phone now uses its Bluetooth or near field link to send the approved transaction back to the vending machine that checks the signature.  The approval could only have been created by somebody that had the secret also.  And it could be a different secret, if you wanted to have multiple secrets.  But again, no need, keeping it simple.  Checks with its secret, verifies it, and then turns on the motor that rotates the screw and drops the candy bar out for the purchaser to pick it up.



At that time, the nonce which had been issued on the transaction which was pending is removed from an outstanding purchases list, probably only one at a time, but it might have two or three people buying things, I guess.  So you might have multiple outstanding transactions, but you probably don't need them.  But the point being that, by having a nonce, which it has now deleted from its outstanding nonces list at the time that it accepted the approved transaction, the app is no longer able to resubmit an approved transaction because the nonce no longer will be accepted by the vending machine because it already issued a candy bar from an approved transaction with a nonce.



So anyway, a nice little fun problem in crypto protocols and engineering, where we engineer a system where the vending machine's cost is kept low because all it has is a very inexpensive secret key, unhackable because it's in its hardware.  It does not need to have the additional complexity and cost of its own connection to the Internet, yet it's able to interact with a nearby user's phone which does have an existing connection, transact through an app where the app is reduced to an intermediary that has no ability to alter anything happening.  It can see it, if it wants to, or it could be encrypted if there's some benefit.



And as a little extra feature we threw in the ability for the vending machine to continuously inform headquarters of how much stuff it has in its physical goods inventory so that headquarters can dispatch someone out to refill the machine if it's getting dangerously low.  And so our listeners can compare the system that I proposed with their own solutions and see if they had a better idea.  I can't think of anything better than that.



LEO:  So a couple of people did get that, though, figured it out.



STEVE:  Cool.



LEO:  No, you're saying.  Oh, you don't know.



STEVE:  Oh.  Oh, no.  Yes, there were some people who I saw.  Certainly the gang that hangs over in crypto land...



LEO:  They figured it out.



STEVE:  ...at GRC, they were on the ball completely, yeah.



LEO:  Wow.  That seems like the obvious way to do it.



STEVE:  Yeah.  There are, I mean, and that's what we've seen with this sort of stuff is we've got some - what I love about crypto is that they are simple little modular things like signatures and asymmetric or symmetric keys and hashes and, you know, some cool components that you get to assemble in different ways to solve, usefully solve problems.  But they're unhackable once you apply them correctly.  And unfortunately, I mean, the reason we have this interesting little thought experiment is that, boy, the company that did it, gave it the first try, really didn't put much time into it apparently because, yeah, they had a very insecure and very easily hackable solution.



LEO:  A friend of mine runs ice cream vending machines is his business.  And the way they know when it's time to refill, they have a little scale in the freezer.  And they know the weight of the machine.



STEVE:  Clever, clever, yeah.



LEO:  And that's sent back to the home office.  And of course you might say, well, then, how do they know which ones to bring?  Well, the truck always has all of the stuff.  And there are certain pretty predictable patterns as to which ice creams sell faster than others.



STEVE:  Ah, nice.



LEO:  Yeah, he's got one of those - if you see it, if you go to a convenience store and see those, they're like, I think it's Hershey's freezers there with the ice cream treats in there.



STEVE:  Ah, right.



LEO:  Little scales on the bottom so he knows when it's time.	



STEVE:  Nice.



LEO:  Send a guy out to refill.



STEVE:  Low-tech, but effective.



LEO:  Yeah, it works.  He also bought all of the pay phones from AT&T because it was too small a business for - this guy's a brilliant guy.  It was too small a business for AT&T.  But payphones are still widely used, especially in poorer communities.  He retrofitted them with cell phones, and it's a good little business.  It's a little cash cow, yeah.



STEVE:  Nice.



LEO:  Poor guy's got to roll those quarters, though.  But other than that.  It is time to wap this up, as they say.



STEVE:  Yes, Donald.



LEO:  Thank you [Elmer Fudd laugh], thank you so much, Steve Gibson.



STEVE:  Daffy.  Daffy.



LEO:  Daffy.  I don't know who it is.  Hello.  It's Elmer Fudd, actually.  [Elmer Fudd laugh]  Steve is at GRC.com.  That's his website, GRC.com.  And that's where you'll find this show, 64Kb audio plus human transcriptions, human written transcriptions.



STEVE:  And the recently blessed SpinRite, Leo.



LEO:  And the recently blessed SpinRite, the world's best hard drive maintenance and recovery utility.  Also lots of good free stuff, too.  He's very good about that.  GRC.com.  He's @SGgrc on the Twitter.  That way if you have something to tweet to him, you can do it.  He even accepts direct messages from all sources.  If you want video of the show, you can get that at our website.  We have video and audio at TWiT.tv/sn of all the 687 episodes on record, and a few that aren't on record.  And you can also subscribe.  Best thing to do really is to get your podcast application to subscribe to Security Now! so you don't miss a minute.  And you will have it all forever.  My friend, we have wapped this thing up.



STEVE:  We have come to the end of another podcast.  Until this time next week - oh, my goodness, and that's Election Day in the U.S.



LEO:  Oh.  I want to see that "I Voted" sticker on every single person.  



STEVE:  That's a biggie, yup. 



LEO:  100%.



STEVE:  I'll be proudly wearing mine.



LEO:  Yeah.  Thanks, Steve.  We'll see you next time.



STEVE:  Okay, my friend.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




