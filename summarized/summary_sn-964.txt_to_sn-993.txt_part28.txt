GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#964

DATE:		March 5, 2024

TITLE:		PQ3

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-964.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Last week we covered a large amount of security news; this week, not so much.  There are security stories I'll be catching us up with next week.  But after sharing a wonderful piece of writing about the fate of Voyager 1, news of an attractive new Humble Bundle, a tip of the week from a listener, a bit of SpinRite news, and a number of interesting discussions resulting from feedback from our listeners, our promised coverage of Apple's new "PQ3" post-quantum safe iMessage protocol consumed the entire balance of this week's podcast budget, bulging today's show notes to a corpulent 21 pages.  I think everyone's going to have a good time.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of security news, including a look at Apple's new PQ3 encryption.  Seems Apple might have oversold its capabilities.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 964, Episode 964, recorded Tuesday, March 5th, 2024:  PQ3.



It's time for Security Now!.  We are not for sale, but we are for free here for you right now.  Steve Gibson, our man in charge.



STEVE GIBSON:  Actually, Leo, for the right price, I have a feeling we would be for sale.



LEO:  Yeah, I mean, everybody has a number.  I'm not saying that.  Yeah, so I'll take that sign off, yeah.



STEVE:  That's right.  No.



LEO:  Hi, Steve.  Good to see you.



STEVE:  Great to be back for your pre-vacation podcast.



LEO:  Ah, yes.  I'll be gone for, I should tell everybody, I'll be gone for two weeks.  Mikah will be filling in, yeah.



STEVE:  Thought we'd catch you before you leave.  So this is, as I was wondering with you before we began recording, whether this was the shortest title we've ever had for a Security Now! podcast.  I mean, maybe.  But I guess there could be one called IQ or something in the past.



LEO:  I think this is the shortest.  Pawn to Queen 3 is the name of this show.



STEVE:  That's right.  Or Post-Quantum Level 3.



LEO:  Oh, what's that?  Ooh.



STEVE:  Well, exactly.  That's something that Apple just made up out of whole cloth because...



LEO:  Of course they did.



STEVE:  They said, we want to be special, and we're going to denigrate all of the other things that have come before us that are down at Level - they even have Level 0, where I think they miscategorized Telegram, saying that it didn't have end-to-end encryption.  It's like, what are you talking about?  Anyway, I'm a little annoyed with Apple...



LEO:  Uh-oh.  Uh-oh.



STEVE:  ...after learning about this.  But speaking of the podcast, before I forget, I brought whole site search back up at GRC.  We've had a number of our listeners who have written saying, hey, I want to, you know, you say you have transcripts, and I know you do, but I can't find what I want.  And of course,  yes, you could use Google and put site:grc and then whatever it is.  Anyway, and in the upper right of all of the pages of GRC has been a search box for a long time.  But Google changed something that broke it.  And I, you know, I figured, well, I've got to get SpinRite done.  So anyway, SpinRite is done, so search is back.  So I'm just saying that the whole site search, and you can narrow the search to just Security Now! podcast transcript search, if you like.  So that's there.



We've got a fun podcast.  Last week we covered a large amount of security news; this week, not so much.  There are security stories which I'll be catching us up with next week.  But after sharing a wonderful piece of writing about the fate of Voyager 1, news of an attractive new Humble Bundle, a tip of the week from a listener, a little bit of SpinRite news, and a number of interesting discussions resulting from feedback from our listeners, our promised coverage of Apple's new "PQ3" post-quantum safe iMessaging protocol consumed the entire balance of this week's podcast budget, bulging today's show notes to a corpulent 21 pages.



LEO:  Whoa.



STEVE:  So we've got lots to talk about, and I think everyone's going to have a good time.  And of course we've got a great Picture of the Week.



LEO:  That would have been a good title, though.  Corpulent.  You might have considered...



STEVE:  Corpulent, a Corpulent Podcast, that's right.



LEO:  You might have considered that.  I have the Picture of the Week teed up.  I have not yet looked at it.



STEVE:  One you will appreciate.  



LEO:  Oh, nuts.  My screen has changed again.  Why don't you describe it while I push the buttons here.



STEVE:  Okay.  Well, so we have one of the wonderfully classic fake O'Reilly...



LEO:  Oh, I love these, yeah.



STEVE:  ...programming coding books.  You know, and this one is titled "Coding..."



LEO:  So true.



STEVE:  "Coding with GPT."  And then the subtitle is "Introducing the uncanny valley into your codebase."  Because, you know, as we would say on this podcast, "What could possibly go wrong?"  And then, you know, O'Reilly is wonderful because they've always got some animal of some sort, I don't know how the animals are chosen, if there's any rhyme nor reason, or if they just roll the dice.



Anyway, this one, of course, in keeping with GPT, has a completely bizarre fictitious made-up animal.  It's got the head of a dog with cute little floppy ears looking off to the left, but then for its body it's got like the backend of a goose or a bird, a turkey, who knows what it is, of some kind, whose feet are on backwards.  And then at the very end it's got like a tail of some other sort of bird looking, I mean, it's just a weird, bizarre animal.  And I don't remember where it was, Leo, but it was on one of your podcasts, I think it might have been on a MacBreak Weekly a couple weeks ago, where we saw some weird animation of like a bunch of monkeys that were...



LEO:  From Sora, yeah.



STEVE:  It was just, it, like, hurt you to watch it because they just kind of kept turning around and merging in...



LEO:  Hallucinating, yeah.



STEVE:  Oh, wow.



LEO:  I remember.



STEVE:  Anyway, so this is a hallucinated animal, and I would recommend that everyone keep ChatGPT away from their code.  Actually, we'll probably get to it next week.  One of the things that I didn't have room for this week, Ben Nassi, a person whose work we've covered before, he was at the University of the Negev, they've produced a ChatGPT worm which they call Morris II.



Okay.  So we've had some fun keeping an eye on Voyager.  And on the occasion that we may have finally lost control of this intrepid explorer, I found a wonderful piece of writing about this that I know our listeners will enjoy and appreciate.  It's a blog post by someone named Doug Muir titled "Death, Lonely Death."  And Doug writes:  "Billions of miles away, at the edge of the Solar System, Voyager 1 has gone mad and has begun to die.



"Let's start with the 'billions of miles.'  Voyager 1 was launched in early September 1977.  Jimmy Carter was a hopeful new President.  Yugoslavia and the USSR were going concerns, as were American Motors, Pan Am, F.W. Woolworth, Fotomat booths, Borders bookshops, and Pier 1.  Americans were watching 'Happy Days,' 'M*A*S*H,' and 'Charlie's Angels' on television; their British cousins were watching 'George and Mildred,' 'The Goodies,' and Tom Baker as the Fourth Doctor.



"If you turned on the radio, 'Hotel California' by The Eagles was alternating with 'Dancing Queen' by Abba (and, if we want to be completely honest, 'Car Wash' by Rose Royce).  Most cars still ran on leaded gasoline, most phones were still rotary dial, and the Internet was a wonky idea that was still a few weeks from a working prototype.  'The Thorn Birds' was on top of everyone's bestseller list."  Blah blah blah.  He goes on to sort of set the place for us, reminding us also that "It was the summer of 'Star Wars.'  And Voyager 1 was blasting off for a tour of the Solar System.



"There's no way to pack the whole story of Voyager 1 into a single blog post.  Here's the TLDR:  Voyager was the second spacecraft to fly past Jupiter, and the first to take close-up photos of Jupiter's moons.  It flew on past Saturn, and examined Saturn's moon Titan, the only moon with an atmosphere.  And then it flew onwards, on and on, for another 40 years.  It officially left the Solar System and entered interstellar space in 2012.  It just kept going, further and further into the infinite emptiness."  And he says:  "You know about the Golden Record?  Come on, everybody knows about the Golden Record.  It's kind of a hokey and cheesy idea; it's also kind of amazing and great."



He says:  "Voyager has grown old.  It was never designed for this.  Its original mission was supposed to last a bit over three years.  Voyager has turned out to be much tougher than anyone ever imagined, but time gets us all.  Its power source is a generator full of radioactive isotopes, and those are gradually decaying into inert lead.  Year by year, the energy declines.  The power levels relentlessly fall.  Year by year, NASA has been switching off Voyager's instruments to conserve that dwindling flicker.  They turned off its internal heater a few years ago, and they thought that might be the end.  But those 1970s engineers built to last, and the circuitry and the valves kept working even as the temperature dropped down, down, colder than dry ice, colder than liquid nitrogen, falling towards absolute zero.



"Voyager stored its internal data on a digital tape recorder.  Yes, a tape recorder, storing information on magnetic tape.  It wasn't designed to function at a hundred degrees below zero.  It wasn't designed to work for decades, winding, rewinding, endlessly re-writing data.  But it did.  Voyager kept going, and kept going, until it was over 15 billion kilometers away.  At the speed of light, the Moon is 1.5 seconds away.  The Sun is about 8 light minutes away.  Voyager is 22 hours away.  Send a radio signal to it at lunch on Monday, and you'll get a response back Wednesday morning."  He says:  "I could go on at great length about Voyager  the discoveries it's made, the Deep Space Network that has maintained contact with it over the decades, the ever-shrinking crew of aging technicians keeping it alive on a shoestring budget, how amazing has it all been.



"In 1990, just before Voyager's camera shut down forever, the probe turned around and looked backwards.  It zoomed in and took a picture of Earth.  By that time, it was so far away that Earth was just a single pale blue pixel.  Seeing the blue pixel, Carl Sagan wrote:  'That's here.  That's home.  That's us.  On it, everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives.  The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every 'superstar,' every 'supreme leader,' every saint and sinner in the history of our species lived there - on a mote of dust suspended in a sunbeam.'"



He says:  "Voyager kept going for another 34 years after that photo.  And it's still going.  It has left the grip of the Sun's gravity, so it's going to fall outward forever.  Here's a bit of trivia:  Voyager 1 currently holds the record for most distant active spacecraft.  It's not even close.  The only other contender is Voyager's little sister, Voyager 2, which had a different mission profile and so lags billions of kilometers behind its older sibling.  Here's another bit of trivia:  If you're reading this in 2024, it's very unlikely that you will live to see that record broken.  There are only two other spacecraft outside the Solar System, Voyager 2 and New Horizons.  Both of them are going to die before they get as far as Voyager 1.  And nobody  not NASA, not the Chinese, not the EU  is currently planning to launch another spacecraft to those distances.  In theory we could.  In practice, we have other priorities.



"We thought we knew how Voyager would end.  The power would gradually, inevitably, run down.  The instruments would shut off, one by one.  The signal would get fainter.  Eventually either the last instrument would fail for lack of power, or the signal would be lost.  We did not expect that it would go mad.  In December 2023, Voyager started sending back gibberish instead of data.  A software glitch, though perhaps caused by an underlying hardware problem; a cosmic ray strike; or a side effect of the low temperatures; or just aging equipment randomly causing some bits to flip.



"The problem was, the gibberish was coming from the flight direction software, something like an operating system.  And no copy of that operating system remained in existence on Earth.  This is a problem NASA long since solved.  These days, every space probe that launches leaves a perfect duplicate back on Earth.  Remember in 'The Martian' how they had another copy of Pathfinder sitting under a tarp in a warehouse?  That's accurate.  It's been standard practice for 30 years.  But back in 1977, no one had thought of that yet.



"Voyager Mission Control used to be a couple of big rooms full of busy people, computers, giant screens.  Now it's a single room in a small building in the San Gabriel Valley, in between a dog training school and a McDonald's. The Mission Control team is a handful of people, none of them young, several well past retirement age.  And they're trying to fix the problem.  But right now, it does not look good. You cannot just download a new OS from 15 billion kilometers away.  They would have to figure out the problem, figure out if a workaround was possible, and then apply it, all with a round-trip time of 45 hours for every communication with a probe that is flying away from us at a million miles a day.  They're trying, but nobody likes the odds.



"So at some point  not tomorrow, not next week, but at some point in the next few months  they'll probably have to admit defeat.  And then they'll declare Voyager 1 officially over, dead and done, the end of a long song."



LEO:  If you want to know more about this, there's a wonderful documentary, came out last year.  I'm playing the trailer from it.



STEVE:  Good.



LEO:  "It's Quieter in the Twilight."  And it focuses on those people, working in that little office in San Gabriel Valley, who have gotten gray with Voyager.  There it is.  There's the little office.  It's a great story.  In fact, during the filming of the documentary, that's when they had decided they had to turn off the heater.  So it's quite a dramatic moment when they're making that decision and what the cost of that will be and so forth.  These are the people, the handful of people still running the Voyager mission.  And they've gotten a lot farther than anybody thought they would, which is why they're getting kind of long in the tooth.



STEVE:  That looks like Gramps.



LEO:  It is, it's great.  This is a fabulous documentary.  I couldn't recommend it more highly.  It's called "It's Quieter in the Twilight."  And they're not just talking about Voyager.  I think they're talking about the scientists, the engineers, the mission specialists that made this happen.  And they're still running this distant, distant object.  It's kind of cool.



STEVE:  It really is something to be said, Leo, for our ability to create something with this kind of endurance in that kind of environment.



LEO:  No kidding.



STEVE:  I mean, it is hostile out there.  You know?



LEO:  Yeah, yeah.  Well, it feels like it's just luck.  But on the other hand, NASA keeps doing it.  Remember...



STEVE:  Yeah, the same thing with Rover.



LEO:  Exactly.



STEVE:  What, Spirit and, I want to say Accountability, that wasn't.



LEO:  Yeah.  They finally had to end the drone, the little hovercraft, single rotor hovercraft they had that was flying around.  What a story.  But they only expected to do a few, a handful of flights with that, and they got dozens.  It's just - it's very impressive, I agree.  It really is. 



STEVE:  Yeah.



LEO:  And it's a great story.  And, hey, salute to Voyager.



STEVE:  Yeah.  I have a feeling that I will briefly let everyone know when they finally pull the plug.  But otherwise...



LEO:  Watch the documentary.



STEVE:  ...we've enjoyed - yeah.



LEO:  So good, yeah.



STEVE:  Good.  So I got a very cool tip of the week from Patrick Johnson, who wrote:  "Hi, Steve.  Long-time listener, and when downloading 6.1 I discovered that I've been a SpinRite owner for 10 years as of Tuesday.  I was surprised to hear in SN-963 that Firefox had let you keep the dedicated search box for so long.  I forgot when it was turned off by default for new installs."



LEO:  That was my reaction, too.



STEVE:  Right, yeah.  "But I did keep turning it back on for quite a while until I discovered that the CTRL+K shortcut for search still worked."



LEO:  Hmm.



STEVE:  Yeah, huh?  Remember how we liked CTRL+L because it immediately selected the URL, which was very handy for, like,  then doing a CTRL+C to copy it and paste it somewhere else.  He said:  "Treating the text entered in the omnibar as a strict search query, even for terms that look like a URL, no quotes needed."  He says:  "As a .NET developer, libraries and frameworks with the something-or-other .NET naming convention turn up quite a bit in my searches."  He says:  "Chromium seems to have copied this, so that now Brave, Edge, and Chrome all behave like Firefox with CTRL+K forcing a search."  And he said:  "Thanks for everything you do.  Patrick."



So Patrick, thank you.  This was news to me, so I tried it, and it works perfectly.  I previously shared my discovery, which I just mentioned, about CTRL+L.  And I've now added CTRL+K to my bag of keyboard shortcuts because it leaves no confusion.  You hit CTRL+K, and immediately the box - in my case I've just left my Firefox with the default Google search.  And so it immediately comes up and like puts Google in front, confirming what engine it will send this to when I hit ENTER.  So very cool tip, thank you.



I have no big SpinRite news, which at this stage is what we hope for.  Everything continues to go well.  I'm pushing forward on several fronts.  I'm spending time in GRC's forums, watching as new people encounter SpinRite 6.1 for the first time.  So I'm learning about their experiences which will be informing SpinRite's forthcoming FAQ.



One thing I've seen is that Linux users need SpinRite in a format that's directly usable to them.  Requiring non-Windows users to briefly somehow use Windows, like someone else's Windows system, to have SpinRite create a bootable USB drive for them is something I always planned to work around, so not to require that.  So I've been working on the tech to add direct bootable image downloading to GRC's servers so non-Windows users will be able to obtain a bootable drive image which they can copy directly to a USB drive, you know, just using "dd" in Linux, although I guess Ubuntu has a nice little drive imaging tool built in, which will allow them to boot their license copies of SpinRite.



I also have the beginnings of GRC's forthcoming email list system running that I know that a lot of our listeners are waiting for.  Someone just said that he received a note from X, you know, Twitter X, that he would no longer, he'd be losing his DM'ing privileges if he didn't get more active or something.  It's like, what?  You know, Elon just insists on creating trouble for people.  So I know that a lot of our listeners are only using Twitter so that they can send me notes from time to time, so I'll be fixing that.  So I'm in the process of bringing up our email list system.  And I plan to just maintain two lists, one specifically for weekly podcast announcements to our Security Now! listeners, and another for GRC-related news, you know, new software, updates, features, and so forth.  So people will be able to join either or both as they choose.  And I'm also working on SpinRite's documentation, which is coming along nicely.



Having learned from Microsoft last week that certificate reputation matters, even though they are deliberately mute about how exactly that reputation is earned, I presume it's a function of the exposure of a new certificate in the signing of non-malignant software.  So right after last week's podcast I used this relatively new certificate that I've got to cosign GRC's top six most downloaded Windows freeware.  In order of decreasing average daily download rates, the top six are ValiDrive, the DNS Benchmark, InSpectre, Securable, InControl, and Bootable.



What surprised me was that, for the first time ever, ValiDrive has taken the top slot to become GRC's most downloaded freeware with nearly 2,200 downloads per day. Anyway, taken together, those top six are being downloaded, in aggregate, 5,323 times per day.  So now they're all carrying this new certificate whose reputation I want to earn, and I expect it'll become golden before long.



I should also mention something really weird that was just pointed out to me, and that is an update of Microsoft's Trusted Root Certificate Program.  If I read this correctly, they're saying that the specialness of EV is being deprecated in August.  They specifically say, in August of 2024, all certificates, EV or not, will be treated identically.  So, what?



Anyway, I wrote to a good friend of mine at DigiCert, who back in the day were podcast listeners, I don't know if he still is, saying, am I reading this right?  Is EV, you know, sort of in the same way that EV has been deprecated for SSL, right, like none of our browsers show us anything special about EV certs anymore.  I'm wondering what's going on with EV code-signing.  Maybe the same thing.



Anyway, I expect I'll have an answer from him, and I'll let everybody know next week.  I'm certainly curious because, you know, I spent a month figuring out how to get dynamic, on-the-fly, server-side hardware security module cosigning for SpinRite, and now it looks like before it really gets used that much, maybe it's going to just be a nonstarter.  Anyway, no, no.



LEO:  We have EV certs for TWiT which we pay a lot of money for.



STEVE:  Now, those are TLS certs, though; right?



LEO:  Yeah.



STEVE:  Yeah.  And so I'm talking about code-signing EV.



LEO:  Oh, for code-signing, of course, yeah, yeah, yeah.



STEVE:  Which Microsoft says, eh, we're not going to bother paying attention to that anymore.



LEO:  So TLS EV certs continue.



STEVE:  Yes.



LEO:  Oh, whew, okay.  Scared me.



STEVE:  They continue, although - yeah.  They continue, although they're like, you know, only if the user goes to look is it clear anymore that a site is an EV cert.  It's, you know, I've stopped using them because there's no point.  There's no benefit.



LEO:  They don't show green anymore or anything like that.



STEVE:  Nope.



LEO:  Yeah.



STEVE:  They don't show anything different.  It's all disappeared from the chrome.  So let's take our second break, Leo.  And then we're going to do some feedback from our listeners before we get to a really interesting topic of what Apple has done with post-quantum crypto for their iMessage.



I am so glad you brought me back to the Humble Book Bundle because you know Cory Doctorow quite well.



LEO:  Oh, good friend, yeah.



STEVE:  Yes.  And this is, as far as I know, Cory's first incredibly affordable Humble Book Bundle.  I mean, I'll just read the description of it from the page.  I've got it in the show notes.  It's humblebundle.com/books/cory-doctorow, and so it's easy to search and find.



The description reads:  "Doctorow's visions of the future:  Lose yourself in the visionary fiction of Cory Doctorow, the celebrated author and digital rights activist known for his masterful explorations of the intersection of tech and society.  'Little Brother' is a stark exploration of surveillance and authoritarianism in the backdrop of a major terrorist attack on San Francisco.  'Radicalized' features four distinct sci-fi novellas, each telling a gripping story inspired by today's technologies and societal trends.  In 'Red Team Blues,' you'll follow along with a well-connected money laundering expert on the most dangerous and exciting gig he's ever taken on."



LEO:  He's actually a forensic accountant is his hero in this series, and I love it.  It's so good.  He's so good.  I love Cory.



STEVE:  Oh, very cool.



LEO:  Yes.



STEVE:  It says:  "Get all these and more, 18 books in total, and help support the Electronic Frontier Foundation with your purchase."



LEO:  Oh, great.



STEVE:  So 18 books.  As always, it's a pay what you feel it's worth, you know, as little as $18, a dollar each or more, in order to support Cory and the EFF.  So anyway, I wanted to make sure everybody knew about this Doctorow Humble Book Bundle.



LEO:  That's such a good thing.  Thank you.



STEVE:  Yeah, very cool.  So Tom Desmond said:  "Hey, Steve.  I'm just finishing listening to Episode 963" - so that was last week - "and I think I'm missing something on the cookie email link login loop.  The" - he has it in quotes - "'absolutely secure' process seems to assume that no bad person has compromised the email.  Think about this.  Someone gains access to the email from wherever.  They see an old email loop link.  They go to the site and guess the username - maybe it's the same email address - and they get the link in the compromised email, and they are in.  Am I missing something?"



And sadly, Tom, you're not.  I hate it when I miss something that's so obvious.  Tom is, of course, completely correct.  The solution which I described last week of embedding the email link requestor's IP address and browser cookie into the link would absolutely and strongly prevent anyone who did not request the email-based login link from using it, if they were to just passively observe it in flight or at rest.  That's nice.  But as Tom points out, that doesn't solve the problem that we're still just as dependent upon email security as ever.  Nothing prevents an attacker, as Tom says, who has the ability to monitor the user's email account, from themselves going to the site, requesting a login link from their IP location with their browser, then clicking the link that arrives in the compromised user's email.  Whoops.  And yes, they're logged in.



So this means that while there's no reason not to at least embed, for example, the requestor's browser cookie in the link, we don't really gain anything from doing that, and we remain dependent upon the security of email for all of our logon security, whether or not it's passwordless using email only or passworded with the ubiquitous "I forgot my password" total security bypass.  So thank you, Tom.  And I should say Tom was just the first of a bunch of listeners who said, uh, Steve, I think you are solving the wrong problem.  And they were right.  So thank you, everybody.



Our longtime listener and early advertiser with the podcast, Alex Neihaus, he wrote:  "Regarding email/password links in SN-962."  So that's two weeks ago.  He said:  "If you send a link with a hash/IP address, et cetera, you eliminate the ability to copy the link and open it in another browser.  It would also encourage clicking on links in email, something enterprises are trying hard to untrain users from doing."  He said:  "Also, if you did get such a link that you opened in Incognito mode, and the server expected a persistent session cookie, next time after restarting the browser you asked for the link in either normal mode or Incognito, it would not exist.  Email links instead of passwords are a good idea," Alex thinks, "and can be made secure as you describe.  But it does not fit with the way many people interact with browsers, especially those who use multiple browsers."



And of course Alex's point is a good one.  Heavy use of email links for common logon would have the effect of "untraining" users against clicking on links in email.  If the world were to switch over to that, and I don't think there's any danger of that happening, there would certainly be a lot more email link clicking going on.  And it's also true that the tighter we make the anti-spoofing security, the greater the chance for rejecting a valid user.  Which of course would upset them because this is the way they log on.



The presumption is that the user wishes to log on now, not later.  So they would request a link, then immediately go to their email to find and execute the link, which would take them back to where they just were.  But Alex is right that there are various things that could go wrong.  So as we so often encounter, everything is a tradeoff.  The great benefit of email link logon is that the user needs to remember and possess nothing, and they are able to log in from anywhere, as long as their email is secure, and they have access to it.



I think the best thing probably to come from the last few weeks of this discussion has been the recognition that passwords really only amount to logon accelerators, and that as long as every username and password logon opportunity is also accompanied by "the dog ate my password" bypass, nothing else we do to further increase logon security actually matters - not hardware dongles, not fingerprints, not one-time passwords, nothing.  It all just amounts to security theater.  This, in turn, implies that the security of our email is far more important than is commonly appreciated.  And so I think that's another good thing to really come from this discussion.



And before we finish this, another listener sent this bit of fun.  He wrote:  "The Taco Bell app seems to have gone passwordless in favor of emailing you a link whenever to try to log in.  It's not great user experience for me in that it's slow and prone to spam filtering.  When you're trying to put in your order from the car on your way to the Bell, it's enough to make you drive to Chipotle instead."



So, yes.  You want to make sure, if you're going to switch to - and I thought it was interesting that Taco Bell, the Taco Bell app now just wants to have you immediately confirm who you are by replying to a link in email.  They don't want to create user friction.  They apparently think that they are reducing it by not requiring the user to use a password all the time.  But as this listener pointed out, they might lose some business.



Joel Clermont said:  "There is a security-focused reason to enforce a max password length when hashing with Bcrypt."  He said:  "I recently discovered the recommendation and wrote up more details here."  And so Joel provided a link to his write-up, which is located at "masteringlaravel.io," which was interesting.  It turns out that Laravel uses Bcrypt as its PBKDF password-hashing algorithm; and for reasons that defy understanding, the Bcrypt algorithm itself has a hard and fixed internal password length limit of 72 bytes.



Okay, now, normally we'd think that 72 bytes was way more than anyone might need.  But unfortunately, many characters in non-Latin alphabets are encoded as three and even four bytes.  So it would be possible for a non-Latin password to max-out Bcrypt's fixed 72-byte limit with just 18 characters.  And again, that's probably enough.



But still, Joel makes a good point about there being possible exceptions to the boundless upper password length presumption.  I'll just note that a simple solution to this problem for anyone who might be stuck using Bcrypt would be to first run the user's truly unlimited-length password through an SHA-256 hash, or even an SHA-512.  Why not?  And do it just once.  SHA-512 will take a password of any length, even a long one using non-Latin characters, and reduce it to exactly 64 bytes, which can then be sent into Bcrypt to be strengthened against brute-force guessing.  Which is why you use Bcrypt at all in the first place.  Anyway, the cute little hack here is just stick a hash in front of something that can't take long enough passwords, and you completely solve that problem.



LEO:  Does Argon2 have the same issue, or just Bcrypt?



STEVE:  No, no.  It just was an early design decision in Bcrypt.



LEO:  Bcrypt's kind of old, actually; isn't it?  And kind of funky.



STEVE:  Yes.  Yes.  And one would not even start using it at this point.  



LEO:  Yeah.



STEVE:  Yeah.  Earl Rodd said:  "Regarding the discussion of Nevada's request for an injunction against Meta encrypting messages for minors in SN-963.  As I listened, I was aware that the great missing piece is data.  All of those quoted on both sides, privacy advocates and law enforcement advocates, have lots of opinions, but they present no data.  Data like, you know, the number of times criminals were believed to go free because of encrypted messages stopping prosecution; or the number of times such crimes were not prosecuted at all due to encryption; or data like times that using unencrypted messages was known to lead to harm to minors.  Is this really a case of balancing privacy, i.e., being pummeled with advertising, versus throttling the ability of law enforcement to find and prosecute serious crime?"  He said:  "I suspect it's not that simple.  But without any data, who knows the actual tradeoff?"



Okay.  I think Earl makes such a good point here.  You know, we've been driven so far away from actual data collection that we appear to have forgotten that actual data could be collected and made available.  You know, we're just never asking for it; right?  We've, you know, we've become the "anecdotal example" culture, being slammed from one extreme to the other.  And here's the problem:  Expressing a strong opinion is easy.  It excites.  It's junk food for the mind.  But actually collecting, tabulating, and evaluating data is difficult, time-consuming, and expensive work.



And the biggest problem is, as Earl suggests, the result of all that actual work probably doesn't support the extreme views of either side.  So it's not in either side's best interest to be presented with any of those pesky facts which would likely lead to some form of compromise policy.  Better just to wave our arms around, attempting to terrify everyone, and jam through regulations that support an ideology rather than reality.  It seems that too much of the world is working this way these days.



We've seen countless examples of how the move to an online digital world has very likely provided law enforcement agencies with a treasure trove of readily accessible, indexable, and searchable information the likes of which hasn't ever been known.  It's never been known before.  We've recently learned that these agencies are consumers of the information that commercial data brokers have been collecting about us for years.  They're buying this data.  Today, everyone leaves footprints and bread crumbs wherever they go online, and whatever they do.



If law enforcement knew what all of our various service providers know, and what our ISPs probably know about where we wander on the Internet - and we should assume that they do know any of that if they wanted to - then law enforcement knows pretty much everything about us, certainly more than was ever knowable about people before the Internet.



As a law-abiding citizen of the United States, which for the most part leaves its citizens alone unless some intervention is required, I'm all for law enforcement working behind the scenes to keep a lid on criminal behavior.  That's what I want them to be doing.  So given the unparalleled tools that are now available to aid in that work, it is difficult to get too worked up over the encryption debate.  Law enforcement authorities can even know who we're talking to if they wish, even if it's much more difficult to peer into the content of those conversations.  Although I also do not have any data to back up my opinion, my intuition is that we're already sitting with a workable compromise for both sides.



Could the privacy absolutists wish for more privacy?  Absolutely.  Are they going to get much more?  Probably not.  Do our law enforcement agencies wish they could also listen in on anyone's conversations?  I'm sure they do.  Are they going to be able to do that?  Let's hope not because that would clearly be a step too far in the "Big Brother" direction.  And again, they must already have access to far more data about us than they even know what to do with.  It seems to me that the right balance has already been struck.  But I think Earl's point is such a good one about hey, you know, like where's the data to back up any of this either way?  So thank you for that.



Elliot Alderson said:  "Hi, Steve.  The issue with passwordless login the way you described is if someone only has email on their phone."  He said:  "I don't have email on my computers because I don't need it."  Really.  "If the IP and everything is baked into the link, I'm SOL.  That also makes it very difficult to log in on other browsers."  That's true.



So, okay.  Having listened to everyone's thoughts about this, it's clear that the potential automation offered by an email link creates more problems than it solves.  So the solution for email-only-based login is to return a visible, easy-to-transcribe, one-time six-digit token, like everyone who uses one-time passwords has become used to.  Just email the token to the user and request that they enter the token into the browser they're currently using.  If they receive the email on their phone, they have the advantage of seeing both the token and their browser page at the same time.



This eliminates the "untraining" about never clicking on a link, and it's not really that much more work for the user to type in six digits.  And they still don't need to remember anything, their password or anything else.  So I think, you know, stepping back from the link idea, you know, more automated though it was, it's got a lot of downsides which our listeners, astute as they are, have all pointed out.  So thank you, everybody.



SecFan said:  "Steve, in SN-963 yesterday, a listener had mentioned the idea of having a standardized way of documenting a site's password requirements so that they could be automated.  In writing my own algorithmic password manager, named Passify" - that's a cute name - "that I messaged you about previously, I had the same thought and wanted Passify's algorithm to accommodate known requirements wherever possible.  I discovered that Apple had put some thought into this and created 'Password Manager Resources' on GitHub."  And that's what it's called, github.com/apple/password-manager-resources.



He said:  "It includes a JSON format" - just sort of as I was suggesting last week - "for documenting password rules as well as a number of other helpful things for Password Managers to automate or semi-automate password management.  The idea is not completely new.  Apple's Safari browser has supported a 'passwordrules' HTML attribute that uses the same rule markup as the JSON to describe requirements for a while.  Thanks again for all your work.  Looking forward to the official release of the new SpinRite."



Anyway, it's cool that Apple has already done the work of creating some of this structure.  And my comment last week was about it being a heavy lift to get the industry adoption, and that appears to be the case.  This is now four years old, and it has remained somewhat obscure.  Of course, if any website wanted to modernize, they could simply place a relatively high lower limit on password length and accept passwords of any greater length containing any characters.  You know, just hash the thing a bunch, and just don't worry about how long it is or what the thing contains, and do it on the browser side, and you're good to go.  And then on the server side, they could prevent endless brute forcing password guessing by putting up a roadblock if someone is misguessing the password to a site some number of times.



Rob said:  "Security Now! question:  After hearing your discussion of session cookies, I was reminded of my own security concern I've had for years.  Seems I'm always logged into my Google account based on session cookies.  And so it seems, if a hacker could grab those cookies from my computer, they'd now have access to all my emails and many other Google things.  This," he says, "even with Google's Advanced Protection Program."



He says:  "I believe I've even migrated to a new MacBook and was automatically logged in fine on the new MacBook," he says, "though my memory could be a bit off on that."  And I don't think so.  I think there's a lot of inter-browser synchronizing going on these days.  He says:  "Didn't seem there was any other verification that my hardware was the same, et cetera.  Just doesn't feel very secure to me.  Am I missing something?"



Okay.  So first of all, a bit of clarification here because Rob used the term "session cookies," which I'm also seeing a lot of casual use about.  When a cookie is set in a browser, the browser can be told how long to retain that cookie in the form of either an expiration date, at which point the cookie will then expire, or a maximum age.  If either of those are provided, then the cookie is considered to be "persistent," and the browser will retain and return that cookie's value until it is refreshed, that is, the setting of the cookie is refreshed, the cookie is deleted by the user or by the website, which is able to do so, or that designated end-of-life is reached, and the cookie self-expires.  Any cookies that have not expired will be retained from one browser session to the next.  That is, they are persistent.



But I mentioned that the specification of these expirations was optional.  If a cookie is set without any explicit expiration, then it is considered to be a session cookie, as in good for this browser session, because it will be retained only for the current browser session.  It is explicitly retained only in RAM, and it is never written in any way to any permanent form of storage.  Once the browser closes, the browser session ends, and the cookie will be forgotten.



So to Rob's question and uneasiness, it is the case that our browser's persistent cookies are now being retained over the long-term, and that's what keeps us logged into our various websites persistently from one boot-up of our computer to the next.  And it's definitely the case that if bad guys could arrange to obtain those persistent login cookies, they could immediately impersonate us.  In fact, this is exactly what that Firesheep Firefox add-on did many years ago.  And this was one of the primary motivators for the move to always-present HTTPS.



Firesheep relied upon the mistake that most websites of the era were making.  After authenticating with a username and password whose communication was protected by HTTPS, most popular sites would drop back to plain, lower overhead, unsecure, cleartext HTTP, figuring wrongly that the user's identity had been protected.  They failed to take into account that the only way that user was remaining logged on from one HTTP page query to the next was that each browser query was accompanied by cookies; but that without the encryption protection provided by HTTPS, those cookies were being sent in the clear so that anyone who could eavesdrop could obtain them and immediately impersonate the user, obtaining parallel access to their currently logged-on web sessions.



So yes, Rob, our browser cookies do serve as persistent long-term authentication tokens; and anyone who might have some means for obtaining them could, indeed, impersonate their owner.  Sometimes when logging into a site you'll see a "Trust this browser and remain logged in?" checkbox.  This is useful when you do not want to leave a persistent cookie behind in that browser, for example, when logging into a shared Internet caf machine.  You'll also want to carefully and explicitly log out of that machine once you're finished in order to at least invalidate that cookie, even if it isn't completely removed from the machine.



But that "Trust this browser and remain logged in?" question is offering to change the logged-on authentication cookie from a temporary session cookie to a long-term persistent cookie.  And there is a difference.  One sticks around.  The other one should, if the browser's behaving itself correctly, and I know we talked about this in the past, there was a point at which the various important browsers, which at this point means Chromium-based browsers and Firefox and Safari, were being very careful never to write that transient true session-level cookie to permanent storage.



"Hi, Steve.  Listened to SN-962 and wanted to provide a bit of additional information regarding your discussion on password requirements.  I work for a company in the financial services sector, specifically insurance.  I did want to note that many companies in this sector still rely on mainframes, and that is likely the case for many of these maximum password length limits and character restrictions due to IBM's fanatical backwards compatibility."



He said:  "Several years ago, I interned for another company in this sector and was shocked at the requirements for passwords.  Seven to eight characters only, and the only symbols allowed were the @ and the $ characters."  He says:  "How much of this is whose fault is a bit opaque to me even still, but I can say that it isn't entirely the mainframe's fault.  I've seen that more 'modern'" - and he has that in quotes - "implementations allow up to a 40-character 'password phrase,' but still restrict a few characters.  All that being said, you see these kinds of restrictions often in sectors that continue to rely on the mainframe, and other legacy platforms and products."



And I should also mention that a number of our other listeners have since sent me notes saying that, Steve, mainframes are not gone.  They still exist.  You know, they're not around the way they used to be, but they're not gone.  So I appreciate the feedback that we still haven't moved very far.  One of the most significant things we've seen and learned through the years of this podcast is the power and prevalence of inertia, which is everywhere, acting to keep things from changing.



One trick that I ought to mention that works quite well is to map a hashed value to some fixed alphabet.  Say that a backend system can only accept eight characters of upper and lower case alpha, numbers, and as we saw in this case, an @ sign and a $ sign.  Okay?  So that's 26 lowercase and 26 uppercase.  So now we're at 52.  Ten digits brings us to 62.  And two other characters brings the total alphabet size to 64.



Now, okay.  First of all, the fact that the alphabet size turned out to be 64, you know, representable by exactly six binary bits, you know, 2^6 is 64, that should jump out at everyone.  Okay.  So let's take this case first.  We want a web frontend to give its users unrestricted passwords.  So it hashes whatever they provide, doesn't matter, absolutely doesn't matter, preferably using some contemporary PBKDF function like Argon2.  The output will be 256 evenly distributed bits.  We know that that's what hashes give us.  So they are simply taken from either end, let's take them from the right end, you know, the least significant right end, if we regard this 256-bit value as a number.



So we'll take them six bits at a time.  Each of those six bits is mapped back into one of those 64 characters.  So after eight groups of six bits have been taken, those eight characters are then passed back to the creaky old mainframe as the user's password.  The result is that the user is able to use whatever crazy long and special-character-filled password they may choose, and whatever they choose is first strengthened by a modern PBKDF to harden it against brute-force attack and then converted into a high-entropy eight-character password which meets the needs of the backend system.



Okay.  But what if the backend system's password alphabet wasn't conveniently 64 characters, which allowed us to take, just simply take six bits at a time?  Okay.  So consider this:  One way to visualize taking six bits at a time is dividing the large hash value by 64, which is to say the size of the alphabet.  In a binary representation, division by two is a right shift of the bits.  So dividing by 64 is shifting right by six bits.  We can think of the bits that are shifted off to the right as the remainder of the division.  So to extract a single character from an alphabet of 64, we are actually dividing the large binary hash value, we're doing a long division by 64 and taking the remainder, which will always range from 0 to 63, thus having one of 64 possible values.



So this means that we can extract characters of an alphabet of any size we wish from a large hash value by performing successive long division of the hash by the size of the alphabet we wish to extract where the remainder from each division will be the value that's mapped back into the alphabet.  Anyway, I've always found this to be very cool and been fond of this solution since it provides high-entropy, evenly distributed characters from an alphabet of any size, extracted from large binary values, such as those that are conveniently produced from hashing.



Our second-to-the-last bit of feedback, we have a bit of a longer format piece, but this is significant and important, and there's some lessons here.  And I've had several bits of conversation with him since, which I'll share.  He said:  "Hello, Steve.  I've been a longtime listener, and even longer time SpinRite user, and I enjoy your weekly updates and common sense perspectives on all kinds of security topics.  I want to share a rather long story with you and would like to hear your opinion about an interesting in-progress responsible vulnerability disclosure.  Here goes."  And it's actually not that long.



He said:  "In October 2023 I came across a sign-in page of a high-profile company with 20-plus billion USD revenue, and more than 100 million monthly active users on their website.  Not a small business.  For my work, I sometimes just for fun regularly visit websites with the Developer Tools tab open.  This time I noticed that the sign-in page of this particular website returned a Response Header that mentioned nginx/1.12.2 as the web server.  In itself not a big problem, but it is better to hide this information from the public.  However, if the page was really served by nginx 1.12.2, then the website would have a big problem because this version of nginx is very old and has a number of critical and high-severity vulnerabilities that are known to be actively exploited.



"So being a good citizen, I wanted to tell them about the issues, just for the sake of improving their security and of course also for improving the security of their 100 million monthly active users."  Apparently he was one of those.  He said:  "So I searched on the company's website for their policy for responsibly disclosing these vulnerabilities, but the best I could find was the email address for privacy-related questions.  I wrote an email telling them about the vulnerabilities, included some screenshots for evidence, and asked some questions about their statements in their privacy policy and terms and conditions on protecting my information and just keeping the website bug-free.  I was not looking for any financial compensation.  I just wanted to make the world a better place.  I soon received a response that a bug report had been filed, and that it could take up to 120 days for bug reports to be tested and validated.  I received no answers for my questions about privacy.



"Fast-forward to February 2024, 118 days since their message that the bug report had been filed.  I politely asked them about the status of the bug report, notified them that the vulnerabilities still existed, and asked them if my personal data is still safe with them.  A week later I received an email from them saying that after review and risk rating by their information security team, the bug report qualifies for a bounty payment of $350.  If I agree to this payout agreement, I also agree to keep my silence about the issue.  My questions about privacy and trust and new questions about their responsible disclosure policies remain unanswered.  In the meantime, the issues still exist, and a small tour around some of the other websites of the same company show similar problems.



"Now, the fact that these vulnerabilities are so easy to find, the fact that the reported web server is so old, the fact that it is so easy to remove a server banner, and the fact that it is still not fixed after more than four months makes me believe that this could be a honeypot."  Well, he's being extremely generous.  I don't believe that.  I think that we're dealing with is a typical irresponsible company.  Anyway, he says:  "If it is, then I think that it is a very, very risky attempt of having a honeypot."  You know, why would you have a honeypot as your main logon server?  I doubt that's the case.  He said:  "Because it is easy to find, it is on pages where people enter their username and password, and if the media get hold of it, then the brand damage of such a high-profile company will be big."



He said:  "I have asked them directly if this is a honeypot, but I did not get a reaction yet.  What do you think about this?  What should I do?  Accept the payout and keep my silence?  Should I also report the 30-plus other vulnerabilities on their websites and sign-in pages?  Should I tell them that just by looking at the web server Response Headers, I know that they use a mix of nginx 1.12.2, Microsoft IIS 10.0, ASP.NET 4.0.30319, Shopify, WordPress, et cetera?  Or should I just leave it up to them to find it all out?  I mean, what could possibly go wrong?  Kind regards; and again, I love your show.  Keep going.  Robert Blaakmeer."



LEO:  Wow.  He's a Good Samaritan.



STEVE:  So Robert, yeah.  And I wrote, I said:  "Robert, I agree that this is a conundrum.  I took a quick peek at the version of nginx, and what immediately jumped out at me - as it would any researcher, nefarious or not - was CVE-2021-23017.  It's a well-known, remotely exploitable, 'off by one' error which gives an attacker unauthenticated remote code execution capability."



LEO:  Oy, oy, oy.



STEVE:  Uh-huh.  And what's more, a three-year-old working Python proof-of-concept exploit is also publicly available.



LEO:  Well, there you go.



STEVE:  I mean, it just doesn't get any worse.



LEO:  Even a three year old could do it.  Geez.



STEVE:  So Robert, I agree that you're right to be worried about them, and worried in general.  It appears that any miscreant wishing to target this enterprise has everything they need to do so.  Unfortunately, although you've done the right thing every step of the way, their ongoing negligence in dealing with this, which you had no way of anticipating upfront, has now compromised you, since you've acknowledged to them that you're in possession of information that could damage them - either reputationally or for use in an attack.  And as we also know, attorneys can allege that, just by looking at their web server headers, you've obtained "non-public network hacking" information, and some judge who knows no better could be convinced.



LEO:  It's happened.



STEVE:  It's happened before.



LEO:  Yup.



STEVE:  I think I would have nothing further to do with them.  The problem with a company that has become this large is that the people who you really should be talking to are unknown and inaccessible to you by design.  Between you and them are layer upon layer of chair-warming functionaries who have no idea what you're even talking about.  At this point, my recommendation would be for you to turn everything over to CISA and let them take it from there.



LEO:  Perfect.



STEVE:  This completely discharges your responsibility in the matter while insulating you from any blowback since no one, not the company in question nor any judge, could fault you for confidentially and privately informing the U.S. Government's Cybersecurity & Infrastructure Security Agency of this potential critical vulnerability after having first given the company in question all of November, December, January, and February to deal with upgrading their servers.



CISA, the rare government agency that appears to actually have its act together, has a simple procedure for report filing.  Go to www.cisa.gov/report.  I also created a GRC shortcut so that even if someone forgets the agency's name, going to grc.sc/report will always bounce you to that page at CISA.  The page has several categories of problems to report.  If you scroll down you'll find "Report software vulnerabilities or ICS vulnerabilities," which appears to fit best.  If you go there, you get bounced over to CERT.org, which is the CERT Coordination Center at Carnegie Mellon University.



So I'd fill out that form to let the U.S. government powers that be know about this.  One of the things you can do is attach a file to your submission.  So I would send them your entire email thread with the company so they can see that you have acted responsibly at every step.  You'll have done the right thing.  And when CISA and CERT reach out to contact the company about their negligence over a four-year-old known critical vulnerability across all their web platforms, you can bet that they'll wind up speaking to someone who can effect the required changes.  And at that point you'll have done everything you can, while insulating yourself as well as possible from any annoyance this company might feel over being made to help themselves.



LEO:  And you're pretty sure, I mean, if you did this, the company might not be happy about it, but honestly I think you'd be safe from prosecution.  Right?



STEVE:  Yes.  I think you're as safe as you possibly could be.  I should mention that he did that.  He sent back to me afterwards that he took everything, he submitted it to them, he got a response saying "Thank you for the submission, and we'll take it from there."



LEO:  Yeah.  Excellent.  That's probably the way to start in these things, I would say.



STEVE:  And finally - I really, I do, I think so, too.  I mean, we've seen and have covered on the podcast, so our listeners have heard too many instances where the company goes after the responsible disclosure guy, saying, you know, you hacked us, you bad person.  It's like, no.  No no no no no no.  Anyway, yeah.  I would just...



LEO:  Randal Schwartz did jail time for that; you know?  I mean...



STEVE:  Yup.



LEO:  It's terrible, yeah.



STEVE:  Yeah.  It's wrong.  Okay.  And finally, for next week, we have the first appearance of a zero-click GenAI application worm.  As I mentioned at the top of the show, Ben Nassi sent me a tweet.  He said:  "Hi, Steve.  I'm Ben Nassi, a former researcher at the Ben-Gurion University and Cornell Tech, and the author of 'Video-based Cryptanalysis and Lamphone' which you previously covered in your podcasts.  I just published a new research paper on a worm that targets GenAI applications.  We named it Morris II.  Guess why?"  He said:  "I think the audience of Security Now! will love to hear more about it.  And keep on with the great podcast that you and Leo are doing beyond  999.  By the way, the research was also published on Bruce Schneier's blog."



LEO:  Oh, that's pretty cool, yeah.



STEVE:  So I suspect that next week's podcast will be titled "Morris II."  So stay tuned for a look at the "worminization" of GenAI, where I'm sure we'll be answering the question, "What could possibly go wrong?"



LEO:  Now, PQ Trois.



STEVE:  So we can guess, we know, that PQ stands for Post-Quantum.  And of course we'd be right.  So is this Apple's third attempt at a post-quantum protocol, after one and two somehow failed or fell short?



LEO:  What?  No.



STEVE:  You know, PQ3, what happened to PQ1 and PQ2?  No, Apple has apparently invented levels of security.



LEO:  Oh.



STEVE:  And put themselves at the top of the heap.  So PQ3 offers what they call "Level 3 Security."



LEO:  Oh.



STEVE:  So here's how SEAR, S-E-A-R, Apple's Security Engineering and Architecture group, introduced this new protocol in their recent blog posting.  They write:  "Today we are announcing the most significant cryptographic security upgrade in iMessage history with the introduction of PQ3, a groundbreaking post-quantum cryptographic protocol that advances the state of the art of end-to-end secure messaging.  With compromise-resilient encryption and extensive defenses against even highly sophisticated quantum attacks, PQ3 is the first messaging protocol to reach what we call Level 3 Security, providing protocol protections that surpass those in all other widely deployed messaging apps.  To our knowledge, PQ3 has the strongest security properties of any at-scale messaging protocol in the world."



Well.  So they're proud of their work, and they've decided to say that not only will iMessage be using an explicitly quantum-safe encrypted messaging technology, but that theirs is bigger than - I mean better than - anyone else's.  Since so far this appears to be as much a marketing promotion as a technical disclosure, it's worth noting that they have outlined the four levels of messaging security which places them alone at the top of the heap.



Apple has defined four levels of messaging with levels 0 and 1 being pre-quantum - offering no protection from quantum computing breakthroughs being able to break their encryption - and levels 2 and 3 being post-quantum.  Level 0 is defined as "No end-to-end encryption by default," and they place QQ, Skype, Telegram, and WeChat into this Level 0 category.  Now, this causes me to be immediately skeptical, you know, of this as being marketing nonsense, since although I have never had any respect for Telegram's ad hoc basement cryptography, which they defend not with any sound theory, but by offering a reward to anyone who can break it, we all know that Telegram is encrypted, and that everyone using it is using it because it's encrypted.  So lumping it into Level 0 and saying that it's not encrypted by default is, at best, very disingenuous; and I think it should be beneath Apple.  But okay.



Level 1 are those pre-encryption algorithms, I'm sorry, pre-quantum because 0 and 1 are both pre-quantum, you know, non-quantum safe, so Level 1 are those pre-quantum algorithms that Apple says are encrypted by default.  The messaging apps they have placed in Level 1 are Line; Viber; WhatsApp; and the previous Signal, you know, before they added post-quantum encryption, which we covered a few months ago; and the previous iMessage, before it added PQ3, which actually it hasn't quite added yet.  But it's going to.



Now we move into the quantum-safe realm for levels 2 and 3.  Since Signal beat Apple to the quantum-safe punch, it's sitting all by its lonesome at Level 2 with the sole feature of offering post-quantum crypto, with end-to-end encryption by default.  Presumably, as Signal's relatively new PQXDH protocol moves into WhatsApp and other messaging platforms based on Signal's open technologies, those other messaging apps will also inherit Level 2 status, lifting them from the lowly levels of 0 and 1.  But Apple's new PQ3 iMessaging system adds an additional feature that Signal lacks, which is how Apple granted themselves sole dominion over Level 3.  Apple's PQ3 adds ongoing post-quantum rekeying to its messaging, which they make a point of noting Signal currently lacks.



This blog posting was clearly written by the marketing people, so it's freighted with far more self-aggrandizing text than would ever be found in a technical cryptographic protocol disclosure, which this is not.  But I need to share some of it to set the stage, since what Apple feels is important about PQ3 is its attribute of ongoing rekeying.



So to that end, Apple reminds us:  "When iMessage launched in 2011, it was the first widely available messaging app to provide end-to-end encryption by default, and we've significantly upgraded its cryptography over the years.  We most recently strengthened the iMessage cryptographic protocol in 2019 by switching from RSA to Elliptic Curve cryptography, and by protecting encryption keys on device with the Secure Enclave, making them significantly harder to extract from a device even for the most sophisticated adversaries.  That protocol update went even further, with an additional layer of defense, a periodic rekey mechanism to provide cryptographic self-healing, even in the extremely unlikely case that a key ever became compromised."



Again, extremely unlikely case that a key ever became compromised, they acknowledge.  But now they have rekeying, so it's possible.  "Each of these advances were formally verified by symbolic evaluation, a best practice that provides strong assurances of the security of cryptographic protocols."  And about all that I agree.



Okay.  So Apple has had on-the-fly ongoing rekeying in iMessage for some time, and it's clear that they're going to be selling this as a differentiator for PQ3 to distance themselves from the competition.  After telling us a whole bunch more about how wonderful they are, they get down to explaining their level system and why they believe PQ3 is an important differentiator.  Here's what they say, and I skipped all the other stuff.



They said:  "To reason through how various messaging applications mitigate attacks, it's helpful to place them along a spectrum of security properties.  There's no standard comparison to employ for this purpose, so we lay out our own simple, coarse-grained progression of messaging security levels.  We start with classical cryptography and progress toward quantum security, which addresses current and future threats from quantum computers.  Most existing messaging apps fall either into Level 0, no end-to-end encryption by default and no quantum security; or Level 1, with end-to-end encryption by default, but no quantum security.



"A few months ago, Signal added support for the PQXDH protocol, becoming the first large-scale messaging app to introduce post-quantum security in the initial key establishment.  This is a welcome and critical step that, by our scale, elevated Signal from Level 1 to Level 2 security.



"At Level 2, the application of post-quantum cryptography is limited to the initial key establishment, providing quantum security only if the conversation key material is never compromised.  But today's sophisticated adversaries already have incentives to compromise encryption keys because doing so gives them the ability to decrypt messages protected by those keys for as long as the keys don't change.  To best protect end-to-end encrypted messaging, the post-quantum keys need to change on an ongoing basis to place an upper bound on how much of a conversation can be exposed by any single point-in-time key compromise, both now and with future quantum computers.



"Therefore, we believe messaging protocols should go even further and attain Level 3 security, where post-quantum cryptography is used to secure both the initial key establishment and the ongoing message exchange, with the ability to rapidly and automatically restore the cryptographic security of a conversation, even if a given key becomes compromised."



Okay.  It would be very interesting to hear Signal's rebuttal to this, since it's entirely possible that this is mostly irrelevant, largely irrelevant marketing speak.  It's not that it's not true, and that continuous key rotation is not useful.  We've talked about this in the past.  Key rotation gives a cryptographic protocol a highly desirable property known as Perfect Forward Secrecy.  Essentially, the keys the parties are using to protect their conversation from prying eyes are ephemeral.  A conversation flow is broken up and compartmentalized by the key that's in use at the moment.  But the protocol never allows a single key to be used for long.  The key is periodically changed.  The reason I'd like to hear a rebuttal from Signal is that their protocol, Signal's protocol, has always featured perfect forward secrecy.  Remember "Axolotl"?



LEO:  Yeah.



STEVE:  Yes.



LEO:  Which is for the same purpose as rekeying.



STEVE:  Yes.  It is rekeying.



LEO:  Oh.



STEVE:  Here's what Wikipedia says.  I'm quoting from Wikipedia.  "In cryptography, the Double Ratchet Algorithm, previously referred to as the Axolotl Ratchet, is a key management algorithm that was developed by Trevor Perrin and Moxie Marlinspike in 2013."



LEO:  Oh.



STEVE:  Uh-huh.  "It can be used as part of a cryptographic protocol to provide end-to-end encryption for instant messaging.  After an initial key exchange, it manages the ongoing renewal and maintenance of short-lived session keys.  It combines a cryptographic so-called 'ratchet' based on the Diffie-Hellman key exchange and a ratchet based on a key derivation function such as a hash function, and is therefore called a 'double ratchet.'  The algorithm provides forward secrecy for messages, and implicit renegotiation of forward keys, properties for which the protocol is named."



Right.  In 2013.  In other words, it would be nice to hear from Signal, since Apple appears to be suggesting that they alone are offering the property of perfect forward secrecy for quantum-safe messaging when it certainly appears that Signal got there 11 years ago.  This is not to say that having this feature in iMessage is not a good thing.  But it appears that Apple may not actually be alone at PQ's Level 3, much as they would like to be.  So when do we see it from Apple?



They write:  "Support for PQ3 will start to roll out with the public release of iOS 17.4, iPadOS 17.4, macOS 14.4, and watchOS 10.4, and is already in the corresponding developer preview and beta releases.  iMessage conversations between devices that support PQ3 are automatically ramping up to the post-quantum encryption protocol.  As we gain operational experience with PQ3 at the massive global scale of iMessage, it will fully replace the existing protocol within all supported conversations this year."



Okay.  So now I want to share Apple's description of the design of PQ3.  It includes a bunch of interesting details and also something that Telegram has never had, which is multiple formal proofs of correctness.  Since we're now able to do this, it's a crucial step for trusting any newly created cryptographic system.  Here's what Apple wrote.



They said:  "More than simply replacing an existing cryptographic algorithm with a new one, we rebuilt the iMessage cryptographic protocol from the ground up to advance the state of the art in end-to-end encryption, and to deliver on the following requirements."  Five of them.  "Introduce post-quantum cryptography from the start of a conversation, so that all communication is protected from current and future adversaries.  Two, mitigate the impact of key compromises by limiting how many past and future messages can be decrypted with a single compromised key.  Three, use a hybrid design to combine new post-quantum algorithms with current Elliptic Curve algorithms" - which Signal has already done - "ensuring that PQ3 can never be less safe than the existing classical protocol.  Four, amortize message size to avoid excessive additional overhead from the added security."  And as we'll see, there really is some.  "And five, use formal verification methods to provide strong security assurances for the new protocol."



Okay.  They say:  "PQ3 introduces a new post-quantum encryption key in the set of public keys each device generates locally and transmits to Apple servers as part of iMessage registration."  So devices generate a post-quantum key on device, store it in the Secure Enclave, and then send the public part of that to Apple.  "For this application, we chose to use Kyber post-quantum public keys, an algorithm that received close scrutiny from the global cryptographic community and was selected by NIST as the Module Lattice-based Key Encapsulation Mechanism standard, or ML-KEM.  This establishes sender devices to obtain a receiver's public keys and generate post-quantum encryption keys for the very first message, even if the receiver is offline."  Because, you know, Apple has the matching public key, which is all they need to perform the handshake.  They said:  "We refer to this as initial key establishment."



They said:  "We then include, within conversations, a periodic post-quantum rekeying mechanism that has the ability to self-heal from key compromise" - which that's their fancy term for just moving forward - "and protect future messages.  In PQ3, the new keys sent along with the conversation are used to create fresh message encryption keys that cannot be computed from past ones, thereby bringing the conversation back to a secure state, even if previous keys were extracted or compromised by an adversary.  PQ3 is the first large-scale cryptographic messaging protocol to introduce this novel post-quantum rekeying property."



LEO:  [Buzzer sound]



STEVE:  And I don't - yes.



LEO:  [Buzzer sound]



STEVE:  Exactly.  This is where we need to hear from Signal because, eh, maybe not.  Maybe not ever since Signal added post-quantum to their system.  They say:  "PQ3 employs a hybrid design that combines Elliptic Curve crypto with post-quantum encryption, both during the initial key establishment and during rekeying.  Thus, the new cryptography is purely additive" - which is what Signal did - "and defeating PQ3 security requires defeating both the existing classical ECC crypto and the new post-quantum primitives.  It also means the protocol benefits from all the experience we accumulated from deploying the ECC protocol and its implementations," even though they also said they redesigned it from scratch, so maybe it actually doesn't apply.  But it makes nice marketing speak.



"Rekeying in PQ3 involves" - oh, yeah.  No, it's the next piece I want to get to.  But they said:  "Rekeying in PQ3 involves transmitting fresh public key material in-band with the encrypted messages that devices are exchanging.  The new public key based on Elliptic Curve Diffie-Hellman (ECDH) is transmitted inline with every response.  The post-quantum key used by PQ3 has a significantly larger size" - oh, and does it ever, 2K, we'll talk about that in a minute - "than the existing protocol, so to meet our message size requirements we designed the quantum-secure rekeying to happen periodically rather than with every message.  To determine whether a new post-quantum key is transmitted, PQ3 uses a rekeying condition that aims to balance the average size of messages on the wire, preserve the user experience in limited connectivity scenarios, and keep the global volume of messages within the capacity of our server infrastructure."  Which is all a fancy way of saying we send it a little bit at a time.



Anyway, I'll just interrupt here also to note that it seems likely that PQ3 is rotating its quantum keys more frequently than Signal.  I don't recall the details of Signal's ratchet, and it may have changed since we last looked at it.  But it might also be that this is a distinction without a difference.  In the case of Signal's ratchet, it was designed, not only to provide useful forward secrecy, but as a means for resynchronizing offline asynchronous end points.



The reason I suggest that it may be a distinction without a difference is that these key compromises are purely what-ifs.  No one knows of any scenario where that could actually happen.  The only reason Apple is mentioning it is that they have a way of sidestepping this as a problem, so now it's a problem.  Okay.  You know, if anyone did have a way of sidestepping it, then this problem, you know, the problem would be eliminated.  You know, it's a bit like saying "My password is way stronger than yours because it's 200 characters long."  Okay, that's good, but does it really matter? 



Anyway, Apple continues:  "With PQ3," they say, "iMessage continues to rely on classical cryptographic algorithms to authenticate the sender and verify the Contact Key Verification account key because these mechanisms cannot be attacked retroactively with future quantum computers."  In other words, they didn't bother upgrading that to post-quantum because they're not quantum vulnerable, in the same way that hashes are not.



"To attempt to insert themselves in the middle of an iMessage conversation, an adversary would require a quantum computer capable of breaking one of the authentication keys before or at the time the communication takes place.  In other words, these attacks cannot be performed in a Harvest Now, Decrypt Later scenario.  They require the existence of a quantum computer capable of performing the attacks contemporaneously with the communication being attacked.  We believe any such capability is still many years away.  But as the threat of quantum computers evolves, we will continue to assess the need for post-quantum authentication to thwart such attacks.



"Our final requirement for iMessage PQ3 is formal verification, a mathematical proof of the intended security properties of the protocol.  PQ3 received extensive review from Apple's own multi-disciplinary teams in Security Engineering and Architecture, as well as from some of the world's foremost experts in cryptography.  This includes a team led by Professor David Basin, head of the Information Security Group at ETH Zurich and one of the inventors of Tamarin, a leading security protocol verification tool that was also used to evaluate PQ3; as well as Professor Douglas Stebila from the University of Waterloo, who has performed extensive research on post-quantum security for Internet protocols.



"Each took a different but complementary approach, using different mathematical models to demonstrate that as long as the underlying cryptographic algorithms" - and they actually meant, yeah, okay, right, algorithms, like the post-quantum NIST-approved algorithms themselves.  "As long as the underlying algorithms remain secure, so does PQ3."  In other words, the protocols built on top of those algorithms will also be secure.  They said:  "Finally, a leading third-party security consultancy supplemented our internal implementation review with an independent assessment of the PQ3 source code, which found no security issues.



"In the first mathematical security analysis of the iMessage PQ3 protocol, Professor Douglas Stebila focused on so-called game-based proofs.  This technique, also known as reduction, defines a series of 'games' or logical statements to show that the protocol is at least as strong as the algorithms that underpin it.  Stebila's analysis shows that PQ3 provides confidentiality even in the presence of some compromises against both classical and quantum adversaries, in both the initial key establishment and the ongoing rekeying phase of the protocol.



"The analysis decomposes the many layers of key derivations down to the message keys and proves that, for an attacker, they are indistinguishable from random noise.  Through an extensive demonstration that considers different attack paths for classical and quantum attackers in the proofs, Stebila shows that the keys used for PQ3 are secure as long as either the Elliptic Curve Diffie-Hellman problem remains hard or the Kyber post-quantum KEM remains secure."



And Apple then inserts a quote from Professor Douglas Stebila, which reads:  "The iMessage PQ3 protocol is a well-designed cryptographic protocol for secure messaging that uses state-of-the-art techniques for end-to-end encrypted communication.  In my analysis using the reductionist security methodology, I confirmed that the PQ3 protocol provides post-quantum confidentiality, which can give users confidence in the privacy of their communication even in the face of potential improvements in quantum computing technology.  Signed, Professor Douglas Stebila."



LEO:  It would have been much better in a German accent.  I'm just saying.  You really want people to believe a scientist, do it in a German accent.



STEVE:  That's true.  That's true.  And then Apple continues:  "In the second evaluation, titled 'A Formal Analysis of the iMessage PQ3 Messaging Protocol,' Professor David Basin, Felix Linker, and Dr. Ralf Sasse at ETH Zurich..."



LEO:  Ah, now we are talking here.  We've got some real German scientists.  Swiss is even better.



STEVE:  Okay.  Well, in that case you're going to read the quote when we get to it in a minute.  "...[U]se a method called 'symbolic evaluation.'  As highlighted in the paper's abstract," writes Apple, "this analysis includes a detailed formal model of the iMessage PQ3 protocol, a precise specification of its fine-grained security properties, and machine-checked proofs using the state-of-the-art symbolic Tamarin prover."



LEO:  Oh-ho.



STEVE:  Yeah, you've got to have that.  "The evaluation yielded a fine-grained analysis of the secrecy properties of PQ3, proving that 'In the absence of the sender or recipient being compromised, all keys and messages transmitted are secret,' and that 'Compromises can be tolerated in a well-defined sense where the effect of the compromise on the secrecy of data is limited in time and effect,' which confirms," writes Apple, "that PQ3 meets our goals."  And they quote Professor Basin.  Leo, take it away.



LEO:  "We provide a mathematical model of PQ3 as well as prove its secrecy and authenticity properties using a verification tool for machine-checked security proofs.  We prove the properties even when the protocol operates in the presence of very strong adversaries who can corrupt parties or possess quantum computers and therefore defeat classical cryptography.  PQ3 goes beyond Signal with regards to post-quantum defenses.  In PQ3, a post-quantum secure algorithm is part of the ratcheting and used repeatedly, over and over, rather than only once in the initialization as in Signal.  Our verification provides a very high degree of assurance that the protocol as designed functions securely, even in the post-quantum world."  Doesn't that sound more credible?



STEVE:  Oh, Professor.



LEO:  Don't you believe that?



STEVE:  Professor.  Now, what I found disturbing about the quote, not Leo's rendition...



LEO:  Well, that's disturbing in its own right.



STEVE:  They had the professor say "PQ3 goes beyond Signal with regards to post-quantum defenses."



LEO:  Yeah, that's really interesting.



STEVE:  The dig at Signal was entirely unnecessary and gratuitous.  And I don't understand why Apple apparently feels so threatened by Signal.  Oh, wait, yes, I do.  Signal is open, open design, open source, and entirely cross-platform.  Anyone's conversations can be protected by Signal on any platform they choose, whereas iMessage, just like everything else Apple does, is all about platform lock-in.



So is PQ3 any reason to choose iMessage over Signal?  No.  When we're talking about cryptography, we've learned that there's nothing wrong with adding a belt to those suspenders.  After all, that's why Apple copied Signal in adding post-quantum crypto to existing and well-proven pre-quantum crypto.  Belt and suspenders.  And so, if your work model allows you to be stuck within Apple's closed ecosystem, then you can be confident that iMessage will be secure against any current and future surprises.  I have no doubt that Apple did all of this right.  But you'll certainly be secure enough using Signal, and you'll have the benefit of also having far more freedom.



LEO:  And the ability to talk to the rest of the world.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  In a secure fashion.  Finally, the paper drops into lots of interesting detail that I won't drag everyone through since we already have the essence of what's going on.  But I did want to share one piece of the techie bits since I think it's interesting regarding the overhead that Apple has introduced by their more or less continuous rekeying.  And actually we find out how continuous.  So keep in mind that text messages are often no longer than old school SMS tweets, or shorter.  You know, sometimes just a few words; right?



Apple explains:  "To limit the size overhead incurred by frequent rekeying while preserving a high level of security, the post-quantum KEM is instantiated with Kyber768.  Unlike the IDS-registered public keys used for the initial key establishment, ratcheting public keys are used only once to encapsulate a shared secret to the receiver, significantly limiting the impact of the compromise of a single key.  However, while a 32-byte Elliptic Curve Diffie-Hellman-based ratchet overhead is acceptable on every message, the post-quantum KEM ratchet increases the message size by more than two kilobytes."



LEO:  Ooh.



STEVE:  Yeah.



LEO:  2,000 characters in English, in ASCII.



STEVE:  Yes.



LEO:  That's lots.



STEVE:  In an "Okay, thanks Mom, I'll be there soon" message.



LEO:  Yeah.



STEVE:  "To avoid visible delays in message delivery when device connectivity is limited, this ratchet needs to be amortized over multiple messages."  In other words, stretched out.



LEO:  Spread out, yeah.



STEVE:  Yeah.  "We therefore implemented an adaptive post-quantum rekeying criterion that takes into account the number of outgoing messages, the time elapsed since last rekeying, the current connectivity conditions.  At launch, this means the post-quantum ratchet is performed approximately every 50, five zero, messages; but the criterion is bounded such that rekeying is always guaranteed to occur at least once every seven days, so at least weekly.  And as we mentioned earlier, as the threat of quantum computers and infrastructure capacity evolves over time, future software updates can increase the rekeying frequency while preserving full backward compatibility."



Okay.  So now everyone knows as much about PQ3 as is necessary.  Apple has followed Signal by adding a believed-to-be-strong post-quantum crypto algorithm to their built-in iMessaging platform, which will be arriving with iOS's next major update.  And if you're talking to somebody who also has been able to upgrade their device to 17.4, then your conversation will be post-quantum encrypted.  Apple has also taken the welcome step of having their largely new iMessaging protocol formally proven by highly qualified academic algorithm researchers with wonderful accents.  It's only a bit sad that Apple clearly feels so threatened by Signal.



LEO:  Celebrity accents impersonated.  Actually, the most important thing is this is Kyber.  All three I think of the NIST protocols are Kyber, as well; right?  There was one that failed.



STEVE:  Yeah, one was like in consideration, and...



LEO:  It crashed.



STEVE:  Problems were found, yeah.



LEO:  Have these been approved yet?  Or is NIST still testing?



STEVE:  We're still - we're in the late phases of this.



LEO:  Okay.



STEVE:  So, I mean, again, everyone is being really careful because this is where you want to be careful.  This is where, you know, care matters.



LEO:  So they're using Kyber768 with perfect forward secrecy.  Those are the two things that probably, you know, should matter to you.  Whether they're better than Signal is a pretty big and loaded question, obviously.  It's a marketing question, I think.



STEVE:  Yeah.  And I'll be surprised after this if we don't hear something from Signal, you know, in their own well-crafted accent.



LEO:  Yeah.  They don't, I mean, honestly, they don't need to prove anything.



STEVE:  No.



LEO:  But it would probably be appropriate for them to say, hey, you know, just so you know, we're not PQ2.  We provide exactly as good protection as Apple does.  And you shouldn't, you know, consider us anything less, despite what Apple may say.



STEVE:  Right.



LEO:  Now, all of this is presuming that at some point we'll get quantum computing, which is still a long shot.  I mean, it's not - we're not close by any means.



STEVE:  Yes.  The only - at this point what everyone is doing is considering the harvest now, decrypt later scenario.



LEO:  Right.



STEVE:  They're wanting to move us into crypto, I mean, it's like, why not?  We've got the computing power.  We've got the bandwidth.  You know, obviously the keys are much longer in order to be as secure as we need them to be.



LEO:  Right.



STEVE:  So there is some cost.  But it's cost we can now afford.  So let's move the entire world now so that, you know, the NSA server farm in Utah can have its next expansion and continue storing stuff, but probably unable to decrypt it once quantum computing does happen.



LEO:  You know, you might wonder, well, when is - so we're talking, yeah, in 20, 30 years, if they still have your messages, and if you still care, absent any kind of post-quantum crypto, they might be able to crack it.  But that's pretty much a long way off.  It's not just around the corner.



STEVE:  Yeah, I mean, we have to imagine that what they're storing are not people saying they'll be home soon for dinner.



LEO:  Right.



STEVE:  You know, that they're storing...



LEO:  Well, they are storing that.  They're storing everything.  That's the thing.



STEVE:  They're storing that, too.  But also, you know, China's conversations with their advanced persistent threat groups and  where the money is being wired in order to fund them and all that.  I mean, all of that that is not yet post-quantum encrypted is, you know, [crosstalk].



LEO:  It's hard to imagine anything, though, that would be decrypted in a few decades that would have anything other than historical interest; right?



STEVE:  I agree.



LEO:  Anyway, it's, you know what, you can do it, we've got the means, why not go ahead and do it.



STEVE:  Yeah, it's like Microsoft publishing the source for MS-DOS finally.  It's like, okay.



LEO:  Thanks.



STEVE:  Does anyone care?  Yeah, thanks a lot.



LEO:  You might care.  But other than that, yeah.  No, and I appreciate you kind of addressing the marketing kind of fluff in here from the technological point of view because I think it is.  It doesn't - it's not needed.  It wasn't necessary, Apple.



STEVE:  I was disappointed.  It's like, wait a minute; you know?  They have a ratchet.  And Signal hasn't told us how often they deploy it.



LEO:  Right.



STEVE:  But, you know, again, nobody doesn't think that Signal is secure.  Everybody knows that Signal, I mean, it's the standard.



LEO:  It's the gold standard.



STEVE:  Yes.



LEO:  I think that's why Apple's attacking them, frankly.  They're the standard.  They're the gold standard.  And there's nothing in here that makes them any less of a gold standard for good post-quantum crypto.



STEVE:  Because Apple went to all this work - it's funny, Leo, I was also thinking about the goggles, whatever the hell they are, you know, where - and I guess I listened to you on Sunday.  I got this - I came away with a clear sense that Apple had gone too far.



LEO:  Yeah.



STEVE:  That they really, they far over-engineered those things.  And I'm beginning to wonder maybe if that's what Apple has become is like a way over-engineering company, because you could argue that there is some cost to mistakenly or needlessly over-engineer things.  And the very fact that they're having to amortize their own post-quantum rekeying over at least 50 messages in order not to bog it down too much suggests that, you know, that this was purely a marketing point.



LEO:  Wow.  Good point, yeah.  But it may be more than just marketing.  It may also be a message to governments, especially China and Russia...



STEVE:  Don't bother.



LEO:  ...but also the U.S.  Yeah.  No, no, we're committed to end-to-end encryption, so knock it off.  I think there's some of that there, as well.  And for that I applaud them.  I think they're doing...



STEVE:  That we're not softening anything.



LEO:  We're going, quite the opposite, we're going the other direction.



STEVE:  Yeah.



LEO:  Yeah.  All right.  Hey, Steve, thank you very much.  I have one more thing I would say is I think Telegram is encrypted, but you have to choose it.  I don't think all Telegram messages are encrypted, at least that's how it used to be.



STEVE:  Yes.  And I agree with you.  I don't think that Apple is lying about saying that it's not encrypted by default.



LEO:  By default is the thing, yeah.



STEVE:  But who doesn't turn that on?  And so it's like, you know...



LEO:  You have to choose an encrypted message, yeah.



STEVE:  Yeah.  And in fact I would argue that having Level 0 is dumb because you either have end-to-end encryption or you don't.



LEO:  Or you don't; right.



STEVE:  So, you know, why have zero and put some people in that category?  Oh, you have to turn it on.  Okay, so everyone does.  It's the first thing they do...



LEO:  Right.



STEVE:  ...when they load it is they turn on the encryption.



LEO:  Yeah, yeah.



STEVE:  Now I'm sure [indiscernible] up, how would you like to be encrypted?  Uh, okay.



LEO:  Okay.  Oh, yeah.



STEVE:  Good idea.  I think I do.



LEO:  And then what's for dinner?



STEVE:  Why do you think I got this dumb thing?



LEO:  Steve Gibson, GRC.com.  That's the place to go.





Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#965

DATE:		March 12, 2024

TITLE:		Passkeys vs 2FA

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-965.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What happened with CERT?  What headache has VMware been dealing with?  What's Microsoft's latest vulnerability disclosure strategy?  What's China's "Document 79," and is it any surprise?  What long-awaited new feature is in version 7.0 of Signal?  How is Meta coping with the EU's new Digital Marketing Act that just went into effect?  What's the latest on that devastating ransomware attack on Change Healthcare?  And after addressing some interesting feedback from our listeners, I want to clarify something about Passkeys that is not at all obvious.



SHOW TEASE:  Hey, I'm Mikah Sargent, subbing in for Leo Laporte.  Coming up on Security Now!, first we follow-up on what happened with CERT.  Yes, the listener who talked about a huge serious security flaw in the website of a major enterprise has some follow-up on speaking to the vulnerability analysis team at CERT.  Then we talk about what VMware is dealing with; what Microsoft is choosing to do when it comes to vulnerability disclosure.  Here's a hint.  They're kind of waiting until the end of the week to tell people what's going on.  Plus China ditching America, at least in terms of its technology.  And easily my favorite part of the show, Steve Gibson explains why Passkeys are "far more secure" than any super-strong password plus any second factor.  All of that coming up on Security Now!.



MIKAH SARGENT:  This is Security Now! Episode 965 with Steve Gibson and Mikah Sargent, recorded Tuesday, March 12th, 2024:  Passkeys vs 2FA.



Hello and welcome to Security Now!.  As you can probably tell by listening, this is not Leo Laporte.  No, it is Mikah Sargent who is filling in for Leo Laporte while he is on vacation.  But across from me, at least in the way of the Internet, is the tried-and-true Steve Gibson, who is here as he always is.  Steve, thank you for always being here.



STEVE GIBSON:  And you know, I've noticed that Leo actually, like you are right now, actually looks at the screen in order to, like, make it look like you're looking at me.  So I appreciate that.  And sometimes when Leo's heading off from the main, well, the studio that you're in at the end of MacBreak Weekly where he is, heading to his office, I'll deliberately look this way.



MIKAH:  Just to watch him go.	



STEVE:  Oh, see you, Leo.  That's right.



MIKAH:  Oh, I like that.  I like that.



STEVE:  Anyway.



MIKAH:  It's good to see you.	



STEVE:  Likewise, and thank you for standing in for Leo.  He's on a beach somewhere.  He says he will come back with a tan.  So that's good.  We'll hope no skin cancer.



MIKAH:  Yes, exactly, a safe tan.



STEVE:  At our age, one considers the downside of being too brown.  So anyway, I had said, had suspected, or planned, to title today's podcast "Morris II" after the sort of, well, it was named that way by the guys who created this thing.  It's a way they found of abusing GenAI to create an Internet worm.  And of course the Morris worm was the very first worm on the Internet and is famous for that reason.



Anyway, something came up, as sometimes happens, and so we'll be talking about Morris II next week unless something else comes up that pushes it a little bit further downstream.  And this is the result of a listener's question, which caught me a little bit by surprise because I understand this stuff because I live in it, this being across the network authentication.  And I thought, okay, I realized that his question was a really good one, and it had a really good answer.  And so first I put it as the first of our listener feedback questions.  But as I began to evolve the answer, I thought, no, no, no, okay, this just has to be something we really focus on because it's an important point.



So today's podcast number 965 for March 12th is titled "Passkeys vs 2FA."  And I would bet that even for people who think they really understand all this, there may be some nuances that have been missed.  So I think it's going to be a great and interesting and useful podcast for everyone.  But of course that's at the end.



We're going to start with whatever happened with that guy who complained to CERT about a vulnerability he found in a major website.  What headache has VMware been dealing with just the last few days?  What's Microsoft's latest vulnerability disclosure strategy?  And why does it, well, suck?  What's China's "Document 79" all about, and is it any surprise?  What long-awaited new feature is in version 7.0 of Signal, currently in beta, but coming out soon?  How is Meta coping with the EU's new Digital Marketing Act that just went into effect and requires its messaging platforms to be interoperable with others?  Whoops.



Also, what's the latest on that devastating ransomware attack on Change Healthcare?  Many of our listeners have said, hey, Steve, did I miss you talking about that, or haven't you?  I haven't, so it's time because now we have a lot of information about it.  And, you know, as I said, after addressing a lot of interesting feedback from our listeners which we're also going to make time for, we're going to talk about a few things about Passkeys that are actually not at all obvious.  And of course we always have a great Picture of the Week.  So I think overall a great podcast for our listeners.



MIKAH:  It is time for the Picture of the Week.



STEVE:  Okay.  So as I said, this relates to one of our more famous earlier pictures of the week where there was a big AC generator sitting on a factory floor somewhere, and next to it was a pail, a large pail of dirt.  And a rod was stuck into the dirt to which was attached a big ground wire.  And really, it just demonstrates a complete lack of understanding of the nature of grounding something.  I mean, it's not like, I mean, the dirt in this first picture, the earlier picture, was in a plastic pail.  So it wasn't connected to the earth, which is really what we're wanting for a ground to be.



Anyway, here we have an updated version of the same thing.  It looks like some electrical wiring in progress.  It's not all finished yet.  The construction, whereas most typical electrical high-power wiring are in steel conduit and steel boxes, this is all white plastic.  So that suggests that the need for grounding is even more imperative than it would be if this was all in steel, which is probably grounded somewhere anyway, but not here.  So for reasons that are unclear, a green wire is coming out of this and going into a plastic bag which has been suspended from a plastic piece of conduit.  And this bag, sure enough, it's got about an inch worth of brown dirt at the bottom of it, and the green wire has been certain to, like, get all the way down to the bottom of the bag and bury itself in the dirt.  And I gave this thing, I gave this picture the caption "It's certainly a good thing that bag of dirt was labeled with a 'ground' symbol, or the dirt's purpose might have been unclear."



MIKAH:  Yeah.



STEVE:  We see, like, the equivalent of what looks like a brown Post-it note, but somebody went out of their way to draw a very pretty ground symbol.



MIKAH:  It's very good, yeah.



STEVE:  I mean, because electricity and electronics was my first passion, I've drawn my share of ground symbols in my life, and I would have to say this is right up there with the best of them.  So yes, we have a - again, it's like, who...



MIKAH:  Who, what, when, where, and why?



STEVE:  Would you trust the wiring of an electrician who did this?  I mean, I don't know.  It just, it really - we have some pictures that really do beg the question, what happened here?  Anyway...



MIKAH:  It's just question marks all around.  It's fantastic, but awful.  Oh, dear.



STEVE:  Okay.  So recall from last week our listener who discovered a serious flaw in the website of a major enterprise whose site entertains millions, hundreds of millions of users.  And in fact since then as a consequence of what he forwarded to me, and which I redacted, I now know who that site is and what that enterprise is.  And wow.  So the problem was that these people were using a very outdated version of the nginx web server which contained a well-known critical remote code execution vulnerability for which working Python proof-of-concept code was readily available.



Which all means that any bad guy who went to any of this company's various websites, and there are several, who then looked at the headers of the response from the website, which would identify the server as nginx and version, could then, as I did, and as our listener did, google that version of nginx and see, whoa, there are some problems with that version from four years ago.  And that person would be able to find the proof-of-concept code, as I did, and use it, which neither our listener nor I did, in order to get inside, crawl inside the web server.



Anyway, serious problem with this major enterprise and their website.  And I'll just say that this enterprise is - it's not like the website happens to be a side-effect feature of this enterprise.  The nature of the enterprise is such that it is the website.



MIKAH:  Oh.



STEVE:  I mean, like, its entire purpose and existence on the planet is this website and others that it also owns.



MIKAH:  So to be clear, it's not as if the website is just a place where you go to learn about its features.  The service is the site that you're going to.  It's not just a pretty little site.



STEVE:  Yes.  It is not Google, but it would be as if - it would be like Google having a problem of this magnitude with its servers.  I mean, you know, its purpose is its site, essentially.  And so it's a big deal.  Now, our listener, as I described last week, gave this company 120 days to, like, deal with it.  He contacted them.  They replied, "Okay, thank you for notifying us.  We'll look into this and get back to you."  Oh, and they said, "And it may take four months for that to happen."  So he waited until 118 days and then said, uh, knock knock.



MIKAH:  You've got two days here.



STEVE:  What's going on?  And they said, oh, oh, right.  Well, let's give it a couple more days.  So they came back and said, well, we've asked our security people, and they're not really thinking that's a big problem.  How about this?  We'll give you 350 bucks to just shut up.



MIKAH:  What?



STEVE:  And go away.



MIKAH:  What?



STEVE:  So at that point he tweeted me, and he said, "Steve, what do you think about this?"  And I said, you know, you've done everything you can.  They're not being very cooperative.  Why don't you send a report to CISA?  Which is, you know, the U.S.'s front facility for dealing with this kind of stuff.  So he thought, okay, that's a good idea, so submitted a report along with his entire email chain, his whole dialogue with these guys, to CISA, which forwards it to CERT.



Okay.  Well, he sent me a tweet updating me on what was going on, and I have it in the show notes, after I removed the identification of the site that we're all talking about here.  And this comes back from CERT saying:  "Greetings.  Thank you for your vulnerability report submission.  After review, we've decided not to handle the case, for two reasons.  First, we typically avoid handling or publishing reports of vulnerabilities in live websites.



"Second, since the vendor is cooperating and communicating with you, there's very little additional action that we can contribute."  What?  Anyway, "We recommend working directly with the affected vendor before proceeding with public disclosure."  Well, okay.  He's never going to disclosure this publically.  This would be a disaster.  "Feel free to contact us again if you run into problems with your coordination effort, or if there is some other need for our involvement.  Regards."  And in like...



MIKAH:  Giant.



STEVE:  50-point type, yeah, "Vulnerability Analysis Team, at the CERT Coordination Center at kb.cert.org."  Okay, well, that's disappointing.  You know, they say:  "We typically avoid handling vulnerabilities in live websites."  So, what?  They limit their handling of vulnerabilities to dead websites?  You know, this doesn't inspire confidence.



MIKAH:  It feels like they didn't read the evidence that was provided.  This feels like when you go and you talk to any tech support person.



STEVE:  Yes.



MIKAH:  And they immediately jump to assumptions, and they don't hear you out first.



STEVE:  Yes.



MIKAH:  And then they're providing advice that you're going, but did you not just hear what I literally just - I don't like this.  Sorry.  Go ahead.



STEVE:  No.  No, and you're exactly right.  Our listener's correspondence, which he had forwarded to CERT with his report, made clear that the company had no intention of doing anything further.  So CERT was just passing the buck back to someone who had already demonstrated that he had no leverage.  I mean, and that was why I recommended these guys.  I figured if Uncle Sam knocks on the door and says, hey, you've got a little problem with your web server, and it would be bad if you got compromised, that then they might listen, because they're not listening to Joe.



So anyway, in my reply to him just now, since he had never disclosed anything publicly, and I know he never would, he's not a bad guy, I suggested that, since he had done all he could reasonably do - and again, never said anything publicly - he should take the money as compensation for his trouble and leave whatever happens up to fate.  You know, if the company does eventually get bitten, it will only be their fault.  He warned them.  He didn't, I mean, he even reminded them after four months.  And they said, oh, yeah, you.  Hmm.



MIKAH:  Not you again.  We thought you'd just disappear.  Why?



STEVE:  So,  yeah, anyway.



MIKAH:  Can I ask you something?  I would like to know when is it reasonable and, as you see it, reasonable and the right thing to do to disclose things publicly?  Not in this situation in particular.  I just mean - because I thought that that was something that some security researchers end up doing is, if the company continues not to behave, or it continues to not work with them, isn't a public disclosure part of the bargaining power that the security researcher has?



STEVE:  Yeah, famously.  Like Google, for example.  You know, they find a problem, they notify the vendor, and they say, "We have started a 90-day clock, and you need to fix this sucker."  So if you don't, we're going, well, or whether or not you do, we're going to go public with this in 90 days.  So take it seriously.  Because unfortunately, this policy is the result of experiences just like this one, where companies are like, eh, you know, we don't think it's that big a problem.  Okay, great.  We're going to let everybody know about it if it's not that big a problem.  Whoa, whoa, whoa, whoa.  Hold on.  Hold on.  Wait a minute.  So this guy's not Google.  He can't do that.  And of course Google also has lord knows what kind of a bank of attorneys they have that are able to...



MIKAH:  To protect themselves, got it, yeah.



STEVE:  ...protect themselves.  Unlike this guy, who the last thing he wants is to get stomped on by a massive enterprise that just turns one of their attorneys loose and says let's go make this guy's life miserable.



MIKAH:  So that's the danger there.  That makes sense.



STEVE:  Yeah.  So first of all, I mean, there have been instances where even Google has not followed their own policy when what they have found has been so egregious that, like, they can't in good conscience let this be known publicly, even though they really want to, because it would just collapse the world.



MIKAH:  Right.



STEVE:  So, you know, you can lead the company to water.  You can say, "Please fix this, won't you?  We all want you to.  The world will be a safer place."  But ultimately, if they say no, it's like, okay, well, you called our bluff because we're not going to tell everybody because that would be really bad.  So good luck.  And at least we're going to protect ourselves from you.  In fact, this guy was worried that the information this company has about him, because he's a subscriber or a member or whatever it is, that that would be at risk.  And he asked them in his correspondence, like, what are you doing about my privacy concerns because you've got an insecure website, and you know a lot about me.  And they never ever said anything about that.  So again, you know, I guess maybe he could contact Google.



MIKAH:  That would be great.



STEVE:  That might be an idea, actually, is contact their threat people and say, hey, what do you think about this?  If you think it's bad, maybe you ought to give them a 90-day countdown and get this thing really fixed.



MIKAH:  Yeah, that is...



STEVE:  Anyway, it was a little - it was a disappointing response, though, from our own government because this is not a company you want to have attacked.  And in fact, speaking of which, we'll be talking about Change Healthcare here in a while.  Talk about a company you don't want to have attacked.  Oh, my lord.  One in three of the nation's health records were compromised in this.



MIKAH:  Love that.  Love that.  That's so great.



STEVE:  What could possibly go wrong?  Speaking of which, a week ago, on March 5th, Broadcom, which is VMware's parent company now after they bought them, issued a security advisory that happens to be VMSA-2024-0006.  And it's encouraging that it's got a one-digit number on it.  You know, 0006, that's nice because now, you know, these days the CVEs have had to expand to six digits.  Well, no, actually five.  But they used to be four.  Just because there are so many problems happening.



Okay.  So this security advisory addresses vulnerabilities that have been discovered in VMware ESXi, VMware Workstation Pro and Player, and Fusion.  In other words, pretty much everything.  If it's got VMware in its name, we've got a problem.  This is because of a problem in the ubiquitous USB virtualization drivers, which have been found to contain four critical flaws.  I didn't dig in to find out who found them, whether they found them, somebody else found them and told them, or what.  But they are so bad that VMware has even issued patches for previous out-of-service-life releases of anything with VMware in its name.  An attacker who has privileged access in a guest OS virtual machine, so a root or admin, may exploit these vulnerabilities to break out of the virtualization sandbox, which of course is the whole point of virtualization, to access the underlying VMware hypervisor.



Patching VMware of any kind immediately is the optimal solution.  But due to the location of the problem, which is just a specific driver, if anything prevented that from being done, and if your environment might be at risk because it includes untrusted VM users, the removal of VMware's USB controllers from those virtual machines will prevent exploitation until the patches can be applied.  VMware said that the prospect of a hypervisor escape warranted an immediate response under the company's, what they call their, for some reason, IT Infrastructure Library, or ITIL.  They said:  "In ITIL terms, this situation qualifies as an emergency change, necessitating prompt action from your organization."



So both the UHCI and XHCI USB controller drivers contain exploitable use-after-free vulnerabilities, each having a maximum severity rating of 9.3 for Workstation and Fusion and a base score of 8.4 for ESXi.  There's also an information disclosure vulnerability in the UHCI USB controller, with a CVSS of 7.1.  Somebody who has admin access to a virtual machine could exploit it, that is, this other vulnerability, to leak memory from the vmx process.  Anyway, I've included a link in the show notes to VMware's FAQ about this for anyone who might be affected and interested in more details.



Basically, you know, if you've got VMware in your world, where somebody running in a virtual machine might be hostile, that's to be taken seriously.  If you're just a person at home with VMware because you like to run Win7 in a VM or whatever, then this is not to worry about.  Update it, you know, when you get around to it.  But anyway, VMware is on alert because of course it is in use in many cloud infrastructures where, you know, you don't know who your tenants are.  So, important to get that fixed.



So unfortunately, Microsoft's networks, and apparently their customers, remain under attack to this day by that Russian-backed Midnight Blizzard group.  And remember that they were originally named Nobelium, and Microsoft said, oh, let's call it Midnight Blizzard because that sounds scarier, I guess.  This is the group that successfully infiltrated some of Microsoft's top corporate executives' email by attacking the security of a system kind of that had been forgotten, it was in the back room somewhere, and it had not been updated to the latest state-of-the-art security and authentication standards.  And so, you know, they said, whoops, we forgot to fix that.



So the sad thing here, the saddest thing of all, I think, is that Microsoft has apparently, believe it or not, now taken to dropping their public relations bombs late on Fridays, as happened again this past Friday, and not for the first time.  This is now their pattern.  The Risky Business security newsletter explained things by writing this.  They said:  "Microsoft says that Russian state-sponsored hackers successfully gained access to some of its internal systems and source code [whoops] repositories.  The intrusions are the latest part of a security breach that began in November of last year and which Microsoft first disclosed in mid-January.



"Initially, the company said hackers breached corporate email servers and stole inboxes from the company's senior leadership, legal, and cybersecurity teams.  In an update on the same incident posted late Friday afternoon - as is the practice of every respectable corporation..."



MIKAH:  That's sarcasm, for those who are...



STEVE:  That's sarcasm, by the way.  "Microsoft says it found new evidence over the past weeks that the Russian hackers were now weaponizing the stolen information."



MIKAH:  Great.  Love that.



STEVE:  Uh-huh.  "The Redmond-based giant initially attributed the attack to Midnight Blizzard, a group also known as Nobelium" - that's right, but before Microsoft gave them a scarier name - "one of the cyber units inside Russia's Foreign Intelligence Service.  Microsoft says that since exposing the intrusion, Midnight Blizzard has increased its activity against its systems."  Well, and how.  "Per the company's blog post, Midnight Blizzard password sprays increased in February by a factor of 10, compared to the 'already large volume it saw in January of 2024.'



"Furthermore," Risky Business notes, "if we read between the lines, the group is now also targeting Microsoft customers.  While Microsoft's legalese makes it clear that no customer-facing systems were compromised, the company weaselly confirmed that customers have been 'targeted.'"  Okay, what does that mean?  "Emails stolen by the Russian group also contained infrastructure secrets, you know, secret tokens, passwords, API keys, et cetera, which Midnight Blizzard has been attempting to use."



So it's not good that Russians managed to compromise Microsoft executives' email in the first place, and that the information they stole is now being leveraged to facilitate additional intrusions, including apparently the exfiltration of Microsoft's source code repositories.  That's not good for anyone.  But as we know, I always draw a clear distinction between mistakes and policies.  Mistakes happen.  Certainly a big one happened here.  But the new policy that caught my eye was that, unfortunately, we are now seeing a pattern develop of late Friday afternoon disclosures of information which Microsoft hopes will be picked up and published when fewer people are paying attention.  So that's...



MIKAH:  So, yeah.  Having worked in journalism for ages, that is, well, ages.  For a long time.



STEVE:  Yes, how old are you exactly?  How many ages ago?



MIKAH:  One age, at least.



STEVE:  Okay.  An age.



MIKAH:  But, yes, it is - that's something that we always pay attention to.  When is that coming out?  And what is the messaging behind there?  And the fact that they're waiting until Friday afternoon is a clear signal that they don't want people to see it.  And that is wrong when it comes to security disclosures.  And luckily you've got your eye on it, and that means we can get to it on Tuesday.  You can let it go on Friday, Microsoft, but we'll pick it up on Tuesday.



STEVE:  Well, yes.  And it's also relevant that this was not like any kind of an emergency announcement that had to go out on Friday afternoon.  I mean, this was just, like, well, we're updating you on the status, but we'd rather you didn't receive this.



MIKAH:  Yeah, exactly.



STEVE:  So here it is.  Right.  Okay.  So the official name for Beijing's secret directive is "Document 79," but it is informally known as the "Delete A" order where "A" is understood to stand for America.  Yes, China wants to delete America.  The secret directive was issued several years ago, back in September of 2022.  It's designed to remove American and other non-Chinese hardware - although apparently since we get the "A," we figure prominently - and so to remove American and other non-Chinese hardware and software from its critical sectors.  The directive mandates that state-owned companies replace all U.S. and other non-Chinese equipment and software in their IT systems by 2027.  So you've got three years left, folks.  And that was a five-year order that began two years ago.  So far the affected companies include Cisco, IBM, Dell, Microsoft, and Oracle.  And my reaction to this is, yeah, who can blame them? 



MIKAH:  Yeah.  [Crosstalk].



STEVE:  You know?  Right.  With tensions on the rise between the U.S. and China, this only makes sense.  The U.S. has done the same thing for the manufacturers of Chinese-made security cameras within, like, sensitive areas like military bases and protected corridors of power.  And we know that Russia is working to remove American technological influence from inside its borders.  The problem for all the parties involved is that today's systems are so complex that trojan capabilities are readily hidden and can be made impossible to find.  No one doubts the influence that Chinese intelligence services are able to exert over the design of Chinese equipment.  And there has long been some question about just how much influence U.S. intelligence services might have over the installation of such backdoors in proprietary software.  Years and years ago there was a key inside of Windows that purely by coincidence had the initials NSA.  And it was like, [gasp], oh, my god.  You know, it's like, folks, if it was an NSA key, they would have changed the name.



MIKAH:  Yeah.  It would not be that obvious.



STEVE:  But it was like, oh, my goodness, I mean, just everyone was all atwitter, if you'll pardon the choice of words, over that the NSA had some secret key buried in Windows.  No.  But which is not to say they don't.  Like, why wouldn't they?  But they wouldn't leave the name the same.  So anyway, to my way of thinking, I just wish we could all get along.  You know, it's unfortunate to be seeing this pulling apart, you know, and this rising global tension because ultimately it is less economically efficient for us all to have to, like, retrench, and like we have to now be making chips in the U.S. when China was so good at it.  And, like, what's the problem?  Well, you know, it could be a problem.  So ultimately it's hardly surprising, but I just really regard it as unfortunate.



Okay.  And I'm keeping my eye on the time, but this is going to be a long podcast because we have a lot of good stuff to get to.  So I'm going to do one more before we take our next break.



MIKAH:  Sounds good.



STEVE:  Signal's reliance - Signal, you know, the messaging app, like the best one there is that is cross-platform.  I would argue that iMessage is good if you're in iOS world, but Signal if you need other stuff.  Their reliance upon physical phone numbers for identifying the communicating parties has been a longstanding annoyance, as well as a concern for privacy and security researchers who have long asked the company to switch from phone numbers to usernames in order to protect users' identities.  The just-announced version 7 of Signal will add the creation of temporary username aliases.



So here's how Signal explains it in their announcement.  They said:  "Signal's mission and sole focus is private communication.  For years, Signal has kept your messages private, your profile information like your name and profile photo private, your contacts private, and your groups private" - notice the word "private" figuring prominently in that single sentence - "among much else.  Now we're taking that one step further by making your phone number on Signal more private.



"Here's how:  If you use Signal, your phone number will no longer be visible to everyone you chat with by default."  And props to them for changing this.  Many organizations will add a feature, but they're afraid to change the behavior.  It's like, oh, well, you know, we don't want to, like, freak everyone out.  So we've got it now, but we're going to have it turned off, and you've got to turn it on if you want it.  No.  Signal says those phone numbers are going to disappear.  So get over it.



They said:  "People who already have your number saved in their phone's contacts will still see your phone number since they already know it.  If you don't want to hand out your phone number to chat with someone on Signal, you can now, as of 7" - which is in beta so it'll be available within a few weeks, they said - "you'll be able to create a unique username that you can use instead."  They said:  "You'll still need a phone number to sign up for Signal."  And they said:  "Note that a username is not the profile name that's displayed in chats.  It's not a permanent handle."  That's why I referred to it earlier as an alias.  And they said:  "And not visible to the people you are chatting with in Signal.  A username is simply a way to initiate contact on Signal without sharing your phone number.



"If you don't want people to be able to find you by searching for your phone number on Signal, you can now enable a new, optional privacy setting.  This means that unless people have your exact unique username, they will not be able to initiate a conversation, or even know that you have a Signal account - even if they do have your phone number.



"These options are in beta," they said, "and will be rolling out to everyone in the coming weeks.  And once these features reach everyone, both you and the people you're chatting with on Signal will need to be using the most updated version of the app to take advantage of them.  Also, all of this is optional.  While we changed the default to hide your phone number from people who don't already have it saved in their phone's contacts, you can change this setting.  You're not required to create a username, and you have full control over whether you want people to be able to find you by your phone number or not.  Whatever choices work for you and your friends, you'll still be able to communicate with your connections in Signal, past and present."



So that seems all good.  They've changed to hiding phone numbers by default, and they've added an optional textual username as a phone number alias.  Signal knows the phone number behind the alias, but users do not.  I think that's pretty slick.  Their blog posting about this contains a great deal more information.  Basically it's a complete user guide to how to work with this coming feature.  I've put it in this week's show notes since it might be of interest to anyone who uses Signal often.  So you can click the link in the show notes, and you'll be taken to their page.  It's phone-number-privacy-usernames.  So you can also probably google that, if you want to just jump right to it at Signal's site.  So anyway, nice improvement coming from Signal.



MIKAH:  VV in the Discord says:  "I set it up last week, and I love it.  This is such welcome news."  So we've already got some folks who have enabled these, you know, the new username feature.  Or as you pointed out or call it, an alias I think is a good way of putting it, given the - and it's clear that, you know, when you can tell, I think of when I used to post on X, formerly known as Twitter, in the past, and I would have a tweet, and I'd have about three things underneath it that are sort of responses to how all of the pedantic people on Twitter will probably take my post and say, "You didn't say this," or "You forgot to say that."



This feels like that.  They're answering every possible question that folks might have.  And as you said, there's a full user guide.  And I think that that's smart given their user base; right?  There are going to be lots of concerns about any change taking place and what it does, what it doesn't do.  So good on them, or good on it, I guess, for figuring out what needs to be said.



STEVE:  Yeah.  Basically they've created a mapping on top of the phone numbers, where, you know, a little dictionary where a user can assign themselves an alias and then say to somebody, hey, contact me on Signal at this username.  You know.  And it's going to be squirrelybumblebee or something, which is not their phone number, but it doesn't need to be.  It just needs to be something else.  And that allows the - so beneath the covers, Signal knows when somebody says, hey, I want to connect to squirrelybumblebee, that that actually means this phone number.  But the user making the connection never sees that.



MIKAH:  Yeah.  As you said, very slick.  I think that's a right way to do it.



STEVE:  Okay.  So last Thursday the 7th, the European Union's DMA, the Digital Markets Act, went into force.  Among many important features, it requires interoperability among specified, probably large, instant messaging systems.  But here's what Meta recently explained about their plans.  I'll give everybody a quick hit.  It amounts to, if you want to talk to us, you've got to use the Signal protocol.  But we'll meet you halfway by making it possible for you to connect with our previously closed servers.



So Meta wrote:  "To comply with a new EU law, the Digital Markets Act, which comes into force on March 7th, we've made major changes to WhatsApp and Messenger to enable interoperability with third-party messaging services.  We're sharing how we enabled third-party interoperability" - which they shortened to "interop" - "while maintaining end-to-end encryption and other privacy guarantees in our services as far as possible.



"On March 7th, a new EU law, the Digital Markets Act, comes into force.  One of its requirements is that designated messaging services must let third-party messaging services become interoperable, provided the third party meets a series of eligibility, including technical and security, requirements."  And that's of course the key because, if not, Meta is able to say, uh, no no no, not so fast.  "This allows," they wrote, "users of third-party providers who choose to enable interoperability to send and receive messages with opted-in users of either Messenger or WhatsApp, both designated by the European Commission as being required to independently provide interoperability to third-party messaging services.



"For nearly two years our team has been working with the European Commission to implement interop in a way that meets the requirements of the law and maximizes the security, privacy and safety of its users.  Interoperability is a technical challenge, even when focused on the basic functionalities as required by the DMA.  In year one, the requirement is for 1:1 text messaging between individual users and the sharing of images, voice messages, videos, and other attached files between individual end users.  In the future, requirements expand to group functionality and calling.



"To interoperate, third-party providers will sign an agreement with Messenger and/or WhatsApp, and we'll work together to enable interoperability.  Today we're publishing the WhatsApp Reference Offer for third-party providers which will outline what will be required to interoperate with the service.  The Reference Offer for Messenger will follow in due course.  While Meta must be ready to enable interoperability with other services within three months of receiving a request, it may take longer before the functionality is ready for public use.  We wanted to take this opportunity to set out the technical infrastructure and thinking that sits behind our interop solution.



"Our approach to compliance with the DMA is centered around preserving privacy and security for users as far as is possible. The DMA quite rightly makes it a legal requirement that we should not weaken security provided to Meta's own users.  The approach we've taken in terms of implementing interoperability is the best way of meeting DMA requirements, whilst also creating a viable approach for third-party providers interested in becoming interoperable with Meta and maximizing user security and privacy.



"First, we need to protect the underlying security that keeps communication on Meta end-to-end encrypted messaging apps secure, the encryption protocol.  WhatsApp and Messenger both use the tried and tested Signal Protocol as a foundational piece for their encryption.  Messenger is still rolling out end-to-end encryption by default for personal communication, but on WhatsApp this default has been the case since 2016.  In both cases, we are using the Signal Protocol as the foundation for these end-to-end encrypted communications, as it represents the current gold standard for end-to-end encrypted chats.



"In order to maximize user security, we would prefer third-party providers to use the Signal Protocol.  Since this has to work for everyone, however, we will allow third-party providers to use a compatible protocol if they're able to demonstrate it offers the same security guarantees as Signal."  And I'll just interrupt here to say that will be a high bar.



MIKAH:  Yeah.  [Crosstalk].  That's it.



STEVE:  And Signal is open source.  It's an open protocol.  Why in the world would anybody roll their own from scratch and say, oh, well, you know, well, like Telegram, that's got a bizarro protocol that nobody has ever understood, and no one has ever tried to prove was secure because it's just garbage.  You know, I mean, it scrambles stuff.  So maybe it's secure.  But it's nice to have proofs, and these days we can get proofs.  Except not for random stuff somebody just made up.



They said:  "To send messages, the third-party providers have to construct message protobuf structures, which are then encrypted using the Signal Protocol, and then packaged into message stanzas in XML," you know, the eXtensible Markup Language.  "Meta servers push messages to connected clients over a persistent connection.  Third-party servers are responsible for hosting any media files their client applications send to Meta clients, such as image or video files.  After receiving a media message, Meta clients will subsequently download the encrypted media from the third-party messaging servers using a Meta proxy service.



"It's important to note that the end-to-end encryption promise Meta provides to users of our messaging services requires us to control both the sending and receiving clients.  This allows us to ensure that only the sender and the intended recipient can see what's been sent, and that no one can listen to your conversation without both parties knowing."  Now, just to decrypt that corporate speak, what they're saying is, if we don't control the sending and receiving clients, which is to say if we are going out of our own - what am I looking for?  Not out of band, out of - there's a current term.  It was a well-known...



MIKAH:  Out of pocket?  Out of environment?  Out of...



STEVE:  Yeah, out of environment.  Then we can't make any representations about what happens to the message that you send out of our services, or where those messages come from into our services.  So that's sort of them just sort of, you know, stepping back, saying only if you stay within Messenger and WhatsApp are we willing and can we take responsibility over the messages that are being transacted.  So, you know, beware.



Anyway, they finish, saying:  "While we've built a secure foundation for interop that uses the Signal Protocol encryption to protect messages in transit, without ownership of both clients' endpoints we cannot guarantee what a third-party provider does with sent or received messages, and we therefore cannot make the same promise."  So again, they just said what I just said.  I didn't realize that was coming.  So anyway.



MIKAH:  With those last two paragraphs, are they talking to - do you think they're talking to the potential services that are trying to connect and explaining their thinking here?  Or is this Meta not being absolutely certain how the EU will take this choice to use Signal and kind of push Signal, and so they're almost trying to explain to them, look, we know that it would be easier if we would just open it up to all of them.  But, you know, they're sort of going through and explaining why Signal is the best.



I guess what I'm asking is, it kind of runs contrary to what the company said early on about having worked with the EU for so long on figuring this out, if what they're trying to do there is like, come on, EU, don't punish us further, we promise this works.  Do you think this is more for those other third-party messaging platforms to go, look, we know that you might want to use something else, but it makes sense why we're doing this and why we have to have control of the sending and receiving.



STEVE:  So my ignorance is showing here.  Can non-Facebook users use WhatsApp?  I know Messenger is Facebook only.



MIKAH:  That is a really good - I don't think you need a Facebook account to have WhatsApp.  But that's a good question.



STEVE:  I think that's correct.  So my feeling is, you know, yes, other Messengers might want to hook up.  But, you know, even if you didn't have WhatsApp, the most obvious messaging system to connect would be Signal itself; right?  Why wouldn't Signal, that invented the Signal protocol, say hey, yeah, we'll - that sounds great.  We'd like to be able to allow Facebook Messenger users and WhatsApp to interact with Signal users.  So, you know, Signal themselves would be the obvious interop choice.  But again, Telegram's going to have a hard time.  I mean, there are lots of other, you know, when I was talking last week about the various levels, Level 0, 1, 2, and 3, that Apple has assigned to messaging, there were some down at Level 0 where encryption was optional that I'd never heard of, like QQ.  What's that?



MIKAH:  Yeah, don't know.



STEVE:  But that's apparently some messaging app somewhere.  So again, any other company is going to have a heavy lift.  And basically I think what this means is, you know, we're using Signal at Meta, and anybody else who wants to talk with us is going to have to do it, too.  Because, again, Signal's bar is very high.  Well, in fact they just added post-quantum encryption in Signal.



MIKAH:  Right.



STEVE:  So, you know, you're going to - if what Meta is saying is we'll connect to you if you're as good as Signal, well, that now means if you also have post-quantum encryption available.



MIKAH:  Yeah.  So basically it is saying you're going to need to use Signal because you're not going to be able to prove to us that anything else is as secure.



STEVE:  It won't be as good if it's not Signal because Signal is the best.  Unless you ask Apple, and they think, well, Signal is our Level 2.  You know, we're Level 3.  It's like, okay, fine.  So.  Because Apple is rotating their keys every 50 messages or every week, whichever comes first.



MIKAH:  Right.



STEVE:  And they're saying, oh, that gives us an advantage.  So it's okay, fine.



MIKAH:  Yeah.  I don't know - I'm with you in that I don't think that should bump it up to Level 3.  But at the same time, when I was reading through the explanation of everything, I did think that was cool because I hadn't considered how having somebody's whole history of messages, even in a post-quantum world...



STEVE:  Ah, is using the same key.



MIKAH:  Yeah, using the same key, that that means the whole thing.  But this just means they only get - that's pretty smart.  That would be cool to see implemented.



STEVE:  I agree with you.  It's like, well, why not do it if we can?  And it was expensive to do it.  You know, they had to amortize that key across many messages because the key's 2K, and many messages are like, "Okay, Mom, I'm on my way."



MIKAH:  Right.



STEVE:  You know?  So the per-message overhead of 2K, that was insane, you know, when the message is 12 characters.  So anyway, okay.  So a lot of our listeners have asked me about this big cyberattack, a ransomware attack, on the U.S.'s Change Healthcare service.  Nearly three weeks ago, on Wednesday, so three weeks ago tomorrow, on Wednesday, February 21st, the American company Change Healthcare, which is a division of UnitedHealth Group, also known as UHG, was hit by a ransomware attack that was devastating by any measure.  That cyberattack shut down the largest healthcare payment system in the U.S.



Then, to give you a sense for this, just this past Sunday, two days ago, the U.S. Department of Health and Human Services addressed an open letter to "Health Care Leaders," writing:  "As you know, last month Change Healthcare was the target of a cyberattack that has had significant impacts on much of the nation's healthcare system.  The effects of this attack are far-reaching.  Change Healthcare, owned by UnitedHealth Group (UHG), processes 15 billion" - with a B - "healthcare transactions every year and is involved in one of every three patient records.  The attack has impacted payments to hospitals, physicians, pharmacists, and other healthcare providers across the country.  Many of these providers are concerned about their ability to offer care in the absence of timely payments, but providers persist despite the need for numerous onerous workarounds and cash flow uncertainty."  So this has really upset a lot.  



Okay.  So backing up here a bit, the day following the attack, February 22nd, this UnitedHealth Group filed a notice, as they must, with the U.S. Securities and Exchange Commission because they're a publicly traded company, stating that "a suspected nation-state associated cybersecurity threat actor" had gained access to Change Healthcare's networks.  Following that UHG filing, CVS Health, Walgreens, Publix, GoodRx, and BlueCross BlueShield of Montana reported disruptions in their insurance claims.  Yeah, basically it was all shut down.  The cyberattack affected family-owned pharmacies and military pharmacies, including the Naval Hospital at Camp Pendleton.  The Healthcare company Athenahealth was affected, as were countless others.



MIKAH:  Wow.



STEVE:  One week later, on the 29th of February, you know, Leap Year Day, UHG confirmed that the ransomware attack was "perpetrated by a cybercrime threat actor who represented itself to Change Healthcare as ALPHV/Blackcat."  And I'm just going to refer to them as Blackcat from now on because it's easier.  In the same update, the company stated that it was "working closely with law enforcement and leading third-party consultants Mandiant and Palo Alto Networks" to address the matter.  And then, four days later, that is, after this disclosure on the 29th, which is eight days ago on March 4th, Reuters confirmed that a bitcoin payment equivalent to nearly $22 million USD has been made into a cryptocurrency wallet "associated with Blackcat."



MIKAH:  We think they paid the ransom?



STEVE:  Yeah.



MIKAH:  Oh, my god.



STEVE:  Yeah.  UnitedHealth has not commented on the payment, instead stating that the organization was "focused on the investigation and the recovery."  Right.  Apparently to the tune of $22 million USD.  On the same day, a reporter at Wired stated that the transaction looked "very much like a large ransom payment."  What's transpired since then is a bit interesting, since Blackcat is a ransomware-as-a-service group.



MIKAH:  Oh.  Okay.



STEVE:  This of course means that they provide the software and the backend infrastructure, while their affiliates are the ones that perpetrate the attacks, penetrate the networks, and in return for that the affiliate receives the lion's share, in this case 70%, of any ransoms paid.  That's always been the way it is.  However, in this instance it appears that Blackcat is not eager to part with that 70%, which amounts to a cool $15.4 million.  So they're claiming that they've shut down and disbanded.



MIKAH:  What?



STEVE:  Nice timing on that.  Okay.  So exactly one week ago, the HIPAA, the U.S. organization, the HIPAA Journal posted some interesting information.  They wrote...



MIKAH:  Oh, sorry.  Sorry, Steve, can I correct you there?  It's H-I-P-A-A, not H-I-P-P-A.



STEVE:  Oh, good, thank you, yes.  Good.  Thank you.  HIPAA.  Thank you.  So they said:  "The ALPHV/Blackcat ransomware group appears to have shut down its ransomware-as-a-service (RaaS) operation, indicating there may be an imminent rebrand.  The group claims to have shut down its servers, its ransomware negotiation sites are offline, and a spokesperson for the group posted a message, 'Everything is off, we decide.'"  Probably a  Russian speaker.  "A status message of 'GG' was later added, and Blackcat claimed that their operation was shut down by law enforcement and said it would be selling its source code.



"However, security experts disagree and say there is clear evidence that this is an exit scam, where the group refuses to pay affiliates their cut of the ransom payments and pockets 100% of the funds.  Blackcat is a ransomware-as-a-service operation where affiliates are used to conduct attacks and are paid a percentage of the ransoms they generate.  Affiliates typically receive around 70% of any ransoms they generate, and the ransomware group takes the rest.



"After the earlier disruption of the Blackcat operation by law enforcement last December, Blackcat has been trying to recruit new affiliates and has offered some affiliates an even bigger cut of the ransom.  An exit scam is the logical way to wind up the operation, and there would likely be few repercussions, other than making it more difficult to recruit affiliates if the group were to choose to rebrand."  Right.



MIKAH:  Oh, wow.



STEVE:  Who's going to do this again if you just screwed the last affiliate that you were working with after they generated a $22 million ransom.



MIKAH:  Hello.  We are Whitecat, and we have no affiliation to the Blackcats.  Would you like to work with us?



STEVE:  Right.  Maybe dark black or dark gray.  Anyway, HIPAA wrote:  "It's not unusual for a ransomware group to shut down operations and rebrand after a major attack, and Blackcat likely has done this before.  Blackcat is believed to be a rebrand of the earlier BlackMatter ransomware operation, which itself was a rebrand of DarkSide."



MIKAH:  Okay.  If they have something with shadows, dark, black, you know it's the same company, people.



STEVE:  That's right.



MIKAH:  Going forward.  Come on.



STEVE:  That's right.  DarkSide was the ransomware group behind the attack on the Colonial Pipeline in 2021 that disrupted fuel supplies on the Eastern Seaboard of the United States.  Shortly after the attack, the group lost access - what do you know - to its servers, which they claimed, probably was, due to the actions of their hosting company.  They also claimed that funds had been transferred away from their accounts and suggested they were seized by law enforcement.  BlackMatter ransomware only lasted for around four months before it was in turn shut down, with the group rebranding in February of 2022 as Blackcat.



On March 3rd of this year, an affiliate - and here it comes - with the moniker Notchy (N-O-T-C-H-Y) posted a message on Ramp Forum claiming they were responsible for the attack on Change Healthcare.  The post was found by a threat researcher at Recorded Future.  Notchy claimed they were a long-time affiliate of the Blackcat operation, and that could have gone back two years because that's when Blackcat reformed, had the "affiliate plus" status granting them a larger piece of the pie, and that they had been scammed out of their share of the $22 million ransom payment.



They claimed that Optum, which is the actual organization within Change that was hit and paid, Optum paid a 350 Bitcoin ransom to have the stolen data deleted and to obtain the decryption keys.  In other words, full-on standard ransomware payment.  Notchy shared the payment address which shows a $22 million payment had been made to the wallet address, and the funds have since been withdrawn.  The wallet has been tied to Blackcat as it received payments for previous ransomware attacks that have been attributed to the group.



Notchy claimed Blackcat suspended their account following the attack and had been delaying payment before the funds were transferred to Blackcat accounts.  Notchy said that Optum paid to have the data deleted, but they have a copy of 6TB, that's how much was stolen, of data in the attack.  Notchy claims the data includes sensitive information from Medicare, Tricare, CVS-Caremark, Loomis, Davis Vision, Health Net, MetLife, Teachers Health Trust, tens of other insurance companies, and others.  The post finishes with a warning to other affiliates that they should stop working with Blackcat.  It's unclear what Notchy plans to do with the stolen data and whether they will attempt to extort Change Healthcare or try to sell or monetize the data.



Fabian Wosar, Emsisoft's CTO, is convinced this is an exit scam.  After checking the source code of the law enforcement takedown notice, he said it is clear that Blackcat has recycled it from December's earlier takedown notice.  Fabian tweeted:  "There is absolutely zero reason why law enforcement would just put a saved version of the previous takedown notice up during a seizure instead of a new takedown notice."  He also reached out to contacts at Europol and the NCA, who said they had no involvement in any recent takedown.  Currently, neither Change Healthcare nor its parent company UnitedHealth have confirmed that they paid the ransom and then been cheated out of it, and issued a statement that they are currently focused on the investigation.



MIKAH:  I'm sure.



STEVE:  So, yeah.  This is all a big mess.  It appears that the Blackcat gang has made off with Optum's $22 million, that Notchy affiliate did not receive the $15.4 million or more that they feel they deserve, Optum got neither the decryption keys nor the deletion of their 6TB of data, which they paid for.  And no one but the Blackcat guys are smiling at this point.  Since rebranding and returning to the ransomware-as-a-service business may be impossible after taking their affiliate's money, and since $22 million is a nice piece of change, they may just go find a nice beach somewhere to lie on.



MIKAH:  One hopes.



STEVE:  You know?  Meanwhile, here in the states, the inevitable class action lawsuits have been filed due to the loss of patient healthcare records.  At last count at least five lawsuits are now underway.



MIKAH:  Oh, my word.  This is awful.  All the way around this is awful.



STEVE:  Big mess.



MIKAH:  It's a huge mess.  And it's, I don't know, it kind of speaks to the dangerous nature maybe of having the entire country's, nearly the entire country's healthcare operating under one company.  That that much can be impacted by getting into one company is pretty scary.



STEVE:  Yes.  They are a service provider.  And so all kinds of other of their clients use them to provide the insurance processing, and in return the payments back to them.  And all of that is shut down now.  They don't have their decryption keys.  All of their server infrastructure is encrypted.  They were willing to pay $22 million to get it back online more quickly.  We don't know anything about the state of their backups and so forth.  But even so, 6TB of medical record history is now also in the hands of bad guys.



And, you know, this is the unfortunate downside of the U.S.'s free enterprise system, which I'm, you know, has lots of upsides because it allows people to apply their efforts and to be clever and to create companies.  Unfortunately, there's a tendency for the big fish to eat the small fish, and for consolidation to happen.  And so what we're seeing here is, as you said, what would have otherwise been a much more distributed set of services have all been consolidated.



You know, on one hand, yes, it's more efficient.  You're getting to use one larger set of infrastructure instead of lots of smaller infrastructures.  But with it comes responsibility.  With that consolidation comes responsibility.  And what we're now seeing is what happens when one could argue that responsibility was not met.



MIKAH:  With great power, they say.



STEVE:  Yes.



MIKAH:  That's so many records.  You said, what, for every three, it's two of every three?



STEVE:  One out of every three medical records in the U.S. is basically in that 6TB of data.  Yeah.



MIKAH:  Yeah, awful.



STEVE:  On my side, everything continues to proceed well with SpinRite.  The limitations of 6.1, you know, being unable to boot UEFI-only systems and its lack of native high-performance support for drives attached by USB and NVMe media, are as annoying as I expected that they would be.  So there just wasn't anything I could do about it.  I'm working to get 6.1 solidified so specifically so that I can obsolete it with 7.0 as soon as I possibly can.  But that said, I still do want to solve any remaining problems that I can, especially when such a solution will be just as useful and necessary for tomorrow's SpinRite 7 as it is for today's 6.1.



And that's the case for the forthcoming feature I worked on all of last week.  It's something I've mentioned before which I've always planned to do, and that's to add the capability for people without access to Windows, mostly Linux users, who are Linux users and so don't have Windows, to directly download an image file that they can then transfer to any USB drive to boot their licensed copy of SpinRite.



For those who don't know, the single SpinRite executable, which is about 280K because of course I write everything in assembly language, is both a Windows and a DOS app.  When it's run from DOS, it is itself SpinRite.  But when that same program is run from Windows, it presents a familiar Windows user interface which allows its owner to easily create bootable media which contains itself.  So it can, you know, that can either format and prepare a diskette - which there's not much demand for, but since the code was there anyway from 6.0, I left it in.  Or it can create an optical disc ISO image file for burning to a disc or loading with an ISO boot utility.  It can also create a raw IMG file, which can be put somewhere, or it can prepare a USB thumb drive.



Twenty years ago, back in 2004 when I first created this code, the dependence upon Windows provided a comprehensive solution because Linux was still mostly a curiosity back in 2004 and had not yet matured into the strong alternative OS that it has since become.  Today, there are many Linux users who would like to use SpinRite, but who don't have ready access to Windows in order to run SpinRite's boot prep.  And this need for non-Windows boot preparation will continue with SpinRite 7 and beyond.



The approach I've developed for use under Windows, which is used by InitDisk, ReadSpeed, and SpinRite, is to start the application, then have its user insert their chosen USB drive while the application watches the machine for the appearance of any new USB drive.  This bypasses the need to specify a drive letter, it works with unformatted drives or drives containing foreign file systems so they wouldn't get a drive letter, and it makes it very difficult for the user to inadvertently format the wrong drive, which was my primary motivation for developing this user-friendly and pretty foolproof approach.



Unfortunately, there's a downside.  It uses very low-level Windows USB monitoring, which was not implemented in WINE, which is the Windows emulator for Linux.  So for Linux users, a ready-to-boot image file is the way to go, and I'm in the process of putting the final pieces of that new facility together.  It should be finished later this week, and I'm sure I'll just make a note of it next week that it's, you know, you can go use it.



And let's take our last break, and then we'll do some feedback from our listeners and get to our episode's main topic.



MIKAH:  Wonderful.  All right.  I do have to take a moment here.  By the way, Mikah Sargent, subbing in for Leo Laporte, who is on vacation.



STEVE:  We knew that.



MIKAH:  It's time to close the loop with feedback from the listeners.



STEVE:  Yes, it does.  Okay.  So John Robinette said:  "Hey, Steve.  I'm sure I'm not the only one to send you this note about Telegram after listening to SN-964.  There's been much chatter about the protocol Telegram uses for end-to-end encryption, but it is a common misunderstanding that they use this by default.  Telegram's default uses only TLS to protect the connection between your device and their servers, and does not provide any end-to-end protection.  They have an additional feature, 'Secret Chats,' that does use end-to-end encryption on the client device.  It is not possible to use end-to-end encryption in group chats, and when used for one-on-one chats it limits the conversation to a specific device.  Based on my anecdotal experience using Telegram with a few friends, most people either do not know about 'Secret Chats' or do not use it.



"I found a post from 2017 that explains Telegram's reasoning for this."  And he quoted it, saying:  "The TL;DR is because other apps, for example WhatsApp, allow you to make unencrypted backups" - this is Telegram speaking, essentially - "actual end-to-end encryption isn't worth being on by default."



MIKAH:  What?



STEVE:  Okay.  So they don't turn it on by default, and claim they are more secure because of how they store backups.  He says:  "Anyway, it's not just Apple marketing speak that Telegram is not end-to-end encrypted.  It's by design from Telegram.  Thanks."



MIKAH:  Wow.



STEVE:  So John, thank you.  And I should mention several other of our listeners who actually use Telegram also shared their experience.  What they showed was that, unlike the way I presumed it would be when I talked about it last week, Telegram really is probably mostly used in its insecure messaging mode since not only is it not enabled by default, enabling it is not even a global setting.  It must be explicitly enabled on a chat-by-chat basis.  So that makes the use of encryption hostile in Telegram, which is certainly not what anyone hoping for privacy wants.  And its fundamental inability to provide end-to-end protection for multiparty group chats strongly suggests that it's being left behind in the secure messaging evolution.  So I presume that they're aware of this, and they're hopefully working behind the scenes to bring Telegram up to speed, since otherwise it's just going to become an historical footnote.



Given these facts, I certainly reverse myself and agree with Apple's placing Telegram down at Level 0, along with QQ, whatever that is, where it certainly belongs.  So thank you very much, John and everybody else, for educating me about Telegram, which I did not take the time to - and I'm amazed, for all the talk of Telegram, that it is as insecure as it obviously is.



Someone who's tweeted before, whose handle is a hacker version of anomaly, it's 3n0m41y, he said:  "Steve, KeePassXC latest release is also supporting Passkeys now.  Great news."  Okay.  I'm not a Passkey user, so I don't know what communication they may provide to their users, so I wanted to pass along that welcome news.  Back when Passkeys first appeared, supported only by Apple, Google, and Microsoft, they each created their own individual and well-isolated walled gardens to manage Passkeys only on their devices, and only within their own ecosystems.  There was no cross-ecosystem portability.



At that time we hoped that our password managers, which are already an inherently cross-platform, would be stepping up and getting in on the act because they would be able to offer fully cross-platform Passkey synchronization.  And as we know, Bitwarden, a TWiT network sponsor, has done so.  And given competitive pressures it will soon become incumbent upon any and all other password managers to offer Passkey support.  And we'll be talking about what that means when we get to this week's main topic.  But for now, for anyone who's using KeePassXC and didn't know, it's now got Passkey support.  So that's all for the good.  A user whose actual...



MIKAH:  Good luck with this one.



STEVE:  Exactly.  His Twitter handle, he later tells us, was created by asking Bitwarden for a random gibberish string.  He said:  "Episode 964."  He said:  "I'm on the go and won't have time to write an eloquently worded message like the ones you read on the show."  Actually, I think his was, but okay.  "So feel free to simply summarize this if you find it relevant.  In 964" - last week - "you mentioned feedback from a listener who mentioned the Taco Bell app using email passwords 'for convenience.'



"Before my main comment, I'd like to mention that they are not the only ones doing this.  I suspect the most popular service out there today that does this is Substack, the blogging service that has exploded in popularity since early/mid COVID.  When I signed up there for the first time, I wasn't aware, and the whole process was very confusing and counterintuitive given my previously learned username/password behavior.  Beyond being confusing, it truly is much more of a pain for those of us who use browser-based password managers, which have made the traditional login process rather seamless."



Okay.  So I'll just interrupt to say, right, you know.  And you probably haven't heard me say this, Mikah.  The more we explore the topic, given that passwords are entirely optional, when every site includes an "I forgot my password" recovery link, I think that viewing passwords as login accelerators, which are handled now by our password managers, is quite apropos.



MIKAH:  I like that, yeah.  That's a good way to think of it because, yeah.  Anecdotally speaking I know many people who are not as techie as we are whose password really is that "I forgot my password button" every time, that's the password because they forget it right after they've changed it, and then they go to their email.  So in a way that - because I have always hated this, the I try to log in, and they send me what they call a "magic link" to my email.  Slack has this functionality.  They kind of suggest you use that first and foremost whenever you log back into Slack.  And mentioned here Substack, there are so many that do this.  And I don't like it.  But when I think about now anecdotally the people I know in my life who are already doing that, going to their email every time to reset their password again, yeah, password or a login accelerator.  That's clever.



STEVE:  That's really what it is.  And in fact we began this dialogue several weeks back.  We did a podcast titled "Unintended Consequences."  And what became clear was that with Google's deprecation of third-party tracking cookies by midpoint of this year, the advertisers are freaked out that they're going to lose the ability to aggregate information about their visitors.  So they're putting pressure on websites to add a "give us your email to join the site."



And so the unintended consequence of third-party tracking being blocked robustly in Chrome, as it's going to be, is that websites are being incentivized to ask their visitors, not necessarily to create an account, but to "join."  And so they're bringing up a request for your email address, then sending you a link which you have to click on so you just can't use gibberish, in order to continue using the site.



So essentially this is going to get worse.  And when we just go to a site, if your browser doesn't already have a persistent cookie for the site, then the site will ask you for an email address just to get it because they want to be able to provide that email address to the advertisers who are on the site because the advertisers will pay more money to the site if they receive an email address which has been confirmed in return.  So we're entering a world of pain here.  But so that brought us to the whole idea of using email in lieu of logon and what that means.  And so that's what this guy's talking about.



So he says:  "Anyway, my main comment is that it may very well be possible that these implementations are not for user convenience," meaning implementations of logon with email.  He says:  "They are for liability."  He says:  "By requiring an email to log in, the host company hoists account breach liability off of their own shoulders onto the email providers.  Your account got hacked?  That's a Gmail breach.  Not our problem."



He says:  "On a side note, you also mentioned in the episode a listener in the financial sector who commented on password length, complexity, and rules being limited by old legacy mainframes.  I just wanted to share that I worked for a federal agency about a decade ago, and thought I discovered a bug when I realized I had just been let into my sole enterprise account despite missing a character on the end of my password."



MIKAH:  What?



STEVE:  "I tested that again and discovered the system was only checking the first eight characters of the password I thought I was using.  I grabbed a fellow developer to show them, thinking I'd blown the lid off some big bug, and he shrugged and said, 'Yeah, it only checks the first eight.'"  He said:  "I was stunned."



MIKAH:  Good god.



STEVE:  "Thanks for the wonderful content.  Cheers."  He says:  "P.S.:  Yes, this account username was generated by my Bitwarden password generator."



MIKAH:  Got it.



STEVE:  Okay.  So we first took up the subject of email-only login with our "Unintended Consequences" episode, since it was becoming clear that websites were planning to react to the loss of third-party tracking with the establishment of first-party identity relationships with their visitors.  And in every instance the company's so-called "privacy policy" clearly stated that any and all information provided by their visitors, including their name and their email address, not only could be, but would be, shared with their "business partners," which we know are the advertising networks providing the advertising for their site.



But I thought this listener's idea about limiting liability was interesting, and it had not occurred to me.  When you think about it, what's the benefit to a website of holding onto the hashes of all of its users' passwords?  The only benefit to the site is that passwords allow its users to log in more quickly and easily, when and if they're using a password manager.  The problem is the site cannot know whether the user is using a password manager.  How big an issue is that?



Today, only one out of every three people, 34%, are using a password manager.  And while this is a big improvement over just two years ago in 2022 when the figure was one in five, or 21%, this means that today two out of every three Internet users are not using password management.  And we know this suggests, exactly as you were saying, Mikah, of your friends, that the quality of those two-thirds of all passwords may not be very high.  They may no longer be using "monkey123," but their chosen passwords are likely not much better.



Now look at what happened with the 23andMe disaster.  A shocking number of 23andMe user accounts were apparently compromised using some form of password spray.  That could never have succeeded if 23andMe users were logging in with highly complex unique-per-site passwords, which of course are encouraged by the use of any password manager.  But with two-thirds of Internet users still today not using a password manager, that suggests that two-thirds of 23andMe users were not being well protected from the abuse of their poor password hygiene.  If you're not someone who understands the value of password management, "monkey123" looks pretty good, and it sure is easy to remember.



My point is the quite successful attack on 23andMe demonstrates that the use of traditional username and password login creates a single point of failure which facilitates automated attack.  A widely distributed network of bots can create authentic-appearing web sessions and attempt to login as legitimate users over and over and over under the cover of darkness, succeeding eventually and incrementally with no involvement on any user's part.  And that makes a crucial difference.  By comparison, if a user's email inbox were to be flooded with 23andMe login requests, every such user - and quite soon 23andMe - would know that someone or something was up to no good.  But would an attacker even bother?  Doesn't the email loop completely thwart such attacks?  Logins are no longer autonomous.  And unless the attacker obtains access to every user's email account, the attacker cannot login using user authentication data contained in the email.  This eliminates all generic remote password spray brute force attacks.



So the point I wanted to make is that this listener's comment about liability is very interesting.  We're constantly hearing about site breaches, with Troy Hunt's "Have I Been Pwned" service constantly collecting massive new troves of breached user login credentials.  All of that disappears if a website no longer offers to store its users' password hashes.  Why bother?  It's just a potential nightmare.  It's a liability, a secret they're not good at keeping.  With two-thirds of users today still not bothering to use a password manager, they are likely using crappy passwords, or the same password at all their different sites.  This renders the site vulnerable to attack, as 23andMe was, through no fault of theirs because their users cannot be bothered to secure themselves better.



So these attacks go unnoticed and unobserved because they can be conducted by autonomous distributed networks of bots.  But that cannot happen if the user's email account is dynamically included in the login process.  This completely changes the attack dynamics.  And don't get me wrong, I'm not suggesting that I think this is a good idea, except that it kind of is, because it definitely represents a nightmare for the other one-third of us who are using password managers stuffed with long passwords we could never begin to memorize or even correctly enter into a password field.



The problem for our future is that email loop login makes a horrible kind of sense for any website that gets to choose whether or not they want the liability of storing the hashes of their users' potentially crappy passwords.  It's easy to imagine websites deciding that they'd prefer to ask their users to obtain a per-login token from their own email.



MIKAH:  Dang.  This makes sense, Steve.



STEVE:  Yeah.



MIKAH:  Sorry, but it's my responsibility to make sure that this part of the episode never makes it to the light of day so that every - no, I'm just kidding.  Now I'm worried that people are going to be listening to this and going, you know what?  Because you just convinced me.  This whole time I've been annoyed by that email login magic link nonsense.  But it's not nonsense, especially because it requires so much extra effort by these folks who are right now not using much effort at all.  That is so very true.  Huh.



STEVE:  And unfortunately it transfers the effort to the user who wants to log in.  So it distributes it.  Now, we're going to be talking by the end of the podcast about Passkeys versus not.  And an interesting compromise suggests itself here, which is, if you use Passkeys, then you get instant logon.  If you don't want to use Passkeys, you get email loop login.  Because as I'm going to demonstrate, usernames and passwords, they're not equivalent, even close.  So more to come.



Mark Jones tweeted:  "Another in the 'it was nice while it lasted' category."  He said:  "I have run into two sites that won't accept anything @duck.com emails."  As we know, DuckDuckGo offers an email anonymizing forwarding service, right, where it's something @duck.com.  We've talked about this for several weeks as a solution to the problem of every website you visit wanting your email address just for the privilege of seeing the content because they want to provide that to their advertisers.  Then the solution is, oh, let's create, you know, anonymous fake email accounts.  Well, Mark is noting that he's run into two sites that will not accept @duck.com emails.  He said:  "A previous solution, using a '+' in a Gmail address, also has been thwarted.



He said:  "That allowed me to" - that, using a "+" - "allowed me to filter easily and reject sites of no interest or that got spammy.  One service stripped the plus sign and what was after it out.  The other trend you haven't mentioned is the insistence on a cell number.  Those are unique, difficult to share, and are harder to make throwaway.  I was trying to set up services for a community organization, only to have a couple of vendors balk because my cell was already associated with a different active account.  Love the podcast, look forward to it every week, and I'm happy it will continue.  Tested 6.1 and am loving that, too."



Okay.  So as Mark wrote:  "It was nice while it lasted."  Unfortunately, while it is possible to hide our real email address behind an identity-protecting email-forwarding service, it's not possible to hide the fact that we're using an identity-protecting email-forwarding service.  And the fact that a site is stripping the plus sign and what follows from a user's email address is nothing short of rude.  You know, it's not their business to alter someone's email address.  That really, you know, wow, that seems so wrong.  The only leverage website visitors have is to choose not to play.



Normally, I would suggest that such email address discrimination would not become widespread.  That is, you're only going to see it at a few sites.  But if websites are asked for their visitors' email as a replacement for third-party tracking, with the incentive of being paid more for ads served to identifiable Internet users, which is the case, then those advertising interests who are definitely paying do not want to pay more for anyone using an anonymizing email-forwarding service.  So you can bet that a site's advertisers will be telling those websites that they won't pay for anonymized email.  And then, in turn, those sites will be telling their visitors exactly what our listener Mark Jones was told:  "Please provide your primary email address, not a forwarding service."



MIKAH:  Wow.  That - so I have not had that issue.  And I use Fastmail, who is a sponsor on the network, along with I use the password manager 1Password.  And so when it automatically generates it, it creates a masked email for me.



STEVE:  Yup.



MIKAH:  And that right now, knock on wood, has not been an issue.  But I have had the issue with the plus sign.  I always - I never thought of it, because this has been a long time ago, and I always - I didn't think of it in the nefarious way that we're thinking of it here.  I just thought it was somebody who didn't know what they were doing coding-wise, and so they couldn't, like, parse the plus and what came after it.  And I thought, oh, maybe they just don't know how to accept that and think that it's a - the rules for what you can type in, it's telling the system that it's a fake email, or that it's not an email address because of the rules that are set up.  But now I can see, too, how in that case it could be somebody going, no, we know that that's just adding on.  Because I did, I had my name plus spam @gmail.com.



STEVE:  Right.



MIKAH:  And I could filter it out that way.  And I had to - I ran into that issue too many times, so I had to stop doing that.



STEVE:  Yup, yup.  Well, let's hope that Fastmail continues to be available to you.



MIKAH:  Yes.



STEVE:  It'd be interesting to find out if it is not at some point.



MIKAH:  Definitely.



STEVE:  Rob Powell wrote:  "Hi, Steve.  Following recent discussions on the podcast, I thought other listeners might also appreciate the below.  It's a tool to detect when Chrome extensions change owners."  And this is really interesting.  I got a kick out of the name of the tool that Rob pointed to.  It's posted on GitHub under the account of "classvsoftware,"  all one word, classvsoftware, with the name "Under New Management."



A couple of weeks ago we were noting that the authors and maintainers of Android freeware which had accumulated large install bases over time and earned some implicit trust from their users have effectively been cashing out their installed bases to less than scrupulous buyers, who then take advantage of that installed base, you know, to stick ads in the apps that never used to have them.  You know, like dumb apps, you know, free things, like measuring sticks and things.  I mean, just nothing.



Since the same thing could - well, actually, specifically it was an app that allowed you to change the brightness of the flashlight.  And, you know, it became adware because the author who, you know, wasn't getting paid anything for it, got tired of maintaining it and finally decided to accept an offer.  Since the same thing could happen with browser extensions whose special position in our browsers make this an even more pressing and worrisome problem, I was glad that Rob brought this up.  I went over to the GitHub site to see what the authors of this extension had to say.  That is, this "Under New Management" extension.



They explain.  They said:  "Intermittently checks your installed extensions to see if the developer information listed on the Chrome Web Store has changed.  If anything is different, the extension icon will display a red badge, alerting you to the change."  And as to why this is an issue, they write:  "Extension developers are constantly getting offers to buy their extensions.  In nearly every case, the people buying these extensions want to rip off the existing users of those extensions.  The users of these extensions have no idea an installed extension has changed hands and may now be compromised.  'Under New Management' gives users notice of the change of ownership, giving them a chance to make an informed decision about the software they're using."



Now, in the text the phrase "constantly getting offers to buy their extensions" was a link.  So I wondered what the author of this "Under New Management" extension might be linking to, and holy crap.  The link was to another GitHub page titled "Temptations of an open-source browser extension developer."  But before I describe what's there, let me back up a bit.  The extension in question is called "Hover Zoom+," and its author explains.



"Hover Zoom zooms images and videos on" - so this is the extension, Hover Zoom.  "Zooms images and videos on all your favorite websites - Facebook, Amazon, et cetera.  Hover your mouse over any image on the supported website, and the extension will automatically enlarge the image to its full size, making sure that it still fits into the browser window.  This is an open-source version of the original Hover Zoom extension, which is now overrun by malware and deleted from the store.



"In this version, all spyware has been removed, many bugs were fixed, and new features were added.  It doesn't" - does not - "collect any statistics whatsoever.  The only permission it needs is to access data on all websites to extract full images, and optional permissions to access browser history, download/save images, or get tab URLs for per-site configuration.  This extension will never be sold out and will never compromise users' privacy.  As a proof, please see the list of all takeover offers I've received over the past years."



Okay.  And this brings us back to the "Temptations of an open-source browser extension developer" page.  It is so astonishing that I've made it the GRC shortcut of the week.  If you're at all curious, and please be curious because it is something, visit grc.sc/965.  So that's today's episode number, 965, grc.sc, you know, short for shortcut, grc.sc/965.  The author starts off, the same author who we just heard from, who fixed this malware-ridden previous Hover Zoom extension, and now vows that he will never sell it.



He said:  "Over the years I have received many proposals to monetize this extension.  So I think I'll just start posting them here for fun, but not for profit.  The main reason I continue to maintain this extension is because I can hardly trust others not to fall for one of these offers.  I'm fortunate to have a job that pays well enough to allow me to keep my moral compass and ignore all of these propositions.  I realize that not everyone has the same financial security.  So hopefully this thread will shed some light on what kind of pressure is put on extension developers."



And what follows really serves to put this into perspective.  The first offer he posts is dated September 28th, 2015.  It reads:  "Hope this message finds you well.  I'm a Strategic Partnerships Manager at a monetization platform for browser extensions."  In other words, there is such a thing.  "I'm contacting you since I came across the extension 'Hover Zoom+' at the Chrome Store.  I consider your product can help you bring profit by means of collaborating together.  I would like to suggest a potential partnership between our companies that will significantly increase your revenues.  Are you interested in discussing our offer in more details?  Hope for positive feedback and ongoing cooperation."  Okay, that was the first one he listed.



The most recent offer the author posted four weeks ago on February 14th, 2024, read:  "I trust you're doing well!  Currently I'm exploring opportunities to grow my business by investing in Chrome extensions.  Your extension Hover Zoom+ has caught my attention, and I am genuinely interested in discussing the possibility of acquiring it.  We can discuss the price and complete the transaction securely through a reputable escrow service, escrow.com or cryptoexchange.com.  Google supports a smooth transfer of extension ownership from one account to another, ensuring your Gmail account remains unaffected.  If you have any inquiries, or if this aligns with your plans, feel free to reach out to us via this email.  Looking forward to hearing from you."



Now, in between that first and last offer are more than - I counted them - 165 other offers this guy has received from parties interested in taking "Hover Zoom+" off his hands.  I say "more than" because for a while I was not counting the follow-ups as being separate offers.  But there were so many of them that after a while I decided a more accurate and fair count should include them.  So 165 and counting.



The danger to the users of extensions that are sold to unscrupulous buyers is that an originally benign extension might be altered to begin harvesting its users' browsing data as the U.S. Federal Trade Commission has formally accused the Avast browser extension of doing.  Since the purchasers of popular extensions with a large installed base have no interest in ongoing support of that extension and simply wish to maximize their return on investment while they can, they will have a plan for somehow monetizing the extension's user base.



Given that, the value of the "Under New Management" extension becomes clear.  So thanks for bringing it to our attention, Rob.  Appreciate it.



And lastly, Max Feinleib said:  "I'm sure I'm not the first person who's mentioned this, but SN-905 will be hard to beat for the shortest title."  And he says that was just the numeral, the digit "1."  And I made a comment to Leo last week that "PQ3," which was last week's title, might be the shortest podcast title we've ever had.  Max was actually the second of two sharp listeners who pointed out what amounted to "Not so fast there, Gibson.  What about podcast number 905?  That was titled with the single digit '1'."



That frighteningly low number was a reference to the password hashing iteration count some unfortunate LastPass users found when they went looking.  So anyway, thank you for catching that, Max and the other person who mentioned it.  You're correct.  I will never have a shorter podcast title than "1."  I can't imagine a null title, so yeah.



Okay.  So I think people are going to be a little surprised.  As I mentioned last week, I intended to title this podcast "Morris II."  That'll be next week.  This is a result of a tweet from Stephan Janssen.  He said:  "Hi, Steve.  I'm an avid SN listener and, in part thanks to your information, moved from LastPass to Bitwarden.  Now that Passkeys are becoming more prevalent, I'm noticing a Bitwarden popup when I try to enroll my YubiKey as my two-factor authentication token.



"I'm tempted to give Passkeys in Bitwarden a try, in part due to your enthusiasm about it, but I'm also hesitant since it feels like I'm losing and giving up my second factor.  With Google, for example, using a Passkey allows me to log in without providing anything else, which makes it feel like I no longer have MFA for my account.  Am I right in thinking that Passkeys will reduce my security if I'm now using a random password with a second factor, or am I missing something in my thinking?"



Okay.  I'm going to give everyone the short TL;DR first.  But then because, as they say, "it's complicated," I'll explain more.  So here it is:  In a properly operating world, Passkeys, all by its lonesome, provides far more security than any super-strong password plus any second factor, period.



Let's start working our way through what has become a confusing authentication minefield.  First off, why does anyone use a second factor?  And I'm going to pointedly ignore the primary reason we, who listen to this podcast, use second login factors, which is mostly because they're cool.



MIKAH:  Fair.



STEVE:  Right?  You know, it feels a little secret agent-like to have to go look up that time-varying code to complete an authentication.  It definitely provides the impression of having much greater security, even if there are still ways around it.  And we've talked about the ways around it.  Multifactor authentication can be and is actively compromised when there's some strong need to do so.  There are two ways this can be accomplished.



The first is arranging to insert a man in the middle.  This is done in today's world by arranging to trick an unwitting user into going to a fake login page.  They provide their username and password, and are then prompted for the second factor.  They provide that to the man in the middle, who turns around and immediately logs in as them before that ephemeral six-digit code has a chance to expire.  This is not easily done because it requires a higher end attacker and some setup.  But it is being done.  Normally this only intercepts a user's username and password, which does give the bad guys more permanent account access until the password is changed.  But when you think about it, once you have that man-in-the-middle proxy in place to intercept the username and password, the addition of an ephemeral second factor adds very little to the user's actual security.



The second way the MFA system can be defeated is the same way any site breach, with a loss of their authentication database, you know, their password hashes, can defeat the security for their users.  Just as the authentication database contains a hashed password which is used to verify the user's freshly- provided login attempt password, that authentication database must also contain the shared MFA secret which keys the user's TOTP, their time-based one-time password.  So if that MFA secret can be stolen along with the username and password hash, then bad guys have potentially acquired all of the secrets they need to impersonate the site's users anytime they wish.



The key here is my use of the term "secrets" because today's time-based one-time passwords are based upon shared secrets.  The site knows the secret key, and your authenticator app has the same secret key.  That's what allows the authenticator to generate a six-digit code based upon the time of day; and for the server, knowing the same time of day, to generate the same code and compare them.



And "storing secrets" is the crucial difference in the Passkeys system.  This was also true with SQRL's technology which I articulated on this podcast more than 10 years ago.  What I used to say was that "SQRL gives websites no secrets to keep."  And exactly the same is true of today's Passkeys.



The crucial difference is secret versus public key technology.  That's what makes Passkeys so very different from everything that came before it, except of course SQRL, which works similarly.  With Passkeys, the user's private key never leaves the authentication device end, and the website only ever receives the user's matching public key.  And the only thing that the website's public key can be used for is verifying the signature of a unique challenge.



The website sends the user a never-before-used large random number to sign.  Since the large random number has never before been used, its signature will also never have been used before, so no form of replay attack is possible.  The user's Passkey agent signs the challenge received from the server and sends it back to the website, signed.  The website then uses its matching public key to verify the user agent's Passkey signature.  And crucially, verifying signatures is all it can do with that public key.  That's the only power it has.  So if that key were to be stolen, no one cares.  Truly, no one cares.



The reason I felt that Stephan's question deserved our attention is that none of this crucial difference between traditional username and password, and even including multifactor authentication, versus Passkeys is in any way apparent to users.  Users just see yet another thing, another way to log on.  What's impossible to see and to appreciate from the user-facing side is how completely and importantly different Passkeys is from everything that has come before.  So to respond to Stephan's uneasiness about whether he's losing anything, sacrificing anything, or giving up anything by choosing to use a Passkey instead of having traditional login - even when it's augmented with multifactor authentication - the bright flashing neon answer is "No."



The best way to think about multifactor authentication is that it was a last-ditch temporary Band-Aid that was added as an emergency measure in an attempt to do something to shore up the existing ecosystem of fundamentally weak and creaky server-side secret-based password-only authentication.  You know, passwords are not that great.  Let's add another factor.  Unfortunately, it's not that great either.  But it's a second one, and it works differently, so that's better than nothing; right?  Everything about Passkeys is superior to everything that has come before it.  Well, okay, except for SQRL, but that argument is now academic.



However, while everything I just said is true in theory, it may not, interestingly enough, be completely true in practice.  As we know, security is always about the weakest link in the chain, and that's every bit as true for authentication.  If a Passkey is set up for website login, that system of authentication will be the most bulletproof solution mankind knows how to design today.  But if that website also still supports username and password authentication, with or without any second factors, then that username and password fallback will reduce the entire system's effective security, the fact that it's still there.



The problem is that, while Passkeys are actually vastly more secure than username and password login, they're primarily seen as being more convenient.  That is, Passkeys; right?  They're vastly more secure than username and password login, but most people just see them as more convenient.



MIKAH:  Yeah.



STEVE:  In other words, it's not necessarily the case that enabling the use of a Passkey will simultaneously and robustly disable all use of any preceding username and password.  And that is what we really want.  If Passkeys are enabled, we want usernames and passwords to be disabled.  If you've never used a website which asks you right off the bat to create an account for the first time ever, and offers to let you use a Passkey with no - never giving it a username or password - you're golden since you won't have ever given that website any secrets to keep because Passkey doesn't.  It cannot disclose what it doesn't know.



The question is, will websites that start allowing the use of Passkeys also allow their users to then erase all traces of, or firmly disable the use of, their previous username and password for login?  Because if they don't, the use of Passkeys is only giving them, the user, more convenience.  It also is more secure in the transaction than a username and password because it can't be intercepted in the same way that a username and password can on the fly.  But it's not the absolute security that it could be if it's still possible to compromise the old-school login.



Okay, now, this problem was something that my design of SQRL anticipated.  After its user had become comfortable with the way SQRL operated and had printed out the necessary identity backups, they were able to turn on a setting that requested every site they subsequently visited to please disable any and all alternative means for logging in, including specifically the email loop fallback.  You know, because I've been doing security for a while, I understood that this is a matter of security only being as good as the weakest link.



Okay.  So here are our takeaways from this:  The crypto technology which is well hidden inside Passkeys totally blows away both username and password and even multifactor authentication, which all rely upon server-side secrets being kept.  That's their universal weakness, which is what makes Passkeys not only more convenient, but also far more secure.  So whenever given the choice, set up a Passkey without a second thought and without looking back.



Secondly, look around to see whether there's any way to disable any other previous less secure logon fallback.  If you cannot disable username and password login, but you are able to strengthen it with MFA, then go ahead and do that.  Adding MFA is still the best way to make username and password login safer against attack, even if you're not using your username and password any longer because you've switched over to Passkeys.



But one thing you can definitely do is remove your old username and password from your password manager.  And if for some reason you cannot remove it, change it to something bogus, like a long string of pound signs.  That will be your sign that this account uses Passkeys.  And by removing your password manager's storage of your account's true password, you're protected from any compromise of that password manager or its cloud backup provider.



The big takeaway here is that, even though they may not look all that different from the outside, the technology underlying Passkeys makes them an unreserved win over everything else that has come before.  Now that our password managers are fully supporting Passkeys natively, which is what we've been waiting for, whenever possible, make the switch, and then follow up by doing anything you can to remove the use of the username and password login that preceded it.



And Stephan, thank you for the question.  This has obviously been an issue that's needed some time and clarification, and I'm delighted that we were able to give it.  Passkeys really do rock.



MIKAH:  Wow.  Okay.  So I have to ask you, how does this compare then - because I was also thinking another thing that sites will often provide you with are those special-use one-time codes that you're supposed to save, that they also are storing obviously, that could then be grabbed off of the site if it was hashed and somehow they got access to it or whatever.  But what about if I stick my little key into the side of - my hardware key.  Is that somewhere in between where Passkeys exist, as the multifactor authentication option, I mean?  I take my little YubiKey and put it into the side and touch it.  Does that provide a little bit more security than using those one-time codes?



STEVE:  Yes, much.  Assuming that it's a FIDO key, and that's the key.  That's the key, so to speak.  Assuming it's a FIDO key, then it is essentially doing what Passkeys is doing.  What Passkeys is, is technically FIDO2.  And the reason FIDO2 happened was that the uptake of the hardware dongles to do FIDO1 was just too slow.  People, you know, again, only one third of the Internet are using password managers?  Two thirds are still using monkey123, or maybe 456?  I mean, that just shows you that this is a heavy lift.



So people just don't really give that much concern to their login security.  So they're not going to go out and buy, you know, spend any money to buy something that they just think, well, you know, what the heck?  I'll just use monkey.  Why do I need a key?  So as long as that dongle is doing FIDO login, and if it's at all recent, that's why you have it, that's what it's doing, then it is performing the same sort of public key transaction that Passkeys does.



MIKAH:  Got it.  And you brought up a really good point, and then we'll say goodbye.  But I just, I really liked what you said earlier about how we view it as so convenient, and that alone makes us maybe go, oh, I don't want to use this, it's too easy.  That must mean that it's not as good.



STEVE:  Right.  We don't get to be a secret agent.



MIKAH:  Yeah.  It sort of sits, like, is it secure?  Because all I did, I just did it this morning, I needed to relog into a Google account that the session had expired, and I popped up to say use my Passkey, and it just went right in.  I'm like, man, this is just so easy.  It's so much simpler.  But yeah, we think, oh, but how come I didn't have to whisper something into the phone and scan my retina and do all this other stuff?  It can't be as...



STEVE:  Tap your heels three times.



MIKAH:  Yeah, exactly, exactly.  Steve Gibson, I want to thank you for an information-packed episode.  I certainly learned a lot today.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#966

DATE:		March 19, 2024

TITLE:		Morris the Second

HOSTS:	Steve Gibson & Mikah Sargent

SOURCE:	https://media.grc.com/sn/sn-966.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Voyager lives!  (Maybe).  The World Wide Web just turned 35.  What does its dad think?  What's the latest horrific violation of consumer privacy to come to light?  Our listeners have been extremely engaged and interested in several of this podcast's recent topics so we're going to use their feedback to finish off several of those topics.  And finally, we look at how a group of Cornell University researchers managed to get today's generative AI models to behave badly, and at just how much of a cautionary tale this may be.



SHOW TEASE:  Coming up on Security Now!, first we talk about Voyager.  You may have thought that Voyager was buh-bye, but it is not.  It is still sending signals.  Plus, you know, Tim Berners-Lee, the Father of the Web, sits down and has a serious talk, well, at least as far as Tim Berners-Lee sees it, about the state of the Internet and maybe some disagreements he has with the way it is all going.  We also have a lot of feedback from listeners about last week's episode regarding Passkeys and the current state of security online when it comes to passwords and two-factor authentication and what you should or shouldn't use.  And we also talk about Morris the Second, a worm that has some serious implications for generative AI agents.  All of that and so much more coming up on Security Now!.



MIKAH SARGENT:  This is Security Now! with Steve Gibson and this week Mikah Sargent, Episode 966, recorded Tuesday, March 19th, 2024:  Morris the Second.



Hello and welcome to Security Now!, the show where the cybersecurity guru, Steve Gibson, provides a week's worth of cybersecurity news in but a small package, so you can just plug in directly and download it into your cranium.  I am just here to help facilitate this.  I am, how do you say, let's go with the super safe USB flash drive that you can plug into your cranium.  I'm just providing the means by which you connect to Steve Gibson, who actually is providing all of the information.  That's the role I play here.  Also the role of shock and awe because occasionally I am gobsmacked by what Steve ends up telling us.  But Steve, it is good to see you again this week.  How you doing?



STEVE GIBSON:  Mikah, great to be with you for our second week in a row, as Leo finishes working on his tan and presumably gets ready to return.  So we're going to do, as we thought we were going to do last week, but got pushed because of, wow, what turned out to be a surprisingly interesting episode for our listeners, the Passkeys vs Multifactor Authentication.  In fact, I was so overwhelmed with feedback from that, and it was useful stuff, comments and questions and so forth, that it ended up still being a lot of what we end up talking about today just because, you know, certainly the whole issue of cross-network proving who you say you are, which is to say authentication, is a big deal, and important to everybody who's using the Internet.  So we're going to talk about that some more.



But we are going to get to what was supposed to be last week's topic, which I had originally as Morris II, but I saw that they're referring to themselves or their creation as Morris the Second.  So Morris the Second is today's number 966 and counting Security Now! podcast title.  But first we're going to talk about how it may be that we were doing the requiem for Voyager I a little prematurely.  It may not be quite dead or insane or whatever it was that it appeared to be a week ago.  Also, the World Wide Web has just turned 35.  What does its dad think about how that's going?  What's the latest unbelievably horrific violation of consumer privacy which has come to light?



We're going to share a lot about what our listeners thought about Passkeys and multifactor authentication and the ins and outs of all that.  And then, as I promised, we're going to look at how a group of Cornell University researchers managed to get today's Generative AI to behave badly, and at just how much of a cautionary tale this may be.  So I think a lot of interesting stuff for us to talk about.



MIKAH:  Absolutely.  Again, shock and awe, I can't wait.  I'm looking forward to it. 



STEVE:  So, okay.  I have a large collection of photos in my archive that I'm ready to deploy on demand.  But this one just caught me by surprise.  Somebody tweeted it, and I thought it was so cute.  The caption I gave it was, "Wait.  You mean you did not put this wonderful gymnasium on the floor because it's the perfect space for us to play in?"  And we have an open old-school large computer case which three kittens have managed to get themselves into, and a fourth one is sort of looking on enviously.



One of them, the upper kitten looks like it's standing on maybe an audio card or a graphics card that's been added to the case.  I actually have a number of these exact cases.  I look at it, and it's very familiar.  The motherboard in there is actually pretty ancient, so I don't know what the story is, where this came from or what's going on.  And the hard drives look like they are in need of some help.  But anyway, just a fun picture of just - and my god, these little kittens are adorable.



MIKAH:  They're so adorable.



STEVE:  I mean, how could anything be that cute as these things are?  So not exactly a cat video, but cat video done Security Now! style.



MIKAH:  Yes, indeed.  Those adorable little cats that you should absolutely keep - you know, I like to imagine that this is somebody who brought in their computer to a place and said, "There's something wrong with this thing."  The person that's cleaning it out opens it up, and these little kittens are inside playing.  It's like, ah, that's what's wrong.



STEVE:  Yeah, a little fur ball, that's right.



MIKAH:  Isn't that kind of - that's what SpinRite does; right?  It just sends little kittens inside to fix the hard drive?



STEVE:  That's my secret formula.



MIKAH:  Aha, I figured it out.



STEVE:  Yes.  That's right.



MIKAH:  All right.  Let's get to the security news.



STEVE:  Okay.  So we have a quick follow-up to, as I said, our recent, perhaps premature eulogy for the Voyager 1 spacecraft.  It may just turn out to have been a flesh wound.  The team occupying that little office space in Pasadena instructed Voyager to alter a location of its memory in what everyone who's covering this news is calling a "poke" instruction.  Okay, now, peek and poke were the verbs used by some higher level languages when the code, which is talking in terms of variables and not in terms of storage, wished to either directly inspect, which was to say "to peek," or to directly alter, "to poke," the contents of memory.



So for the past several months there has been a rising fear that the world may need to say farewell to the Voyager 1 spacecraft after it began to send back just garbled data that nobody understood.  And so we were saying, well, it's lost its mind.  It's gone insane.  It's just spitting gibberish.  But after being poked just right, and then waiting, what, 22.5 hours twice I think is the current roundtrip time, the speed of light roundtrip - so, you know, you poke, and then you're very patient - the Voyager 1 began to read out the data from its Flight Data Subsystem, the FDS.  That is, basically, it began doing a memory dump.



And this brought renewed hope that the spacecraft is actually still, somewhat miraculously, in better condition than was feared.  In other words, it hasn't gone insane.  And the return of the Flight Data Subsystem memory will allow engineers to dig through the returned memory readout for clues.  Although, paradoxically, the data was not sent in the format that the FDS is supposed to use when it's working correctly, it is nevertheless readable.  So we're not out of the woods yet, and it still could be unrecoverable, and this is just another one of its death throes.  And really, I mean, realistically, at some point it will be.  I mean, these veterans are going to have to turn the lights off for the last time and put their office space back up for lease.  But apparently not yet.



So I expect that we'll be checking in from time to time to see how our beloved Voyager 1 spacecraft is doing.  But the game is not up yet.  So that's very cool.  And at this point, you know, it's not clear how much new science is being sent back.  I mean, it was incredibly prolific while it was moving through the planets of the solar system and sending back amazing photos of stuff that we'd never seen before.  At this point it's sort of being kept alive just because it can be.  So, you know, why not?  It's not very expensive to do.



Okay.  So the web officially turned 35, and its dad, Tim Berners-Lee, has renewed his expression of his disappointment over how things have been going recently.



MIKAH:  Oh, this is one of those "I'm not angry, I'm disappointed" situations?  I was hoping he was happy with his son.



STEVE:  Yes, my son.  I'm disappointed in the way you have turned out.



MIKAH:  Oh.



STEVE:  So one week ago, on March 12th, Tim wrote, he said:  "Three and a half decades ago, when I invented the web" - which, you know, few people can say - "its trajectory was impossible to imagine.  There was no roadmap to predict the course of its evolution.  It was a captivating odyssey filled with unforeseen opportunities and challenges.  Underlying its whole infrastructure was the intention to allow for collaboration, foster compassion, and generate creativity."  Okay, I would argue that we got two of those three, at least; you know?  He says:  "What I term the three C's."



Now, of course a lot of this is retrospective; right?  It's like, it's easy to rewrite history 35 years later.  But we'll see.  Anyway, he says:  "It was to be a tool to empower humanity.  The first decade of the web fulfilled that promise.  The web was decentralized, with a long tail of content and options.  It created small, more localized communities, provided individual empowerment, and fostered huge value.  Yet in the past decade, instead of embodying these values, the web has instead played a part in eroding them.  The consequences are increasingly far reaching.  From the centralization of platforms to the AI revolution, the web serves as the foundational layer of our online ecosystem, an ecosystem that is now reshaping the geopolitical landscape, driving economic shifts, and influencing the lives of people around the world.



"Five years ago, when the web turned 30, I called out some of the dysfunction caused by the web being dominated by the self-interest of several corporations that have eroded the web's values and led to breakdown and harm.  Now, five years on, as we arrive at the web's 35th birthday, the rapid advancement of AI has exacerbated these concerns, proving that issues on the web are not isolated, but rather deeply intertwined with emerging technologies.



"There are two clear, connected issues to address.  The first is the extent of power concentration, which contradicts the decentralized spirit I originally envisioned."  If indeed he originally did.  He says:  "This has segmented the web, with a fight to keep users hooked on one platform" - gee, wonder what that could be - "to optimize profit through the passive observation of content."  You know, like while they drool.  "This exploitative business model is particularly grave in this year of elections that could unravel political turmoil.  Compounding this issue is the second, the personal data market that has exploited people's time and data with the creation of deep profiles that allow for targeted advertising and ultimately control over the information people are fed.



"How has this happened?  Leadership, hindered by a lack of diversity, has steered away from a tool for public good and one that is instead subject to capitalist forces resulting in monopolization.  Governance, which should correct for this, has failed to do so, with regulatory measures being outstripped by the rapid development of innovation, leading to a widening gap between technological advancements and effective oversight.  The future," he writes, "hinges on our ability to both reform the current system and create a new one that genuinely serves the best interests of humanity."  To which I'm just going to insert here, good luck with that.



Anyway:  "To achieve this," he writes, "we must break down data silos to encourage collaboration, create market conditions in which a diversity of options thrive to fuel creativity, and shift away from polarizing content to an environment shaped by a diversity of voices and perspectives that nurture empathy and understanding."  Or we could just all watch cat videos because, you know, those are cute.  Anyway, he says:  "To truly transform the current system, we must simultaneously tackle its existing problems and champion the efforts of those visionary individuals who are actively working to build a new, improved system.



"A new paradigm is emerging, one that places individuals' intention rather than attention at the heart of business models, freeing us from the constraints of the established order and returning control over our data.  Driven by a new generation of pioneers, this movement seeks to create a more human-centered web, aligned with my original vision.  These innovators hail from diverse disciplines  research, policy, and product design  united in their pursuit of a web and related technologies that serve and empower us all.  Bluesky and Mastodon don't feed off of our engagement, but still create group formation.  GitHub provides online collaboration tools.  And podcasts contribute..."



MIKAH:  Nice.



STEVE:  "...to community knowledge.  As this emergent paradigm gains momentum" - I should mention podcasts that are disappearing rapidly, unfortunately.  "As this emergent paradigm gains momentum, we have the opportunity to reshape a digital future that prioritizes human well-being, equity, and autonomy.  The time to act and embrace this transformative potential is" - guess what.



MIKAH:  Now.



STEVE:  Now.  Uh-huh.  "As outlined in the 'Contract for the Web,' a multitude of stakeholders must collaborate to reform the web and guide the development of emerging technologies. Innovative market solutions, like those I've highlighted, are essential to this process.  Forward-thinking legislation" - okay, now, there's an oxymoron for you - "from governments worldwide can facilitate these solutions and help manage the current system more effectively.  Finally, we as citizens all over the world need to be engaged and demand higher standards and greater accountability for our online experiences.  The time is now to confront the dominant system's shortcomings while catalyzing transformative solutions that empower individuals.  This emergent system, ripe with potential, is rising, and the tools for control are within reach."



MIKAH:  It's starting to sound like a manifesto a little bit.



STEVE:  It really is.  And I only have a little bit more, and then I'm going to - we'll discuss this.  "Part of the solution is the so-called Solid Protocol" - capital S, capital P - "a specification and a movement to provide each person with their own 'personal online data store,' known as a POD."  P-O-D; right?  Personal Online Data.  "We can return the value that has been lost and restore control over personal data."  By putting it in a POD.  "With Solid, individuals decide how their data is managed, used, and shared.  This approach has already begun to take root, as seen in Flanders, where every citizen now has their own POD after Jan Jambon announced four years ago that all Flanders citizens should have a POD.  This is the result of data ownership and control, and it's an example of the emergent movement that is poised to replace the outdated incumbent system."



And finally, "Realizing this emergent movement won't just happen" - boy, is he right about that.  Oh, I mean, he says:  "It requires support for the people leading the reform, from researchers to inventors to advocates.  We must amplify and promote these positive use cases, and work to shift the collective mindset of global citizens.  The Web Foundation, that I co-founded with Rosemary Leith, has and will continue to support and accelerate this emergent system and the people behind it.  However, there is a need, an urgent need, for others to do the same, to back the morally courageous leadership that is rising, collectivize their solutions, and overturn the online world being dictated by profit to one that is dictated by the needs of humanity.  It is only then that the online ecosystem we all live in will reach its full potential and provide the foundations for creativity, collaboration, and compassion."  Tim Berners-Lee, 12th of March, 2024.



MIKAH:  Well, you've got your PODs; right?



STEVE:  Call me jaded.  Call me old.  But I do not see any way for us to get from where we are today to anything like what Tim envisions.  The web has been captured - hook, line and sinker - by commercial interests.  And they are never going to let go.  Diversity?  Well, one browser most of the world uses is maintained by the world's largest advertiser.  And no one forced that to happen.  For some reason most people apparently just like that colorful round Chrome browser icon.  You know, and Chrome is cleaner looking.  Its visual design is appealing. Somehow the word spread that it was a better browser, and nothing convinced people otherwise.  And what Microsoft has done to their Edge browser would drive anyone anywhere else.



But I've wandered away from my point.  People do not truly care about things that they neither see nor understand.  You know?  How do you care about something that you don't really understand?



MIKAH:  Yup.



STEVE:  The technologies that are being used to track us around the Internet and to collect data on our actions are both unseen and poorly understood.  People have some dull sense that they're being tracked, but only because they've heard it said so many times.  Oh, I'm being tracked; you know?  But they don't know.  They don't see it.  They just kind of think, okay.  It makes them feel uncomfortable, but they still do what they were doing; you know?  They don't have any idea what that really means.  They certainly have no idea about any of the details, and they have better things to worry about.



MIKAH:  Yes.  Most importantly, they have better things to worry about.



STEVE:  Yes.  Right.



MIKAH:  Yeah, absolutely.



STEVE:  Tim writes:  "Part of the solution is the Solid Protocol, a specification and a movement to provide each person with their own 'personal online data store,' known as a POD.  We can return the value that has been lost and restore control over personal data."  Now, okay.  While I honor Tim's spirit and intent - I really do - I seriously doubt that almost anyone could be bothered to exercise control over their online personal data repository.  I mean, I don't even know what that looks like.  The listeners of this podcast would likely be curious to learn more.  But as one of my ex-girlfriends used to say, "We're not normal."



My feeling is that the web is going to do what the web is going to do.  Yes, there are things wrong with it.  And, yes, it can be deeply invasive of our privacy.  But it also appears to be largely self-financing, apparently at least in part thanks to those same privacy invasions.  We pay for bandwidth access to the Internet, and the rest is free.  Once we're connected, we have virtually instantaneous and unfettered access to a truly astonishing breadth of information.  And it's mostly free.  There are some annoying sites that won't let you in without paying, so most people simply go elsewhere.



The reason most of the web is free is that, with a few exceptions such as Wikipedia, for-profit commercial interests see an advantage to them for providing it.  Are we being tracked in return?  Apparently.  But if that means we get everything for free, do we really care?  If having the Internet know whether I wear boxers or briefs means that all of this is opened up to me without needing to pay individually for every site I visit, then, okay, briefs have always been my thing.



Tim may have invented the World Wide Web 35 years ago, but he certainly did not invent what the web has become.  That of course is why he's so upset.  The web has utterly outgrown its parent, and it's finding its own way in the world.  It is far beyond discipline, and far beyond control.  And most importantly of all, today it is already giving most people exactly what they want.  Good luck changing that.



MIKAH:  Well put, Steve.  Honestly.  When I think about this - and here you go.  You said maybe you're jaded and old and this and that and the other.  I may be jaded, but I'm not exactly aged.  And so even as a relative youth, hearing that, you know, I want to, I don't know, put on a French beret and chant and say hurrah and feel it.  And I do feel it.  But I think realistically it is not - it's not realistic, if we're being honest.



STEVE:  Right. 



MIKAH:  And so as cool as that would be, and as amazing as that would be, yeah, ultimately what you're saying about the stuff that Tim is talking about here, you know, Tim Berners-Lee is talking about here, is so abstracted from how people use these devices to connect to the Internet and to communicate with one another that, yeah, it would require some level of sitting everyone down across the entire world and explaining to them how all of this works for there to be even the beginning of a concern about what would be necessary to convince everybody that they should care about this.  And as I'm saying just then, you heard all the hedging that kind of took place there.  It wouldn't even necessarily make a difference, even if you did explain it, because they still have to care about it.  And most importantly, most people don't need to care about it.  And so they...



STEVE:  Yes. 



MIKAH:  ...have bigger, better things in their world that they have to care about.  And that is, I think, always going to be the case.  And that's, you know, the people who do care about this stuff, we do our best to communicate and educate.  But, yeah, I don't know, I mean, as much as you might - I mean, I don't know.  To take the time to write all this out and to put forth this idea I think is a very noble thing.  But I do wonder.  I wish I could talk to Tim Berners-Lee sort of just, you know, face to face and say...



STEVE:  What are you thinking?



MIKAH:  Yeah, what are you - do you really think that anyone's going to do this?  Or are you  just - this is just a hopeful sort of I'm putting it out into the world, like...



STEVE:  How high is that ivory tower?



MIKAH:  Exactly.  Exactly.  We're down here.  Yeah.  I don't know.



STEVE:  Yeah.  And again, I really do believe that most users' wishes are now being fulfilled.



MIKAH:  Right.



STEVE:  You know?  I mean, my wife asks me a few questions every evening, and I say, well, did you google it?  You know, it's like, that's what I do.  I just ask.  I ask the Internet, and it tells me the answer.  Because there's so much going on, it's so complicated now, that the right model is no longer to try to know everything.  It's simply to know how to find out.  You know, that's the future, with the knowledge explosion that we're now in, and the content explosion.



So I just, again, there was an anecdote for a while, I don't remember now exactly what it was, but it was something like I have - back when people were typically using a single password, like, universally for all their stuff, someone did an experiment where they went up to people and said, here's a lollipop.  I'll trade you for your password.  And most people said okay.  You know?  I mean, they just didn't give a rat's ass, you know, about security.



MIKAH:  Right.



STEVE:  And most people just aren't as focused, I mean, this podcast is all about this kind of focus.  And as I said, an ex-girlfriend used to say to me, "You're not normal."  So, yeah, we're not.  But, you know, most of the world, they just, you know, the Internet does what they want.  And start asking them to pay...



MIKAH:  Right.



STEVE:  ...in some significant way?  I mean, look at the Club.  Look at Club TWiT.  I mean...



MIKAH:  Yeah, it's a very small percentage of the overall listener base, yes.  And, I mean, and you say pay in some significant way.  Anecdotally, it is going to have to be a significant amount of payment for somebody to go, well, suddenly I don't care about that anymore.  There have been a number, like an app, oh, I saw that everybody's posting these photos of themselves that have been AI generated.  How do they do that?  I say, oh, it's this app, and you pay like 56 cents to get a photo generation.  Oh, never mind.  I don't care about that anymore. 



STEVE:  Yeah.



MIKAH:  It's 50 cents.  But yeah, it doesn't take that much for them to be like, no no no no no.  That's not something I'm into.



STEVE:  No.  And as we know, there are some sites which have survived in the "pay to enter" model.  But many of the early attempts fell flat because the moment the sites put up a paywall, most people said, eh, you know, I clicked the first link in Google, and it took me to the paywall.  What's the second link take me to?  Oh, look, it's free.



MIKAH:  Able to get to it, yeah.



STEVE:  Okay.  So In the show notes, I gave the title of this bit of news the title "Wow, Just Wow" because it tells the story of something that's so utterly violating of consumer rights and privacy that it needed that title.  The headline in last week's New York Times read:  "Automakers Are Sharing Consumers' Driving Behavior With Insurance Companies."  And the subhead read:  "LexisNexis, which generates consumer risk profiles for insurers, knew about every trip GM drivers had taken in their cars, including when they sped, braked too hard, or accelerated rapidly."



MIKAH:  Wow.



STEVE:  Okay.  So here - it's astonishing.



MIKAH:  Yeah, wow.



STEVE:  I know, exactly.  Here's the real-world event that The New York Times used to frame their disclosure.  They wrote:  "Kenn Dahl says" - now, that's not D-O-L-L, that's D-A-H-L, and it's K-E-N-N, Kenn Dahl - "says he has always been a careful driver.  The owner of a software company near Seattle, he drives a leased Chevrolet Bolt.  He's never been responsible for an accident.  So Mr. Dahl, at age 65, was surprised in 2022 when the cost of his car insurance jumped by 21%.  Quotes from other insurance companies were also high.  One insurance agent told him his LexisNexis report was a factor.



"LexisNexis," they write, "is a New York-based global data broker with a Risk Solutions division that caters to the auto insurance industry and has traditionally kept tabs on car accidents and tickets."  Okay, right, public record things; right?  I mean, like accidents and tickets, that's out there.  "Upon Mr. Dahl's request, LexisNexis sent him a 258-page 'consumer disclosure report,' which it must provide per the Fair Credit Reporting Act.  What it contained stunned him:  more than 130 pages detailing each time he or his wife had driven the Bolt over the previous six months.  It included the dates of 640 trips, their start and end times, the distance driven, and an accounting of any speeding, hard braking, or sharp accelerations.  The only thing it didn't have is where they had driven the car.  On a Thursday morning in June, for example, the car had been driven 7.33 miles in 18 minutes.  There had been two rapid accelerations and two incidents of hard braking.



"According to the report, the trip details had been provided by General Motors, the manufacturer of the Chevy Bolt.  LexisNexis analyzed that driving data to create a risk score 'for insurers to use as one factor of many to create more personalized insurance coverage,' according to a LexisNexis spokesman, Dean Carney.  Eight insurance companies had requested information about Mr. Dahl from LexisNexis over the previous month.  Mr. Dahl said:  'It felt like a betrayal.  They're taking information that I didn't realize was going to be shared and screwing with our insurance.'"



Okay, now, since this behavior is so horrifying, I'm going to share a bit more of what The New York Times wrote.  They said:  "In recent years, insurance companies have offered incentives to people who install dongles in their cars or download smartphone apps that monitor their driving, including how much they drive, how fast they take corners, how hard they hit the brakes, and whether they speed.  But Ford Motor put it:  'Drivers are historically reluctant to participate in these programs.'"  And this was written in a patent application that describes what is happening instead:  "Car companies are collecting information directly from Internet-connected vehicles for use by the insurance industry."  In other words, monetizing; right?  Because you know the insurance industry is paying to receive that information.  So another means by which today's consumer is being monetized without their knowledge.



The New York Times says:  "Sometimes this is happening with a driver's awareness and consent.  Car companies have established relationships with insurance companies, so that if drivers want to sign up for what's called 'usage-based insurance'  where rates are set based on monitoring of their habits  it's easy to collect that data wirelessly from their cars.



"But in other instances, something much sneakier has happened.  Modern cars are Internet-enabled, allowing access to services like navigation, roadside assistance, and car apps that drivers can connect to their vehicles to locate them or unlock them remotely.  In recent years, automakers including GM, Honda, Kia, and Hyundai have started offering optional features in their connected-car apps that rate people's driving.  Some drivers may not realize that, if they turn on these features, the car companies then give information about how they drive to data brokers like LexisNexis."  And again, not give, sell.



"Automakers and data brokers that have partnered to collect detailed driving data from millions of Americans say they have drivers' permission to do so.  But the existence of these partnerships is nearly invisible to drivers, whose consent is obtained in fine print and murky privacy policies that few ever read.  Especially troubling is that some drivers with vehicles made by GM say they were tracked even when they did not turn on the feature, called OnStar Smart Driver; and that their insurance rates went up as a result."



MIKAH:  I do have a problem with that last bit, simply because someone says that.  I almost wish that there was some due diligence there.  I'm sure you've seen it.  Somebody's having an issue, a tech issue, and you say, "Oh, here's how you fix it," and they say, "I've done that," and then you go and check and they didn't do that thing that you told them to do and that they should have done it.



STEVE:  Right.



MIKAH:  I wouldn't be surprised that they did accidentally opt-in.  But all that's to say, whether you opt in or not, this is still something that should be brought to light.  And as for all of it, especially if it's kind of being put forth as an idea of, oh, here are these cool features you get, and secretly underneath what it's doing is giving access, yeah, that's bad.  I just, I don't know if I like that from The New York Times there at the end.



STEVE:  Well, so one analogy that occurs to me is how - and we've mentioned this a number of times in prior years during the podcast - is employees in an organization sometimes believe that what they do on their corporate computer is private, is, like, their business, and even when the employee agreement and occasional reminder meetings and so forth say that's not the case, that this is a corporate network, corporate bandwidth, a corporate computer, and what you do is owned by the company.  So we have suggested that that really ought to be on a label running across the top of their monitor.



MIKAH:  Yes.



STEVE:  Like, it literally ought to say right in front of them, you know, please remember that everything you do on this computer, which is owned by the company, on the bandwidth owned by the company, and the data owned by the company, is not private.



MIKAH:  Right.



STEVE:  Similarly, by analogy, the screens that all these computers have, imagine if they said along the bottom, your driving is being monitored by the company you purchased this from and is being sold to your car insurance provider.



MIKAH:  They don't want to do that, Steve.



STEVE:  Obviously we're never going to see that.  But, you know, that's the point, is that this is going on surreptitiously, and it being surreptitious is clearly wrong.  So anyway, stepping back, okay, from the specifics of this particularly egregious behavior, add the context of Tim Berners-Lee's unhappiness with what the web has become, and the growing uneasiness over the algorithms being used by social media companies to enhance their own profits, even when those profits come at the cost of the emotional and mental health of their own users, we see example after example of amoral aggressive profiteering by major enterprises, where the operative philosophy appears to be "We'll do this to make as much money as we can, no matter who is hurt, until the governments in whose jurisdictions we're operating get around to creating legislation which specifically prohibits our conduct.  But until that happens, we'll do everything we can to fight against those changes, including where possible lobbying those governmental legislators."



MIKAH:  Honestly, we could just take that text and slap it on the screen anytime we talk about any antitrust legislation across any of our shows, and that perfectly sums up exactly what's going on in every single case.



STEVE:  It's like, make us stop.



MIKAH:  Yeah, exactly.



STEVE:  And until you do, we're going to use every clever means we have of profiting from every area in which we have not been made to stop.  You know, there's no more morality.  There's no more ethics.  It's, you know, it's profit, profit, profit, profit, profit.



MIKAH:  Wherever possible, yeah.



STEVE:  And that is exactly Tim Berners-Lee's complaint, and it's never going to change.  Because it's just too pervasive; right?  I mean, it just; you know?  And again, as a consequence of this, you know, the Internet is largely free.  And I think that's a tradeoff most people would choose to make, rather than having to pay, like, you know, remember that there was early talk about micropayments where, when you went to a website, it would ding you some fraction of a something or other?  And that would make people very uncomfortable.  They'd be like, well, wait a minute, you know, suddenly links are not free to click on.



MIKAH:  Yeah, exactly.



STEVE:  There's a cost to clicking on that link.



MIKAH:  How many times per month should I click on this?  And you've got - you're telling your kids don't click on links.  And it would just completely reshape everything.  And then there would be so many - I can imagine how much more money and time would have to go into customer support because someone would click on a link and then say, I'm not satisfied with this page, and I don't want to have paid for this page because I didn't get the answer I wanted.



STEVE:  That's a very good point.  That's a very good point because right now it's like, well, it's free.  So, you know, go pound sand somewhere; you know?  It's like, tough.



MIKAH:  Yeah.



STEVE:  Well, again, I don't mean to be just simply complaining because I also recognize, as I said, this is why the web is here.  I mean, I was present during, like, pre-web, during the early days, and when there was like not much on the 'Net.  And the question was, well, why is anyone going to put anything on the Internet because there's nobody on the Internet to go see it.  So there was like this chicken-and-egg problem; right?  No, like, vast population are actually going onto the Internet to do anything.  So why is anyone going to put anything there?  And if no one puts anything there, then no one is going to be incentivized to go and get what's not there.  So it happened anyway.



And the way it's evolved, as I said, I'm really - I'm actually not complaining.  This is not, I mean, if it sounds like I'm, you know, doing some holier than thou rant, it's not the case.  I like it the way it is.  And those of us who are clever enough to mitigate the tracking that's being done and the monetizing of ourselves, well, we get the benefit that is being reaped by all those who aren't.  So it works.  



MIKAH:  Steve Gibson, let's close that loop.



STEVE:  Let's do it.  So Montana J, he wrote:  "Hey, a flaw in Passkey thinking.  I teach computer science at a college.  Like many in the educational field, I log onto a variety of computers a day that are used by myself, fellow instructors, and students.  Using a Passkey in this environment would allow others to easily gain access to my accounts.  Not a good thing.  So turning off passwords is not an option.  Just something to think about.  Jim."



Okay, right.  Well, it's a very good point which I tend to forget since none of my computers are shared.  But in a machine-sharing environment there are two strong options.  FIDO in a dongle is one way to obtain the benefits of Passkey-like public key identity authentication while achieving portability.  Right?  Your Passkeys are all loaded into this dongle, and that's what the website uses.  But also, reminiscent of the way I designed SQRL originally, a smartphone can completely replace a FIDO dongle to serve as a Passkeys authentication client by using the QR code presented by a Passkeys website.



And in that model, Passkeys probably provides just about the best possible user experience and security for shared computer use.  So you go to a site.  You log in, only with giving them your username.  At that point the site looks up your username, sees that you have registered a Passkey with the site, and moves you over to the Passkey login.  Part of that will be a QR code.  You take your Android or Apple phone, open the Passkey app, let it see the QR code, and you're logged in.  So that computer and the web browser never has access to your Passkeys.  They remain in your phone.  So it's absolutely possible to get all the benefit of Passkeys in a shared usage model with arguably the best security around.



MIKAH:  I have to say I'm kind of confused by Jim's suggestion.  I don't understand.  Jim is suggesting somehow that after he sits down, logs into stuff, is done, logs out, that another person could sit down and log in because of a Passkey.  How is that - I don't understand how that would even work.  Is Jim suggesting that there's a...



STEVE:  I believe it's because last week one of the things we talked about was Passkeys being stored in a password manager in the browser.



MIKAH:  Ah.  But you would - okay.  So if you forgot to log out of the - oh, I see.  If it's in the browser, and some person uses it across - okay, got you.  That makes sense.  Okay.  Got it.



STEVE:  Yeah.  So the idea is you don't want, obviously, a shared machine to store anyone's Passkeys.



MIKAH:  Right.



STEVE:  You want that to all be provided externally on the fly.



MIKAH:  Yeah.  I mean, in theory, you also don't want an in-browser system storing passwords if you're in...



STEVE:  I agree completely.  Exactly.  There should be no password manager, and like would you like me to remember this password for you.  I don't even know if there's a way to turn that off.  That should be like...



MIKAH:  Heck, no.



STEVE:  ...forced off so that, like, it can't even ask you.  Gilding_timings, he wrote:  "Hey, Steve.  I just finished watching Episode 965 on Passkeys vs two-factor authentication.  I was wondering, don't Passkeys just change who is responsible for securing your authentication data?  With passwords and two-factor authentication, the responsibility is with the website.  With Passkeys, the responsibility is with the tool storing the Passkeys, for example, a password manager.  If the password manager is compromised, an attacker has all they need to authenticate as you."  So again, we're talking about storing Passkeys in the password manager, which is something that we talked about last week, which is why our listeners are coming back with questions about this practice, you know, deservedly so.



So he says:  "If a password manager is compromised, an attacker has all they need to authenticate as you.  I would think that, if the website doesn't allow disabling password authentication, then two-factor authentication still has some value, if we're talking about password managers being compromised."  And of course there he's talking about external storage of the two-factor authentication code, like in your phone, which is again something we've also talked about in the past.  He said:  "You can at least store the two-factor authentication data separately from your password manager."  He says:  "I'm loving SpinRite.  It's already come in handy multiple times."  SpinRite 6.1, he said.  "Thanks so much for continuing the show.  I look forward to it every week."



Okay.  So first, thank you.  After three years of work on it, I certainly appreciate the SpinRite feedback, and I'm delighted to hear that it's come in handy.



So here's the way to think about authentication security:  All of the authentication technology in use today requires the use of secrets that must be kept.  All of it.  The primary difference among the various alternatives is where those secrets are kept, and who is keeping them.  In the username/password model, assuming the use of unique and very strong passwords, the secrets must be kept at both the client's end, so that they can provide the secret, and the server's end, so that it can verify the secret provided by the client.  So we have two separate locations where secrets must be kept.



By comparison, thanks to Passkeys' entirely different public-key technology, we've cut the storage of secrets in half.  Now, only the client side needs to be keeping secrets, since the server side is only able to verify the client's secrets without needing to retain any of them itself.  So it's clear that by cutting the storage of secrets in half, we already have a much more secure authentication solution.  But the actual benefit is far greater than 50%.  Where does history teach us the attacks happen?  When the infamous bank robber, Willie Sutton, was asked why he robbed banks, his answer was obvious in retrospect.  He said:  "Because that's where all the money is."



For the same reason, websites are attacked much more than individual users because that's where all the authentication secrets are stored.  So when the use of Passkeys cuts the storage of authentication secrets by half, the half that it's cutting is where nearly all of the theft of those secrets occurs.  So the practical security gain is far more than just 50%.  Now, our listener said:  "I would think that if the website doesn't allow disabling password authentication, then two-factor authentication still has some value if we're talking about password managers being compromised.  You can at least store the two-factor authentication data separately from your password manager."  That's true.  And there's no question that requiring two secrets to be used for a single authentication is better than one, and that storing those secrets separately is better still.



But as we're reminded by the needs of the previous listener who works in a shared machine environment, just like two-factor authentication, Passkeys can also be stored in an isolated smartphone and thus kept separate from the browser.  Having our browsers or password manager extensions storing our authentication data is the height of convenience.  And we're not hearing about that actually ever having been a problem, that is to say, browser extension compromise.  And, you know, that's very comforting.



But a separate device just feels as though it's going to provide more authentication security, if only in theory.  The argument could be made that storing Passkeys in a smartphone still presents a single point of authentication failure.  But it's difficult to imagine a more secure enclave than what Apple provides, backed up by per-use biometric verification before unlocking a Passkey.  So the strongest protection I think you can get today.



Mike Schepers says:  "Hi, Steve.  I'm a long-time listener of Security Now! and love the podcast.  Thank you so much for all your contributions for making this world a better place and freely giving your expertise to educate many people like myself.  I do have a question for you related to Passkeys, Episode 965, that I'm hoping you can help me understand.  There are many accounts that my wife and I share for things like banking and health benefits websites where we both need access to the same accounts.  If they were to use only Passkeys for authentication, is sharing possible?  Thank you, Mike."



In a word, yes.  Whether Passkeys are stored in a browser-side password manager or in your smartphone, the various solutions have all recognized this necessity, and they provide some means for doing this.  For example, in the case of Apple, under Settings > Passwords, it's possible to create a Shared Group for which you and your wife would be members.  It's then possible for members of the group to select which of their passwords they wish to share in the group, and Apple has seamlessly extended this so that it works identically with Passkeys.



Apple's site says:  "Shared password groups are an easy and secure way to share passwords and Passkeys with your family and trusted contacts."  Very trusted.  "Anyone in the group can add passwords and Passkeys to the Group Share.  When a shared password changes, it changes on everyone's device."  So it's a perfect solution.  And yes, that appears to be universal.  So Passkey sharing has been provided.



Senraeth says - well, I got a tweet from him.  And there's been such an outsized interest shown in this topic by our listeners that I wanted to share his restatement and summary of the situation, even though it's a bit redundant, so that everyone can just kind of check their facts against the assertions that he's making.



He said:  "Hi, Steve.  Just listened to SN-965 and have a thought about Passkeys security.  Completely agree with your assessment of the security advantages of Passkeys vs Passwords and multifactor authentication in general.  But another practical difference occurs to me when using a password manager to store your Passkeys.  With password plus MFA, if your password manager is breached somehow, you can still rest easy knowing that only your passwords were compromised" - again, assuming multifactor authentication is in a separate device, right, because password managers are now offering to do your multifactor authentication for you, too.



He said:  "You can still rest easy knowing that only your passwords were compromised, and that hackers could not actually gain access to any of the accounts in your vault that were also secure with a second factor.  Of course, this is not true if you also use your password manager to store your MFA codes, which is why you've said in the past that you would not do that as it puts all of your eggs in one basket."  Right.



"With passkeys stored in a password manager, this is no longer the case.  If the password manager is breached, the hacker can gain access to every account that was secured with the Passkeys in your vault.  So while Passkeys most definitely make you less vulnerable to breaches at each individual site, the tradeoff is making you much more vulnerable to a breach of your password manager, if I'm understanding this correctly," he writes.  "Like the original listener from last week, Stephan Janssen, this leaves me feeling hesitant to use Passkeys with a password manager.  I think using passkeys with a hardware device like a YubiKey would be ideal, but then you have to deal with the issue of syncing multiple devices," he says, "which of course wouldn't have been an issue with SQRL."  True.  "Thanks for all you do."



So Apple and Android smartphones support cross-device passkey syncing and website logon via QR code.  So passkeys remains the winner.  No secrets are stored remotely by websites.  So the impact of the most common website security breaches is hugely reduced.  If you cannot get rid of, or disable, a website's parallel use of passwords, then by all means protect the password with MFA, just so that the password by itself cannot be used.  And perhaps remove the password from your password manager if its compromise is a concern.



So that leaves a user transacting with Passkeys for their logon, and left with the choice of where they are stored, in a browser or browser extension or on their smartphone.  I would suggest that the choice is up to the user.  The listeners of this podcast will probably make a different choice than everybody else.  Right?  Because ease of use generally wins out here.  The browser presents such a large attack surface that the quest for maximum security would suggest that storing Passkeys in a separate smartphone would be most prudent.  But that does create smartphone vendor ecosystem lock-in.



And I'll remind everyone that we do not have a history of successful major password manager extension attacks.  Why, I don't know.  But it just doesn't, you know, like attacks on our - although we're all worried about them, we're worried about the possibility because we know it obviously exists.  But what we see is websites being attacked all the time, not apparently with any success, the password manager extensions.  Which is somewhat amazing, but it's true.  So the worry over giving our Passkeys to our password managers to store is only theoretical.  But it's still a big "what if?"  And I recognize that.



At this point I doubt that there's a single right answer that applies to everyone.  You know, when a user goes to a website that says "How would you like to switch to Passkeys," and they say okay, and they press a button, and their browser says "Done, I know your Passkey now, I'll handle login for you from now on," they're going to go, "Yay."  You know, like, great.  You know, not a second thought.  Not this podcast's audience.  But again, the majority.



And I'll just finish by saying the lack of Passkey portability is a huge annoyance, but we're still in the very early days.  And we do know that the FIDO group is working on a portability spec.  So there is still hope.  I think one of the things that make us feel a little queasy about Passkeys is that we can't see them.  We can't touch them.  We can't hold them.  A password you can see, you can write it down, you can copy it somewhere else, you can copy and paste, I mean, it's tangible.  And as I've said on the podcast, I print out the QR codes of all of my one-time password authenticator QR codes.



Whenever a site gives me one, and I'm setting it up, I make a paper copy.  And I've got them all stapled together in a drawer because, if I want to set up another device, and I'm unable to export and import those, I'm able to expose them to the camera again and recreate those.  So the point is they're tangible.  But at this point no one has ever seen a Passkey.  They're just, like, somewhere in a cloud or imaginary something.  And it makes us feel uncomfortable that, you know, they're just intangible the way they are.



CR said:  "Hi, Steve.  On Episode 965 a viewer commented on how some sites are blocking anonymous http://duck.com email addresses or stripping out the '+' symbol.  I want to share my approach that gets around these issues."  He said:  "First, I registered a web domain with WHOIS privacy protection to use just for throwaway accounts.  I then added the domain to my personal ProtonMail account, which requires a plan upgrade, but I'm sure there are many other email hosting services out there that are cheap or possibly free.



"Finally, I enabled the catch-all address option.  With this in place, I can now sign up on websites using anyname@mydomain, and those emails are delivered to the catch-all in ProtonMail.  You can set up filters or real addresses if you want to bypass the catch-all, should you want some organization.  ProtonMail also makes it really easy to block email senders by right-clicking the email item in your inbox and selecting the block action.  So far this setup has been serving me well for the past year without any problems."



Okay.  So I wanted to toss this idea just out there into the ring as an idea that might work for some of our listeners.  And I agree that it solves the problem of creating per-site or just random throwaway email addresses.  But the problem it does not solve, for those who care, is the tracking problem, since all of those throwaway addresses would be at the same personalized domain.  The reason the @duck.com solution was so appealing is that everyone using @duck.com is indistinguishable from everyone else using @duck.com, making obtaining any useful tracking information from someone's use of @duck.com, or any other similar mass-anonymizing service, futile.  And this, of course, is exactly why some websites are now refusing to accept such domains, and why this may become, unfortunately, a growing trend for which there is no clear solution at this point.  And I don't think there can be one, really.  It's going to be a problem.



Gabe Van Engel said:  "Hey, Steve.  I wanted to send you a quick note regarding the vulnerability report topic over the last two episodes.  I don't know the specifics of the issue the listener reported, but I can provide some additional context as someone who runs an open bounty program on HackerOne.  We require that all reports include a working proof of concept to be eligible for bounty.  The reason is that many vulnerability scanners flag issues simply by checking version headers; however, most infrastructure these days does not run upstream packages distributed directly by the author, and instead use a version packaged by a third party providing backported security patches, for example, repositories from Red Hat Enterprise Linux, Ubuntu, Debian, FreeBSD, et cetera.



"It is totally possible the affected company is vulnerable to the trivial nginx remote code execution.  But if they think the report isn't worth acting on, it's also possible they're running a version which isn't actually vulnerable, but still returns a vulnerable-looking version string.  To be clear, I'm not trying to give the affected company a free pass.  Even if they aren't vulnerable, the timeframe over which the issue was handled, and the lack of a clear explanation as to why they chose to take no action is inexcusable.  All the best.  Keep up the good work.  Gabe."  And he said:  "P.S.:  Looking forward to email so I can delete my Twitter account."



Okay.  So I thought that Gabe's input, as someone who's deep in the weeds of vulnerability disclosures at HackerOne, was very valuable.  And it's interesting that they don't entertain any vulnerability submission without a working proof of concept.  Given Gabe's explanation, that makes sense.  And it's clear because they just have too many false-positive reports, right; and people saying, hey, why didn't I get a payment for my valuable discovery?  It's like, well, it didn't work.



MIKAH:  Yeah, you didn't prove that it actually worked.



STEVE:  Exactly.  And it's clear that a working proof of concept would move our listener's passive observation from a casual case of "Hey, did you happen to notice that your version of nginx is getting rather old?" to "Hey, you better get that fixed before someone else with fewer scruples happens to notice it, too."



As we know, our listener was the former of those two.  He only expressed his concern over the possibility that it might be an issue.  And he even, in his conversation with me, recognized that it could be a honeypot, where they deliberately had this version header and were collecting attacks, though I think he was being very generous with that possibility.  He understood that the only thing he was seeing was a server's version headers, and that therefore there was only some potential for trouble.  And had the company in question clearly stated that they were aware of the potential trouble, but that they had taken steps to prevent its exploitation, the issue would have been settled.  It was only their clear absence of focus upon the problem and never addressing his other questions that caused any escalation in the issue beyond an initial casual nudge.



But Gabe also said:  "Looking forward to email," meaning GRC's soon-to-be-brought-online email system, he said, so that he could delete his Twitter account.  I also wanted to take a moment to talk about Twitter.  Many of this podcast's listeners take the time to express similar sentiments.  And at the same time I receive tweets from listeners arguing that I'm wrong to be leaving Twitter, as well as the merits of Twitter and how much Elon has improved it since his purchase.



MIKAH:  Okay.



STEVE:  So for the record, let me say again that I am entirely agnostic on the topic of Elon and Twitter.  In other words, I don't care.



MIKAH:  Right.



STEVE:  One way or the other.  More than anything, I'm not a big social media user.  What we normally think of as "social media" doesn't interest me at all.  That said, GRC has been running quiet backwater NNTP-style text-only newsgroups for decades, since long before social media existed.  And we have very useful web forums.  But Twitter has never really been social media for me.  I check in with Twitter once a week to catch up on listener feedback, to post the podcast's weekly summary and link to the show notes.  And then, recently, to add our Picture of the Week.



What caught my attention and brought me out of my complacency with Twitter was Elon's statement that he was considering charging a subscription for everyone's participation, thus turning Twitter into a subscription-only service.  That brought me up short and caused me to realize that what was currently a valuable and workable communications facility, for as little as I use it, might come to a sudden end because it was clear that charging everyone to subscribe to use Twitter would end it as a means for most of our current Twitter users to send feedback.  They're literally only using Twitter as I am, to talk to me.



We don't all have Twitter, but we do all have email.  So it makes sense for me to be relying upon a stable and common denominator that will work for everyone.  And since I proposed this plan to switch to email, many people like Gabe have indicated to me, through Twitter, that not needing to use Twitter would be a benefit for them, too.  So I just wanted to say again, to explain again, you know, because there are people [grumbling sounds].  Like, fine, you know, I don't have an issue.



MIKAH:  You're not taking it away.  You're trying to make it available to more people.  That's it.



STEVE:  Right.  That is exactly it.  Exactly it.  And Elon appears to be making it available to fewer, and maybe many fewer.  So, you know, that would be a problem for me, so I'm switching before that happens.



Markzip wrote:  "@SGgrc Just catching the update about the guy who found the flaw in the big site and the unsatisfactory response from CISA/CERT.  I think he should not take the money. I think he should tell Brian Krebs or another high-profile security reporter.  They can often get responses."



Okay, now, this is another interesting possible avenue.  My first concern, however, is for our listener's safety.  And by that I don't mean his physical safety, I mean his safety from the annoying tendency of bullying corporations to launch meritless lawsuits just because they easily can.  Our listener is on this company's radar now, and that company might not take kindly to someone like Brian Krebs using his influential position to exert greater pressure.  This was why my recommendation was to disclose to CISA and CERT.  Being U.S. government bodies, disclosing to them seems much safer than disclosing to an influential journalist.



Now, recall from earlier Gabe from HackerOne.  I subsequently shared my reply with him, and he responded to that.  And he said:  "This is one of the benefits of running a program via HackerOne or other.  By having a hacker register and agree to the program terms, it both lets us require higher quality reports and to also indemnify them against otherwise risky behavior like actually trying to run remote code executions against a target system."



So, yeah.  That indemnification could turn out to be a big deal.  And, of course, when working through a formal bug bounty program like HackerOne, it's not the hacker who interfaces with the target organization.  It's HackerOne who is out in front, so not nearly as easy to ignore or silence an implied threat.



MIKAH:  Are you hearing that, secret person who messaged before?  Perhaps HackerOne would be a good place for you to go next.



STEVE:  Yup.  Another of our listeners said:  "This website with this big vulnerability should be publicly named.  You are doing a disservice to everyone who uses that site by keeping it hidden.  To quote you in your own words, 'security by obscurity is not security.'  Let us know which site it is so that we can take action."



Well, wouldn't it be nice if things were so simple.  In the first place, this is not my information to disclose, so it's not up to me.  This was shared with me in confidence.  The information is owned by the person who discovered it, and he has already shared it with government authorities whose job, we could argue, it actually is to deal with such matters of importance to major national corporations.  The failure to act is theirs, not his, nor mine.



The really interesting question all of this conjures is whose responsibility is it?  Where does the responsibility fall?  Some of our listeners have suggested that bringing more pressure to bear on the company is the way to make them act.  But what gives anybody the right to do that?  Publicly naming the company, as this listener asks, would very likely focus malign intent upon them.  And based upon what I've previously shared about their use of an old version of nginx, the cat, as they say, would be out of the bag.  At this point it's only the fact that the identity of the company is unknown that might be keeping it, and its many millions of users, safe.  Security by obscurity might not provide much security, but there are situations where a bit of obscurity is all you've got.



This is a very large and publicly traded company.  So it's owned by its shareholders.  And its board of directors, who have been appointed by those shareholders, are responsible to them for the company's proper, safe, and profitable operation.  So the most proper and ideal course of action at this point would likely be to contact the members of the board and privately inform them of the reasonable belief that the executives they have hired to run the company on behalf of its shareholders have been ignoring, and apparently intend to continue ignoring, a potentially significant and quite widespread vulnerability in their web-facing business properties.  While some minion who receives anonymous email can easily ignore incoming vulnerability reports, if the members of the company's board were to do so, any resulting damage to the company, its millions of customers, and its reputation would be on them.



Stepping back from this a bit, I think that the lesson here is that at no point should it be necessary for untoward pressure to be used to force anyone to do anything, because doing the right thing should be in everyone's best interest.  The real problem we have is that it's unclear whether the right person within the company has been made aware of the problem.  At this point it's not clear that's happened, through no fault of our original listener who may have stumbled upon a serious problem and has acted responsibly at every step.  If the right person had been made aware of the problem, we would have to believe that it would be resolved, if indeed it was actually a problem.



So my thought experiment about reaching out to the company's board of directors amounts to "going over the heads" of the company's executives who do not appear to be getting the message.  And that has the advantage of keeping the potential vulnerability secret while probably resulting in action being taken.  I'm not suggesting that our listener should go to all that trouble, since that would be a great deal of thankless effort.  The point I'm hoping to make is that there are probably still things that could be done short of a reckless public disclosure which could result in serious and unneeded damage to users and company alike.  And maybe even to the person who made that disclosure.  I mean, likely to the person who made that disclosure.



Marshall tweeted:  "Hi, Steve.  A quick follow-up question to the last Security Now! episode" - okay, here's one more, I thought we were done with them - "on MFA vs Passkeys.  Does the invention" - oh, this is actually a good one.  I know why I put it in here.  "Does the invention of Passkeys invalidate the 'something you have,' 'something you know,' and 'something you are' paradigm?  Or does Passkeys provide a better instantiation of those three concepts?"  Great question.  "Because the idea with multi-factors is that you'd add another factor for greater security.  But with Passkeys, do you still consider those factors?  Thanks for everything you do."



Okay, I think this is a terrific question.  The way to think of it is that the "something you know" is a secret that you're able to directly share.  The use of "something you have" - like a one-time password generator - is actually you sharing the result of another secret you have, where the result is based upon the time of day.  And the "something you are" is some biometric being used to unlock and provide a third secret.  In all three instances, a local secret is being made available through some means.  It's what's done with that secret where the difference between traditional authentication and public key authentication occurs.



With traditional authentication, the resulting secret is simply compared against a previously stored copy of the same secret to see whether they match.  But with public key authentication such as Passkeys, the secret that the user obtains at their end is used to sign a unique challenge provided by the other end.  And then that signature is verified by the sender to prove that the signer is in possession of the secret private key.



Therefore, the answer, as Marshall suggested, is that Passkeys provides a better instantiation of those original three concepts.  For example, Apple's Passkeys system requires that the user provides a biometric face or thumbprint to unlock the secret before it can be used.  Once it's used, the way it's used is entirely different because it's using Passkeys.  But a browser extension that contains Passkeys merely requires its user to provide something they know to log into the extension and thus unlock its store of Passkey secrets.



As we mentioned earlier, all of these traditional factors were once layered upon each other in an attempt to shore each other up, since storing and passing secrets back and forth had turned out to be so problematic.  We don't have this with Passkeys because the presumption is that a public key system is fundamentally so much more secure that a single very strong factor will provide all the security that's needed.  And just for the record, yes, I think the Passkeys should be stored off a browser because, even though we're not seeing lots of browser attacks, they do seem more possible than an attack on an entirely separate facility which is designed for it.



Rob Mitchell said:  "Interesting to learn the advantages of Passkeys.  It definitely makes sense in many ways.  The one disadvantage my brain sticks on, vs TOTP" - time-based one-time passwords - "is that I'd imagine someone who can get into your password manager" - okay, so here he's talking about it, you know, he says hack into a cloud backup or signed onto your computer - "now can access your account with Passkeys.  Like if Passkeys were a thing when people were having their LastPass accounts accessed.  But if your time-based token is only on your phone, someone who gets into your password manager still can't access a site because they don't have the TOTP key stored on your phone.  Maybe Passkeys are still better, but I can't help but see that weakness."



So again, I've been overly repetitive here.  Rob's sentiment was expressed by many of our listeners.  So I just wanted to say that I agree.  And as I mentioned last week, needing to enter that ever-changing secret six-digit code from the authenticator on our phone really does make everything seem much more secure.  Nothing that's entirely automatic can seem as secure.  So storing passkeys in a smartphone is a choice I think that makes the most sense.  And as I've mentioned, the phone can be used to authenticate through the QR code that a Passkeys-enabled site presents to its users.



Christian Turri said:  "Hi, Steve.  On SN-965 you discussed the issue with Chrome extensions changing owner and how devs are being tempted to sell their extensions.  There is a way to be safe when using extensions in Chrome or Firefox."  Now, this is interesting.  "Download the extension, expand it, and inspect it.  Once you are sure it's safe, you can install it on Chrome by enabling Developer Mode under chrome://extensions/ and selecting Load Unpacked.  The extension will now be locally installed, which means it will never update from the store or change.  It's frozen in time.  If it ain't broke, don't fix it.  And if the extension does break in a future update due to Chrome changes, you can get the update and perform the same process again.  While using these steps requires some expertise, it should be fine for most Security Now! listeners."



MIKAH:  Interesting.



STEVE:  Anyway, thank you, Christian.  Yes.  I think that is a great tip, and I bet it will appeal to many of our listeners who generally prefer taking automatic things into their own hands.  So again, chrome://extensions, and then select Load Unpacked, and you're able to basically unpack and permanently store your extensions, which stops Chrome from having, you know, from auto-updating them from the store.  So if an extension goes bad, you get to keep using the good one.  Very cool.



And lastly, Bob Hutzel:  "Hi, Steve.  Before embracing Bitwarden's Passkey support, it is important to note that it is still a work in progress.  Mobile app support is still being developed.  Also, Passkeys are not yet included in exports.  So even if someone maintains offline vault backups, a loss of access to or corruption of the cloud vault means Passkeys are gone.  Thank you for the great show.  Bob Hutzel."



And finally, so yes, in general, as I said, with the FIDO folks still working to come up with a universal Passkeys import/export format, which my god do we need that...



MIKAH:  Yeah, seriously.



STEVE:  ...it doesn't feel right to have them stuck in anyone's walled garden.  The eventual addition of Passkey transportability should make a huge difference.  Again, it'll allow us to see, to hold, to touch Passkeys.



MIKAH:  My precious Passkey.



STEVE:  I just think we need that; you know?  Like, where is it?



MIKAH:  That is honestly what's keeping me from using Passkeys as anything other than a second factor of authentication.  That's where I end up because there are a few sites like GitHub that give you the option, either use it as just a straight-up login or use it as the second factor of authentication.  I'm okay with doing that, knowing that I can only have it one place, but I haven't completely removed my password and username login yet because I want that transportability before I feel comfortable completely saying, okay, I'll shut off my username and password, if I'm even given that option.



STEVE:  Right.  I think that, you know, I've talked about like waiting for Bitwarden to add the support to mobile because then we get it everywhere.  But looking at the responses from our users, and my own, I don't think I want Passkeys in my password manager.  I still need Bitwarden, remember, a sponsor of the TWiT network.  I need it for all the sites where I still only can use passwords.  So it's not going away.  But I think that, you know, I mean, I'm 100% Apple mobile person for phone and pad.  So I don't mind having Apple holding all those.  But I just - I still want to be able to get my hands on them.



MIKAH:  Yeah.  I want to see it.  I want to touch it.  I want to print it out and frame it.  No.  But, yeah, I'm with you.



STEVE:  Yeah, don't write it on the chalkboard behind you when you're doing a video podcast.



MIKAH:  Now let's hear about Morris the Second.



STEVE:  Okay.  So since Ben Nassi, one of the researchers behind this, reached out to me a couple of weeks ago via Twitter - and added his voice, by the way, to those who are looking forward to having a non-Twitter means of doing so in the soon future - the work that he and his team have done has garnered a huge amount of attention.  It's been picked up by Wired, PCMag, Ars Technica, The Verge, and many more outlets.  And there are a bunch of videos on YouTube that are like jumping up and down worrying about this.



In thinking about how to characterize this, I'm reminded of our early observations of conversational AI.  We talked about how the creators of these early services had tried to erect barriers around certain AI responses and behaviors, but that clever hackers quickly discovered that it was possible to essentially seduce the AIs into ignoring their own rules.  By asking nicely, or by being more demanding, and like even actually getting mad, like sounding upset, the AI would capitulate.  So it was like, okay, okay, fine, here's what you wanted to know.



What Ben and his team have managed to do here can be thought of as the exploitation of that essential weakness on steroids.  Okay.  So to quickly create some foundation for understanding this, I want to run through the very brief Q&A that they've provided since it establishes some terms and sets the stage for their far more detailed 26-page academic paper, only pieces of which I'm going to share.



But they said:  "Question:  What is the objective of this study?  Answer:  This research is intended to serve as a whistleblower to the possibility of creating generalized AI worms in order to prevent their appearance."  In other words, hey everybody, hold on here, hold up, look what we did.  You'd better do something about that.



MIKAH:  I notice you did use the term "generalize."  Would that be Generative AI worms?



STEVE:  I'm sorry, yeah, generative.  Yes.  They're saying GenAI, and generative is exactly what they mean.  Thank you for catching that.  "Question:  What's a computer worm?"  They answer:  "A computer worm is malware with the ability to replicate itself and propagate or spread by compromising new machines while exploiting the sources of the machines to conduct malicious activity through a payload."  And they've done that.



"Why did you name the worm Morris the Second?  Answer:  Because like the famous 1988 Morris worm that was developed by a Cornell student, Morris II was also developed by two Cornell Tech students, Stav and Ben.  What is a GenAI ecosystem?  It is an interconnected network consisting of GenAI-powered agents.  What is GenAI-powered application/client/agent?  A GenAI-powered agent is any kind of application that interfaces with, one, GenAI services to process the inputs sent to the agent; and, two, other GenAI-powered agents in the ecosystem.  The agent uses the GenAI service to process an input it receives from other agents.



"Where is the GenAI service deployed?  The GenAI service that is used by the agent can be based on a local model, i.e., the GenAI model is installed on the physical device of the agent; or a remote model.  The GenAI model is installed on a cloud server, and the agent interfaces with it via an API.  Which type of GenAI-powered applications may be vulnerable to the worm?  Two classes of GenAI-powered applications might be at risk:  GenAI-powered applications whose execution flow is dependent upon the output of the GenAI service.  This class of applications is vulnerable to application-flow-steering GenAI worms.  And GenAI-powered applications that use RAG to enrich their GenAI queries.  This class of applications is vulnerable to RAG-based GenAI worms."  And I looked up the acronym of RAG, and now I've forgotten what it is, but it's in their paper.



They said:  "What is a zero-click malware?  Malware that does not require the user to click on anything - a hyperlink, a file, whatever - to trigger its malicious execution.  Why do you consider the worm a zero-click worm?  Due to the automatic inference performed by the GenAI service, which automatically triggers the worm, the user does not have to click on anything to trigger the malicious activity of the worm or to cause it to propagate.  Does the attacker need to compromise an application in advance?  No.  In the two demonstrations we showed, the applications were not compromised ahead of time.  They were compromised when they received the email.



"Did you disclose the paper with OpenAI and Google?  Yes, although this is not OpenAI's or Google's responsibility.  The worm exploits bad architecture design for the GenAI ecosystem and is not a vulnerability in the GenAI service.  Are there any similarities between adversarial self-replicating prompts and buffer overflow or SQL injection attacks?  Yes.  While a regular prompt is essentially code that triggers the GenAI model to output data, an adversarial self-replicating prompt is a code, a prompt, that triggers the GenAI model to output code, another prompt.  This idea resembles classic cyberattacks that exploited the idea of changing data into code to carry out an attack.  A SQL injection attack embeds code inside a query, which is its data.  A buffer overflow attack writes data into areas known to hold executable code.  An adversarial self-replicating prompt is code that is intended to cause the GenAI model to output another prompt as code instead of data."



Okay.  So one thing that should be clear to everyone is that a gold rush mentality has formed in the industry, with everyone rushing to stake out their claim over what appears to be a huge new world of online services that can be made available by leveraging this groundbreaking new capability.  But as always, when we rush ahead, mistakes are inevitably made; and some stumbles, perhaps even large ones, can occur.  The wakeup call this Morris the Second research provides has arrived, I think, at a vital time, and certainly not a moment too early.  Here's how these researchers explain what they've accomplished.



MIKAH:  I want to pause here for just a second.  I don't want to interrupt your flow, but I do because I know we have a lot of people who listen to this show who are not incredibly security versed because they find everything that happens in the show very interesting, and they learn things.  And so I'm having a little bit of an audience, you know, playing the audience role here.  In what you're about to read to us, is it heavy jargon?  Or before people start to kind of go, oh, I don't know what's about to happen, is it heavy jargon?  Or are we going to actually understand what everything you've just read about means and like what the outcome of what they've done means?  Or will we be provided a Steve Gibson explanation after the fact where I can go, oh, that's what they're doing here?



STEVE:  It's definitely an overview, so not getting too deep into the weeds.



MIKAH:  Okay, excellent.  Wonderful.  Because I want to know what all of this means, but I was just worried that I would not find out.



STEVE:  So they said:  "In the past year, numerous" - now, so this is the researchers, from their perspective.  "In the past year, numerous companies have incorporated Generative AI capabilities into new and existing applications, forming interconnected Generative AI ecosystems consisting of semi and fully autonomous agents powered by Generative AI services.  While ongoing research highlighted risks associated with the GenAI layer of agents, for example, dialog poisoning, membership inference, prompt leaking, jailbreaking, et cetera, a critical question emerges.  Can attackers develop malware to exploit Generative AI components of an agent and launch cyberattacks on the entire GenAI ecosystem?



"This paper introduces Morris the Second, the first worm designed to target Generative AI ecosystems through the use of adversarial self-replicating prompts.  The study demonstrates that attackers can insert such prompts into inputs that, when processed by Generative AI models, prompt the model to replicate the input as output, which yields replication" - that is, of the worm - "engaging in malicious activities, which is what the payload of malware does.



"Additionally, these inputs compel the agent to deliver them, so we get propagation, to new agents by exploiting the interconnectivity within the Generative AI ecosystem.  We demonstrate the application of Morris the Second against Generative AI-powered email assistants in two use cases (spamming and exfiltrating personal data), under two settings (black box and white box), using two types of input data (text and images).  The worm is tested against three different Generative AI models (Gemini Pro, ChatGPT 4, and LLaVA); and various factors (propagation rate, replication, malicious activity) influencing the performance, and the performance of the worm is evaluated."



Under their "Ethical Considerations" section of this, you know, they then go into like a deep 26-page paper.  But their ethical considerations I thought was interesting.  They wrote:  "The entire experiments conducted in this research were done in a lab environment.  The machines used as victims of the worm, the 'hosts,' were virtual machines that we ran on our laptops.  We did not demonstrate the application of the worm against existing applications to avoid unleashing a worm into the wild.  Instead, we showcased the worm against an application that we developed running on real data consisting of real emails received and sent by the authors of the paper and were given by the authors of their free will to demonstrate the worm using real data.  We also disclosed our findings to OpenAI and Google using their bug bounty systems."



Okay.  So unlike the first, Morris the First worm, which escaped from MIT's network at 8:30 p.m. on November 2nd, 1988, after having been created by Cornell University graduate student Robert Morris, today's Cornell University researchers were extremely careful not to "see what would happen."



MIKAH:  Let's just see what'll happen, yeah.



STEVE:  Let's see if it really works; shall we?  You know, they weren't going to do that.  They were not going to turn their creation loose upon any live Internet services.  One thing we've learned quite well during the intervening 36 years since Morris the First is exactly what would happen.  And it would be neither good, nor would it further the career of these researchers.  Their paper ends on a somewhat ominous note that feels correct to me.



They conclude by writing:  "While we hope this paper's findings will prevent the appearance of Generative AI worms in the wild, we believe that Generative AI worms will appear in the next few years" - if not sooner, I'm worried - "in real products and will trigger significant and undesired outcomes," as they phrased it.  "Unlike the famous paper on ransomware that was authored in 1996 and preceded its time by a few decades, until the Internet became widespread in 2000, and Bitcoin was developed in 2009, we expect to see the application of worms against Generative AI-powered ecosystems very soon, perhaps maybe even in the next two to three years."  And again, if not sooner.  "Because, one, the infrastructure (the Internet and Generative AI cloud servers) and knowledge (adversarial AI and jailbreaking techniques) needed to create and orchestrate Generative AI worms already exists.



"Two, GenAI ecosystems are under massive development by many companies in the industry that integrate GenAI capabilities into their cars, smartphones, and operating systems.  And, three, attacks always get better; they never get worse."  And we know at least one podcast these guys listen to because that's something we're often saying here.  And, they said:  "We hope that our forecast regarding the appearance of worms in Generative AI ecosystems will turn out to be wrong because the message delivered in this paper served as a wake-up call."



So in other words, they're hoping that by developing a working proof of concept, which is what they have, where they were able to send a crafted email to a local instance of a Generative AI, which suborned that AI, causing it to, for example, spam everybody in the person's contact lists with itself, thus sending itself out to all of their email contacts, which when received would immediately spawn a second tier of worms which would then send itself to all of those email contacts.  You can see that in like 10 minutes...



MIKAH:  Yeah, that's exponential.



STEVE:  ...this thing would have spread to every Gmail user on the planet.



MIKAH:  Yeah, wow.



STEVE:  So, yeah.  What these guys have done is crucial.  They have vividly shown - by demonstration that cannot be denied - just how very immature, unstable, and inherently dangerous today's first-generation open-ended interactive Generative AI models are.  These models are extremely subject to manipulation and abuse.  The question in my mind that remains outstanding is whether they can actually ever be made safe?  I'm not at all sure.  That's not necessarily a given.  Safer, certainly.  But safe enough to be stable while still delivering the benefits that competitive pressure is going to push for?  That remains to be seen and to be proven.  We still can't seem to get the bugs out of simple computers whose operation we fully understand.  How are we ever going to do so for systems whose behavior is emergent and whose complexity literally boggles the mind?  I'm glad it's not my problem.



MIKAH:  Oh, dear.  I just - I'm thinking about all of the companies and services and subscriptions and all these places that are integrating all of this AI technology across so many aspects of so many types of business right now.



STEVE:  Without a single thought to this.



MIKAH:  Without a single thought to how easy - and you just described it right there, a worm that goes in and then sends itself, and then it goes to them, then it goes - and now somebody could just...



STEVE:  It would explode.



MIKAH:  It would absolutely.



STEVE:  It's a chain-reaction explosion.



MIKAH:  And that's earlier, whatever they said, this isn't OpenAI or Google's responsibility.  I was a little confused about that.  But now I understand that what they're saying is, it's not their responsibility because it's bigger than that.



STEVE:  Right.



MIKAH:  It is a fundamental issue that is only partially their responsibility.  It is everyone's responsibility.  And as you point out, is there a way to fix this, to correct it?



STEVE:  Again, we're still having buffer overflows.  We're having, you know, re-use of variables that were released.  I mean, these are the simplest concepts in computing, and we can't get them right.  And now we're, you know, we're going to turn something loose in people's email that's going to read their email for them to summarize it.  But it turns out that the email it's reading could have been designed to be malicious so that when it reads it, it suddenly sends itself to all their contacts.  Holy crap.



MIKAH:  NVIDIA just showed an example of talking to an AI that helps provide information for whether you should take a medication.  And so I'm talking to this bot that's like - and I say, oh, I'm taking St. John's wort as a supplement.  Should I also take this depression medication at the same time?  For anyone who knows anything about that, no, you should not.  But imagine a world where there's this worm that goes through, and it's doing all this self-replication stuff that causes it to put out - I mean, oh, Steve, what are we going to do?



STEVE:  Yes, to just like malignly diagnose.



MIKAH:  Yes, exactly.  What do we do?  You said it's not your problem.  You're right, it's not our problem.  But somebody's [crosstalk].



STEVE:  Yeah, just, you know, just be careful.  Again, to me, it's when we learned that asking like in a seductive way or like sounding angry to get the AI to become apologetic and then give you what it had been instructed not to is like, oh, this does not sound good.  And, you know, these guys have demonstrated just how bad it is.  And the good news is they showed Google, I'm sure Google just lost their you know what.



MIKAH:  Yes.  You hope so.



STEVE:  And said, oh, let's rethink launching this tomorrow.



MIKAH:  Yeah, they need to institute the "No means no" protocol for this because, no, this is - that's scary.  I mean, honestly, this is easily the scariest thing that I've heard you mention, only because I'm thinking about how quickly - you even mentioned it earlier.  It's like a gold rush slash...



STEVE:  We know how irresponsible companies will be when they're rushing to be first...



MIKAH:  So much rushing.



STEVE:  ...to get something on the market.  It's like, oh, AI this, AI that.  You've got an AI coffee pot.  You've got an AI toothbrush.  It's like, oh, god.



MIKAH:  Yeah.  Oh, boy.  Folks, got a lot of meditation to do and a lot of thinking to do.  Which is what Steve brings you every week right here.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#967

DATE:		March 26, 2024

TITLE:		GoFetch

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-967.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  After I comment on U.S. Department of Justice's antitrust suit against Apple, we'll update on General Motor's violation of its car owners' privacy and answer some questions, including what happy news is Super Sushi Samurai celebrating?  Has Apple abandoned its plans for HomeKit-compatible routers?  And what appears to be shaping up to take their place?  Will our private networks be receiving their own domain names?  And if so, what?  The UN has spoken out about AI.  Does anyone care?  And what do I think the prospects are of us controlling AI?  What significant European country just blocked Telegram?  What did the just-finished 2024 Pwn2Own competition teach?  Might the U.S. be hacking back against China as they are against us?  And after a bit of interesting SpinRite news and a bit of feedback from our listeners, we're going to spent the rest of our time looking into last week's quite explosive headlines about the apparently horrific unfixable flaws in Apple's M-series silicon.  Just how bad is it?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Most importantly, that Apple exploit that everybody said was unpatchable, the end of the world, Steve says not so fast.  GoFetch our topic, next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 967, recorded Tuesday, March 26th, 2024:  GoFetch.



It's time for Security Now!, yay, the time I look forward to all week.  In this case, for the last three weeks.  Thank you to Mikah Sargent for filling in.  Steve Gibson, the man about town, is here to talk security.



STEVE GIBSON:  Mikah did a great job.	



LEO:  Of course he did.



STEVE:  He held down the fort and was engaging and...



LEO:  Good, I'm glad you like him because in about a year he's going to be in charge of the whole damn thing.  I notice you're doing the Leonard Nimoy salute.  Just want to tell you it's Live Long and Prosper Day, Leonard Nimoy's birthday.  Would be today, March 26th.



STEVE:  Yeah, and boy, he was born in '31, I think.  So, you know?  And last time we saw him he was looking it, too.  But, you know, he and the old Captain Kirk are still...



LEO:  Is Nimoy still alive?  I thought he'd passed.  He's still alive?



STEVE:  Oh, that's right, I remember, he did pass.  



LEO:  He passed, yeah.



STEVE:  Of course he did.  Well, I haven't yet, and he and I have the same birthday.



LEO:  Oh.  You weren't born in 1931, however.



STEVE:  No, no, no, no.  '55, baby.



LEO:  Happy birthday.  I didn't know that.  Happy birthday.



STEVE:  Yeah.



LEO:  Well, you are, so that means you're four months older than me.



STEVE:  Yes, I am.  Well, and a couple years; right?



LEO:  No, wait a minute.  I'm November '56.  You're March '55.  So yes.



STEVE:  Correct. 



LEO:  One year and a few months.  Okay.



STEVE:  Yeah.



LEO:  Okay.  So happy birthday.  You going to do anything special to celebrate?



STEVE:  Thank you.  We initially had some plans to go have a fancy dinner.  But I said to Lorrie yesterday, I said, you know, I just would rather have a nice steak at home.



LEO:  You get to do what you want.



STEVE:  So she's out picking up some beautiful - that's what she said, actually.



LEO:  Yes.



STEVE:  You'd make a great wife, Leo.



LEO:  Well, it should be, you know, you should be.  When I was a kid, my mom, there was a birthday dinner that was the same every year my mom would make for us.  And I looked forward to that.  It was wonderful.  So, yeah, happy birthday.



STEVE:  So we have a tremendous podcast today.  Of course it's titled GoFetch, which is the name that's been given by the - I would call it the discoverers, but it's sort of the re-discoverers because they first stumbled onto this two years ago.



LEO:  Oh, interesting.  Oh.



STEVE:  And brought it up.  And in fact my theory is it's the reason that the M3 chip has a switch, which M1 and M2 doesn't because...



LEO:  Interesting.



STEVE:  Because they kind of scared Apple, but then they weren't really able to make a strong case.  Well, boy, has that case been made now.  And in fact we're going to start off when we talk about this here in an hour or so about how wound up the tech press has gotten and, you know, mis-wound because, boy, did they get it wrong.  But we'll have some fun with that.  And again, this is going to be one of our listeners' favorite types of episodes because it's going to be a deep dive.  So get out your propeller cap beanies and wind them up because, by the time we're done, everyone is going to understand exactly what happened, why it happened, how it happened, what it means, and like you could go to a cocktail party and really put your friends to sleep.



LEO:  Well, I've been saying, because we've been talking about it, obviously, on TWiT and today on MacBreak Weekly, and I've been saying, you know, I'm sure Steve will cover this much more accurately and much more granularly so tune in to Security Now! today.  So I'm glad...



STEVE:  Everybody will know.



LEO:  I didn't coordinate with you, I just figured, oh, he's going to jump into this one.  So GoFetch.



STEVE:  I'm also going to jump in briefly, because I'm not a legal scholar or expert, just I have a couple things to say about the U.S. Department of Justice's antitrust suit against Apple.  There are some arguments that they'll make that are security related.  So it does impinge on us a little bit.



LEO:  Yes, it's true.



STEVE:  But I just sort of have a little sort of an overview of that and, you know, capitalism and monopolies and so forth.  We're going to update on General Motors.  I don't know if you heard about this, Leo, this astonishing violation of their car owners' privacy.



LEO:  Oh, boy.



STEVE:  Oh, boy.  It's unbelievable.  Also we're going to look at - we're going to answer the question, what happy news is Super Sushi Samurai celebrating today?



LEO:  Okay.  I don't even know what that is.  Okay, good.



STEVE:  We're also going to look at whether Apple has abandoned its plans - you were talking about this at the end of MacBreak, actually - for HomeKit-compatible routers, and what appears to be shaping up to take their place.  Will our private networks - oh, this is cool - be receiving their own domain names?  ICANN has been busy.  And if so, what is it?  The UN has spoken out about AI.  Does anyone care?  And what do I think the prospects are of us controlling AI?  What significant European country just blocked Telegram?



Also, what did the just-finished 2024 Pwn2Own competition teach us once again?  Might the U.S. be hacking back against China as they are against us?  I've long been bemoaning the fact that we never hear anything about the other direction.  Well, we've heard something.  And after a bit of interesting SpinRite update news and a bit of feedback from our listeners, as I said, we're going to spend the rest of our time looking into last week's quite explosive headlines about the apparently horrific unfixable toxic flaws in Apple's M-series silicon.  Just how bad is it?



LEO:  Okay.  Good.  And I've been saying don't worry, but we'll find out what the real expert has to say in just a little bit.  I look forward to that.



STEVE:  And of course we do have a fantastic Picture of the Week, courtesy of our marvelous listeners.



LEO:  A great life hack, I think, something everybody might want to adopt.



STEVE:  That's right.



LEO:  So Picture of the Week time.



STEVE:  So for all of my life, Leo, I have found coat hanger wire to be really convenient.



LEO:  So useful.



STEVE:  It is.  And, you know, you used to get the coat hangers back from the dry cleaners with your shirts on them, and they'd have a little bit of a paper wrapping on them, but you can take that off.  But that gauge of coat hanger is...



LEO:  Perfect.



STEVE:  I mean, you can bend it into all kinds of useful shapes.



LEO:  You can bend it.  Stick it down the drain to hook a ring that somebody lost, or, yeah.



STEVE:  Exactly.



LEO:  So useful.



STEVE:  Exactly.  You know, it's just super handy.  Well, now,  here we have an application that I would not recommend.



LEO:  Well, I think this is a great life hack.  Don't you think everybody should do this?



STEVE:  Uh....



LEO:  Let me show people what we're talking about here.



STEVE:  So somebody has a USB charging cable which is way too long.  And maybe they need it to be long.  But this is like a neatnik person.  And I think we're seeing sort of a theme here because they've coiled up this way-too-long, I mean, it's like - this looks like 15 feet of USB charging cable.  But, you know, you don't want it lying around on the floor; right?  So they've coiled it all up.  Now, okay.  Now what are you going to do?  You've got this coil of USB charging cable.  You need to hang it somewhere.  So, and it doesn't really - you can't really hang it on the charger because it'll fall off.  It needs to be more secure than that.  So this clever OCD person thought, hey, I've always found coat hanger wire to be really handy for making stuff.  So, and happened to have a pair of pliers around.  So basically fashioned this beautiful, I mean, by all measures this is a beautiful hook.



LEO:  He spent some time with his little pliers there, bending and curving.  It's gorgeous.



STEVE:  Yeah.  It is great.  And, boy, does it work as a hook.



LEO:  Yes.



STEVE:  To hang this cord on.



LEO:  Right around the prongs of that Apple 5-watt charger, just goes right around those beautifully.



STEVE:  Therein lies the problem.



LEO:  Uh-oh.  What?



STEVE:  I've never - I don't recall ever actually touching the leads of my ohmmeter to this wire.  It must be that it is coated with some sort of an insulating varnish of some sort.



LEO:  Is it?  Is it, though?  Is it?



STEVE:  Otherwise this would have already exploded because...



LEO:  Notice, though, there's a switch here.  I think that he has not switched it on yet.



STEVE:  Oh, boy.



LEO:  And I think he's going to get quite a surprise.



STEVE:  Put your shoes on and use the toe of your shoe to turn this plug on.



LEO:  Another point, because he looks like he really is, you know, OCD and careful, he has installed his plugs upside down, which...



STEVE:  It's true.



LEO:  Right?



STEVE:  You've got the, yeah, not smiley little face.



LEO:  Got the ground, they're not smiley.



STEVE:  They're not looking like a little happy face.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  What's going on there?



STEVE:  It's not good.



LEO:  Yeah.



STEVE:  So anyway, so just so people are thinking that we haven't completely lost our mind, the point is that this hook has two legs that go up behind this USB charger and then bend around, you know, in U shape, to hang over the two prongs of the AC plug.



LEO:  Yeah.  There you go.



STEVE:  Wire on wire.



LEO:  It's like they made it for this.



STEVE:  Oh, it's beautiful.  I mean, it is a beautiful construction.  But no.



LEO:  The minute you plug it in, what would happen?  Would it heat up?  Would it start to glow?  Or would it actually short the thing out?



STEVE:  No, no.  You would have an immediate - the good news is, all homes ever made, even when they had screw-them-in fuses in the fuse box in the basement, they had some cutout such that, if any circuit suddenly drew too much power, rather than it exploding in your face...



LEO:  Right.



STEVE:  ...down in the basement something would go boom, and then you'd - now, of course, what you don't want is to run out of fuses because, you know...



LEO:  Don't put a penny across it.  No no no no no no.



STEVE:  That's right.  Then what people did was like, oh, shoot, you know, I don't know why this fuse blew, but it's inconvenient.  So I seem to be fresh out of fuses here.



LEO:  Yeah, but here's some good news...



STEVE:  Stick a penny in the hole, we can stick a penny in the socket and screw the blown-out circuit, you know, the blown-out fuse on top.  Anyway, yes, folks, do not do this at home.  The only thing I can think, Leo, is that there must be some varnish on this.  But...



LEO:  Even then.



STEVE:  ...over time, as it's used, it's going to get moved back and forth, riding on the top of the prongs of this plug, and it's just going to explode at some point.



LEO:  Never, ever put metal around the prongs of your plug.  I learned that in eighth grade when a...



STEVE:  Leo, on that note, we do have the picture queued up for next week, and it's another goodie.



LEO:  Good.  Oh, no.



STEVE:  It's a variation.  It's not the same.  We don't want to get repetitious here.  But we're going to have just as much fun with it.



LEO:  Somebody told me that that is the commercially preferred way of installing a plug socket is upside down like that.  And then somebody else in the Discord says that's how you know it's a switched circuit.  I've never seen that before.  But just to - reason I'm saying that, to preclude all the email that you and I inevitably will get from licensed electricians who will say, absolutely, that's the way to do it correctly.



STEVE:  Thank you for not overflowing my inbox.



LEO:  I also hope that I am not the subject of the Picture of the Week next week because I installed yesterday, we had a little lighting problem, and I and my brother-in-law did a little electric work, electrical work installing a new under-counter lamp.  And you see that switch right there, that's just to the right of Joe there?  As he was installing the wires, he accidentally backed into it and switched it on and got a little bit of a shock.



STEVE:  Oh, yes.



LEO:  Ow.



STEVE:  I see you're still wearing the avocado shirt from Sunday.



LEO:  I am wearing - this was right after I got home Sunday.  They said, "Get in here."  And I am also wearing, as sharp eyes will see, the most useful device for a home handyman anywhere.



STEVE:  Yes.  You have a head-mounted lamp.



LEO:  A head lamp.



STEVE:  Yeah.



LEO:  Please, do not make that the Picture of the Week next week.  I'm just begging of you; okay?



STEVE:  Yes.  Sometimes when Lorrie...



LEO:  Oh, you've got one, too.  Look at you.



STEVE:  Well, I've got, no, these are the magnifiers.



LEO:  Oh, that's - and what do you wear that for, Steve, besides looking like an alien?



STEVE:  When I'm building things like this.



LEO:  Oh, yes.  You've got to get very close.



STEVE:  That's right.



LEO:  Are you soldering with those on?



STEVE:  Those are little surface-mount components.  They're little itty-bitty.  So, yeah.



LEO:  That's a lot of work.  Anyway.



STEVE:  Anyway.



LEO:  Take that off and let's continue.



STEVE:  Believe it or not, we have news to get to.



LEO:  Yes, okay.



STEVE:  Last Thursday March 21st was - it was by all measures a rough day for Apple.  Not only, as I mentioned, did the tech press explode with truly "hair on fire" headlines about critical, unfixable, unpatchable, deeply rooted cryptographic flaws rendering Apple's recent M-series ARM-based silicon incapable of performing secure cryptographic operations...



LEO:  Incapable.  Incapable.



STEVE:  Can't be done.  Which is the topic we'll be spending the end of today's podcast looking at in some detail, once we get this thing started, because actually it's super interesting.  But before that, also last Thursday, the U.S. Department of Justice was joined by 15 other states and the District of Columbia, which wishes it was a state, but isn't, in a lawsuit alleging that Apple has been willfully and deliberately violating Section 2 of the Sherman Antitrust Act.



Now, I'm just going to share five sentences from the DOJ's comments which were delivered last Thursday.  They read:  "As our complaint alleges, Apple has maintained monopoly power in the smartphone market not simply by staying ahead of the competition on the merits, but by violating federal antitrust law.  Consumers should not have to pay higher prices because companies break the law."  Okay.  "We allege that Apple has employed a strategy that relies on exclusionary, anticompetitive conduct that hurts both consumers and developers.  For consumers, that has meant fewer choices; higher prices and fees; lower quality smartphones, apps, and accessories; and less innovation from Apple and its competitors.  For developers, that has meant being forced to play by rules that insulate Apple from competition."



Okay.  Now, this is not, clearly, a podcast about antitrust law.  We all know I'm not an attorney, nor am I trained in the law.  So I have no specific legal opinion to render here.  However, I've been a successful small business founder/owner/operator throughout my entire life.  And I'm certainly a big fan and believer in the free enterprise system and in the principles of capitalism.  But I also appreciate that this system of competition is inherently unstable.  It has a natural tendency for the big to get bigger through acquisition and the application of economies of scale and leverage.  That same system that creates an environment which promotes fair competition can be abused once sufficient power has been acquired.



Those of us of a certain age have watched Apple being born, then fall, only to rise again from the ashes.  My own first commercial success was the design, development, production, and sales of a high-speed, high-resolution light pen for the Apple II which allowed its users to interact directly with the Apple II's screen.  To my mind, there is no question that as a society we are all richer for the influence that Apple's aggressive pursuit of perfection has had on the world.  Things as simple as product packaging will never be the same.



But for some time we've been hearing complaints about Apple's having taken this too far.  It's understandable for competitors to complain, and to ask the government to step in and do something.  At some point that becomes the government's very necessary role, just as we saw previously when the same thing happened with Microsoft, and some would argue ought to happen again with Microsoft.  For many years the U.S. government has done nothing, while Apple has continued to grow and continued to aggressively use its market power to increase its shareholders' wealth.  The question is, when does use of market power become abuse of market power?  The next few years will be spent in endless depositions and expert testimony working to decide exactly what sort of cage Apple needs to be constrained within.



One thing we know is that many of the arguments Apple will be making on its own behalf will involve security, the security inherent in its closed messaging system, the inherent security of its closed App Store.  You know, things we've touched on many times in this podcast.  Apple will allege that by keeping its systems closed, it is protecting its users from unseen nefarious forces.  But, for example, the presence of Signal and WhatsApp in the App Store and on Apple devices, which create freely interoperable super-secure cross-platform messaging, suggests that Apple's own messaging technology could work similarly if they wished it to.



During the news coverage of this since Thursday, I've encountered snippets of evidence which suggest that the government has obtained clear proof of Apple's true motives where Apple's technology has been designed to support Apple's interests rather than those of its users.  In any event, and, you know, maybe those are aligned.  That's really the question; right?  Are Apple's interests and its users' interests perfectly aligned?



Nothing is going to happen on this front for a long time.  Years will pass, and this podcast will be well into four digits by the time anything is resolved with the DOJ's antitrust lawsuit.  The way things have been going, it seems to me much more likely that the laws being written and enacted within the European Union today will be forcing Apple's hand long before the DOJ finishes making its case.  All that may eventually be required will be for the U.S. to force Apple to do the same thing that they're already doing over in Europe, here as well.  But as for whether Apple-designed silicon cannot perform secure cryptographic operations, that is something this podcast can speak to authoritatively, and we'll be doing so once we've caught up with some more interesting news and feedback.



LEO:  Good.  I always said back in the day, in fact it was during - it's funny how you began this with the good old days of Apple because back in the day when the Department of Justice was suing Microsoft, I always said, if Apple were as big and powerful as Microsoft, they'd be just as bad.  But they aren't.  In fact, they almost went out of business in '97.  And now that they are even a little bit bigger than Microsoft, yeah, they're just as - it's what happens.



STEVE:  It is.  It is exactly what happens.  And it's not that anybody is a bad person.  You know, I mean, they argue, the executives argue that it's their job to maximize shareholder wealth.



LEO:  Right.  That's capitalism.



STEVE:  Yes, exactly.  Exactly.  And so it's a fundamental property that there need to be constraints.  And of course in the U.S. we have those.  Boy, is it painful to get them.  But, you know, it's interesting.  And I think I heard Rene saying that he thought he was going to have to spin up another podcast in order to keep track of this.



LEO:  Oh, I'm sure.



STEVE:  I'm not going to bother with that.



LEO:  No, we're not going to do it, yeah.



STEVE:  No.



LEO:  No.



STEVE:  It'll go on and on and on, and we'll mention it once in a while, and that'll be it.



LEO:  Right.  This thing will go for years, exactly as it happened with Microsoft



STEVE:  So last week we shared the difficult, I mean, truly difficult to believe, but true story that General Motors had actually been sharing - and by sharing I'm pretty sure the proper term would be selling - the detailed driving record data of its car owners, down to how rapidly the owner's car accelerated, how hard it braked, and its average speed from Point A to Point B.  Leo, they literally have instrumentation in there that is monitoring everything the car does.  And these cars are all interconnected now.  It was all being beamed back to GM, who it turns out was selling it to LexisNexis, a major data broker.



Anyway, so what happened was - and this was a New York Times or Washington Post, I think it was The New York Times piece last week that just blew the lid off this.  Some guy, I think he was in Canada, or maybe he was just up north, he saw his insurance go up 21% in one year, although he'd never been in an accident and didn't have tickets.  And so when he asked his insurance company why, they sort of hemmed and hawed.  He also tried to obtain alternate insurance, and all the quotes that he got back from competing companies were the same.  Finally one of them said, well, you should check your LexisNexis report because it's a little worried about your driving habits.



LEO:  Oh, so now there's like a credit report, there's now a car driving report.



STEVE:  Yes.



LEO:  But you know what, in some ways, A, I'm not surprised.  Insurance companies have for years offered good driver discounts.  In the past you used to have an app and stuff.  I'm not surprised to hear this.  And honestly...



STEVE:  Optionally installed app.



LEO:  Right.



STEVE:  For, like, low-mileage drivers where it would monitor - I'm sure we talked about it on the podcast.



LEO:  Yeah, but this is good for you and me because insurers, instead of this guy who really is not a safe driver paying the same as you and me who drive like little old men, because we are, we should get reduced; right?  And he should pay more.  It's fair, I think.



STEVE:  And should it be done without consent.



LEO:  Well, in a way I bet you he did consent.  I bet you there is somewhere a document that he signed when he bought that car that said data's being collected.  You saw the Mozilla report last year.  We talked about it, about how cars are a privacy nightmare.



STEVE:  Yeah, well, we were all wondering recently how your sexual habits were being recorded by a car.  It's like, what?  Like, is it monitoring the suspension?



LEO:  Should this car be rockin', as you well know, don't you be knockin'.



STEVE:  Okay.  So the good news is this produced an outcry which caused GM to immediately terminate this conduct.  And no doubt threats of lawsuits were involved, too.  They said GM is immediately stopping the sharing of this data with these brokers.  The report said:  "After public outcry, General Motors has decided to stop sharing driving data from its connected cars with data brokers.  Last week, news broke that customers enrolled in GM's OnStar Smart Driver app have had their data shared with LexisNexis and Verisk.  Those data brokers in turn shared the information with insurance companies, resulting in some drivers finding it much harder or more expensive" - exactly as you said, Leo - "to obtain insurance.  To make matters much worse, customers allege they never signed up for OnStar Smart Driver in the first place, claiming the choice was made for them by salespeople during the car-buying process."



LEO:  Yeah.  And, you know what, it comes with the car.  And, you know, it's good, it's all for your safety.  That's why we put it in, so that if you get a wreck, you can press the OnStar button.



STEVE:  That's right.



LEO:  That's why they did it.



STEVE:  And people will come.



LEO:  Yes.



STEVE:  We're not Big Brother watching over you.



LEO:  No, of course not.



STEVE:  Okay.  So I saw this bit of happy cryptocurrency news that just made me smile.  It seems that last week the blockchain game - I didn't know there was a blockchain game.  But yes, someone has made a game out of blockchain, and it's called Super Sushi Samurai.  Super Sushi Samurai had $4.6 million worth of its tokens stolen.  However, it just reported that they had all been recovered.  So what happened?  They explained that the hack was actually the work of a security researcher who exploited a bug in their code to move the funds out of harm's way to prevent future theft.



LEO:  Yeah, that was all.  Yeah.



STEVE:  That's right.



LEO:  Just want to move them out of harm's way.



STEVE:  Super Sushi Samurai described the incident as a "white hat rescue" and has ended up hiring the white hat to be a technical advisor.  So that's what I call a G-rated happy ending.



LEO:  Okay.



STEVE:  Yeah.



LEO:  I believe it.  Why not?



STEVE:  And also you guys touched on this on MacBreak.  Apple Insider has some interesting coverage about Apple's apparently failed initiative to move their HomeKit technology up into home routers.  I was a fan of this, since it promised to provide router-enforced inter-device traffic isolation, and the only place that can really be accomplished is at the router.  Our listeners know that I've been advocating for the creation of isolated networks so that IoT devices would be kept separate from the household's PCs.  But what Apple proposed five years ago, back in 2019, would have additionally isolated each IoT device, like with that level of granularity, from all the others.  So here's what Apple Insider explained.



They said:  "Apple's HomeKit Secure Routers were announced in 2019, but were never really taken up by manufacturers.  And now some vendors are claiming Apple is no longer pursuing the technology."  And we'll get to why in a minute.  "HomeKit Secure Routers," they wrote, "were introduced by Craig..."



LEO:  Federighi.



STEVE:  Federighi.  I know his name.  The problem is, you know, I'm a big Star Trek person, and I want to say Ferengi.  Which, you know...



LEO:  Craig Ferengi over there at Apple headquarters.



STEVE:  Had to stop from saying it.  I was, it's not Ferengi.  Come on, Steve.



LEO:  That's pretty funny.  I would never have guessed that.  Wow.



STEVE:  So "...Craig Federighi at World Wide Developer Conference 2019, and in the same breath and at the same time they introduced HomeKit Secure Video.  The latter, that is, HomeKit Secure Video, took time to reach the market, but it was used, and manufacturers adopted it, even if others would not."



Okay.  Now, "During [this year's just happened] CES 2024, two router vendors separately told AppleInsider that Apple is no longer accepting new routers into its program.  If that claim is correct  and it probably is, since it came from the same rejected manufacturers  given the lack of HomeKit Secure Routers on the market" - that is, in five years not much happened - "it appears that Apple has abandoned the idea, even though Apple still has active support pages on the matter.  However, Apple Insider noted that it also has support pages on AirPort routers too, and those are," as they put it, "dead as a doornail."



LEO:  Those really are dead, yeah, yeah.  I was so excited that Apple would offer this security standard, that we could have some confidence in the security and, frankly, firmware updatability of our routers.  It's a little disappointing to me.



STEVE:  Yeah.  Anyway, it's not going to happen.  They backed out.  Apple Insider, to make a long story short, polled the various routers that Apple listed.  There is one Linksys Velop AX4200 and an AmpliFi Alien router are apparently the only two that are currently listed by Apple as being supported.  Eero has a notice saying that its Eero Pro 6E and 6+ do not support Apple HomeKit, and they have no plans to offer Apple HomeKit router functionality.



Anyway, so not everything that gets announced happens.  And asking router manufacturers to modify their firmware to incorporate the required HomeKit functionality, and it appears that it may have taken some significant customization, it was just never going to get off the ground.



And this is probably for the better since it appears that we have already - and oh, thank god, blessedly quickly - moved beyond disparate proprietary closed IoT ecosystems.  Which, you know, is where it looked like we were headed with Amazon Alexa and Apple's HomeKit and Google's Home and Samsung's SmartThings, all creating their, oh, let's do our own thing.  All the buzz appears to now be surrounding the interoperability technology known as "Matter."  This was formerly known as CHIP, which stood for Connected Home over IP.  It's now been rebranded as Matter, and everyone appears to be seeing the light.  Nobody wants to be left out.



All those guys I just mentioned - Amazon with Alexa, Apple with their with HomeKit, Google with Home, and Samsung's SmartThings - all have announced and are supporting Matter.  It's now at v1.2, open, open source, license free.  Anyone can create Matter-compatible devices.  If they follow the spec, they will interoperate.  And more than 550 companies have announced their commitment to Matter.  So this is done; right?  I mean, all of the biggies are going to be supporting Matter.  They really have no choice.  And at this point I wanted to make sure I brought it up because I wouldn't purchase something, you know, that random AC plug that I got for a shockingly, I don't know, it was $4 or something.  It's amazing.  How can this be an Internet-connected device?  Just like, what, the plastic and the prongs would cost $4.



LEO:  It does report back on your driving habits, however.



STEVE:  Oh, it's got a little eyeball in it that follows you around the room.



LEO:  Yeah.



STEVE:  It's kind of freaky.



LEO:  Does Matter have, though, I mean, the thing about Apple's HomeKit router standard was it had security requirements built in.  Does Matter have something like that?



STEVE:  That's what they were, you're right, that's what they were going to produce.



LEO:  Yeah.



STEVE:  I think Matter is about interconnectivity.



LEO:  Right.  Right. 



STEVE:  Which is not to say it couldn't be made more secure, but that's not their focus.



LEO:  Right.



STEVE:  Thank you, my friend.  Okay.  In a cool bit of news, ICANN, the Internet Corporation for Assigned Names and Numbers, is going to make an assignment.  It's in the process of designating and reserving, get this, a top-level domain specifically for use on private internal networks.  In other words, our 10-dot and 192.168-dot networks, and there's a 17.16 thing in there, too, will be obtaining an official TLD of their own.  So localhost may soon be less lonely.  Here's the Executive Summary which explains and lays out the rationale behind ICANN's plans.



They wrote:  "In this document, the SSAC" - that's the Security and Stability Advisory Committee because, you know, that's what you want in your Internet is some security and stability advising.  They recommend "the reservation of a DNS label that does not and cannot correspond to any current or future delegation from the root zone of the global DNS," which is the very long-winded way of saying we're going to get our own dot-something TLD.



They said:  "This label can then serve as the top-level domain name of a privately resolvable namespace that will not collide with the resolution of names delegated from the root zone."  That is, you know, the public DNS root zone.  "In order for this to work properly, this reserved private-use TLD must never be delegated in the global DNS root.  Currently, many enterprises and device vendors make ad hoc use of TLDs that are not present in the root zone when they intend the name for private use only.  This usage is uncoordinated and can cause harm to Internet users."  Oh, my.



"The DNS has no explicit provision for internally-scoped names, and current advice is for the vendors or service providers to use a sub-domain of a public domain name for internal, or private use.  Using sub-domains of registered public domain names is still the best practice to name internal resources.  The SSAC concurs with this best practice and encourages enterprises, device vendors, and others who require internally-scoped names to use sub-domains of registered public domain names wherever possible.  However, this is not always feasible, and there are legitimate use cases for private-use TLDs."



And I'll just note that, you know, for example, an individual could register a domain with Hover, who I don't know if they're still a sponsor of the TWiT Network.  They are still my domain name provider.  I moved everything away from Network Solutions...



LEO:  I agree.



STEVE:  ...once it became clear...



LEO:  I don't think they're a sponsor anymore, but we still love them.



STEVE:  Yup.  They're the right guys.  Anyway, so, you know, Johnny Appleseed, you could get that.  Of course you can't get dot Johnny Appleseed, so that wouldn't work.  But you could get, you know, a dotcom or some inexpensive subdomain of some established top-level domain and just use that for your own purpose.  Because you have that subdomain, nobody else is going to be able to use it publicly.  So you're safe.  So that's what these guys were saying.



So they continue:  "The need for private-use identifiers is not unique for domain names, and a useful analogy can be drawn between the uses of private IP address space and those of a private-use TLD.  Network operators use private IP address space to number resources not intended to be externally accessible, and private-use TLDs are used by network operators in a similar fashion.  This document proposes reserving a string in a manner similar to the current use of private IP address space.  A similar rationale can be used to reserve more strings in case the need arises."  Okay.  So they go on and on.



Anyway, finally, after all the bureaucrat boilerplate has settled down, ICANN wrote:  "The Internet Assigned Numbers Authority (IANA) has made a provisional determination that '.internal' should be reserved for private-use and internal network applications.  Prior to review and approval of this reservation by the ICANN Board, we are seeking feedback on whether the selection complies with the specified procedure from SAC113" - more bureaucracy - "and any other observations that this string would be - to verify that it would be an appropriate selection for this purpose."  So it's all but certain that .internal will be reserved and will never be used for any  public purpose, and therefore it would be safe for anyone to start using it for any internal purpose.



LEO:  That's nice to have, thank you.



STEVE:  Very cool, .internal.  I saw some commentary saying, well, it only took 30 years.



LEO:  That's true, yeah.



STEVE:  Yeah, that is true.



LEO:  Took them a little while.



STEVE:  Okay.  So last Thursday, as I said earlier, was a very busy day.  Not only did the DOJ announce their pursuit of Apple, and Apple's M-series silicon was discovered to be useless for crypto, but the United Nations General Assembly adopted a resolution on artificial intelligence, not that anyone cares or that anyone can do anything about AI in any event.  But for the record, UN officials formally called on tech companies to develop safe and reliable AI systems that comply with international human rights.  And I loved this.  They said:  "Systems that don't comply should be taken offline."  So, you know, you have a mean AI, just unplug it, folks.  Officials said the same rights that apply offline should also be protected online, including against AI systems.



I've never said much about AI here.  Just as I'm not trained as an attorney, I do not have any expertise in AI systems.  What I do have, however, is stunned amazement.  As they would say over in the UK, I am gobsmacked by what I've seen.



LEO:  It is impressive; isn't it.  What do you...



STEVE:  Oh, my god.



LEO:  You know, I haven't ever asked you, and we talk about it all the time on the other shows, but what do you think the future holds?



STEVE:  Well, here I come.  So what I may lack in expertise appears to have been made up for by my intuition, which has been screaming at me ever since I spent some time chatting with ChatGPT 4.  My take on the whole AI mess and controversy can be summed up in just four words, and they are:  "Good luck restraining anything."



LEO:  Yeah, that's my attitude.  Exactly.



STEVE:  Yes.  I doubt that any part of this is restrainable.  At some point in the recent past we crossed over a tipping point, and we're seeing something that no one would have believed possible even five years ago.  Everyone knows there's no going back.  Only people who have not been paying attention imagine that there's any hope of controlling what happens going forward.  I don't know, and I can't predict what the future holds.  But whatever is going to happen is going to happen, and I'm pretty sure that it's bigger than us.



LEO:  Yes.



STEVE:  We're not a sufficiently organized species to be able to control or contain this.



LEO:  Mm-hmm.  Look how well we've done with nuclear proliferation.



STEVE:  Yeah.



LEO:  And that's - it's still incredibly hard to purify enough plutonium to make a bomb.  It's trivially easy, and the process is well, well known, to make an LLM.



STEVE:  It's out.



LEO:  It's out.  It's done.



STEVE:  You know, it would be like government saying, whoops, stop exporting crypto.  Like, what?



LEO:  Yeah, exactly.



STEVE:  You know.  And so Leo, you and I are on the same page.  I mean, it is - and if we don't, like, do it, we know North Korea's not sitting around doing nothing.  They apparently have quite smart people.



LEO:  Yes.



STEVE:  It annoys me that they're so good at hacking.  But, boy, they are serious hackers.  And so, you know...



LEO:  It's going to happen.



STEVE:  It is.  I would argue it already has, and we just haven't - it hasn't dawned on us yet.



LEO:  Yeah.



STEVE:  Right?  Like there's some inertia of recognition.



LEO:  I for one, I'm excited.  I mean, this is sci-fi.  We are going to live in, I think I might even live to see it, a very weird and different future.  It's coming.



STEVE:  Yeah.



LEO:  It's going to be fun.  Buckle up.



STEVE:  That's exactly right.  I think that's exactly right.  Okay.  So a few more points to get to.  In a somewhat disturbing turn, Spain has joined the likes of China, Thailand, Pakistan, Iran, and Cuba to be blocking all use of and access to Telegram across its territory.  This came after Spain's four largest media companies successfully complained to the high court in Spain that Telegram was being used to propagate their copyrighted content without permission.



A judge with Spain's high court had asked Telegram to provide certain information relating to the case, which apparently Telegram just blew off and ignored.  They chose not to respond to the judge's request.  So he ordered all telecommunications carriers to block all access to Telegram throughout the country.  That began yesterday.  So, you know, it's a problem.



LEO:  I'd be very interested to see how this holds up because about - I heard that about a third of Spain uses Telegram.



STEVE:  Yes.  It has already created a huge...



LEO:  Yeah.



STEVE:  Yes.  There's a huge consumer backlash against this, as one would expect.



LEO:  Yeah, remember Brazil tried to do this.  And they ended up having to back down.  I think it was for WhatsApp.  But they ended up having to back down because you can't - we can't communicate.  What are you doing?



STEVE:  Well, have you seen the movie "Brazil," Leo?  That explains the whole problem.



LEO:  You know it was a great movie.



STEVE:  That's right, yes.  Wonderful movie.  So last week Vancouver held its 2024 Pwn2Own hacking competition.  One security researcher by the name of Manfred Paul distinguished himself by successfully exploiting, get this, all four of the major web browser platforms.



LEO:  Wow.



STEVE:  He found exploits in Chrome, Edge, Firefox, and Safari.  He became this year's "Master of Pwn" and took home $202,500 in prize money.  Overall, and here's really the lesson, the competing security researchers-turned-hackers successfully demonstrated 29 previously unknown zero-days during the contest and took home a total of $1.1 million in prize money.



LEO:  That money comes from the companies that they're pwning, pretty much; right?



STEVE:  Yes,  yes.  Twenty-nine, okay, 29 previously unknown zero-days were found and demonstrated.  To me this serves to demonstrate why I continue to believe that the best working model that's been presented for security - and okay, yes, I'm the one who presented it - is "porosity."  Porosity.



LEO:  Porosity.



STEVE:  You know, we don't want it to be, but security is porous.  How else can we explain that one lone research hacker is able to take down all four of the industry's fully patched browsers whenever someone offers him some cash to do so?  And that overall, 29 new previously unknown zero-days were revealed, when others were similarly offered some cash prize incentive.  You know, you push hard, and you can get in.  That's the definition of porous.  And that's the security we have.



LEO:  Yeah.



STEVE:  I should also take a moment to give a shout-out to Mozilla's Firefox team, who had patched and updated Firefox in fewer than 24 hours following the vulnerability disclosure.  Frederik Braun posted on Mastodon:  "Last night, about 21 hours ago, Manfred Paul demonstrated a security exploit targeting Firefox 124 at Pwn2Own.  In response, we have just published Firefox 124.0.1 (and Firefox ESR 115.9.1) containing the security fix."  He says:  "Please update your foxes.  Kudos to all the countless people postponing their sleep and working toward resolving this so quickly.  Really impressive teamwork again.  Also, kudos to Manfred for pwning Firefox again."



So, you know, this is the way security is supposed to work at the best of times.  White hat hackers are given some reason to look, and compensated for their discoveries, which makes the products safer for everyone.  And then the publishers of those  products promptly respond to provide all of that product's users the benefits of that discovery.  Yay. 



And in this welcome bit of news, perhaps we and others are giving as good as we get.  I've often noted that all we ever hear about, you know, about attacks on our infrastructure are Chinese state-sponsored attacks that are successfully getting in.  You know, and I note that naturally we never hear about our similar successes against China.  It's not like the NSA is going to brag.  So I've wanted to believe that, while we would not destructive if we were to get in, that we'd only seek to have a presence inside Chinese networks so that they understand that we're just not sitting here defenseless over on this side of the Pacific.



Well, it turns out that last week China's state security agency themselves urged their local companies to improve their cybersecurity defenses.  The Ministry of State Security said that foreign spy agencies have infiltrated hundreds of local businesses and government units.  So that does sound like we may be at parity in this weird cyber cold war that we're in.  I hate it; but, you know, it's what we've got.



Oh, and just a reminder.  There has been an observed significant increase in tax season-related phishing.  So I just wanted to remind everyone that, as happens at every time this year, you know, phishing scams suddenly jumped with all kinds of like, oh, you just - we received your electronically submitted return, but it had a problem.  Please click here.  But that's not from the IRS.  So everybody put up your skepticism shields and resist clicking.



I have two quick notes of news that I think everyone'll find interesting on the SpinRite front.  One of the things that quickly became apparent as our listeners were wishing to obtain and use 6.1 was that the world had changed in another way since SpinRite 6's release back in 2004.  Back then, Linux was still largely a curiosity, you know, with a relatively small fan base and no real adoption.  Not so today, at least not among our listeners.



Back in 2004 it was acceptable to require a SpinRite user, I mean, just assumed that a SpinRite user would have Windows, which they would use to set up the boot media since Windows and Mac was pretty much all there was, and SpinRite was never really targeted at the Mac market.  Today, we've encountered many would-be users who do not have ready access to a Windows machine.  And they've been having a problem.  So I needed to create a non-Windows setup facility that I have long envisioned but never needed until now.  Today it exists.



Over at GRC's prerelease.htm page is, as before, the downloadable Windows/DOS hybrid executable, and now also a downloadable zip file.  The zip file, which is smaller than 400K, contains the image of the front of a 4GB FAT32 DOS partition.  So any SpinRite owner without access to Windows, because using Windows is still easier, may choose to instead download this zip file.  And it's personalized.  I've added on-the-fly partition creation, and SpinRite is added to the file system.  It's then truncated, and I've got on-the-fly zipping.  I've been busy.  It contains about an - the zip file, which is as I said less than 400K, contains an 8.3MB file which is named SR61.img.  Any Linux user can, you know, "dd" copy that file onto any USB thumb drive to create an up to 4GB FAT32 partition that will immediately boot and run SpinRite.



But the tricky bit that I worked out last week is that when this drive is booted for the first time, if the media onto which this image file was copied is smaller than the partition described by the image, which is a 4GB partition - for example, SpinRite's owner copies the image to an old but trusted 256MB thumb drive - a little built-in utility named "downsize" kicks in, examines the size of the partition's underlying physical drive, and dynamically on the fly "downsizes" the partition to fit onto its host drive.  It's all transparent and automatic.  And since this same technology was also going to be needed for SpinRite 7, it made sense to get it done, so it's there now.



Second point.  A new wrinkle to surface last week is bad RAM.  Over in GRC's web forums, a SpinRite 6.1 user reported data verification errors being produced by SpinRite when running on his cute little ZimaBoard.  SpinRite always identified and logged the location of the apparent problem.  But from one run to the next there was no correlation in where the problems appeared to be occurring.  And when he ran the same drive under SpinRite on a different PC, it passed SpinRite's most thorough Level 5 testing without a single complaint.  And he was able to go back and forth to easily recreate the trouble multiple times on one system, but never on the other.



The inhabitants of the forums jumped on this and suggested a bad or undersized power supply for his ZimaBoard, flaky cabling, and anything else they could think of, all great suggestions.  Finally, I asked this user to try running the venerable MemTest86 on his brand new ZimaBoard.  And guess what?  Yep.  Memory errors.  There should never be any.  But the first time he ran MemTest86 it found six, and the second time it found 101.



Seeing that, we ran MemTest86 on all of our ZimaBoards, that is, all of the developers, and they all passed with zero errors, as they always should.  So this user had a ZimaBoard with a marginal DRAM memory subsystem.  There was no correlation in the locations of the errors that SpinRite was reporting from one memory - oh, that his MemTest was reporting from one pass to the next.  But there were always two specific bits out of the 32 that MemTest86 always identified as being the culprits.  They were soft.  And SpinRite was getting tripped up by this machine's bad RAM when it was performing data verification that's available from SpinRite's levels 4 and 5.



The problem was not the drive.  It was the machine hosting SpinRite and the drive.  So by this point our longtime listeners who've grown to know me, listening to this podcast, know what I'm going to say next.  Yep, SpinRite 6.1 now tests the memory of any machine it's running on.



LEO:  Oh, clever.  Wow.  Who needs MemTest?  I've got SpinRite.



STEVE:  That's right.  It works great.  It, like, immediately found the errors this guy was having.  What's interesting is that SpinRite 1.0, back in 1988, also built-in a memory test.  Back then it made sense to verify the RAM memory that would be used to temporarily hold a track's data while SpinRite was pattern testing the physical surface and giving it a fresh new low-level format.  But I don't know when it happened, somewhere along the way I removed that feature from SpinRite.  We never heard of it ever being useful, so my initially over-cautious approach seemed to have been proven unnecessary - until last week.



So late last week I implemented a very nice little DRAM memory tester right into SpinRite and then had the guy with the bad ZimaBoard give it a try.  It successfully determined that his machine's memory was not reliable, and SpinRite will then refuse to run on any such machine after making that determination.  You know, it's just not safe to run it.  And of course no such machine should be trusted for actually doing anything else.  You know, it's like send it back to the manufacturer; or, if you can, change the RAM, or diagnose it.



So anyway, this new built-in RAM testing feature - which is not yet present.  Don't go download an updated copy of SpinRite.  It's not there yet, not yet present in any SpinRite that's available for download.  But it'll appear, along with a few other minor improvements that I've made, shortly.  So I'm sure I'll be announcing it next week.



And I just have two little pieces of feedback from our listeners because we have lots to still talk about here.  I got a note from someone whose handle is Jazman.  He said:  "Hi, Steve.  Great show, as always.  I work in a cell phone-free environment.  Not only no service, but we're not allowed to bring them.  We have Internet computers, but we're not trusted to install anything on them."  Sounds like he is with the NSA.  "The problem is I like to have two-factor authentication to protect my email and other stuff.  My understanding is, if I were to use Passkeys, I need my phone.  I use Bitwarden with two-factor authentication.  My question:  Are there any good solutions for a cell-free environment?  Kind regards, Bjorn."



Okay.  So, and we've been talking about this for the last couple weeks, whether to have two-factor and now optionally Passkeys managed by your password manager, or to keep it separate.  In a phone-free environment, I agree that relying upon Bitwarden for all authentication services is likely the best bet.  I think it's probably your only bet; right?  You know, we would usually prefer to have Passkeys or our authenticator on a separate device like a phone.  But where that's not possible, merging those functions into a single password manager like Bitwarden makes sense.  And I should just note that YubiKeys are also Passkeys-capable, and they're able to store up to 25 Passkeys in a YubiKey.  So a YubiKey is another possibility, if that somewhat limited Passkeys capacity doesn't pose a problem.



And finally, William Ruckman.  He said:  "Hi, Steve.  Are Passkeys quantum safe?  I thought public key crypto was vulnerable."  And we'd also been speaking this recently about how the big difference between username and password and Passkeys is the essentially symmetric crypto secret keeping, whereas Passkeys uses public key crypto, which is why William's asking.  So it's a terrific question because, as we know, it's the public key crypto that Passkeys offers, which is why it's so valuable.  The good news is the FIDO2 specification which underlies WebAuthn, which underlies Passkeys, already provides for plug-in future-proof crypto.  So Passkeys and WebAuthn/FIDO2 will all be able to move to quantum-safe algorithms whenever that's appropriate, and as soon as we've settled on them, and they've been standardized.  So yes.



LEO:  Good.  That's good news.  And it would be backward to all the Passkeys you already are using and all that.  Right?  Maybe not.



STEVE:  No.  Could not be.



LEO:  You'd have to regenerate them all.  Okay.



STEVE:  Yes.  If you change the crypto, you would have to regenerate the Passkeys because you're holding private keys with a specific algorithm.



LEO:  Right.



STEVE:  And there is actually no way for the website even to help you.  I mean, it might say, you might go through a "use your old Passkey, now use your new Passkey."



LEO:  Yeah.



STEVE:  And if you did that, you know, sequentially, then it would get - actually SQRL had a similar facility.  So it would use the first authentication to assert your identity and thus honor the second authentication, which would be from the newfangled crypto.  And now it would have the public key under the new algorithm.



LEO:  Cool.  We don't have to worry about it yet.  There's only about four sites that use this.



STEVE:  I know.  I saw, in doing some research just yesterday, I saw someone who had something to sell, they were trying to sell some equivalent of a YubiKey, I think.  And it said, "Since the  majority of the Internet's websites are now using Passkeys."  And I thought, are you in a time machine?  What are you talking about?



LEO:  Oh, you're talking 2030.  Oh, yeah.  Maybe.  Maybe.



STEVE:  Yeah, yeah.



LEO:  There's literally...



STEVE:  The majority as long as you only log into PayPal.



LEO:  Yeah, right.  There's literally just a handful of sites that use it.



STEVE:  I know.  I know.



LEO:  It's too bad because it's so easy when it works.  I heard you talked a lot about that last week with Mikah.  And I agree with you, I think it's going to be a big improvement someday.  Someday our prince will come.



STEVE:  Okay.  After our final announcement, Leo, oh, boy, we're going to have some fun.



LEO:  Oh, boy.



STEVE:  Get the beanies lubed.



LEO:  I don't know if you should say that.  Shouldn't say that out loud, anyway.  And now, let's talk about Fetch.  GoFetch.



STEVE:  So, GoFetch.  Last Thursday the world learned that Apple had some problems with their cryptography.  Unfortunately, it would be impossible to determine from most of the tech press's coverage of this whether this was an apocalyptic event or just another bump in the road.  Ars Technica was apparently unable to resist becoming click-bait central with their headline "Unpatchable vulnerability in Apple chip leaks secret encryption keys."  Wow.  That would be bad if it was true.



Fortunately, it's not the least bit true.  It's not unpatchable, and it's not a vulnerability in an Apple chip.  Kim Zetter's Zero Day goes with:  "Apple Chip Flaw Lets Hackers Steal Encryption Keys."  This "chip flaw" [in air quotes] theme seems to have become pretty popular, even though nowhere did any of the actual researchers ever say anything about any chip flaw.  Even Apple Insider's headline read "Apple Silicon vulnerability leaks encryption keys and can't be patched easily."  What?



Apple was told 107 days before the disclosure, back on December 5th of last year.  Apple is certainly quite aware of the issue, and I'm sure they're taking it seriously.  And for their newer M3 chips, all that's needed is for a single bit to be flipped.  Tom's Hardware went with:  "New chip flaw hits Apple Silicon and steals cryptographic keys from system cache.  'GoFetch' vulnerability attacks Apple M1, M2, M3 processors, can't be fixed in hardware."  Oh dear.



Except for a few details:  It's not new.  It's not a flaw.  Nothing ever "hit" Apple Silicon.  And as for it not being fixable in Apple M1, M2 or M3 processors, if you have an M3 chip, just flip the bit "on" during crypto operations and the unfixable problem is solved.  And finally, as we'll see by the end of this topic today, there are equally simple workarounds for the earlier M-series processors.



Okay.  So I could keep going because the material in this instance was endless.  Not a single one of the headlines of the supposedly tech press stories that covered this characterized this even close to accurately.  It's not a flaw.  Nothing is flawed.  Everything is working just as it's supposed to.  It's not a vulnerability in Apple Silicon.  Apple Silicon is just fine, and nothing needs to change.  And it's certainly not unfixable or unpatchable.  CyberNews headline was "M-series Macs can leak secrets due to inherent vulnerability."  The only thing that's inherently vulnerable here is the credibility of the tech press's coverage of this.



LEO:  Holy cow.



STEVE:  It really has been quite over the top.  After sitting back and thinking about it, the only explanation I can come up with is that, because what's actually going on with this wonderfully and subtly complex problem, no one writing for the press really understood what the researchers have very carefully and reasonably explained.  So they just went with variations on Ars Technica's initial "unpatchable vulnerability in Apple's chip" nonsense, under the assumption that Ars must have actually understood what was going on, so everyone just copied them.



LEO:  I just assumed Dan Gooden knows.  If he doesn't know - right, yeah.



STEVE:  You know, Dan's on the ball, typically.  And we do know, in fairness to Dan, he doesn't provide the headlines.  Back when I was writing the Tech Talk column for InfoWorld, I was often really annoyed by what my columns were headlined because that's not what I said in the text.  But, you know, some copy editor, I guess that's what they're called, gave it the headline that would get people to turn to the page.  So, okay.  Not Dan's fault.



Okay.  The TL;DR of this whole fiasco is that a handful of researchers built upon an earlier two-year-old discovery, which three of them had been participants in back then, that was dismissed at the time by Apple, you know, as being of only academic interest.  It's yet another form of side-channel attack on otherwise very carefully designed to be side-channel attack-free constant-time cryptographic algorithms.  The attack surrounds an ARM-based performance optimization feature known as DMP.  And I was thinking, boy, if the acronym had been EMP, that would have really blown the tech press right off the top.  Anyway, not EMP, DMP.  And a variation of the same type of optimization is also present in the newest Intel chips, their Raptor something or other.  Anyway, I'll get to that.



Okay.  And so, true to Bruce Schneier's observation that attacks never get worse, they only ever get better, about a year and a half after that initial discovery two years ago which never amounted to much, it turned out that the presence of this DMP, which I will be explaining in detail, optimization feature actually did and does create an exploitable vulnerability that can be very cleverly leveraged to reveal a system's otherwise well-protected cryptographic secrets.  After verifying that this was true, the researchers did the responsible thing by informing Apple, and we have to assume Apple decided what they wanted to do next.  Okay.  Unfortunately, that true story doesn't make for nearly as exciting a headline.  So none of the hyperventilating press explained it this way.



One important thing that sets this apart from the similar and related Spectre and Meltdown vulnerabilities from yesteryear is that this new exploitation of the DMP optimizer is not purely theoretical.  All we had back in those early days of speculative execution vulnerabilities was a profound fear over what could be done, over what this meant.  It was clear that Intel had never intended for their chip's internal operation to be probed in that fashion, and not much imagination was required to envision how this might be abused.  But we lacked any concrete real-world proof of concept.  Not so today.  And not even post-quantum crypto is safe from this attack since we're not attacking the strength of the crypto, but rather the underlying keys are being revealed.



The GoFetch proof-of-concept app running on an Apple Mac connects to the targeted app, also on the same machine, which contains the secrets.  It feeds the app a series of inputs that the app signs or decrypts or does something using its secret keys, basically inducing it to perform cryptographic operations that require it to use the secrets it's intending to keep.  As it's doing this, the app monitors aspects of the processor's caches which it shares with the targeted app in order to obtain hints about the app's secret key.  Okay.  So how bad is it?



As I mentioned, the attack works against both pre- and post-quantum encryption.  The demo GoFetch app requires less than an hour to extract a 2048-bit RSA key, and a little over two hours to extract a 2048-bit Diffie-Hellman key.  The attack takes 54 minutes to extract the material required to later assemble a Kyber 512-bit key and about 10 hours for a Dilithium 2 key, though some time is also required afterwards for offline processing of the raw data that is collected.  In other words, it is an attack that is practical to employ in the real world.



Okay.  So what exactly is DMP, what did the researchers discover, and how did they arrange to make their Macs give up the closely held secrets being hidden inside?  The research paper is titled "GoFetch:  Breaking Constant-Time Cryptographic Implementations Using Data Memory-Dependent Prefetchers."  Okay, now, that sounds more complex than it is.  We have "Breaking Constant-Time Cryptographic Implementations."  We already know that a classic side-channel vulnerability which is often present in poorly written crypto implementations is for an algorithm to in any way change its behavior depending upon the secret key it's using.  If that happens, the key-dependent behavior change can be used to infer some properties of the key.  So the first portion of the title tells us that this attack is effective against properly written constant-time cryptographic implementations that do not change their behavior in any way.  That's not where things got screwed up.



The second part of the paper's title is "Using Data Memory-Dependent Prefetchers," and that's what's new here.  If you guessed that the performance optimization technique known as DMP stands for "Data Memory-Dependent Prefetchers," you'd be correct.  Three of the seven co-authors of today's paper co-authored the earlier groundbreaking research two years ago which described their reverse-engineered discovery of this DMP facility residing inside Apple's M-series ARM-derived chips.  Back then they raised and waved a flag around, noting that what this thing was doing seemed worrisome; but they stopped short of coming up with any way to actually extract information.  And the information that they had was made public.



Now, we don't know for sure that sophisticated intelligence agencies somewhere might not have picked up on this and turned it into a working exploit, as has now happened.  But we do know for sure that Apple apparently didn't give this much thought or concern two years ago, since every one of their Mac M-series chips was vulnerable to exploitation several years later.



Okay.  I'm going to share today's research abstract, today's, the updated current research abstract and introduction, since it's packed with information and some valuable perspective.  And then I'll break it down.  So they wrote:  "Microarchitectural side-channel attacks have shaken the foundations of modern processor design.  The cornerstone defense against these attacks has been to ensure that security-critical programs do not use secret-dependent data addresses.  Put simply, do not pass secrets as addresses, for example, data memory instructions.  Yet the discovery of data memory-dependent prefetchers (DMPs), which turn program data into addresses directly from within the memory system, calls into question whether this approach will continue to remain secure.



"This paper shows that the security threat from DMPs is significantly worse than was previously thought and demonstrates the first end-to-end attacks on security-critical software using the Apple M-series DMPs.  Undergirding our attacks is a new understanding of how DMPs behave which shows, among other things, that the Apple DMP will activate on behalf of any victim program and attempt to 'leak' any cached data that resembles a pointer.  From this understanding, we design a new type of chosen-input attack that uses the DMP to perform end-to-end key extraction on popular constant-time implementations of classical and post-quantum cryptography."



And by way of introduction, they said:  "For over a decade, modern processors have faced a myriad of microarchitectural side-channel attacks, for example, through caches, TLBs" - Translation Lookaside Buffers - "branch predictors, on-chip interconnects, memory management units, speculative execution, voltage/frequency scaling, and more."  You know, as we know, even like the sound of the power supply changing can leak information.



They said:  "The most prominent class of these attacks occurs when the program's memory access pattern becomes dependent on secret data.  For example, cache and TLB side-channel attacks arise when the program's data memory access pattern becomes secret dependent.  Other attacks, for example, those monitoring on-chip interconnects, can be viewed similarly with respect to the program's instruction memory access pattern.  This has led to the development of a wide range of defenses - including the ubiquitous constant-time programming model, information flow-based tracking, and more  all of which seek to prevent secret data from being used as an address to memory/control-flow instructions.



"Recently, however, Augury" - that's what they called their first research two years ago, A-U-G-U-R-Y, and it related to an auger being used - "demonstrated that Apple M-series CPUs undermine this programming model by introducing a Data Memory-Dependent Prefetcher that will attempt to prefetch addresses found in the contents of program memory.  Thus, in theory, Apple's DMP leaks memory contents via cache side channels, even if that memory is never passed as an address to a memory/control-flow instruction."  Okay.  And again, I will explain exactly what all that means.  I've got a couple paragraphs left.



They said:  "Despite the Apple DMP's novel leakage capabilities, its restrictive behavior has prevented it from being used in attacks.  In particular, Augury reported that the DMP only activates in the presence of a rather idiosyncratic program memory access pattern, where the program streams through an array of pointers and architecturally dereferences these pointers.  This access pattern is not typically found in security critical software such as side-channel hardened constant-time code, hence making that code impervious to leakage through the DMP.



"With the DMP's full security implications unclear, in this paper we address the following two questions:  Do DMPs create a critical security threat to high-value software?  And can attacks use DMPs to bypass side-channel countermeasures such as constant-time programming?  This paper answers the above questions in the affirmative, showing how Apple's DMP implementation poses severe risks to the constant-time coding paradigm.  In particular, we demonstrate end-to-end key extraction attacks against four state-of-the-art cryptographic implementations, all deploying constant-time programming."  And just to be clear, when they say "end-to-end attacks," they mean they run something, and they get the key.  Meaning all the work is done, nothing left for the reader to finish.  You know, this thing works.



Okay.  As we've had the occasion to discuss through the years on this podcast, the performance of DRAM, the dynamic RAM memory that forms the bulk of our system's memory, has lagged far behind the memory bandwidth demands of our processors.  Through the years we've been able to significantly increase the density of DRAM, but not its performance.  And as we know, even the increase in density has met with challenges in the form of susceptibility to adjacent row interference which led to the various DRAM hammering attacks.



But on the performance side, the saving grace has been that processor memory access patterns are not linear and non-repetitive.  They are typically highly repetitive.  The programs almost always "loop," meaning that they are executing the same code again and again, over and over.  And that, in turn, means that if a much smaller but much faster "cache" of memory is inserted between the main DRAM and the processor, the processor's repetition of the same instructions, and often the data for those instructions, can be fulfilled much more quickly from the local cache than from main memory.



During our discussions of speculative execution we saw that another way to speed up our processors was to allow the processor to run well ahead of where execution was; and, if the code encountered a fork in the road, in the code's flow, it would fetch ahead down both paths of the fork so that once the path to be taken became known, whichever way that went, the system would already have read the coded instructions for that path and have them ready to execute.  In practice, this is accomplished by breaking our processors into several specialized pieces, one being the prefetch engine whose job it is to keep the execution engines fed with data from main memory.



Many instructions do not make any main memory accesses.  They might be working only within the processor's internal registers or within what's already present in the processor's local cache.  So this gives the prefetching engine time to anticipate where the processor might go next and to guess at what it might need.  In a modern system, there's never any reason to allow main memory to sit idly by, not even for a single cycle.  A good prefetching system will always be working to anticipate its processor's needs and to have already loaded the contents of slower DRAM into the high-speed cache when the processor gets to needing it.



Okay.  Now let's add one additional layer of complexity.  One of the features of all modern processor architectures is the concept of a pointer.  A location in memory or the contents of a register could contain an object's value itself, or instead it could contain the memory address of the object.  In that second case we would say that the value in the memory or register contains, instead of the value of the object itself, a pointer to the object.  As a coder, I cannot imagine my life without pointers.  They are absolutely everywhere in code because they are so useful.



We need one bit of new vocabulary to talk about pointers.  Since a pointer is used to point to or refer to something else, the pointer contains a reference to the object.  So we call the act of following a pointer to the object "dereferencing" the pointer.  We'll see the researchers using that jargon in a minute.  But first let's think about that cache-filling prefetch engine.  Its entire reason for existence is to anticipate the future needs of its processor so that whatever the processor wants will already be waiting for it and instantly available from its cache.  The processor will think that its prefetch engine is magic.



So one evening, probably about seven years ago, some Apple engineers are sitting around a whiteboard with a bunch of half-eaten pizzas.  They're brainstorming ways to further speed up Apple's proprietary silicon.  Given the timeframe, this would first be able to appear in their A14 Bionic processor.  So one of them says:  "You know, we're already doing a great job of fetching the data that the processor is going to ask for.  But when we fetch data that contains what looks like pointers, we're not fetching the data that those pointers are pointing to.  If the data really are pointers, then there's a good chance that once the processor gets its hands on them, it's going to be asking for that data next.  We could anticipate that and have it ready, too, just in case it might be useful.  I mean, what's the whole point of being a prefetching engine; right?  That's the whole point.  That's what we're here for."



Now, at this point, the pizza is forgotten, and several in the group lean forward.  They're thinking about the kinds of cars they're going to be able to get with the raises this idea will earn them.  Then they realize they need to make it work first.  Although they're immediately hooked by the idea because they know there's something there, one of them plays devil's advocate, saying:  "But the cache is context-free."  What he means by that is that the prefetch engine sees everything as data.  It's all the same to it.  The prefetcher doesn't know what the data means.  It has no meaning in DRAM.  It's all just mixed bytes of instructions and data.  It's a hodgepodge.  It's not until that data is fetched from the cache and is actually consumed by the processor that the data acquires context and meaning.



The answer to the "but the cache is context-free" guy is, yeah, and so what?  If some data that's being added to the cache looks like a pointer, and if it's pointing into valid DRAM memory, what's the harm in treating it as a pointer and going out and also grabbing the thing that it might be pointing to?  If we have time, and we're right, it's a win for the processor.  The processor won't believe its luck in already having the thing it was just about to ask for already magically waiting there in its local cache.



So finally, after their last dry-erase marker stops working from the hastily scribbled diagrams on their whiteboards, they're satisfied that they're really onto a useful next-generation optimization.  So one of them asks:  "Okay.  This is good.  But it needs a name.  What are we going to call it?"  One of them says, "Well, how about Data Memory-Dependent Prefetching, or DMP for short?"



So here we've just seen a perfect example of where and how these next-generation features are invented, over pizza and dry-erase markers.  And it's also easy to see that the security implications of this don't even make it onto the radar.  All they're doing, after all, is anticipating a possible future use of what might be a pointer, and prefetching the thing it's pointing to in case they're right, and it is a pointer, and in case the processor might eventually ask for it.  It's disconnected from whatever the processor is doing; right?  It's a Data Memory-Dependent Prefetcher.  What this amounts to is a somewhat smarter prefetcher.  It cannot be certain whether it's fetching a pointer.  But in case it might be, it'll just jump ahead even further to also prefetch the thing that what might be a pointer may be pointing to.



Okay.  So now let's hear from the geniuses who likely also consumed their share of pizza while they scratched the itch that had apparently been lingering with at least three of them for a couple of years, ever since that first bit of work, when they discovered that Apple had dropped this Data Memory-Dependent Prefetcher into their silicon.  Here's how they explain what they came up with.



They said:  "We start by reexamining the findings in Augury.  Here we find that Augury's analysis of the DMP activation model was overly restrictive and missed several DMP activation scenarios.  Through new reverse engineering, we find that the DMP activates on behalf of potentially any program, and attempts to dereference any data brought into cache that resembles a pointer.  This behavior places a significant amount of program data at risk, and eliminates the restrictions reported by prior work.  Finally, going beyond Apple, we confirm the existence of a similar DMP on Intel's latest 13th-generation Raptor Lake architecture with more restrictive activation criteria.  Next, we show how to exploit the DMP to break security-critical software.  We demonstrate the widespread presence of code vulnerable to DMP-aided attacks in state-of-the-art constant-time cryptographic software, spanning classical to post-quantum key exchange and signing algorithms."



Okay.  And then finally, this last bit is the key to everything.  I'll read it first, then take it apart.  They said:  "Our key insight is that while the DMP only dereferences pointers, an attacker can craft program inputs so that when those inputs mix with cryptographic secrets, the resulting intermediate state can be engineered to look like a pointer, if and only if the secret satisfies an attacker-chosen predicate.  For example," they said, "imagine that a program has secret s, takes x as input, and computes and then stores y = s?x (s XORed with x) to its program memory.  The attacker can craft different x's and infer partial or even complete information about s by observing whether the DMP is able to dereference y.  We first use this observation to break the guarantees of a standard constant-time swap primitive recommended for use in cryptographic implementations.  We then show how to break complete cryptographic implementations designed to be secure against chosen-input attacks."



Okay.  So they realized that Apple's DMP technology is far more aggressive than they initially appreciated.  It is busily examining all of the data that's being put into the cache for all of the processes running in the system.  It's looking for anything that looks "pointer-like"; and, when found, it's going out and prefetch that because it may be pointing to something that the process was going to ask for in the future.



Their next step was to realize that, since this "pointer-like" behavior is highly prone to producing false positive hits which would prefetch miscellaneous bogus data, and since it operates indiscriminately on any and all data in the system, they can deliberately trick Apple's DMP system to misfire.  When it does, it will prefetch data that wasn't really being pointed to, and they can use standard, well-understood cache probing to determine whether or not the DMP did in fact misfire and prefetch.  Since the cause of that mixes secrets with what they provide, it reveals information about the secret.



They induce the isolated process containing the secrets to perform a large number of cryptographic operations on their deliberately crafted data while using the now well-understood behavior of the DMP to create an inadvertent side channel that leaks the secret key, even though the cryptographic code itself is being super careful not to behave differently in any way based upon the value of the secret key.  In other words, it's being betrayed by this advanced operation of their prefetching cache.  The code's care doesn't matter because the cryptographic code, as I said, is being betrayed.  What I've just explained is a version of what these very clever researchers revealed to Apple back 107 days ago from last Thursday in early December last year.



So what does Apple do about this?  This does seem like the sort of thing Apple ought to be able to turn off.  One of the things we've learned is that these initial nifty-seeming slick performance optimizations, like Spectre and Meltdown and all the others, always seem to come back to bite us sooner or later.  So the lesson we absolutely as an industry have to take away, and it's surprising we haven't yet, is that anything like this should have an off switch.



And what do you know?  It may have been, and likely was, in reaction to these researchers' initial Augury DMP paper back in 2022 that Apple added that off switch to their M3 chip.  Apple announced it on October 30th last year, the day before  Halloween.  And that M3 can have DMP turned off.  I've heard, but I haven't confirmed, that Apple's own crypto code is flipping DMP off during any and all of their own cryptographic operations.  So it may only be non-Apple crypto code running on Macs that are endangered on M3-based machines.  The researchers cite their compromise of the Diffie-Hellman Key Exchange in OpenSSL, you know, not an Apple library, and the RSA key operations in the Go language library.  So again, not Apple's.



So what about the non-M3 chips, the Apple A14 Bionic, the M1, and M2?  Well, it turns out that these so-called SoC, you know, Systems on a Chip, all have multiple cores, and the cores are not all the same type.  Only half of the cores are vulnerable because only half of them incorporate the DMP.  Apple's M-series have two types of cores:  the bigger Firestorm cores, also known as the performance cores; and the smaller Icestorm cores, also known as the efficiency cores.  On the M1 and M2 chips, only the Firestorm performance cores offer the problematic DMP prefetching system.  So all Apple needs to do is to move their crypto over to the smaller efficiency cores.  Crypto operations will run more slowly there, but they will be completely secure from this trouble.



So is Apple going to do any of these things?  Have they already?  The press thinks that nothing has been done yet.  I find that curious given that the concerns are real and that solutions are available.  But so far all the press has reported - now, again, Apple knew about this in early December.  All the press has reported that Apple has been curiously mute on the subject.  Apple just says "no comment."  This is doubly confounding given that Thursday's research disclosure came as no surprise to them, right, and also that the firestorm of truly over-the-top apoplectic and apocalyptic headlines that have ensued as a result really does need a response.



I imagine that something will be forthcoming from Apple soon.  Until then, for what it's worth, the attack, if it were to happen, would be local, and would be targeted, and would require someone arranging to install malware onto the victim's machine.  It's not the end of the world.  And as I'm always saying around here, anyone can make a mistake.  But Apple's customers would seem to need and deserve more than silence from Apple.  So we ought to hear something.



LEO:  Yeah.



STEVE:  But at least now we understand exactly what's going on.



LEO:  And by the way, if somebody can install that on your system, they could also just put a keystroke logger on there.  There's all sorts of ways they can get full access.  That's, you know, in fact that's probably a lot easier, to do it some other way than a side-channel attack.  Does it take a lot of monitoring and trial and error to have the side-channel...



STEVE:  No.



LEO:  It doesn't.



STEVE:  It takes an hour, and you get the key.



LEO:  Okay.



STEVE:  And so you actually do get a secret that was trying to be protected.



LEO:  So I can see a nation-state saying, oh, good, all right.  What we'll do is we'll get this on there through some other malware exploit.  We'll run it, and then we'll erase all traces. Guy will never know he was hacked.  But we've got the key, and we've got the key forever.



STEVE:  Right.



LEO:  Until he changes it.  I can see that.



STEVE:  Right.  And, yeah, and the point I made was that, you know, when this became public two years ago, these guys apparently stopped their research.  We don't know that the NSA did.  The NSA might have gone, hey, that's interesting.  Let's take a look at that.



LEO:  Oh.  The NSA could - NSA's probably been working on the same thing forever; right?  I mean, they know about these side-channel attacks.  They know about speculative execution.  They know what Spectre and Meltdown produced on the x86 platforms.  I'm sure they were looking for it, too.  Just whose professors are better, I guess.



STEVE:  Yeah.  Hopefully we have good profs.



LEO:  I think we have good profs in the NSA.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#968

DATE:		April 2, 2024

TITLE:		A Cautionary Tale

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-968.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Why should all Linux users update their systems if they haven't since February?  What do 73 million current and past AT&T customers all have in common?  What additional and welcome, though very different, new features await Signal and Telegram users?  Which major IT supplier has left Russia early?  What did Ghostery's ad blocking profile reveal about Internet users?  Whatever happened with that Incognito Mode lawsuit against Google?  And how are things going in the open source repository world?  And then, after I share something kind of special that happened Sunday involving my wife, SpinRite, and her laptop  and it's probably not what you think  we're going to take a look at another rather horrifying bullet that the Internet dodged again.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And yes, we are going to talk about one of the most interesting and in many ways scary security flaws on the Internet - and I'm not talking about AT&T, although we'll talk about that, as well - Steve's explanation of the XZ flaw.  We will talk about AT&T.  What's taking them so long in telling us what's going on?  Clearly there's a problem.  There's also a lot more to talk about, including problems with NPM and PyPI.  A great SpinRite success story in Steve's own household.  That, and a lot more.  Oh, and also some scary numbers about how many people use ad blockers.  That and a lot more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 968, recorded Tuesday, April 2nd, 2024:  A Cautionary Tale.



It's time for Security Now!.  You wait all week for this.  I know you do.  It's a long seven days.  But yes, Tuesday is here again, and so is Steve Gibson, the man in charge of Security Now!.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.	



LEO:  Hello, Steve.



STEVE:  It's great to be back.  Hello, Newman.



LEO:  Hello.



STEVE:  Great to be back with you again, as always.  And you asked me right off the bat, are we going to be talking about XZ.  And I said, "Of course."  So, but I'm, you know, and of course this is what was discovered that almost, I mean, actually I was going to say almost got into the outside world, but actually it did, and there's had to be some rollback.  But I titled today's podcast "A Cautionary Tale" because, you know, we're seeing more problems like this as we're moving forward.



LEO:  Well, you've been talking about supply chain attacks for ages, like PyPI and all of these libraries.



STEVE:  Yup.  And in fact PyPI had a problem.  NPM had a problem.  There's more pressure being turned up.  So anyway, we have a great podcast today.  We're going to look at a different reason why all Linux users need to update their systems if they haven't since February.  What 73 million current and past AT&T customers all have in common.  We're going to answer the question of what additional and welcome, though very different, new features await Signal and Telegram users?  Which major IT supplier just left Russia early last week?  What did Ghostery's ad blocking profile survey reveal about Internet users?



Whatever happened with that Incognito Mode lawsuit that was a class-action suit that actually looks like it did more than generate money for the attorneys?  What happened with that?  And how are things going in the open source repository world, as I already, you know, suggested not well?  And then, after I share something kind of special that happened day before yesterday, on Sunday, involving my wife, SpinRite, and her laptop.  And it's probably not what you think.  We're going to take a look at another rather horrifying bullet that the Internet dodged once again.  But it begs the question, you know, they're coming faster, and the dodge is, like, barely working.  At some point one might hit us.  And what would that be like?



LEO:  It's an amazing story.  I can't wait to hear you talk about it.  And it really is completely serendipitous that we did dodge that bullet because it wasn't even a security researcher who discovered it.  It's just mindboggling how close we came on this one.  And it's inevitable that it's not going to always be that way.



STEVE:  Well, and Leo, had we not - and we'll get into this, of course.  But the bullet that would have hit is serious.



LEO:  Yeah.



STEVE:  I mean, it's like, not like, oh, if you, you know, tap your heels three times, you know, and in some nonstandard configs that almost no one has.  This would have been - it would have slammed the world.



LEO:  Every bit of this story is a novel.  I mean, it's just - there should be a movie about it because it's so mindbogglingly wild.  Anyway, I can't wait to hear you tell it.  Maybe if you do it just right we can option it out and make some money.  The Picture of the Week.



STEVE:  Yeah.  Nobody wants to sponsor this picture.



LEO:  Except maybe Underwriters Lab.



STEVE:  Wow.  Oh.



LEO:  All right.  What is this?  What's going on here?



STEVE:  So this is roughly related to last week's disastrous, let's use coat hanger wire hanging over the prongs of a USB charger to hang our cables.  This is what happens when you're in a room that only has a single one of those European-style, you know...



LEO:  This looks like a prison room, I'll be honest with you.  It doesn't look...



STEVE:  It is a little depressing, yes, I agree.  It's a little horrifying.  So if you, you know, the European-style plug that looks like a vampire has...



LEO:  I think it's in the UK.  It's a UK plug style.



STEVE:  Oh, it's a UK.  Okay.



LEO:  I think so, yeah.  I'm not - no, no, it isn't.  No, you're right, it's EU.  You're right, yeah.



STEVE:  And so it's just two round holes.  And but you've got three things that you're trying to put into the round holes.  So some enterprising individual said, well, okay, I don't have one of those AC expansion boxes.



LEO:  You know what I do have?  I have a coat hanger.



STEVE:  They've got some spare wire.  So suffice to say that this person basically created a physical expansion using some wire successively wrapped around three sets of these two pin plugs, which then...



LEO:  This is not a surge suppressor.  This is a surge aggressor.  This is...



STEVE:  This is a surge disaster.



LEO:  Disaster.



STEVE:  Yeah, not good.  So anyway, just, you know, I mean, this exists in the real world.  I thank our listeners for finding these and saying, "Okay, Steve, I saw this photo and thought of you."  This definitely needs to be something to be shared.



LEO:  Wow, wow.



STEVE:  Do not try this at home.



LEO:  No.



STEVE:  No matter where in Europe you may live.



LEO:  Nightmare.  Nightmare.



STEVE:  Because, yeah.  And needless to say, this is all exposed; right?  You don't want to touch any of this because the whole point of having those two pins is there's really no way you can hurt yourself electrically.  In the U.S., and I'm sure this has happened to you, Leo, back in your earlier days, you'd be pulling a cord out and inadvertently let a thumb touch across the two metal prongs and go [sound], you know, get yourself a little zap.



LEO:  Oh, yeah.



STEVE:  And, you know, it's like, [sound].  I, frankly, know the feeling quite well.



LEO:  Yeah, somebody in our Discord is pointing out that in fact, Chickenhead says, "With great foresight, they've placed it very close to a metal bed frame."  This may be some sort of torture device.  I don't know what's going on.



STEVE:  Not good.  Not good.  Okay.  So Last Friday, which of course was Good Friday, was not a good Friday for Linux.  Not only did the world first learn, which we'll be talking about later, of a near miss on the dissemination of an SSH backdoor - again, this will be the way we wrap up today's podcast.  But the public learned of a widespread elevation of privilege vulnerability affecting the Linux kernel from at least versions 5.14 through 6.6.14.  Running the exploit as a normal user - which was released as a proof of concept on GitHub, I have a link in the show notes if anyone wants to go look - on a vulnerable machine will grant root access allowing the exploiter to do whatever they wish.  Now, the good news, a local-only exploit, so not remote.  This could be used, however, by rogue insiders, or perhaps by malware that's already on a computer, allowing it to cause further damage and problems.



The affected distributions include Debian, Ubuntu, Red Hat, Fedora, and doubtless many other Linuxes.  And it had - I love this - a 99.4% success rate given 1,000 tries.  The author of this tried it a thousand times.  In other words, it worked 994 of those thousand times, and only failed six times.  I noted that the public learned of it last Friday because it was responsibly disclosed - that's the good news - to the Linux community insiders back in January; and then since the end of January updates to this, which was given a CVSS 7.8 flaw, have been rolling out.  You know, the fix was clear and simple, didn't take long to patch or test.



It was very obvious once everyone saw it, it was like, ooh, yeah, that's not what we meant, or we intended.  So it's because the updated code had been available by last Friday for two months that its discoverer felt it was time, enough time had gone by, and the updates had been pushed out for two months, that he was able to disclose it all on GitHub, along with a proof of concept.  So, and boy, he has a super detailed description of what he found.  Again, links to all that in the show notes.



The takeaway for anyone using Linux who may not have updated since before February, meaning that their current build is probably still buggy, is that knowledge of this is now public and widespread.  So if anyone untrusted might have or be able to gain access to any unpatched machines, be a good time now to get them updated because, as I said, the word is out.



TechCrunch has been on top of the news of a significant breach of very sensitive customer data from AT&T.  The story begins five years ago, back in 2019, which TechCrunch first reported 10 days ago on the 22nd of March.  They gave it the headline, and this is the first of two stories because then they updated their coverage.  So this first report they gave the headline "AT&T won't say how its customers' data spilled online."  And in fact that's still the case.  But that's a little more difficult, as we'll see, for them to be denying it.



So TechCrunch explained.  They said:  "Three years after a hacker first teased an alleged massive theft of AT&T customer data, a breach seller this week" - meaning three weeks ago - "dumped the full dataset online.  It contains" - and this is the real concern - "the very personal information of 73 million AT&T customers."



LEO:  Oh, boy.  Including me, I'm sure, yeah.



STEVE:  Yeah, yeah.  And I was an AT&T customer back in the day.  I finally, I had AT&T landlines until all they were ever getting was robocalls.



LEO:  Yeah.



STEVE:  And I thought, oh, come on.  Why am I paying for this anymore?  "So a new analysis of the fully leaked dataset, which contains names, home addresses, phone numbers, Social Security numbers, and dates of birth..."



LEO:  Ooh, everything.



STEVE:  In other words, everything you need for identity theft  "...points to the data being authentic.  Some AT&T customers have confirmed their leaked customer data is accurate.  But AT&T still hasn't said how its customers' data spilled online."  They're like, well, I don't know, you know.  We don't know if it was us.  "The hacker, who first claimed in August of 2021" - which was two years after he got the data - "to have stolen millions of AT&T customers' data, only published a small sample of the leaked records online," hoping to use that as proof.  But the sample was so small that it was a little difficult to verify that, like, a much larger breach was authentic.



"AT&T, the largest phone carrier in the U.S., said back in 2021 that the leaked data 'does not appear to have come from our systems,'" - you know, because we would like it not to appear that way - "but AT&T chose not to speculate as to where the data had originated or whether it was valid."  Now, Troy Hunt, who we know well, a security researcher and owner, TechCrunch wrote, "of data breach notification site Have I Been Pwned, recently obtained a copy of the full leaked dataset.  Hunt concluded the leaked data was real by asking AT&T customers if their leaked records were accurate.  In a blog post analyzing the data, Troy said that of the 73 million leaked records, the data contained 49 million unique email addresses, 44 million Social Security numbers, and customers' dates of birth.



"When reached for comment, AT&T spokesperson Stephen Stokes told TechCrunch in a statement, once again:  'We have no indications of a compromise of our systems.'"  Everything's working great here, folks.



LEO:  Very angering.  That's just awful.



STEVE:  Yeah.  He said:  "We determined in 2021 that the information offered on this online forum did not appear to have come from our systems."  So still just head buried in the sand.  He said:  "This appears to be the same dataset that has been recycled several times on this forum."  Meaning this is nothing new.  We'd prefer to close this statement now.  "The AT&T spokesperson," they said, "did not respond to follow-up emails by TechCrunch asking if the alleged customer data was valid or where its customers' data came from."  So basically, again, just head in the sand.



"As Troy Hunt notes," they wrote, "the source of the breach remains inconclusive.  And it's not clear if AT&T even knows where the data came from.  Hunt said it's plausible that the data originated either from AT&T or 'a third-party processor they use or from another entity altogether that's entirely unrelated.'"  Again, we just don't know, and they're not saying.



TechCrunch said:  "What is clear is that even three years later, we're still no closer to solving this mystery breach, nor can AT&T say how its customers' data ended up online.  Investigating data breaches and leaks takes time.  But by now," they wrote in their conclusion of the first posting, "AT&T should be able to provide a better explanation as to why millions of its customers' data is indeed online for all to see."



Okay.  So that was 10 days ago.  In follow-up reporting just last Saturday, under their headline "AT&T resets account passcodes after millions of customer records leak online" - I guess maybe AT&T, you know, lumbers along, and it just takes them a while to say, oh, maybe this is new - TechCrunch explains.  They said:  "TechCrunch has exclusively learned that phone giant AT&T has reset millions of customer account passcodes after a huge cache of data containing AT&T customer records was dumped online earlier this month.  The U.S. telco giant initiated the passcode mass reset after TechCrunch informed AT&T on Monday that the leaked data contained encrypted passcodes that could be used to access AT&T customer accounts."  So whoops.  It's no longer all just five years ago and we don't know what it is or even if it's actually our data.  Suddenly, whoops, we're going to mass reset passcodes.



"A security researcher who analyzed the leaked data told TechCrunch that the encrypted account passcodes are easy to decipher."  Now, that's not quite correct technically.  I'll explain what happened here in a second.  "TechCrunch alerted AT&T to the security researcher's findings.  In a statement provided Saturday, AT&T said:  'AT&T has launched a robust investigation'" - right, not a moment too soon - "'supported by internal and external cybersecurity experts.'"  Oh, they had their own internal experts.  "'Based on our preliminary analysis, the data set appears to be from 2019 or earlier, impacting approximately 7.6 million current AT&T account holders and approximately 65.4 million former account holders.'"  You know, count me among the former category because I left AT&T.



"The statement also said:  'AT&T does not have evidence of unauthorized access to its systems resulting in exfiltration of the data set.'"  Yeah, five years ago.  Right.  "TechCrunch withheld the publication of this story until AT&T could begin resetting customer account passcodes.  AT&T also has a post on what customers can do to keep their accounts secure.  AT&T customer account passcodes are typically four-digit numbers that are used as an additional layer of security when accessing a customer's account, such as calling AT&T customer service, in retail stores, and online."



LEO:  We've told people, to avoid SIM swapping, set a PIN with your carrier.  That's the PIN; right?



STEVE:  Yup, exactly.  And I know that when I did have occasion to call AT&T, they would say, "What's your PIN?"



LEO:  Right.



STEVE:  And I would tell them what it was, and it was indeed four digits.  So, and that was, you know, not optional.  It was like four, you know, that was all you could do.  So this, writes TechCrunch, "is the first time AT&T has acknowledged that the leaked data belongs to its customers, some three years after a hacker claimed the theft of 73 million AT&T customer records."  Which now has been, you know, even AT&T said, yup, uh, we're not happy with that number, but we have to tell the truth when we're confronted with no choice.  "AT&T had denied a breach of its systems" - and actually they're still saying, eh, well, we don't know how it got out there, but it does look like it's ours.  On Saturday they said:  "It is not yet known whether the data in those fields originated" - wow, they're still denying it - "from AT&T or one of its vendors."



Now, "Security researcher Sam Croley told TechCrunch that each record in the leaked data also contains the AT&T customer's account passcode in an encrypted format.  Croley double-checked his findings by looking up records in the leaked data against AT&T account passcodes known only to him.  Croley said it was not necessary to crack the encryption cipher to unscramble the passcode data."  Why?  We're going to see here in a second.  "He took all of the encrypted passcodes from the 73 million dataset and removed every duplicate.  The result amounted to" - get this - "10,000 unique encrypted values."  In other words, starting at 0000 and finishing at 9999.  That's 10,000.  So they're encrypted.  But after he sorted all 73 million of them and removed duplicates, what do you think he got?  Well, he got encrypted representations of every passcode from 0000 to 9999.



Okay.  So in other words AT&T, we now know, used a fixed cipher with no salt to encrypt the entire dataset, and the encryption never changed.  Every encryption maps one-to-one to some four-digit passcode.  This means that there's no mystery here. TechCrunch wrote:  "According to Croley, the insufficient randomness of the encrypted data means it's possible to guess the customer's four-digit account passcode based on surrounding information in the leaked dataset."



Okay, in other words, since all 73 million users were restricted to using four-digit passcodes, and all passcodes are identically encrypted, once any single user's passcode can be guessed, and we see what that encrypts to, we also know everyone else who chose the same four digits because they will have an identical encrypted code.



LEO:  Because of the lack of salt.



STEVE:  Correct.  Exactly.  It's kind of a mess.



LEO:  Yeah.



STEVE:  TechCrunch wrote:  "It's not uncommon for people to set passcodes, particularly if limited to four digits, that mean something to them.  That might be the last four digits of a Social Security number [which is in the data set], or the person's phone number [in the data set], the year of someone's birth [in the data set], or even the four digits of a house number {in the data set]."



LEO:  By the way, this was AT&T's practice, I think it still is, to set your voicemail passcode to the last four digits of your phone number.  So I'll bet you anything that...



STEVE:  People didn't change it.



LEO:  It's in the data set.  Nobody changed it, yup.



STEVE:  Yup.  "All of this surrounding data is found," they wrote, "in almost every record in the leaked dataset.  By correlating encrypted account passcodes to surrounding account data  such as customer dates of birth, house numbers, partial Social Security numbers, and phone numbers  Croley was able to reverse engineer which encrypted values matched which plaintext passcode."  In other words, zero protection.



"AT&T said it will contact all of the 7.6 million existing customers whose passcodes it just reset, as well as current and former customers whose personal information was compromised."  And that's 76 million in total.  So let's hope that, when they do this, like this passcode reset, they have a new and improved means of encrypting passcodes, hopefully using a unique per-user random salt so that it's no longer possible to create a simple one-to-one mapping of passcodes to their encryptions.  And, you know, doing this without salt is really so last century.



So I haven't, and really don't have much need for cross-platform secure messaging.  iMessage does everything I need.  But as I mentioned a few years ago, the talented PHP programmer Rasmus Vind, who's a listener of ours - he's the guy who helped integrate SQRL into the XenForo web forums that GRC uses.  And by the way, I'm so happy with that choice.  If anyone is looking for forum software, I could not be more pleased with the decision I made many years ago.  Anyway, he suggested that we use Signal to converse while we were working to get all of the details worked out of getting SQRL to log in.  So that exposed me to Signal, and to its somewhat awkward restrictions, which Leo, you and I have talked about from time to time because it's kind of a pain.



So I was interested and pleased to read that Signal is beginning to loosen things up a bit.  We talked about another instance of that loosening I think a week or two ago.  The site SignalUpdateInfo.com, which apparently exists just to watch and report on Signal happenings, they wrote:  "Cloud Backup Status in Development."  They said:  "There has not been any official announcement, but it appears that Signal is currently working on cloud backup for iOS and Android.  This feature would allow you to create cloud backups of your messages and media.  While we have known," they wrote, "about cloud backups since a commit to Signal-iOS on October 20th, 2023, a recent commit to Signal for Android has revealed many more details.  It looks like there could be both free and paid tiers.  The free tier would provide full message and text backups, but only media backups from the last 30 days.  The paid tier could provide full message text and media backups with a storage limit of 1TB.



"From the commit," they wrote, "we also know that Signal backups will be end-to-end encrypted and completely optional, with the ability to delete them at any time.  Cloud backups could significantly improve usability on all platforms by preventing complete data-loss in the event that you lose your phone or encounter some other issue."  And apparently it also suggests that there would be a means for migrating content between devices, which would really be a nice boost in usability.  So, you know, bravo for Signal.  I don't know if they're feeling competitive pressure; if they're feeling like, well, you know, their own users are saying why can't I do X, Y, or Z?  And they're saying, well, yeah, okay, that's probably a good point.  We need to make this better.



On the Telegram side, Telegram users, initially located in Russia, Ukraine, and Belarus, have received a welcome new feature which allows users to restrict who can message them.  Last Wednesday, Telegram's founder, Pavel Durov, sent the following message through Telegram.  Now, when I looked it up, it was in Russian, so I fired up a copy of Chrome in order to bring it up under Chrome and have Google translate it for me.  And I have to say it did a pretty good job.



So the English translation of what Pavel wrote in Russian reads:  "Four days ago, Russian-speaking Telegram users began to complain about messages from strangers calling for terrorist attacks.  Within an hour of receiving such complaints, we applied a number of technical and organizational measures in order to prevent this activity.  As a result, tens of thousands of attempts to send such messages were stopped, and thousands of users participating in this flash mob faced permanent blocking of their Telegram accounts.  From the beginning of next week" - and he wrote this last week, so this week - "all users from Russia, Ukraine, and Belarus will be able to limit the circle of those who can send them private messages.  We are also implementing AI solutions to handle complaints even more efficiently."  And he finishes:  "Telegram is not a place for spam mailings and calls for violence."  And bravo to Google Translate for coming up with that in English for me from the Russian because, you know, that was flawless.



But speaking of Google Translation, when I fired up Chrome to view the page and had it perform the translation, I then copied it and posted the result over to Firefox.  And I just sort of forgot about Chrome.  I left it running.  So there was one page open, doing nothing, yet it caused my machine's fans to spin up to full speed.



LEO:  Yes.



STEVE:  Which I thought, what the heck.  And, you know, when I noticed that, I opened Task Manager and saw that Chrome was at the top of the process utilization list, consuming half of my very strong multi - I think I've got 16 cores or something - while it was doing nothing.  So I had forgotten that it used to do that when I was using it there for a while, before I switched back to Firefox.  And my machine became blessedly silent.  So thanks anyway, Chrome.  Wow.



Also, speaking of Russia, HP has ceased all operations and has exited the Russian market ahead of schedule.  HP had terminated all shipments into Russia two years ago, a little over two years ago, in February of 2022, after Russia's invasion of Ukraine.  Three months later, in May of 2022, HP began the slow process of winding down all of their corporate operations, and had planned to make their final exit next month, in May 2024.  So basically they said, back in 2022, in May, over the next two years, 24 months, we're going to be winding things down.  You know, maybe they hoped they wouldn't have to leave.  They hoped that this mess with Ukraine would get resolved; and, you know, they could stay.  But no, that hasn't happened.



So HP pulled out of Russia last week at the end of March, two months ahead of its planned departure.  The move surprised Russian companies, which are now no longer able to update drivers or contact HP support.  And I have to say I was wondering, what about those printers that tend to be rather persnickety?  I wonder what's going to happen with, you know, if they try to put non-HP ink into an HP printer in Russia, you know, what'll happen.  Who knows?



Okay.  This was - I got a kick out of this next piece because of what we learned about advertisers' use of ad blockers.  Ghostery published what they called their Tracker and Ad Blocker Report, which was the result from research conducted by an independent third-party research firm Censuswide.  It found that, and there are four main bullet points:  Individuals who have experience in advertising, programming, and cybersecurity are significantly more likely to use ad blockers than average Americans.



LEO:  That's hysterical.



STEVE:  Isn't that?  I love it.  Just great.



LEO:  I guess they know something, huh?



STEVE:  Uh-huh.  These industry insiders are more skeptical of their online safety, underscoring concerns about the current severity of user tracking.  Third bullet point:  Americans are underestimating the dangers of health and political tracking.  And finally:  Lesser-known Big Tech players sow distrust among these experts.



Okay.  So I have an infographic that I duplicated from the report.  It shows that, whereas 52% of all Americans use an ad blocker...



LEO:  That number is actually astounding.  That's more than half.



STEVE:  I know.  It is a far higher number than I would have ever guessed.  That's, like, yes, surprising.



LEO:  Cory Doctorow has called this the largest consumer boycott in history.  And I think these numbers bear it out.  That's incredible.



STEVE:  Wow, yeah.  Okay.  So 52% of all Americans.  On the other hand, 66%, so two out of every three, of experienced advertisers block their own ads.  You know?  Or theirs and everybody else's.  And I suppose they don't wish to be tracked, either.



LEO:  Yeah.



STEVE:  Okay.  The infographic divides all of those surveyed into four groups:  everyone - that was that all Americans - everyone; experienced advertisers; experienced programmers, you know, coders; and cybersecurity experts.  And that's also the order of increasing use of ad blockers.



LEO:  Yeah.



STEVE:  Everyone, advertisers, coders, and security experts, with the percentages of use respectively being 52%, 66, 72, and 76%.  So just over three quarters, 76%, of cybersecurity experts surveyed are using ad blockers.



The other interesting question each group was asked was why those who are using ad blockers are doing so.  They were asked to choose among one of four reasons:  for the protection of their online privacy, to block ads, to speed up page loading, or none of the above.  Interestingly, for every group of users, those four groups, the ranking among those four reasons - privacy, freedom from ads, web speed, or other - was identical and in that order.



The only difference was in the distribution among those reasons.  The generic "All Americans" group was the most evenly split between privacy and not wishing to be confronted with ads, at 20% and 18% respectively, and only 9% were using ad blockers for speed.  But among programmers and cybersecurity experts up at the other end, the split was much greater, at 30% wishing for privacy and 19% wanting not to see ads.



Now, with the changes Google has not only been promising but is now well on the way to delivering, with the implementation and enforcement of their Privacy Sandbox technologies in Chrome, the connection between advertising and privacy encroachment is truly being broken for the first time ever.  But it's a significant change that we can expect the recognition of to take quite some time to spread.  And I would argue probably much more time than we even expect.  You know, I've heard some of our listeners who simply don't believe it.  They don't really care about the technology behind it.  For them it's just blah, blah blah, blah blah.  You know, even if they understand it, they're just, you know, they're so jaded that they cannot conceive of a world where we are not implicitly paying for the web by relinquishing our personal information.



On my side, you know, I believe in technology that I understand.  And thanks to this podcast, which forced me to invest in acquiring an understanding of it so that I could share that understanding, I get what Google has wrought. It is good, it is right, and it makes sense.  If it was me doing this, you know, like me doing SQRL, it wouldn't matter how good it was.  But it's not me.  It's Google.  And it's Chrome.  And that means it's going to matter.  It's just going to take a long time, I mean, you know, a long time for the rest of the world to catch up.  And, you know, that's not a bad thing, either; right?  Inertia creates stability, and that's also good.  And all of those techies who have been coding the technology underlying the tracking, you know, they're going to need some time to find new jobs.  So this will give them some time.



As I noted recently, it's the presence of advertising that's financing the web.  And that shows no sign of changing.  Advertisers appear to be willing to pay double when they know that their ads are being well targeted, and "double" can easily make the difference between a website being financially viable and not.  But as the statistics which Ghostery collected showed, everyone knows that the way advertising initially evolved has not been privacy friendly.  So we've needed a long-term solution for giving advertisers the ad targeting that they're willing to pay for, while not trading away our personal information and privacy.



Google's final solution, which amounts to moving all advertising auctioning and ad choice into the user's web browser, I think is brilliant.  It flips everything on its head.  But it's a massive change, and that will not happen overnight.  So we're getting there, but it's going to take a while.



And speaking of Google and privacy, remember that confusion over just exactly how incognito those using Chrome's Incognito Mode really were?  Google's defense was that its users who were upset to learn that they were still being tracked and profiled while using Incognito Mode had simply failed to read the fine print about what, exactly, Incognito Mode did - and mostly did not - protect them from.  Well, it's time to massively delete, it turns out, everything that Google learned about their Chrome users while they believed they were in fact incognito and weren't so much.



The Hacker News just carried this morning a bit of news and what's been learned during the lawsuit discovery, which, you know, many companies try to avoid because, you know, their employees go under oath to be deposed, and the truth comes out; right?  So here's what Hacker News said.



"Google has agreed to purge billions of data records reflecting users' browsing activities to settle a class-action lawsuit that claimed the search giant tracked them without their knowledge or consent in its Chrome browser.  The class action filed back in 2020 alleged the company misled users by tracking their Internet browsing activity while they thought it remained private when using the Incognito or Private mode on web browsers like Chrome.  In late December 2023, it emerged that the company had consented to settle the lawsuit.  The deal is currently pending approval by U.S. District Judge Yvonne Gonzalez Rogers.



"A court filing yesterday (on April Fool's day) said:  'The settlement provides broad relief regardless of any challenges presented by Google's limited record keeping.  Much of the private browsing data in these logs will be deleted in their entirety, including billions of event level data records that reflect class members' private browsing activities.'  As part of the data remediation process," writes the Hacker News, "Google is also required to delete information that makes private browsing data identifiable by redacting data points like IP addresses, generalizing user-agent strings, and remove detailed URLs within a specific website, you know, retain only the domain-level portion of the URL.



"In addition, it has been asked to delete the so-called X-Client-Data header field, which Google described as a Chrome-Variations header that captures the 'state of the installation of Chrome itself, including active variations, as well as server-side experiments that may affect the installation.'  What's significant there is that this header is generated from a seed value, making it potentially" - a random seed - "making it potentially unique enough to identify specific Chrome users."  In other words, there's a serial number in the query headers that Chrome has been using.  Whoops.



"Other settlement terms require Google to block third-party cookies within Chrome's Incognito Mode for five years, a setting the company has already implemented for all users.  The tech company" - meaning Google - "has separately announced plans to eliminate tracking cookies by default by the end of the year."  And of course that's the whole Privacy Sandbox thing.  "Google has since also updated the wording of Incognito Mode as of January of this year to clarify that the setting will not change 'how data is collected by websites you visit and the services they use, including Google.'"  So, you know, they're being more clear and more explicit.



And here's the biggie that came out of the depositions:  "The lawsuit extracted admissions from Google employees that characterized the browser's Incognito browsing mode as 'a confusing mess,' 'effectively a lie,' and 'a problem of professional ethics and basic honesty.'"  Ouch.  "It further laid bare internal exchanges in which executives argued Incognito Mode should not be called 'private' because it risked 'exacerbating known misconceptions.'"



LEO:  Like that it was private.



STEVE:  Oh, yeah, that one.



LEO:  That one.



STEVE:  Right.  Just a little problem.



LEO:  A minor detail.



STEVE:  And finally, we will be talking shortly about the problems of malice in the open source world.  It's a problem everyone is having to deal with.  The NPM repository discovered eight new malicious packages last week, and PyPI was again forced to disable new user account creation and package uploads after being hit by a malware submission wave.



LEO:  Hundreds; right?  It was a ton of them, yeah.



STEVE:  Yes, yes.  And Ubuntu's caretaker, Canonical, has just switched to manual reviews for all apps submitted to the Ubuntu OS app store.  They needed to do this after multiple publishers attempted to upload malicious crypto-wallet applications to the store over the past couple of weeks.  The problem focused enough upon crypto-wallets that they plan to create a separate app submission policy for cryptocurrency wallets going forward.  So yes, it's kind of a mess out there.



LEO:  That's bad.  These people are bad.  I know we're getting to the fabulous story of XZ, but I want to hear what you did for Lorrie the other day. 



STEVE:  Okay.  So day before yesterday my wonderful wife Lorrie had her first experience of what SpinRite can mean for long-term solid-state storage management and maintenance.  It meant a lot to me, so I wanted to share it with our listeners.



Sunday morning she set up a Dell laptop to take a "Q."  That's what we call a QEEG, which is short for Quantitative Electroencephalogram.



LEO:  Wow.



STEVE:  This is the first phase for delivering EEG biofeedback therapy to her clients.  A client sits motionless for 15 minutes with their eyes open, and another 15 minutes with their eyes closed.  They're wearing an elastic cap covered with EEG electrodes recording the surface electrical potentials across the exterior of their skull.  Lorrie does these on Sunday mornings since there's no noise from nearby gardeners, and it's important to have a quiet setting.



While she was getting everything ready on Sunday, she turned the Dell laptop on to boot it up.  And she began waiting.  And waiting.  And waiting.  And I don't recall whether I happened to wander by or she called me over.  But the spinning dots were circling, and the little hard drive light icon was on solid.  She was a little bit impatient finally, and beginning to get a little worried, like she depends upon this, and the people were on their way, and was she going to be able to do this.  I pointed to the hard drive light that was on solid and just told her that, you know, the machine was still working on getting booted up.



But finally, after a truly interminable wait, Windows showed some signs of life.  When the desktop finally appeared, she started moving the cursor around, but the desktop was incomplete.  And I pointed to the little hard drive light again, showing her that, even though the desktop was visible, Windows was still working to finish getting itself ready.  And, you know, those of us who have known Windows for what seems like our entire lives know that Microsoft used to get complaints over how long Windows was taking to boot, so they arranged to put the desktop up, like, immediately, as fast as it possibly could, even though Windows wasn't actually ready.



LEO:  Drives me crazy.  Oh.  So cheap.  So cheap.



STEVE:  I know.  Yeah, exactly.  So after, like, another full minute or so, the rest of the tray icons finally populated, and the drive light began flickering, meaning spending some time being off, as Windows finally began to wrap up its work and get settled.  The program she uses to record EEGs is a true monstrosity.  It was written, unfortunately, by the engineers who designed and built the multi-channel EEG amplifier.  The hardware is a true work of art.  But the software that goes with it, not so much.  So she wondered how long that would take to start up.  She launched it, and again waited and waited and waited.  And it, too, finally showed up.  She was relieved and knew that when the family that she would be working with arrived, she'd be ready.



Having observed this, I said, "It looks like it would be worth running SpinRite on that laptop."  So a few hours later, after a successful session of EEG recording, she brought the laptop to me.  I plugged it in, booted into SpinRite, and set it going on Level 3 to perform a full rewrite/refresh of the drive.  And I had to push past 6.1's new warning screen reminding me that I was going to be rewriting mass storage media that preferred being read to being written.



So I did that and started SpinRite running on Lorrie's EEG recording laptop.  Looking at the Real Time Monitoring display, where the reading and writing cycles are very obvious, most of the time was being spent reading.  And once a 16MB block had been successfully read, rewriting it was just a flicker on the screen.  It has a series of bars, and reading is the top bar.  The final rewrite is the bottom bar.  And so there were little bursts of rapidly alternating reading and writing, but most of the very long pauses were just the SSD sitting there trying to read its own media.



This was a 256GB SSD, and I had looked.  About 89GB of that were in use.  And I noticed that by the time SpinRite was about halfway through, the reading and writing phases were flickering back and forth almost without interruption, with occasional pauses during writing while the SSD buffered, flushed, and rearranged its data.  So by that point I knew that we were through with the region that mattered.  I interrupted SpinRite, removed the little USB boot drive, and rebooted.  And the difference was truly astonishing.  Windows booted right up and was ready in maybe, like, 15 seconds or so.  So I shut it down, moved it back to where the rest of her EEG equipment was that she'd been using earlier.  Before we left the house for Easter dinner, I told her that SpinRite had finished with her machine and invited her to sit down and boot it up.  Needless to say, she was astonished by the difference.  She then launched the EEG recording software; and it, too, started almost immediately.



So, you know, she's put up with me for the past three and a half years, working every day and weekends and nearly every evening, without a break or vacation.  She's taken a few trips to visit friends while I've stayed home to work.  And she's let me because she knows it's what I have wanted to do more than anything else.  But for this work I've done to have made that much difference in the performance and operation of a machine she depends upon, well, it was a gratifying moment.  And, you know, "Yes, honey, your husband's not insane after all."



LEO:  You earned your keep, Steve.  You've got another 10 years.  Good job.



STEVE:  So we discovered this phenomenon early in SpinRite's development after I wrote the ReadSpeed utility, which was  meant as just a platform for testing SpinRite's new drivers.  We know and expected that spinning hard drives transfer data more slowly as we move toward the end of the drive.  But we naturally expected solid-state storage to have uniform performance across its media, and brand new SSDs do.  What we discovered was that SSDs that had been in service for a while had grown astonishingly slow at the front, where files live that are often read but not often rewritten.  You know, that's the operating system, which, you know, has gotten ridiculously large.



What's fascinating to me as an engineer is that the SSD in Lorrie's laptop could have become that slow while still eventually managing to return error-free results.  You know, there were no bad spots found.  It was just - it was astonishingly slow.



LEO:  That's interesting because on spinning drives it's usually there's something bad, it can hardly read the sector and has to try a lot.  But this isn't that.



STEVE:  Well, we know that inside SSDs are analog bit storage cells where each cell contains a voltage that represents some number of binary bits.



LEO:  Right.



STEVE:  Originally, it was just one bit, so the cell was either all on or all off, you know, fully charged or fully discharged, high or low.  There's always pressure to achieve higher density.  But the cells were already tiny.  So designers decided to store four voltages per cell instead of only two voltages.  Since four voltages could be used to represent two binary bits instead of just one, that allowed them to instantly double the storage density for the entire system.  Same number of cells, just store two bits per cell.  But why stop there?  Eight voltages could store three bits; and 16 voltages, god help us, could be used to store four.



Well, there's a well-known problem with SSDs known as "read disturb," where reading from a block of cells subtly disturbs the storage of neighboring cells.  We're witnessing that, as time passes, SSDs gradually require more and more time to perform error correction, or error correction is being used across a greater and greater portion to fulfill a request.  And it may also be that an SSD's error correction is not a fast thing to do since it's assumed not to be needed often.  And it's not a stretch to imagine that, if things are allowed to get bad enough, a solid-state drive will eventually report that it's unable to read and recover the contents of a block of data.  This is why preventative maintenance makes just as much sense, it turns out, for solid-state media as it always has for spinning platter drives.  This was not something that anyone really understood and appreciated until we began working on SpinRite 6.1.



An SSD is not only being restored to "factory fresh" performance, but the reason its performance is restored is that the voltages in the storage cells that had gradually drifted away over time from their original ideal levels will be reset.  That means no more extensive and slow error correction; much faster performance as a result; and, presumably, restored reliability.



So anyway, as I said, it meant a lot to me that Lorrie was able to finally see what I'd been up to for the past three and a half years, and experienced some of the benefit, you know, firsthand.



LEO:  Did she look at you and go, oh, it really works?



STEVE:  She was astonished.  And it's funny, too, because she said, she immediately said, "Oh, we should have taken a before video."



LEO:  Yes.



STEVE:  "So we would have" - and then she said, "Is there any way to put it back?"



LEO:  Oh, no.



STEVE:  And I said, "No, honey.  We'll have to just wait a few years for it to slow down again."  And then, you know...



LEO:  So every, what, every year or so you should do this just to refresh something?



STEVE:  I think so.  Yeah.  I mean, you know, people - we've had people say, like, who are using MacBook Airs, that, like, you know, I'm sure it was faster when it was new.



LEO:  Gotten slower, yeah, yeah, yeah.



STEVE:  Yeah.  It turns out, yes, it actually has.  It's not - we know with an SSD it's not fragmentation because they're solid-state.  It turns out it's actually the SSD itself is spending more and more time doing error correction as the sectors are becoming softer.  The actual bits are softening, and a rewrite is what they need.



So what's exciting is that 6.1 works.  But it's a bit of a blunt instrument.  I mean, it's all we have now.  What I'm excited about is that 7 will be able to find the individual slow spots and surgically selectively rewrite them.



LEO:  Oh, that would be nice.  Yeah.  This is not trim, though.  This is not - everybody in the chatroom is saying, oh, this must be trim.



STEVE:  Nope.



LEO:  No, this is separate from trim.



STEVE:  Nothing to do with trim.



LEO:  Yeah, yeah.  It's just voltage fluctuation and error correction.



STEVE:  Yeah.



LEO:  That makes a lot of sense, actually.



STEVE:  Yeah.



LEO:  So in effect you're just erasing that sector and rewriting it so that it's back to the way it was.



STEVE:  Yes, I am, exactly, I'm putting the data back, but after the error correction.  So the individual bits had drifted.  And error correction was able to - because basically error correction is extra bits.  And so by using the extra bits that are not part of the data, it's possible to, for the entire thing, to still be reconstructed.  But that takes time, which is why reading slows down.  But because SSDs know that they're fatigued from rewriting, they don't do it themselves.  Allyn once told me, you know, Malventano...



LEO:  Yeah, Allyn Malventano, yeah, yeah.



STEVE:  Allyn Malventano told me that no SSDs ever rewrite themselves.  So it takes an external agent to do that.  And so SpinRite serves that function.  Basically, while the sector can still be corrected, it takes the corrected data and rewrites it.  Which basically resets all the bits firmly so that it no longer needs correction.  And thus it can be read much more quickly.



LEO:  it would also probably be accurate to say you don't want to do this too often.  I mean, the reason they don't...



STEVE:  Correct.  Correct.



LEO:  ...they don't do it is they don't want to wear themselves out.



STEVE:  On the other hand, you look at the number of rewrites that SSDs are able to...



LEO:  Nowadays, yeah.



STEVE:  You know, it's hundreds of thousands.  So one a year is not making a big difference.



LEO:  Would that be your recommendation, maybe once a year?



STEVE:  Yes.



LEO:  Or when you see it slowing down, I guess.



STEVE:  Yup.



LEO:  Well, as soon as you get this working for the Mac, let me know.



STEVE:  Will do.



LEO:  Because it's true, these MacBooks tend to slow down over time.



STEVE:  Well, and the one that will work on the Mac will be the UEFI-based.



LEO:  I know, I'm excited.



STEVE:  Because Apple's earlier ones would boot on Macs, on older Macs.  So people who have Macs from like 2008 are able to run SpinRite on them.  It runs on older Macs, but not on new ones.



LEO:  Right, right, right.



STEVE:  Because Apple went pure UEFI.  And it will not run on ARM-based Macs, probably ever.



LEO:  Oh.



STEVE:  So, yeah.



LEO:  Well, that pretty much eliminates everything that's been sold in the last couple of years.



STEVE:  I mean, it is a PC-based maintenance tool.



LEO:  Yeah, it's Intel.  Yeah, yeah.



STEVE:  Yeah, yeah.



LEO:  Because, you know, you write in x86 ASM, there really...



STEVE:  Yea, I am.



LEO:  It's not a high-level language we're talking here.  Okay.  I've been looking forward all week, ever since this XZ vulnerability surfaced, to hear your telling because it's a story with so many layers.  So interesting.  Oh, I've got to turn your microphone on.  Otherwise we won't hear it.  Go ahead.



STEVE:  And it's got some cool technology, too.



LEO:  Yes, it does, yes.



STEVE:  So the runner-up title for today's podcast, which I decided, I settled on "A Cautionary Tale," was "A Close Call for Linux" because it was only due to some very small mistakes made by an otherwise very clever malicious developer that the scheme was discovered.  What was discovered was that by employing a diabolically circuitous route, the system SSH daemon, which is to say the SSH incoming connection accepting server for Linux, would have had a secret and invisible backdoor installed in it that would have then allowed someone, anyone, anywhere, using a specific RSA public key, to authenticate and login to any Linux system on the planet and also provide their own code for it to run.  So to say that this would have been huge hardly does it justice.



Okay, now, to be clear, unlike last week's coverage, I mean, again, I want to put this in the proper context.  Unlike last week's coverage of the GoFetch vulnerability, where Apple was informed 107 days before the public disclosure by a team of responsible researchers, in this case someone actually created this extremely complex and very well hidden backdoor.  And had it not been discovered by chance, due to some small errors made by this malefactor, it would have happened.  This would have happened.  The Linux distros would have been updated - some actually were - and refreshed, and they would have gradually moved out into the world to eventually become widespread.  And this code was so well and cleverly hidden that it might have remained unseen for months or years.



Fortunately, we'll never know.  I imagine it would have been eventually discovered, but who knows what damage might have been done before then.  And imagine the frenzied panic that would have ensued when it was discovered that all builds of Linux with publicly exposed SSH services contained a remotely accessible keyed backdoor.  That's a podcast I would have liked to deliver; but, fortunately, we dodged this one.  



So I went with the title "A Cautionary Tale" because the bigger takeaway lesson here should not be this specific instance, which I'll describe in further detail in a minute.  It should be that there's nothing about this that's necessarily a one-off.  This is the threat and the dark side of a massive community of developers working collectively and largely anonymously.  There's no question that, by far, nearly every developer is well meaning.  I'm certain that's the case.  But the asymmetry of the struggle for security, where we must get every single thing right every time to be secure, and only one mistake needs to be made once to introduce an insecurity, means that, unfortunately, the good guys will always be fighting an uphill battle.



What has just been discovered present in Linux demonstrates that the same asymmetric principle applies to large-scale software development, where just one bad seed, just one sufficiently clever malicious developer, can have an outsized effect upon the security of everything else.  Okay, now, I'm going to give everyone the TL;DR first because this is just so cool and so diabolically clever.



How do you go about hiding malicious code in a highly scrutinized open source environment which worships code in its source form, so that no one can see what you've done?  You focus your efforts upon a compression library project.  Compression library projects contain test files which are used to verify the still-proper behavior of recently modified and recompiled source code.  These compression test files are, and are expected to be, opaque binary blobs.  So you very cleverly arrange to place your malicious binary code into one of the supposedly compressed compression test files for that library, where no one would ever think to look.



I mean, again, one of the points here that I didn't put into the show notes is unfortunately, once something is seen to have been done, people who wouldn't have had this idea originally, you know, wouldn't have the original idea, they're like, oh.  That's interesting.  I wonder what mischief I can get up to?  So we may be seeing more of this in the future.



In their reporting of this, Ars Technica interviewed Will Dormann, who we've referred to before in the past.  He's a senior vulnerability analyst at the security firm Analygence, A-N-A-L-Y-G-E-N-C-E, Analygence.  Will noted that because the backdoor was discovered before the malicious versions of XZ Utils were added to production versions of Linux, he said:  "It's not really affecting anyone in the real world.  BUT [in all caps] that's only because it was discovered early due to bad actor sloppiness.  Had it not been discovered, it would have been catastrophic to the world."



Okay.  So all of this erupted last Friday, on Good Friday, with a posting to the oss-security list by the Microsoftie who stumbled upon this, Andres Freund.  As a consequence of this being a huge deal, security firms and developers all plowed into and reverse engineered all of the available evidence.  Now, I'm sure that Ars Technica felt last Friday, being a news outlet, that they needed to say something.  So Dan Goodin wrote what he could, given the at the time limited amount of available information.  But then, late Sunday morning, Dan published a second piece that did a far better job of pulling this entire escapade together and characterizing what's known, as well as some of the intriguing back story.  And actually Kevin Beaumont, who posts on Mastodon now from his GossiTheDog profile, also had something really great that I'll share.  Anyway, I've edited from Dan's second posting, so here's what we know thanks to Ars Technica and Dan's update.



He wrote:  "On Friday, a lone Microsoft developer rocked the world when he revealed a backdoor had been intentionally planted in XZ Utils, an open source data compression library available on almost all installations of Linux and other Unix-like operating systems.  The person or people behind this project likely spent years on it.  They were likely very close to seeing the backdoor update merged into Debian and Red Hat, the two biggest distributions of Linux, when an eagle-eyed software developer spotted something fishy.



"Software and crypto engineer Filippo Valsorda said of the effort, which came frightfully close to succeeding:  'This might be the best executed supply chain attack we've seen described in the open, and it's a nightmare scenario:  malicious, competent, authorized, upstream in a widely used library.'"



Dan writes:  "XZ Utils is nearly ubiquitous in Linux.  It provides lossless data compression on virtually all Unix-like operating systems, including Linux.  XZ Utils provides critical functions for compressing and decompressing data during all kinds of operations.  XZ Utils also supports the legacy .lzma format, making this component even more crucial.



"So what happened?  Andres Freund, a developer and engineer working on Microsoft's PostgreSQL offerings, was recently troubleshooting performance problems a Debian system was experiencing with SSH, the most widely used protocol for remotely logging into devices over the Internet.  Specifically, SSH logins were consuming too many CPU cycles and were generating errors with valgrind, a utility for monitoring computer memory.  Through sheer luck and Freund's careful eye, he eventually discovered the problems were the result of updates that had been made to XZ Utils.  On Friday, Freund took to the Open Source Security List to disclose that the updates were the result of someone intentionally planting a backdoor into the compression software.



"It's hard to overstate the complexity of the social engineering and the inner workings of the backdoor.  Thomas Roccia, a researcher at Microsoft, published a graphic on Mastodon that helps visualize the sprawling extent of the nearly successful endeavor to spread a backdoor with a reach that would have dwarfed the SolarWinds event of 2020."  And for anyone who's interested, I've reproduced the infographic on page 15 of the show notes. 



So what does the backdoor do?  "Malicious code added to XZ Utils versions 5.6.0 and 5.6.1 modified the way the software functions.  The backdoor manipulated sshd, the executable file used to make remote SSH connections.  Anyone in possession of a predetermined encryption key could stash any code of their choice in an SSH login certificate, upload it, and execute it on the backdoored device.  No one has actually seen code uploaded, so it's not known what code the attacker planned to run.  In theory, the code could allow for just about anything, including stealing encryption keys or installing malware."



Answering the question about how a compression utility can manipulate any process as security-sensitive as SSH, Dan explains:  "Any library can tamper with the inner workings of any executable it is linked to.  Often the developer of the executable will establish a link to a library that's needed for it to work properly.  OpenSSH, the most popular sshd implementation, does not link the liblzma library, but Debian and many other Linux distributions add a patch to link sshd to systemd, a program that loads a variety of services during the system boot-up.  And systemd, in turn, links to liblzma, and this allows XZ Utils to exert control over sshd."



Okay.  And then of course everyone wants to know how this all happened.  Dan writes - and this should give everyone some chills, and we'll get more into this with what Kevin wrote.  Dan said:  "It would appear that this backdoor was years in the making.  In 2021, someone with the username JiaT75" - and I'll just refer them as "Ji" - "made their first known commit to an open source project.  In retrospect, the change to the libarchive project is suspicious because it replaced the safe_fprint function with a variant that has long been recognized as less secure.  No one noticed at the time.



"The following year, JiaT75 submitted a patch over the XZ Utils mailing list; and, almost immediately, a never-before-seen participant named Jigar Kumar joined the discussion and argued that Lasse Collin, the longtime maintainer of XZ Utils, had not been updating the software often or fast enough.  Kumar, with the support of Dennis Ens" - also never before seen and never since - "and several other people who had never had a presence on the list and were never seen again, pressured Lasse Collin to bring on an additional developer to maintain the project.



"In January 2023, JiaT75 made their first commit to XZ Utils.  In the months following, JiaT75, who used the name Jia Tan, became increasingly involved in XZ Utils affairs.  For instance, Tan replaced Collin's contact information with their own on oss-fuzz, a project that scans open source software for vulnerabilities that can be exploited.  Tan also requested that oss-fuzz disable the ifunc function during testing, a change that prevented it from detecting the malicious changes Tan would soon make to XZ Utils."



Okay, now, I'll just note that that sort of request is not unusual in itself.  It's a bit like a website using the /robots.txt file to keep spiders out of places where they might cause some damage to the site or get themselves hopelessly lost and tangled up.  You know, it's possible that aggressive random fuzzing, which is hoping to detect unknown problems, might inadvertently trigger a known deliberate problem and behavior.  So asking for fuzzing exceptions is not inherently suspicious.  In this case, the perpetrator did have an ulterior motive, as it turned out.



So Dan finishes:  "In February of this year, Tan issued commits for versions 5.6.0 and 5.6.1 of XZ Utils.  The updates implemented the backdoor.  In the following weeks, Tan or others appealed to developers of Ubuntu, Red Hat, and Debian to merge the updates into their OSes.  Eventually, according to security firm Tenable, one of the two updates" - both of which had the backdoors - "made their way into Fedora Rawhide; Fedora 41; the Debian testing, unstable and experimental distros versions 5.5.1alpha-0.1 to 5.6.1-1; openSUSE, both Tumbleweed and openSUSE MicroOS; and Kali Linux."  So it was well on the way to going into wide distribution.



Also, additional reporting and research has found that Arch Linux, Alpine Linux, Gentoo, Slackware, PCLinuxOS, and others were also infected.  However, this was all recent enough that most of the affected distros were still in their experimental / unstable releases, so the malicious code did not make it into any widespread production systems.  The macOS Homebrew package manager and OpenWRT router firmware also included backdoored XZ Utils versions.



Some other reporting into the deeper back story and various motivations wrote:  "All of this started two years ago, in April 2022.  The attacker contributed code to XZ Utils, gradually built his reputation and trust over time, and used various sock puppet accounts to pressure XZ Utils project's author Lasse Collin to add them as one of the project's maintainers.  At the time, Collin was burnt out and dealing with mental health issues and welcomed the help since they were a single developer on a highly popular project."



And as I said, I want to share one more piece of writing about this, a wonderful narrative by, oh, it was by Michal Zalewski.  He's a Polish security expert, white hat hacker from Poland, a former Google employee.  We've quoted him in the past.  He's currently the VP of Security Engineering at Snap, Inc.  So here's Michal's take on this most recent near-catastrophic misadventure.



In his posting Sunday, which he titled "Techies vs Spies:  The XZ Backdoor Debate - diving into some of the dynamics and the interpretations of the brazen ploy to subvert the liblzma compression library," Michal wrote:  "Well, we just witnessed one of the most daring infosec capers of my career.  Here's what we know so far.  Some time ago, an unknown party evidently noticed that liblzma, aka XZ  a relatively obscure open source compression library  was a dependency of OpenSSH, a security-critical remote admin tool used to manage millions of servers around the world.  This dependency existed, not because of a deliberate design decision by the developers of OpenSSH, but because of a kludge added by some Linux distributions to integrate the tool with the operating system's newfangled orchestration service, systemd.



"Equipped with this knowledge about XZ, the aforementioned party probably invented the persona of Jia Tan, a developer with no prior online footprint who materialized out of the blue in October of 2021 and started making helpful contributions to the library.  Up to that point, XZ had a single maintainer, Lasse Collin, who was dealing with health issues and was falling behind.  Shortly after the arrival of Jia, several apparent sock puppet accounts showed up and started pressuring Lasse to pass the baton.  It seems that he relented at some point in 2023.



"Since then, Jia diligently contributed the maintenance work, culminating in February 2024 with the seamless introduction of a sophisticated, well-concealed backdoor tucked inside one of the build scripts.  Full analysis of the payload is still pending, but it appears to have targeted the pre-authentication crypto functions of OpenSSH.  It's probably safe to assume that it added 'master key' functionality to let the attackers access all affected servers at will."  And I'll note that's exactly what was, indeed, being discovered over the weekend while Michal was writing this.  The exploit provides for remote code execution with root privileges.



Michal continues:  "Some time after getting the backdoor in, Jia  along with a new cast of sock puppet accounts  started pinging Linux distro maintainers to have the backdoored library packaged and distributed to end users.  The scheme worked until Andres Freund, a PostgreSQL developer in the employ of Microsoft, reportedly decided to investigate some unexpected SSH latencies caused by a minor bug in the backdoor code.



"If," he says, "this entire exploit timeline is correct, it's not the modus operandi of a hobbyist.  In today's world, if you have the technical chops and the patience to pull this off, you can easily land a job that would set you for life without risking any prison time.  It's true that we also have some brilliant folks with sociopathic tendencies and poor impulse control; but almost by definition, such black hat groups seek instant gratification and don't plan major heists years in advance.  In other words, all signs point to this being a professional, for-pay operation, and it wouldn't be surprising if it was paid for by a state actor."



So as we might imagine, this has really shaken the entire Unix open source - I'm sorry, the entire Linux open source community because everyone understands just how close these malicious modifications came to being incorporated into all mainstream Linux distributions and then filtering out into the world.  Were it not for Andres Freund happening to wonder why SSH latencies were slightly higher than expected, in what was referred to by someone as a micro-benchmarking, you know, how much farther might this have gone.



Kevin Beaumont posted from his cyberplace.social Mastodon account.  He said:  "Before everyone high-fives each other, this is how the backdoor was found:  Somebody happened to look at why CPU usage had increased in sshd and did all the research and notification work themselves.  By this point, the backdoor had been there for a month unnoticed."  He said:  "I've made the joke before that, if GCHQ aren't introducing backdoors and vulns into open source, that I want a tax refund.  It wasn't a joke.  And it won't be just GCHQ doing it."



So all of this begs the question, what will be next?  And will someone catch that one, too, before it gets loose?  Lately, we appear to be dodging more bullets, which are coming more often.  In fact, our recent Episode 962 from February 20th was titled "The Internet Dodged a Bullet," when researchers stumbled over a way of bringing DNS to its knees.  If I hadn't used that title just six weeks ago, I probably would have used it today because we did, again, just dodge another bullet.



LEO:  Amazing.



STEVE:  Given the inherently precarious nature of security, and that we appear to be dodging more bullets recently, I won't be surprised if one comes along that we don't dodge in time.  I wonder what lessons we'll learn from that?  Fortunately, we'll be here.  Stay tuned.



LEO:  It's probably - I'm thinking it's not far off, in the next year.



STEVE:  That's my feeling, too, Leo.  That's why I say there is so much pressure now.



LEO:  And I think it's probably a nation-state that was going to use this in a targeted fashion against specific...



STEVE:  Oh, Leo, whoa.



LEO:  ...people that it wanted.  Not us, probably.  But doesn't mean...



STEVE:  You mean not you and me. 



LEO:  Right.  But, you know, dissidents, journalists, you know, people the state doesn't like.  And so it's not a general threat to all of us, but it is a serious threat.  And I do hope they - there just doesn't seem much will or maybe even ability to check  these repositories.



STEVE:  Well, and in this case, you know, the fact is the library did not need updating.  There was a ton of...



LEO:  That's a red flag, yeah.



STEVE:  ...of very old code.



LEO:  Right.  Yeah, sometimes it's fixed, it's done.



STEVE:  It's done.  It's finished.  It's like RTOS32 which I bought from that guy when he was going to take it off the market; you know?  He hadn't been able to sell licenses because there were no more bugs.



LEO:  Right.



STEVE:  And so...



LEO:  It was done.



STEVE:  ...it was like, oh, well, you know, no more revenue from fixes.  So I said thank you very much.



LEO:  The proud owner of his own operating system, ladies and gentlemen.  We're going to call it, I don't know, GDOS, Gibson DOS, something like that.  Steve Gibson.



STEVE:  Not GIBDOS.



LEO:  GIBDOS.



STEVE:  DOSGIB.



LEO:  DOSGIB.  He is at GRC.com, the Gibson Research Corporation.  That's where you'll find the latest version of SpinRite 6.1.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#969

DATE:		April 9, 2024

TITLE:		Minimum Viable Secure Product

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-969.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  When is it far better for a security researcher to just keep their mouth shut?  Are all Internet-based secure note exchanging sites created equal?  What's been happening in the lucrative and slimy world of zero-days for pay?  And what has NASA just learned about the state of Voyager 1?  Something momentous has happened with SpinRite, and we're going to take a deep dive into an important industry initiative that just acquired an important new contributor.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a great show planned for you.  Coming up we're going to say hello to V'Ger, Voyager 1.  There's some really interesting news in attempting to reestablish communications.  Why is it better sometimes for a security researcher to keep their mouth shut?  Steve has some strong words.  And we'll talk about the slimy world of zero-days for pay.  Turns out you can make a lot of money if your ethics are a little, you know, fuzzy.  All that and more, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 969, recorded Tuesday, April 9th, 2024:  Minimum Viable Secure Product.



It's time for Security Now!, oh, yes, once a week, a must-listen, Steve Gibson and the latest security news.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for another, well, we've got a bunch of really cool stuff to talk about.



LEO:  Oh, good.	



STEVE:  The podcast does not have a particularly catchy title.  I guess I would argue that the podcast that we titled "1" was probably...



LEO:  Was catchy, yeah.



STEVE:  That was catchy and pithy and all that.



LEO:  Pithy.



STEVE:  This one is Minimum Viable Secure Product.  Which I had fun with it, mostly because it is so dry.  But it's also very important.  So that's how we're going to - we're going to do a close look at an important industry initiative, which is what this is, titled unfortunately - you can just say MVSP.  That's, like, better, right, MVSP.  But it's Minimum Viable Secure Product, which just acquired an important new contributor.



But first we're going to look at when is it far better for a security researcher to just keep their mouth shut, and what happened in this case when he didn't?  Are all Internet-based secure note exchanging sites created equal?  What's been happening in the lucrative, if slimy, world of zero-days for pay?  And what has NASA just learned about the state of Voyager 1?  That little intrepid puppy just - oh, and by the way, Leo, I watched the documentary you referred to, "It's Quieter in the Twilight"?



LEO:  Oh, isn't that a great - all about V'Ger?  I loved that.



STEVE:  It was really, really fun, yes.  Also I will tell everybody that something momentous has happened with SpinRite, and then we'll take our deep dive into the fascinating - actually it is because it's going to make a lot of our listeners smile, Minimum Viable Secure Product, because it encapsulates the history of this podcast in many senses.  So I think - and of course a great Picture of the Week.  So for Podcast 969 for  April 9th, here we go.



LEO:  Love it.  Here we go.  969 is on the air.



STEVE:  We have a picture.  So I've had this one for a while.  I've mentioned having a bunch that were in the queue.  I got a kick out of this one because - so we have a one-story building, sort of it's like the side of maybe it would be a strip mall or something, where you have, you know, the need for roof access, but it's in sort of a public environment, so you just don't want random people climbing up on the roof; right?



LEO:  Climbing up, yeah.



STEVE:  Yet you still need, like when one of the AC units dies or something happens, you need service people to get up.  So you've got a ladder running up the side.  Now, okay.  How do you lock the ladder?



LEO:  [Laughing]



STEVE:  And you've just seen it.



LEO:  Yes.  Go ahead.  So this is clever, in a way.



STEVE:  Uh-huh.  So how do you lock the ladder?  You know?  Ladder's got rungs.  I know.  It's great.  So some clever individual figured, okay, we're going to put a hinged sheet metal panel, like across the front of this ladder.  And we're going to lock it with a padlock, and only the manager of the complex has the key for this padlock.



LEO:  Brilliant.  Brilliant.



STEVE:  And, yeah.  So, like, so basically it's completely closing off the rungs of the ladder.



LEO:  Yeah.



STEVE:  So, you know, you can't climb up a smooth surface.



LEO:  No.



STEVE:  It just, okay.



LEO:  Can you?  No.



STEVE:  Now, unfortunately, the ladder is mounted to the side of the building with a set of rungs.



LEO:  That are exactly ladder-like.



STEVE:  So, gee.  If I wanted to climb up on the roof, how would I do it?  Well, I'd just go up the side of the ladder where the rungs are mounted to the side of the building, until I get above the sheet metal sheet.  Then I can switch over and climb up the rest of the ladder.  So...



LEO:  Wow.  Well, they handily give you a ladder on the ladder, which is great.



STEVE:  Yeah.  We have here a failure of imagination.



LEO:  Or malicious compliance; you know?  The guy who installed it said, I know this is useless, but that's what they're paying me to put in, so...



STEVE:  This will serve you right, folks.



LEO:  Serves him right.



STEVE:  Yes.  Thank god it's got a padlock, that's all I can say.



LEO:  That's hysterical.  Oh my god.  



STEVE:  Wow.  Okay.  So what year is it, Leo?  It's 2024; right?



LEO:  2024, I believe, yes, sir, yes.



STEVE:  Okay.  It's still the case that publicly accessible, Internet-connected, high-profile devices are being found to have manufacturer hardcoded remote access credentials.  Now, I suppose that would be more difficult for us to believe if it wasn't so long ago, like it wasn't that long ago is what I meant, that Cisco's own internal audit of their devices, which was spurred - and we covered this on the podcast - by repeated discovery of their own hard-coded credentials in their equipment, kept turning up instance after instance of the same thing.  So like it's not shocking because, you know, here even the market leader at the high-end enterprise-grade networking gear was suffering from the inertia of the way they'd always done things before.  Even a decade after, it had really no longer been safe to do that.



Okay.  So it's taken some time.  But as we've been discussing more recently, we finally have - we're kind of coming to an agreement, and we're going to see more of this agreement here at the end of the podcast, on a set of straightforward and widely recognized best practices.  So I've just put out five of them here.



No default manufacturer-set credentials of any sort, anywhere.  Right?  The first time a device's firmware boots, it should see that its credentials are blank.  After it's had the chance to generate sufficient internal entropy for random number generation, it should create a very strong new default credential from scratch and present it to its user.  Now, the user will be free to then change it to whatever they want.  But by default, any newly installed device will give its own security a head start by defaulting to something strong.  And nothing from the manufacturer.



You know, yes, if it makes technical support more difficult, that's just tough.  Once upon a time it might have been fine to default the username and password to "admin" and "admin," and to print that on the quick start guide.  But those days are long past.  All of our listeners have seen those days.  Security is inherently porous, and the pressure against our porous security is now steadily on the rise.  That's the other thing that it is like so clear now.  Okay.  So first, nothing by default.



Second, and this was one of the new things that has surfaced recently, proposed by CISA, that I really think is right, and that is physical access required.  Changing the security of anything in any dangerous direction, like turning on UPnP, which should certainly not be on by default, including that administrative username and password we were just talking about, must be accompanied by a physical button press, either beforehand to enable admin access for some period of time, or afterward to confirm the application of the new settings into the current setup.



And yes, again, that will make fully remote admin impossible.  But it will also make fully remote admin by bad guys impossible.  Right?  And we've seen over and over again that it's not actually possible to have one without the other.  So yes, it'll be inconvenient.  Yes, you may have to call down to the basement and tell Morris to press the button now because you want to apply these settings from the comfort of your office on the 20th floor.  Fine.  Do what has to be done.  But it will keep the bad guys out.



Number three, only absolute minimum functions publicly exposed by default.  A router, for example, must have its WAN interface publicly exposed to be at all useful as a router.  But it does not need a single additional service beyond that.  So therefore not one additional service should be bound to that interface, the WAN interface, without explicit manual enabling accompanied by clear caution notices and, yes, pressing a physical button on the router.  And needless to say, when that service is instantiated, it will start off with a fully random and strong username and password.  You know, we know how to do this now.



Four, autonomous firmware updates received and applied by default.  All connected devices must ship from the factory with their auto-firmware-update system enabled by default.  Again, it could be turned off.  But it should, everything should default to most secure.  These devices should then periodically ping a factory server to check for the availability of new firmware.  The router's ping should include its current firmware version, and the cryptographically signed ping reply should provide the router with the current version and the urgency associated with moving from where it said it is to the latest today.  The device's admin can decide which levels of urgency are permitted to auto-update.  Maybe you want to back off and only allow the ultra super emergent updates to happen immediately.  Otherwise they might need management oversight, or they might  be deferred until 2:00 a.m., whatever.  So there could be administration over that, but the system ought to take care of itself.



And, last, firmware for devices must be maintained as long as they're in service.  And this is probably the trickiest of the five because you have to then solve the problem of how long should a supplier be reasonably responsible for making the firmware of an end-of-life and out-of-service device current?  You know, this is a decision that is reasonably up to each supplier.  But because it affects the buying decisions, whatever the supplier's decision may be, it must be clearly and publicly stated so that prospective purchasers can plan accordingly.



Okay.  So we have five fundamental principles:  No default credentials, physical access required to apply any configuration changes that might be dangerous, no unnecessary services enabled, auto-firmware updates enabled by default, and a clearly stated end-of-life ongoing support commitment.  So if you think back through all of the individual events, many of them having significantly catastrophic consequences that we've covered through this podcast's 19 years, almost every one of them could have been prevented if these five fundamental principles of safe Internet connectivity had been in place.



So today, as luck would have it, we have news of yet another:  A researcher discovered that D-Link Network Attached Storage, NAS models DNS-320L, 325, 327L, and 340L all share a distinguishing and, I don't know if it's horrifying or it's disappointing.  Anyway, he found that the firmware they share had been hard coded by D-Link with the same, fixed, remote access backdoor username and password.  And even the term "password" is being generous here because it's left blank.  His GitLab handle is NetSecFish, obviously as in "Network Security Fish," and his page on GitHub says:  "The described vulnerability affects multiple D-Link NAS devices," and then he enumerates those four.



"The vulnerability lies within the nas_sharing.cgi URL, which is vulnerable due to two main issues:  a backdoor facilitated by hard-coded credentials, and a command injection vulnerability via the 'system' parameter.  This exploitation could lead to arbitrary command execution on the affected D-Link NAS devices, granting attackers potential access to sensitive information, system configuration alteration, or denial of service by specifying a command."



Okay.  This guy's subsequent Internet search revealed 92,589 of these devices currently exposed on the public Internet.



LEO:  Oh, man.



STEVE:  Yes, 92,589, right now on the Internet, with the highest concentration appearing at IP addresses in the U.S.  The big problem is that this family of D-Link NAS devices were first phased out of D-Link's product line on October 29th, 2017.  And two years later, on the same day in October 2019, all support for them was terminated by D-Link.  Even so, it appears that our Network Security Fish gave D-Link no prior notice of his discovery before he published it, no opportunity to decide whether they wished to alter their standing policy for out-of-service-life products.  Two weeks ago, on March 26th, a date which I'm fond of because it's the day I was born...



LEO:  And Leonard Nimoy.



STEVE:  Leonard Nimoy and I.  He simply published everything he knew about this.  Today, his public GitHub page thoroughly documents - today and for the last week, or the last two weeks, publically documents how a simple HTTP GET query, containing the username "messagebus" and a blank password, can be used to cause any of these 92,589 currently online network attached storage devices to execute arbitrary commands.  We see from the packet capture he enclosed that he issued - I have it in the show notes.  He issued a command to a machine at a redacted address - and even doing that would be controversial unless the machine was one he owned, which seems unlikely - a command to which the machine replied with a valid HTTP 200 OK response.  It included other reply headers and XML in the body of its reply.  So from that we learn that the D-Link NAS devices are running the "lighttpd" web server v1.4.28.



Now, you know, I'm unhappy with what D-Link has been found to have done.  We'll get to them in a minute.  But it's not clear to me that anyone is helped by Fish's public disclosure, and that 92,589 people and their networks and their data stand to be attacked and compromised as a result.  You know, before he made this needless public disclosure, it might have been that someone might have eventually stumbled onto this, too.  And it's true that we don't know that it hasn't happened already.  But we do know that it was never public before, and that now it is.



And another lesson we've learned through the years of shared experience on this podcast is that the height from which the fruit is hanging matters a lot.  Far more low-hanging fruit is plucked than high-hanging fruit.  There is an endless supply of script kiddies able to issue a series of simple HTTP GET queries.  And how are any of them going to be able to resist any target as tempting as this?  This is the definition of a script kiddie-class vulnerability.  So this person's disclosure two weeks ago has, without any question, compromised the security of 92,589 still online, still functioning, still in use, and still sitting on other people's networks, D-Link network attached storage devices.



At the same time, a fair question is "What would Google do?"  And by "Google," I'm referring to Google's Project Zero with their famous 90-day disclosure policy.  Now, in fairness, he didn't give D-Link even a one-day.  This was a zero-day disclosure.  But in the case of Google with Project Zero, you know, they give manufacturers 90 days to produce a patch for a discovered flaw before Google will release the flaw's technical details.  And as we know, many companies have felt significant pressure to fix a vulnerability that Project Zero had found in their products.



But in this instance, the answer to the question "what Google would do" is that I'm not 100% sure.  For one thing, these are long-out-of-service devices that are no longer being maintained by their supplier.  And we know that even Google has, like, known vulnerabilities in Android that is no longer being maintained on Android smartphones.  And it's like, well, sorry, you know, we're not patching that version of Android any longer.  So that's a policy.  But the other thing is this is not actually a bug.  This is an undocumented deliberate backdoor that D-Link embedded into their devices, their consumer network NAS devices, for some unclear purpose.  So because it's old, out of service, and not a bug, I'm pretty certain that Google's Project Zero would never even consider taking this up in the first place.  And if they knew about it, they would just be mum.



So this brings us back to D-Link, a well-known, reputable, popular, Taiwanese network hardware manufacturer that's been around since 1986.  I've purchased a bunch of D-Link equipment, mostly network hubs, I think, through the years.  So I suppose that at least among those of us here and others who learn of this past behavior, this at least tarnishes their brand in our mind.  Right?  I'm like, I'm not buying a D-Link NAS now when I know that for some reason they've got this undocumented backdoor.  They did.  We don't know about them today.  



So with devices that were discontinued six and a half years ago, that went out of support four and a half years ago, we would have to assume that they have, even if they wanted to, no way of remotely updating their firmware.  You know, that's like a thing that we're beginning to see now on new devices because it's been determined that that's the best practice.  If the owners of those devices had registered them, well, then, D-Link could conceivably today reach out to them.  But what would they say?  "Uh, you know that NAS that you purchased from us 10 years ago?  Well, uh, we had planted a secret remote access backdoor into it, and it was recently discovered and has now been made public.  So, you know, we're really sorry about that, and you should probably definitely immediately disconnect your D-Link device from the Internet.  Have a nice day.  And would you like to consider buying a newer NAS from us?  We have a bunch of nice shiny ones for you to consider."  Well, right.  So they're not going to say anything.



And I don't know that they're even legally vulnerable here.  So it's not clear that anything could be done on the legal front.  This was a design decision made by D-Link.  Bad, in retrospect, but there it is.  And they may have some justifiable rationale for it.  The username "messagebus" kind of suggests that perhaps these devices could be set up in a cluster, where this "messagebus" allowed them to interoperate in some fashion.  So, you know, should have never been put on the public Internet.  Maybe that was a mistake.  Maybe it's only meant to be on the LAN, and somehow that wasn't done right.  In other words, bad design not to control who could use it on which interface, but it wasn't an obvious crime on D-Link's part.



And speaking of crime, here it comes.  Yesterday, entirely predictably, Ars Technica's headline read "Critical takeover vulnerabilities in 92,000 D-Link devices under active exploitation."  Now, that didn't take long.  Ars wrote:  "On Monday," meaning yesterday, "researchers said their sensors began detecting active attempts to exploit the vulnerabilities starting over the weekend.  GreyNoise, one of the organizations reporting the in-the-wild exploitation, said in an email that the activity began around 02:17 UTC on Sunday.



"The attacks attempted to download and install one of several pieces of malware on vulnerable devices depending on their specific hardware profile.  One such piece of malware is flagged under various names by 40" - four zero - "endpoint protection services.  "Security organization Shadowserver has also reported seeing scanning or exploits from multiple IP addresses, but did not provide additional details.



"The vulnerability pair, found in the nas_sharing.cgi programming interface of the vulnerable devices, provide an ideal recipe for remote takeover.  The first, tracked as CVE-2024-3272 and carrying a severity rating of 9.8 out of 10, is a backdoor account enabled by credentials hardcoded into the firmware.  The second is a command-injection flaw tracked as CVE-2024-3273 with a severity rating of 7.3.  It can be remotely activated with a simple HTTP GET request.  NetSecFish, the researcher who disclosed the vulnerabilities, demonstrated how a hacker could remotely commandeer vulnerable devices by sending a simple set of HTTP requests to them."



So thank you and congratulations, Network Security Fish.  What a nice public service you have performed for 92,589 D-Link users whom you've just single-handedly turned into victims.



LEO:  Yikes.  Yikes, yikes, yikes.  That's bad news.



STEVE:  It is.  Before long, if not already, those 92,589 D-Link devices will be infected with malware, if they aren't all already.  After all, today's Tuesday.  And as I mentioned at the start of this adventure, the Internet scan distribution shows that networks in the United States contain more of them than any other single region.  I wonder whose networks those devices may be sitting on, and I wonder who might be interested in finding out.



So I suppose the final takeaway lesson for us is that complex devices of this sort that are no longer being actively supported cannot be used safely, at least not in settings such as connected to the Internet, where the security risk is high.  Of course, that's easily said; right?  It's difficult to retire a perfectly good working device for no obvious reason other than that its manufacturer is no longer active supporting it.  A mature policy would ideally rotate such devices into less security-sensitive roles.  Give them a place inside the network, behind the firewalls, where they can live out their lives in peace while continuing to be productive.  I very much hope that the 92,589 owners of these surviving NAS devices do not experience much hardship as a consequence of Network Security Fish's needless, pointless, and destructive disclosure.  It should be clear that there are times when it's far better to just say nothing.



LEO:  Wow.



STEVE:  You know...



LEO:  It's like he's showing off, really; right?



STEVE:  Yes, that is all it - as there's no other justification, Leo.  Nobody's learning a lesson.  D-Link can't do anything about this.  They won't do anything about it.  It is, it is just hubris.  It is just ego.  It is just, hey, look what I did.  Look what I found.  But he could have said this without, I mean, well, better to say nothing; right?  But, you know, he gave a complete, here's how you attack 92,000-plus open, wide-open, innocent devices that are never going to be patched.  It's just like, hey, here, take them over.



LEO:  I guess...



STEVE:  Install malware, you know, create a botnet.  See whose network they're on.  Pivot.  Because who knows, some enterprise, some IT guy could have said, hey, I have one of these at home.  Works great.  Let's buy one for the company.  And so it's on the enterprise's network.  Someone's going to get in, pivot, now have access to their LAN from this device that is straddling the LAN and the WAN and you know, install ransomware on their network.  Why?  Because some guy says, hey, look what I found. 



LEO:  It could have been worse.  He could have sold it to Zerodium.  Right?



STEVE:  True.



LEO:  I mean, at least he didn't sell it to a nation-state.  Not that they would have paid much for it.  But still...



STEVE:  No.  Although I'm not sure that this isn't worse because now it's a feeding frenzy.



LEO:  It's free, yeah.  He's given it away.



STEVE:  Well, it's every script kiddie who ever, you know, had a wget command, or what's the other one, the famous Linux...



LEO:  cURL.



STEVE:  cURL, cURL, of course.  It's like, wow.



LEO:  Yeah.



STEVE:  So Leo, on that happy note...



LEO:  Yeah, thanks for cheering me up.



STEVE:  Why we're here, and we're going to find out why you don't want to use a private note-sharing site.



LEO:  Oh, interesting.  Yeah, the only D-Link thing I - I used to buy a lot of switches.  I think I might have some still rolling around, but those are passive.  They're not...



STEVE:  Yeah, exactly.  I think they had really nice-looking little network...



LEO:  Yeah, they were nice metal hubs and switches.



STEVE:  Yup.



LEO:  And I think I own a D-Link cable modem.  I hope that - that's such a bad practice, to hardwire a backdoor.



STEVE:  Oh, Leo.  It's just...



LEO:  It's just depressing.



STEVE:  I mean, at some point we need legislation where...



LEO:  Yeah, you're responsible.  Just you're responsible.



STEVE:  Yes, exactly, exactly.  If something happens because of what you did, you are now liable, you know, you're on the receiving side of lawsuits because that's just - you just can't do it.



So get a load of this one.  Last Thursday, Brian Krebs of krebsonsecurity.com fame, who really likes doing deep security research, posted a piece that just makes you shake your head.  He wrote:  "A cybercrook who has been setting up websites that mimic the self-destructing message service Privnote.com" - you know, as in obviously privacy - "accidentally exposed the breadth of their operations when they threatened to sue a software company."  Wow.  This is chutzpah.  "The disclosure revealed a profitable network of phishing sites that behave and look like the real Privnote."



LEO:  Oh, no.



STEVE:  Except that, get this...



LEO:  Not so Privnotes.



STEVE:  Unh-unh.  "Any messages containing cryptocurrency addresses will be automatically altered with a different payment address...



LEO:  Brilliant.



STEVE:  It is.  It is diabolical.  "...controlled by the scammers."  So Brian explains, he says:  "Launched in 2008, Privnote.com employs technology that encrypts each message so that even Privnote itself cannot read its contents.  And it doesn't send or receive messages.  Creating a message merely generates a link.  When that link is clicked or visited, the service warns that the message will be gone forever after it is read.



"Privnote's ease-of-use and popularity among cryptocurrency enthusiasts has made it a perennial target of phishers" - who could have seen that coming? - "who erect Privnote clones that function more or less as advertised, but also quietly replace their own cryptocurrency payment addresses when a note is created that contains crypto wallet addresses.  Last month, a new user on GitHub named Fory66399 lodged a complaint on the 'issues' page for MetaMask, a software cryptocurrency wallet used to interact with the Ethereum blockchain.  Fory66399 insisted that their website  Privnote.co  was being wrongly flagged by MetaMask's 'eth-phishing-detect' list as malicious."  Which of course it was.



"Fory66399 wrote," with their arms crossed:  "We filed a lawsuit with a lawyer for dishonestly adding a site to the block list, damaging reputation, as well as ignoring the moderation department and ignoring answers.  Provide evidence, or I will demand compensation."  So MetaMask's lead product manager Taylor Monahan replied by posting several screenshots of Privnote.co showing that the site did indeed swap out any cryptocurrency addresses.



LEO:  Oh.  Oh.



STEVE:  After being told where they could send a copy of their lawsuit, Fory66399 appeared to become flustered.  And went silent.



Now, Brian's piece continues with one of his terrific deep dive researches into all the details, and he uncovers a large network of very similar clone websites by backtracking their domain registrations.  What I found interesting about this was that this is not hacking some fancy new blockchain technology contract thing that like nobody understands, but to steal like a windfall of $50 million all at once.  No.  Instead, this is stealing individual small cryptocurrency transactions from cryptocurrency end users.  And you can imagine the dialogue; right?  You know, "I haven't received the drugs I sent you the money for.  What do you mean, you never received the payment?  I sent it right after I received the email with your wallet address, and the money was taken from my wallet.  If you didn't get it, then where did it go?"



LEO:  Who did?



STEVE:  That's right.



LEO:  Maybe Fory got it.



STEVE:  Uh-huh.  Well, gee, it appears likely that it may have made its way into some Russian's pocket, perhaps because you were not paying close attention and used Privnote.co or Privnota.com or Privatemessage.net or Privatenote.io or Tornote.io or Privnode.com or Privnate.com or Prevnote.com.  Believe it or not, Brian's research traced each of those Privnote.com malicious copycat domains back to a handful of apparent Russians who had been enjoying fleecing the corrupt Western capitalists.



So we know that it's entirely possible to create a simple website that encrypts a visitor's note on their PC so that it cannot be decrypted except by another third party.  The problem is, the technology required to do this is not readily visible and auditable by a site's user.  Nor would they understand the crypto code, even if it was visible.  The site clearly and cleanly claims that they're unable to read anything that's being sent.  And the unwitting user has heard of such sites, like Privnote.com, that's authentic, you know, and that that site arranged to do just that.  So it's got a terrific reputation.  And Privnote.io, well, it looks the same, so it's probably just the same people who also got that cool .io domain, and it's one fewer characters to type.  Might as well use Privnote.io instead of Privnote.com.  What could possibly go wrong?  You know, and, after all, Privnote.io, it says that it cannot read anything that's sent.  So let's just copy and paste our wallet address into it.  Right.  Good luck with that.



We talked a lot about Zerodium in the past.  They're the folks who offer extremely large bounties for new and unknown zero-click vulnerabilities.  And unlike the good guys at HackerOne or Zero Day, these creeps sell these zero-days, doubtless at significant profit, to unknown but certainly big-time buyers such as Israel's NSO Group for use by the Pegasus spyware and almost certainly to governments and intelligence services around the world.  In other words, the platform publishers such as Apple and Google are the last to learn of these exploits.



Well now, on Saturday, TechCrunch brings us news of a newcomer named "Crowdfense" - maybe they're fencing, I guess they're fencing the illegal, the ill-gotten goods of a zero-day.  And Crowdfense intends to give Zerodium a run for its money.  And, you know, this is really not a market where we'd like to see competition flourishing.  We'd prefer that the market didn't exist at all.



I have a screenshot in the show notes of Crowdfense's current offering lineup.  At the top of the heap they've got SMS and MMS full-chain zero-click compromises, the discovery and disclosure to them of which would net someone selling it somewhere between 7 and 9 million USD.  I mean, basically, you find one, and if you check your ethics at the door, you're done for life.  Right?  I mean, you could probably survive on $9 million.  You know, I mean, actually just off the interest.



Okay.  So in this case "full-chain" means something that gets the entire job done, not just an "Oh, look, it crashed," but "Oh, look, we now have root access to do whatever we want" sort of thing.  A full-chain zero-click for Android brings in 5 million, whereas the same thing for iOS is priced at between 5 and 7.  And these are all just within the top paying Mobile Platform category.  Crowdfense is also interested in mobile apps, other mobile things, desktop, virtualization, baseband, meaning, you know, the radio that underlies our smartphones, enterprise, web apps, and more.



TechCrunch's headline was "Price of zero-day exploits rises as companies harden products against hackers."  Okay, well, that's good.  With the subheading "A startup is now offering millions of dollars for tools to hack iPhones, Android devices, WhatsApp, and iMessage."  TechCrunch writes:  "Tools that allow government hackers to break into iPhones and Android phones, popular software like the Chrome and Safari browsers, and chat apps like WhatsApp and iMessage, are now worth millions of dollars  and their price has multiplied in the last few years as these products get harder to hack.



"On Monday, startup Crowdfense published its updated price list for these hacking tools, which are commonly known as 'zero-days' because they rely on unpatched vulnerabilities in software that are unknown to the makers of that software.  Companies like Crowdfense and one of its competitors, Zerodium, claim to acquire these zero-days with the goal of reselling them to other organizations, usually government agencies or government contractors, which claim they need the hacking tools to track or spy on criminals."  And of course we have lots of evidence we've discussed through the years on this podcast that, you know, politicians and political activists and enemies of powerful people, whoever, end up getting spied on, not just intelligence services tracking known bad guys.



"Crowdfense," they write, "is now offering between 5 and 7 million for zero-days to break into iPhones; 5 million for Android; 3 million and 3.5 for Chrome and Safari zero-days, respectively; and 3 to 5 million for WhatsApp and iMessage zero-days.  So the increase in prices comes as companies like Apple, Google, and Microsoft are making it harder to hack their devices and apps, which means their users are better protected."



Okay.  So in other words, of course, as zero-days become more rare, they naturally become more valuable.  It's good news for everyone that they are becoming more rare.  TechCrunch continues:  "Dustin Childs, the head of threat awareness at Trend Micro's Zero-Day Initiative said:  'It should be harder year over year to exploit whatever software we're using, whatever devices we're using.'  Unlike Crowdfense and Zerodium, ZDI pays researchers to acquire zero-days" - on the other hand, not 7 or $9 million - "then reports them to the companies affected with the goal of getting the vulnerabilities fixed."  So those are the good guys, where you get to keep your ethics with you while you accept some good money, but not enough so that you never have to do anything and can retire on a beach.



"Shane Huntley, the head of Google's Tag Team" - their TAG, Threat Analysis Group - "tracks hackers and the use of zero-days.  He said:  'As more zero-day vulnerabilities are discovered by threat intelligence teams like Google's, and platform protections continue to improve, the time and effort required from attackers increases, resulting in an increase in the cost for their findings.'



"In a report last month, Google said that last year in 2023 it saw hackers use a total of" - here it comes - "97 zero-day vulnerabilities in the wild."  That was last year, in all of 2023, 97 zero-day vulnerabilities.  "And the various spyware vendors, like the NSO Group, which often work with zero-day brokers, were responsible for three quarters of all zero-days targeting Google products and Android.  People in and around the zero-day industry agree that the job of exploiting vulnerabilities is getting more difficult.



"David Manouchehri, a security analyst with knowledge of the zero-day market, said that 'hard targets like Google's Pixel and iPhone have been becoming harder to hack every year.'  He said:  'I expect the cost to continue to increase significantly over time.'"



"Paolo Stagno, the director of research at Crowdfense" - the new bad guys - "told TechCrunch:  'The mitigations that vendors are implementing are working, and it's leading the whole trade to become much more complicated, much more time-consuming, and so clearly this is then reflected in the price.'"



The first time I read that I thought, trade?  What trade?  Then I realized that they're calling this zero-day vulnerability finding and selling "a trade."  And I suppose it is, though it feels like ransomware gangs talking about their "profit."  Profit?



LEO:  Yeah.  No, no, it's just trade.



STEVE:  How about theft through extortion?  You know, how is that profit?  But I suppose that it is, sadly, profitable; though it hardly seems earned.  Anyway, the Stagno guy from Crowdfense explained:  "In 2015 and/or '16, it was possible for only one researcher to find one or more zero-days and develop them into a full-fledged exploit targeting iPhones or Androids."  He says:  "Now 'this is almost impossible' as it requires a team of several researchers, which also causes prices to go up.  Crowdfense currently offers the highest publicly known prices to-date outside of Russia, where a company called Operation Zero announced last year that they was willing to pay up to 20 million for tools to hack iPhones and Android devices.  The prices in Russia, however, may be inflated because of the war in Ukraine and the subsequent sanctions, which could discourage or outright prevent people from dealing with a Russian company.  Outside of the public view, it's possible that governments and companies are paying even higher prices.



"David Manouchehri previously worked at Linchpin Labs, a startup that focused on developing and selling zero-days."  And again, there's no way that selling zero-days is ethical and cool.  Linchpin Labs, unfortunately, was acquired by U.S. defense contractor L3 Technologies, now known as L3Harris, in 2018."  And that's encouraging.  Anyway, David said:  "The prices Crowdfense is offering researchers for individual Chrome Remote Code Execution and Sandbox Escapes are below market rate from what I've seen in the zero-day industry."



LEO:  What?  What?  Wow.



STEVE:  Wow.  A low market rate.



LEO:  Holy cow.  That seemed like a good price to me.



STEVE:  Wow.  "Alfonso de Gregorio, the founder of Zeronomicon, an Italy-based startup that also acquires zero-days" - so, you know, they're scattered around - "agreed with this, telling TechCrunch that prices could certainly be higher."  Well, I guess he wants them to stay low because that's the price he has to pay researchers or hackers who find them.  I really don't want to call them "researchers."  "Zero-days," TechCrunch writes, "have been used in court-approved law enforcement operations.  In 2016, the FBI used a zero-day" - we know where this is going - "provided by a startup called Azimuth to break into the iPhone of one of the shooters who killed 14 people in San Bernardino, according to The Washington Post.  In 2020, Motherboard revealed that the FBI  with the help of Facebook and an unnamed third-party company  used a zero-day to track down a man who was later convicted for harassing and extorting young girls online.



"There have also been several cases where zero-days and spyware have allegedly been used to target human rights dissidents and journalists in Ethiopia, Morocco, Saudi Arabia, and the UAE, among other countries with poor human rights records.  There have also been similar cases of alleged abuse in democratic countries like Greece, Mexico, Poland, and Spain.  Neither Crowdfense, Zerodium, or Zeronomicon have ever been accused of being involved in similar cases."  On the other hand, remember, they're one party removed.  They're not the guys who are doing the exploiting of these exploits.  They're selling to entities which are then turning around and doing its abuse.  So, yeah, these resellers would not be in the loop.



They said:  "Zero-day brokers, as well as spyware companies like NSO Group and Hacking Team, have often been criticized for selling their products to unsavory governments.  In response, some of them now pledge to respect export controls in an effort to limit potential abuses from their customers.



"Stagno said that Crowdfense follows the embargoes and sanctions imposed by the United States, even if the company is based in the UAE.  For example, Stagno said that the company would not sell to Afghanistan, Belarus, Cuba, Iran, Iraq, North Korea, Russia, South Sudan, Sudan, and Syria  all on the U.S. sanctions lists."



LEO:  That's a really long list.  Wow.



STEVE:  It is.  And he said:  "Everything the U.S. does, we are on the ball."



LEO:  You know why?  I bet we're one of their biggest customers.



STEVE:  Uh...



LEO:  Yup.



STEVE:  Uh-huh.



LEO:  We don't want to get them mad.



STEVE:  I would bet that the NSA is probably taking our taxpayer  money and buying these exploits so that they can do things with it.  I'll bet you're right, Leo.  He said if an existing customer gets on the U.S. sanctions list, Crowdfense would abandon it.



LEO:  Mm-hmm.



STEVE:  "All the companies and governments directly sanctioned by the USA are excluded."



LEO:  Of course they are.



STEVE:  Uh-huh.  "At least one company, spyware consortium Intellexa, is on Crowdfense's particular blocklist.  Of Intellexa, Stagno said:  'I can't tell you whether it has been a customer of ours and whether it has stopped being one.  However, as far as I am concerned, now at this moment Intellexa could not be a customer of ours.'



"In March, the U.S. government announced sanctions against Intellexa's founder Tal Dilian, as well as a business associate of his, the first time the government imposed sanctions on individuals imposed in the spyware industry.  Intellexa and its partner company Cytrox is also sanctioned by the U.S., making it harder for the companies, as well as the people running it, to continue doing business.



"Intellexa's spyware has been reported to have been used against U.S. Congressman Michael McCaul, U.S. Senator John Hoeven, and the president of the European Parliament, Roberta Metsola, among others."  And finally:  "De Gregorio, the founder of Zeronomicon, declined to say who the company sells to.  On its site, the company has published a code of business ethics."



LEO:  Oh, well.  Okay.



STEVE:  These guys have business ethics, Leo.



LEO:  They're so ethical.



STEVE:  I'm wondering how many sentences or how many words in their ethics statement, "which includes vetting customers with the goal of avoiding doing business 'with entities known for abusing human rights,' and respecting export controls."



Now, reading about these so-called export controls, one has to wonder how difficult it would be for any major country on the U.S. sanctions list to establish a behind-the-scenes relationship with another company in a non-sanctioned region to use as a middleman.



In any event, I thought that checking in on the state of the zero-day market would be useful.  While it may not be good news that prices are increasing, since that significantly increases incentives to find the fewer and fewer remaining zero-days that exist, the fact that prices are rising because these remaining zero-days are becoming ever more scarce, well, that's certainly good news.



I have one bit of miscellany which is happy.  Last Thursday, NASA updated the world with the news of the status of the our intrepid Voyager 1 spacecraft.  The headline of their posting was "Engineers Pinpoint Cause of Voyager 1 Issue, Are Working on Solution."  They explained:  "Engineers have confirmed that a small portion of corrupted memory in one of the computers aboard NASA's Voyager 1 has been causing the spacecraft to send unreadable science and engineering data to Earth since last November.  Called the Flight Data Subsystem (FDS), the computer is responsible for packaging the probe's science and engineering data before the telemetry modulation unit (TMU) and radio transmitter send the data to Earth.



"In early March, the team issued a 'poke' command to prompt the spacecraft to send back a readout of the FDS memory."



LEO:  Is that a Facebook poke command?



STEVE:  They've poked it.  That's similar to a Like, but it dates from 1971, so it's not quite the same.



LEO:  Yeah, that's right, the original poke.



STEVE:  That's right, "which includes the company's software code as well as variables are all being sent back, the computer's software code as well as variables.  Using the readout that they received, the team has confirmed that about 3% of the FDS memory has been corrupted, preventing the computer from carrying out normal operations.



LEO:  That's fascinating, huh.



STEVE:  "The team suspects that a single chip responsible for storing part of the affected portion of the FDS memory is not working.  Engineers cannot determine with certainty what caused the issue.  Two possibilities are that the chip could have been hit by an energetic particle from space, or that it simply may have worn out over 46 years."  Just like the rest of us.



LEO:  I understand that.  I get that.  I can understand that completely.



STEVE:  "Although it may take weeks or months, engineers are optimistic they can find a way for the FDS to operate normally again without the unusable memory hardware, which would enable Voyager 1 to begin returning science and engineering data."



LEO:  What a miraculous story.  Unbelievable.



STEVE:  Leo, it is astonishing.



LEO:  Isn't it.



STEVE:  And when you consider, what is it, is it billions or millions of miles away?  It is astonishingly far away.  How can that thing be pointing at Earth?  I mean, talk about, I mean, it's a fraction of a degree at that distance for its antenna, a directional dish, to still be perfectly aligned.  To me, that's what is stunning.



LEO:  Amazing.  And they don't have enough power to send a broad beam.  It's got to be a fairly tight beam.  So, amazing.



STEVE:  Right.  And, I mean, the power is dropping as we've covered through the years.  They're using a radioisotope-based system.  Basically, the decaying radioisotopes are heating a thermocouple, which is generating the power to drive this stuff.  And they've had over time, as less and less radiation is being produced because it's just, you know, it's just winding down, the power produced has diminished, so they've been having to judiciously turn off successive instruments of their total instrument package because they're looking at the total number of watts being generated.  It's just astonishing.



LEO:  It is 15 billion miles from Earth,



STEVE:  That's insane.



LEO:  It is incredible.



STEVE:  Insane.



LEO:  Forty-six years, seven months, four days, nine hours, four minutes, and 23 seconds.  15 billion.  That is insane.  I almost feel like that can't be right.



STEVE:  I know.  And how can it still be pointing with enough accuracy at us?  That's mind-blowing to me.



LEO:  Twenty-two hours, 22.6 hours light distance away.  That's - how did it happen?



STEVE:  Oh, and there you just scrolled by the instruments which are now on and off on both of the devices. 



LEO:  Oh, yes, yeah.  So one is the first column.  Plasma science is off.  Imaging science is off.  Infrared interferometer spectrometer is off.  Quite a bit.  But there's still science projects still on, cosmic rays, low-energy charged particles, and magnetometer.  Wow.  Oh, and plasma web subsystem. 



STEVE:  Yeah.



LEO:  Still on.



STEVE:  And actually we learned that when Voyager 1 passed out of the Heliosphere, the models which cosmologists and astronomers had made were found to be wrong.  So it was never expected to be anything other than a fly by some of our planets to take some pictures.  But it just wouldn't die.  So it kept on going, and it had so much useful instrumentation on it that they're learning new things...



LEO:  Incredible.



STEVE:  ...which are still valuable.



LEO:  Patrick said, wait a minute, it's getting closer to the Earth?  Yeah, because the Earth's rotating in its direction right now.  The distance from the Sun is going up.  It's 15.1 billion miles from the sun, or 163 astronomical units.



STEVE:  So that means that it's moving away from the sun more slowly than the Earth is currently moving around and thus toward it at the moment.



LEO:  Right.  It's gaining.  And then of course as the Earth processes, it will go the other way.



STEVE:  Right.



LEO:  And it'll gain faster.



STEVE:  And there's only now one radio telescope in Australia which is able to talk to it.  And unfortunately it's tasked with doing lots of other things.  So they need to like steal a little bit of time at the right moment when they're able to send a burst of instructions to it.  And then they wait 44 hours, 22 for it to go out and 22 - I mean, and Leo, the other thing, think of the science we had in '71.



LEO:  Oh, yeah.



STEVE:  That's when I had my first car that had an empty engine compartment because there was an engine and a gas line going to a carburetor.  I mean, there was no - and you had a throttle.  I had to pull the throttle to start the engine when it was cold.



LEO:  Oh, the choke, yeah, yeah, yeah.



STEVE:  Yeah, the choke, the choke, not the throttle.  Right, the choke.  And that was the world in '71, were Wozniak was saying, Steve, I think I can get rid of one more Apple computer because, you know, we need the space.



LEO:  Look, that's where it is, way the hell out there.  Way beyond anything else.  Farthest manmade object in the world.  In the universe.  Unless there's other men elsewhere.  But we don't know that, so.



STEVE:  I don't know where that Tesla coupe is at the moment.



LEO:  A lot closer, I can promise you.



STEVE:  I think so.



LEO:  Wow.  Just a great story.  It's such a great story.



STEVE:  And again, 1971 technology, it's just astonishing.



LEO:  But think, I mean, in 1969 we landed on the moon, two years before that.  And we're having a hard, a devil of a time doing it again.  So maybe those guys back in the '60s knew something.



STEVE:  Yeah, the bits were bigger, and so they were more robust back then.  What you want is bigger bits, really.



LEO:  Yeah, our bits have gotten way too small.



STEVE:  Speaking of bits, I was tempted to name this podcast "SpinRite 6.1" because what happened Sunday afternoon means so much to me.  But since it doesn't mean that much to the rest of our listeners, that didn't seem appropriate.  What happened on Sunday is that I finally updated GRC for the first time ever to begin offering 6.1 as its official SpinRite.  So 6.1 is finally what new purchasers will receive when they go to GRC for the first time.  So, you know, it's huge for me.  I've been living with my commitment to offer - and I don't have the website all...



LEO:  Oh, yes.  So it's still 6.0.



STEVE:  So if you go to the top, Leo - I know, I've just got this done.  If you click on the menu under SpinRite and the top left...



LEO:  Okay.



STEVE:  There's a little - yup.



LEO:  Upgrade to 6.1, okay.



STEVE:  Yup.  Or if you just click on Purchase SpinRite, go down a few, and then you'll see...



LEO:  There it is.  Woohoo!



STEVE:  ...that it is 6.1 is what we are offering.



LEO:  That's pretty cool, Steve.  By the way, this website looks like it came from 1971.  Congratulations.



STEVE:  I came from 1971.



LEO:  But the bits are bigger here.  They really are.



STEVE:  That's right, the bits are bigger, baby.



LEO:  Is the website written in assembly?  Tell the truth.



STEVE:  It's all hand-coded.  I wrote it in HTML.



LEO:  Nice.  Nice.



STEVE:  Before CSS even.



LEO:  Yeah, yeah, yeah.



STEVE:  And I do look at the code, and it hurts, actually.  And I've had a number of our listeners who've said, Steve, Steve, Steve.



LEO:  No, no, no.



STEVE:  I'm a website designer.  Let me fix this for free.  I will be happy to fix it for free.



LEO:  No, don't.  Okay.



STEVE:  And I, you know, I want to do it.  One of the problems I had with employees was they were having all the fun doing the work.  And so consequently...



LEO:  Yeah, but you update that, and then you've got to maintain it.  Trust me, as somebody with a modern website, stick with what you've got.  Static is a good thing.



STEVE:  I am.  Anyway, I've been living with my commitment to offer 6.1...



LEO:  Yay.



STEVE:  ...for so long now, you know, it's been more than a decade, and I felt guilty whenever I'd stolen time from that.  You know, and now every time I remember that 6.1 is there being sold, it's like, oh.  I mean, it gives me a great sense of peace.



LEO:  Good for you.



STEVE:  One thing I wanted to mention, though, to our listeners is that, after sharing the experience my wife and I had with her Dell laptop during last week's podcast, I did notice an uptick in SpinRite sales; in other words, more "Yabba Dabba Doo's."  Now, I'm fine with that since many people have reported significantly improved performance after running SpinRite at Level 3 over an SSD, which is almost certainly going to also improve the long-term reliability of their system.  So I can stand behind the benefits that people are likely to see.  My only concern was that until the afternoon of this past Sunday, April 7th, which also happened to be my third wedding anniversary, so it was a busy day...



LEO:  Oh, congratulations.



STEVE:  Unless those new purchasers knew to then go to GRC's prerelease.htm page, they would have obtained 6.0.  So I just wanted to make sure that anyone who may have heard that and been excited and went out to get SpinRite, that they knew that they were getting 6.0, and everyone should go get 6.1.  You can use your receipt that you received to bring up the page and download, you know, 6.1 is what we're offering now.  So anyone who goes to get it now who bought 6.0 will get 6.1, or you can go to the /upgrade.html.  Greg said to me, my tech support guy, why are we still calling it "prelease"?  And this was, I think, yesterday.  I said, oh, yeah.  So I created another page called upgrade.htm, so you can use that, too.



Anyway, the second thing I wanted to share was inspired by someone using the handle "The Big Bear" who posted over in GRC's SpinRite newsgroup.  After running SpinRite 6.1 on two Macs, he posted:  "And now I have two Macs tested and refreshed."  He said:  "And it makes quite a difference.  The frequency with which the colorful beach ball had shown was starting to worry me, and it has all but disappeared now.  And the startup times feel halved, at least."  He said:  "Wish I had measured it before and after."  He said:  "Documentation is not my strong suit, but I will put it on my to-do list, to write an update mentioning the extra hoops required with the latest Sonoma macOS."



Now, the reason I'm mentioning that, aside from it being another instance of welcome feedback about my use of the past 3.5 years creating 6.1, is that while FreeDOS and SpinRite will run on Intel-based Macs, getting them to boot from a CD or USB can be a bit tricky.  So I wanted to remind any would-be Mac purchasers that this is the reason I created GRC's freeware named "Bootable," in favor of "DOS Boot," although that was, you know, I was tempted to name it that.  You know, you can get Bootable.  Download it for free.  If you can get it to congratulate you on your success in booting it, then exactly the same path can be taken with SpinRite.



And as I said, Bootable can be freely downloaded at any time.  And GRC's web forums at forums.grc.com contain a growing knowledge base of help for Mac users.  And that's where this Big Bear guy will post instructions and more details, although there's already a lot of stuff there because, you know, Mac users have said, hey, how do I get this to go?  So anyway, next up is email.  I know our listeners can't wait to for GRC to have a mailbag that they can send things to.  And then is to update the documentation.  And then this project will be finished.  So yay.



LEO:  And now what?  Next, after this?



STEVE:  I know what, but...



LEO:  Oh, it's a secret.



STEVE:  ...I'm not going to say yet.



LEO:  Okay.  Smart.  Wise.



STEVE:  One thing at a time.  I always get myself in trouble by overcommitting.



LEO:  Exactly.



STEVE:  And precommitting.  And my wife, bless her heart, says, "Now, you're not going to promise anything again, are you?"  I was like, no, I'm still recovering from the last promise.



LEO:  Yeah.  Well, she's put up with this, if you got married three years ago, for the entire life of your marriage.  Maybe it's time to take a little break.



STEVE:  No, she is a dream.



LEO:  Yeah, that's awesome.  All right.  Minimum viable - I know what a minimum viable product is.  When you're an app developer or creating a new site, you create the minimum viable product.  You know?  But what is this MVSP?



STEVE:  Yup.  And I'm sure that's where they came from.  So our beloved industry is slowly, very slowly, getting its act together.  But it is happening.  And I've been encouraged by some recent news surrounding the Minimum Viable Secure Product effort.  The group's list of contributors has been growing, and it now includes some well-known names such as Salesforce, Google, Okta, Slack, Vanta and about 20 others.  I guess that's Okta.  The reason this is today's primary podcast topic - aside from the whole thing being an extremely worthwhile effort, which it is - is due to the announcement of the effort's latest member, which was made on Thursday last week.



The posting reads:  "Today, we're excited to announce that CISA is joining the Minimum Viable Secure Product Working Group.  Since launching CISA's global Secure by Design initiative last year, we've received a tremendous amount of feedback, including through our Request for Information that recently closed."  And of course it was the Secure by Design initiative where we first saw this notion suggested of requiring a manual action on the device in order to change the configuration in any way that could be foreseen as being dangerous, which I think is brilliant because, you know, again, inconvenient, yes, but it's the right thing to ask for.



Anyway, so CISA's been leading on a lot of this.  They said:  "One of the key questions we've gotten is how organizations consuming software can ask the right questions of their software manufacturers.  Such a 'secure by demand' approach" - as opposed to secure by design, which is what CISA's is - "is crucial to drive the uptake of secure by design principles and practices.  Too often, procurement questionnaires are filled with long lists of questions which don't always correlate with positive security outcomes.  In order to achieve a future where technology is secure by design, companies buying software should have simple and to the point questions for their vendors.



"The MVSP is an important step toward this goal.  MVSP offers a simple checklist that organizations can use to strengthen security at multiple stages - to review their software vendors' security during procurement, as a self-assessment tool for their own software, as part of their software development lifecycle, or as contractual controls - which can go a long way toward helping ensure secure by design principles are followed.  We're excited to join the MVSP working group to help shape the direction of the initiative going forward.  The MVSP is composed of a broad coalition of technology manufacturers, and the working group is open for anyone to join."



Okay, now, reading through the MVSP's checklist put a smile on my face, as I mentioned at the top of the show, as I imagine it will for our listeners, since we have carefully examined many of these issues in the past here.  What everyone is hoping is that powerful technology procurement bodies  like for example the various branches of the U.S. government, including both its civilian and military bureaucracies  might start making an adherence to these principles more than just requests, but make them mandatory for future purchases.



The MVSP web site is just MVSP.dev, and they explain their mission by writing:  "Minimum Viable Secure Product is a list of essential application security controls that should be implemented in enterprise-ready products and services.  The controls are designed to be simple to implement and provide a good foundation for building secure and resilient systems and services.  MVSP is based on the experience of contributors in enterprise application security and has been built with contributions from a range of companies.



"We recommend that all companies building enterprise software" - and notice today, today it's a recommendation.  Let's see how this evolves over time.  It would be just terrific if it became more than a recommendation.  So "We recommend that all companies building enterprise software and services, or otherwise handling sensitive information, implement the MVSP controls and, where possible, go well beyond them in their application security programs.  We welcome constructive feedback to help us continue to improve MVSP and provide a control set that meets the needs of its users."



Okay.  So MVSP is a list of essential application security controls at the enterprise level.  Let's look at what they are.  So under business controls they have:  "Publish a vulnerability disclosure policy that outlines the testing scope, provides a legal safe harbor, and gives contact details for security reports."  Okay.  Those are all things we've discussed in this podcast.  Vulnerability researchers should be free, should know that they're free to research a company's products, the security of them, while being legally protected from retribution or reprisals if they hack a supplier's product without malicious intent for the sole purpose of discovering and responsibly reporting vulnerabilities, even if it's for pay.  You know, if it's to report them to HackerOne or a legitimate service.  But the point is, publish a vulnerability discovery and disclosure policy and make it clear.



Also, flesh out, or rather fleshing that out, the MVSP lays out the required components:  "Develop and document procedures for triaging and remediating reported vulnerabilities, respond to reports within a reasonable time frame, and patch vulnerabilities quickly."  They also suggest contracting with a security vendor to perform comprehensive penetration testing of products, services, and dependent systems at least once a year.  We know that's been done, but we know that's not probably common practice enough.  So yes, you need third-party eyes on something.  Your own developers cannot aggressively test the security of the products they develop.  It just doesn't work.



They have:  "Notify relevant parties about any security breach that affects sensitive information no later than 72 hours upon discovery, and upon learning any additional details of the breach.  Consider reporting the breach to relevant national cybersecurity agencies in line with local guidance and regulations.  In any such reporting, include the nature of the breach, relevant contact information, the consequences of the breach, and the measures taken and needing to be taken to remediate the issue."



"Be certain to sanitize all storage media holding unencrypted production data.  Implement single sign-on using modern, maintained, and industry-standard protocols for all customers at no additional cost.  Redirect traffic from HTTP (port 80) to HTTPS (port 443).  Exceptions to this are internally secure protocols designed to run over unencrypted connections," you know, such as OCSP, the Online Certificate Status Protocol, that doesn't need it.  And I also noted, whenever I've been doing digital signing of code, the connection to the timestamp server is just HTTP because it provides - it's another example of a protocol that uses its own internal security.



Also, "Include Strict-Transport-Security header with a long max-age value and set authentication cookies as Secure."  We know that this prevents the browser from ever sending those cookies out over non-encrypted connections.  They've got "Apply appropriate HTTP security headers to reduce the application's attack surface and limit post exploitation.  These should include setting a minimally permissive Content Security Policy," you know, the CSP, "limiting the ability to in-line frame the application by enabling framing controls with X-Frame-Options or CSP frame-ancestors and disable caching for APIs and endpoints that return sensitive data."



Again, these are all, like, these are all things that web designers should do.  But they don't unless they have to, or unless they're told to, or unless there's some policy to make that happen.  I remember doing that for the SQRL queries and responses from the SQRL server.  And, you know, it's a little nerve-wracking because you're - especially nerve-wracking to add them after the fact because you're not sure what you're going to break.  If maximally restrictive policies are there from the start, when you add a feature and test it, and it doesn't work, then you can look at how to make the smallest change required to loosen the policy to allow the technology you want to work to work.  That's entirely different from just not having anything at all, where everything works.  The problem is lots of things you don't want to have work will then also work.



And of course the MVSP also had a lot to say about password policies.  They wrote:  "If password authentication is used in addition to single sign-on, then" - and we have a series of bullet points:  "Do not limit the permitted characters that can be used."  Yay.  "Do not limit the length of the password to anything below 64 characters.  Do not use secret questions as a sole password reset requirement.  Require email verification of a password change request, and require the current password in addition to the new password during password change.  Store passwords in a hashed and salted format using a memory-hard or CPU-hard one-way hash function.  Enforce appropriate account lockout and brute-force protection on account access.  And do not provide default passwords for users or administrators."



And what about the use of security-sensitive third-party libraries?  You know, like Log4j.  They say:  "Use modern, maintained, and industry-standard frameworks, template languages, or libraries that systemically address implementation weaknesses by escaping the outputs and sanitizing the inputs.  Ensure third-party dependencies are maintained and up-to-date, with security-relevant updates having a security score of medium or higher applied in line with your application patching schedule.  Upon becoming aware of a Known Exploited Vulnerability" - that's CISA's KEV collection - affecting a third-party dependency, the patch should be prioritized.  Where dependency patching or upgrades are not possible, equivalent mitigation should be implemented for all components of the application stack."



And so we can sort of group all of this as things that somebody trained in modern security measures would know, but also things that are, you know, not fun to spend time on; right?  It's like, gee, what did you do all last month?  Well, I brought us more into compliance.  What?  Well, you know, what new functions work as a result?  Uh, well, none.  But we're more in compliance than we were.  The point is, you know, it's thankless; right?  It's not until everybody around you gets attacked and hacked, and you don't, that you begin to look like a star.



So anyway, hardly surprising, these guys are also big fans of logging, recommending that logs be kept of all authentication events, both successes and failures.  And I thought that was interesting; you know?  Log all security-relevant configuration changes, including of course disabling of logging, and log application owner access to customer data to provide access transparency.  The logs must include the user ID who's involved, the IP address from which anything is happening, a valid timestamp, the type of action performed, and the object of the action.  Logs must be stored for at least 30 days at no additional charge - I thought this was interesting - to the client or customer.  We know that we've recently seen instances where some cloud provider said, well - actually Microsoft in particular - we'll provide you with more advanced security logging, but that's an extra cost option.  And as we know, they've backpedaled on that a bit.  And not surprising that the logs should not contain sensitive data or payloads.



And though it barely needs saying, they recommend using current, maintained, industry-standard means of encryption to protect sensitive data in transit between systems, and at rest in all online data storage and backups.  And when vulnerabilities are found, they say:  "Produce and deploy patches to address application vulnerabilities that materially impact security within 90 days of discovery.  For vulnerabilities with evidence of active exploitation, production and deployment of patches should be prioritized."  Okay, that one is a "duh."  And finally:  "Publish a security bulletin that details the vulnerability and its root cause if the remedy requires action from customers.  We've seen many instances where permissions were far too open.  We've noted that nothing breaks when everyone can access everything, so it's often discovered after a breach of some kind that the source of the breach was overly permissive access controls.



So they write:  "Limit sensitive data access exclusively to users with a legitimate need.  The data owner must authorize such access.  Deactivate redundant accounts and expired access grants in a timely manner.  Perform regular reviews of access to validate need to know.  Ensure that remote access to customer data or production systems requires the use of multifactor authentication."



And I know, yeah, everyone knows, all of that is obvious.  I will say, you know, I'm subjected to it by Level 3 because my badge, they insist on continually expiring my badge.  And it's annoying because it's me.  And I have to make sure that I'm keeping it renewed because, if it does expire, then renewing it is a bigger pain.  And if I need to run over there, I need to have my badge current.  And they're constantly expiring it.  On the other hand, yes, it's more secure.  If it were not just me, if it were an organization where, for example, 20 people had access, the act of having to renew all of those badges would be like, oh, wait a minute, Herman no longer works here.  So it's, yeah, I'm not renewing that badge.  But if it didn't expire, your attention wouldn't be brought back to it.  So, you know, all of these things, they're little incremental increases in pain, but they're important because they do things.



So, you know, nobody wants to sit and make time to review access permissions.  It's boring.  And, you know, it might not even be productive.  You may not find anything that needs to be changed, while a million other actually important things need to be done.  As a consequence, typically, it never happens.  But it can be a big source of problems.  So they also advise that it's important to maintain a list of third-party companies with access to customer data, which can and will be made available to clients and business partners upon request.



And what occurred to me is since that list, providing that list, might be embarrassing if, for example, it were to contain the names of contractors who were no longer affiliated with the organization, this also, again, forces a review and provides some incentive to remove access once relationships have terminated, and instances where the access doesn't, you know, remove itself.  And again, through the years we've seen that instances where that not happening has come back to bite companies.  And since it often doesn't happen automatically, it's another of those things that often falls through a crack.



They close this comprehensive list, you know, of things that, as I said, everybody knows would be good to do, but many organizations are still not doing, by reminding about the need for and importance of backups.  They recommend not only securely backing up all data to a different location - you know, Leo, this is your standard, you know, 3-3-3 or whatever it is, backup system.



LEO:  3-2-1, yes.



STEVE:  3-2-1, right.  I knew there were some numbers involved.  And, you know, we all know that everybody would like to be in full compliance with these guidelines.  And we also know about inertia, and that few things change on their own for the better, or for that matter change at all.  It is for this reason that these MVSP guidelines exist, and it's the reason CISA has added their name to the group's growing list of contributors.



The bottom line is that doing all of these things would come at some cost, and most businesses are looking for ways to cut costs, rather than ways to incur additional expense.  And the businesses don't pay the price until they get bit by a security problem.  So it's going to be a difficult lift.  But at least now all of these useful concepts have been pulled together in a single place, MVSP.dev.  And if at some point world governments were to require compliance, well, then everyone who wanted to sell to those major markets would need to clean things up.  And that would help everybody.



LEO:  Well, it's a step.



STEVE:  Yes.  It is.  And, you know, we're not going to get there without it.



LEO:  Right.



STEVE:  It doesn't mean we're going to get there with it.  But, you know, it's better to have it than not.



LEO:  Right.



STEVE:  Bye.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#970

DATE:		April 16, 2024

TITLE:		GhostRace

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-970.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What's the latest on that massive five-year-old AT&T data breach?  Who just leaked more than 340,000 Social Security numbers, Medicare data and more, and what does that mean?  Are websites honoring their cookie banner notification permissions?  And why do we already know the answer to that question?  What surprise has the GDPR's transparency requirements just revealed?  And after sharing a bit of feedback from our listeners, we're going to go deeper into raw fundamental computer science technology than we have in a long time - and it may be inadvisable to operate any heavy equipment while listening to that part.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  I'm at, well, not here, I'm at my mom's house.  But we do have a great show.  It's a propeller beanie show this week.  He's going to explain what race conditions are and why it contributes to problems like Spectre and Meltdown.  We've got the latest numbers on the massive five-year-old AT&T data breach.  You won't believe how many customers are affected.  You'll also be curious to know who just leaked more than 340,000 Social Security numbers, Medicare data, and more, and what you can do about it.  And GDPR transparency requirements, you know, those cookie pop-ups?  Are they honored?  What do you think?  We've got the deets, all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 970, recorded April 16th, 2024:  GhostRace.



It's time for Security Now!, the show where we cover the latest news about what's happening in the world of the Internet in security.  Bad guys, good guys, black hats, white hats.  I am  broadcasting from the East Coast this week.  I'm at Mom's house.  Steve Gibson never leaves his fortress of solitude.  Hello, Steve.



STEVE GIBSON:  Why would I ever leave?  And Leo, this is not a green screen behind me, as Andy, or was it, no, it was one of your guys on MacBreak Weekly said.  No, this is not a green screen.  This is, yeah...



LEO:  Well, Jason has the same background, whether he's real or not.  And I always have to ask him.  But if this were a green screen, could I do this?



STEVE:  That's a very nice little bunny.  That's a very - that's a very nice bunny.



LEO:  I have lots of tchotchkes behind me.



STEVE:  Yeah, I think I grew up with one of those clocks in my grandparents' house.



LEO:  Oh, yeah.  That's a classic...



STEVE:  Yeah.



LEO:  Isn't that neat?  There's a name for it.



STEVE:  That's neat, with a big golden one that - yup.



LEO:  Is it a steeple clock?  I can't remember.  There is a name for it, yeah.  Lots of antiques here in beautiful Providence.



STEVE:  And in fact your own mother could be considered an antique.



LEO:  Yes, absolutely.  Although she doesn't live here anymore.  She's at the home.  But she's loving it.  It's an assisted living facility.  She loves it.  She's in memory care because she has no ability to form memories at all; you know?  She can't, like she won't remember that I was there today.  But she remembers everything else perfectly.



STEVE:  Isn't that...



LEO:  So it's a really interesting conversation.  She's jolly and happy as ever.  She's not - I said, "Sometimes people get grumpy when they can't remember from day to day."  She says, "No, no, it's good.  Everything's fresh and new."



STEVE:  I had one of my very good lifelong friends - who's no longer on the planet.  But we had been driving to each other's place to go see a movie together for decades.  And one day he said, "I've lost my maps."  And I said, "What?"  And he said, "Everything's fine," he said, "but apparently I had a little stroke, and I don't know where anything is anymore."



LEO:  Interesting.  Interesting.



STEVE:  And it was so selective.



LEO:  Precise, yeah.



STEVE:  I mean, it was like, it was just his mapping center.



LEO:  Wow, yeah.



STEVE:  Was, like, lost.  And so I said, "Well, okay, I'll come, I'll pick you up instead of the reverse."



LEO:  It happens.



STEVE:  But it was - and that's - he said, "I lost my maps."  He, like, realized, he didn't know where, how to go places.



LEO:  Isn't that interesting.



STEVE:  But everything else was still...



LEO:  The little vessel that burst just...



STEVE:  Breaking something.



LEO:  One little zone of the brain, and it's gone, yeah.



STEVE:  Yeah.  Okay, so this week, oh, boy.  Probably the most consistent feedback that we received through the life of this podcast has been that our listeners love the deep technology dives.  And, oh, get your scuba equipment.



LEO:  Oh, good.



STEVE:  Because in order for today's revelation to make sense, today's podcast is titled "GhostRace" - "race" as in race conditions, and "ghost" as in Spectre.  So what has been found by a team of researchers at AU Amsterdam and IBM is yet another problem with our industry's attempt to accelerate CPU performance.  But in order to understand it, it's going to be necessary for our listeners to understand about preemptive multithreading environments and thread concurrency and synchronization of shared objects.  And that may sound like a lot, but everyone's going to be able to go, oh, you know, "that's so cool" by the end of this podcast.



So we're going to talk about an update on that massive five-year-old AT&T data breach.  Also, who just leaked an additional 340,000 Social Security numbers, Medicare data, and more.



LEO:  Ay ay ay.



STEVE:  And what that means.  I know.  It's just, gosh, really?  Are websites honoring their cookie banner notification permissions?  And why do we already know the answer to that question?  What surprise has the GDPR's transparency requirements just revealed?  And then, after sharing a bit of feedback from our listeners, as I said, oh, boy.  Given the feedback that I've had about what people like when we do it on this podcast, we're going to go deeper into raw fundamental computer science technology than we have in a long time.



LEO:  Good.



STEVE:  And I'll just caution our listeners, it may be inadvisable to operate any heavy equipment while listening to that portion of today's presentation.



LEO:  Well, I always call these, and I think you do, too, the propeller hat episodes.



STEVE:  Yep.



LEO:  And we love the propeller hat episodes.  So get your beanie ready.  We're going to talk about some deep stuff.  And of course our Picture of the Week this week, which I have not seen.  So you'll get a clean take from me on this one.  Always a fun part of the show.



STEVE:  Okay.  So this is just a wonderful picture.  I gave it the headline "This wonderful concept suffers from being a bit of an inside joke, but for those who understand it..."



LEO:  I get it.



STEVE:  "...well, it's quite wonderful."  What we're seeing is the result from some random guy in Nebraska no longer supporting his 20-year-old, you know, thankless maintenance of some random piece of open source software upon which the entire Internet's infrastructure has been erected.



LEO:  It's a sequel to a famous xkcd cartoon, yeah.



STEVE:  Yes, yes, xkcd.com/2347.  I put below the picture, I said, "If you need a hint, see xkcd.com/2347," where...



LEO:  Which is a perfect little sandcastle of blocks and then giant things, all relying on one little tiny brick.



STEVE:  Yup, yup.  And so anyway, today's picture is what happens if the brick gets pulled.



LEO:  Or the maintainer retires, which has been a problem of late.



STEVE:  Yeah, exactly.  I mean, we don't know how to launch shuttles anymore because all the shuttle guys are gone.  And it's like, what do you mean, I need a different kind of epoxy for this tile on the bottom of the ship?  



LEO:  We don't know how to build spacesuits.  Did you see that?



STEVE:  Yeah.



LEO:  Nobody knows how to build those original Apollo spacesuits.  They're starting from scratch.



STEVE:  Wow.  Okay.  So I wanted to follow up on our report from two weeks ago about the massive and significant AT&T data breach.  Reading BleepingComputer's update, I was put in mind of the practice known as "rolling disclosure," where the disclosing party successively denies the truth, only admitting to what is already known when evidence of that has been presented.  The trouble here, of course, is that AT&T is a massive publicly owned communications carrier that holds deep details about their U.S. consumer users.  And an example of this incident suggests that they could be acting far more responsibly; and, moreover, that by not having done so for the past five years, the damage inflicted upon their own customers has likely been far worse than it needed to be.  I mean, it's just - it's such bad practice.



Anyway, so BleepingComputer's reporting was headlined:  "AT&T now says data breach impacted 51 million customers," and the annoyance in this piece's author is readily apparent.  I've edited it just a bit to be more clear for the podcast, but BleepingComputer posted this.



They said:  "AT&T is notifying 51 million former and current customers, warning them of a data breach that exposed their personal information on a hacking forum.  However, the company has still not disclosed how the data was obtained.  These notifications are related to the recent leak of a massive amount of AT&T customer data on the breach hacking forums that was first offered for sale for $1 million back in 2021.



"When threat actor ShinyHunters first listed the AT&T data for sale in 2021, AT&T told BleepingComputer that the collection did not belong to them, and that their systems had not been breached.  Last month, when another threat actor known as MajorNelson" - I guess he's an "I Dream of Jeannie" fan - "MajorNelson leaked the entire dataset on the hacking forum, AT&T once again told BleepingComputer that the data did not originate from them, and that their systems had not been breached.



"After BleepingComputer confirmed that the data did belong to AT&T and DirecTV accounts, and TechCrunch reported" - as we've reported two weeks ago - "that AT&T's logon passcodes were part of the data dump" - a little hard to deny that one - "AT&T finally confirmed that the data did belong to them."  You know, it's like, "Oh, that data."  Uh-huh.  Right.



Anyway, "While the leak contained information for more than 70 million people, AT&T now says that it impacted a total of 51,226,382 customers."  So apparently they've been doing some research into that data.  "AT&T's most recent notification states:  'The exposed information varied by individual and account, but may have included full name, email address, mailing address, phone number, social security number, date of birth, AT&T account number, and AT&T passcode.  To the best of our knowledge, personal financial information and call history were not included.'"  Oh, goodie.  So call history not there, but who cares?  They said:  "'Based on our investigation to date, the data appears to be from June 2019 or earlier.'



"BleepingComputer contacted AT&T, asking why there is such a large difference in impacted customers.  They said:  'We are sending a communication to each person'" - that is, again, 51,226,382 people.



LEO:  What?



STEVE:  Yeah.



LEO:  51 million?



STEVE:  226,382.



LEO:  Wasn't it, like, eight or nine million when this all began?



STEVE:  Yes.  That's right.  They've done a little more research and said, well, it's a little worse than we first told you.



LEO:  52 million?



STEVE:  That's the definition of a rolling disclosure.



LEO:  Yes.  Holy cow.



STEVE:  So, yeah, it's like, oh, that data.  Oh.



LEO:  Yeah, yeah, the ones with the PINs.  Oh, yeah.



STEVE:  That's right.



LEO:  Yeah.



STEVE:  Got a little hard to deny that one now.



LEO:  Well, they think maybe a contractor - now, that's the thing.  They don't know.



STEVE:  So, oh, but they said:  "We're sending a communication to each person whose sensitive personal information was included.  Some people had more than one account in the dataset, and others did not have sensitive personal information."  Because BleepingComputer said, wait a minute, you said more than 70 million, but now you're sending notices to 51,226,382.7.  Anyway, "The company has still," writes BleepingComputer, "not disclosed how the data was stolen, and why it took them almost five years to confirm that it belonged to them, and to alert their customers."



So okay.  I can understand that today, five years downstream, they may not know how it was stolen.  It's at least believable that, if they didn't ever look, then they would have never found out; right?  But the denial of the evidence they were shown many years ago is, you know, is difficult to excuse.  They were first shown this in 2001.  And it does appear that it's not going to be excused.



BleepingComputer said:  "Furthermore, the company told" - meaning AT&T told - "the Maine Attorney General's Office that they first learned of the breach on March 26, 2024, yet BleepingComputer first contacted AT&T about it on March 17th and the information was for sale first in 2021.



"While it is likely too late, as the data has been privately circulating for years, AT&T is offering" - this is so big of them - "one year of identity theft protection" - whatever that means - "and credit monitoring services through Experian, with instructions enclosed in the notices.  The enrollment deadline was set to August 30th of this year."  So a few months from now.  "But exposed people should move faster" - yeah, all 51 million of you - "to protect themselves.



"Recipients are urged to stay vigilant, monitor their accounts and credit reports for suspicious activity, and treat unsolicited communications with elevated caution.  For the admitted security lapse and the massive delay in verifying the data breach claims and informing affected customers accordingly" - not surprisingly - "AT&T is facing multiple class-action lawsuits in the United States.



"Considering that the data was stolen in 2021, cybercriminals had ample opportunity to exploit the dataset and launch targeted attacks against exposed AT&T customers.  However," finishes Bleeping Computer, "the dataset has now been leaked to the broader cybercrime community."  That is, no longer privately circulated.



LEO:  Oh, good.



STEVE:  Now everybody has it.



LEO:  The broader cybercrime community.



STEVE:  That's right, "exponentially increasing the risk for former and current AT&T customers."  



Okay.  So just so we're clear here, and so that all of our listeners new and old understand the nature of the risk this potentially presents to AT&T's customers:  Armed with the personal data that has been confirmed and admitted to having been disclosed - specifically Social Security numbers, dates of birth, physical addresses, and of course names - that is all that's needed to empower bad guys to apply for and establish new credit accounts in the names of creditworthy individuals.  This is one of the most severe consequences of what is commonly known as "identity theft," and it is a true nightmare.



The way this is done is that an individual's private information is used to impersonate them when a bad guy applies for credit in their name, using what amounts to their identity.  That bad guy then drains that freshly established credit account and disappears, leaving the individual on the hook for the debt that has just been incurred.  Since anyone might do this themselves, then claim that it really wasn't them, and that "It must have been identity theft, your Honor," proving this wasn't really them running, like, a scam of their own can be nearly impossible.  And among other things, it can mess up someone's credit for the rest of their lives.



There's only one way to stop this, which is to prevent anyone, including ourselves, from applying for and receiving any new credit by preemptively freezing our credit reports at each of the three major credit reporting agencies.  I know we've covered this before, at the beginning of the year, in fact.  But it just bears repeating.  And we may have some new listeners who haven't already heard this, or existing listeners who intended to take this action before, but never got around to it.



So here's my point:  In this day and age of repeated, almost constant, inadvertent online information disclosure, it is no longer safe or practical for our personal credit reporting to ever be left unfrozen by default.  Rather than freezing our credit if we receive notification of a breach that might affect us - and after all it could take five years, you know, and 51,226,382 of AT&T's current and former customers are now being informed five years after the fact, everyone should have their credit always frozen by default.



And in fact I can see some legislation in the future where somehow this is the policy because it's still wrong that by default it is open.  But of course all the bureaus want it to be because that's how they make money is by selling access to our credit, which we never gave them permission to do, and they just took it.  And then, once our credit is frozen by default, only briefly and selectively unfreezing it when a credit report does actually need to be made available for some entity to whom we wish and need to prove our credit worthiness.  



Anyway, the last time I talked about this I created a GRC shortcut link for that podcast, which was #956.  But I want to make it even easier to get to this page.  So you can get all the details about how to freeze your credit reports by going to grc.sc/credit.  So just put into your browser grc.sc, as in shortcut, grc.sc/credit.  That will bounce your browser over to a terrific article at Investopedia that I verified is still there, still current, and still great information.



And on the heels of that, just to put an exclamation mark at the end of that news, another disclosure was just made and reported under the headline "Hackers Siphon 340,000 Social Security Numbers From U.S. Consulting Firm."  CySecurity wrote:  "Greylock McKinnon Associates (GMA) has discovered a data breach in which hackers gained access to 341,650 Social Security numbers.  The data breach was disclosed last week on Friday on Maine's government website, where the state issues data breach notifications.  In its data breach warning mailed to impacted individuals, GMA stated that it was targeted by an undisclosed cyberattack in May of 2023 and 'promptly took steps to mitigate the incident.'"  Unfortunately, they didn't promptly apparently disclose it until now, almost a year later.  



"GMA provides economic and litigation support to companies and government agencies in the U.S., including the DOJ, that are involved in civil action.  According to their data breach notification, GMA informed affected individuals" - okay, they were informed - "that their personal information 'was obtained by the U.S. Department of Justice as part of a civil litigation matter' which was supported by GMA.  The purpose and target of the DOJ's civil litigation are unknown.  A Justice Department representative did not return a request for comment.



"GMA stated that individuals that were notified of the data breach are 'not the subject of this investigation or the associated litigation matters,' adding that the cyberattack 'does not impact your current Medicare benefits or coverage.  We consulted with third-party cybersecurity specialists to assist in our response to the incident, and we notified law enforcement and the DOJ.  We received confirmation of which individuals' information was affected, and obtained their contact addresses on February 7, 2024.'"  So, whoops, it was almost a year before people were notified.



"GMA notified victims that" - here it is - "'Your private and Medicare data was likely affected in this incident.'"  Now, you've got to love the choice of the word "affected."  You mean as in "obtained by malicious hackers?"  Yeah.  Anyway, they said:  "...which included names, dates of birth, home addresses, some medical and health insurance information, Medicare claim numbers, and Social Security numbers.



"Finally, it remains unknown why GMA took nine months to discover the scope of the incident and notify victims.  GMA and its outside legal counsel" - been very active lately - "Linn Freedman of Robinson & Cole LLP, did not immediately respond to a request for comment."



So as I noted above, this is now happening all the time.  It is no longer safe to leave one's credit report unfrozen, and freezing it everywhere will only take a few minutes.  Since you won't want to be locked out of your own credit reporting afterwards, however, be sure to securely record the details you'll need when it comes time to briefly and selectively unfreeze your credit in the future.



And Leo, let's take a break before we talk about cookie notice compliance or lack thereof.



LEO:  By the way, you mentioned in your credit report you didn't give them permission.  You did.  But it was in the very, very fine print of that credit card agreement of every agreement you make.  They put it in the fine print that we will submit information to the credit reporting bureaus.



STEVE:  Okay, thank you.  I'm glad you caught that.



LEO:  Yeah.  So you did agree to it.  But, you know, I mean, the truth is it's how the world works because it is a sensible system for having somebody who wants to lend you money have some way of verifying that you're a good prospect.



STEVE:  Yup.



LEO:  So every time you buy a car, rent a house, get a new phone, cell phone, set up a cell phone carrier, they do that.  They pull those credit reports.  I freeze mine, though, and I think you're absolutely right.  Everything I have is frozen.  And thanks to a federal law, they can't charge you to unfreeze it.  They used to charge you 35 bucks.  In some states it was more to unfreeze it.  They can't do that anymore.



STEVE:  Yeah.  Yeah, and to add confusion, there's also the term "lock," which does not mean the same as "freeze."



LEO:  Yeah, not the same, yeah.



STEVE:  So it's freezing your report, not locking your report.



LEO:  Freeze it, don't lock it.



STEVE:  Yup.



LEO:  Now, there were some good show titles in there, but I know you already have one, so I won't belabor it.  But "Freeze It, Don't Lock It" would be a good show title.



STEVE:  Okay.  So the following very interesting research was originally slated to be this week's major discussion topic.



LEO:  Ah.



STEVE:  Uh-huh.  But after I spent some time looking into the recently revealed GhostRace problem, I wanted to talk about that instead since it brings in some very cool fundamental computer science about the problem of "race conditions" which, interestingly, in our 20 years of this podcast - well, we're in our 20th year - we've never touched on race conditions.  So we're going to resolve that today.



But what this team of five guys from ETH Zurich discovered was very interesting, too.  So here's that.  They began asking themselves:  "When we go to a website that presents us with what has now become the rather generic and GDPR-required Cookie Permission pop-up, do sites where permission is not explicitly granted and cookie use is explicitly denied actually honor that denial?"  And with that open question on the table, the follow-up question was:  "Can we arrange to create an automated process to obtain demographic information about the cookie permission handling of the top 100,000 websites?"



Well, we already know the answer to the second question.  That's "yes."  You know?  They figured out how to automate this data collection.  And the results of their research will be presented during the 33rd USENIX Security Symposium, which will take place this coming August 14th through the 16th in Philadelphia.  Their paper is titled "Automated Large-Scale Analysis of Cookie Notice Compliance."  And for anyone who's interested, I have a link to their PDF research and the presentation prepub in the show notes.



Perhaps it won't come as any shock that what they found was somewhat disappointing.  Did they find that 5% of the 97,090 websites they surveyed did not obey the sites' visitors' explicit denial of permission to store privacy invading cookies?  No.  Was the number 10%?  Nope, not that either.  How about 15?  Nope, still too low.  Believe it or not, 65.4% of all websites tested, so just shy of fully two thirds of all websites tested - two thirds - do not obey their visitors' explicit privacy requests.  They wrote:  "We find that 65.4% of websites do not respect users' negative consent, and that top-ranked websites are more likely to ignore users' choice, despite having seemingly more compliant cookie notices."



The Abstract of their paper says:  "Privacy regulations such as the General Data Protection Regulation (GDPR) require websites to inform EU-based users about non-essential data collection and to request their consent to this practice.  Previous studies have documented widespread violations of these regulations.  However, these studies provide a limited view of the general compliance picture.  They're either restricted to a subset of notice types, detect only simple violations using prescribed patterns, or analyze notices manually.  Thus, they're restricted both in their scope and in their ability to analyze violations at scale.



"We present the first general, automated, large-scale analysis of cookie notice compliance.  Our method interacts with cookie notices, in other words, by navigating through their settings.  It observes declared processing purposes and available consent options using Natural Language Processing and compares them to the actual use of cookies.  By virtue of the generality and scale of our analysis, we correct for the selection bias present in previous studies focusing on specific Content Management Platforms.  We also provide a more general view of the overall compliance picture using a set of 97,000 websites popular in the EU.  We report, in particular, that 65.4% of websites offering a cookie rejection option likely collect user data despite explicit negative consent."



Okay.  So I suppose we should not be surprised to learn that what's hiding behind the curtain is not what we would hope.  Two thirds of companies in general - the larger they are, the worse is their behavior - are simply blowing off their visitors' explicit requests for privacy.  And this is in the EU, where the regulation is present and would be more likely to be strongly enforced.  Except who would know if the regulation was being ignored unless you checked.  So not only are we now being hassled by the presence of these cookie permission banners, but two out of every three required "do nothing clicks."  When we say we want you not to do this, they have no actual effect.



The researchers discovered additional unwanted behavior which they termed "dark patterns."  Specifically, 32% are missing the notice entirely.  56.7% don't have a reject button.  So they display the notice, but they don't give you the required option to opt out.  And then there's those 65.4% that do have the button, but they ignore it.  Then there's 73.4% with implicit consent prior to interaction, meaning they assume you are consenting, and they do things in 73.4% of the cases prior to giving you the option to not have them do that.  There is a category, implicit consent after close, that's there in 77.5% of the instances.  Undeclared purposes in 26.1, so they don't tell you what's going on.  There's interface interference in two thirds of the cases, 67.7%; and a forced action in almost half, 46.5.  So bottom line, you know, this is all a mess. 



What this means is that the true enforcement of our privacy cannot be left to lawmakers and their legislation, nor even to websites.  New technology needs to be brought to bear to take this out of the hands entirely of third parties.  And I know it seems insane for me to be saying this, but this really is what Google's Privacy Sandbox has been designed to do.  And as with all change, advertisers and websites are going to be kicking and screaming, and they're not going to go down without a fight.



But the advertisers are also, reluctantly, currently in the process of upgrading their ad delivery architectures for the Privacy Sandbox because they know that, with Chrome commanding two thirds market share, they're going to have to work within this brave new world that's coming soon, you know, and real soon, as in later this year.  Google's not messing around this time.  They've given everyone years' of notice.  Change is coming.  And I say thank goodness because it's obvious that just saying, oh, you know, you've got to get permission, well, looks like websites, most of them, are asking for permission.  It turns out two thirds of them are ignoring it when we say no.



And speaking of the GDPR, which created this accursed cookie pop-up legislation which currently plagues the Internet, holding to the "there's two sides to every coin" rule, the requirements for disclosure that's also part of the GDPR legislation occasionally produces some startling revelations when conduct that parties would have doubtless much preferred to remain off the radar are required to be seen.  Get a load of this one.



In this case, we have the new Outlook app for Windows.  Thanks to the GDPR, users in Europe who download the Outlook for Windows app will be greeted with a modal pop-up dialog that displays a user agreement and requires its users' consent.  So far, so good.  The breathtaking aspect of this is that the dialogue starts right out stating:  "We and 772 third parties process data to store and/or access information on your device."



LEO:  Oh, only 700, huh?



STEVE:  772 third parties that Microsoft is sharing their users' data with.



LEO:  More computers is slow, but lord.



STEVE:  Wow.



LEO:  Wow.



STEVE:  "To develop and improve products, personalize ads and content, measure ads and content, derive audience insights, obtain precise geolocation data" - yes, we know right where you are, and we're telling everyone - "and identify users through device scanning."  And, no, that was not a typo.  It says "We and 772 third parties."



LEO:  At least they're honest.  I mean...



STEVE:  Well, they have no choice; right?  In the EU.



LEO:  Yeah, that's awesome.



STEVE:  You know, I don't know what to think about that.  It's mindboggling.  For one thing, it's disappointing that only those in the EU get to see this.  U.S. regulators are not forcing the same transparency requirements upon Microsoft, and Microsoft certainly doesn't want to show any more of this than they're forced to.



LEO:  No.  Yeah.



STEVE:  You know, earlier we were talking about the confidentiality of our data and the challenge that presents.  Having Microsoft profiting from the sale of the contents of its users' email seems annoying enough.  At the same time, you know, there's Gmail, and they're doing some of that, too.  But when there are, by Microsoft's own forced admission, 772 such third parties who all, presumably, receive paid access to this data, doesn't it feel as though Microsoft should be paying us for the privilege of access to our private communications, which is apparently so valuable to them?  Instead, we get free email.  Whoopee.  Wow.



LEO:  Does Google do anything like that with Gmail?  It's got to be - that's wild.



STEVE:  No.  I've never - whoa.  Oh, you mean in the EU.  That's a good question, whether they have had to start that, too.  Yeah.



LEO:  I think it's the case, although I'm not a lawyer, that this is in the EU, all of this; that even though we all in the U.S. and everywhere else in the world see these cookie monster announcements, they don't - they're not technically required to do them for us.  It's only for EU people. 



STEVE:  Well, as I understand it, if an EU person visits your website...



LEO:  No, it's still not liable because it has to be a company doing - well, I don't know, that's a good question.  There was a very interesting piece which I've mentioned on other shows.  And of course the guy's not in [indiscernible] either.  Blog piece that said no one has to do this except EU companies.  They're the only companies liable for this.  And he points out Amazon does not.  Right?



STEVE:  Interesting.



LEO:  You've never seen it on Amazon.



STEVE:  Interesting.



LEO:  You don't see it on a lot of sites.  



STEVE:  Not mine.



LEO:  I don't see cookies on Google sites, come to think of it.



STEVE:  GRC doesn't have it, although I'm not collecting any information and using cookies that way.



LEO:  Well, we do it on TWiT because there's a whole secondary business of people who go around saying, "You'd better do this for compliance," and "Pay me, and I'll do it for you."  And they said otherwise you're going to be in trouble.  So we have a lot of privacy stuff on our site.  We don't - you can't log into our site.  But we, as every site does, we have cookies.  We have loggers, you know, I mean, I do it on my personal website.  I say, "Look, the only cookie I have is whether you like the light mode or dark mode.  So suck it.  If you don't like that, too bad."  I mean, that's why this whole thing is so silly.  I just, it's so silly.



STEVE:  Well, it's probably going to go away because if Google has their way...



LEO:  Maybe.



STEVE:  ...and Google generally gets their way, you know, they've got the demographics, the market share.  They really are terminating third-party cookies.



LEO:  We've done this story before because remember you talked about Do Not Track, which is in the spec.  Everybody has it.  Nobody honors it.



STEVE:  It's coming back.  It's on its way back.



LEO:  I think it would be nice to see, yeah.



STEVE:  Yeah, yeah.  Well, and actually, again, what's happening is that the next-generation technology will not track.



LEO:  Right.



STEVE:  By siloing each site, the tracking becomes, or the tracking happens because traditionally browsers have had one big cookie jar.  As soon as you start creating individual per-domain cookie jars, then each domain is welcome to have its own local set of cookies, but they're not visible from any other domain.  And that terminates tracking.



LEO:  Right.



STEVE:  Unfortunately, that's why we're now doing the, oh, please join our email, you know, join our community, give us your email.  And so basically first-party cookies are being used to track through the browser.



So, okay.  We have a bit of feedback from our listeners.  And then, boy, have we got a fun techie episode.  Okay.  Let's see.  I got:  "@SGgrc.  Been hearing you talk about the physical input requirement for security setting modifications," meaning like push the button on the browser if you want to make a security change, and that keeps somebody in Russia from being able to push a button on your browser.  This person said:  "Just wanted to mention Fritz and Fritz!Box, a common German home router manufacturer who's been doing this for a while."  And he said:  "Annoying, yes.  Effective, also yes." 



So, you know, when we've talked about this previously I haven't mentioned that many SoHo routers, probably everyone's router if it's at all recent, have that dedicated WPS hardware button that is a physical button for essentially the same reason.  It allows a network device to be persistently connected to a router without the need to provide the device with the router's WiFi SSID name and password.  That button being physical requires someone to prove their presence at the router by pressing the button.  So the notion of doing something similar to similarly protect configuration changes makes a lot of sense.



But then another listener suggested, this was Rob Woodruff who tweeted.  He said:  "I'm listening to the part of SN-969 where you're talking about the five security best practices.  And it occurs to me that, regarding the requirement for a physical button press, it won't be long before we start seeing cheap, Chinese-made, WiFi-enabled IoT button pressers on Amazon.  What could possibly go wrong?"



LEO:  They exist, by the way.  Stacey Higginbotham used to talk about them.



STEVE:  A little thing that presses a button?



LEO:  Yeah.  It's a WiFi-enabled little finger that goes like this.



STEVE:  Oh.



LEO:  And for lights that don't - you can't change or whatever the - yeah, yeah.  So it exists.



STEVE:  Too funny.



LEO:  I don't know if it would work with that WPS-type button, but... 



STEVE:  Yeah.  And actually I think I remember watching that This Week in Google when you guys talked about that.



LEO:  Yeah, yeah, yeah.



STEVE:  And I thought that was...



LEO:  Stacey used them.



STEVE:  Wow.



LEO:  She liked them.



STEVE:  Remember that thing when we were growing up, the little black box that you would stick a penny on, and the hand would reach out and grab the penny?



LEO:  Right.  But my favorite one, though, there's varieties of these, you'd flip the switch, the hand comes out and turns the switch off.



STEVE:  Yes, yes.



LEO:  Loved those.



STEVE:  Okay.  So the downside of the physical button press requirement, you know, obviously is its inherent inconvenience.  You know?  But that's also the source of its security.  It would be nice if we could have the security without the inconvenience, but it's unclear how we could go about doing that.  But to Rob's point, even if a user chose to employ a third-party remote control button presser, that would still provide greater security than having no button at all.  For one thing, it would be necessary to hack two very different systems, kind of like having multifactor authentication.



But the most interesting observation, I think, is that generic remote attacks against an entire class of devices that were known to be "button protected" would never even be launched in the first place because it would be known that they could not succeed.  So even a router that technically breaks the rules with an automated button presser gains the benefit of what is essentially herd immunity because, you know, attackers wouldn't bother attacking that model router because they're unattackable, thanks to having the physical button.



Rami Vaspami tweeted:  "@SGgrc I don't see the benefit if NetSecFish kept quiet."  So this person's talking about that premature, well, the disclosure of the incredible amount of old D-Link NAS devices.  So they write:  "This would have been quietly exploited in perpetuity.  Instead, now, everyone knows to deprecate those devices, and the issue has an end in sight.  Better to know."



Okay.  If, indeed, current D-Link NAS owners could have been notified as a consequence of NetSecFish's disclosure on GitHub, then okay, maybe.  But they still would have had to act quickly since the attacks against these D-Link NAS boxes required less than two weeks to begin.  But the problem is that nothing about NetSecFish's disclosure on GitHub suggests that even a single end-user of these older but still in use D-Link NAS devices would have ever received notification.  Notification from whom?  You know, he posted this on GitHub.  That's not notifying all those D-Link NAS end-users.  Why would any random user know what's been posted in someone's GitHub account?  D-Link said, "Sorry, but those are old devices for which we no longer accept any responsibility."



But we know who does know what's been posted to someone's random GitHub account, since it took less than two weeks for the attacks to begin.  So I would argue that it is not "better to know" when it's by far predominantly bad guys who will be paying attention and who will be doing the knowing.



And finally, John Moriarty wins the "feedback of the week award."  He tweeted:  "Catching up on last week's Security Now!  and hearing @SGgrc lament the fans spinning up and resources being swamped by Google's Chrome bloat while it was apparently 'doing nothing.'  Perhaps it was busy running and completing ad auctions."



LEO:  Oh.



STEVE:  So yes, indeed.  It's quite true that our browsers are going to be much busier once responsibility for ad selection has been turned over to them.  Although, you know, our computers are so overpowered now it's ridiculous.  You know, using a browser shouldn't even wake the computer up.



I don't have any big SpinRite news this week.  Everything continues to go well, and I'm working to update GRC's SpinRite pages just enough to hold us while I get email running, and then back over to work on SpinRite's documentation.  I've been seeing more good reports of SSD original performance recovery.  While catching up in Twitter, I did run across a good question about SpinRite.  Paul, tweeting from @masonpaulak, he said:  "Hi, Steve.  Long-time podcast listener and SpinRite owner.  Thanks for the many gems of info over the years."  And boy, have we got a good one for this podcast coming up.  "Over the weekend," he said, "I saw the RAM test in SpinRite 6.1.  Please, would you give some insight into how long I should let it run to test 1GB?  And am I wearing it out as writing to an SSD would?  Thanks, Paul."



I don't think I remembered to respond to the wearing it out question.  No.  DRAM absolutely, thank goodness, does not wear out when it's being read and/or written.  Okay.  So but to his question of how long you should run it, the best answer to this is really quite frustrating because the best answer is "the longer the better."  You know, now, that said, I can clarify a couple of points.  The total amount of RAM in a system has no bearing on this, since SpinRite is only testing the specific 54MB that it will be using once the testing is over.  While the testing is underway, a test counter is spinning upwards on the screen, and a total elapsed testing time is shown.  For each test it fills 54MB with a random pattern, then comes back to reread and verify that memory was stored correctly.  So the longer the testing runs, the greater the likelihood that it will find a pattern that causes some trouble.



The confusion, and Paul's question, comes from the fact that the new startup RAM test can be interrupted.  It doesn't stop by itself.  It just goes.  And eventually the user's going to interrupt it in order to get going with SpinRite.  So when should that be?  Best practice, if it's feasible, would be for any new machine where SpinRite 6.1 has never been run before, to start it up and allow it to just run on that machine overnight, doing nothing but testing its RAM.  In the morning, the screen will almost certainly still be happy and still showing zero errors.  I mean, that's by far the most likely case.  But the screen might have turned red, indicating that one or more verification passes failed.



After I asked that first guy who was having weird problems that made no sense to try running MemTest86, the venerable, you know, full system RAM test that's been around forever, it reported errors.  So I decided I needed to build that into SpinRite, just to be safe.  Users do not need to use it at all, but it's there.  And several other of SpinRite's early testers were surprised to have SpinRite detect previously unsuspected memory errors on some of their machines.  So SpinRite's built-in RAM memory test is sort of useful all by itself, just for that.  But once it's been run for a few hours on any given machine, then it can just be bypassed quickly from then on, and you don't need to worry about it.



Okay.  So if anybody is doing brain surgery right now...



LEO:  Driving a large truck...



STEVE:  Yes, close the skull.  Put down your tools.



LEO:  Pull over.



STEVE:  Yes.  The first question to answer is "What's a Race Condition?"  A race condition is any situation which might occur in hardware or software systems where outcome uncertainty or an outright bug is created when the sequence of closely timed events, in other words a race among events, determines the effects of those events.  For example, in digital electronics, it is often the case that the state of a set of data lines is captured when a clock signal occurs.  This is typically known as "latching" the data lines on a clock.  But for this to work reliably, the data lines need to be stable and settled when the clocking occurs.  If the data lines are still changing state when the clock occurs, or if the clock occurs too soon, a race condition can occur where the data that's latched is incorrect.



Here's how Wikipedia defines the term.  They write:  "A race condition or race hazard is the condition of an electronics, software, or other system where the system's substantive behavior is dependent on the sequence or timing of other uncontrollable events, leading to unexpected or inconsistent results.  It becomes a bug when one or more of the possible behaviors is undesirable.  The term 'race condition' was already in use by 1954, for example in David A. Huffman's doctoral thesis 'The synthesis of sequential switching circuits.'  Race conditions can occur especially in logic circuits, multithreaded, or distributed software programs."



Okay.  Race conditions are also a huge issue with modern software systems because our machines now have so many things going on at once where the independent actions of individual threads of execution must be synchronized.  Any failure in that synchronization can result in bugs or even serious security vulnerabilities.



To understand this fully, we need to switch into computer science mode.  And as I said at the top of the show, I often receive feedback from our listeners wishing that I did more of this, but I don't often encounter such a perfect opportunity as we have today.  Back when we were talking about how processors work during one of our early sets of podcasts on that topic, we talked about the concepts of multithreading and multitasking.



In a preemptive multithreaded environment, "multithreaded" meaning multiple threads of execution happening simultaneously, the concept of a thread of execution is an abstraction.  The examples I'm going to draw assume a single core system, but they apply equally to multicore systems.  When a processor is busily executing code, performing some task, we call that a "thread of execution" since the processor is threading its way through the system's code, executing one instruction after another.  After a certain amount of time, a hardware interrupt event occurs when a system clock ticks.  This interrupts that thread of execution, causing it to be suspended and control to be returned to the operating system.



If the operating system sees that this thread has been running long enough, and that there are other waiting threads, the OS decides to give some other thread a turn to run.  So it does something known as a "context switch."  A thread's context is the entire state of execution at any moment in time.  So it's all of the registers the thread is working with and any other thread-specific volatile information.  The operating system saves all of that thread context, then reloads another thread's saved context, which is to say everything that had been previously saved, you know, from other thread when it was suspended.  The OS then jumps to the point in code where that other thread was rudely interrupted the last time it was preempted, and that other thread resumes running just as though nothing had ever happened.



And that's a critical point.  From the viewpoint of the many threads all running in the system, the threads themselves are completely oblivious to all of this going on above them.  As far as they're concerned, they're just running and running and running without interruption, as if each of them were all alone in the system.  The fact that there's an overlord frantically switching among them is unseen by the threads.  I just opened Task Manager on my Windows 10 machine.  It shows that there are 2,836 individual threads that have been created and are in some state of execution right at this moment.  That's a busy system.



Okay.  So now let's dive deeper into very cool and very important coding technology.  The reason for this dive will immediately become clear once we look at the vulnerabilities that the researchers from VU Amsterdam and IBM have uncovered in all contemporary processor architectures - not just Intel this time, or AMD, also ARM and RISC-V - any architecture that incorporates - get ready for it - speculative execution technology, as all modern processor architectures do.



Okay.  Imagine that two different threads running in the same system share access to some object.  Anyone who's been around computing for long will have heard the term "object oriented programming."  An "object" is another abstraction that can mean different things in different contexts.  But what's important is that it's a way of thinking about and conceptually organizing complex systems that often have many moving parts.  So when I use the term "object," I'm just referring to something as a unit.  It might be a single word in memory, or it might be a large allocation of memory, or it could be a network interface, whatever.  In the examples we'll be using, our object of interest is just a word of RAM memory that stores a count of events.  So this object is a simple counter.



And in our system, various threads might, among many other things, have the task of incrementing this counter from time to time.  So let's closely examine two of those threads.  Imagine that the first thread decides it wishes to increment the count.  So it reads the counter's current value from memory into a register.  It might examine it to make sure that it has not hit some maximum count value.  Then it increments the value in the register and stores it back out into the counter memory.  No problem.  The shared value in memory was incremented.  But remember that, in our hypothetical system, there are at least two separate threads that might be able to do this.



So now imagine a different scenario.  Just like the first time, the first thread reads the current counter value from main memory into one of its registers.  It examines its value and sees that it's okay to increment the count.  But just at that instance, at that instant, the system's thread interrupt occurs; and that first thread's operation is paused to give other threads the opportunity to run.  And as it happens, that other thread that may also decide to increment that counter decides that it wishes to do so.  So it reads that shared value from memory, examines it, increments its register, and saves it back out to memory.  And as luck would have it, for whatever reason, while that second thread is still running, it does that four more times.  So the counter in memory was counted up by a total of five times.  Then after a while that second thread's time is up, and the operating system suspends it to give other threads a chance to run.



Eventually, the operating system resumes our first thread from the exact point where it left off.  Remember that before it was paused by the operating system, it had already read the counter's value from memory, examined it, and decided that it should be incremented.  So it increments the value in its register and stores it back out to memory, just as it did the first time.



But look what just happened.  Those five counter increments that were performed by the second thread, which shares access to the counter with the first thread, were all lost.  The first thread had read the counter's value into a register.  Then, while it was suspended by the operating system, the second thread came along and incremented that shared storage five times.  But when the first thread was resumed, it stored the incremented counter value that it had previously saved back out, thus overwriting the changes made by the second thread.



This is a classic and clear example of a race condition in software.  It occurred because of the one in a million chance that the first thread would be preempted at the exact instant, between instructions, when it had made a copy of some shared data that another thread might also modify, and it hadn't yet written it back to the shared memory location.  The picture I painted was deliberately clear because I wanted to highlight the problem.  But in the real world it doesn't happen that way, and these sorts of race condition problems are so easy to miss.



Imagine that two different programmers working on the same project were each responsible for their piece of code, and each of them increments that counter.  If they didn't realize that the other had code that might also choose to increment that counter, this would introduce an incredibly subtle and difficult to ever find bug.  Normally, everything would work perfectly.  The system's tested.  Everything's great.  But then every once in a blue moon the code, which looks perfect, would not do what it was supposed to.  And the nature of the bug is so subtle that coders could stare at their code endlessly without ever realizing what was going on.



Last Tuesday was Patch Tuesday, and Microsoft delighted us all by repairing another 149 bugs, which included two that created vulnerabilities which were under active exploitation at the time.  Microsoft's software does not appear to be in any danger of running out of bugs to fix, and race conditions are one of the major reasons for that.  How many times have we talked about so-called "use-after-free" bugs, where malicious code retains a pointer to some memory that was released back to the system and is then able to use that pointer for malicious purposes?  More often than not, those are deliberately leveraged race condition bugs.



Okay.  So now the question is, as computer science people, how do we solve this race condition problem?  The answer is both tricky and gratifyingly elegant.  We first introduce the concept of ownership, where an object can only be owned by one thread at a time, and only the current owner of an object has permission to modify its value.  In the example I've painted, the race condition occurred because two threads were both attempting to alter the same object, the shared counter, at the same time.  If either thread were forced to obtain exclusive ownership of the counter until it was finished with it, the competing thread might need to wait until the other thread had released its ownership, but this would completely eliminate the race condition bug.



Okay.  So at the code level, how do we handle ownership?  We could observe that modern operating systems typically provide a group of operations known as synchronization primitives.  They're called this because they can be used to synchronize the execution of multiple threads that are competing for access to shared resources.  The primitive that would be used to solve a problem like this one is known as a "mutex," which is short for "mutual exclusion," meaning that threads are mutually excluded from having simultaneous access to a shared object.



So by using an operating system's built-in mutex function, a thread would ask the operating system to give it exclusive ownership of an object and be willing to wait for it to become available.  If the object were not currently owned, then it would become owned by that first requesting thread, and that owning thread could do whatever it wished with the object.  Once it was finished, it would release its ownership, thus making the object available to another thread that may have been waiting for it.



But I said that what actually happens is both tricky and gratifyingly elegant, and we haven't seen that part yet.  How is a mutex actually implemented?  Our threads are living in an environment where anything and everything that they are doing might be preempted at any time, without any warning.  This literally happens in between instructions, between one instruction and the next instruction.  And in between any two instructions, anything else might have transpired.



If you think about that for a moment, you'll realize that this means that acquiring ownership of an object cannot require the use of multiple instructions that might be interrupted by the operating system at any point.  What we need is a single indivisible instruction that allows our threads to attempt to acquire ownership of a shared object, while also determining whether they succeeded.  And that has to happen all at once.



It is so cool that an instruction as simple as "exchange" can provide this entire service.  The exchange instruction does what its name suggests.  It exchanges the contents of two "things."  These might be two registers whose contents are exchanged, or it might be a location in memory whose contents are exchanged with a register.  We already have a counter, the ownership of which is the entire issue.  So we create another variable in memory to manage that counter's ownership, to represent whether or not the counter is currently owned by any thread.  We don't need to care which thread owns the counter because, if any thread does, no other thread can touch it.  All they can do is wait for the object's ownership to be released.



So we just need some binary, you know, a one or a zero.  If the value of this counter ownership variable is zero, then the counter is not owned by any thread at the moment.  And if the counter ownership variable is not zero, if it's a one, then some thread currently owns the counter.



Okay.  So how do we work with this and the exchange instruction?  When a thread wants to take ownership of the counter, it places a one in a register and exchanges that register's value with the current value of the ownership variable in memory.  If, after the exchange, its register, which received the previous value of the ownership variable, is a zero, then that means that until the exchange was made, the counter was not owned by any thread.  And since this exchange simultaneously placed a one into the ownership variable at the same time, with the same instruction,  our thread is now the exclusive owner of the counter and is free to do anything it wishes without fear of any collision.



If another thread comes along and wants to take ownership of the counter, it also places a one in a register and exchanges that with the counter's ownership variable.  But since the counter is still owned by the first thread, this second thread will get a one back in its exchanged register.  That tells it that the counter was already owned by some other thread, and that it's going to need to wait and try again later.  And notice that since what it swapped into the ownership variable was also a one, this exchange instruction did not change the ownership of the counter.  It was owned by someone else before, and it still is.  What the exchange instruction did in this instance was to allow a thread to query the ownership of the counter.  If it was not previously owned, it now would be by that query.  But if it was previously owned by some other thread, it would still be.



And finally, once the thread that had first obtained exclusive ownership of the counter is finished with the counter and is ready to release its ownership, it simply stores a zero into the counter's ownership variable in memory, thus marking the counter as currently unowned.  This means that the next thread to come along and swap a one into the ownership variable will get a zero back and know that it now owns the counter.  We call the exchange instruction when it's used like this an "atomic" operation because, like an atom, it is indivisible.  With that single instruction, we both acquire ownership if the object was free, while also obtaining the object's previous ownership status.  Okay.  I suppose I'm weird, but this is why I love coding so much.



The holistic way of viewing this is to appreciate that only atomic operations are safe in a preemptively multithreaded environment.  In our example, if the counter in question only needed to be incremented by multiple threads, and that incrementation could be performed by a single indivisible instruction, then that would have been "thread safe," which is the term used.  And there are instructions that increment memory in a single instruction.  But in our example, the threads needed to examine the current value of the counter to make a decision about whether it had hit its maximum value yet, and only then increment it.  That required multiple instructions and was not thread safe.  So what do we do?  We protect a collection of non-thread safe operations with a thread-safe operation.  We create the abstraction of ownership and use an exchange instruction, which being one instruction is thread safe, to protect a collection of operations that are not thread safe.



And this brings us to "GhostRace," the discovery made by a team of researchers, and the question, what would it mean if it were discovered that these critical, supposedly atomic and indivisible synchronization operations were not, after all, guaranteed to do what we thought?  And Leo, let's tell our listeners of our last sponsor, and then we're going to answer that question because the bottom just fell out of computer security.



LEO:  I guess you could have a race for the lock; right?  That would be part of it?  That's all you need to make them atomic operations; right?  You can have races all the way down.



STEVE:  Well, you can't have a race for the lock or for the ownership because only actually one instruction is executing at any time.



LEO:  Because it's atomic; right.



STEVE:  It feels like multiple things are going on.  But actually, as we know, the processor is just switching among threads.



LEO:  Right.



STEVE:  So at any given time there's only one thing happening.  And if that one thing grabs the lock, if it grabs ownership, then it has it.



LEO:  Right.



STEVE:  And that's the elegance of being able to do something in a single instruction.



LEO:  Right.  If it's multiple instructions, then you've got a problem.



STEVE:  Right.



LEO:  If it was a resource that takes, you know, three cycles to poll, then you have the problem of you could have it interrupted.



STEVE:  Exactly.



LEO:  It's really a fascinating ecosystem that's evolved.  And this actually, this happened even before there were multithreaded computers; right?  I mean, you could have a race condition anytime you have a branch, I guess.



STEVE:  Well, anytime you have timesharing - basically it's a matter of contention whenever there are multiple things contending for a single resource.



LEO:  Right.



STEVE:  They're all free to read it at once; right?  Everybody can read.



LEO:  It's writing it that's the problem.



STEVE:  But if anybody modifies it, then you're in trouble.



LEO:  Right.



STEVE:  Okay.  So for their paper's Abstract, the researchers wrote:  "Race conditions arise when multiple threads attempt to access a shared resource without proper synchronization, often leading to vulnerabilities such as concurrent use-after-free.  To mitigate their occurrence, operating systems rely on synchronization primitives such as mutexes, spinlocks, et cetera.



"In this paper, we present GhostRace, the first security analysis of these primitives on speculatively executed code paths.  Our key finding is that all of the common synchronization primitives can be microarchitecturally bypassed on speculative paths, turning all architecturally race-free critical regions into Speculative Race Conditions, or SRCs.



"To study the severity of SRCs, these Speculative Race Conditions," they wrote, "we focus on Speculative Concurrent Use-After-Free and uncover 1,283 potentially exploitable instances in the Linux kernel.  Moreover, we demonstrate that" - they abbreviate it SCUAF, Speculative Concurrent Use-After-Free - "information disclosure attacks against the kernel are not only practical, but that their reliability can closely match that of traditional Spectre attacks, with our proof of concept leaking kernel memory at 12KB per second.



"Crucially, we develop a new technique to create an unbounded race window, accommodating an arbitrary number of Speculative Concurrent Use-After-Free invocations required by an end-to-end attack in a single race window.  To address the new attack surface, we propose a generic SRC mitigation" - and we'll see about how expensive that is in a minute - "to harden all the affected synchronization primitives on Linux.  Our mitigation requires minimal kernel changes, but incurs around a 5% geometric mean performance overhead on LMbench."



And then they finish their Abstract by quoting Linus Torvalds on the topic of Speculative Race Conditions, saying:  "There's security, and then there's just being ridiculous."  Okay.  Well, they may not be so ridiculous after all.  What this comes down to is a mistake that's been made in the implementation of current operating system synchronization primitives in an environment where processors are speculating about their own future.  Somewhere in their implementations is a conditional branch that's responsible for making the synchronization decision.  And in speculatively executing processors it's possible to deliberately mistrain the speculation's branch predictor to effectively neuter the synchronization primitive.  As they say in their paper, to turn it into a no-op, which is the programmer shorthand for no operation.  And if that can be done, it's possible to wreak havoc.



Here's what they explained.  They said:  "Since the discovery of Spectre, security researchers have been scrambling to locate all the exploitable snippets or gadgets in victim software.  Particularly insidious is the first Spectre variant, exploiting conditional branch misprediction, since any victim code path guarded by a source 'if' statement may result in a gadget.  To identify practical Spectre-v1 gadgets, previous research has focused on speculative memory safety vulnerabilities, use-after-free, and type confusion.  However, much less attention has been devoted to other classes of normally architectural software bugs, such as concurrency bugs.



"To avoid, or at least reduce concurrency bugs, modern operating systems allow threads to safely access shared memory by means of synchronization primitives, such as mutexes and spinlocks.  In the absence of such primitives, for example, due to a software bug, critical regions would not be properly guarded to enforce mutual exclusion, and race conditions would arise.  While much prior work has focused on characterizing and facilitating the architectural exploitation of race conditions, very little is known about their prevalence on transiently executed code paths.  To shed light on the matter, in this paper we ask the following research questions:  First, how do synchronization primitives behave during speculative execution?  And second, what are the security implications for modern operating systems?



"To answer these questions, we analyze the implementation of common synchronization primitives in the Linux kernel.  Our key finding is that all the common primitives lack explicit serialization and guard the critical region with a conditional branch.  As a result, in an adversarial speculative execution environment, i.e., with a Spectre attacker mistraining the conditional branch, these primitives essentially behave like a no-op.  The security implications are significant, as an attacker can speculatively execute all the critical regions in victim software with no synchronization.



"Building on this finding, we present GhostRace, the first systematic analysis of Speculative Race Conditions, a new class of speculative execution vulnerabilities affecting all common synchronization primitives.  SRCs are pervasive, as an attacker can turn arbitrary architecturally race-free code into race conditions exploitable on a speculative path, in fact, one originating from the synchronization primitives' conditional branch itself.  While the effects of SRCs are not visible at the architectural level, for example, no crashes or deadlocks, due to the transient nature of speculative execution, a Spectre attacker can still observe their microarchitectural effects via side channels.  As a result, any SRC breaking security invariants can ultimately lead to Spectre gadgets disclosing victim data to the attacker."



Okay.  So essentially the specter of Spectre strikes again.  Another significant exploitation of speculative execution has arisen.  And need we echo once again Bruce Schneier's famous observation about attacks never getting worse and only ever getting better?



Under the paper's mitigation section they write briefly:  "To mitigate the Speculative Race Condition class of vulnerabilities, we implement and evaluate the simplest, most robust, and generic one, introducing a serializing instruction into every affected synchronization primitive before it grants access to the guarded critical region, thus terminating the speculative path.  This provides a baseline to evaluate any future mitigations; and, as mentioned, mitigates not only the Speculative Condition Use-After-Free vulnerabilities presented in the paper, but all other potential Speculative Race Condition vulnerabilities."



Okay.  So essentially what they're saying is that, as with all anti-Spectre mitigations, the solution is to deliberately shut down speculation at the spot where it's causing trouble.  In this instance, this requires the insertion of an additional so-called "serializing" instruction into the code at the point just after the synchronization instruction and before the result of the synchronization instruction is used to inform a branch instruction.  This prevents any speculative execution down either path following the branch instruction.  The problem with doing that is that speculation exists specifically because it's such a tremendous performance booster.  And that means that even just pausing speculation will hurt performance.



To determine how much this would hurt, for example, Linux's performance, these guys tweaked Linux's source code to do exactly that, to prevent the processor from speculatively executing down both paths following a synchronization primitive's conditional branch.  What they found was that the effect can be significant.  It introduces a 10.82% overhead when forking a Linux process, 7.53% overhead when executing a new process, 12.35% when deleting an empty 0K file, and 11.37% when deleting an 10K file.  Those are particularly bad.



But overall, as they wrote in their abstract, they measured a 5.13% performance cost overall from their mitigation.  Now, 5.13%, that's huge, and it suggests just how dependent upon synchronization primitives today's modern operating systems have become.  For a small change in one very specific piece of code to have that much impact means it's being used all the time.  



And finally, as for the disclosure of their discovery, they wrote:  "We disclosed Speculative Race Conditions to the major hardware vendors - Intel, AMD, ARM, and IBM - and the Linux kernel in late 2023.  Hardware vendors have further notified other affected software - OS and hypervisor - vendors, and all parties have acknowledged the reported issue, which has been given CVE-2024-2193.



"Specifically, AMD responded with an explicit impact statement which was that existing Spectre-v1 mitigations apply, pointing to the attacks relying on conditional branch mis-speculation like Spectre-v1."  In other words, they're just saying, yeah, this is more of the same nightmare.  If you want to fix it, deal with it the way you do conditional branch misprediction.  Which is to say block speculation before the branch.  The Linux kernel developers wrote back saying they have no immediate plans to implement serialization of synchronization primitives due to performance concerns.  In other words, it slows down Linux too much for them to do this, and no one has yet shown them that they absolutely must.  The bottom line is, it is unclear today what the ultimate solution will be.  And it's clear we don't have one yet.



Stepping back from all of this to look at the whole picture, we have a processor performance-addicted industry that several years ago awoke to the unhappy news that the extremely complex, tricky, and sophisticated way our processors had been arranging to squeeze out every last possible modicum of performance was also inherently exploitable because it opened the processor to side-channel attacks.



It turned out that performance was being enhanced by allowing the processor to dynamically learn from the execution of its past code and data.  Not only could this stored knowledge reveal secrets about the data that had recently been processed, but it was possible for any other software sharing the same hardware to probe for and obtain those secrets.  And that's still where we are today.  No one wants to relinquish the performance we've come to need, but the way we're achieving that performance inherently comes at the cost of security and privacy.



LEO:  Wow.  Very interesting.  So now you understand, everybody, why the speculative execution processors are so problematic.  It's got to be similar to what Apple's experiencing with the M1, M2, and M3 processors.  Same idea.



STEVE:  Yeah.  ARM processors have the same problem.  I mean, we have a problem, which is our DRAM is too slow.  So we've had to cache it.  And processors need to minimize the hits on the cache.  So they desperately need to know as much as they can about what's going to happen in the future.  And that means the only way they can know what's going to happen in the future is if they know what happened in the past.  And knowing what happens in the past means that the past affects their future behavior.  And that's where we get the performance, but that also means that their current behavior is affected by the past, which is why they leak information.



LEO:  Yeah.  Very interesting.  Wow.



STEVE:  It is.  It's a Catch-22.  It's a classic conundrum.  And it's just not clear.  The only thing I could see, and this sort of relates to what we were talking about with the recent Apple problem with their M series chips, is that the cryptographic code needs to turn off that feature...



LEO:  Right.



STEVE:  ...while it's running so in order...



LEO:  Selective slowness.



STEVE:  Exactly.  Exactly.  Selective performance retardation.



LEO:  But in a mission-critical environment, and where you really do want to preserve your integrity.  And you know what, it doesn't make that much of a difference.  I mean, yeah, 5%, 10%.  But these things, as you pointed out...



STEVE:  And next year we'll have processors that are 20% faster.



LEO:  Exactly.  It's not - yeah, yeah.  But I think, you know, the Apple fix is very straightforward.  In fact, Apple's already implemented it.



STEVE:  Yes.



LEO:  It's not as easy to do in Spectre and Meltdown, though, in the x86.



STEVE:  No.  We don't, I mean, all of Intel's cores have it, so it's not a matter of, like, running on a core that doesn't have it.  You can turn it off, you know.  That's what my InSpectre freeware does is allow you to globally shut it down.  But, you know.  But, okay.  And again, we need to maintain some perspective.  If the only risk is hostile software in your system...



LEO:  On your system, exactly.



STEVE:  Yes.  It is sharing - it's a processor sharing problem where you've got, what is it, the evil maid.  You have...



LEO:  Or a server where multiple processes are running in the same...



STEVE:  Well, exactly.  So an end-user working at home who's got a bunch of things that it's just them, it's nothing to worry about, really.



LEO:  Yeah.



STEVE:  But it's in the virtualized environment in the cloud, and you have to, you know, we have to admit that more and more is in the cloud.  You know, we're rushing headlong into the cloud.  So there it does matter.



LEO:  It affects you if your data is encrypted with keys that are held by the server, in the processor on the server, and somebody else is using the same processor because they're also on that shared cloud server.  That is a problem.  Apple, I don't know if it's luck, but Apple seems to have dodged a bullet by not having its efficiency cores use this kind of speculative execution.



STEVE:  Yeah.



LEO:  That was really - worked out well because that could say, well, if you're encryption, use the efficiency cores.



STEVE:  Yeah.  It wasn't actually luck.  It's efficient because it's smaller.  So they ripped out a bunch of things in order to make it small and more efficient.



LEO:  Including that.



STEVE:  Including that.  And that ended up being a good thing.  They're just able to move the crypto over into that core.



LEO:  I know, you know, you're concerned that some of our audience might go, this is way over my head.  But I have to tell you everybody in the Discord was really engrossed, and many of them deal with this.  I mean, this is - we had one person who says, "I work with this, I have to survive this all the time, every day."  Vivi, or Vivi, I'm not sure how you say it, says, "I love this topic.  I deal with this all day long with concurrent network programming."  And that's exactly right.  You have to have locks there, too, yeah.



STEVE:  Yeah.



LEO:  It's really great.  I love this stuff.  And I know our audience does, too.  And thank you, Steve.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#971

DATE:		April 23, 2024

TITLE:		Chat (out of) Control

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-971.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What would you call Stuxnet on steroids?  What's the latest on the Voyager 1 drama?  What new features are coming to Android and Thunderbird?  What's China done now?  Why did Gentoo Linux say "no" to AI?  And after sharing and discussing a bunch of feedback from our terrific listeners and a SpinRite update, we're going to examine the latest update to the European Union's worrisome "Chat Control" legislation, which is reportedly just over a month away from becoming law.  Is the EU about to force the end of end-to-end encryption in order to enable and require the scanning of all encrypted communications?  It appears ready to do just that.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  The latest chapter in the Voyager 1 drama coming up.  We'll talk about the graybeard at Gentoo who says, "No AI in Linux."  About the Hyundai owner whose car really is tracking him.  And then what the EU plans to do with end-to-end encryption.  I can give you a little tip.  It's not good news.  All that coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 971, recorded Tuesday, April 23rd, 2024:  Chat (out of) Control.



It's time for Security Now!, the show where we cover the latest security, privacy, Internet updates, even occasionally some good books and movies, with this guy right here, Steve Gibson, the security guy in chief.  Hello, security guy.



STEVE GIBSON:  Yeah.  And, you know, where are the good movies, Leo?



LEO:  I know.  	



STEVE:  I mean, like, we used to have a lot of fun.  I did see that you're doing the Bobiverse with Stacey's book club.



LEO:  Yes, Thursday, yes.



STEVE:  She was chagrined when you reminded her.  She said, "Ooh, I forgot to read that."



LEO:  Yeah.



STEVE:  So of course it'll take her an hour.



LEO:  It's pretty quick, yeah.



STEVE:  And my wife apparently just glances at pages when she - like I see her, like with eBooks, I said, "Can I test you on the content of this after you're through, like?"  And she says, "Oh, I'm getting most of it.  It's fine."



LEO:  Did she go to Evelyn Wood's Speed Reading Academy?



STEVE:  Apparently.  Or she's in some sort of a time warp.  I don't know.  Because, I mean, I'm an engineer.  I read every word.



LEO:  Yes. 



STEVE:  And in fact that's why Michael McCollum was sending me his books before publication because it turns out I'm a pretty good proofreading editor because I spot every mistake.  Of course not my own.  So other people's.  Much easier to see.



LEO:  Yes.  It's really true.  Same with code; right?  You know there's a bug in there, you can stare at it till the cows come home, and you go...



STEVE:  Absolutely.  One of the neatest things that the GRC group has done for me, our SpinRite news group, is, you know, this thing, basically it's not - we're not having any problems.  Thousands of people are coming onboard with 6.1, and it's done.  



LEO:  Isn't that a nice feeling?



STEVE:  We're not, you know, I'm not chasing bugs around.



LEO:  Oh, that's nice.



STEVE:  You know, it is, it is good.



LEO:  You've got a great bunch of beta testers.



STEVE:  That's in control.  We're going to talk about what is out of control.  Today's title is "Chat (out of) Control" for Security Now! Episode 971, for the second-to-last episode of the month of April, which becomes important because of what's going to happen in June.  But anyway, I'm getting all tangled up here.  We're going to talk about a lot of fun things, like what would you call Stuxnet on steroids?  What's the latest on the Voyager 1 drama?  We've got even more good news than we had last week.



What new features are coming to Android, probably in 15 - we're not sure, but probably - and also in Thunderbird this summer?  What's China gone and done now?  Why did Gentoo Linux say "no" to AI; and, like, what's that all about?  And after sharing and discussing a bunch of feedback - because there wasn't a huge bunch of, like, really gripping news, but we had a lot of feedback from our listeners that we're going to have fun with - and a brief little update on SpinRite, we're going to examine the latest update to the European Union's quite worrisome "Chat Control" legislation, which is reportedly just over a month away from becoming law.



Is the EU about to force the end of end-to-end encryption in order to enable and require the scanning of all encrypted communications for the children?  And it appears ready to do just that.  This latest update, it came onto my radar because somebody said that the legislators had excluded themselves from the legislation.



LEO:  Of course.  Of course.



STEVE:  Well, and so I got this 203-page tome, and its Section 16a was in bold because it just got added.  Anyway, we'll talk about it.  I think that the person doing that speaking that caught my attention, and I'm glad he caught my attention, but he was overstating the case in order to make a point.



LEO:  Oh, okay.



STEVE:  But the case that we have doesn't need overstating because it looks really bad.  You know, there's no sign of exclusion like the UK gave us on their legislation in September which said "where technically feasible."  That's completely missing from this.  So anyway, I think we have a lot of fun to talk about, fun things to talk about.



I did make sure that the pictures showed up this week in Apple devices.  What's interesting is I have an older 6 - I think it's a 6, or maybe it's a 7 or 8, I don't know.  Anyway, the pictures all work there.  Even last week's pictures worked there.  But not on my iPhone 10.  So Apple did in fact change the rendering of PDFs which caused some problem, some incompatibility.  Anyway, I don't know why it was last week but not this week.  We're all good to go this week.  So even Mac people can see our Picture of the Week.  Which is kind of fun.  So lots of good stuff.



LEO:  I can see it, so I verify that it does in fact work.  Oh, that's good news.  Now I believe it's time for a Picture of the Week.



STEVE:  Yeah.  So this just caught my attention because lately  I've been seeing, as I'm sure our listeners have, so much of this.  You know, AI everything.



LEO:  AI, AI, AI, yup.



STEVE:  AI everywhere.  So the picture shows a couple of young upstarts in a startup venture who are - they've got some ideas for some product that they want to create.  And one of the things that happens when you're going out to seek financing and funding, you're typically going and giving presentations to, like, venture capital firms, and explaining what you're going to do and how you're going to do it.



And so PowerPoint presentations are put together, and they're called "pitch decks" because you're making a pitch to whomever you're explaining your ideas to.  So we see in this picture two guys facing each other, each behind their own display, one of them saying to the other, "Can you go through all the old pitch decks and replace the word 'crypto' with 'AI'?"  And of course the point being that we were just, what was it, like a year ago, Leo, that everything...



LEO:  It's just a catchphrase, yeah.



STEVE:  Yeah, exactly.  I mean, it was, like, time must be accelerating because it was just so recently that everything was blockchain this and blockchain that; crypto currency, you know, crypto this and that.  And so, no, that's all yesterday.  That's so, what do they say, so last minute or something?  Anyway, now it's AI.  So, yes.  And we have a couple things during this podcast that will be touching on this, too.  So anyway, just, you know, not a fantastic picture, but I thought it was just, like, so indicative of where we are today.



I've been dealing with Bing.  I don't know why I've been launching it, but it's been launched a few times in the last week.  And, you know, Microsoft...



LEO:  Because you use Windows, that's why.



STEVE:  Oh, that would definitely...



LEO:  They do everything they can to get Bing in your face.



STEVE:  Oh, my god, yes.  And so it's like, no, I don't want this.  And also it's, for me, since I'm not normally using Edge or Bing, it's like, okay, how do I close this?  It looks like it takes over the whole UI, and very much like that old, you know, when people were being forced to upgrade to Windows 10 against their will, where for a while it said, "No, thank you," and then it changed to "Later tonight."  So it's like, wait a minute.  What happened to not at all, never ever?  You know, it's like you want to do it now, or do you want to do it in an hour?  Uh, wait.  Those are my only two options?



Anyway, okay.  So as we know, Security Now! is primarily an audio podcast.  But even those watching, though it remains unclear to me why anyone would, don't have the advantage of looking at my show notes.  If anyone were to be reading the notes, they would see that the spelling of the name of this new attack is far more, shall we say, acceptable in polite company than the attack's verbal pronunciation.  But this is an audio podcast, and the story of this attack that I very much want to share refers to the attack by name.  And that name, which rhymes with "Stuxnet" is spelled "F-U-X-N-E-T."  And there's really no other way to pronounce it than just to spit it out.  But I'm just going to say Fnet for the sake of the children.



LEO:  Thank you, Steve.  Thank you.



STEVE:  Because, yes, you know.  So it's not really an F-bomb.  But it's audibly identical, and there's no point in saying it.  Everybody understands how you would pronounce F-U-X-N-E-T.  Which is what the Ukrainians named the weapon which they reportedly, and this was confirmed by an independent security company, successfully launched into the heart of Russia.



So with that preamble and explanation, let's look at the very interesting attack that was reported last week by Security Week.  Their headline, which also did not shy away from using the attack's name, said "Destructive ICS Malware 'Fuxnet' used by Ukraine Against Russian Infrastructure."  So here's what we learn from what they wrote.  They said:  "In recent months, a hacker group named Blackjack, which is believed to be affiliated with Ukraine's security services" - so, you know, as in state-sponsored - "has claimed to have launched attacks against several key Russian organizations.



"The hackers targeted ISPs, utilities, data centers, and Russia's military, and allegedly caused significant damage and exfiltrated sensitive information.  Last week, Blackjack disclosed the details of an alleged attack aimed at Moscollector (M-O-S-C-O-L-L-E-C-T-O-R), a Moscow-based company responsible for underground infrastructure, meaning things like water, sewage and communication systems."



So, quoting, they said:  "Russia's industrial sensor and monitoring infrastructure has been disabled."  So said the hackers.  "It includes Russia's Network Operation Center that monitors and controls gas, water, fire alarms, and many others, including a vast network of remote sensors and IoT controllers. 



"So the hackers claimed to have wiped database, email, internal monitoring, and data storage servers.  In addition, they claimed to have disabled some 87,000" - 87,000 - "sensors, including ones associated with airports, subway systems, and gas pipelines.  To achieve this, they claimed to have used Fuxnet, a malware they described as 'Stuxnet on steroids,' which enabled them to physically destroy sensor equipment."



You know, our longtime listeners and anybody's who's been in, you know, around IT will recall that Stuxnet was a previous, also physically destructive malware.  I guess we have to call it malware, even though we were apparently part of - the U.S. participated, or U.S. intelligence services was involved in its creation.  It caused the centrifuges used in Iran to overspin and essentially self-destruct.  So those were being used to enrich uranium at the time.  Anyway, so that's why they're calling this thing "Stuxnet on steroids" is that they worked to cause actual physical damage, as we'll see in a second, to hardware.



LEO:  There's a big difference, though, between destroying centrifuges which have one purpose, which is enriching uranium, and destroying sensors which prevent gas leaks and, I mean...



STEVE:  Yeah, yeah.



LEO:  I mean, this is a civilian attack.  Finish the story, but I would love to talk at the end of it about how you feel about this.



STEVE:  Good.  And I agree with you.  So they wrote:  "Fuxnet has now started to flood the RS485/MBUS and is sending random commands to 87,000 embedded control and sensory systems."  And they did say "(while carefully excluding hospitals, airports, and other civilian targets)."  Now, they said that.  So, you know, they share some of our sensitivity to that.  And I do question, you know, given that they're also claiming 87,000-some sensors, how they can be that careful about what's, you know, what they've attacked and what they haven't.



Anyway, the report goes on, saying:  "The hackers' claims are difficult to verify, but the industrial and enterprise IoT cybersecurity firm Claroty was able to conduct an analysis of the Fuxnet malware based on information and code made available by Blackjack.  Claroty pointed out that the actual sensors deployed by Moscollector, which are used to collect physical data such as temperature, were likely not themselves damaged by Fuxnet.  Instead, the malware likely targeted roughly 500 sensor gateways."



So, right?  So the idea is that the gateway is a device out located remotely somewhere, and it has RS485 lines running out to a ton of individual sensors.  So it's the sensor data collector and forwarding device.  So the malware targeted around 500 of these sensor gateways, which communicate with the sensors over a serial bus such as RS485 or Meter-Bus that was mentioned by Blackjack.  These gateways are also connected to the Internet to be able to transmit data to the company's global monitoring system.  So that was probably the means by which the Fuxnet malware got into the sensor gateways.



"Claroty notes:  'If the gateways were indeed damaged, the repairs could be extensive given that these devices are spread out geographically across Moscow and its suburbs, and must be either replaced or their firmware must be individually reflashed.'  Claroty's analysis of Fuxnet showed that the malware was likely deployed remotely.  Then, once on a device, it would start deleting important files and directories, shutting down remote access services to prevent remote restoration, and deleting routing table information to prevent communication with other devices.



"Fuxnet would then delete the file system and rewrite the device's flash memory.  Once it has corrupted the file system and blocked access to the device, the malware attempts to physically destroy the NAND memory chip and then rewrites the UBI volume to prevent rebooting.



"In addition, the malware attempts to disrupt the sensors connected to the gateway by flooding their serial communications channels with random data in an effort to overload the serial bus and sensors, essentially performing an internal DoS attack on all the devices the gateway is connected to."  And I'll argue that if these are not sensors, but these are actuators, as you said, Leo, this could be causing some true damage.  I mean, like true infrastructure [crosstalk] damage.



LEO:  Well, they said subway systems, airports, gas pipelines.



STEVE:  Yeah.  "Claroty explained:  'During the malware operation, it will repeatedly write arbitrary data over the Meter-Bus channel.  This will prevent the sensors and the sensor gateway from sending and receiving data, rendering the sensor data acquisition useless.  Therefore, despite the attackers' claim of physically destroying 87,000 devices,' wrote Claroty, it seems that they actually managed to infect the sensor gateways and were causing widespread disruption by flooding the Meter-Bus channel connecting the sensors to the gateway, similar to network fuzzing the different connected sensor equipment.  As a result, it appears only the sensor gateways were bricked, and not the end-sensors themselves.'"



So, okay.  I particularly appreciated the part about attempting to physically destroy the gateway's NAND memory chip.  Because it could happen.  As we know, NAND memory is fatigued by writing because writing and erasing, which needs to be part of writing,  is performed by forcing electrons to tunnel through insulation, thus weakening its dielectric properties over time.  So the attacking malware is likely writing and erasing and writing and erasing the NAND memory over and over, as rapidly as it can.  And since such memory is likely embedded into the controller and is probably not field replaceable, that would necessitate replacing the gateway device, and perhaps all 500 of them spread across Moscow and its suburbs.



And even if the NAND memory was not rendered unusable, the level of destruction appears to be quite severe.  Wiping stored data and directories and killing the system's boot volume means that those devices probably cannot be remotely repaired.  Overall, I'd have to say that this extremely destructive malware was well named.



And we live in an extremely, and increasingly, cyber-dependent world.  Everyone listening to this podcast knows how rickety the world's cybersecurity truly is.  So I shudder at the idea of any sort of all-out confrontation between super powers.  I don't want to see that.



LEO:  Do you think there should be a, I don't know, Geneva Convention-style accord between nations about cyberwarfare?  I mean, it's - the problem is you can do it, but then you're just going to escalate.  It's going to go back and forth.  Which is why we decided, for instance, not to allow bioweapons.  Now, they still get used.  But it's, you know, the civilized world agrees not to use biologic weapons in war.  



STEVE:  Well, and the feeling is, of course, that COVID was a lab escape.  Right?



LEO:  Well...



STEVE:  I mean...



LEO:  There's some evidence, but not a lot.



STEVE:  There's no evidence.



LEO:  That's a question, yeah.  It wasn't a very good, it wasn't a very effective warlike attempt since it killed far more people in China than it did elsewhere.  But anyway...



STEVE:  It was clearly a mistake.



LEO:  Yes.  It wasn't intentional.  So what do you think?  I mean...



STEVE:  So I agree with you.  The problem is it's tempting because it doesn't directly hurt people; right?  I mean, so like right now we're in a cold war.  We're constantly on this podcast talking about state-sponsored attacks.  Well, those are attacks.



LEO:  Yeah.  And especially infrastructure attacks.



STEVE:  Yes.



LEO:  Which this was.



STEVE:  Yes.  I mean, the whole Colonial Pipeline thing; you know?



LEO:  Right.



STEVE:  That really damaged the U.S.  And, I mean, it was a true attack.  So, you know, and we just talked about how China was telling some of their - China told their commercial sector, you need to stop using Windows.  You need to stop using, you know, this Western computer technology because the West is able to get into it.  So that was the first indication we really had that, as I put it at the time, that we're giving as well as we're getting.  Unfortunately, this is all happening.  I mean, I wish none of it was happening.  But the problem is security is porous.  And I guess the reason a nuclear weapon and a bioweapon are unconscionable, you know, is that they are so tissue damaging, for lack of a better word.  I mean, they really - they're, like, really going to kill people.



Whereas, eh, a network got breached, whoops.  You know, I mean, it doesn't have the same sort of visceral grip.  And unfortunately, here's an example.  And I'm glad you brought it up, Leo.  Ukraine, sympathetic as we can be for their situation, this was a blunt-edged attack; right?  I mean, this was, you know, sewage and water and gas and airports and, you know, I mean, it's - they couldn't have controlled what damage was caused.  And, you know, you mess up water and sewage, and you're really hurting actual people who are innocent of what, you know...



LEO:  Or subways.  Or airports.  Or gas pipelines.  I don't know what the answer is.  I mean, I'm no fan of Putin.  He brought the war upon himself.  But hurting civilians, I don't know, this is not a good situation.



STEVE:  It's the world we're in.



LEO:  It's the world we're in.



STEVE:  Yeah.  And it is technology we created.  I mean, you know, oh, let's have the password be admin/admin because we don't want people calling us and asking what the password is.  Or, I mean, it's like we've made so many bad decisions.  And while we're now making them better today, we have seen how long the tail of inertia is.  I mean, it's, you could argue, infinite.  You know?  We still have Code Red and Nimda out there, you know, sending packets out.  Somewhere there's an NT machine just hoping to find something that it can infect.  When is it going to die?  I don't know.



We have another update on Voyager 1.  Apparently, if Voyager is not going to give up on us, we're not going to give up on it.  But remember that, no matter what, Voyager is deriving all of its diminishing operating power from the heat being generated by the decay of radioisotopes.  And through the years and now decades, since this thing left Earth in '73, those isotopes are continuing to put out less and less heat.  And thus Voyager has less and less energy available to it.  So it can't go forever.  But it, you know, it amazes everybody that it's gone as long as it has, and it is still going.



What equally amazes me is the intrepid group of well-past-their-retirement engineers who are now endeavoring to patch the code of this ancient machine that's 22.5 light hours away.



LEO:  Oh, my god.  It's amazing.



STEVE:  It boggles the mind.



LEO:  It's so amazing.



STEVE:  Just yesterday, on April 22nd, JPL, NASA's Jet Propulsion Laboratory, posted the news under the headline "NASA's Voyager 1 Resumes Sending Engineering Updates to Earth."  They wrote:  "After some inventive sleuthing, the mission team can  for the first time in five months  check the health and status of the most distant human-made object in existence.  For the first time since November, NASA's Voyager 1 spacecraft is returning usable data about the health and status of its onboard engineering systems.



"The next step is to enable the spacecraft to begin returning science data again.  The probe and its twin, Voyager 2, are the only spacecraft to ever fly in interstellar space, the space between the stars.  Voyager 1 stopped sending readable science and engineering data back to Earth on November 14th, 2023, even though mission controllers could tell the spacecraft was still receiving their commands and otherwise operating normally.  In March, so last month, the Voyager engineering team at NASA's Jet Propulsion Laboratory in Southern California confirmed that the issue was tied to one of the spacecraft's three onboard computers, called the flight data subsystem (FDS).  The FDS is responsible for packaging the science and engineering data before it's sent to Earth.



"The team discovered that a single chip responsible for storing a portion of the FDS's memory  including some of the FDS computer's software code  is no longer working.  The loss of that code rendered the science and engineering data unusable.  Unable to repair the chip" - right, after 22.5 light, what is it, days, light days away - "the team decided to place the affected code elsewhere."  They're relocating the code, Leo, at this distance.



LEO:  I know, it's amazing.



STEVE:  On a probe built in, or launched in '73.



LEO:  How cool.



STEVE:  It's, you know, it's insane.  "But," they said, "no single location is large enough to hold the section of code in its entirety."  So they're having to fragment it.  



"They devised a plan to divide the affected code into sections and store those sections in different places in the FDS.  To make this plan work, they also needed to adjust those code sections to ensure, for example, that they all still function as a whole.  Any references to the location of that code in other parts of the FDS memory need to be updated, as well."  So they're relocating and then patching to relink the now-fragmented code sections so that they jump to each other.  It's, you know, dynamic linking in a way that was never designed or intended.



They wrote:  "The team started by singling out the code responsible for packaging the spacecraft's engineering data. They sent it to its new location in the FDS memory on April 18th.  A radio signal takes about 22.5 hours to reach Voyager 1, which is now over 15 billion" - with a "b" - "miles from Earth, and another 22.5 hours" - hours, not days - "for a signal to come back to Earth.  When the mission flight team heard back from the spacecraft on April 20th, they saw that the modification worked.  For the first time in five months, they have been able to check the health and status of the spacecraft.



"During the coming weeks, the team will relocate and adjust the other affected portions of the FDS software.  These include the portions that will start returning science data," rendering the satellite again back to doing what it was designed to do, which is using its various sensor suites and sending back what it's seeing and finding out in interstellar space, which as I mentioned previously has surprised the cosmologists because their models were wrong.  So Voyager 1 is saying, uh, not so fast there.  Nice theory you've got, but it's not matching the facts.  Wow.



LEO:  Yay, V'Ger.



STEVE:  Yeah.



LEO:  And yay, those brilliant scientists who are keeping her alive.



STEVE:  Oh, and Leo, I did take - Lorrie and I watched "It's Quieter in the Twilight."



LEO:  Oh, yeah, "It's Quieter in the Twilight," yeah.



STEVE:  And what was interesting was that this announcement, and it was picked up in a few other outlets, showed a photo of the event where the team were gathered around their conference table.  I recognized them.



LEO:  Yes.



STEVE:  From the documentary.



LEO:  It's the same people since 1974.



STEVE:  Yeah, exactly.  They're all still there.  And in fact some of them don't look like they've changed their clothes, but that's what you get, you know, with old JPL engineers.



LEO:  Oh, I just love it.  It's such a great, wonderful story.  It really is.



STEVE:  Let's take a break.  I'm going to catch my breath, and then we're going to talk about changes coming to Android 15 and Thunderbird.



LEO:  Yes.  All right.  As we continue with the best show on the podcast universe, 22 light hours ahead of everyone else.  On we go.



STEVE:  Okay.  So there's not a lot of clear information about this yet, but Google is working on a new feature for Android which is interesting.  They're going to start watching their apps' behavior.  It will place under quarantine any applications that might sneak past its Play Store screening, only to then begin exhibiting signs of behavior that it deems to be malicious.  The apps will reportedly have all their activity stopped, all of their windows hidden, and notifications from the quarantined apps will no longer be shown.  They also won't be able to offer any API-level services to other apps.



The reports are that Google began working on this feature during Android 14's development last year, and that the feature is expected to finally appear in forthcoming Android 15.  But we don't have that confirmed for sure.  So, you know, there wasn't - I wasn't able to find any dialogue or conjecture about why the apps aren't just removed.  Maybe - oh, and that they do still appear to be an app installed on the phone.  They're not hiding it from the user.  They're just saying, no, you bad app, we don't like what you've been doing.



Maybe it reports back to the Play Store, and then Google takes a closer look at the app which is in the Play Store, which of course is how the user got it, and then says, oh, yeah, we did miss this one.  And at that point it gets yanked from the Play Store and yanked from all the Android devices.  So it could just be like essentially functioning as a remote sensor package.  Anyway, I'm sure we'll learn more once it becomes official, hopefully in this next Android 15.



Also this summer, Thunderbird will be acquiring support for Microsoft Exchange email for the first time ever.  It will only be email at first.  The other Exchange features of Calendar and Contacts are expected to follow at some later date, although Mozilla's not saying for sure.  Now, I happen to be a Thunderbird user.  I was finally forced to relinquish the use of my beloved Eudora email client once I began receiving email containing extended non-ASCII character sets that Eudora was unable to manage.  I got these weird capital A with little circles above them things in my email, which was annoying, instead of line separators.



At the same time I have zero interest in Exchange.  GRC runs a simple and straightforward instance of a mail server called hMailServer which handles traditional POP, IMAP, and SMTP, and does it effortlessly with ample features.  But I know that Exchange is a big deal, and obviously Mozilla feels that for Thunderbird to stay relevant, it probably needs to add support for Exchange.



In any event, to support this rather massive coding effort, in Mozilla's reporting of this they mentioned that it had been 20 years since - because, you know, email is kind of done - 20 years since any code in Thunderbird dealing with email had been touched.  They've just been, you know, screwing around with the user interface.  And that during those 20 years a lot of, as they put it, institutional knowledge about that code had drained.  So they've decided now that they're going to recode in Rust.  Rust is their chosen implementation language.  And they did so for all the usual reasons.



They cited memory safety.  They said:  "Thunderbird takes input from anyone who sends an email, so we need to be diligent about keeping security bugs away."  Performance:  "Rust runs as native code with all the associated performance benefits."  And modularity and ecosystem.  They said:  "The built-in modularity of Rust gives us access to a large ecosystem where there are already a lot of people doing things related to email which we can benefit from," they said.  So anyway, for what it's worth, you know, Thunderbird is a strong client for, you know, from Mozilla.  Is it multiplatform, Leo, do you know?  Is Thunderbird Windows-only?  Or Mac and Linux?  I don't know either way.



Anyway, China.  The Chinese government has ordered Apple to remove four Western apps from the Chinese version of the Apple App Store.  Those are Meta's new social network Threads, which is now gone; Signal; Telegram; and WhatsApp, all removed from the Chinese App Store.  China stated that they have national security concerns about those four.  And as we've seen, and as I fear we'll be seeing shortly within the EU, what countries request, countries receive.  Technology is ultimately unable to stand up to legislation.  And this is going to cause a lot of trouble, as I mentioned, in the EU.  We'll be talking about that here at the end of the podcast.



LEO:  Yeah.  And I think the Chinese government's removing it for the same reason the EU wants to remove it.  They don't like end-to-end encryption.



STEVE:  Yes, exactly.



LEO:  Threads is something else.  But Signal and Telegram and WhatsApp, that's all E2E encryption.



STEVE:  Yeah.



LEO:  By the way, to answer your question, I was down the hall.  Thunderbird is Mac, Windows, Linux.



STEVE:  Okay.



LEO:  It's completely open source and everywhere.



STEVE:  Very cool.



LEO:  Nice program.



STEVE:  In that case, it will have access to Exchange Server, which may allow it to move into a corporate environment, which is probably what they're thinking.



LEO:  Which would be great, yes.



STEVE:  Yeah.



LEO:  That would be great, yeah.  Well, we'll see if Microsoft does it.  Oh, no, they're going to do it.  Oh, cool.



STEVE:  Oh, yeah.  You mean Mozilla.



LEO:  I just wish they'd kill Exchange Server.



STEVE:  And, I mean, just get out of the Exchange Server, like...



LEO:  I wish Microsoft would kill Exchange.  That's been a problem since forever.



STEVE:  Yeah, yeah.  Since it was created.



LEO:  Exactly, yeah.



STEVE:  And do you think that China's move is like in response to TikTok and what's happening here in the U.S.?



LEO:  Well, I mean, we had a discussion on MacBreak about that.  The Times says it's because nasty things were said on those platforms about Xi Jinping.  Which is possible.  There's no corroboration of that.  Apple says no.  I think it's just that they were - Threads, maybe that would be because Threads doesn't have any, you know, it's just a social network.  But for sure they don't want - I think Threads is being killed probably because of TikTok.  Andy I think pointed out that it happened immediately after the TikTok ban was approved in the House.  



STEVE:  Ah.



LEO:  So it's likely, by the way, that that this time will be approved in the Senate because it's part of a foreign aid package.



STEVE:  Right.



LEO:  So get ready to say goodbye to TikTok.



STEVE:  Wow, Leo.  That'll be an event, won't it.



LEO:  I don't think - I think the courts will block it.  I'm hoping they will, but I don't know.  It's a very weird thing.  They have a year and a half to do it.



STEVE:  Well, and, I mean, here, you know, we were talking about a cold war.  And there's this, you know...



LEO:  There's an economic cold war, absolutely, yeah.



STEVE:  Right.  And China understandably is uncomfortable about Western-based apps using encryption that they're unable to compromise.



LEO:  Right, right.



STEVE:  So, I mean, I get it.  You know?  And so it's sort of like we lived through this brief period where, you know, there was global encryption and privacy, and everybody had apps that everybody could use.  And then barriers began getting erected.  Right?  I mean, so, sorry.  If you're Chinese, you've got to use China Chat.



LEO:  Well, and the numbers for these particular apps in China are pretty low.  We're talking hundreds of thousands of users, not millions.



STEVE:  Ah, okay.



LEO:  Or billions, yeah.



STEVE:  Okay.  So not a huge actual impact.



LEO:  I think it's an easy thing for them to do, yeah.



STEVE:  Okay.  So this was interesting.  I'll just jump right in by sharing the posting to the Gentoo mailserve.  This was posted by a longstanding, since 2010, so 14 years of involvement, and he's very active, Gentoo developer and contributor.  He wrote:  "Given the recent spread of the 'AI' bubble" - and he has AI in quotes, like, everywhere, so he's obviously immediately exposed himself as not being a fan.



LEO:  Gentoo, you should understand, is the ultimate graybeard Linux; okay?  That's all you need to know.



STEVE:  That totally explains it, yes.  "Given the recent spread of the 'AI' bubble, I think we really need to look into formally addressing the related concerns."  Oh.  He says:  "In my opinion, at this point the only reasonable course of action would be to safely ban 'AI'-backed contribution entirely.  In other words, explicitly forbid people from using ChatGPT, Bard, GitHub Copilot and so on, to create ebuilds, code, documentation, messages, bug reports and so on for use in Gentoo.  Just to be clear, I'm talking about our original content.  We can't do much about upstream projects using it."



Then he says:  "Here's the rationale:  One, copyright concerns.  At this point, the copyright situation around generated content is still unclear.  What's pretty clear is that pretty much all LLMs (Large Language Models) are trained on huge corpora of copyrighted material, and the fancy 'AI' companies don't care about copyright violations.  What this means is that there's good risk that these tools would yield stuff we cannot legally use.  Two, quality concerns.  LLMs are really great at generating plausible-looking BS."  And he didn't actually say "BS," but I changed it for the podcast.



LEO:  He's cranky.



STEVE:  Oh, he is.



LEO:  That's what happens when you have to compile every piece of software you use from scratch.  It makes you cranky.



STEVE:  Right.



LEO:  It makes you cranky.



STEVE:  That's right.  "I suppose," he says, "they can provide good assistance if you are careful enough, but we can't really rely on all our contributors being aware of the risks."  Then there's ethical concerns, number three.  "As pointed out above, the 'AI' corporations care about neither copyright nor people."



LEO:  That's probably true.



STEVE:  Yeah.  "The 'AI' bubble is causing huge energy waste.  It's giving a great excuse for layoffs and increasing exploitation of IT workers.  It is driving the further" - and here I felt I had to use the word because it is now become a common word.  And I think I've heard it on the TWiT Network.



LEO:  Yeah, we allow this word.



STEVE:  "The 'enshittification' of the Internet.  It is empowering all kinds of spam and scam."  And that is the case.  "Gentoo has always stood," he concludes, "as something different, something that worked for people for whom mainstream distros were lacking.  I think adding 'made by real people' to the list of our advantages would be a good thing.  But we need to have policies in place to make sure that AI-generated crap" - and again, not the word he chose - "doesn't flow in."



LEO:  I like this guy.  He's right.  I think that's fair.  Did you see the study from the University of Illinois in Urbana-Champaign?



STEVE:  No.



LEO:  They used ChatGPT 4, the latest version of OpenAI's model.  They gave it the CVE database.  That's it.  Nothing more than the description and the CVE.  And it was able to successfully attack 87% of those vulnerabilities.  It was able to craft an attack based merely on the CVE description, an effective attack.



STEVE:  Wow.  Wow.



LEO:  Yeah, I mean, I think he's probably right.  But I don't know how you enforce this because...



STEVE:  No.  That's exactly the problem.



LEO:  Yeah.



STEVE:  In his posting, he had a link, he referred to some, we'll call it a "crap storm," over on GitHub.  And I went, I followed the link because I was curious.  There is a problem underway where what is clearly AI-generated content which, you know, looks really reasonable, but doesn't actually manage to get around to saying anything, is like becoming a problem over on GitHub.



LEO:  Yeah. 



STEVE:  So anyway, in order to share that, as we saw, I had to clean up the language in his posting since he clearly doesn't think much of AI-generated code.  And as I said, there have been some signs over on GitHub, which he referred to, of descriptions appearing to be purely AI-generated.  You know, they're not high quality.  And I suppose we should not be surprised, Leo, that there are people, maybe we'll call them "script kiddies," who are probably incapable of coding from scratch for themselves.  So why wouldn't they jump onto Large Language Model systems, which would allow them to feel as though they're contributing.  But are they really contributing?



LEO:  Now, look, let's face it.  Humans are just as capable of introducing bugs into code as anybody else, and more often maliciously than AIs.  I mean, AIs aren't natively malicious.  The other thing I would say is there is a lot of AI-generated prose on GitHub because English is often not the first language of the people doing the coding.  A lot of repositories on GitHub are by non-English speakers.  And I think that that's more likely the reason you'll see kind of AI-like prose on there because they don't speak English that well, or at all.  And so they're using ChatGPT, for instance, to generate the text.



STEVE:  Right.



LEO:  Human, I don't think, I mean, honestly, I've used Copilot.  I have my own custom GPT for LISP.  The code it generates is indistinguishable from human code, probably because it is, at some point, from human code.



STEVE:  Right, right.



LEO:  I don't know how you're going to stop it.  It's not - doesn't have a big red flag that says an AI generated this.



STEVE:  Right.  And as we've noted, the genie is out of the bottle already.  So, yeah.  We are, we're definitely in for some interesting times.



Okay.  We've got a bunch of feedback that I found interesting, that I thought our listeners would, too.  Let's take another break, and then we will get into what - what was this one?  Oh, oh.  We have a listener whose auto was spying on them, and he's absolutely sure it never had permission.  And we have a picture of the report that it generated.



LEO:  We here at Nissan see that you've been using your vehicle for lovemaking.  And we want you to knock it off.  But we'll find out more about that in just a second.  But first, a word.



STEVE:  And apparently you're doing it wrong.



LEO:  You're doing it wrong.  We have some tips we'd like to share.  Steve, let's close the loop.



STEVE:  So we have a note from a guy who is no slouch.  He's a self-described user of ShieldsUP!, SpinRite, and an avid listener of Security Now!.  He's also an Information Security Practitioner, and I think he said a Computer Geek.  Oh, yeah, he does.  So he said:  "Hi, Steve.  I apologize for sending to this email" - probably came through Sue or Greg - "as I couldn't find a different email for contact information."  And yes, that's by design, but okay.  He said:  "Anyway, longtime follower of ShieldsUP! and SpinRite, and an avid listener of Security Now!.  My full-time gig is an InfoSec Security Practitioner/ Computer Geek.



"We have a couple of Hyundais in the family, and I purchased one last fall.  I use the Hyundai Bluelink app on my phone, as I can make sure I locked my doors, and get maintenance reminders.  I made a point to NOT opt-in for the [he has in quotes] 'driver discount'; and, as a privacy cautious person, I decline sharing data wherever possible.  But after the story in The New York Times regarding carmakers sharing data, I contacted Verisk and LexisNexis to see what they had on me.



"LexisNexis had nothing other than the vehicles I have owned in the past, but Verisk had a lot.  I have attached a page of the report.  It includes driving dates, minutes (day and night), acceleration events, and braking events.  The only thing missing is the actual speeds I was going, or if I was ever speeding.



"What bothers me most about this is that I have no way to challenge the accuracy.  For events that are not illegal, I can still be penalized.  Braking hard and accelerating fast should not be safety concerns without context.  And today's smarter cars are still imperfect.  My adaptive cruise control" - he has in parens (radar) - "will still brake hard at times it shouldn't, and I will get penalized by that data.  My car is also a turbo, and if I accelerate for fun or safety, that too can be a penalty.  And if I happen to drive in Texas where there are highways with an 85 MPH speed limit, I would be down-rated for that legal behavior.



"My family tried the 'safe driving' BT dongles from another insurer years ago, but the app had too many false positives for driving over speed" - he said posted speed limit doesn't agree with the app - and hard braking and accelerating, that we decided it wasn't worth our time or the privacy concerns.  My wife and I are close to Leo's age, and she drives like a grandmother, but her scores were no better than mine.



"I have attached a picture of the document I got from Verisk," he says, "name and VIN Number removed, to give you an idea of what is reported without my consent from my car.  I've contacted Hyundai and told them I do not and did not consent to them sharing my data with Verisk.  After a few back and forths, I got this reply on April 12th:  'Thank you for contacting Hyundai Customer Care about your concerns.  As a confirmation, we have been notified today that the driver's score feature and all data-collecting software has permanently disabled.  We do care. As always, if you ever need additional assistance, you can do so either by email or phone.  Case number....'"



So he said:  "I will request another report from Verisk in the future to validate this report from Hyundai.  Keep up the good work.  I thought you would like to see the data and hear from someone who is 100% certain they never opted in.  All the best, Andrew."



And in the show notes, sure enough, we've got the page with a report from the period September 26th of last year through March 25th of this year.  So just last - toward the end of last month, showing things like the number of trips, vehicle ignition on to ignition off was 242 instances.  Speeding events where the vehicle speed is greater than 80 miles per hour has an NA.  Hard braking events, where they say change in speed is less than, because it's braking, negative 9.5 kph per second is 24.  So during that period of time what the car regarded as a hard braking event occurred 24 times.



Rapid acceleration events, change in speed is greater than 9.5 kph per second is 26.  Daytime driving minutes between the hours of 5:00 a.m. and 11:00 p.m., 6,223.  Nighttime minutes, actually very few between 11:00 p.m. and 5:00 a.m., just 25 minutes.  Miles driven, 5,167.6 miles during this period.  And then an itemized daily driving log showing the date, the number of trips taken that day, the number of speeding events, the number of hard braking events, rapid acceleration events, driving minutes both daytime and nighttime.



So, yes, just to close the loop on this, as we first talked about from The New York Times reporting which informed us that both this Verisk and LexisNexis were selling data to insurers, and as a consequence those insurers were relying on that data to set insurance premium rates.



LEO:  And look what it says at the bottom:  "This report may display driving data associated with other individuals that operated insured's vehicle."



STEVE:  Yup.



LEO:  So my guess, this is a report for an insurance company; right?



STEVE:  Right, right.



LEO:  Whether he agreed to it or not, it may be that he can turn off some things, like I noticed that speeding events is N/A all the way through.  Either he's a really careful driver, or they're not recording that.  Which may well be something he didn't agree to; right?



STEVE:  Yup.  So anyway...



LEO:  I know my BMW records that because I have it on my app.  And my Mustang used to give me a report card after every trip.



STEVE:  Right.  And, I mean, compared to the way I used to drive when I was in my younger years...



LEO:  Yeah, good information, yeah.



STEVE:  I would be happy to have my insurance company privy to the fact that I drive about three miles a day at 60 miles an hour, surrounded by other traffic.  I mean, it's just, you know.



LEO:  Here's my driving performance for the month of March.



STEVE:  And in fact Lorrie added me to her car insurance, and her rate went down.



LEO:  Yes, exactly.  Because you're safe; right?



STEVE:  Yeah.



LEO:  This is more because it's an EV.  You want to know a little bit about - reason you want to know about hard braking and hard acceleration and stuff is going to [crosstalk].



STEVE:  Right, battery treatment and, yeah.



LEO:  Battery, right, right.  So I think that's - I think that's great.  You know.  But, yeah, I understand why he doesn't want Hyundai to record it.



STEVE:  Well, and I would argue that a consumer who says no, I don't want to be watched and spied on and reported on, that ought to be a privacy right is available.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  I'd like to see the fine print in the rest of the contract.  Those are long, those contracts, you know.  They go on and on.



STEVE:  Yeah.  So Lon Seidman said:  "I'm listening to the latest Security Now! episode.  Definitely agree that freezing one's credit needs to be the default position these days."



LEO:  Yes, yes.



STEVE:  "One question, though.  Most of these credit agencies rely on the types of personal information that typically get stolen in a data breach for authentication.  Certainly a bad actor will go for the lowest hanging fruit and perhaps move on from a frozen account.  But if there's a big whale out there, they may go through the process of unlocking that person's credit, then stealing their money.  What kind of authentication changes do you think are needed?"



Okay, well, that's an interesting question.  Since I froze my credit reporting, I've only had one occasion to temporarily unfreeze it, which is when I decided to switch to using an Amazon credit card for the additional purchase benefits that it brought since I'm a heavy Amazon user.  And that's when I discovered to my delight that it was also possible to now specify an automatic refreeze on a timer...



LEO:  Yes.  Isn't that great?



STEVE:  ...to prevent the thaw from being inadvertently permanent.  Since I had very carefully recorded and stored my previously freezing authentication, I didn't need to take any account recovery measures.  So I can't speak from experience.  But one thing does occur to me is that strong measures are available.  The reporting agencies, for example, will have our current home address.  So they could use the postal system to send an authentication code via old school paper mail that would be quite difficult, if not effectively impossible, for a criminal located in some hostile foreign country to obtain.  So there certainly are strong authentication measures that could be employed if needed.



Again, I don't have any experience with saying, whoops, I forgot what you told me not to forget when I froze my credit.  So, but it's me.  Hi.  It's me, really.  Unfreeze me, please.  You know, because Lon's right that, you know, so much information is in the report, or in the data which is being leaked these days, for example in that massive AT&T leakage, that, you know, something over and above that needs to be used.



LEO:  They gave me a long PIN.  I mean, like a really long PIN.



STEVE:  Yes.  I have that, too.  I'm not, you know, I wrote it down and saved it.



LEO:  Yeah.  But...



STEVE:  But then what happens if you say, oh, it's me.



LEO:  I forgot, yeah.



STEVE:  Yeah.



LEO:  They say if you don't, you know, you can't log in, you forget it, call us.  Which means you could easily social engineer a customer service rep because, let's face it, the credit reporting agencies don't want you to have a credit freeze.



STEVE:  Correct.  Correct.



LEO:  That's how they make their money is selling your information.



STEVE:  Correct.



LEO:  So I suspect it's pretty easy to get it turned off.  I would guess.  By a third party.



STEVE:  Eric Berry, he tweeted:  "What was that credit link from the podcast?  I tried the address you gave out and got 'page not found.'"



I don't know why.  But it is grc.sc/credit.  And I just tried the link, and it works.  So grc.sc/credit.  That bounces you over to the Investopedia site.  And I've just verified it.  I said that it is still working.  And for what it's worth, in what, page 9 of the show notes is the Investopedia link, all the way spelled out.  So if something about your computer doesn't follow HTTP 301 redirects, then the link is there.  And it's at Investopedia, How to Freeze and Unfreeze Your Credit.  So you could probably also just google that.



LEO:  I should also point out the FTC has a really good page about credit freezes, fraud alerts, what they are, how they work and so forth.



STEVE:  Yeah.



LEO:  Yeah.  You could also just google "FTC and credit freeze," and they have a lot of information on there.



STEVE:  Does that provide links to the actual freezing pages in the bureaus?



LEO:  Yeah.



STEVE:  Because that's why I chose - oh, good.



LEO:  Yeah.  Absolutely.



STEVE:  Good, good, good.  Okay.



LEO:  They point you to a website run by the FTC called IdentityTheft.gov.  And they're going to give you those three.  Now, I should point out, there's more than three.  These are the three big ones.  But when I did a credit freeze, I think I did five or six of them.  There's others.  And it's probably not a bad idea to seek them all out.  But obviously these are the three the FTC mentions, as well as Investopedia.



STEVE:  Right.  So someone who tweets from the handle or the moniker The Monster, he said:  "@SGgrc The race condition isn't solved solely with the exchange 'counter ownership' protocol unless the owner immediately rereads the owned memory region to be sure it wasn't altered before it got ownership."



Okay, now, I don't think that's correct.  There are aspects of computer science that are absolutely abstract and purely conceptual.  And I suppose that's one of the reasons I'm so drawn to it.



LEO:  Yeah, me, too.



STEVE:  I think, Leo, I think you are, too.



LEO:  Yes, exactly.



STEVE:  One of the time-honored masters of this craft is Donald Knuth.  And the title of his masterwork, it's a multivolume, just it's a tour de force.



LEO:  I have all three, although there supposedly are five.  He's working on them.



STEVE:  Yes.  He calls the other ones "fascicles."  And I have those, as well.



LEO:  Oh, yeah?  How do you get those?



STEVE:  Oh, yeah, they're available.



LEO:  I wanted the full bookshelf, but I only could find three.



STEVE:  Yeah.  Three are in that original nice classic binding.



LEO:  Yeah, beautiful, yeah.



STEVE:  And then he has a set of what he calls "fascicles."



LEO:  Okay.



STEVE:  Which are the other two.



LEO:  "The Art of Computer Programming."  They're brilliant, brilliant, yes.



STEVE:  Yes, it's titled, the masterwork is "The Art of Computer Programming."  And saying that is not hyperbole.



LEO:  Yeah, yeah.



STEVE:  There are aspects of computer programming that can be true art, and his work is full of lovely constructions similar to the use of a single exchange instruction being used to manage inter-thread synchronization.  In this case, as I tried to carefully explain last week, the whole point of using a single exchange instruction is that it is not necessary to reread anything because the act of attempting to acquire the ownership variable acquires it only if it wasn't previously owned by someone else, while simultaneously - and here simultaneity is the point and the requirement - also returning information about whether the variable was or was not previously owned by any other thread.



So if anyone wishes to give their brain a bit more exercise, think about the fact that in an environment where individual threads of execution may be preempted at any instant, nothing conclusive can ever be determined by reading the present state of the object ownership variable.  Since the reading thread might be preempted immediately following that reading, and during its preemption the owner variable might change, the only thing that anyone reading that variable might learn, that is, just simply reading it, is that the object being managed was or was not owned at the instant in time of that reading.



While that might be of some interest, it's not interesting to anyone who wishes to obtain ownership, since that information was already obsolete the instant it was obtained.  So that's what is so uniquely cool about this use of an exchange instruction which both acquires ownership only if it isn't owned and returns the previous state, meaning if it wasn't previously owned, now the thread that asked owns it.  And it's as simple as a single instruction, which is just, you know, conceptually so cool.



Javamantis said:  "Regarding episodes 970 and 969 with 'push button hardware config' options, my first thought is of the 2017 Saudi chemical plant attacked with the Triton malware.  The admins working on the ICS controllers deliberately left an admin permission key in the controllers, instead of walking the 10 minutes required to insert the key every time a configuration needed changing."



LEO:  I don't blame them.  That's 10 minutes, man.



STEVE:  Uh-huh.  "As a result, the attackers were able to access the IT systems and then the OT systems because the key was always left in and in admin mode."  He says:  "Lazy people will always work around inconvenient, very secure systems."



LEO:  Like me, yes.



STEVE:  And he finishes with "To 999 and beyond, like Voyager."  Yes, this podcast is going into interstellar space.



LEO:  999 and beyond.  To boldly go where no podcast has gone before.



STEVE:  That's not true, though, because I keep hearing TWiT talking about, oh, we - yeah, that's right.  Anyway, I thought he made a good point.  For example, the push button dangerous config change enabler should work on a change from not pushed to pushed, rather than on whether the button is depressed.  The electrical engineers among us will be familiar with the concept of "edge triggered" versus "level triggered."  If it's not done that way, people will simply depress the button once, then do something like wedge a toothpick into the button in order to keep it depressed.



My feeling is, the ability to bypass well-designed and well-intentioned security does not matter at all.  There's a huge gulf separating "secure by design" and "insecure by design."  And it's absolutely worth making things "secure by design" even if those features can be bypassed.  The issue is not whether they can be bypassed, but whether they are there in the first place to perhaps be bypassed.



If someone goes to the effort to bypass a deliberately designed security measure, then the consequences of doing that is 100% on them.  It's a matter of transferring responsibility.  If something is insecure by design, then it's the designers who are at fault for making the system insecurely.  You know, designing the system insecurely.  They may have assumed that someone would come along and make their insecure system secure, but we've witnessed far too many instances where that never happened.  So the entire world's overall net security will be increased if systems just start out being secure and are then later, in some instances, forced against their will to operate insecurely.



And if someone's manager learns that the reason the enterprise's entire network was taken over, all their crown jewels stolen and sent to a hostile foreign power, and then all their servers encrypted, is because someone in IT wedged a toothpick into a button to keep it held down for their own personal convenience.



LEO:  Of course they did.



STEVE:  Well, you won't be asking that manager for a recommendation on the rsum that will soon need updating.  Right.



LEO:  Wow.



STEVE:  It'll be your fault and no one else's.  David Sostchen tweeted:  "Hi, Mr. Gibson.  Longtime listener" - very formal - "and SpinRite owner."  Actually, Ant used to call me Mr. Gibson, but nobody else does.  He said:  "I was listening to podcast 955, and I meant to message you about the Italian company Actalis, but life has a tendency to get in the way.  They happen to be one of the few remaining companies that issue free S/MIME certificates.  I've been using them for years to secure all my email.  All the best, David."  So I just wanted to pass that on, David, thank you.



LEO:  That's good to know.  Who is it again?



STEVE:  An Italian company, Actalis, A-C-T-A-L-I-S, are issuing free...



LEO:  Because I've been paying for my S/MIME.



STEVE:  S/MIME certs.



LEO:  I mean, I use PGP most of the time, which is free.  But that's cool.  S/MIME is a lot easier for some people, so that's cool.



STEVE:  Yup.  Meanwhile, the FeloniousWaffle has tweeted:  "Hi, Steve.  I created an account on this platform to message you."  Oh, thus FeloniousWaffle.  He said:  "I cannot wait for your email to be up and running."  Neither can I.  "I was just listening to Episode 968 on my commute and believe the outrage of AT&T's encryption practices to be undersold."  Oh.  He says:  "You mention that if someone is able to decrypt one string to get the four-digit code, then they have everyone's code who shares the same string.  I believe it to be far worse than that.  Am I wrong in thinking that if they crack one, then they have all 10,000?



"I'm making some assumptions that there are only two ways that 10,000 unique codes produces exactly 10,000 unique encrypted strings.  The first, and this is what I'm assuming, AT&T used the same key to encrypt every single code."  That's right.  "The second would be to have a unique key for each code.  So code 123 would have to be a different key than 5678.  That seems farfetched to me.  Is there an error to my thinking?  Thanks for the podcast and everything you do.  Glad you are sticking around beyond 999.  Daryle."



Okay.  So I see what Daryle is thinking.  He's assuming that what was done was that, if the encrypted string was decrypted to obtain the user's four-digit passcode, then the other 9,999 strings could similarly be decrypted to obtain the other four-digit passcodes.  And he's probably correct in assuming that, if one string had been decrypted, then all the others could be, too.  But that isn't what happened.  No encrypted strings were ever decrypted, and the encryption key was never learned.  But due to the static nature of the passcodes' encryption, that wasn't necessary.



I wanted to share Daryle's note because it reveals an important facet of cryptography, which is that it's not always necessary to reverse a cryptographic operation, as in decryption in this case; but it's also true of hashing, where we've talked about through the years many instances where we don't need to unhash something.  You know, going only forward, in the forward direction, is often still useful.  If the results of going in the forward direction can only be reapplied to other instances, then a great deal can still be learned.



In this case, since people tended to use highly non-random passcodes, you know, reusing their birthday, their house's street number, or the last four digits of their phone number or Social Security number - all things that were also part of the exfiltrated data - and assuming a fixed mapping between their plaintext passcode and its encryption, meaning the key never changed, examining, for example, the details of all of the records having a common encrypted passcode - imagine that you, from this big massive database, you pulled together all the records with the same encrypted passcode, and you look at them, just that observation would very quickly reveal what single passcode most of those otherwise unrelated records shared, and thus all of them used.



For example, one household lived at 1302 Willowbrook, whereas the birthday of someone else was February 13th, and someone else's phone number ended in 1302.  So by seeing what digits were common among a large group of records all sharing only the same encrypted passcode, it would quickly become clear what identical passcode they all chose.  No decryption necessary.  So that's, you know, one of the cool things that we've seen about the nature of crypto in the field is there actually are some interesting ways around it when you have the right data, even if you don't have the keys.



Skynet tweeted:  "Hi, Steve.  Would having DRAM catch up and be fast enough eliminate the GhostRace issue?"  And I thought that was a very interesting question.  You know, we've talked about how caching is there to decouple slow DRAM from the processor's much more hungry need for data in a short time.  So the question could be reframed a bit to further clarify what we're really asking.  So let's ask:  If all of the system's memory were located in the processor's most local, instant access L1 cache, that is, if its L1 cache were 16GB in size, so that no read-to or write-from main memory took any time at all, would speculative execution still present problems?  And I believe the answer is yes.



Even in an environment where access to memory is not an overwhelming factor, the work of the processor itself can still be accelerated by allowing it to be more clever about how it spends its time.  Today's processors, for example, are not executing instructions one at a time.  And in fact processors have not actually been executing one instruction at a time for quite a while.  The concept of "out of order" instruction execution dates way back to the early CDC (Control Data Corporation) 6600 mainframe, which was the first commercial computer system, a mainframe, to implement out-of-order instruction execution.



And that was in 1968, I believe, is when the CDC 6600 happened.  It sucked in instructions ahead of them being needed.  And when it encountered an instruction whose inputs and outputs were independent of any earlier instructions that were still being worked on, it would execute that later instruction in parallel with other ongoing work because the instruction didn't need to wait for the results of previous instructions.  Nor would its effect change the results of previous instructions.



The same sort of instruction pipelining goes on today, and we would still like our processors to be faster.  If a processor had perfect knowledge of the future by knowing which direction it was going to take at any branch or where a computed indirect jump was going to land it, and it could reach its - if it had perfect knowledge of those things, it would be able to reach its theoretical maximum performance given any clock rate.  But since a processor's ability to predict the future is limited to what lies immediately in front of it, it must rely upon looking back at the past and using that to direct its guesses about the future - or, as we say, its speculation about its own immediate future.



Here's something to think about.  The historical problem with third-party cookies has been that browsers maintained in the past a single large shared cookie jar, as we've discussed before, in fact just recently.  So an advertiser could set its cookie while the user was at site "A," and read it back when the same user had moved to site "B."  This was never the way cookies were meant to be used.  They were meant to be used in a first-party context to allow sites to maintain state with their visitors.  The problem is that, until very recently, there has been no cookie compartmentalization.



We have the same problem with microprocessor speculation that we have had with third-party cookies, lack of compartmentalization.  The behavior of malware code is affected by the history of the execution of the trusted code that ran just before it.  Malware is able to detect the behavior of its own code, which gives it clues into the operation of previous code that was running in the same set of processors.  In other words, a lack of compartmentalization.  Malicious code is sharing the same microarchitectural state as non-malicious code because today there's only one set of state.  That's what needs to change.  And I would be surprised if Intel wasn't already well on their way to implementing exactly this sort of change.



I have no idea how large a modern microprocessor's speculation state is today.  But the only way I can see to maintain the performance we want today in an environment where our processors might be unwittingly hosting malicious code is to arrange to save and restore the microprocessor's speculation state whenever the operating system switches process contexts.  It would make our systems even more complicated than they already are, but it would mean that malicious code could no longer obtain any hints about the operation of any other code that was previously using the same system it is.



I'll omit this listener's full name since it's not important.  We'll call him John.  He says:  "I got nailed in a phishing email for AT&T."



LEO:  Oh.



STEVE:  Yeah, see the attached picture.  He said:  "No excuse, but at least I realized it immediately and changed my password," he said, "which is not one that has been used anywhere else of course."  He ended, saying, "Feel stupid...."



LEO:  No, because we've been talking about this AT&T breach.  He was expecting this email from AT&T.



STEVE:  Yup.  Yup.  Yup.  Exactly.  The email says:  "Dear Customer.  At AT&T we prioritize the security of our customers' information and are committed to maintaining transparency in all matters related to your privacy and data protection.  We are writing to inform you of a recent security incident involving a third-party vendor.  Despite our rigorous security measures, unauthorized access was granted to some of our customer data stored by this vendor.  This incident might have involved your names, addresses, email addresses, social security numbers, and dates of birth.



"We want to assure you that your account passwords were not exposed in this breach."  But they're about to be.  "We have notified federal law enforcement about the unauthorized access.  Please accept our apology for this incident.  To determine if  your personal information was affected, we encourage you to follow the link below to log into your account."



LEO:  Oh, boy.



STEVE:  And then there's a little highlight that says "Sign In."  And finally, "Thanks for choosing us.  AT&T."



LEO:  I'm willing to bet this is a copy of the actual email because it's too much corporate, like, rigorous security measures, but they did gain all your data.  It's very much what AT&T said.  So I bet the bad guy just copied the original AT&T email and just changed this little link here.



STEVE:  Exactly.  And, well, I would imagine that there was probably no sign-in in the original link because, you know, that's really what changes it into a phishing attack.  And so anyway, I just wanted to say this is how bad it is out there.  I mean, as you said, Leo, you saw it immediately.  We've been talking about it.  This is a listener of ours.  He knew about it before it came.



LEO:  He expected it.  He expected it.



STEVE:  So absolutely authentic-looking.



LEO:  They're so smart.  They're so evil.



STEVE:  You know, we really - we absolutely need to always be vigilant.



LEO:  And never click links in email.



STEVE:  No.  No.



LEO:  Never.



STEVE:  Even from Mom.  Tom Minnick said:  "With these 'atomic operations' to mitigate race conditions, how does that work with multicore processors when multiple threads are running in parallel?  Couldn't a race condition still occur?"  He says:  "I probably don't understand enough about how multicore processors handle threads."



So Tom's question actually is a terrific one, and it occurred to many of our listeners who wrote.  He and everyone were right to wonder.  The atomicity of an instruction only applies to the threads running on a single core since it, the core, can only be doing one thing at a time by definition.  Threads, as I said, are an abstraction for a single core.  They are not an abstraction if multiple cores are sharing memory.



So what about multiple cores or multi-processor systems?  The issue is important enough that all systems today provide some solution for this.  In the case of the Intel architecture, there's a special instruction prefix called "Lock" which, when it immediately precedes any of the handful of instructions that might find it useful, forces the instruction that follows to also be "atomic" in the sense of multiple cores or multiple memory-sharing processors.



Only one processor at a time is able to access the targeted memory location.  And after all, it's just an instant; right?  It's just essentially there's actually a - there is a locked signal that comes out of the chip that all the chips are participating with.  So the processor, when it's executing a locked instruction, drops that signal, performs the instruction, and immediately raises it.  So it's just an, you know, it's as infinitesimally brief lockout as could be.  So it doesn't hurt performance, but it prevents any other processor from accessing the same instruction at the same time.  Only one processor at a time is able to access the targeted memory location.



And there's one other little tidbit.  That simple exchange instruction is so universally useful for performing thread synchronization that the lock prefix functionality is built into that one instruction.  All the other instructions that can be used require an explicit lock prefix.  Not the exchange instruction.  It automatically is not only thread safe, but multicore and multiprocessor safe, which I think is very cool.



Finally, Michael Hagberg said:  "Credit Freeze.  Rather than unlock your entire account, it should work this way:  I'm buying a car.  The dealer tells me which credit service they use and the dealership's ID number.  I go to the credit service website, provide my Social Security number, PIN assigned by the site when I froze it, and the car dealer's ID number.  My account will then allow that car dealer only to access my account for 24 hours."



And Michael, I agree 100%.  And this just shows us that the child in you has not yet been beaten into submission, and that you are still able to dream big.  More power to you.  Wouldn't it be nice if the world was so well designed.



LEO:  I actually do everything but that last piece where you give the car dealer's ID to the credit bureau.  But I do ask them which credit bureau are you going to use.



STEVE:  Good, good.



LEO:  And then that's the one I unfreeze.  And I tell them, you've got whatever, three days to do this, and it's going to automatically lock up again.  And nowadays enough people use freezes that when they get that, they kind of know what happened.  And they'll call you and say, hey, your credit's frozen, yeah.



STEVE:  Yeah, right.  It's not unusual to encounter a freeze.  And in fact I did some googling around before I got my card with Amazon to find out which of the services they used.



LEO:  Yeah, exactly.



STEVE:  And, you know, and then that's the one I unlocked so that...



LEO:  I'd be more judicious, yeah.  I love the idea, though, of saying, hey, credit bureau, this guy is going to ask.  Don't tell anybody else.



STEVE:  Wouldn't that be?  Yeah, but Leo, you know, all the junk  mail we receive as elders...



LEO:  All those credit card offers.  Oh.



STEVE:  Yes.  It's because everybody's pulling our credit.



LEO:  By the way, when I froze all my accounts, I stopped getting those.



STEVE:  Yeah, I haven't had any for years.



LEO:  The only ones I get are from cards, existing cards saying, you know, hey, you've got a blue card.  Would you like a green one?  That's it.  Because no new card companies can get my information.  So it works.



STEVE:  Right.



LEO:  It works.



STEVE:  Right.  Okay.  We've got just two little bits regarding SpinRite.  Mike Shales said:  "Recently I've run into some issues with my old iMac, a mid-2017 model."  He said:  "I've wanted to support your valuable Security Now! efforts for some time, but investing the time to see if I could even run SpinRite on my Macs when they were all running without problems discouraged me.  But now.  You mentioned on your April 9th podcast:  'I wanted to remind any would-be Mac purchasers'" - oh, this is me speaking.  He's quoting now.  "'I wanted to remind any would-be Mac purchasers that this is the reason I created GRC's freeware named Bootable in favor of'" - okay, the name - "'In favor of 'DOS Boot.'  If you can get Bootable to congratulate you on your success in booting it, then exactly the same path can be taken with SpinRite.'"  Right.



So he said, he wrote:  "But Bootable is a Windows .exe file and needs a Windows machine to create a bootable USB flash drive; right?  Lacking a Windows machine, I made a bootable DOS drive from your ReadSpeed image download."  Wow.  Good going there.



LEO:  It's ambitious, yeah.



STEVE:  "Following instructions from ChatGPT, I used dd to write the ReadSpeed image to a 4GB flash drive.  Then following instructions in the GRC forum post 'Boot a Mac into FreeDOS for SpinRite or ReadSpeed,'" he said:  "I succeeded in booting my iMac into DOS and running ReadSpeed.  So far so good.  But I believe the current SpinRite 6.1 includes the capability to recognize more drives than previously, and might rely on features not provided in the version of DOS that I now have installed on my flash drive.  If so, perhaps downloading the SpinRite 6.1 .exe file and copying to my flash drive might not be ideal.  Is this an issue?  Thanks for the help.  Mike."



Well, okay.  Mike very cleverly arranged to use various tools at GRC - and, amazingly enough, ChatGPT - to create a bootable USB drive which successfully booted his mid-2017 iMac.  So first responding to Mike directly:  Mike, everything you did was 100% correct.  And if you place your copy of the SpinRite EXE into that USB stick and boot it, everything will work perfectly.  And if you run it at Level 3 on any older Macs with solid-state storage, you can expect to witness a notable and perhaps even very significant subsequent and long-lasting improvement in the system's performance.



And while it won't be obvious, there's also very a good reason to believe that in the process you'll have significantly improved the system's reliability.  The reason the SSD will now be much faster is that it's needing, as I mentioned before, to struggle much less after running SpinRite to return the requested information.  And we will be learning far more about this during the work on SpinRite 7.  And although 6.1 is a bit of a blunt instrument in this regard, it works, and it's here today.  To Mike's question, the specific version of FreeDOS does not matter at all, since DOS is only used to load SpinRite and to write its log files.  Otherwise, SpinRite ignores DOS and interacts directly with the system's hardware.  So yes, you can run it on your ReadSpeed drive.



I wanted to share Mike's question because I just finished making some relevant improvements.  He mentioned correctly that Bootable is Windows-only freeware.  But over the weekend the Bootable download was changed from an EXE to a ZIP archive, and the ZIP archive now also contains a small Bootable file system image which can be used by any Mac or Linux user to directly create a Bootable boot-testing USB drive.



LEO:  Any Intel Mac or Linux user.



STEVE:  Yes, Intel, yes.



LEO:  We should really emphasize that because most Mac users now are no longer Intel, yeah.



STEVE:  Yeah.  Yeah.  One of the guys in GRC's web forums put me onto a perfect and easy-to-use - oh, I should mention, Leo, we've solved the Intel problem, but that's for another time.



LEO:  Oh.  Tease me, why don't you.



STEVE:  Yeah.  We've got - we have some guys have figured out how to boot on UEFI-only systems and on ARM-based...



LEO:  On silicon, wow.



STEVE:  ...silicon using some concoction of virtual machines.  And I haven't followed what they're doing because I'm just focused on getting what all of this has done done.  Anyway, there's something known as Etcher by a company called Balena.  It is a perfect, easy-to-use for an Intel Mac person means of moving the Bootable image onto a USB without the dd command.  Dd makes me nervous because you need to know what you're doing.  I mean, it's a very powerful direct drive copying tool.  Linux people are probably more comfortable with dd.  I'm glad that this Mac user, Mike, was able to get ChatGPT to help him, and I'm glad that ChatGPT just didn't stumble over hallucination at that particular moment.



LEO:  You can erase everything with dd very easily.



STEVE:  Yeah.



LEO:  Careful.



STEVE:  Yeah.  And lastly, Sean wrote:  "Hey, Steve.  I'm sure you're hearing this a lot, but Windows" - oh.  "Windows did not trust SpinRite despite all your signing efforts.  I had to clear three severe warnings before it would allow me to keep 6.1 on my system for use.  I hope it gets better soon for users less willing to ignore the scary warnings from Microsoft."  Signed, Sean.



And yup.  I don't recall whether I had mentioned it here also, since I've participated a lot about this over in this discussion in the GRC's newsgroups.  One thing that has been learned is that Microsoft has decided to deprecate any and all special meaning for EV, extended validation, code signing certificates.  It's gone.  All those hoops I jumped through to get remote server-side EV code signing to work remotely on an HSM device will have no value moving forward, except having the signing key in the HSM does prevent anybody, even me, I mean, from extracting it.  It can't be extracted.  It can only be used to sign.



When I saw this news, I reached out to Jeremy Rowley, who's my friend and primary contact over at DigiCert, to ask him if I had read Microsoft's announcement correctly.  And he confirmed that Microsoft had just, like, the week before surprised everyone in the CAB forum with that news.  Apparently, what's at the crux of this is that for, you know, historically, end users were able to use EV code signing certificates to sign their kernel drivers.  That was the thing Microsoft most cared about as far as EV was concerned.



But after the problems with malicious Windows drivers, Microsoft has decided to take away that right and require that only they, meaning Microsoft, will be authorized to sign Windows kernel drivers in the future.  In their eyes, this eliminated the biggest reason for having and caring at all about EV code signing certs.  So they will continue to be honored for code signing, but EV certs will no longer have any benefit.  They will confer no extra meaning.



What I think is going on regarding SpinRite is that something Windows Defender sees inside SpinRite 6.1's code, which was not in 6.0, absolutely convinces it that this is a very specific Trojan named "Wacatac.B," which I guess you pronounce "whack attack."  If I knew what part of SpinRite was triggering these false positives, I could probably change it.  I have some ideas, so I'm going to see, because we just can't keep tolerating these sorts of problems from Microsoft, and it doesn't now look like my having an EV cert - it's been three months now, and tens of thousands of copies of GRC's freeware, because, you know, thousands, plural thousands of copies are being downloaded every day.  I resigned them all with this new certificate in order to get it exposed and let Microsoft see that, you know, whoever was signing this wasn't producing malware.  But, you know, here's Mike, or no, sorry, Sean, who just said, you know, he had to go to extreme measures to get Windows to leave this download alone.



LEO:  Huh.



STEVE:  So, grumble.



LEO:  Grumble, grumble.



STEVE:  Big-time.  Okay.  We're going to talk about what the EU is doing, Leo, after you share our last sponsor with our listeners.



LEO:  Yes.  Breaking news, however, that you will, depending on your point of view, will either be surprised or not surprised to hear.



STEVE:  Yeah.



LEO:  Google has decided to delay third-party cookie blocking.



STEVE:  Oh.



LEO:  Until next year.



STEVE:  Wow.



LEO:  From Digiday, this fantastic opening sentence by Seb Joseph:  "Google is delaying the end of third-party cookies in its Chrome browser again.  In other unsurprising developments, water remains wet."



STEVE:  Wow.



LEO:  So they did not outline a more specific timetable beyond hoping for 2025.



STEVE:  Okay.  And that, I mean, it does show you the...



LEO:  Resistance.



STEVE:  ...problem with taking this away.



LEO:  They promised it originally in January 2020.  This is the third time they've pushed it back.  And I'm guessing it's not going to be the last.  Some of this is actually, you know, intertwined with the UK Competition and Markets Authority.  They want, they say:  "It's critical the CMA has sufficient time to review all the evidence, including results from industry tests, which the CMA has asked market participants to provide by the end of June."



STEVE:  Ah, in order to see whether the Privacy Sandbox will be a replacement.



LEO:  Right, right.  "We recognize," Google says, "there are ongoing challenges related to reconciling divergent feedback from the industry.  Regulators and developers will continue to engage closely with the entire ecosystem."  Yeah, but some of this is that the CMA wants to see proof, and they're not ready to provide proof.



STEVE:  Well, Leo, here's another good - another reason I'm so happy we're going past 999.



LEO:  Yeah.



STEVE:  Because November is when we hit 999.  And I would not be here for...



LEO:  Let's make a deal that you'll keep doing the show until Google phases out third-party cookies.



STEVE:  Oh, no.



LEO:  Rats.



STEVE:  Well past.



LEO:  Almost fooled him.



STEVE:  No, no.  I think it's going to happen.  I think it's inevitable.



LEO:  You think.  Okay.  We'll see.  It's been four years.



STEVE:  I would wager to 2025.  I'll go for next year.



LEO:  Let's go for 2025.  Let's do it.  Let me real quickly mention our great sponsor, and then we can get to the meat of the matter, these Chat guys here going on, what's going on in Europe.  All right.  Let's talk about Chat, Steve Gibson.



STEVE:  Okay.  So, oh, boy.  Across the pond from the U.S., the EU is continuing to inch forward on their controversial legislation, commonly referred to as "Chat Control," thus today's title is "Chat (out of) Control," which proposes to require providers of encrypted messaging services to somehow arrange to screen the content that's carried by those services for child sexual abuse material, commonly known as CSAM.  As I said when we last looked at this last year, 2024 will prove to be quite interesting since all of this will likely be coming to a head this year.  What's significant about what's going on in the EU, unlike in the UK, is that the legislation's language carries no exclusion over the feasibility of performing this scanning.



Just to remind everyone who has a day job and who might not be following these political machinations closely, last year the UK was at a similar precipice.  And with their own legislation, at the 11th hour, they added some language that effectively neutered it while allowing everyone to save face.  For example, last September 6th, Computer World's headline read "UK rolls back controversial encryption rules of Online Safety Bill," and followed that with "Companies will not be required to scan encrypted messages until it is 'technically feasible,' and where technology has been accredited as meeting minimum standards of accuracy in detecting only child sexual abuse and exploitation content."



So since it's unclear how any automated technology might successfully differentiate between child sexual abuse material and, for example, a photo that a concerned mother might send of her child to their doctor, there's little concern that the high bar of "technical feasibility" will be met in the foreseeable future.  While the UK came under some attack for punting on this, the Big Tech companies all breathed a collective sigh of relief.



But so far - and, boy, there's not much time left - there is no sign of the same thing happening in the EU, not even a murmur of it.  One of the observations we've made about all such legislation was the curious fact that, if passed, the legislation would mean that the legislator's own secure, encrypted, and private communications would similarly be subjected to surveillance and screening.  Or would they?



Two weeks ago, on April 9th, the next iteration of the legislation appeared in the form of a daunting 203-page tome. Fortunately, the changes from the previous iteration were all shown in bold type, or crossed out, or bold underlined, or crossed out and underlined, all meaning different things.  But that made it at least somewhat possible to see what's changed.  As you can tell, I spent way too much time with that 203 pages.  This was brought to my attention by the provocative headline in an EU website, "ChatControl:  EU ministers want to exempt themselves."



And what that article went on to say was:  "According to the latest draft text of the controversial EU Child Sexual Abuse Regulation proposal leaked by the French news organization Contexte, which the EU member states discussed, the EU interior ministers want to exempt professional accounts of staff of intelligence agencies, police, and military from the envisioned scanning of chats and messages.  The regulation should also not apply to 'confidential information' such as professional secrets.  The EU governments reject the idea that the new EU Child Protection Centre should support them in the prevention of child sexual abuse and develop best practices for prevention initiatives."



Okay.  So the EU has something called the "Pirate Party," which doesn't seem to be well named, but it is what it is.



LEO:  No, it's a real - it's, you know, the Pirate Bay people.



STEVE:  Yeah, the Pirate pirates.



LEO:  Yeah.  It's a party of pirates, yeah.  And popular, I might add.



STEVE:  Yes.  It's formed from a collection of many member parties across and throughout the European Union.  The Party was formed 10 years ago, back in 2014, with a focus upon Internet governance.  So the issues created by this pending legislation is of significant interest to this group.  To that end, one of the members of Parliament, Patrick Breyer, had the following to say about these recent changes to the proposed legislation which came to light when the document leaked.



He said:  "The fact that the EU interior ministers want to exempt police officers, soldiers, intelligence officers, and even themselves from chat control scanning proves that they know exactly just how unreliable and dangerous the snooping algorithms are that they want to unleash on us citizens.  They seem to fear that even military secrets without any link to child sexual abuse could end up in the U.S. at any time.



"The confidentiality of government communications is certainly important, but the same must apply to the protection of business and of course citizens' communications, including the spaces that victims of abuse themselves need for secure exchanges and therapy.  We know that most of the chats leaked by today's voluntary snooping algorithms are of no relevance to the police, for example family photos or consensual sexting.  It is outrageous that the EU interior ministers themselves do not want to suffer the consequences of the destruction of digital privacy of correspondence and secure encryption that they are imposing upon us.



"The promise that professional secrets should not be affected by chat control is a lie cast in paragraphs.  No provider and no algorithm can know or determine whether a chat is being conducted by doctors, therapists, lawyers, defense lawyers, et cetera, so as to exempt it from chat control.  Chat control inevitably threatens to leak intimate photos sent for medical purposes and trial documents sent for defending abuse victims.  It makes a mockery of the official goal of child protection that the EU interior ministers reject the development of best practices for preventing child sexual abuse.



"It couldn't be clearer that the aim of this bill is China-style mass surveillance, and not better protecting our children.  Real child protection would require a systematic evaluation and implementation of multidisciplinary prevention programs, as well as Europe-wide standards and guidelines for criminal investigations into child abuse, including the identification of victims and the necessary technical means.  None of this is planned by the EU interior ministers."



So after the article finished quoting Patrick Breyer, it noted that the EU governments want to adopt the chat control bill by the beginning of June.  We're approaching the end of April, so the only thing separating us from June is the month of May.  I was curious to see whether the breadth of the exclusion might have been overstated in order to make a point, so I found the newly added section of the legislation on page 6 of the 203-page PDF.  It reads - this is Section 12a.  The "a" is the new part.



"In the light of the more limited risk of their use for the purpose of child sexual abuse and the need to preserve confidential information, including classified information, information covered by professional secrecy and trade secrets, electronic communications services that are not publicly available" - that's the key.  "Electronic communications services that are not publicly available, such as those used for national security purposes, should be excluded from the scope of this Regulation.  Accordingly, this Regulation should not apply to interpersonal communications services that are not available to the general public, and the use of which is instead restricted to persons involved in the activities of a particular company, organization, body, or authority."



Okay, now, I'm not trained in the law, but that doesn't sound to me like an exclusion for legislators who would probably be using iMessage, Messenger, Signal, Telegram, WhatsApp, et cetera.  It says, "This Regulation should not apply to interpersonal communications services that are not available to the general public."  So, you know, internal proprietary intelligence agency communication software, you know, applications.



Remember that it's this proposed EU legislation which includes the detection of "grooming" behavior in textual content.  So it's not just imagery that needs to be scanned, but the content of all text messaging.  We're also not talking about only previously known and identified content which is apparently circulating online, but also anything the legislation considers "new" content.  As I read through section after section of what has become a huge mess of extremely weak language that leaves itself open to whatever interpretation anyone might want to give, my own lay feeling is that this promises to create a huge mess.  I've included a link to the latest legislation's PDF in the last page of the show notes for anyone who's interested.



You'll only need to read the first eight pages or so to get a sense for just what a catastrophic mess this promises to be.  As is the case with all such legislation, what the lawmakers say they want, and via this legislation will finally be requiring, is not technically possible.  They want detection of previously unknown imagery and textual dialog which might be seducing children while at the same time honoring and actively enforcing EU citizen privacy rights.  Oh, and did I mention that 78% of the EU population that was polled said they did not want any of this?



And it occurred to me that encryption providers will not just be able to say they're complying when they are not, because activist children's rights groups will be able to trivially test any and all private communications services to verify that they do, in fact, detect and take the action that the legislation requires of them.  All that's needed is for such groups to register a device as being used by a child, then proceed to have a pair of adults hold a seductive grooming conversation and perhaps escalate that to sending some naughty photos back and forth.  And you can believe that, if the service they're testing doesn't quickly identify and red-flag the communicating parties involved, those activist children's rights groups will be going public with the service's failure under this new legislation.



I've said it before, and I understand that it can sound like an excuse and a copout, but not all problems have good solutions.  There are problems that are fundamentally intractable.  This entire debate surrounding the abuse of the absolute privacy created by modern encryption is one such problem.  This is not technology's fault.  Technology simply makes much greater levels of privacy practical, and people continually indicate that's what they prefer.  As a society we have to decide whether we want the true privacy that encryption offers, or whether we want to deliberately water it down in order to perhaps prevent some of the abuse that absolute privacy also protects.



LEO:  Agreed, agreed, and agreed.



STEVE:  Yeah.  I do commend to anyone, the last page of the show notes has a link.  It's not widely available publicly because it was leaked, and Patrick-Breyer.de, so a German site, has it on his site and is making it available.  So you'll need to get it if you're interested.  But, boy, as I said, just reading through it, it is, again, it's insanely long at 203 pages.  I struggled to find any language about, like, what time period this takes effect over.  I couldn't find any.  It all seems to indicate, once this legislation is in place, that the organizations need to act.



But I just think the EU is stepping into a huge mess.  And again, as I said, 2024, I said last year this next year, 2024 we're in now, is going to be one to watch because lots of this is beginning to come to a head.  Although, Leo, as you just shared with us, not the third-party cookie issue with Chrome.  That's been punted into when we have four digits on this podcast.



LEO:  In the future.  Ah, yeah.  It's an interesting world we live in.



STEVE:  So I imagine when the legislation happens, and it's supposed to be happening in early June, there will be lots of coverage.  We'll be back to it, and we'll know, you know, we'll have some sense for when it's taking effect and what the various companies are choosing to do.



LEO:  Yeah, they might be well modified from this leaked document.  There will certainly be amendments and things like that.  So we'll have to look at the actual legislation to see what's happening.



STEVE:  Right.



LEO:  And we will because that's what we do.  That's what we do here.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#972

DATE:		April 30, 2024

TITLE:		Passkeys:  A Shattered Dream?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-972.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  The choice for this week's main topic received some serious competition from some surprising legislation that came into effect yesterday in the United Kingdom.  So we're going to start by taking a close look at what happened in the UK that promises to completely change the face of consumer IoT device security.  As we'll see, that's not an overstatement; the world as we've known it just changed.  While that exploration is going to consume most of the first half of today's podcast, I also want to look at what happened last week with Chrome's change of plan regarding third-party cookies, I have a bit of listener feedback to share, and news of the next installment in a long-running science fiction book series.  I also have the welcome news that I am finally working on bringing up GRC's email communications system.  Then we'll finish by taking a look at a blog posting by an industry insider that many of our listeners forwarded to me, asking "What do you think about this?"



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Last night he was sitting in his armchair, having a nice glass of Cabernet Sauvignon, reading the news, when he saw the announcement that the United Kingdom is going to make a major change to how IoT devices work.  He is so excited, he's come here, he leapt to his feet and said "We've got to talk about this on Security Now!."  So we will.  What happened with Chrome dumping its plans to dump third-party cookies?  And let's talk a lot about Passkeys.  There was an interesting post Steve read, and a lot of you read, saying it's not working.  Are Passkeys over?  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 972, recorded April 30th, 2024:  Passkeys:  A Shattered Dream?



It's time for Security Now!, the show where we - I know you wait for this all week long; don't you.  We cover the security field, the computer field, every other field, the sci-fi field, with this guy right here, Mr. Steven "Tiberius" Gibson.  May the Fourth be with you.



STEVE GIBSON:  And Leo?	



LEO:  Yes.



STEVE:  You are - oh, the Fourth.



LEO:  Yeah, it's coming up.



STEVE:  You are correct about the breadth of our coverage.  I do have a little sci-fi mention, a real quickie.



LEO:  Oh, good.  Oh, good.



STEVE:  For those listeners who have been following along, what has turned out to be my favorite saga, which promises - well, actually I was going to say it promises to be never-ending, but it actually does have an end; and this author is, you know, continuing to deliver on the promise.



LEO:  Nice.



STEVE:  Okay.  But the choice for this week's main topic received some serious competition from some surprising legislation that came into effect just yesterday in the UK, in our dear beloved United Kingdom.  So we're going to start by taking a close look at what happened in the UK that, I kid you not, promises to completely change the face of consumer IoT device security, like, immediately.  I know that's, you know, I don't think that's an overstatement.  I think that the world as we have known it has just changed.  So you could see why that was competing for today's topic, which we'll get to after that.  We're going to explore a little bit of, like, what happened with the news that just came in that you announced during last week's podcast, Leo, which was Chrome's sudden change of plan regarding its third-party cookie handling.



LEO:  Yeah.



STEVE:  I'm going to get a little bit into that.  But I'll explain why I'm probably going to punt most of it till next week because it turns out there's a lot.  I also have a little bit of listener feedback to share.  And as I said, news of the next installment in a long-running sci-fi book series.  I've also got some welcome news that I'm finally working on GRC's email system, which will come as great news for our listeners.  And then we're going to finish by taking a look at a blog posting by an industry insider developer that a surprising number of our listeners, this thing has gotten a lot of traction out in the industry, and our listeners kept forwarding me links to it, asking whether Passkeys is a shattered dream.



LEO:  Oh.



STEVE:  They've all been saying, "What do you think about this?"  And so, you know, some neat stuff to talk about, and of course a great Picture of the Week that's apropos of the topic that we've been covering lately of Voyager.



LEO:  Yes.  Passkeys:  A Shattered Dream, the topic on today's - it sounds like, you know, [sound], today.



STEVE:  We go up, and we come down.



LEO:  And we go down, yeah.  I like Passkeys.  It is a shattered dream.  Although one could say you might have a little dog in this hunt because honestly the SQRL protocol that you created is a far better way of doing it.



STEVE:  It solved the problem that is dogging this.  But, you know, I don't even mention it in my coverage because all of our listeners already know.



LEO:  They know.



STEVE:  That, you know, I did solve it the right way, and that's not what we got.



LEO:  And I suppose that ship has sailed.



STEVE:  Yeah.



LEO:  And you just have to have the big guys behind it before it has any chance of being adopted.



STEVE:  Well, and in fact this author, this guy is the author of a very strong WebAuthn library, the one for Rust that SUSE is using, and many others have forked from.  He notes that Chrome has succeeded in killing some features just by not adopting it. 



LEO:  Right, exactly.  Yeah.  That's too much power in Google's hands, if you ask me.



STEVE:  Well, they've got it.



LEO:  We will get the - yeah, I know, it's too late now.  We will - by the way, you know, I used OpenAI's chat for a long time and made some GPTs and really like it, especially v4.  But I just started using on Sunday, Kevin Rose said try Gemini.  Have you not tried - and so I tried Gemini Advanced, and it is mind-blowingly good.  So we've got us a race, which is exciting.  Very interesting.  And I'm sure there'll be lots of security implications there.  Gemini writes excellent code, unfortunately.



STEVE:  Well, and did you see the blurb?  I didn't get it in the show notes where ChatGPT4, just given the list of CVEs...



LEO:  Of CVEs, we talked about it, yes.



STEVE:  Is able to generate exploits.  Just like, oh, here you go.



LEO:  They took 87% of the CVEs they were given, just the description, and created the proof of concept.



STEVE:  Working exploits for them.  So script kiddies have been elevated.  Oh.



LEO:  What a world.  We are - this is just getting wild.  That's all I can say.



STEVE:  Yeah.



LEO:  And not in a good way, to be honest with you.  Ready for the Picture of the Week.



STEVE:  Okay.  So this resonated with me from something in my youth, and I figured that you would, being the king of pop that you are, you know, you'd go, oh, yeah, that's about this.  But it drew a blank for you.



LEO:  Kind of old pop, yeah.  Well, it sounds like, I don't know what, a joke, maybe, an old joke.



STEVE:  Okay.  So we have a cartoon which is apropos of the topic we've been following about the fate of Voyager 1 and how by some miracle it is back on the air.  Anyway, so the cartoon shows a couple of cute little green aliens in their classic circular UFO saucer with the glass bubble.  And the saucer, however, is labeled "Salvage."  And it's got the tow truck hook off the back with a hook.  So they're, you know, they're flying around out in space looking for stuff.  And so the alien - and then so what they have done is they just - we see them looking at the Voyager 1 probe labeled Voyager 1977, you know, with a NASA on it, and its various probes and sensors and so forth.  And the one alien is saying, "15 billion miles on it, but the radio still works."



LEO:  It's true.



STEVE:  And I'm reminded of something about used - it is true, absolutely true.



LEO:  Yeah, yeah.



STEVE:  I'm reminded of something about used cars back from my high school days.



LEO:  Yeah, I think it's an old joke.



STEVE:  Like there was some meme about, well, it's broken down and only goes downhill or something, but the radio still works.



LEO:  Right, right.



STEVE:  Anyway, 15 billion miles, the radio still works.  I thought that was a cute little observation.



Okay.  So - okay.  Yesterday - mark this day in your calendars, yesterday, April 29th, 2024 - a new law went into effect in the UK.  Not just like legislation got submitted somewhere; or, well, we're going to give this some consideration; and not just like, oh, recommendations like we're always getting.  This is a law that went into effect yesterday.  And I'm astonished by this - not only by the fact that it happened and by the content, but the depth of the quality of the baseline requirements that this proposes, which is why we're going to spend some time on this because this is huge.



Now, the Guardian, what was also odd was I guess I must have seen this, like, immediately after it happened because I looked around for, like, other more fleshed-out coverage, and nobody was picking up on it.  Now there's like, overnight it's like beginning to happen in the security info sphere because it's like, whoa, what?



So yesterday, only the Guardian in the UK seemed to have anything to say.  And they didn't say much, but their headline was "No more 12345:  Devices with weak passwords will be banned in the UK."  And the subhead was "Makers of phones, TVs, and smart doorbells legally required to protect devices against access by cybercriminals."  And, you know, not just passwords, baby.  That's just the tip of the iceberg.  I mean, this is - it's comprehensive legislation.



In order to get more detail, I went to the source.  So this is the GCHQ's National Cyber Security Centre blog posted yesterday which was titled "Smart devices:  New law helps citizens to choose secure products."  And actually even that's an understatement because this impacts not only the manufacturers of devices that may not be in the UK, but also anyone importing them and anyone retailing them.  And it's got some teeth behind it.  So it's not that the consumers in the UK are going to have a choice.  There aren't going to be any non-compliant options to purchase.  And of course this ends up being global because we're in a global economy.



Okay.  So what GCHQ said in sort of their top-level blog announcement, they said:  "From April 29th, 2024, manufacturers of consumer 'smart' devices must comply with new UK law."  That is yesterday.  "The law, known as the Product Security and Telecommunications Infrastructure act, or PSTI act, will help consumers to choose smart devices that have been designed to provide ongoing protection against cyberattacks.  The law means manufacturers must ensure that all their smart devices meet basic cyber security requirements."



Now, here they just highlight three, and they happen to be the first three.  But they're the first three of, like, 14, and they're all really significant.  But the first three are:  "The manufacturer must" - get this.  I mean, it sounds like I wrote this from the podcast.  "The manufacturer must not supply devices that use default passwords, which can be easily discovered online and shared.  If the default password is used, a criminal could log into a smart device and use it to access a local network or conduct cyberattacks."  So again, this is just sort of the blog discussing the legislation.  We'll get to the actual legalese here in a second.  But so they're sort of like explaining the why of these requirements at this point.



Second one, the manufacturer must provide a point of contact for the reporting of security issues which, if ignored, could make devices exploitable by cyber criminals.  Right.  And three, the manufacturer must state the minimum length of time for which the device will receive important security updates.  And then they flesh that out by explaining:  When updates are no longer provided, devices are easier to hack, or may stop working as designed.  So that's just the top three, which this law makes a requirement for the sale of anything; and, I mean, and again, there just - there aren't any loopholes in this.  It's like, if it's going to connect to the Internet or a network, it has to have this.



So they said:  "Although most smart devices are manufactured outside the UK, the PSTI act also applies to all organizations importing or retailing products for the UK market."  I mean, like, Amazon.  So they said:  "Failure to comply with the act is a criminal offense, with fines up to 10 million, or 4% of qualifying worldwide revenue, whichever is greater."



They said:  "The law applies to any 'consumer smart device' that connects either to the Internet or to a home network, for example, by WiFi.  This may include" - and here's the first of several enumerations that we're going to be encountering.  But here they said:  "Smart speakers, smart TVs, and streaming devices; smart doorbells, baby monitors, and security cameras; cellular tablets, smartphones, and games consoles; wearable fitness trackers including smart watches; smart domestic appliances such as light bulbs, plugs, kettles, thermostats, ovens, fridges, cleaners, and washing machines."



They finish:  "The NCSC has produced a 'point of sale' leaflet for retailers to distribute in-store to their customers.  It explains how the PSTI regulation affects consumers, and why it's important to choose smart products that protect against the most common cyberattacks."



So, you know, it's the end of April; right?  It's not the beginning.  This is not April Fool's Day.  This happened on the 29th.  And the first thing I need to say is "Holy crap.  Where did this come from?"  Turns out it's been in the works for five years and just not much is, you know, it hasn't been drawing much attention to itself.  So fines in the amount of the greater of 10 million, I think that was 12.5 million USD at this point, at the current exchange rate, or 4% of a manufacturer's qualifying worldwide revenue, whichever is the greater.  This is the sort of legislation that can really make a profound overnight difference in consumer security.  And since a great many manufacturers have shown through their actions or, you know, deliberate inaction, that they need to be made to change, this is the change that's required to make them.



Since this is huge and potentially affects all products worldwide which might find their way to the UK because it impacts not only manufacturers, as I said, but anyone who imports or retails such products, I needed to get a bit more back story.  So I did some digging.  On the gov.uk website I found the actual legislation.  It turns out, as I said, it's been quietly in the works for several years, and it really is a law, not just some watered-down milquetoasty "recommendations" that we see too often here in the U.S.



The Verge picked up on this, and in their coverage they noted that, here in the United States, our FCC is trying something similar in its forthcoming "Cyber Trust Mark" program.  You know, they liken it to the federal ENERGY STAR program, explaining that the Cyber Trust Mark logo indicates which products comply with the program's recommendations, which include strong default passwords.  But like ENERGY STAR, nobody's forcing companies to go along with it; and consumer product packaging has become so encrusted with certifications and compliance logos that it's unclear whether anyone even notices.



As we know, consumers are focused on three things:  Does it do what I need?  What does it cost?  And what additional nice-to-have features does it offer?  Whether or not a connected light switch has a default password is the last thing on anyone's, like, shopping list.  In other words, while the United States continues to be completely lame on this, the United Kingdom has taken the only action that has any chance of actually producing results for the consumer.  And thanks to the fact that we still live in a blessedly globalized economy, everyone everywhere will obtain the security benefits that the kingdom is now requiring as a matter of law.



Since this matters to everyone everywhere, and since it's going to change the face of Internet-connected consumer technology, let's take a closer look at what the legislation actually has to say.  First of all, this comes from a non-profit organization, that is, the actual think-tank behind the legislation, which is what got enacted, known as the European Telecommunications Standards Institute, or ETSI, E-T-S-I.  We've spoken of them in the past.  This work has been underway, as I mentioned, for the past five years, having started back in 2019, in February, with the publication of its v1.1.1, and it's been quietly making its way forward year by year.



The Baseline Requirements document is the one that's most relevant.  It's a 34-page PDF that I've given a GRC shortcut to.  The shortcut is ETSI, E-T-S-I.  So if you're curious, you can just put grc.sc/etsi into your browser, and you'll be bounced over to a document titled "Cyber Security for Consumer Internet of Things:  Baseline Requirements."



The document's introduction explains.  They said:  "As more devices in the home connect to the Internet, the cyber security of the Internet of Things (IoT) becomes a growing concern.  People entrust their personal data to an increasing number of online devices and services.  Products and appliances that have traditionally been offline are now connected and need to be designed to withstand cyber threats.  The present document brings together widely considered good practice in security for Internet-connected consumer devices in a set of high-level outcome-focused provisions.  The objective of the present document is to support all parties involved in the development and manufacturing of consumer IoT with guidance on securing their products.



"The provisions are primarily outcome-focused, rather than prescriptive, giving organizations the flexibility to innovate and implement security solutions appropriate for their products.  The present document is not intended to solve all security challenges associated with consumer IoT.  It also does not focus on protecting against attacks that are prolonged or sophisticated or that require sustained physical access to the device.  Rather, the focus is on the technical controls and organizational policies that matter most in addressing the most significant and widespread security shortcomings.  Overall, a baseline level of security is considered.  This is intended to protect against elementary attacks on fundamental design weaknesses," they say, "such as the use of easily guessable passwords.



"The present document provides a set of baseline provisions applicable to all consumer IoT devices.  It's intended to be complemented by other standards defining more specific provisions and fully testable and/or verifiable requirements for specific devices which, together with the present document, will facilitate the development of assurance schemes.  Many consumer IoT devices and their associated services process and store personal data.  The present document can help in ensuring that these are compliant with the General Data Protection Regulation (GDPR).  Security by design is an important principle that is endorsed by the present document."



Okay.  So my goal for today's discussion of this is to accurately convey how comprehensive this document, and the legislation that backs it up, actually is, which I was astonished by.  So here's the bullet point.  I assembled this from the document, just to give you a sense for how comprehensive it is.  So here's the list of its main topics:  No default universal passwords.  Implement a means to manage reports of vulnerabilities.  Keep software updated.  Securely store sensitive security parameters.  Communicate securely.  Minimize exposed attack surfaces.  Ensure software integrity.  Ensure that personal data is secure.  Make systems resilient to outages.  Examine system telemetry data.  Make it easy for users to delete their data.  Make installation and maintenance of devices easy.  Validate input data.  And data protection provisions for consumer IoT they go into at length.



And again, these are not, like, gee, we wish we had these.  The legislation is about the policy.  As I've always said, there's a complete difference between policy and, you know, mistakes.  Anybody can make a mistake; but policy is, you know, your stated goal.  And so this legislation enshrines these goals as the policy that consumer IoT devices sold in the UK must incorporate.  They must have these as policies, as operating goals which are implemented in the device.  So each of these major topics is broken down into multiple pieces, and each of those pieces is tagged with one of four possible requirement levels.



Now, here's where for a moment I was concerned that my enthusiasm for this was going to be dashed because we've got mandatory, recommended, conditionally mandatory, or conditionally recommended.  And I thought, oh, great.  Well, if everything is just recommended, then we've gotten nothing.  Turns out almost everything is mandatory.  So there are a few things where they've backed off of mandatory.  But for the most part it's mandatory across the board.



The scope of what the document covers is also very clearly laid out, that is, you know, to not allow people to say, oh, that doesn't apply to us, so we don't have to do that.  No, I don't think anyone's going to get a free pass here.  It says, and this is in the baseline requirements document:  "The present document specifies high-level security and data protection provisions for consumer IoT devices that are connected to network infrastructure."  So they even broadened it to network infrastructure, meaning if you're connected to something, this is for you.



They say, parens:  "...(such as the Internet or home network), and their interactions with associated services.  The associated services are out of scope.  A non-exhaustive list of examples of consumer IoT devices includes" - and it's pretty much what I just said.  But there's additional ones here:  "Connected children's toys and baby monitors; connected smoke detectors, door locks, and window sensors; IoT gateways, base stations, and hubs to which multiple devices may connect; smart cameras, TVs, and speakers; wearable health trackers; connected home automation and alarm systems, especially their gateways and hubs; connected appliances, such as washing machines and fridges; and smart home assistants."



They said:  "The present document provides basic guidance through examples and explanatory text for organizations involved in development and manufacturing of consumer IoT on how to implement those provisions.  Table B.1 provides a schema for the reader to give information about the implementation of the provisions."  Now, okay, Table B.1, that's the table which breaks down all the topics and subtopics with the mandatory or recommended categories.  But then the idea is that manufacturers will print this out and be required to fill in for every one of those items the compliance level of their device, attesting then to the fact that they have met the recommendations.



So they said:  "Devices that are not consumer IoT devices, for example those that are primarily intended to be used in manufacturing, healthcare, or other industrial applications, are not in scope of the present document."  So this is squarely aimed at consumer IoT residential-style devices.  Of course, once that's all that's available for the consumer, other enterprises and anyone who purchases the devices get the benefit of all these features which have been required at this level.  And they said:  "The present document has been developed primarily to help protect consumers; however, other users of consumer IoT equally benefit from the implementation of the provisions set out here."



Okay, now, the document does differentiate something it considers to be a "constrained device," basically.  And again, nobody gets a free pass on this one.  But they did recognize that there are some things that, while they're connected and consumer devices, they just - they have too many constraints to meet what is otherwise a set of very high bars.  So they said:  



"The present document addresses security considerations specific to constrained devices.  For example, window contact sensors, flood sensors, and energy switches are typically constrained devices."



So to give everyone a sense for how well thought-out and specific this is, here's the document's definition of what it means by a "constrained device."  They said:  "A constrained device is a device which has physical limitations in either the ability to process data, the ability to communicate data, the ability to store data, or the ability to interact with the user, due to restrictions that arise from its intended use."  They said:  "Note:  Physical limitations can be due to power supply, battery life, processing power, physical access, limited functionality, limited memory, or limited network bandwidth.  These limitations can require a constrained device to be supported by another device, such as a base station or companion device."



And they give some examples:  "A window sensor's battery cannot be charged or changed by the user; this is a constrained device."  Another example:  "The device cannot have its software updated due to storage limitations, resulting in hardware replacement or network isolation being the only options to manage a security vulnerability."  And that ends up being important because this astonishing baseline requirement also deals with firmware updating.  And yes, it has to be enabled by default.  This is just like, where did this come from?



Third example:  "A low-powered device uses a battery to enable it to be deployed in a range of locations.  Performing high-power cryptographic operations would quickly reduce the battery life, so it relies on a base station or hub to perform its validations of updates.  Another example, device has no display screen to validate binding codes for Bluetooth pairing.  Or five, the device has no ability to input, such as via a keyboard, any sorts of authentication information."  So obviously, where things are either impossible or impractical for these requirements to be met, well, they're excused under the grounds that it is a constrained device.  But otherwise no free pass.



Okay.  So the document's too long and detailed for me to go through in detail here; but I do want to, again, give everyone a sense for how well thought out, thorough, and serious this is.  So I want to look deeply at this "No universal default passwords."  And Leo, let's take our second break, and then I will do that.



LEO:  I'm sorry, I'm talking to Mom.  You want to say hi?  You want to say hi to Mom?  Say hi to Steve.



STEVE:  Hey, Mom.



LEO:  I'm going to run.  Go to dinner; okay?  I love you, Mama.  Bye.



STEVE:  That's right, it's 5:00 o'clock there.



LEO:  It's 5:00 o'clock there.  And you know what happens is she's got an alarm on her Amazon Echo to wake her up.  So she called me, and then it starts going off, and I had to explain to her how to go over to it and turn it off.  So I guess what she normally will do is leave, go to dinner, and by the time she gets back the alarm has stopped because I don't think she knew how to stop it.



STEVE:  Ah, these fancy consumer devices.



LEO:  She loves it.  You know, she wanted to thank me because I gave her - and I hope that this has a good password on it.  I gave her a - there's a company called Nixplay makes a 15-inch frame, it looks like a painting frame, but it's a digital photo frame.  And I can email pictures to it.  I control it from here.  So, and she just sits and looks at it all day.



STEVE:  Nice.  Aww.



LEO:  So it just makes her really happy.  She says it's like having my family here.  So anyway.  We will continue in a moment.  Sorry for that personal interlude.



STEVE:  Yeah, no problem.  And, boy, May 7th is going to be a busy day.



LEO:  It is.  The Apple event is May 7th.  RSA conference is going on.  You should come up sometime for that.



STEVE:  I have a large inventory of very aging and slowing down iPads.



LEO:  Oh, iPads.  You're an iPad user, I know.



STEVE:  So I've not - I love my iPads, and I've not purchased any for many years because there's, you know, no new iPads.



LEO:  There's no reason to.



STEVE:  Right.



LEO:  And I think that this time they're going to - they know this, by the way.  I mean, I've been using - I have an iPad 6 which I've had for three years.  I very rarely use my iPad Pro.  They've got to get a way to get...



STEVE:  I didn't even know they had numbers.



LEO:  Oh, well, they don't.  You have to know.  You have to know.



STEVE:  Oh.  I think I have a - I know I don't have the iPad 1.  Remember how that was like - it was like a whale?



LEO:  Oh, yeah.



STEVE:  It had a weird bowed back.



LEO:  Oh, yeah.  You don't want that, yeah.  No, we've come a long way.  But you will want these because they have OLED screens.  And what's intriguing is this rumor you're going to have the M4 and a lot of AI built in.  So this might be something you really do want, I think.



STEVE:  Well, and I read on a black screen with amber type, so that would be...



LEO:  Perfect for you.



STEVE:  ...great for low power consumption.



LEO:  I just ordered - there's a new Kobo reader, the Libra 2 Color, that uses color eInk.  And I'm very intrigued by that.  And because it will have some color.  It won't have amber on black, probably, like you just said.



STEVE:  All the color I've seen has been on...



LEO:  So it's washed out, yeah, yeah.



STEVE:  Yes, exactly, very low contrast.



LEO:  Well, we'll see.  It also has a stylus, and you can take notes and stuff.  So, you know...



STEVE:  I do love my ReMarkable.  You turned me onto that.



LEO:  Isn't that great?  Yeah.



STEVE:  That is the best thing.  It really is...



LEO:  Yeah, yeah, yeah.  It's great for coding because I use it all the time to sketch out, like, problems and what I'm [crosstalk].



STEVE:  Oh, that's exactly what I do.  I'm a big diagram drawer.



LEO:  Exactly, yeah.



STEVE:  When I'm, like, trying to parse something.  It's like, wait a minute, because that's the only way you can deal with the off-by-one problems is do little examples of it.



LEO:  Yup, see it.  Yup, I'm the same way.



STEVE:  Okay.  So as I said, I'm not going to go through the entire document.  It is too long and wonderful.  I mean, I already sound like I'm over-caffeinated at this point because I'm so excited by what is in here.  It's just astonishing.  I mean, again, it's like you took 20 years of this podcast and distilled all of its recommendations that we've come up with on the fly, when things have been obvious and when we've seen something go wrong over and over and over, it's like, okay, when are we finally going to do this?  And here it is.  And they just didn't miss anything.



LEO:  That's great.  That's fantastic.  Wow.



STEVE:  It's just, you know.  And again, it's not like - again, it's not recommendations, it's law.



LEO:  Yeah.



STEVE:  In the UK.  Okay.  So I do want to, like, do a little bit of a deeper look into the first one they address because it is crucially important, under the banner "No Universal Default Passwords."  They say:  "Where passwords are used, all consumer IoT device passwords shall be unique per device or defined by the user.  There are many mechanisms used for performing authentication, and passwords are not the only mechanism for authenticating a user to a device.  However, if they are used, following best practice on passwords is encouraged."  By which they mean, again, they have not softened this as a recommendation.  They're saying, you know, encourage people when they use a password to use a good one.



They said:  "Many consumer IoT devices are sold with universal default usernames and passwords such as admin/admin for user interfaces through to network protocols.  Continued usage of universal default values has been the source of many security issues in IoT, and the practice needs to be discontinued.  The above provision can be achieved by the use of pre-installed passwords that are unique per device and/or by requiring the user to choose a password that follows best practice as part of initialization, or by some other method that does not use passwords.  For example, during initialization a device generates certificates that are used to authenticate a user to the device via an associated service like a mobile application.



"To increase security, multifactor authentication, such as use of a password plus OTP procedure, can be used to better protect the device or an associated service.  Device security can further be strengthened by having unique and immutable identities.  Where pre-installed unique-per-device passwords are used, these shall be generated with a mechanism that reduces the risk of automated attacks against a class or type of device.  Pre-installed passwords must be sufficiently randomized.  Passwords with incremental counters such as 'password1,' 'password2' and so on are easily guessable.  Further, using a password that is related in an obvious way to public information sent over the air or within a network, such as MAC address or WiFi SSID, can allow for password retrieval using automated means.



"Authentication mechanisms used to authenticate users against a device shall use best practice cryptography, appropriate to the properties of the technology, risk, and usage."  And I should mention that also in here is a complete description of their use of all the terminology.  So here they just said "best practice cryptography."  But in an addendum they specifically outline what that means that is required to be used where they use this term.  So again, there's nothing that they missed.  They said:  "Where a user can authenticate against a device, the device shall provide to the user or an administrator a simple mechanism to change the authentication value used.



"When the device is not a constrained device, it shall have a mechanism available which makes brute-force attacks on authentication mechanisms via network interfaces impractical. For example, a device has a limitation on the number of authentication attempts within a certain time interval.  It also uses increasing time intervals between attempts.  Or the client application is able to lock an account or to delay additional authentication attempts after a limited number of failed authentication attempts.  This provision addresses attacks that perform 'credential stuffing'" - it actually says that in this document for consumer IoT devices - "or exhaust an entire key space.  It is important that these types of attacks are detected by the consumer IoT device and defended against, whilst guarding against a related threat of 'resource exhaustion' and denial of service attacks."



Incredible.  What I just summarized is broken into five individual provisions in the document, but each and every one of them is tagged as "Mandatory."  So, for example, if a device offers password-based authentication, it can no longer be shipped from the factory with a default password, and the device must also incorporate proactive defenses against brute force and credential stuffing attacks.  It must incorporate some form of lock-out mechanism.  This legislation changes everything.  It takes the well-understood, but still not often implemented, best practice, you know, even not yet implemented from, you know, high-end enterprise level devices, and mandates its use today for a residential doorbell.  This is huge.



Section 5.3 is titled "Keep software updated."  And picking some bits from it, for example, it says:  "Developing and deploying security updates in a timely manner is one of the most important actions a manufacturer can take to protect its customers and the wider technical ecosystem.  It is good practice that all software is kept updated and well maintained.  All software components in consumer IoT devices should be securely updateable.  When the device is not a constrained device, it shall have an update mechanism for the secure installation of updates."  They have in quotes:  "'Securely updateable' and also in quotes 'secure installation' means that there are adequate measures to prevent an attacker misusing the update mechanism.



"Measures can include the use of authentic software update servers, integrity protected communications channels, verifying the authenticity and integrity of software updates.  It is recognized that there are great varieties in software update mechanisms and what constitutes 'installation.'  An anti-rollback policy based on version checking can be used to prevent downgrade attacks.  Update mechanisms can range from the device downloading the update directly from a remote server, transmitted from a mobile application, or transferred over a USB or other physical interface.  If an attacker compromises this mechanism, it allows for a malicious version of the software to be installed on the device."  Meaning that there are provisions for preventing that happening.  Thus they're explaining what the danger is.



"An update shall be simple for the user to apply.  The degree of simplicity depends on the design and intended usage of the device.  An update that is simple to apply will be automatically applied, initiated using an associated service such as a mobile application, or via a web interface on the device.  If an update is difficult to apply, then that increases the chance that a user will repeatedly defer updating the device, thus leaving it in a vulnerable state.  Automatic mechanisms should be used for software updates.  If an automatic update fails, then a user can, in some circumstances, no longer be able to use a device.  Detection mechanisms such as watchdogs and the use of dual-bank flash or recovery partitions can ensure that the device returns to either a known good version or the factory state.



"Security updates can be provided for devices in a preventative manner, as part of automatic updates, which can remove security vulnerabilities before they are exploited.  Managing this can be complex, especially if there are parallel associated service updates, device updates, and other service updates to deal with. Therefore, a clear management and deployment plan is beneficial to the manufacturer, as is transparency to consumers about the current state of update support.  In many cases, publishing software updates involves multiple dependencies on other organizations such as manufacturers that produce sub-components; however, this is not a reason to withhold updates.  It can be useful for the manufacturer to consider the entire software supply chain in the development and deployment of security updates.



"It is often advisable not to bundle security updates with more complex software updates, such as feature updates.  A feature update that introduces new functionality can trigger additional requirements and delay delivery of the update to devices.  The device should check after initialization, and then periodically, whether security updates are available."  Again, the device should check.  They said:  "If the device supports automatic updates and/or update notifications, these should be enabled in the initialized state and configurable so that the user can enable, disable, or postpone installation of security updates and/or update notifications."



They said:  "It is important from a consumer rights and ownership perspective that the user is in control of whether or not they receive updates.  There are good reasons why a user may choose not to update, including security.  In addition, if an update is deployed and subsequently found to cause issues, manufacturers can ask users to not upgrade their software in order that those devices are not affected."  But they're saying, you know, again, secure by default.  Secure by policy.  If it can update itself, it should.  But recognize that there are reasons that it might not.



They said:  "The device shall use best practice cryptography to facilitate secure update mechanisms.  Security updates shall be timely.  'Timely' in the context of security updates can vary, depending on the particular issue and fix, as well as other factors such as the ability to reach a device or constrained device considerations.  It is important that a security update that fixes a critical vulnerability. i.e., one with potentially adverse effects of a large scale, is handled with appropriate priority by the manufacturer.  Due to the complex structure of modern software and the ubiquity of communication platforms, multiple stakeholders can be involved in a security update."



Okay.  So what we're talking here - and I just shared the tip of the iceberg.  But it is all like that.



LEO:  I have this vision of you last night or the night before, sitting by the fire, your feet up, you've got a little something to drink, you're reading through this.  Your beautiful wife is across the way sitting reading her magazine.  And every five minutes you go, "Wow."  Or, "Yes."  I imagine every step of the way because this is all the stuff you've been saying all this time.



STEVE:  I know.  It's astonishing, Leo.  I mean, it is.  They even have in there protocols which are used on the WAN should not be exposed - or on the LAN should never be exposed to the WAN.



LEO:  They came this close to saying "three dumb routers"; right?  I mean, it's impressive.  I am very impressed.



STEVE:  It is astonishing that this thing exists.



LEO:  Mm-hmm.



STEVE:  So again, grc.sc/etsi, E-T-S-I.  That will bring this 34 or whatever number of page things it was to you, and it is all like that.  What we're talking about here amounts to nothing less than the forced and immediate maturation of more than a decade of lazy consumer product security design.



LEO:  Yup, yup, yup.



STEVE:  End-user security is finally being prioritized over device development and support costs and, yes, even convenience.  It had to happen sometime, and all indication is that sooner or later it was going to need to be forced.  That happened yesterday.



LEO:  Wow.  And it's law; right?



STEVE:  It's law.



LEO:  I mean, in the UK this is the law.  Wow.



STEVE:  It is the law.



LEO:  That's amazing.



STEVE:  Yes.  And a law with teeth in it.  It's not like, you know, oh, you pay a $2,500 penalty.  No.  It's 10 million, or 4% of your revenue, whichever is greater. 



LEO:  It's significant, yeah.



STEVE:  Yeah.  So, I mean, basically, like, everything on the shelf that isn't already in compliance.  And we'll note that many routers, many consumer routers now are; right?  Although I don't think they've gotten - I think they're still, you know, like the default username and password still starts off with admin and password or something.  So, but they are updating themselves, and they do have that on by default.  So, you know, that's good.



But, you know, one of the reasons, in fact there was a reference somewhere to this crazy, huge, like the Mirai botnet that owe their existence to the fact that these policies have never been required, enforced, or present before.  And as a consequence of web authentication being on the WAN, and default usernames and passwords, these things could just be taken over.  And so the UK is finally saying, enough of this.  What?  You know, come on.  You guys haven't gotten your act together in the last decade?  Well, we're going to require it now.  If you want to sell stuff in the UK, you have to do this.



And it will probably increase cost a little.  But as we know, once this is done, it's in a chip.  You just stamp it out.  And so, yeah, users are going to have to learn that they're going to have to, you know, like read the directions instead of assuming it's admin/admin or just has no password.  And like, oh, well, I'll give it one later, and then never get around to it.  Instead, every device will have a unique password which they'll have to write down or change to make it one that they want.  But it won't be, you know, off the shelf, all of them sitting there on the shelf with the same password to start.  And again, that's just, like, one of 40 different topics that they've dealt with.



LEO:  Amazing.  Amazing.  It's amazing.



STEVE:  It's astonishing.



LEO:  Yeah, yeah.



STEVE:  Okay.  So while we were recording last week's podcast, the news dropped, thanks to you, Leo, covering it, that Google's plans for Chrome's full phaseout of third-party cookies would not be occurring this year as had been planned and, needless to say, much anticipated.  For this week's podcast I had hoped to follow up on that news to learn and then report on what was going on.  The beginning of my research into Google's interactions with the UK's CMA, their Competition and Markets Authority, suggested that everything is actually going quite well overall, only somewhat slower than was expected.  My preliminary examination suggested that the UK's concerns are actually more along the lines of whether Google's new system goes far enough.



And actually, having just covered what we covered, I'm not that surprised at this point because they really seem to be getting busy.  Where concerns had previously been raised, for example, that smaller advertisers may be disadvantaged, the UK now appears to be discounting those concerns and complaints.  But I ran out of time for this research because, Leo, in front of the fireplace as I was last evening, my glass of Cabernet got empty, and I thought, okay, well, we'll tackle this next week.  We ran out of space.  So I will have the news of this next week because there was lots of material, and it's possible to get a much better understanding of what's going on.



LEO:  You need an overflow podcast.  That's what you need.



STEVE:  That's right.



LEO:  No one would object if you decided to do one, that's for sure.



STEVE:  Okay.  So a bit of Closing the Loop, just two pieces of Closing the Loop feedback.  Guillermo Garca said:  "Hi, Steve.  Listening to the feedback on the counter race condition," which our listeners had a lot of fun with, he said:  "I wonder what would happen to a process that wants to increase the counter if the previous owner of the counter was switched out of context and did not return the ownership before being switched off.  Would this active process get stuck and have to wait for the previous one to regain context and return it?  Again, many thanks."



Okay.  Many of our listeners reported that they found the discussion of object ownership within a multithreaded environment very interesting.  But at no point did I talk about what happens when something goes wrong.  Right?  I just talked about if everybody does the right thing and behaves themselves, this is how cool it is when everything is perfect.  But, for example, notice that nothing actually prevents the shared counter from being incremented by a thread that does not first acquire ownership of the object.  In this instance, in the example that I painted, there is no enforcement.  It's all and only by agreement among the process's threads.  And since they're all part of the same process, it's in their best interest to abide by the rules mutually.



But it doesn't take a deliberate act to mess something up.  Bugs happen, as we know.  A typical bug in a complex multithreaded environment is that, for example, a thread will acquire ownership, then follow some code path that causes it to fail to release its ownership.  At that point, that counter can never be incremented again, and any and all threads that need to may stall, waiting for an object's ownership to be released.  And who among us, especially back in the early days of Windows, has not experienced an application lock up and freeze?  Or its menuing UI mysteriously stops responding.  Or the app apparently dies and refuses to do anything, although it's still there on the screen and looks like it should be going, but it's not.  Some things might still be functioning, whereas other things suddenly become non-responsive.



One of the most common causes of such things happening is that somehow the ownership of a shared object was not freed by its owner.  Threads can sometimes get into trouble.  If a thread, for example, attempts to divide by zero, that thread will be terminated by the operating system.  Or if a thread mistakenly attempts to execute some data, an illegal instruction can be encountered and, again, the thread will be immediately killed.  In any event, if that thread happened to own some shared objects at the time of its termination, they would likely not be freed.  And other threads, or even a re-spawn of the terminated thread, might then be unable to succeed ever again after that.  Shutting down and restarting the application might be the only way to clear out such stuck ownership.



Not surprisingly, many solutions for these sorts of problems have been created over time.  One of the best things about software is if you've got a problem, there's probably a way to fix it, which is what makes it so fun.  Not surprisingly, people have been very clever.  For example, there's a system known as "Structured Exception Handling," or SEH for short, which actually allows a thread to protect the system and itself from its own possible misdeeds.  I've implemented Structured Exception Handling in assembler, and I've used it when my code had no choice, for some reason, other than to try to do something that might fail catastrophically.  And what doing this allowed me was then to try something and not lose control, but to have that failure recoverable, and then I could deal with the consequences.  And there are also entirely different ways to manage shared object ownership than that simple exchange instruction which I chose specifically to demonstrate the simplest of all possible solutions, which it is.



But before I close out this conversation, I would be remiss if I did not mention one of the classic problems with multithreaded environments which is known as the "deadlock."  A deadlock can be created when two threads each separately own an object, but also need ownership of another object that the other owns.  In other words, say that there are two objects, both of which need to be simultaneously owned by a thread in order to complete some work.  One thread currently owns the first object, and the second thread currently owns the second object.  And each of them needs to obtain ownership of the object that the other one already has.



Both threads will patiently wait for something that will never occur, since neither will relinquish its ownership of the object it owns until it, however briefly, is able to obtain ownership of the object it needs.  Which the other thread owns, and it's also waiting for the object the first thread has.  So consequently, neither will ever succeed, and we have a classic deadlock, as it's known in computer science.



But Guillermo's question highlights something else that I did not talk about.  He talked about multiple processes sharing objects, in other words, inter-process object sharing; whereas all of my discussion has been about multiple threads within a single process, intra-process object sharing.  It is possible to share objects between processes.  For example, Windows allows this by using unique names to identify objects.  Then separate processes that know the common name for an object can open the object to obtain its handle, very much like opening a file by name, after which operating system calls can be used to check the shared object's status, to obtain and release ownership of it, and so on.



And it's also possible to set timeouts while waiting for an object's ownership to be granted.  If that amount of time passes, the object wait will be ended, and the waiting process will be notified that the object never became available during the amount of time that the thread said it was willing to wait for it.



And even returning to our original example with the exchange instruction, remember that a thread that wants to obtain ownership attempts to obtain it, and the result of the exchange instruction informs it whether or not it was successful.  If it was not successful, it's fully able to decide what to do next.  Right?  I mean, it's not - it doesn't have to wait forever.  It can go do other things and try again later.  Or it might ask the operating system to put it to sleep for some length of time.  That's an extremely friendly thing to do since the thread is voluntarily giving up the rest of its running time slice, which allows the OS to schedule other threads.  Then when the thread is reawakened by the operating system, it can again attempt to obtain ownership, and then decide what to do.



Anyway, I know I'm weird.  All this fascinates me.  And I have never encountered, as I said before, anything as pure and clean and gratifyingly complex as coding.  I get it.  It's not for everyone.  But if it is, it can be terrifically rewarding.  And I know, Leo, that you also love to code.



LEO:  Oh, I love it.  I live for it.



STEVE:  One last tiny bit of news.  Apparently The New York Times last week picked up on the story of, you know, we were talking about the LexisNexis selling drivers' driving habits.  Turns out that The New York Times had a story on Wednesday that  General Motors had "accidentally," says GM, enrolled millions of people into its "OnStar Smart Driver+" program.  Consequently, if consumers chose not to enroll through the phone app, it would do it anyway.  Unenrolling requires consumers to contact OnStar customer support line.  However, turns out some people do not trust them and have started stripping the electronic devices out of their cars.  So reports The New York Times.



Anyway, just a little bit of follow-up on that.  Mistakes had happened, and people, you know, we showed that detailed report last week, and I've seen several others since then, all looking identical because it's coming from the same company.  So, Leo, let's take our last break, or no, our second-to-last break.  I want to again update our listeners on a bit of sci-fi and where I am in my work.  And then we're going to get into our main topic.



LEO:  You bet.  By the way, the book club loved the Bobiverse.  So much so, a lot of them are now on to Book 2.  Anthony Nielson, who read Book 1 for the book club - it was Stacey's book club.  Stacey was a little reluctant.  She wasn't crazy about it because she didn't like Bob.  Which is, you know, if you don't like Bob, there's a lot of Bob in the Bobiverse.  But Anthony Nielson said, oh, yeah, Book 2 kind of eased his concerns about Book 1.  I'm going to have to reread the whole thing because September Book 5 is coming out.  Very exciting.  The fifth book of the Bobiverse.



STEVE:  Wait, there's going to be a fourth Bobiverse book?



LEO:  Yes.  There are four.  There's going to be another one.



STEVE:  You mean a fifth one; right.



LEO:  Yeah, it's amazing.



STEVE:  Cool.



LEO:  Anyway, so that was your recommendation.  Thank you, and I look forward to hearing more.



STEVE:  Well, and it came through our listeners.  It was from our listeners.



LEO:  Yes, I remember that, yeah.



STEVE:  Who kept saying to me, Steve, you know, check it out.  And I have to say that my taste often differs from our listeners.



LEO:  It's a little lightweight.  It's lightweight.  It's fun.  It's not...



STEVE:  Yeah.  And actually what I'll be recommending in a minute is also.



LEO:  Lightweight?



STEVE:  Yeah.



LEO:  Okay.  I'm reading "Hyperion" right now, which is the opposite of lightweight, one of the classic science fiction novels that I never got around to reading, so I'm enjoying it quite a bit.



STEVE:  Yeah, I think I have the paperback around here.



LEO:  Yeah, I mean, it's a classic; right?  I mean, that I've never read it is amazing.  Now back we go to Mr. G. and sci-fi time.



STEVE:  Yes, on the science fiction reading front I wanted to mention to our many listeners who've been enjoying Ryk Brown's ongoing Frontiers Saga, that book 11 of 15 in his third of five 15-book story arcs, became available yesterday in the Amazon U.S. store.  I received Ryk's announcement that his latest novel titled "The First Ranger" is now available for download in the U.S., although apparently international availability may lag a bit, as is apparently common.



I've received so much feedback through the years from our listeners who've enjoyed following this adventure - and it is one long adventure, now at 41 full-length novels - that I wanted to make sure everyone knew that book 11 was here now.  I told some family members and friends, and they just jumped up and down because there's just - if it's right for you, then it is really right for you.  It's primarily character driven.  He offers us very fully formed individuals with very distinct and at times annoying personalities.  And in a way it's a bit like Star Trek, where it's less about whiz-bang science fictional technologies than about how the various characters whom we've come to know over time, how they deal with what comes their way.  I find it very satisfying.  And in addition to many in this podcast audience, as I said, you know, I've turned friends and family members onto it, and they're completely hooked.



For those who have never looked at the series, as I mentioned, Amazon Kindle is where it is.  It's part of the Kindle Unlimited plan.  And the novels, if you just buy - if you're not a member of Kindle Unlimited, they're not very expensive if you just purchase them outright.  So here's what I know.  Anybody starting the first book will know within an hour whether they have just started into a journey that already has 41 additional books waiting for them.  And they are just as compelling as the first one.  So it's, you know, sci-fi is a passion of mine.  We've talked about it, Leo, you and I, through the, you know, we're in our 20th year of the podcast now.



LEO:  Oh, my god.  And your skin is still beautiful, by the way.



STEVE:  Many authors, many adventures, a lot of fun.



LEO:  Yes, yes.  I'm referring to something that happened before the show.  Don't worry, folks.  You didn't miss anything.



STEVE:  Okay.  And finally, on my own work front, last week I finished updating various GRC pages with the news that 6.1 was now available.  This is not 6.1's documentation, which is still quite sorely needed.  This is just enough to hold us over until I have email communication up and running, after which my plan is to plow into SpinRite's extreme need for documentation.  I cringe whenever someone asks a question that really should be there on the website, documented.  But I'm getting there as soon as I can.



But I have a method here.  And getting email up and having our podcast audience help me develop an email presence and a reputation is, you know, part of what it takes these days because, you know, as we know, spam is such a problem that the large receivers of email have gotten very picky about the bounce rates and spam flaggings and so forth.  So anyway, I want to get that working.  And then while that's happening I will be able to start working on the documentation.  So anyway, I'm working on email, and I know how many of our listeners are excited that Twitter will not be the only way to get a hold of me.



So our main topic.  Actually, after that first topic, you can see why it was competition for this one.  So we have two big ones.  Today's podcast title, "Passkeys:  A Shattered Dream?"  It gets its title from a blog posting from last Friday.  It was a thoughtful posting by a guy named William Brown, who is the author of a popular WebAuthn package for Rust.  In fact, it's pretty much the WebAuthn package for Rust.  It generated significant attention within the security community, and a little bit later within our own listener community because, after I had already chosen it as our topic, everybody started  tweeting me links saying, oh, my goodness, what do you think about this?  So his WebAuthn package is "webauthn-rs," which describes itself as "WebAuthn Framework for Rust Web Servers."



To remind everyone how and where WebAuthn fits within the overall Passkeys solution, it's the protocol and specification that a Passkeys client on the user's side uses to communicate with a web server that supports WebAuthn.  So, for example, just as a web server will offer some form of username and password login, possibly with additional factors such as time-based one-time passwords or something else, such a server might also offer support for the WebAuthn protocol as a means for allowing remote clients to identify and authenticate their identity over a network.  In the case of this author's Rust implementation of WebAuthn, he described WebAuthn by writing:



"WebAuthn is a modern approach to hardware-based authentication" - notice he says hardware-based authentication - "consisting of a user with an authenticator device, a browser or client that interacts with the device, and a server that is able to generate challenges and verify the authenticator's validity.  Users are able to enroll their own tokens through a registration process to be associated to their accounts, and then are able to log in using the token which performs as a cryptographic authentication.  This library" - meaning his that he wrote - "aims to provide useful functions and frameworks allowing you to integrate WebAuthn into Rust web servers.  This means the library implements the Relying Party component of the WebAuthn/FIDO2 workflow.  We provide template and example Javascript and web asm bindings to demonstrate the browser interactions required."



Okay, now, the only thing I'll note about what this author wrote, as I pointed out right at the start, and it might be significant, is that this appears to have first been written back in the earlier FIDO1 era when hardware dongles were the only way the FIDO group was willing to roll.  As we know, the requirement for purchasing a piece of hardware, while potentially ensuring greater security, was finally accepted to be a bar too high.  So the FIDO group basically capitulated to allow software-only solutions the privilege of authenticating with what evolved into FIDO2.



So my point is that the author of this WebAuthn crypto library for Rust appears to have started this back at the hardware-only dongle stage of FIDO1, and he simply changed "FIDO1" to "FIDO2" in his introduction.  This may be significant for what he subsequently wrote and published last Friday, since the introduction of FIDO2, with its accompanying Passkeys, promised to make his work far more relevant.



Before I share what he wrote Friday, I wanted to note that the section following that brief introduction, I really loved it, it was titled "Blockchain Support Policy."  Okay, now, this is for, right, a WebAuthn package that has nothing to do with blockchain.



LEO:  I'm thinking it's going to say "none," but okay, yeah.



STEVE:  Uh-huh, exactly.  So he said:  "Blockchain Support Policy."  And he wrote:  "This project does not and will not support any blockchain-related use cases.  We will not accept issues from organizations, or employees thereof, whose primary business is blockchain, cryptocurrency, NFTs, or so-called 'Web 3.0 technology.'"



LEO:  Right on, right on, right on.



STEVE:  Period.



LEO:  Period.  End of statement.  Yeah.



STEVE:  And of course, you know, we know why he said that; right?  There's been so much nonsense surrounding, you know, the "Blockchain will solve all of society's ills" nonsense, and especially within the identity authentication space, that it's easy to imagine how much of that this guy may have been fending off through the years.  Elsewhere he notes that his library has passed a security audit performed by SUSE Linux's product security, and that other security reviews are welcome.



And as a total aside, I thought it was also interesting that on the topic of compatibility he wrote, under "Known Supported Keys/Hardware," he said:  "We have extensively tested a variety of keys and devices, not limited to Yubico 5c, 5ci, FIPS, and Bio; Touch ID, Face ID, meaning iPhone, iPad, MacBook Pro; Android; Windows Hello with TPM; and soft tokens."  And then he said, and under "Known BROKEN Keys/Hardware," he notes:  "Pixel 3a, Pixel 4 + Chrome does not send correct attestation certificates, and ignores requested algorithms."  And he said:  "Not resolved."  And "Windows Hello with Older TPMs," he said, "Often use RSA-SHA1 signatures over attestation, which may allow credential compromise or falsification."



Okay.  So Friday he gave his blog posting the title, as I said, that I reused for today's podcast, although, well, his was "Passkeys: A Shattered Dream," although I added the question mark.  His posting was not a rhetorical question.  His was meant as a statement.  So before I share what William has written, I wanted to take a moment to note that in order to do justice to his choice of words, I'm going to again need to use a term on this podcast that makes me uncomfortable.



LEO:  Uh-oh.



STEVE:  Although the fact that the term was the American Dialect Society's Word of the Year for 2023 suggests that it's a term we're all destined to be encountering more and more often.  That term is "enshittification."



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  The American Dialect Society's Word of the Year last year, Leo.



LEO:  Yeah.



STEVE:  So I am somewhat eased about its usage due to its lineage and the fact that Wikipedia does not shy away from devoting a rather extensive page to its definition, its description, and its discussion with extensive examples of this happening.  Wikipedia describes "enshittification" as "the pattern of intentional decreasing quality observed in online services and products such as Amazon, Facebook, Google Search, Twitter, Bandcamp, Reddit, Uber, and Unity.  The term," they write, "was used by writer Cory Doctorow in November 2022, and the American Dialect Society selected it as its 2023 Word of the Year.  Doctorow has also used the term 'platform decay' to describe the same concept."



And, for what it's worth, you know, allow me to commend this Wikipedia page to our listeners.  I found it to be somewhat gratifying and affirming because, while I was reading and agreeing with everything it said, I felt a little bit less like the crotchety old-timer yelling at the kids to get off the lawn.  You know?  In other words, objectively and, sadly, deliberately, some things actually are getting worse.  It's not just that you and I, Leo, are getting older, and everything seems worse.



And also, not far into this, he refers to - "he" meaning, sorry, done with Wikipedia, "he" meaning William Brown, the author of this - refers to a system whose name is spelled "Kanidm," a term I had never encountered before.  Its home page explicitly explains it's pronounced "kar-nee-dee-em."  Even though there's no "R" anywhere to be seen.  I mean, I guess it's, I would say, "ka-nee-dee-um," maybe.  But, you know, "kar-nee-dee-em," which of course reminded me of "carpe diem."  And I don't know, maybe it's - I don't know where the name came from.



LEO:  It's intentional, I'm sure, yeah.



STEVE:  Yeah.



LEO:  Sade is pronounced "shar-day."  I think it's a British-ism.  The "R" is inserted, yeah.



STEVE:  Oh, that's interesting, you're right, it is.



LEO:  Yeah.



STEVE:  Yeah.  Okay.  Kanidm.  So it is a sprawling open source identity management platform developed in Rust by the SUSE Linux project.



LEO:  I've been looking for something like this.



STEVE:  Well, there it is.



LEO:  Okay.



STEVE:  It appears that William's WebAuthn library was adopted into that multi-faceted identify project to provide its WebAuthn functionality.  So when we hear him refer to, in his blog posting, "kar-nee-dee-um," he's refer to his library's significant participation in that project.  And that explains also why SUSE's security people have reviewed and approved of his library, because it's the one they chose for their big identity management platform.



Okay.  So he said:  "At around" - you know, Leo, let's take our last break before I get into this.



LEO:  Yeah, yeah.  Keep it a secret what he says.



STEVE:  Then I won't have to break in the middle of it.



LEO:  Right, right.  This is good.  I'm enjoying it.  And, yeah, we've all decided here that you can say "enshittification" because it's just a word with a bad word in the middle, but not in the beginning or the end.



STEVE:  Well, and encrappification...



LEO:  Doesn't have the same...



STEVE:  ...doesn't have quite the same ring to it.



LEO:  Yeah.  We've been saying "enshirtification," just like Kanidm.  You're adding a phantom "R."  I don't know.  That's confusing.



STEVE:  But does that mean you have to wear more shirts?



LEO:  Need to wear more shirts.



STEVE:  Yeah.



LEO:  Okay, now, back to the enshittification.



STEVE:  Of Passkeys.



LEO:  Of Passkeys.



STEVE:  Okay.  So the author of this WebAuthn library, well regarded, built into SUSE's Linux identity platform, written in Rust, lots of experience from back in the dongle days, the early FIDO1 days, he wrote last Friday:  "At around 11:00 p.m. last night, my partner went to change our lounge room lights with our home light control system.  When she tried to log in, her account could not be accessed.  Her Apple Keychain had deleted the Passkey she was using on that site.  This is just the icing on a long trail of enshittification that has undermined WebAuthn.  I'm over it at this point, and I think it's time to pour one out for Passkeys.  The irony is not lost on me that I'm about to release a new major version of webauthn-rs today as I write this.



"In 2019 I flew to my mate's place in Sydney and spent a week starting to write what is now the WebAuthn library for Rust.  In that time I found a number of issues in the standard and contributed improvements to the WebAuthn working group, even though it took a few years for those issues to be resolved.  I started to review spec changes and participate more in discussions.  At the time there was a lot of optimism that this technology could be the end of passwords.  You had three major use cases:  second factor, passwordless, and usernameless.  Second factor was a stepping stone toward the latter two.  Passwordless was where you would still type in an account name, then authenticate with PIN and touch your security key.  And usernameless was where the identity of your account was resident and thus discoverable on the key.  This was, from my view, seen as a niche concept by developers since, really, how hard is it for a site to have a checkbox that says 'remember me'?



"This library ended up with Kanidm being, to my knowledge, the very first open source identity management platform to implement 'passwordless,' which is now Passkeys.  The user experience was wonderful.  You went to Kanidm, typed in your username, and then were prompted to type your PIN and touch your key.  Simple, fast, easy.  For devices like your iPhone or Android, you would do similar, just touch your Touch ID and you're in.  It was so easy, so accessible.  I remember how it almost felt impossible that authentication could be cryptographic in nature, but so usable and trivial for consumers.  There really was the idea and goal within FIDO and WebAuthn that this could be 'the end of passwords.'



"This is what motivated me to continue to improve webauthn-rs.  Its reach has gone beyond what I expected, with parts of it being used in Firefox's authenticator-rs, a whole microcosm of Rust Identity Providers being created from this library and my work, and even other languages' WebAuthn implementations and password managers using our library as the reference implementation to test against.  I cannot understate how humbled I am by the influence webauthn-rs has had.



"However, warnings started to appear that the standard, the WebAuthn standard, was not as open as people envisioned.  The issue we have is well known:  Chrome controls a huge portion of the browser market, and development is tightly controlled by Google.  An example of the effect was the 'Authenticator Selection Extension' of the WebAuthn specification.  This specification extension is important for sites that have strict security requirements" - like, you know, the government - "because the extension supports the attestation of the make and model of the authenticator in use.  If you know that the website's attestation will only accept certain devices, then the browser should filter out and only allow those acceptable devices to participate."



So, like, just to pause here for a second, that would be so cool; right?  If your bank, for example, required more than just a browser-based Passkey because it is pure software, but needed a hardware dongle, or needed a biometric, you know, reaffirmation of your identity when you tell it that you want to transfer some amount of money somewhere, then you absolutely want this protocol to be able to specify the type of authentication device that would be used and for the browser to then prompt for that level of authentication.



Anyway, he says:  "However, Chrome never implemented it.  That alone led to the entire feature being removed from the spec.  It was removed because Chrome never implemented it.  This demonstrates that if Chrome doesn't like something in the specification, they can just veto it without consequence.  Later, the justification for this not being implemented was:  We never implemented it because we don't feel that authenticator discrimination is broadly a good thing.  They, users, should have the expectation that a given security key will broadly work where they want to use it."  He says:  "I want you to remember this quote and its implications:  Users should be able to use any device they choose without penalty."



He says:  "Now, I certainly agree with this notion for general sites on the Internet; but within a business where we have a policy around what devices may be acceptable, the ability to filter devices does matter."  So he says:  "This makes it possible to go to a corporate site and apparently successfully enroll a security key, only to then have it fail to register.  Even better if this burns up" - you know, consumes - "one of your limited resident key slots which cannot be deleted without a full reset of your device.  This might happen since the identity provider rejected the device's attestation."  And he says:  "That's right.  Even without this, identity providers can still discriminate against devices without this extension; but the user experience is much worse, and the consequences far more severe in some cases."



He says:  "The kicker is that Chrome has internal feature flags that they can use for Google's needs.  They can simply enable their own magic features that control authenticator models for their policy, while everyone else has to have a lesser experience.  The greater warning here is that many of these decisions are made at F2F," as he puts it, "Face to Face meetings held in the U.S.  This excludes the majority of international participants, leading some voices to be stronger than others.  It's hard to convince someone when you aren't in the room, even more so when the room is in a country that has a list of travel advisories for foreign travelers, including 'Violent crime is more common in the U.S. than in Australia,' 'There is a persistent threat of mass casualty violence and terrorist attacks in the U.S.,' and 'Medical costs in the U.S. are extremely high.  You may need to pay upfront for medical assistance.'"



Okay, now, the point he's making here is that Google has outrageously outsized power to decide what does and does not succeed in the world, due to their unilateral control of their Chrome browser.  That which Chrome does not support, dies.  And he's also observing something that might not ever occur to those of us who are happily camped out here in the U.S., which is that, unfortunately, the U.S. can apparently be somewhat frightening and expensive for volunteer open source developers wishing to have their voices heard from other countries.  His point is those voices are too easy for Google to ignore.



And Leo, when I was thinking about this, this brought to mind something that Stina Ehrensvard often mentioned to me through the years.  After founding Yubico in Sweden, she understood the critical importance of geographic location.  So she deliberately uprooted her young family and relocated to Silicon Valley.  She knew that, if she was going to succeed, she needed to be where the action was, and specifically to be able to attend face-to-face meetings with Google executives and others.  In the list of authenticators on William's webauthn-rs site, Yubico's products are all mentioned first because, when it mattered, she was there in person.  And I know, as you know, Leo, truth be told, it's often quite difficult to say no to Stina.



LEO:  Yeah, and that's how you met her at RSA coming down the escalator.



STEVE:  That's right.



LEO:  So you're right.  In person makes a big difference.



STEVE:  Yup.  Yup.



LEO:  But don't ask Marcus about what it means to be an open source developer in the United States, Marcus Hutchins, because of course he was arrested on the tarmac...



STEVE:  Trying to leave.



LEO:  Trying to leave the United States.



STEVE:  Yup.



LEO:  So I understand why there's a little chilling effect on open source developers here.



STEVE:  Well, and I guess you really do, I mean, I'm sure those travel advisories exist.  I don't know how much you have to heed them, but still.



Then, under the topic of, or the subtopic of "The Descent," as he put it, he said:  "In 2022 Apple announced Passkeys.  At the time, this was really just nice marketing, a nice marketing term for passwordless, and Apple's Passkeys had the ability to opportunistically be usernameless, as well.  It was, all in all, very polished and well done.  But of course thought leaders exist, and Apple hadn't defined what a Passkey was, exactly.  One of those thought leaders took to the FIDO conference stage and announced 'Passkeys are resident keys,' while at the same time they unleashed a Passkeys dev website.



"The issue is described in detail in another of my blog posts.  But to summarize," he writes, "this push to resident keys means that physical hardware security keys are excluded because they often have extremely low limits on storage, the largest being 25 for YubiKeys.  That simply won't cut it for most people, who have more than 25 accounts."  And that, I'll just mention, that's one of the biggest annoyances with the whole Passkeys technology is the requirement for significant storage per Passkey.  That is, you know, the big thing that I don't have.  The approach that I took with SQRL explicitly avoided that by, you know, being able to create per-domain, similar security per-domain keys, meaning that you only had to have one, instead of this problem, in the case of the YubiKey, they're able to store 25.  But once you hit that limit, you need another one.



LEO:  I know.  That's a frustration for me.  I wish they'd add more memory.



STEVE:  Yeah,  yeah.  Anyway, William then coins a term that Cory Doctorow might appreciate.  He terms the period following the announcement of Passkeys as "The Enshittocene Period."



LEO:  I like it.



STEVE:  Yeah.



LEO:  All right.



STEVE:  The Enshittocene Period.  He says:  "Since then, Passkeys are now seen as a way to capture users and audiences into a platform.  What better way to encourage long-term entrapment of users than by locking all their credentials into your platform; and, even better, credentials that cannot be extracted or exported in any way.  Both Chrome and Safari will force you into using either hybrid where you scan a QR code with your phone to authenticate.  To use a hardware security key requires clicking through multiple menus.  And even their default is not a good experience, taking more than 60 seconds' work in most cases.  The UI is beyond obnoxious at this point.  Sometimes I think the password game has a better user experience.



"The more egregious offender is Android, which won't even activate your security key if the website sends the set of options that are needed for Passkeys.  This means the identity provider gets to choose what device you enroll without your input.  And of course all the developer examples only show you the options to activate 'Google Passkeys stored in Google Password Manager.'  After all, why would you want to use anything else?



"A sobering pair of reads are the GitHub Passkey Beta and GitHub Passkey threads.  There are instances of users whose security keys are not able to be enrolled as the resident key slots are filled.  Multiple users describe that Android cannot create Passkeys due to platform bugs.  Some devices need firmware resets to create Passkeys.  Keys can be saved on the client, but not on the server, leading to duplicate account presence and credentials that don't work; or, worse, lead users to delete the real credentials.  The helplessness of users on these threads is obvious, and these are technical early adopters, the very users we need to be advocates for changing from passwords to Passkeys.  If these users cannot make it work, how will normal people from other disciplines fare?



"Externally, there are other issues.  Apple Keychain has personally wiped out all my Passkeys on three separate occasions.  There are external reports we've received of other users whose Keychain Passkeys have been wiped just like mine.  Consequently, as users we have the expectation that keys won't be created correctly, or they will have disappeared when we need them most.  In order to try to resolve this, the working group seems to be doubling down on more complex JavaScript APIs to try to patch over the issues that they created in the first place.  All this extra complexity comes with fragility and more bad experiences, but without resolving the underlying core problems.  It's a mess."



And then for the future, he says:  "At this point I think that Passkeys will fail in the hands of the general consumer population.  We missed our golden chance to eliminate passwords through a desire to capture markets and promote hype.  Corporate interests have overruled good user experience once again.  Just like ad blockers, I predict that Passkeys will only be used by a small subset of the technical population, and consumers will generally reject them."



LEO:  Wow.



STEVE:  "To reiterate, my partner, who is extremely intelligent, an avid computer gamer, and veterinary surgeon has sworn off Passkeys because the user experience is so crappy.  She wants to go back to passwords.  And I'm starting to agree.  A password manager gives a better experience than Passkeys.  That's right. I'm here saying," he writes, "passwords are a better experience than Passkeys.  Do you know how much it pains me to write this sentence?  And yes, that means multifactor authentication with time-based one-time passwords is still important for passwords that require memorization outside of a password manager.



"So do yourself a favor."  This is what he writes.  "Get something like Bitwarden; or, if you like self-hosting, get Vaultwarden.  Let it generate your passwords and manage them.  If you really want Passkeys, put them in a password manager you control."



LEO:  Oh, I agree 100%.  Yes.



STEVE:  "But don't use a platform-controlled Passkey store."



LEO:  Yes, yes.



STEVE:  "And be very careful with physical hardware security keys.  If you do want to use a security key, only use it to unlock your password manager and your email.



"Within enterprise, there still is a place for attested security keys where you can control the whole experience to avoid the vendor lock-in parts.  It still has rough edges, though.  Just today I found a browser that has broken attestation, which is not good.  You still have to dive through obnoxious user-experience elements that attempt to force you through the default QR code path, even though your identity provider will only accept certain security models, so you're still likely to have some confused users.



"Despite all this, I will continue to maintain webauthn-rs and its related projects.  They're still important to me even if I feel disappointed with the direction of the ecosystem.  But at this point, in Kanidm we're looking into device certificates and smartcards instead.  The UI is genuinely better.  Which says a lot considering the state of the PKCS11 and PIV specifications.  But at least PIV won't fall prone to attempts to enshittify it.  PIV stands for 'Personal Identity Verification.'  It's a standardized physical smartcard system that's heavily used by government and military.  The technology to create digital identity cards has been around for a long time, and they are so fraught with their own problems that they aren't really an alternative to solve the web's authentication needs."



So I think that, for me, the thing that's so sad is that Cory Doctorow's term, and the examples of enshittification that Wikipedia documented, make very clear that these are deliberate usury outcomes.  The shortest of Wikipedia's examples is what Uber did.  Wikipedia writes:  "App-based ridesharing company Uber gained market share by ignoring local licensing systems such as taxi medallions, while also keeping customer costs artificially low by subsidizing rides via venture capital funding.  Once they achieved a duopoly with competitor Lyft, the company implemented surge pricing to increase the cost of travel to riders and dynamically adjust the payments made to drivers."



So nearly all of the problems William observed in his posting are the things we on this podcast independently noted from the start as inherent problems with the way Passkeys have been rolled out.  As I've observed on several occasions, the fact that Passkeys were implemented in a non-portable way, as a vehicle for creating implicit platform lock-in, is almost a crime.



But the new thought that William proposes is something that had never occurred to me.  Perhaps it's because I've been blinded by Passkeys' superior public key technology which offers so many potential authentication benefits.  Even though the benefits are theoretical, I've never questioned whether or not Passkeys would eventually become the new standard for the web.  But William writes:  "At this point I think that Passkeys will fail in the hands of the general consumer population.  We missed our golden chance to eliminate passwords through a desire to capture markets and promote hype."



When I read that the first time I was surprised. But at the same time, I have still not adopted Passkeys.  I've never registered a single Passkey.  I don't have even one, anywhere.  I don't encounter websites that offer Passkey authentication, so there's that.  But mostly, because authentication matters crucially to me, I want to feel that I'm in control of my authentication.  And that starts with thoroughly and deeply understanding it.  I do thoroughly and deeply understand Passkeys' underlying cryptography.  But as William explains, what's then been done with that underlying crypto has been made deliberately opaque as a means of "just trust us" individual platform lock-in.  The problem is, my authentication is far too important for me to entrust to any company that might choose to, dare I say, enshittify it.



Having unique, per-site, insanely long high-entropy passwords that I can touch and feel and copy and paste and see, stored and managed by a cross-platform password manager, which is everywhere I need it to be, allows me to really understand the status of my authentication.  One thing I also have is a long and growing list of TOTP one-time passcodes.  And the reason I'm absolutely comfortable with that is that, again, they're tangible things that I can control, see, and understand.



Has the entire techie insider industry just been playing with itself this whole time?  Have we been imagining that authentication can and should be made entirely invisible because Passkeys can theoretically make that happen?  Will end users who don't know anything about the underlying technology say, "Well, I don't know how it works, but it certainly was easy?"  But then what about when it doesn't work?  What about when someone needs to log on from a device that's outside the provider's walled garden?  These are all problems we've previously identified and questions we've asked before.  I've always assumed that this was just the typical extreme adoption inertia we always see.  It never occurred to me that Passkeys might ultimately fail to ever obtain critical mass and to eventually become more dominant than passwords.



While poking around to get a broader perspective, I encountered a recent piece in Wired titled "I Stopped Using Passwords.  It's Great - and a Total Mess," with the intro "Passkeys are here to replace passwords.  When they work, it's a seamless vision of the future.  But don't ditch your old logins just yet."  The author explained that, as William said, things didn't always work.  He also noted that having multiple clients all popping up and asking whether you want to save Passkeys with them had become annoying.  But the biggest problem he had was remembering where he had stored which Passkey.  Now, as someone who spends some time pondering which of the multiple streaming providers carries the show my wife and I have been watching, that definitely resonated.



Our advice at the start of this Passkeys saga was to wait until a single provider offered Passkeys support across every platform that might conceivably be needed, since Passkeys portability was not something that anyone was even talking about back then.  In fact, back then it was clearly an overt password lock-in move.  So I wanted to share the news that Bitwarden, the solution that William's posting referred to and a sponsor of the TWiT network, earlier announced on the 10th of this month that Passkeys for iPhone and Android clients had just entered beta testing.  So I'm very glad to see that my chosen open source password manager will soon be offering Passkey support.



Now what's going to be needed, based upon the experience of the author of the Wired article, will be the ability to assign a single Passkey handler to a platform, much the way we currently assign a handler for a platform's URL links.  Having all of a platform's Passkey-aware clients popping up solicitations to store a Passkey with them seems like it would quickly become annoying.  On the other hand, you know, setting up a new Passkey doesn't happen that often.



On balance, I still don't feel much pressure to give up my use of passwords since they're working perfectly for me today.  The other factor is that website login has become so persistent that I rarely need to re-authenticate to most sites.  Each of the browsers I use carries a static cookie for each of the sites I frequent, so I'm already known everywhere I go.  For the foreseeable future, I expect to hang back and wait.  The dust is still settling on Passkeys, and Passkeys doesn't solve any problem I have today, even though they're cool.



LEO:  If SQRL had only taken off.  But you see the problem, which is...



STEVE:  Anything.  Anything new.



LEO:  Well, but it's not just that.  The platforms aren't going to support it unless they own it.



STEVE:  Right.



LEO:  And it gives them lock-in. 



STEVE:  Right.



LEO:  That's why Apple loves this.  That's why Google and Microsoft and - they want the lock-in.  And that's why I use, and I agree with him, only use Bitwarden or some sort of open source manager that you can at least take with you to do it.  But, oh, this is sad because it really - it's a great idea.  But the writing's on the wall.  Very few websites use it.



STEVE:  Yes.  Yes.  And if they start to use it, and then their users have problems with it, they'll pull it.



LEO:  Right.



STEVE:  I mean, it could disappear as an option because it's not worth it to them.



LEO:  Not worth it.  No benefit.



STEVE:  Everybody knows how to use a username and password.



LEO:  Sigh.  SQRL really solved all these problems, and that's sad because it really - it was - oh, well.  What can you do?  You've kind of gotten over it.  I'm still...



STEVE:  I've gotten over it.  I solved the problem.  I satisfied myself.  And we had a lot of fun developing it and working out all of the edge cases and so forth.  And, you know, now I'm on to solving other problems.



LEO:  This is where you jump up from your easy chair, grasp your Cabernet, and you shake your fist at the skies and say, "Why, I oughta...."  Steve Gibson, GRC.com.  That's his home on the Internet, the Gibson Research Corporation.  Go there to get SpinRite, the world's best mass storage maintenance and recovery utility.  6.1's out and fantastic.  If you've got an SSD, this is the kind of unexpected benefit of it.  You can use 6.1 to speed up your SSD.  That's fantastic.  What an improvement.



STEVE:  Yeah.  As I say, "recover lost performance."



LEO:  Yeah.  That's his promise, yeah.



STEVE:  It is data recovery and performance recovery.



LEO:  Yeah.  We'll have to say mass storage performance, recovery, and maintenance utility.  We'll add that.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#973

DATE:		May 7, 2024

TITLE:		Not So Fast

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-973.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What danger is presented by the world's dependence upon GPS?  And why is that of any concern?  Has the sky fallen on all VPN systems?  And why does the tech press appear to think so?  Today's myriad network authentication options are confusing and incomplete.  What does the future promise?  Why might Apple have been erasing iCloud Keychain data?  And what's actually going on between Google and the United Kingdom regarding the sunsetting of third-party cookies?  What's the problem?  Or is there one?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He'll talk about GPS fuzzing, how it works, what one can do to avoid it.  You've all heard about that VPN flaw that Ars Technica says makes all VPNs useless.  Not so fast.  Steve explains why it is not anything to panic about.  And then, speaking of not so fast, Google has stopped progress on abandoning third-party cookies.  Steve now knows why.  He will explain all that and a whole lot more coming up next on Security Now!.  Stay here.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 973, recorded Tuesday, May 7, 2024:  Not So Fast.



It's time for Security Now!, the show where we cover the latest security and computer news and privacy news, and of course a little sci-fi and TV thrown in, with this guy right here, Steve Gibson, the arbiter of all that is good and kind.  Hello, Steve.



STEVE GIBSON:  Oh, well.  I'll go for that.



LEO:  Yeah.



STEVE:  Yeah.  Hello, Leo.  So here we are at the beginning of May.  And as I promised, I did have some time to dig into the issue that came up actually two weeks ago when in the middle of the show you said, "Hey, Google just changed their plans on third-party cookies."  And I said, "What?"  Anyway, so we're going to talk about that.  Today's episode is titled "Not So Fast," which as in that expression, "Not so fast, there."  Which is what the UK is saying to Google.



But we're going to first look at what danger is presented by the world's current and growing dependence upon GPS, and why is that any concern?  Has the sky fallen on all VPN systems, as the tech press has been reporting since yesterday, when a blog post...



LEO:  Oh, good.



STEVE:  ...really went a little out of control.



LEO:  I was really hoping, and I wanted you to explain Option 102 or whatever.



STEVE:  Option 121.



LEO:  Oh, yes.  I really want to know about that.



STEVE:  We will know all about that by the time we're done today.



LEO:  Thank you.



STEVE:   Also a couple questions more from our listeners still bogged down in what is arguably a quagmire of network authentication options.  So I'm going to spend a little more - that's continuing to come into crisper focus for me, so I figured let's - I'm going to spend a little more time on what's going on there.  Also, we may have an answer to what Apple was doing with the iCloud Keychain deleting and what was going on, something that absolutely makes sense, so we're going to cover that.



And also, finally, as I said, I invested no little bit of time in - you'll hear the term "bureaucracy" used more times probably than any large word in this podcast because, boy, you know, I guess any kingdom that's been around as long as the United Kingdom and has continued to survive has also developed quite a system of bureaucrats, and they all want to weigh in on Google's plan.  So anyway, I think another great podcast for our listeners.  And a Picture of the Week that's kind of a hoot, too.



LEO:  Oh, good.  Always enjoy the Pictures of the Week.  Well, Security Now! is ready to get underway.  I hope you are, as well, boys and girls, cats and kittens, club members and others.  



STEVE:  Well, the important work will be appearing shortly.



LEO:  Hey, this is important work.  Do not knock this work.		



STEVE:  Now, we have a picture, a Picture of the Week, from somewhere, it looks like in the U.S. Southwest.  There's no signs of any telephone poles or structures, so we're kind of out in the desert somewhere.  And so one of the things that people want is they want their cell phones to work out in the middle of nowhere.  And actually this is a problem I have with many movies these days, which seem to forget that it's necessary to have a cell tower not too far away from where your cellular device is in order for it to get any connection.  You know, we see people wandering out in the middle of literally nowhere, and they're on the phone.  Unless the writers don't want them to be, in which case they're holding the phone up, you know, scanning around, trying to find a signal.



Well, the way we solve the problem of people wanting cell phone coverage wherever they are, yet nobody wanting to despoil the landscape as a means of providing it, is we come up with stealth cell phone towers.  And I'm not sure how truly stealthful this is because it looks a little square to be a cactus.  But I gave this picture the caption, "Oh, don't mind us.  We're just putting the lid back on the cactus."  Because this is clearly a cell phone tower cactus which is meant, I mean, it actually, you know, it's got the little extra, what do you call it, arm, off the side of the cactus, to make the whole thing look a little more cactus-like.



LEO:  It's so funny.



STEVE:  And actually you can see some other cacti in the neighborhood that look decidedly less mechanical than this one.



LEO:  All over Mexico you see these saguaro cactuses, and I guess the Southwest, as well.



STEVE:  Yeah.



LEO:  So, you know, you see it with a hundred others, you probably wouldn't look twice.  It's actually clever.



STEVE:  Yeah, it's certainly not an eyesore, looking like this thing would look like with the lid off, which we can see here because...



LEO:  Right.  The lid's off.



STEVE:  Yeah, the lid is off and the crane has lifted the lid off the cactus.  Anyway, I just got a kick out of this.  And I've seen fake palm trees, and I know that here on the so-called sort of now the famous 405 in Southern California there are power lines that run alongside the freeway, and every, like, very often there's a big cluster of cell equipment on the power lines because it's a perfect place for them to be, you know, there's already a right-of-way.  There's some ability to run a service vehicle along the back and so forth.  And many, many, many moons ago, back in the SpinRite - actually it was after SpinRite 2 because I remember I was working on SpinRite 3, I built a building in Aliso Viejo, you know, a corporate headquarters, 20,000 square feet, two stories, and 1.43 acres of land and so forth.



LEO:  Holy moly.  Wow.



STEVE:  Anyway, the cell companies came to me and said, hey, this building is like up on a point on a bluff looking out over this valley.  You can make some extra money by letting us put some cell things, like ringing along the edge of your roof.  Well, you know what my answer was.



LEO:  You said no?



STEVE:  The same answer - I said no.



LEO:  Why?



STEVE:  This is a beautiful building.  I'm not going to have warts of cell crap all over the...



LEO:  I bet they're there now, Steve.



STEVE:  They are.



LEO:  Oh, yeah.



STEVE:  I mention that because I drove by not long ago, looking wistfully up at the building, and there it was, just - I don't know.  I don't think you could get more cell tower crap around the perimeter of this roof than there is there now.  But not while I was in control.  But immediately after I left, apparently.  Anyway, such is the world, you know.  And that's why I also have no ads on my site.  Mark Thompson made a case, he said, at one point he said, "Steve, there's something wrong now with a website that doesn't have ads."



LEO:  Yeah, what's wrong with you?  Yeah.



STEVE:  No, thank you.  Anyway, I wanted to start off this week by sharing an important piece of interesting news that's not Internet security-related, that is nevertheless potentially quite a big and serious issue in the real world.  Last Thursday's headline in Wired was "The Dangerous Rise in GPS Attacks," with the subhead "Thousands of planes and ships are facing GPS jamming and spoofing.  Experts are warning these attacks could potentially impact critical infrastructure, communication networks, and more."



Okay.  So I thought that was interesting, got my attention.  They said:  "The disruption to GPS services started getting worse on Christmas Day."  Meaning at the end of 2023.  "Planes and ships moving around southern Sweden and Poland lost connectivity as their radio signals were interfered with.  Since then, the region around the Baltic Sea - including neighboring Germany, Finland, Estonia, Latvia, and Lithuania - has faced persistent attacks against GPS systems.



"Tens of thousands of planes flying in the region have reported problems with their navigation systems in recent months amid widespread jamming attacks, which make GPS inoperable.  As the attacks have grown" - no surprise to anyone - "Russia has increasingly been blamed, with open source researchers tracking the source to Russian regions such as Kaliningrad.  In one instance, signals were disrupted for 47 hours continuously.  On Monday, marking one of the most serious incidents yet, airline Finnair canceled its flights to Tartu, Estonia for a month, after GPS interference forced two of its planes to abort landings at the airport and turn around."  Talk about dependence on GPS.  Apparently you just can't land anymore without it.



"The jamming in the Baltic region," they wrote, "which was first spotted in early 2022, is just the tip of the iceberg.  In recent years there's been a rapid uptick in attacks against GPS signals and wider satellite navigation systems, known as GNSS (Generic Satellite Navigation), including those of Europe, China, and Russia.  The attacks can jam signals, essentially forcing them offline, or spoof the signals, making aircraft and ships appear at false locations on maps."  Which you can imagine might be even more damaging than just jamming outright.  "Beyond the Baltics, war zone areas around Ukraine and the Middle East have also seen sharp rises in GPS disruptions, including signal blocking meant to disrupt airborne attacks."  Which actually, as we'll see a little bit later, I think is the actual goal of this because of the degree to which drones are now using GPS.



Wired wrote:  "Now governments, telecom, and airline safety experts are increasingly sounding the alarm about the disruptions and the potential for major disasters.  Foreign ministers in Estonia, Latvia, and Lithuania have all blamed Russia for GPS issues in the Baltics this week and said the threat should be taken seriously.  Jimmie Adamsson, the chief of public affairs for the Swedish Navy, told Wired:  'It cannot be ruled out that this jamming is a form of hybrid warfare with the aim of creating uncertainty and unrest.  Of course there are concerns, mostly for civilian shipping and aviation, that an accident will occur, creating an environmental disaster.  There's also a risk that ships and aircraft will suspend their traffic to this area and thereby affect global trade.'



"Joe Wagner, a spokesperson from Germany's Federal Office of Information Security, told Wired:  'A growing threat situation must be expected in connection with GPS jamming.'  Wagner said there are technical ways to reduce its impact.  Officials in Finland say they have also seen an increase in airline disruptions in and around the country.  And a spokesperson for the International Telecommunication Union, a United Nations agency, told Wired that the number of jamming and spoofing incidents have 'increased significantly' over the past four years, and interfering with radio signals is prohibited under the ITU's rules."  Gee.  You think Russia is slowed down by a NATO agency, the International Telecommunications Union, saying, well, you shouldn't be doing that?  Right.



"Attacks against GPS, and the wider GNSS category, come in two forms.  First, GPS jamming overwhelms the radio signals that make up GPS and make the systems unusable.  Second, spoofing attacks" - which actually are far more sophisticated - "can replace the original signal with a new location.  Spoofed ships can, for example, appear on maps as if they're at inland airports."  And actually that did happen recently.  "Both types of interference have increased in frequency.  Disruptions, at least at this stage, mostly impact planes flying at high altitudes and ships that can be in open water, not people's individual phones or other systems that rely on GPS.



"Within the Baltic region, 46,000 aircraft showed potential signs of jamming between August 2023 and March this year, according to reports and data from tracking service GPSJam.  Benoit Figuet, an academic at the Zurich University of Applied Sciences who also runs a live GPS spoofing map" - there is such a thing - "says there have been an additional 44,000 spoofing incidents logged since the start of this year.  Earlier this month more than 15,000 planes - earlier this month more than 15,000 planes had their locations spoofed to Beirut Airport, according to data that Figuet shared with Wired.  More than 10,000 were spoofed to the Cairo Airport, while more than 2,000 had their locations showing in Yaroslavl, Russia, the data shows.



"Separate analysis from geospatial intelligence company Geollect shared with Wired showed that on April 16th around 55 ships broadcast their location as being over the main runway at Simferopol International Airport in Crimea, Ukraine.  The airport is around 19 miles inland from the Black Sea, where it's believed the ships were actually located."  So, yeah, it's no longer possible to believe what GPS is showing you.  You need to look out the window and see where you actually are.



"Zach Clements, a graduate research assistant at the University of Texas here in Austin, said:  'The biggest change in the past six months is definitely the amount of spoofing.'"  As I said, spoofing is far more sophisticated and difficult than just jamming, and potentially far more dangerous.  "He said:  'For the first time, we're seeing widespread disruptions in civil aviation, especially in the Eastern Mediterranean, the Baltics, and the Middle East.  In prior years, there were reports of spoofing impacting marine vessels, but not aviation.'



"Clements says there appear to be three spoofers that can be traced back to Russia.  One open source intelligence analyst, going by the pseudonym Markus Jonsson, has located jamming in the Baltics, and that which impacted the Finnish airline this week" - so that was the one that was causing them trouble - "to Kaliningrad and other Russian locations.  One research group has suggested disruption near Poland impacted Russia's own GNSS system less than others."  Not surprisingly, Russia doesn't want to hurt themselves.  They just want to disturb everybody else.  And "Russia has a long history of interfering with GPS signals, both within its borders and internationally.  Russia's embassy" - not surprisingly - "in the UK did not respond to a request for comment.



"The disruptions can cause uncertainty and potential safety issues for airline pilots and their passengers."  Yeah, no kidding.  "A spokesperson for Eurocontrol, a European aviation organization with more than 40 countries as members, says its analysis shows disruptions are happening in the Eastern Mediterranean, areas around Ukraine and the Black Sea, as well as the Baltic states.  During one week in March, 4,387 aircraft reported issues.  The Eurocontrol spokesman says for the same time last week there were 2,646 flights reporting problems.



"The Eurocontrol spokesman says planes can fly safely without GNSS, but interference 'puts a higher workload on pilots and air traffic control.'  A safety notice issued by the UK's Civil Aviation Authority this month says loss of GNSS, which is just, you know, general satellite-based navigation, can result in serious navigation issues, incorrect emergency terrain warnings that the plane is too low to the ground, and failure to various other systems."



And finally, in a NASA report detailing GPS incidents that was also published this month, one pilot said:  "I have flown with crew members who were not fully aware of this problem."  Other pilots said they had received "false terrain warnings" that caused them to pull up.



LEO:  Ooh, that's not good.



STEVE:  Yes.  And that pilots should have a "thorough review of jamming effects on the different aircraft systems" as part of their training.  And here's the problem, of course.  Because this is a relatively new phenomenon relative to when the pilots were trained, it may just be the fact that the pilots are trusting their avionics and not being sufficiently skeptical.  So it does look like these GPS disruptions are coinciding with Russia's full-scale war in Ukraine.



And also it looks like Israel's attacks in Gaza have also been tied into this.  As we know, disrupting GPS as part of electronic warfare has become common on Russia and Ukraine's battlefield as a way to try to limit the operation of drones.  And while Iran launched a barrage of missiles and drones against Israel last month on the 13th, Israeli GPS disruption designed to limit the impact of the attack also impacted mapping and taxi services, as well as food delivery.  So here was an instance of Israel doing some GPS jamming which was somewhat indiscriminate, and the mapping and taxi services as well as food delivery within their own country took a hit as a consequence.



"Kevin Heneka," writes Wired, "the founder of cybersecurity company Hensec, whose work includes detecting GPS disruptions, says jamming and spoofing technology has become cheaper and smaller over the years, to the extent that individuals can install them in their cars to hide their own movements."  That is, you know, you're blocking your own GPS receiver so your car doesn't know where it is.  "However," Heneka says, "more sophisticated attacks use equipment that can cost huge sums."  Yes.  Anytime you're doing spoofing, as I said, spoofing is a whole 'nother level than just blanket jamming.  He said:  "In conflict zones, in military terms and in professional terms, this spoofing is very sophisticated, and it always goes hand in hand with jamming."



Okay.  So since both the jamming and the location spoofing disruptions are enabled through the use of very powerful local radio transmitters, which overwhelm the reception of the authentic signals being beamed down from the GPS satellite systems in orbit, so long as you're not in the region of the Baltics, where Russia appears to have taken serious action to create major disruptions, the good news is, these attacks are inherently local in nature.  You know, here in the U.S. we're not being affected by it at all, as is most of Europe.  They are inherently very local.



But the problem for those who are in the region is that GPS and the wider GNSS, which again, Global Navigation Satellite System, have always been incredibly reliable sources.  And not just of location, but also of time.  They are master sources of essentially time of day.  And as we know, when something is both very useful and has earned the reputation for also being very reliable, I mean, you know, these things are up in the sky beaming down at us, they end up creating a strong dependence.  We end up becoming very dependent upon them.  So many modern, non-military, commercial systems have become so reliant upon GPS that the deliberate disruption of that service for military purposes such as Russia has likely been perpetrating, can cause dramatic collateral damage.



The GPS system, which is put out by the U.S., was conceived quite a while ago, a little over 50 years ago, back in 1973.  It took five years to package this in the first satellite that began launching at that time, and today we have 24 satellites up in the GPS constellation.  They've been up and operating since 1993.  And talk about depending upon something that's more fragile than we might want.  Our phones and automobiles today only know where they are largely thanks to GPS signals coming from space.



LEO:  I only know where I am thanks to GPS signals.  Forget the car.



STEVE:  Yeah.



LEO:   I can't drive without GPS.



STEVE:  And I'm sure, you know, a sports wristwatch, you know, health-tracking wristwatches are doing the same thing.



LEO:  Oh, absolutely, very much.



STEVE:  And we have recently been talking about the militarization of space and the idea that having satellites attacking one another "up there" is no longer the territory of James Bond science fiction.  You know, it's actually happening.  In some cases robot satellites are there in order to repair others.  But the same robot that can function to fix a broken antenna can also go over and break one off of some other satellite.  So unfortunately they also have multiple purposes.



And unfortunately, as global political tensions increase, we can hope, and we need to hope, that no major powers having space-based military capabilities, nor the ability to kill satellites from the ground, believe that denying the entire world these benefits would create an advantage for them because it's difficult now to conceive of a world where GPS was just shut down, like destroyed deliberately by a power hostile to - it wouldn't even necessarily have to be hostile to the U.S.  It could be because everyone's using GPS, killing it for everyone also succeeds in killing it for a specific targeted country.



Before GPS, the only way for something to know where it was, was through a system of inertial navigation.  Inertial navigation, like its name suggests, is a closed system which relies upon the system's precise measurement of its own linear and angular accelerations.  It integrates those over time to determine its velocities, and then integrates those over time to determine its velocities, and then integrates those over time to determine its position.  Even though inertial navigation systems are still in use due to the nearly instantaneous position, and especially angular feedback that they provide, the errors that tend to creep in over time can only be eliminated with the use of slower but far more accurate input from the global GPS system.



I suspect Russia's primary concern is with the use of autonomous military drones, which may rely upon GPS to determine their in-flight location.  But since the risks presented by GPS jamming, although they haven't been prevalent, and it hasn't been a big concern for airline pilots until recently, especially operating over there in the East in the Baltic areas, since jamming has been a possibility for some time, I suspect that the latest technologies are much more immune to GPS outages than those in Russia might wish.



Given all of the advantages and the advances made in vision and in real-time recognition, I would be surprised if the latest autonomous technologies were not able to fly nearly as well by sight as they can these days by GPS.  They might well use GPS as a first choice, but use vision to detect location spoofing, while also being able to switch to pure vision if GPS should fail completely.  And another likely strategy which, again, you don't worry about or deal with until it becomes a problem, is that since GPS signals will always be originating from above, would be to shield any GPS receiver and its antennas...



LEO:  Oh, from below.



STEVE:  Yes.



LEO:  Because the jammers are on the ground.



STEVE:  Exactly.



LEO:  That's clever.



STEVE:  Yeah.  So planes can do that because they're well above ground.  Unfortunately, it's probably not practical for ships at sea.



LEO:  Yeah, I mean, when you listen to ground air traffic control talking to an airplane, which I used to always do on United Channel 9, used to love to do that, they often have visual markers, you know, they say "Turn right at the Big Rock Candy Mountain" and things like that.  I don't know if they still do that.  I haven't listened in a while.  But I bet they do.  I mean, there's always - you always want redundancy in any system like that; right?



STEVE:  Yeah.  And of course the problem is that, you know, we all - okay.  I remember, Leo, when I guess this must have been in driver ed, we were supposed to go out and walk around our car to check all four tires.



LEO:  Yeah, we don't do that anymore.



STEVE:  No.



LEO:  Do you do that?



STEVE:  When was the last time anybody did that?



LEO:  Pilots do that.  And commercial jet pilots do that.  And I think that goodness that they do.  I think that's really great.  But, no, I haven't done that to my car in a while.  I figure if it's flat, I'll know.  Right?  It'll go frump, frump, frump.



STEVE:  That's right.  That's right.   But I do remember being told that's what we're supposed to do.  So here we have a problem where GPS has been so reliable and relied on that I'm just hoping, I mean, in this NASA report last week where one of the guys said, you know, I've been with flight crews that just assumed that the GPS was telling the truth, even though they were suddenly being told to pull up because you're about to hit the Rock Candy Mountain, and that would not be good.  But there's nothing there.



LEO:  Pull up.  Pull up.



STEVE:  So Leo, let's take another break, and then we're going to talk about whether the sky is falling on all VPN systems.



LEO:  Yeah.



STEVE:  As the tech press seems to believe.



LEO:  I was counting on you to cover this because I read the stories.  Thank god you're covering it before I actually did the stories.  Keep me out of trouble, please.  Now, what's all this about VPNs, Steve?



STEVE:  Okay.  So yesterday, Ars Technica got a little carried away in their reporting of what amounts to a clever hack that a Seattle, Washington-based pen testing firm known as the Leviathan Security Group posted in their blog.  And of course the rest of the tech press picked up on it quickly, too.  The blog posting carried the headline "How Attackers Can Decloak Routing-Based VPNs for a Total VPN Leak."  And what I found curious was that they assigned - "they" meaning the Leviathan Security Group - assigned a CVE number to their discovery, even though nothing about this is a bug or a flaw.



LEO:  Oh.



STEVE:  It's just a clever local exploit of a little-used feature of DHCP servers.  Unfortunately, Ars Technica's headline for their story was headlined "Novel attack against virtually all VPN apps neuters their entire purpose."  Agh, run away.  Okay, which of course makes this sound more like the end of VPNs as we've known them.  It isn't.  Here's what's going on.



Okay.  So going to do a bunch of propeller head cool stuff in order to get a real grip on this.  Our PCs all interact with both internal and external networks through network interfaces.  Most systems typically have a single physical network interface, or NIC; but it's possible for a machine to have more than one physical network interface with each interface connected to different physical networks.  In that case, it's important for outgoing network traffic to know which physical interface any given packet should be routed out through.



To answer that question, our machines contain a routing table.  The routing table performs a "most specific match" function, based upon the destination IP address.  And in years past we've talked about Internet routing tables and all of this.  So we've covered this in detail.  But the key here is most specific match.  And that all of our PCs, every one of them, pads, phones, you name it, anything that's networked using Internet protocol, IP protocol, has a routing table.  Under Windows, for example, opening a command prompt and entering the command "route print" will display a list of the system's interfaces, followed by the IPv4 and IPv6 routing tables, respectively.  And they're interesting, and you can get a sense for the fact that there's a lot going on under the covers that we don't appreciate, we normally don't even see. 



Okay.  So this set of network communication, that is, IP-based network communication, comes in so handy that in addition to true physical interfaces, many of our machines will have one or more virtual network interfaces.   In fact, the so-called "localhost," you know 127.0.0.1, that's a virtual network interface that all stacks have.  And, for example, the use of virtual machines has become very popular, and they create their own virtual network interfaces to talk to their host machine, as well as to the outside world.



Okay.  So here's the main point:  Many VPNs, like OpenVPN for example, operate by creating their own virtual interface in the hosting machine.  It looks like and operates like any other network interface.  But being a VPN (Virtual Private Network) which is used to transact privately with encryption, any packets sent out of that virtual interface are first encrypted, then rerouted out of an actual physical interface to be sent to the VPN's matching endpoint.



Since the typical VPN user, while using a VPN, wants all of their machine's traffic to be tunneled through the VPN, when the VPN tunnel is brought up, the VPN software dynamically edits the system's global routing table in such a way that, instead of the system's traffic by default being routed out through its normal actual physical interface, all of its traffic is instead routed to the VPN's software-created virtual network interface.  This is the way that, deep down inside the guts of our machines, all of the traffic that's normally unencrypted suddenly becomes encrypted when we activate our VPN.



Essentially, it's like a man in the middle.  It sticks a shim into our network so that all of the traffic that would normally just go straight out that physical interface instead is routed to the VPN.  And that's done, as I said, by making just a slight change to the routing table so that all of the traffic, instead of going out the physical interface, goes to the VPN.



We need one other piece of information just to be certain that everyone's on the same page.  DHCP stands for Dynamic Host Configuration Protocol.  By default, when any networked machine boots up and gets itself going, it needs to be using an IP address for itself on its local network that's unique for that network.  And it needs to know the IP address to which it should address packets bound for the outside world, in other words, the network's gateway IP.  It may also want to know the IP addresses of some DNS servers that will honor its requests for domain name lookup.



It's the network's inward-facing DHCP server that answers all these needs.  When any networked machine starts up, by default it will emit a broadcast packet onto the network announcing its presence and asking for any listening DHCP server to please provide it with all the information it requires to become a well-behaving citizen on the local network and to connect to the rest of the global Internet.



DHCP cleanly organizes the various types of information it can supply into, like, to the clients who are requesting it, by number.  Each one of these is known as an "option," where the option number is a single byte, thus having a value from 0 to 255.  Zero is a null option and can be used for padding; 255 is the marker for the end of the list of options.  So the options are provided as a list of information terminated by Option 255, which of course, you know, is a byte of all ones.



So, for example, Option 1 provides the network's subnet mask to the requesting client.  Option 2 specifies the offset of the client's subnet in seconds, that is, in real-time, from UTC, Coordinated Universal Time.  Option 3 specifies a list of the IP addresses of routers on the client's subnet, what we know as the Gateway IP.  Option 4 specifies a list of time servers which are available to the client.  Number 6 provides a list of DNS servers for the client's use.  And, you know, there's a bunch of them, all kinds of different things that have been added through the years.  And there are even some surprises.  For example, options 69 and 70 provide the IP addresses of SMTP and POP3 email servers, which I thought was kind of cool.  We're all used to specifying those ourselves; but back in 1997, when this was first created, that information was available via DHCP.



Something else that DHCP was able to provide is the source of today's trouble.  The RFC's definition for Option 33 defines it as the "Static Route Option" and says:  "This option specifies a list of static routes that the client should install in its routing cache."  Okay, now, everybody who's been paying attention and enjoys networking stuff just went "aha" and knows what the problem is.



This thing continues:  "If multiple routes to the same destination are specified, they are listed in descending order of priority.  The routes consist of a list of IP address pairs.  The first address is the destination address, and the second address is the router for the destination."  Again, if some of you just said "Oh, crap!" that would be the correct reaction.  What this means...



LEO:  And it would mean they're paying attention.  Good job.



STEVE:  That's right.  That's right.  What this means is that the response from a DHCP server can be used to mess with a machine's routing table.  And as we noted earlier, a machine's traffic is routed to the VPN's virtual interface through a dynamic modification of the machine's routing table.



Now, as it happens, Option 33 is not really the problem because it was defined back in 1997 when IP networks were all class A, B, or C.  That meant that networks were defined to always have exactly one, two, or three bytes of host machine addresses.  As we know, this was extremely wasteful of IP addresses for networks falling into intermediate sizes.  So something known as CIDR, C-I-D-R, which stood for Classless Inter Domain Routing, was adopted.  That's what we have today, where the network mask can have any number of contiguous bits set, thus allowing scaling of networks by factors of two, all the way from one machine, well, technically up to 4.3 billion, but no one network has that except the Internet itself.



Okay.  So the adoption of CIDR obsoleted Option 33, forcing its replacement five years later in 2002 under the guidance of RFC 3442 which introduced Option 121, which allows for exactly the same thing, but under the specification of classless static routes.



Now, I mentioned that I was surprised that these Leviathan Security Group guys had arranged to get a CVE assigned for this, since technically this is a feature, not a bug.  And all the way back in 1997 the fundamental vulnerability of DHCP was quite well understood.  Again, 1997, Section 7 of the original RFC 2131 dated March of 1997, is titled, it was Section No. 7, "Security Considerations."  It says:  "DHCP is built directly on UDP and IP, which are as yet inherently insecure.  Furthermore, DHCP is generally intended to make maintenance of remote and/or diskless hosts easier.  While perhaps not impossible, configuring such hosts with passwords or keys may be difficult and inconvenient.  Therefore, DHCP in its current form" - which, by the way, is the form it has today in 2024 because, you know, if it's not broke - "in its current form is quite insecure," says the RFC from 1997.



LEO:  Wow.



STEVE:  They said:  "Unauthorized DHCP servers may be easily set up.  Such servers can then send false and potentially disruptive information to clients such as incorrect or duplicate IP addresses, incorrect routing information including spoofing routers, et cetera, incorrect domain nameserver addresses to spoof nameservers, and so on.  Clearly," they wrote, "once this seed information is in place, an attacker can further compromise affected systems."



Okay.  So here's how the Leviathan folks describe the attack they've devised by abusing Option 121.  They said:  "Our technique is to run a DHCP server on the same network as a targeted VPN user and to also set our DHCP configuration to use itself as a gateway.  When the traffic hits our gateway, we use traffic forwarding rules on the DHCP server to pass traffic through to a legitimate gateway while we snoop on it.  We use DHCP Option 121 to set a route on the VPN user's routing table.  The route we set is arbitrary, and we can also set multiple routes if needed.  By pushing routes that are more specific than a /0 CIDR range that most VPNs use, we can make routing rules that have a higher priority than the routes for the virtual interface the VPN creates."



As we know, because that means it's a more specific route, so the routing system will always route a more - will always take the most specific route available.  So by doing something, creating a network smaller than the /0, which is the everything, the routing table ends up routing to the intercepting DHCP server rather than to the user's VPN.  They said:  "We can set multiple /1 routes to recreate the 0.0.0.0/0 all traffic rule set by most VPNs.



"Pushing a route," they wrote, "also means that the network traffic will be sent over the same interface as the DHCP server instead of the virtual network interface.  This is intended functionality that is not clearly stated in the RFC.  Therefore, for the routes we push, it is never encrypted by the VPN's virtual interface, but instead transmitted by the network interface that is talking to the DHCP server.  As an attacker, we can select which IP addresses go over the tunnel, and which addresses go over the network interface talking to our DHCP server."  So in other words, they're able to literally select by destination IP.  If they don't want everything they can say, oh, just give us this chunk of your traffic.  You think it's going through your VPN, but it's not.



They said:  "We now have traffic being transmitted outside the VPN's encrypted tunnel.  This technique can also be used against an already established VPN connection once the VPN user's host needs to renew a lease from our DHCP server.  We can artificially create that scenario by setting a short lease time in the DHCP lease, so the user updates their routing table more frequently.  In addition, the VPN control channel is still intact because it already uses the physical interface for its communication."  That is, you know, the control channel meaning the channel to the remote end that is outside of the tunnel.  They said:  "In our testing, the VPN always continued to report as connected, and the kill switch was never engaged to drop our VPN connection."  Meaning there was never a panic that the VPN was concerned that it was being intercepted and so shut things down."



So then, to their credit, they raise the question that we've had all along, by asking "Is TunnelVision a vulnerability?"  And I appreciated their answer.  They wrote:  "This is debatable.  We're calling it a technique because TunnelVision doesn't rely on violating any security properties of the underlying technologies.  From our perspective, TunnelVision is how DHCP, routing tables, and VPNs are intended to work.



"However, it contradicts VPN providers' assurances that are commonly referenced in marketing materials.  In our opinion, TunnelVision becomes a vulnerability when a VPN provider makes assurances that their product secures a customer from an attacker on an untrusted network.  There's a big difference between protecting your data in transit and protecting against all LAN attacks.  VPNs were not designed to mitigate LAN attacks on the physical network, and to promise otherwise is dangerous.



"In our technique, we have not broken the VPN's cryptographically secured protocol, and the VPN is still fully functional.  An attacker is instead forcing a target user to not use their VPN tunnel.  Regardless of whether we classify this as a technique, VPN users are affected when they rely on assurances that a VPN can secure them from attackers on their local network."  And finally...



LEO:  Hmm.  Interesting.  That is one of the primary uses, isn't it, for a coffee shop and their open WiFi networks.



STEVE:  Exactly.



LEO:  Yeah, okay.  But this has been around forever, so.



STEVE:  Yes, exactly.  And they finished:  "As for what systems are affected, the short version is everything except Android."



LEO:  Isn't that funny.



STEVE:  Uh-huh.  Android doesn't support Option 121.  So it's completely excluded from these attacks.



LEO:  Isn't that wild.



STEVE:  They wrote:  "In our testing, we observed that any operating system that implements a DHCP client according to its RFC specification and has support for DHCP Option 121 routes is affected.  This includes Windows, Linux, iOS, and MacOS.  Notably," they wrote, "it does not affect Android as they do not have support for DHCP Option 121."  Which, you know, really is interesting.



LEO:  I wonder why not, yeah.



STEVE:  I do, too, because I did some digging, and there have actually been instances where Android's lack of Option 121 support has caused problems for Android users because it turns out this is not obscure, Leo.  This is the first time we've ever talked about it on the podcast because it's just never come up.  You know, we've covered DHCP in depth in the past.



Okay.  So just to be clear about the scope of the danger presented by the potential abuse of DHCP's Option 121, this is strictly a local LAN-side attack.  But Leo, as you correctly point out, you know, we do operate in essentially LAN networks where we're assuming a VPN is going to trust us where untrusted peers are on the same LAN we are.  So that's a thing.  The attacker needs some means of defeating the network's actual DHCP server.  Since DHCP clients will and do accept the first reply to their query, simply being faster to reply is typically all that's needed.



And, you know, as we know, most routers use the slowest chip that the manufacturer was able to get away with.  Boy, I tell you, those web interfaces on routers, it's like, okay, I click the button, hello, hello.  Should I click it again or just wait?  You know?  So the point is it's not going to be quick to fire off a DHCP reply because it doesn't need to; right?  That's going to be way down the priority queue of traffic that it needs to deal with.  So an attacker probably doesn't have much difficulty being able to respond with DHCP queries faster.  So it's definitely conceivable.  Also, in an enterprise environment, that if you had somebody untrusted on an enterprise network, that would be a problem.  And it also turns out that Option 121 is not the least bit obscure in the enterprise.  Turns out it's under heavy use.



I found two little samples through a quick search.  A posting over on Stack Exchange says:  "I'm running OpenVPN on a CentOS 7 server.  The DHCP server on the LAN uses Option 121 to tell other devices to use this CentOS server if they want to get to the VPN subnets the OpenVPN server is connected to.  This works great.  The problem is that this CentOS server is getting these same routes from the DHCP server, which breaks things."  And then he goes on to talk about how he can manually remove the static routes that the CentOS server is receiving from DHCP.  But my point is here's an example of where Option 121 is being used to inform machines on the LAN where to route the traffic they want to go through the CentOS 7 server's VPN subnets.  So it's very useful for that.



And also, just as recently as last Tuesday, someone posted to the what I have to categorize as the embarrassingly useless Microsoft answers forum.  I don't know if anybody has ever seen any of the crap that is there.  But, you know, if Microsoft really wants to lead in AI, they should remove whatever poor humans they have that are being forced to respond to forum postings there and put ChatGPT 12 or something in there instead.  It is, I mean, it is excruciatingly bad.



Anyway, someone posted, and needless to say they got no useful answer:  "When connected to my office network, its DHCP server," meaning his office's network DHCP server, "will use Option 121 to assign three different networks to be reached using a router which is not the default gateway.  This works absolutely.  The networks appear in my routing table in active routes.  Everything works.  Networks are reachable."  Anyway, so he wrote that, and I just grabbed that as a little snippet of another example of like Option 121 is really out there, and it turns out has really been useful, as I said.



He goes on to explain at some length.  He's complaining that when he boots his PC without any network connectivity, then it has a problem.  Uh, yeah.  That would be a problem.  So anyway, I wanted to point this out, again, that this DHCP option is in heavy use within more complex corporate networks.  What that means is that simply, like, blacklisting Option 121 is not viable.  In my opinion, it would be extremely unlikely for anyone at home to ever have anything to worry about, though it's still instructive to paint a picture.



The way I can see this might occur to somebody at home would be if some malicious device were connected to a residential network and wished to capture all of the user's traffic, whether tunneled through a VPN or not.  By being the first device to respond to any DHCP query, such a malicious device could establish itself as the network's gateway to receive, inspect, and forward all traffic from the network's many machines.  And then, by additionally using Option 121, such a device could use that to also insert entries into the user's routing table to prevent their VPN, if any, from tunneling the user's traffic.  Even though the VPN would show that everything was working and the user's traffic was protected, none of it would be.  The VPN tunnel would be up and established, but it would not be carrying any of the user's traffic.



Since there are many environments where Option 121 is not needed and is never used, like probably most of ours at home, I think it would be nice for our operating systems to provide the option, like, to hard disable it.  But I dug around, and I couldn't find any indication that that's being done.



LEO:  Yeah.



STEVE:  I would imagine the Windows firewall could be configured to just, you know, to look for any incoming DHCP port, what is it, it's been so long, is it 163 is DHCP?



LEO:  No idea, yeah.



STEVE:  I don't remember now the port numbers.



LEO:  So the best mitigation would be to turn off Option 121, but that's not an option.



STEVE:  No.



LEO:  What happens now?  Unless can VPN software be updated to have that as a feature?



STEVE:  The problem is this gets in underneath the VPN software.



LEO:  Yeah, yeah.



STEVE:  The VPN software, I mean, I suppose it could be updated to monitor the routing table and proactively determine whether or not it's been rerouted.  So that's certainly something that could be done.  Right now the VPN, when you bring up the VPN tunnel, it inserts a new default route for everything, and points it at its virtual interface so that it receives everything.  What it would need to do would be to - and I guess it actually could - would be to send itself a test ping.



LEO:  Ah, there you go.



STEVE:  From an IP in the user's IP space and verify that its virtual interface receives that ping.



LEO:  Right.



STEVE:  If it doesn't receive the ping, that tells it something has interfered with the routing between the user's localhost IP and its own interface.  So, yeah, that would be a cool feature for a VPN to add.



LEO:  And meanwhile there's not really a mitigation, is there.



STEVE:  No.  No.  And I think your use case is exactly the right one, Leo, because, you know, where do people deliberately bring up a VPN?  It's when they're in a hotel, in a caf, in any untrusted environment.  And they don't want to be sharing their traffic with everybody else.



LEO:  Yeah, yeah.  I wonder if commonly used hacking tools like WiFi Pineapples and stuff are able to do this.  They probably are.  I mean, it's been around for 30 years.



STEVE:  Yeah, but, well, oh, so you mean whether they're able to perform the hack?



LEO:  Yeah.



STEVE:  I bet that, you know, that intercepting...



LEO:  That seems like something you'd build in.



STEVE:  Well, and intercepting DHCP is such a juicy target.



LEO:  Yeah.



STEVE:  I mean, I'll bet you that hacking tools actively have DHCP server spoofing and are able to get a response out immediately.



LEO:  Interesting.  Wow.  This is good stuff, thank you.  Because this has been everywhere, this story.



STEVE:  Yeah.



LEO:  And I was really curious what you thought of it.



STEVE:  So it's a problem.  It's not, again, what are you going to do with a CVE?



LEO:  Right.



STEVE:  Like, hello.  Like, okay.  I mean, maybe that gets it more attention.



LEO:  It gets the word out; right?



STEVE:  Yeah.  Unfortunately, apparently GPT something is able to read the CVE and immediately design a hack that the script kiddies can then use.  So, great.



LEO:  Would you like to take a break?  Is that what you're looking at me like that for?  I know that look.  We will have more with Mr. Gibson in just a little bit.  You know, every week there's a story or two that in my mind, and I bet your mind, too, you go, I wonder what Steve has to say about that.  That's why we love you, Steve.  And that's why we listen to the show.  We're so glad to carry the show.  On we go with Mr. G.



STEVE:  Okay.  So a bit of feedback.  Dave Brenton tweeted:  "Mr. Gibson.  Quickly may I say, as a machine language coder, I admire your work in that area.  I'm a SpinRite owner/user and longtime fan since near the beginning of Security Now!.  My question is about security keys.  I hope this is not too long a question."  And it wasn't.



He says:  "I'm about to make the transition to YubiKey, and so I intend to purchase two, to have a safe fallback in case of loss.  I'm also planning to convert the wife over to the Passkey world.  My question is can the Passkeys be paired across two user accounts, thereby ensuring recovery in case of loss with only three keys?  My mental model says it made sense, but I do not know for sure.  One, can the same key be applied to two different people?  Two, to assure full backup protection, can all three keys be coded into both users?  It may be a silly notion, but could it work?  Or should I just buy four keys to begin with?  Thank you for all your good work and propeller-head installments.  On to 999 and beyond."



LEO:   Yes.



STEVE:  Dave.  Yes.  And I said at the beginning of the show on Episode 973 we are closing in on 999.



LEO:  We are.



STEVE:  Yet we're no longer fearful of that fatal number.



LEO:  Made me sad.  Hey, before you get to the answer, I just want to, well, actually do the answer.  And then I want to ask you about machine language and assembly.  I had some questions.



STEVE:  Well, cool.



LEO:  Yeah.  Go ahead.



STEVE:  Okay.  So I chose to share Dave's question because it so perfectly demonstrates the near total mess the user authentication world has fallen into today.



LEO:  It really has.  It really has.



STEVE:  It is just a catastrophe.  I'm hopeful this may just be a transition phase.  But truth be told, all of our collective experience also leaves me feeling somewhat skeptical.  I worry that all we have done by having the FIDO Group lower the bar for entry from requiring physical key dongles to allowing pretty much anything else - smartphones and PCs running simple software Passkey clients - is to expand upon the number of available options, with an additional and, difficult as it is to believe in this day and age, not very well-thought-out system.  And we've added this new and not well-thought-out system without removing any of the previous options.



Have traditional username and passwords been replaced?  No.  Are they ever going to be?  Not in this lifetime.  Have the "I forgot my password" links gone away?  No.  Are they ever going to?  No.  What about those time-based one-time passcodes?  Are they going away?  No.  Any plan for that?  No.  What about OAuth, which brings us the "Log on with your Google or Facebook or some other account?"  Have those been obsoleted and removed?  Nope.  Can they be?   Well, not easily, since many sites only know their users thanks to their redirection through another web service's authentication.



And so to this pile of existing half-baked remote network authentication solutions we are now adding Passkeys, a mysterious new solution that its designers all say is amazing and far more secure, which works sort of like magic right up until it doesn't work at all.  And when that happens, what do we do?  Well, we fall back to "Send me an email."



What we've wound up with is the well-known and often observed phenomenon of "solution spread."  We invent a better idea than what we had before.  Perhaps it's because the times have changed and the older solutions are no longer adequate.  Or perhaps we have more technology and available processing power than we had before, so new solutions are available than were previously.  But the problem is, we rarely are able to kill off the things that came before.  Why?  Because by the time we can do something more, too many people have come to depend upon the previous solution, and the one before that, and the one before it.



And this solution-spread doesn't just apply to the authentication domain.  Just look at Windows.  Without getting bogged down into the details, every few years Microsoft comes up with a new and much improved way of writing applications for their Windows OS.  And they promote the hell out of it, explaining how and why it's so much better than everything that came before.  And do they then kill off the previous ways of programming Windows?  No.  Of course not.  They can't.  They were once promoting the hell out of those previous solutions, and they got lots of people onboard using them then.  So even though they no longer love them and are urging everyone to use the new system, that never happens.  I've heard Paul over on Windows Weekly saying that the original Windows API, Win32, should have died off long ago.



LEO:  Oh, yeah.



STEVE:  That's what all of my Windows are written in; you know?  And not just mine; a gazillion others, as well.  And that's "gazillion" with a "G."  I am certain Paul knows that Microsoft will never abandon Win32.  They can't, any more than websites will ever be able to stop offering username and passwords with an "I forgot how" email link.



So just to be clear, the industry has added a bright and shiny additional way for people to log into their accounts.  But none of the existing ways are, or will be, removed.  Remember that today in 2024, only one out of every three Internet users is using any form of password manager.  I really don't know what the rest are doing.  Perhaps these are the people whose iOS and Android support for Passkeys is mostly aimed at.  You know, these people don't know, don't understand, and don't care about their online identity.  So when Apple or Google comes along and asks, "How would you like to log in instantly with Passkeys and never worry about another password?" well, that sounds great.



But that's not Dave, our listener whose questions launched me into first taking a bit of a rant into a wider view of where we stand today.  So let's look at Dave's situation.  Dave says he's planning to convert his wife over to Passkeys.  I'm sure he means that he would like to have his wife begin to use Passkeys, since it's not possible to "convert over" to Passkeys in any meaningful way when so few websites offer the option at all.  The caution there, since we do not yet have Passkey transportability, is to be careful about which app is holding a site's Passkeys.  As I mentioned last week, iOS, Windows, Android, and now an increasing number of traditional password managers will all be vying to be the app that generates the Passkey to be provided to a website.  Since only that app will then be able to authenticate the user to that site with a Passkey, the only sound strategy will be to only and always use a single platform for Passkeys.



This issue, and Dave's other questions, require a quick bit of foundation about the operation of Passkeys.  When an application prompts its user about whether the user wishes to have it create a Passkey, that's exactly what's happening.  The application generates a cryptographically strong secret and private key, which never leaves the application and which the application guards carefully.  From that closely held private key it then generates a public key, and only the public key is sent to and retained by the website.  In the future, that website will use the public key it holds to verify the signature of a challenge that it sends to the user's Passkey authenticator.



So my point here is that, today, there is no provision for these private keys, which were generated internally and have ever since been guarded by the application, to ever leave that application's control.  And a security-conscious organization like Apple can make the defensible claim that since all of the Passkeys' security derives from the "secretness" of these private keys, which is crucial, no other application, including its user, can or should be entrusted with their stewardship, with the stewardship of the Passkey's private key.  Since this represents a powerful platform lock-in, it's not at all clear to me that Apple will ever allow for Passkeys export.



That being the case, I think that a very strong case can be made for only ever storing Passkeys in a third-party Passkeys client, such as a browser extension.  In theory, it ought to be possible for a website to allow its user to replace one Passkey with another.  So if Apple or Android were to inadvertently become the generator and holder of a Passkey, if a website supported Passkey replacement, it should be possible to migrate away from one Passkey application to another.  And I was thinking about this.  If a website doesn't explicitly allow you to migrate  between Passkeys, hopefully it allows you to delete a Passkey, in which case your account would not be associated with one, and then you could reassociate it with a Passkey from the provider that you're wanting to switch over to.  So the real point here is that it is the application that generates the Passkey.  It is never something that we're able to supply from the outside.  



So just to put a bit of frosting on this discussion before we talk about the platforms with hardware authentication dongles, I wanted to share a few points from Google's Chrome FAQ.  This is Google's Chrome browser FAQ about Passkeys.  They start off, of course, with all of the glowing bits.  Under "Manage Passkeys in Chrome," they say:  "You can use a passkey to sign in easily and securely with just a fingerprint, face scan, or screen lock.  Passkeys are a simple and secure way to sign in to both your Google account and all the sites and apps you care about without a password.  You may be asked to sign into a website with a Passkey or create one to improve your account's security."  And then they have a little tip:  "Passkeys are built on industry standards, so you can use them across many platforms."



LEO:  Gotta love those industry standards.



STEVE:  Oh, Leo.  That's the happy news.  That all sounds terrific.  And of course we ask here, what could possibly go wrong?  Well, here's what Google has to say about that.  Under "Store Passkeys in Windows" they said:  "If you have Windows 10 or up, you can use Passkeys.  To store Passkeys, you must set up Windows Hello.  Windows Hello does not currently support synchronization or backup, so Passkeys are only saved to your computer.  If your computer is lost, or the operating system is reinstalled, you cannot recover your passkeys."  Whoops.



Or "Store Passkeys in macOS.  You can save Passkeys in your Chrome profile, where they're protected by a macOS Keychain."  Then under "Important" they said:  "Chrome cannot save or use Passkeys stored in iCloud Keychain.  If your computer is lost or your Chrome profile is deleted, you cannot recover your Passkeys."



And third:  "You can use a security key to store your Passkeys.  Important:  Passkeys stored on security keys are not backed up.  If you lose or reset the security key, you cannot recover your passkeys."  What a wonderful system.  This clearly represents a huge leap forward.



LEO:  Sigh.



STEVE:  Wow.  It's clear that, unfortunately, what we have at the moment is an extremely fragile system.  The problem is the extreme secrecy surrounding the private keys which create the Passkeys.  It's true that they do need to be guarded.  Unfortunately, at the moment they're being jealously guarded.  How Microsoft could possibly imagine that it's practical to have all of a user's Passkeys locked up in a single machine, unable to synchronize with any of a user's other devices is beyond me.



But we're ready to entertain the second part of Dave's question, where he asked:  "Can the passkeys be paired across two user accounts, thereby ensuring recovery in case of loss with only three keys?"  He says:  "My mental model said it made sense, but I do not know for sure.  Can the same key be applied to two different people?  To assure full backup protection, can all three keys be coded into both users?"



The answer is that not one of those operations Dave is asking for is available.  Not one.  And what's more, I just double-checked.  As we learned last week, Yubico's YubiKeys have the most ample storage for Passkeys of any hardware Passkey dongle in the industry, and even it is limited to a total of only 25.  And they are utterly and absolutely non-exportable.  A YubiKey is at its heart an HSM, a hardware security module.  The internal YubiKey dongle hardware contains a very high-entropy random number generator that's used to synthesize a unique private key.



That private key never leaves the device.  There is no way to export it.  Exportation does not exist.  There's no way to put a Passkey in, and no way to take a Passkey out.  This would not be a problem if sites were to allow multiple passkeys to be registered for a single account.  And there's no reason that would not be possible.  But how many sites today support the use and management of multiple passwords for a single account?  I've never seen one.  So it's unclear why support for multiple passkeys would ever be created, even though nothing prevents it.



With YubiKeys having a 25-passkey limit, other than for experimentation, they seem most practical for higher-end enterprise-grade security applications, and perhaps for eventually signing into only a few of the most secure sites where the inconvenience of having an absolute hardware-lock is warranted by its ultimate level of hardware-level security.  And as we noted last week, a YubiKey might be used to unlock a password manager, which is where, we would all have to conclude, all of a user's Passkeys should probably be stored.



The only sane conclusion we can draw is that, while this is all very interesting, none of this is yet ready for prime time.  Poke at it, experiment with it, but wait until Bitwarden's Passkey-supporting mobile clients emerge from their current beta-testing state, at which point it will be practical to start depending upon Passkeys because they will be in a single, sane, multiplatform client.  And Bitwarden, which is we should say a sponsor of the TWiT Network, will likely be offering backup and support and exportation of those once the security protocol for doing that, which is reportedly underway within the FIDO Group, is concluded.



So Bitwarden will then generate and hold our Passkeys, even when other Passkey clients on iOS and Android might be trying to.  And then of course, as we said last week, the challenge is making sure that your chosen Passkey authenticator is universally used, even in an environment where multiple authenticators are all vying for attention.  So I have to say it's the things, reasonable things that people would want to do are not available.  They cannot be done.



LEO:  God.  Wow.



STEVE:  Yeah.  Yeah.



LEO:  Yeah.  I saw there was a Hacker News story about - somebody wrote about why it's a hundred times harder to implement Passkeys on your website than you might imagine.  I mean, it's just - I think this is going to be - I feel like people are going to throw up their hands and say, okay, fine, never mind.  And that's depressing.



STEVE:  Right.  Right.  And as we said last week, if it doesn't achieve critical mass, then it'll just be, well, exactly as one of our listeners said - or no, no, it was the guy who did the Rust WebAuthn client.  He said, "I feel that this will, you know, it'll be like ad blockers.  A small percentage of people take the trouble to do it, but it's sort of a niche, and it never really becomes a problem for ad companies."  And in this case it just never takes hold.



LEO:  So speaking of...



STEVE:  I mean, it is a mess.



LEO:  It is a mess.  And it's not getting any better.  This did not solve it.  We've been trying.  I mean, I remember when Microsoft tried the single sign-on thing 20 years ago.  We've been trying to solve this.



STEVE:  And they had something called Passports.	



LEO:  That's what I was talking about, Passport, exactly.  It was a single sign-on.  And it didn't get adopted, and that's that.  And...



STEVE:  Yup.



LEO:  Oh, well.  Oh, well.



STEVE:  So one last piece of feedback from Willie Scott.



LEO:  Before you do that, can I ask the other question about assembly language?



STEVE:  Yeah.  Yeah, yeah, yeah.  Yeah.



LEO:  I was thinking the other day about how one debugs in a higher level language.  You'll write a print statement, for instance, and it'll tell you all your stuff.  You must have some macros you've written over the years to help you debug assembly.  Or do you?



STEVE:  No.



LEO:  I knew it.  I knew it.  You just write it right the first time.



STEVE:  Well, so for - not for debugging.  But, for example, one of the reasons it would be difficult for me to share my assembler is that I have built up a macro archive of things I do.



LEO:  Oh, I'm sure you have; right.



STEVE:  For example, I use a macro called "zero," and it takes a register name.  Well, it simply expands to XOR register comma register.



LEO:  Right, to zero it out.



STEVE:  Because you know when you XOR something, exactly, when you XOR something with itself you get zeroes, and it's very fast because it doesn't depend upon a memory fetch or the previous data or the previous contents of the register.  So, and the point is, if I wrote XOR something comma something, I would have to look at it and say, okay, XOR, and then look at what am I doing, and then realize, oh, I'm wanting to zero that.  Well, it's much better if I just say zero and then the thing.



LEO:  Right.  Right.



STEVE:  So anyway, and you cannot do that for variables, that is, the Intel architecture will not allow you to XOR memory with another memory.  You can only XOR a register with a register or a register with memory, but not memory with memory.  So when I have a variable, I use the macro "reset," which moves a zero into it.



LEO:  But you don't have any macros for kind of displaying the contents of the stack, purely for debugging?  You don't have anything like that?  You just look at the code and figure out what's going on?



STEVE:  Oh, no, no.  So I definitely have a debugger.



LEO:  Oh, good, okay.



STEVE:  Oh, yeah, yeah, yeah.



LEO:  MASM comes with a debugger; right?  Or no?



STEVE:  So MASM doesn't, but there were back in the day a bunch of third-party debuggers.



LEO:  Right.



STEVE:  I use something called Periscope, which was written by a guy named Brett Salter years ago.



LEO:  I remember that, yeah.



STEVE:  He passed away a few years ago.  There was also something called SoftICE.  And ICE stands for In Circuit Emulator.  And in the really old days you would pull the processor off the motherboard and plug in this paddle that then had a cable running to a bunch of things that emulated the processor that allowed you essentially to get inside the processor.



LEO:  Wow.  That's wild.



STEVE:  So that was called an ICE, an in-circuit emulator.  And so SoftICE was essentially using protected mode to do all the same sorts of things.  So there have absolutely always been debuggers.  And one of the banes of developing for SpinRite was that I'm DOS and 16 bits.  And it was very difficult to create an environment where I was able to have networking in order for my code to get down into the target machine and debugging at the same time.  So one of the things I'm really looking forward to as I move to my own environment is, for example, this RTOS32 that will be the home for SpinRite 7.  It works with Visual Studio transparently.  So I get to just live in a really nice GUI IDE and do all of my debugging.



LEO:  Oh, that's nice.



STEVE:  And what's really cool, Leo, I bought so many motherboards and so many random hard drives through eBay when our testers were reporting that on my Gimcrack 27Z it does such-and-such.



LEO:  Right, right.



STEVE:  And it's like, oh, my god.  So I'd have to go look.  I'd go to eBay, search for a Gimcrack 27E and...



LEO:  And buy one.



STEVE:  There's one, yeah, and I would buy one.  And my amazing wife put up with having motherboards everywhere.



LEO:  All over the dining room table, I'm sure.



STEVE:  So what's very cool about RTOS32 is it allows Internet, trans-Internet debugging.



LEO:  Oh, nice.



STEVE:  So if something is happening on that guy's Gimcrack 27Z, I'll be able to actually have him contact me and debug it on his machine.



LEO:  Wow.  Oh, that's really cool.  Wow.



STEVE:  Yeah.



LEO:  Very neat.  All right.  Okay.  So you have some pretty good tools, it sounds like.



STEVE:  Oh, yeah.  And in fact one of the things that I've learned is invest in your tooling infrastructure before you do anything.  It is so nice...



LEO:  Absolutely.



STEVE:  ...to have a convenient debugging environment.



LEO:  Absolutely.  On we go.  I'm sorry, I didn't mean to interrupt.  I was just curious.  I was debugging the other night, and I was thinking, I wonder how Steve does this.  So now I know.



STEVE:  Yeah.  You absolutely have to have a good debugger.



LEO:  Oh, yeah, absolutely.



STEVE:  That allows you to see the stack and the contents of the registers and what's in memory and what your local variables are.  All of that is made really very nice with Visual Studio.



LEO:  Nice.



STEVE:  Okay.  Willie Scott.  He says, okay, he has some feedback and advice about the operation of the iCloud keychain.  And I bet you he knows what's going on.  Or at least gave us enough of a clue.  He said:  "Hi, Steve.  In regards to your discussion of Passkeys on last week's show, the part about the author's partner losing her iCloud Keychain passwords intrigued me.  After the LastPass hack, I decided to switch to using iCloud Keychain for my passwords because I'm in the Apple ecosystem and wanted to start using Passkeys instead of passwords wherever possible.



"I'm writing to mention that I, too, have had passwords and two-factor authentication codes wiped from my iCloud Keychain..."



LEO:  Oy.



STEVE:  Uh-huh, exactly, "...although my Keychain has never been fully wiped, like the poor partner's Keychain did.  As near as I can tell, I believe I know the culprit of why it may be wiping credentials from iCloud Keychain and wanted to pass this along to anyone who might still be using iCloud Keychain to store their passwords, or who knows somebody who may.



"When I started changing all my passwords and adding accounts into iCloud Keychain, I noticed that an old Amazon password that I don't use anymore was already stored in there, probably from when the Amazon app asked, 'Do you want me to remember your password?'  It was an old password that I don't use anymore, so I deleted it.  However, a couple of days later, I noticed that even though I deleted that password, or so I thought, it had somehow reappeared in my iCloud Keychain.  Not only that, but I also noticed that one or two accounts that I had recently added to the Keychain were missing.  And this process repeated itself a few more times.  So that's when I started investigating.



"While digging through the settings, I went through my Apple ID account settings, and that's when I realized that my old iPhone 6S Plus, which was running an old version of iOS - iOS 14 to be exact - was still signed into my iCloud account and had iCloud Keychain turned on.  I removed that old iPhone from my iCloud account.  And ever since I did that, no passwords have been wiped.  If you're in an Apple ecosystem, it's always a good idea to keep your devices up to date, but it might also be a good idea to do some spring cleaning and remove old Apple devices from your iCloud that you don't use anymore.



"Having said all that, I sadly was agreeing with a lot of the points you were making about Passkeys.  And I think I've decided that I will probably switch over to Bitwarden once Passkeys become officially supported in Bitwarden, using," and he says,  "https://bitwarden.com/twit, of course."



LEO:  Thank you.  That's our special sponsor link, yeah.



STEVE:  Which I think we're about to talk about.



LEO:  Yes, actually.



STEVE:  "Thank you for a great show.  I look forward to it each week.  I'm also a proud SpinRite owner and can't wait to start using 6.1 on my SSDs and a troubled hard drive."



So this mysterious iCloud credential removal has all the feel of something Apple would be deliberately doing out of their typical abundance of caution.  I'll bet there's a security model behind it.  For example, while an older iPhone is also signed into an account's iCloud Keychain, Apple might be deliberately limiting what they're willing to save into that shared Keychain while an older and presumably lower-security device also shares access.  In other words, it's a feature, not a bug.



LEO:  I guess it could be that.  I don't like that kind of unexplained behavior, however.



STEVE:  It sounds like Apple, though, to say, oh, we're not going to let you hurt yourself.  We're going to delete the keys you've just saved because otherwise one of your insecure devices might get them.



LEO:  Ay ay ay.



STEVE:  Yeah.



LEO:  I'll be sure to - you always should remove old devices.  That's maybe why I've never run into this.  I always remove the old devices.  So, hmm.  Very interesting.



STEVE:  Yup.  I do happen to have an iPhone 6 right here.



LEO:  Wow, look at that.



STEVE:  That doesn't work anymore.



LEO:  Look at that home button and think fondly on it because Apple has, as of today, discontinued all the devices that had home buttons.  The last one you could buy was the iPad base model, and that's now been superseded.  So the home button is officially a thing of the past, as is the headphone jack, I think.  I think...



STEVE:  Is it all facial recognition?



LEO:  Yeah.  It's all Face ID now.



STEVE:  Makes sense.  Today's podcast is titled "Not So Fast" because that's the absolutely best way to characterize what's going on in the United Kingdom with Google.  As we know, during our podcast two weeks ago Leo dropped the news that Google's third-party cookie deprecation would not be happening as had been long planned for this summer.  And of course I was getting all excited about that because, you know, I've been on this third-party cookie thing for a long time.  I think it was in 2008 I created that whole cookie forensics facility.  GRC understands which types of assets carry cookies and which ones are first-party and third-party and everything.



I mean, and there were, back then, browsers were not handling cookies correctly.  When you turned them off, sometimes they didn't get turned off.  Or turning them off would keep new ones from being stored, but would not cause old ones to start getting blocked.  And there was just all kinds of screwy things that were going on.  So this has been a hobbyhorse of mine for decades.



So it is the case that the abandonment and deliberate blocking of all third-party cookies and other web-tracking hacks represents such a dramatic sea change for the web that, I get it, many understandably skeptical observers doubt it can or ever will actually come to pass.  And, you know, we've been abused for so long it's difficult to imagine that could ever end.  So, self-confessed technology fanboy that I am, I wanted to determine what was going on.  Were some stuffed-shirt bureaucrats somewhere going to screw this all up?



When I went to take a look at that for last week's podcast I quickly became lost in a paper shuffle.  I decided that whatever was going on was worthy of understanding, since I consider this single forthcoming change, that the largest browser maker in the world by far wants to make, to be one of the most important things that's going on today.  That and the question about, you know, are we going to keep our conversations encrypted in messaging apps, which the EU seems determined to say no to.  As I've previously said, this represents a complete - what Google is doing represents a complete reconceptualization of the way the Internet will finance itself going forward.  And we could have it soon.



So the news that Leo had picked up on came in the form of an announcement that left actually more questions than it answered.  On the 23rd of last month - which was, you know, Tuesday before last - on their PrivacySandbox.com site, Google posted under the headline "Update on the plan for phase-out of third-party cookies on Chrome."  That's very clear.



Their brief introduction said:  "The UK's Competition and Markets Authority" - known as the CMA, and we'll be using that acronym a lot here, or abbreviation - "and Google publish quarterly reports to update the ecosystem on the latest status of Privacy Sandbox for the Web.  As part of Google's first-quarter 2024 report, we will include the following update" - that is, in the report - "about the timeline for phasing out third-party cookies in Chrome in the April 26th report."



Okay.  So the update, very short, it simply reads:  "We are providing an update on the plan for third-party cookie deprecation on Chrome."  They said:  "We recognize that there are ongoing challenges related to reconciling divergent feedback from the industry, regulators, and developers, and will continue to engage closely with the entire ecosystem.  It's also critical that the CMA has sufficient time to review all the evidence, including results from industry tests which the CMA has asked market participants to provide by the end of June."  Okay, now, that means essentially June is when third-party cookies were supposed to be ending; but, you know, things are taking longer than expected.



"Given both of these significant considerations, we will not complete third-party cookie deprecation during the second half of Q4.  We remain committed to engaging closely with the CMA and ICO and we hope to conclude that process this year.  Assuming we can reach an agreement, we envision proceeding with third-party cookie deprecation starting early next year."  So early 2025.  And then they conclude by noting:  "Once published, you will be able to view both Google and the CMA's full reports."  Those reports were published three days later, on April 26th.  So this was on the 23rd they said this.  Surprised the industry.  Three days later on the 26th we got the whole story.



So the entire issue is best described by the following statement:  "On 7 January 2021" - okay, so a little over three years ago.  "On January 7th, 2021, the CMA commenced an investigation under Section 25 of the Act" - some, you know, UK, the equivalent of legislation to prevent monopoly misbehavior, you know, antitrust we have here in the U.S. - "in relation to Google's Privacy Sandbox proposals.  The CMA subsequently informed Google that the CMA was concerned that Google's proposals, if implemented without regulatory scrutiny and oversight, would be likely to amount to an abuse of a dominant position."



So basically a little over three years ago Google says we're going to change the way the Internet is financed.  And among those things we're going to kill off third-party cookies.  There's no question that people in the UK whose income and livelihoods depend upon tracking, like, you know, their data resellers, they said, whoa, whoa, whoa.  We don't want third-party cookies to go away.  We like third-party cookies.  So UK bureaucrats, please tell Google no.  Please tell Google we need those cookies.



Okay.  So I don't know that for a fact.  It's unclear.  And it's frankly not really important to know the genesis of the inquiry, but it's probably something like that.  Since we're talking about the elimination of all third-party cookies and the curtailment of what had become the widespread practice of tracking Internet users around the web as a means of determining their interests, it may well have been the advertising technology companies based in the UK which were crying foul behind the scenes.



LEO:  That's even more exciting, really, yeah.



STEVE:  Yes, yes.  What ensued was about what you'd expect from any healthy and well-established bureaucracy as old and wizened as the United Kingdom.  Experts were - yeah.  Experts, you know, I mean, even the name United Kingdom sort of suggests, oh, crap.



LEO:  [Expostulating]



STEVE:  Experts were found, neutral third-party "monitors" were enlisted, and Google created a document describing the - and, boy, are you going to hear this word - "the Commitments it was prepared to make," with a capital C.  I mean, it sounds religious almost.  These are our commitments.  A document titled "Investigation into Google's 'Privacy Sandbox' browser changes" opens with the assertion that:  "The CMA has accepted commitments offered by Google that address the CMA's competition concerns resulting from investigating Google's proposals to remove third-party cookies and other functionalities from its Chrome browser."  Which begs the question, what exactly are these commitments that the CMA has accepted?



I found the points of concern in the description of the roles of the appointed technical expert that will be supporting the monitoring agent.  The document states:  "On the 26th of September 2022, the CMA approved the appointment of S-RM Intelligence and Risk Consulting Limited by the Monitoring Trustee (ING Bank N.V.) as an independent Technical Expert to support the Monitoring Trustee in monitoring compliance with the following provisions of the binding commitments accepted by the CMA on February 11th, 2022."  Whew.  Okay.  And then the good news is this next line is short.



"Google's use of data (paragraphs 25 through 27), non-discrimination (paragraphs 30 and 31), and, with respect to those provisions, anti-circumvention (paragraph 33), the role of the Technical Expert is to provide specialized knowledge to support the Monitoring Trustee, particularly in relation to monitoring data flows and understanding the possible impacts of the Privacy Sandbox changes on ad tech markets."



Okay.  So we have the ING Bank serving as the neutral monitor, and this monitor has appointed another firm with the required technical expertise.  And everything it focused upon is in a small handful of paragraphs somewhere.  I found out where.  They are in Appendix 1A of the latest version of the "Google's final commitments" document.  The first set of paragraphs, 25 through 27, basically amount to Google promising not to use any personal data from a user's past Chrome browsing history, a customer's Google Analytics account, or to in any way track users.  So that's all pretty much what Google has explained to be its intentions and goals.  So it appears that the CMA just wanted that very clearly and succinctly spelled out.



The non-discrimination, that's paragraphs 30 and 31, state that Google promises to create a totally level playing field.  Having examined, explored, and shared on this podcast the operation of Google's cookie-replacement technologies as they have evolved through the years, this was, you know, it was always clear to me and those who understood this that this was inherently level, the playing field was.  That is, you know, Google was getting a very proscribed amount of information, and everybody was equally - had equal access to it.  It's implicit throughout Google's design, though I have to agree that Google's design has grown to be much better thanks to all the feedback and criticism the various pieces have received through the years.  So yes, it's a good thing we did not get stuck with Google's first idea.  What we've got is something far better than what we would have had if, you know, if there was sufficient scrutiny done.  And there was.



So I can understand how bureaucrats, who will never understand how Google's Topics API functions, need a simple "okay, but what does it mean" spelled out in English.  Since this is crucial to the acceptance of Google's technology, and it's only two paragraphs, I'm going to share them.



Paragraph 30 says:  "Google will design, develop, and implement the Privacy Sandbox proposals in a manner that is consistent with the Purpose of the Commitments and take account of the Development and Implementation Criteria.  Google will ensure that it does not distort competition by discriminating against rivals in favor of Google's advertising products and services.  In particular, Google will not" - and we have three things - "design and develop the Privacy Sandbox proposals in ways that will distort competition by self-preferencing Google's advertising products and services; also will not implement the Privacy Sandbox in ways that will distort competition by self-preferencing Google's advertising products and services; and, finally, also will not use competitively sensitive information provided by an ad tech provider or publisher to Chrome for a purpose other than that for which it was provided."



Then it says:  "For the avoidance of doubt, Privacy Sandbox proposals that deprecate Chrome functionality will remove such functionality for Google's own advertising products and services, as well as for those of other market participants."  That was paragraph 30.  And yes, I mean, that's exactly what Google has said they're going to do.  But essentially what has happened is a legally binding contract has been created that Google - that's what these commitments are which Google is saying they're going to honor.



And paragraph 31 just says:  "Google will not change its policies for customers of Google Ad Manager, Campaign Manager 360, Display & Video 360, or Search Ads 360 to introduce new provisions restricting a customer's use of Non-Google Technologies before the Removal of Third-Party Cookies, unless in exceptional circumstances - such circumstances to be discussed with the CMA - or as required by law.  For the duration of the Commitments, Google will inform the CMA ahead of any such change to these policies."



And this leaves us with the final "anti-circumvention" paragraph 33 which is just a blessedly single line which reads:  "Alphabet Inc., Google UK Limited, and Google LLC will not in any way, whether by acts or omissions, directly or indirectly, circumvent any of the Commitments."  Now, that sort of language will be familiar to any businessman or anyone who's been involved in any contractual agreements where attorneys are engaged.  You know, it's boilerplate; right?  And it's important to understand that both the United Kingdom government and Google's various corporations recognize those provisions to be now contractually and legally binding.



So it has been upon those representations, which are enumerated as "Commitments" with a capital "C," that the UK then proceeded to carefully examine Google's proposal.  So now we return to the timeline for phasing out third-party cookies.  That work appears in a document titled "CMA Q1 2024 update report on implementation of the Privacy Sandbox commitment," dated last month, April of 2024.  Actually it was April 26th.



The document's summary lays out the entire story, and it's interesting enough and short enough to share.  They said:  "This report sets out the CMA's updated views on the issues we identified in our January 2024 report."  So January was the previous report.  So it's basically quarterly; right?  So this is the result of the first quarter.  So this is from January 2024.  Where are we now?  We're in April.  So we've had the first quarter go by.  "Our analysis is based on the framework for assessment set out in the legally binding Commitments that Google made in February 2022 to address competition concerns relating to its proposals to remove third-party cookies from Chrome."  So in other words, yeah, this is a big deal for the entire Internet.  It's a big deal.  "The January 2024 report set out our provisional views on the impact of the Privacy Sandbox on competition, publishers, and advertisers, and user experience.



"We outline Google's response to the concerns we identified in that report, the January report, and the steps it is taking to resolve pending issues.  We've also considered the feedback received from market participants on these points.  We've included a summary of this feedback in the sections below.



"This report also incorporates the preliminary assessment of the ICO, the Information Commissioner's Office, on the privacy and data protection impacts of the Privacy Sandbox.  Having consulted with the ICO, we set out our current views on these concerns for each of the APIs.



"Although there are a number of concerns to work through, based on the available evidence, we consider that from 1st of January 2024 through the 31st of March 2024, the relevant reporting period, Google has complied with the Commitments.  This means that, in our view, Google has followed the required process set out in the Commitments and is engaging with us and the ICO to resolve our remaining concerns ahead of third-party cookie deprecation.



However, further progress is needed by Google to resolve our competition concerns ahead of deprecation.  We will continue to work with Google to resolve our concerns between now and the point at which Google triggers the Standstill Period.  We will provide an update on progress in our next update report.  Testing of the Privacy Sandbox tools is also currently underway.  The test results will form part of a wider evidence base that we will use to assess the effectiveness of the Privacy Sandbox.  The test period runs until the end of June this year."  And as I said before, because this is running through June, that's what kept the cookies from being, you know, for the beginning of the deprecation to start at the end of June.  



They said:  "Given the time needed to resolve outstanding issues and take account of testing results, we have agreed with Google that there should be a limited delay to third-party cookie deprecation.  Subject to resolving our remaining competition concerns, Google is now aiming to proceed with third-party cookie deprecation starting in early 2025.  Under the Commitments, it is for Google to decide when the Standstill Period is triggered.  We encourage market participants taking part in testing to submit their results directly to us by the end of June deadline.  We also welcome any additional feedback from stakeholders on the concerns identified in this report.  Our contact details are included at the end of the report."



Okay.  So one last thing.  This made reference to a "Standstill Period" several times, so I tracked that down.  In the earlier Commitments documents it appears to be just more bureaucracy for its own sake.  It says, on paragraph 19:  "Google will not implement the Removal of Third-Party Cookies before the expiration of a standstill period of no less than 60 days after Google notifies the CMA of its intention to implement their Removal.  Google may increase the length of such a standstill period at any time between giving such notice and the period's expiration.  At the CMA's request, Google will increase the length of this standstill period by a further 60 days to a total of 120 days."



Okay.  So what follows all of that is - that was the document summary.  There are 97 pages of interesting, but ultimately mind-numbing, back and forth detail, as every conceivable facet of this big change Chrome will be implementing is examined under a bureaucratic microscope.  The real concern is over Google's size and whether the changes it is making will disadvantage smaller ad tech players.  But what becomes clear after reading at least some, and that's what I did, I could not go through 97 pages of this, my eyes started to cross and I couldn't see, it is very clear that the UK is moving clearly in Google's direction.



Both parties are truly negotiating in good faith.  That's one thing that also is very clear.  This is not the UK stonewalling and being unreasonable.  It really is, as Leo portrayed, a bureaucratic walrus that just absolutely does not have any idea what is going on.  People are nipping at it, saying this is bad, you can't let Google do this.  So Google is saying this is not bad, this has to happen, we want to stop tracking on the Internet.  People who make their living from tracking are saying, yeah, but we like tracking.



LEO:  Yeah.



STEVE:  Yes.  And so the UK is sort of stuck in the middle. Google is being reasonable.  They are, I mean, there must be a division of Google where they're intoxicated in hot tubs somewhere, just in order to maintain their sanity.  There's no way that the developers are dealing with any of this nonsense because, I mean, ultimately that's what it is.  But the UK needs to be placated through having this explained, you know, what exactly this is and does.  So that's what's happening.  Again, progress is being made.  In the January Report, for example, there was an instance where the ad tech companies were trying to claim that because of their shorter reach, they were being disadvantaged.  The expert looked at it under the watchful eye of the monitor, and now in the April Report the conclusion is, no, that is not the case.  There is no disadvantageous handling based on size of advertiser.  We see no evidence of that.  We understand the technology.  That's not the case.



So it does not appear to me that Google's Privacy Sandbox technology is in any trouble at all.  The truth is, as I've said, it represents a massive change to the way the Internet pays for itself and is going to fund itself in the future.  And it's also true that many companies whose revenue has been entirely derived from the oh-so-slimy practice of tracking users and aggregating their data without our knowledge or permission, for the purpose of selling that data to anybody with a wallet, will be - their income will be impacted.  And not in a good way.



So having read through the documents, I can understand that the process is taking place, and it's taking time.  And in retrospect, you know, though I would have never expected this would happen, it is at least understandable, and it appears that the world will indeed soon be receiving this dramatic change in the way Internet-based advertising is carried out.  It is, you know, clearly far superior to the status quo.  I mean, we can't keep going on the way we have been.  And it takes something no less large than Google to just simply make it an ultimatum.  We are going to do this.  So I understand they've got to satisfy the walruses of the world.  It looks like that process is close to being done.



LEO:  Yeah.  I hope so.  Of course advertisers don't like it.  That's why we like it.  And I think Google, obviously they're trying to balance the interests of both parties because they are - they sell ads.  They buy ads.  It's their business.  It's their revenue.  But they also understand that consumers are not happy, and I think they need to...



STEVE:  No.



LEO:  ...find a way everybody's happy.



STEVE:  And Leo, I'm impressed by the minimization of the information that Google themselves are willing to obtain.  I mean, it's, as we've seen, Topics is not invasive.  They are, you know, no one can be identified from their Topics.  They are chosen at random.  I mean, the system has incredible checks and balances built in which we've talked about on the podcast when we explained it.  And I think we're probably due for a re-explanation when it actually goes into effect because, you know, it's the way the world's going to work.  And I loved the comment about the reason my machine's fans were spinning up was that my Chrome browser, when I was running Chrome, was busy holding auctions with all of the world's ad agencies.



LEO:  Yeah, well, that's coming, maybe, anyway.  We'll see.



STEVE:  It's the only way to do this is to make it user side.



LEO:  Yeah.



STEVE:  You move it to the user, and then the user's browser chooses what they're going to see.  It's brilliant.



LEO:  Yeah.  Makes sense.



STEVE:  I'm going to tease next week's topic, I believe.  I think next week's topic will be ZTDNS, which stands for Zero Trust DNS.  Last Thursday Microsoft published a preview of a forthcoming security solution they call "Zero Trust DNS."  It's been clear for a long time that DNS represents, as we know, both an Achilles heel of network security and a point where it's also very possible, if you're clever, to introduce a significant new level of security.  From my brief scan of the technology Microsoft has outlined, it appears that any of our listeners who may have followed up on my discovery a few months back of ADAMnetworks' DNS solution, which they call "Don't Talk to Strangers," may already be enjoying the benefits of dramatically improved security, thanks to leveraging the power of DNS.  But I needed more time to dig into what Microsoft is doing.  So for next week's podcast I plan to take a deep look into what Microsoft has announced.



Now, one thing I should say that immediately stood out was that Microsoft might be attempting to use this as a way of driving enterprises to Windows 11, since enterprises don't want Windows 11, as we've heard Paul Thurrott mention many times.  No one really does.  And in Microsoft's diagrams which I briefly scanned, they're explicitly labeling the clients as Windows 11 machines.



Now, that just might be Microsoft, you know, because Windows 11 is what they're all using.  You know, since no one actually wants Windows 11, since Windows 10 still commands more than twice the number of desktops as Windows 11, and a much greater percentage within the enterprise because most, you know, new computers come with Windows 11, but enterprise machines that have been running for 10 years don't.  And since a huge install base of machines won't even run Windows 11, if what Microsoft is planning to do is truly a Windows 11-only solution, then the client agnostic system that the ADAMnetworks guys already have working and well-proven seems like a far more practical one to me.



But in any event, by the end of next week's podcast, we'll know exactly what's going on.  And, you know, it's a good thing that Microsoft is stepping up and looking to improve DNS security because we all know it needs it.  But seems to me there's already a solution in place.  But not from Microsoft.  And so when the biggy does it, you know, I remember, Leo, it was fantastic.  Brad Silverberg and Brad Chase came down from Redmond and took me out to lunch and said, Steve, we're going to be announcing DOS 6 pretty soon, you know.  And I said, uh-huh.  And they said, "We're a little self-conscious about this, but we're adding something called Scandisk."



LEO:  Oh.  It's nice of them to warn you.



STEVE:  "Now, don't worry."



LEO:  It won't work well.



STEVE:  "It's doesn't do what SpinRite does."  And I said, "Uh-huh, great.  That's just effin' wonderful."



LEO:  Was that later than Chkdsk?  Because they had Chkdsk.



STEVE:  Yes.  For the rest of our existence we are answering the question, "Well, I already have Scandisk.  What do I need SpinRite for?"



LEO:  Oh, right.



STEVE:  Anyway, the point is, it matters when the giant offers the...



LEO:  It does, yeah.



STEVE:  Oh, it does.



LEO:  Yeah.



STEVE:  I've been there firsthand.  I liked Silverberg a lot.  I never was fond of Brad Chase.



LEO:  You weren't Sherlocked, though, as the Mac people call it.  So that's the good news.



STEVE:  No, Norton did that.



LEO:  Tried.  Do you think he sold more copies of Disk Doctor than you sold of SpinRite?  Probably, huh.



STEVE:  Well, what he did was, when I refused to sell SpinRite to Peter, he sent a developer home with a copy of it and said...



LEO:  Oh, that's not nice.



STEVE:  Oh, yeah.  And we know that because one of my guys looked inside and saw code that was our code.  I mean, there was a place where I needed to see whether the BIOS handled a certain API call.  So I put some specific random data in the registers when I made the call to see whether they got changed.



LEO:  Oh, that's kind of a smoking gun.



STEVE:  Their clone of SpinRite...



LEO:  So the same data.



STEVE:  ...used the same values, the same data, because they didn't know what I was doing.



LEO:  They didn't know, exactly.  They said, well, we'd better do it this way because we don't know if it does something.



STEVE:  Yeah.  The good news is, since they didn't actually create - what was it called?  Calibrate was their clone.  Since they didn't create it, when their customers called for support, they said, "Well, we're not sure.  Call Gibson Research."  I'm not kidding.  We got calls for our support people, "Well, Norton said to ask you about Calibrate."  And we said, "Well, when you buy a copy of SpinRite, we'd be happy to answer all your questions."



LEO:  We'll tell you.  Steve is at GRC and still selling SpinRite, now v6.1, many moons later.  And it's even better than ever.  In fact, now it speeds up SSDs.



STEVE:  Yup.  It's doing really well, actually.



LEO:  Yeah, yeah.  Really, congratulations.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#974

DATE:		May 14, 2024

TITLE:		Microsoft's Head in the Clouds

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-974.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What fascinating insights do we obtain from examining 3.4 million four-digit PINs?  What plans are already underway as a backup for today's vulnerable GPS technology?  How many Passkeys will websites store per account?  And what's all this about Microsoft promising to get serious about their cloud-based services security?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  He's ready.  He's champing at the bit, excited to get the show on the road.  He's going to talk about what we learned by examining 3.4 million four-digit PINs.  You guys have some bad habits.  He'll also talk about an interesting old-school approach to solving the GPS fuzzing problem.  And then Microsoft and how they're getting serious about cloud-based services security.  Or are they?  It's a Big Yellow Taxi moment coming up next.  He'll explain on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 974, recorded Tuesday, May 14th, 2024:  Microsoft's Head in the Clouds.



It's time for Security Now!, the show where we cover the latest news from the security-verse, as it were.



STEVE GIBSON:  Second verse, same as the first.



LEO:  There you go.  That's Steve Gibson, the guru around here when it comes to security, privacy, and technology in general.  Hi, Steve.



STEVE:  Yo, Leo.  It's great to be with you again for this May 14th episode of Security Now! as we continue to approach 999.



LEO:  Wow.



STEVE:  I actually had intended to make time to take some of these whiskers off, but I ran out of time, so our audience will have to - those unfortunate enough to be watching the video can just bear with us.



LEO:  You're with it.  That's hip now to have a little bit of a beard and scruff.



STEVE:  Oh, hey.  See, if you just wait long enough...



LEO:  It all comes around.  Are you a three-blade, four-blade, or a five-blade guy?



STEVE:  Oh, god.  There was the greatest piece that SNL did back in the day.  Remember that?



LEO:  That's why I asked.



STEVE:  And the tagline was "Because they'll believe anything."



LEO:  It was their response, I think, to two blades.  But anyway, it was so old that they thought five blades was funny.  And then in fact that's exactly what we've got now.



STEVE:  I think the more the merrier as far as the blades go.



LEO:  All the blades.  All the blades.



STEVE:  That's right.  So, okay.  This is another of those episodes where there are such rich topics to discuss that we've going to do a few of them rather than a gazillion little tiddy bitty topics.



LEO:  Careful there. 



STEVE:  So we're going to look at what fascinating insights we have obtained from the examination of 3.4 million four-digit PINs.



LEO:  Oh, I love this.  I know what you're talking about.  I saw this.



STEVE:  I posted this picture, a heat map that we'll be describing in detail here in a minute, on Twitter yesterday.  Nothing I have ever posted before has generated so many little heart things, "likes" I guess we call them when we're a hipster like I am with my unshaven face.  So that's going to be a lot of fun.



We're also going to look at an interesting surprise which is the plan that is already underway as a backup for today's vulnerable GPS technology which we talked about, we opened the show with last week, was like looking at what Russia is doing over in the Baltics and the vulnerability that we may not be taking seriously enough.  It turns out we've got an answer for that.



Also there was a lot of feedback from our listeners who are avid Passkeys users about their experiences.  I want to share some of those and essentially correct the record about one aspect that was wrong from last week.



And then we're going to take, as the title of today's podcast suggests, which is "Microsoft's Head in the Clouds," a look at a topic that everybody else in the industry has already covered, but we haven't yet here.  And we're going to do it in our own way, as we always do, which is Microsoft's promise to get serious about their cloud-based services security.  What happened?  What has been found?  And we have an interesting take, as we always do.  So I think our listeners are going to have a great podcast.  You know, surprise. 



LEO:  Surprise, surprise.  You're going to have a great podcast.  Well, we're very excited about that.  Meanwhile, let me talk about one of our great sponsors as we get ready.  Picture of the Week coming up, as well.  All right.  I'm ready for picture time.



STEVE:  So, yeah, this is just a quick simple cartoon.  We've got two people sitting behind their laptops.  One is sort of curious about what the other one is doing.  And she looks over at his screen and says, "What are you doing on the dark web?"  And his reply is, "I forgot my password, so I'm looking it up."



LEO:  Of course.  The NSA knows.  And it's not been the hackers.  Right.



STEVE:  That's where you'll find your password, on the dark web.



LEO:  You bet, you bet, you bet.  That's awesome.



STEVE:  Okay.  So this, as I said, this is just a great chart.  This is from the Information Is Beautiful project, which, you know, demonstrates that, if you graph things in creative ways, you can learn a lot.  And this is a perfect example of that.  3.4 million four-digit PINs which were obtained from multiple data breaches were aggregated.  Now, you know, this is a wonderfully enlightening graphic chart that I want to share.  Unfortunately, the terms "graphic chart" and "listeners" are at odds.



LEO:  You're going to have to describe it, Steve.



STEVE:  Yeah.  I'm going to note that this delightful chart is at the top of this week's show notes.  I tweeted it.



LEO:  Yeah.  You need to see it.



STEVE:  And I gave it a permanent GRC shortcut of pin, P-I-N.  So anybody can see it at any time from grc.sc/pin, P-I-N.  Okay.  But, you know, I can do this verbally also.  Okay.  So this chart, as I said, takes 3.4 million four-digit PINs which were recovered from, and disclosed by, multiple data breaches.



Now, of course a four-digit PIN can have any value between 0000 and 9999.  So there are 10,000 possible PINS.  And this wonderful chart contains 10,000 little itty-bitty squares arranged in a flat two-dimensional map.  So it's got rows, you know, it's got 100 rows and 100 columns.  And of course 100 times 100 is 10,000.  So one way to think of this is that the first two digits of the PIN - which, you know, 00 through 99, specify one axis, and the last pair of digits specify the other.  So every single possible four-digit PIN has its own square on this chart.  And within this 3.4 million PIN dataset, the relative number of times every single possible PIN appears in the dataset determines the brightness of its square on the chart.



Okay.  So what do we learn from this?  Okay.  Possibly the most prominent feature is a bright diagonal line running from the lower left corner of the chart, where both of the first two and the last two digits are 00, to the chart's upper right corner, where the first two and the last two digits are both 99.  The diagonal line, then, is formed by all of the intermediate squares where their first two and last two digits are identical.  And naturally like 00 in the far lower left, that's bright because a lot of people just chose 0000 as their PIN.  And similarly, the very far upper right corner, also very bright because 9999 is many people's PIN.  So there is some variation in the brightness along the diagonal which is interesting.  You know, and of course human nature being what it is, the PIN 6969 appears to be overrepresented relative to its neighbors.  No surprise.



Two other solitary bright spots would also not surprise anyone.  They are the locations of the 1234 and 4321 PINs.  Not very creative, and thus bright on the map.  Another really interesting prominent line is the 20th line up from the bottom.  Since lines are numbered from 0, the 20th line is the line for all PINs beginning with 19.  And what's so interesting is that the line gets gradually brighter as it moves to the right, then dims a bit toward the end and wraps around a bit to the 20 line on the left. So what's going on here?  Well, if you guessed people's birth year, you would be correct.  PINs often begin, it turns out, with 19 and they appear to be brightest somewhere around 1980 seems to be the place where it's most, you know, most people have their PINs clustered there.



LEO:  A lot of 40 year olds.



STEVE:  Exactly.



LEO:  I would have thought it would be the baby boomers that would be the brightest, but maybe not.



STEVE:  Yeah, it's kind of fading out for us, Leo.  And on the other hand, then so are we.



LEO:  Yes.



STEVE:  Yeah.  Another notable feature is a generally brighter region down at the lower left of the chart.  This would be where both the first two and the last two digits form low numbers.  Okay.  Why?  Because people used their month and day of birth within the month, running from 1 to 12 of course for the month, and then day of month 1 through 31.  And what's interesting, there's a brighter horizontal stopping at 12 than the vertical stopping at 12, both which however are clear.  This indicates that most people chose the ordering with the month first and the day of month second as their PIN.



Now, stepping way back from it and looking at the overall illumination, there's a top-to-bottom brightness variation, with it being brighter at the top and dimmer toward the bottom, suggesting that most PINs have low starting numbers.  But there's less left-to-right variation.  So people are generally choosing four-digit PINs with, as I said, smaller first two digits, but for some reason more randomly distributed last two digits.



And the final really interesting observation is that whereas most of the chart shows varying shades of illumination, there are around 40 distinct cells that are black or nearly black.  Like, I mean, dramatic contrast against their neighbors.  In other words, out of all 10,000 possible four-digit PINs, there are around 40 of those that are significantly underrepresented.



LEO:  Isn't that weird.



STEVE:  Isn't that?  It's so odd.



LEO:  Yeah.



STEVE:  For some reason...



LEO:  It looks kind of randomly distributed, but maybe not.



STEVE:  Yeah, well, most of them have high...



LEO:  They're mostly above 60, yeah.



STEVE:  Yeah.  Almost all of them are in the upper third of the chart, so their first two digits are larger.  For some reason, for example, very few people have chosen 6806.  So if you're looking for a lesser chosen four-digit PIN, there you go.



LEO:  They're all in there.



STEVE:  That's right.



LEO:  Or 68 whatever this one is.  You know, it's interesting.  There are three dots on the 68 line.



STEVE:  Yeah.  And in fact that first one on the 68 line was the 6806 that I just chose to highlight.  But you're right.  And looks like there's also three on the 60 or on the 70 line.



LEO:  Yeah.  Huh.  I mean, how odd is that?



STEVE:  It's really non-random in that area.  Okay.  So, and as for the extremely low entropy skewing observed in the dataset, again, low entropy skewing, get this.  Just the top 20, the top 20 most used PINs out of, remember, 10,000 that are possible, right, just the top 20 account for 27% of all PINs observed in use.



LEO:  Oh.  Oh, that's terrible.



STEVE:  Those top 20 are 1234, 0000, 7777, 2000, 2222, 9999, 5555, 1122, 8888, 2001 1111, 1212, 1004, 4444, 6969, 3333, 6666 1313, 4321, 1010.



LEO:  If any of those sound like your PINs, you're in trouble.



STEVE:  Yeah, just very, very...



LEO:  It means you can guess, you know, 10 or 20, and have a one in four chance of being right.



STEVE:  Right.  If, for example, something prevented you from brute-forcing all 10,000, you would absolutely want to go for those 20 as your first 20 guesses.



LEO:  It also means you should use more than four digits in your PIN; right?



STEVE:  Yeah.  So I think we're still at four-digit PINs purely for historical reasons.  It's just, you know, it's because that's, you know, once upon a time we didn't have computers, and people had to actually remember them.  And I'm sure all the people used, you know, their month and day of birth, or the last four digits of their Social Security number, or digits from their license plate or, you know, something.  The point being four digits was all they could actually remember.  We didn't have technology to say, oh, yeah, I know, here's a string of 20 digits, you know, repeat after me.



LEO:  Pick something, you know, what I always do is I pick the last four digits of a phone number, not my current phone number, but maybe my childhood phone number or phone number I particular remember because those are mostly pretty random.  They certainly don't have anything to do with my birth date.  I don't know.  Or just pick something random.  You can remember four digits.  Or better yet, use an alphanumeric password, not a PIN.



STEVE:  Yeah.  Well, and of course back once upon a time - or, no.  I was going to say "once upon a time" we were keying them into our touchtone phones in order to authenticate ourselves.



LEO:  Oh, yeah, right.



STEVE:  But even then, unless you used Q - I think was Q missing?



LEO:  Q was missing; that's right.



STEVE:  There are a couple things that were not there on our models.



LEO:  Well, you know where these are mostly still used is on ATM machines.  I don't know of any ATM machine that uses more than four digits.



STEVE:  Yeah.



LEO:  Right? 



STEVE:  Again, because there's some backend, some old creaky backend machine that can only take four digits.  Anyway, this was a huge win for our audience, who got a big kick out of it.  So again, if you want to see what we were talking about, grc.sc/pin, and that will bounce you over to my site.  I grabbed the - I actually could have just pointed to it.  The original source was over on Reddit, and that got tweeted to me.  But I was afraid that that might not last, you know, it could disappear.  So I grabbed it and stuck it on GRC's server just because it's just such a cool infographic.



Okay.  We started off last week with the piece in Wired about the growing threat to GPS.  While the mischief Russia has been getting up to in the Baltic region is quite localized, we also noted that space is, sadly, not necessarily a benign environment anymore.  A piece of our listener feedback which was generated by this discussion last week led me to look at what's being done about this.



Shaun Merrigan wrote.  He said:  "Steve, regarding SN-973 and GPS vulnerability, the U.S. is testing an updated version of the LORAN system which was shut down in the 1980s, called eLORAN.  I've been monitoring the eLORAN test signals on 100kHz since August of 2023.  My ancient LORAN receivers woke up and started giving me timing signals output again at that time, and have been receiving continuously ever since."



Okay.  So this note from Shaun got me to poke around a bit, and I quickly learned that, indeed, there is an acute recognition of the inherent vulnerability of any satellite-based navigation system.  LORAN is an abbreviation for Long Range Navigation, and the "e" in eLORAN stands for "enhanced."  The original LORAN dates back from World War II.  It's a ground-based navigation system that operates entirely differently from GPS.  And of course entirely differently is what you want in something that's going to withstand an attack on GPS.  You want something very orthogonal to the thing that you're trying to create a second solution for.



I found an interesting summary on the site GPS World.  The article's title was "eLORAN:  Part of the solution to GNSS vulnerability."  Under the heading "Opposite and Complementary," the article leads with:  "Though marvelous, GNSS are also highly vulnerable.  eLORAN, which has no common failure modes with GNSS, could provide continuity of essential timing and navigation services in a crisis."



So here's what they explain.  They said:  "GPS fits Arthur C. Clarke's famous third law:  'Any sufficiently advanced technology is indistinguishable from magic.'  Yet, it also has several well-known vulnerabilities including unintentional and intentional RF interference, the latter known as jamming; spoofing; solar flares; the accidental destruction of satellites by space debris; and their intentional destruction in an act of war; system anomalies and failures; and problems with satellite launches and the ground segment.



"Over the past two decades, many reports have been written on these vulnerabilities, and calls have been made to fund and develop complementary positioning, navigation, and timing, which are collectively referred to as PNT - Positioning, Navigation, and Timing - PNT systems.  In recent years, as vast sectors of our economy and many of our daily activities have become dependent on GNSS, these calls have intensified.  A key component of any continent-wide complementary PNT would be a low frequency, very high power, ground-based system because it does not have any common failure modes with GNSS" - collectively meaning satellite based - "which are high frequency, very low power, and space-based.  Such a system already exists, in principle.  It is LORAN, which was the international PNT gold standard almost 50 years prior to GPS becoming operational in 1995.  At that point, LORAN-C was scheduled for termination at the end of 2000.



"However, beginning in 1997, Congress provided more than $160 million to convert the U.S. portion of the North American LORAN-C service over to enhanced LORAN.  In 2010, when the U.S. LORAN-C service ended, it was almost completely built out in the continental United States and Alaska.  During the following five years, Canada, Japan, and European countries followed the United States' lead in terminating their LORAN-C programs.  Today, however, eLORAN is one of several PNT systems proposed as a backup for GPS."



Okay.  So first of all, it's great news that the U.S. has been seriously looking into a backup technology.  Since I think our listeners will find this interesting, I'll share a little bit of background:  "In the 1980s," this author writes, "I used LORAN-C to navigate on sailing trips off the U.S. East Coast.  It had an accuracy of a few hundred feet and required interpreting blue, magenta, black, and green lines that were overprinted on nautical charts."  And we'll get to why that is here in a minute.  "The system was a modernized version of what was originally launched in 1958, a radio navigation system first deployed for U.S. ship convoys crossing the Atlantic during World War II.  Its repeatability was greater than its accuracy.  Lobster trappers could rely on it to return to the same spots where they'd been successful before, though they may have had some offset from the actual latitude and longitude.



"By contrast, eLORAN has an accuracy of better than 20 meters, and in many cases better than 10.  It was developed by the U.S. and British governments, in collaboration with various industry and academic groups, to provide coverage over extremely wide areas using a part of the RF spectrum protected worldwide.  Unlike GNSS" - which is to say GPS - "eLORAN can penetrate to some degree indoors, under very thick canopy, underwater, and underground.  And it is exceptionally hard to disrupt, jam, or spoof.  Unlike LORAN-C, eLORAN is synchronized to UTC and includes one or more data channels for low-rate data messaging, added integrity, differential corrections, navigation messages, and other communications.



"Additionally, modern LORAN receivers allow users to mix and match signals from all eLORAN transmitters and GNSS satellites in view.  For the eLORAN system to cover the contiguous United States, between four and six transmission sites could provide overlapping timing coverage, and 18 transmission sites could provide overlapping positioning and navigation."



Okay.  The article quoted Charles A. Schue, the CEO of UrsaNav. He said:  "Think of a resiliency triad, consisting of GNSS, global; eLORAN, continental; and an inertial measurement unit with a precise clock.  It is extremely difficult to jam or spoof all three sources of location and time at the same time, in the same direction, and to the same amount."  In other words, great for protecting ourselves.



So it's cool that Shaun's ancient LORAN receivers woke up and began picking up LORAN signals.  I don't know where he's located, but the intention is to cover the continental U.S. with multiple overlapping transmitters.  The author of that article:  "It had an accuracy of a few hundred feet and required interpreting blue, magenta, black, and green lines that were overprinted on nautical charts."  Right?  Why these fancy charts?



Imagine for nautical navigation, so you're out on the ocean somewhere, that two synchronized radio transmitters have been placed on the coast, several hundred miles apart.  These two stations both emit a pulse of radio frequency energy at precisely the same time, and the pulses radiate outward spherically from each station at the speed of light, so 186,000 miles per second.  So the ship at sea will receive these two pulses, but it does not know when they were sent.  So it doesn't know its distance from these transmitters.  The only thing it knows is the relative timing separation between them when they arrived.



Now, you can get out a pencil and paper and play with this a bit, but the LORAN system is called a "hyperbolic positioning system" because any given pulse separation describes a hyperbola.  In other words, when a ship received a pair of pulses, their relative spacing would tell the ship's navigator which of many possible hyperbola, plotted on their navigational charts, the ship was currently sitting on.  It would not yet have any way of knowing where it was sitting along that hyperbola, but it would have that one piece of information.  The ship would get a fix on its position along that hyperbola by tuning to a different pair of transmitters.  It would get another pulse spacing, which would identify another hyperbola on the navigation chart, and its location would be at the intersection of the first and second hyperbola.



So that's the way we located ourselves back during World War II.  The good news is that today we have far more advanced technology with integrated circuits and fancy computers that can do all of this for us.  But what hasn't changed is the decision to use low frequency, high power terrestrial transmitters to provide precise timing and location data as a backup for GPS.  It's dispiriting to imagine that we might need it, but what's been going on over in the Baltics with Russia and GPS probably helped to get those projects funded here in the United States.  So just a little very cool bit of technology.



LEO:  That is really interesting, yeah, very cool.



STEVE:  Yeah.  Hyperbolic positioning system.  And on that note let's take a non-hyperbolic break, and then we're going to talk about Passkeys.



LEO:  Yes, indeed.  Coming up, we cover security from A to Z with this guy right here, Steve Gibson.  And on we go with the show.



STEVE:  So, okay.  A number of bits of feedback from our listeners.  Jeff Urlwin, he said:  "Just listened to SN.  Passkeys are even worse based upon website implementation.  Some sites" - get this - "use a cookie to 'know'" - he has in air quotes - "they issued you a Passkey.  So even with 1Password, which supports and synchronizes among Passkeys," or among browsers, he says, "I can't use the Passkey from a different browser than originally set."  He says:  "CVS pharmacy is one with this bad implementation.  Thanks for all your great shows."



RG tweeted:  "Regarding Passkeys, for what it's worth, every website I have set up with a Passkey has let me set up multiple Passkeys, so I have not been limited to a single ecosystem."



Lachlan Hunt tweeted:  "Regarding what you said in Episode 973 about Passkeys, you'll be happy to hear that every single account for which I've been able to register a Passkey and store in 1Password has been able to support registering multiple Passkeys.  For some of my most important accounts, I've registered additional Passkeys stored on my YubiKeys.  In my experience, storing Passkeys in 1Password has been fantastic.  The only major issue I've encountered has been with certain sites, for example PayPal and LinkedIn, that do browser sniffing to unnecessarily prevent Passkeys from being used within Firefox.  This can usually be worked around by simply spoofing the User-Agent string."  But again, you know...



LEO:  Why would you do that?  That's weird.



STEVE:  We've talked about poor implementations.  So poor implementations certainly exist.  And Miguel Frade said:  "Hi, Steve.  In SN-973 you read Dave Brenton's questions about using a backup YubiKey.  To complement your answer, I'd like to share my personal experience.  I've owned two YubiKeys for several years, one with me all the time, and a backup stored in a safe place.  Some services, like Gmail, GitHub, and Bitwarden, allow us to register more than one YubiKey.  In case of Bitwarden's Family plan it allows registering up to five YubiKeys.  I guess it should be the same for Bitwarden's individual premium plan.



"Unfortunately," he writes, "PayPal only allows registering one YubiKey.  Regarding the question 'Can the same key be applied to two different people,'" he says, "the answer is yes, if we're talking about the physical key YubiKey.  Each service will use one of the 25 available slots inside the YubiKey, regardless of the person owning the account."  He finishes, "I hope this information can be useful to other SN listeners.  All the best, Miguel."



Okay.  So last week's discussion of this generated, as I've said, significant feedback from our listeners.  And the thing that stood out more than anything was that everyone showed a somewhat different set of facts.  Some said that WebAuthn/FIDO2 providers would allow any number of Passkeys to be registered with a service.  Some said that only one could be.  And others like Miguel noted that this varied by provider, with PayPal, for example, only allowing for a single registration.  If this were true, it would mean that separate "his" and "her" YubiKeys could not be used with some services.  But all other listeners noted that they had never encountered a site that did not allow for any number of Passkey registrations.  And doing so is part of the Passkeys specification.  So all sites should.



I went over to PayPal to take a look, and their Passkeys management page makes it very clear that they support multiple Passkeys without any trouble.  However, PayPal appears to only support Passkeys generated by iOS and Android devices.  Its FAQ is quite clear about that, and there's no mention of YubiKeys.  So perhaps that's what stopped Miguel.  He didn't actually register a Passkey at all over on PayPal.  He was using PayPal's longstanding and much older...



LEO:  Football.



STEVE:  ...multifactor authentication.



LEO:  Football.



STEVE:  Right.



LEO:  My first football, yeah.



STEVE:  The multifactor authentication over on PayPal.  But this further demonstrates the mess that we're currently working through.  The fact that something stopped Miguel even though he has a perfectly secure authentication device, arguably even though smartphones are now very secure, but you can argue that - you can make a strong argument that the YubiKey being so focused and single purpose and simpler, you know, and doesn't have multiple radios hooked to it, is more secure than the two smartphone brands that PayPal does support.  But this all shows that we're still in the early days of this technology.  You get a YubiKey which supports Passkeys.  PayPal supports Passkeys.  But PayPal won't support a Passkey generated by a YubiKey.



One thing that all the feedback made very clear was that many of our listeners have jumped into the Passkeys world with both feet.  They like them, and I think that's great.  Really.  I think that those of us in the industry who are grousing at the moment - and Paul Thurrott, for example, went on a nice rant again about this last week.



LEO:  Easily triggered.



STEVE:  Yes.  He's doing so, well, no, all of the users are doing - those of us in the industry are ranting because we're disappointed with the rollout and are impatient for Passkeys to live up to their potential.  We know that change takes time, and that this is still the very early days for this new technology.  Browser and browser extension support for original username and password authentication has created a system that's mostly good enough for now, with second-factor authentication adding additional protection where needed.  Your football, Leo.



None of us can predict the future, and today's Passkeys support remains really disappointing.  But in the grand, you know, if nothing else, in the fact that so few sites, you know, have jumped on the bandwagon.  On the other hand, why would they if it's not urgent for them?  But in the grand scheme, relative to how slowly new technology is adopted, Passkeys only became available yesterday.  Once the various kinks are ironed out and any device we wish to use can supply a previously generated Passkey to a website, the traditional problems with passwords will begin to fade.



I think that the most compelling use case of all is the typical user, you know, and there are a lot more of those typical users than there are listeners to Security Now!, the typical user who has no interest whatsoever in any of this.  They could care less.  They're using an iOS or Android smartphone, a Mac or Windows device having strong biometric hardware authentication.  They visit a site which newly supports Passkeys, and the site says:  "Hey.  How would you like to never need to use a username or password to log in with this device ever again?"  You know, who's not going to click yes?  Any regular user will think, "That's great.  Passwords are annoying as hell.  If I don't need to use one here anymore, count me in."



Presumably, and this is what remains unknown, whether and to what extent additional sites will offer this support over time.  If it does succeed in setting a new standard, then Passkeys will just gradually and organically seep into the world and become the way Internet users authenticate.  I think, you know, we're excited by the potential, those of us who are into the technology, and we want it to happen immediately.  But it's just going to take some time.  And clearly a lot of the listeners of this podcast have been curious about this.  And mostly their experiences have been all good, which I think is great.



Okay.  Microsoft's Head in the Clouds.  SC Magazine's headline read:  "Sweeping cybersecurity improvements pledged by Microsoft," and follows with "Numerous cybersecurity incidents" - I'm sorry.  Numerous cyber secure - but there were incidents.  But they wrote:  "Numerous cybersecurity enhancements..."



LEO:  Enhancements in response to incidents, that's what it was.



STEVE:  Uh-huh, "...will be adopted by Microsoft to address the woeful security failures driven by poor cybersecurity practices and lax corporate culture identified in a report issued by the Cyber Safety Review Board last month."  And SecurityWeek carried the headline:  "Microsoft Overhauls Cybersecurity Strategy After Scathing CSRB" - that's the same, the Cyber Safety Review Board - "Report."  And then they follow with "Microsoft Security Chief Charlie Bell pledges significant reforms and a strategic shift to prioritize security above all other product features," basically saying we're going to stop with the features here.  Although one could argue that they're not stopping with their AI push.  But otherwise, we're really prioritizing security.



Now, anyone who's been following this podcast for the past year will have heard me go off on Microsoft over their truly astonishing apparent lack of concern or accountability over egregious security practices.  Doing so always leaves me feeling a bit odd, since I'm sitting in front of Windows machines, all of my coding for the PC has been for Microsoft operating systems, from DOS through desktop and server, and I love the Windows working and development environments.



But as we've clearly documented on this podcast over and over, security researchers repeatedly hand Microsoft every detail, complete with working proofs of concept demonstrations, for various vulnerabilities which Microsoft will seemingly ignore for months and even years until that vulnerability is actually used to cause a highly public catastrophe.  And only then will Microsoft apparently think, "Huh.  Why does that exploit path have a familiar ring to it?"  Right.  You know?  And we understand why; right?  Microsoft is a monopoly. You cannot build a large modern enterprise without Microsoft glue.  Too many things require Microsoft.  So the simple fact is, Microsoft does not have to care.  And we've seen example after example of Microsoft not doing anything it does not want to do.



All of this makes Microsoft's recent pronouncements about their new focus upon security all the more interesting.  Two weeks ago, the "Cybersecurity Dive" site posted an article with a headline that caught my eye.  They wrote:  "At Microsoft, years of security debt come crashing down," and the subhead was "Critics say negligence, misguided investments, and hubris have left the enterprise giant on its back foot."



They wrote:  "Years of accumulated security debt at Microsoft are seemingly crashing down upon the company in a manner that many critics warned about, but few ever believed would actually come to light.  Microsoft is an entrenched enterprise provider, owning nearly one-quarter of the global cloud infrastructure services market and, as of the first quarter last year, nearly 20% of the worldwide Software as a Service application market.  Though not immune from scandal, in the wake of two nation-state security breaches of its core enterprise platforms, Microsoft is facing one of its most serious reputational crises.  Adam Meyers, Senior Vice President at CrowdStrike, said:  'It's certainly not the first time a nation-state adversary has breached Microsoft's cloud environments.  After so many instances, empty promises of improved security are no longer enough.'"



Okay.  Now, to review a bit, in January, Microsoft said a Russia-backed threat group called Midnight Blizzard gained access to emails, credentials, and other sensitive information from top Microsoft executives as well as certain corporate customers and a number of federal agencies.  We're going to see that the numbers were actually somewhat worse than that.  Then in early April, the federal Cyber Safety Review Board released a long-anticipated report which showed the company failed to prevent a massive 2023 hack of its Microsoft Exchange Online environment.  The hack by a People's Republic of China-linked espionage actor led to the theft of 60,000 State Department emails and gained access to other high-profile officials, actually many.



Just weeks ago, CISA issued an emergency directive to order federal civilian agencies to mitigate vulnerabilities in their networks, analyze the content of stolen emails, reset credentials, and take additional steps to secure Microsoft's Azure accounts.  While the order only applies to Federal Civilian Executive Branch agencies, CISA warned other organizations could be impacted.  For many critics of Microsoft, the events of the past nine months are the logical conclusion of a company that has ridden the wave of market dominance for decades and ignored years of warnings that its product security and practices failed to meet the most basic standards.



"AJ Grotto, the director of the Program of Geopolitics, Technology, and Governance at the Stanford Cyber Policy Center and a former White House Director for Cyber Policy said:  'In a healthy marketplace, these would be fireable offenses.  Regrettably, the marketplace is far from healthy.  Microsoft has the government locked in as a customer, so the government's options for forcing change at Microsoft are limited, at least in the short term.



"The concern was, and is, that Microsoft's security gaps would potentially lead to catastrophic outcomes.  According to Karan Sondhi, CTO at Trellix:  'Microsoft needs to dedicate its internal resources towards zero-trust initiatives and make new investments in its infrastructure.  Currently,' he says, 'Microsoft directs the vast majority of their security investments toward revenue-generating roles instead of internal security roles.'"  And we'll come back and talk about that here toward the end.



"Microsoft has a considerable stake in the cloud security space.  Not only is Microsoft one of the world's largest cloud providers, but according to Microsoft's CEO Satya Nadella during the company's fiscal second quarter conference call in January, 'It is also a major security provider to the enterprise.  Microsoft has more than one million security customers, with 700,000 using four or more of its security products.  Microsoft generates more than $20 billion in revenue per year from its security business."  In other words, by selling security that one could argue ought to be baked in.



Okay, now, I should note here for the record that I don't have any feelings at all of schadenfreude.  Really.  I'm not the least bit happy that it took some seriously frightening and damaging security lapses within Microsoft to get them to finally start thinking about taking security seriously.  It would have been better for everyone if those breaches never occurred.  But, unfortunately, all evidence suggests that nothing would have changed at Microsoft ever but for those breaches.  So the way things have been going, it was probably inevitable.



The trouble they've fallen into feels like the result of a cycle, a cultural cycle within Microsoft.  We've witnessed such cycles within Microsoft in the past.  I think that happens when a company grows so much that it keeps creating very wealthy upper management, who then, no longer needing to work, eventually leave the company.  But they're not the only things that leave.  What leaves with them is their deep understanding of the culture their leadership created while they were there.  Those who replace them think they know how to keep everything running.  But not having created it, they lack the same deep experience-based understanding of what's important.  And then, over time, the ship drifts off course.  Since I cannot even conceive of captaining a ship the size and complexity of Microsoft, it doesn't surprise me that it might lose its way from time to time; you know?  I'm amazed it's still afloat.  



Early last month, the Department of Homeland Security's CSRB, as I've mentioned, the Cyber Safety Review Board, released their findings following a deep and detailed investigation into Microsoft's recent security breach troubles.  I'm going to share the summary of that report from the Cybersecurity Dive people.



LEO:  On we go with the sad tale of Microsoft finally waking up to the security needs.  This, by the way, was not their first time.



STEVE:  No.



LEO:  At this rodeo.  They keep waking up to security requirements.  It's like, one more memo.  Okay.  That'll fix it.



STEVE:  Yeah.  And as we'll see here when I sort of summarize and wrap this all up, that it's not clear what this means, but we can hope.



LEO:  Pray.



STEVE:  Yeah.  The thing that this article from - I've lost my cursor - from the Cybersecurity Dive brings is some additional color and quotes and background from other people.  They wrote:  "The CSRB report laid out a blistering assessment of a corporate culture that has failed for years to take cybersecurity seriously.  The report was designed to assess the company's response to the summer 2023 breach from the People's Republic of China-linked threat actor that breached the company's Microsoft Online Exchange environment.  However, it also laid out a security culture that failed to adhere to the most basic standards, given the enormous market power that Microsoft yields across modern business applications in government and the private sector.



"One of the more damaging findings was that Microsoft learned of the attacks only because the State Department had set up an internal alert system after purchasing from Microsoft at additional cost a G5 license.  Customers who failed to purchase the enhanced security license were not able to see the extensive logging capabilities that would have alerted them to a breach."  And we'll get back to the implications of that also.



"Many in the security community see the CSRB report and the recent CISA emergency directive as direct indictments, not only of Microsoft's security culture, but a government that has allowed Microsoft to maintain lucrative government contracts with no fear of competition across many of its services.  Mark Montgomery, senior director at the Center on Cyber and Technology Innovation at the Foundation for Defense of Democracies said:  'The federal government gets off the hook a little easy in this report.  Despite significant encouragement from outside experts, the Biden administration  and its predecessors have failed to treat cloud computing as a national critical infrastructure that is itself critical to maintaining the security of our other national critical infrastructures.'"  So it's a, you know, a critical infrastructure infrastructure.



"Senator Ron Wyden, who called for a federal investigation following the State Department email hack, said the federal government shared responsibility for the negligent behavior disclosed in the report.  Wyden said Microsoft has been rewarded with billions of dollars in federal contracts, while not being held to account for even the most basic security standards."  Wyden told the author of this article:  "The government's dependence on Microsoft poses a serious national security threat, which requires strong action."  Now, think about that for a minute.  "The government's dependence on Microsoft poses a serious national security threat."  I know that the practice of politics generates a great deal of rhetoric, but that's not something you want a well-placed and respected U.S. Senator saying about your company.



And speaking of rhetoric, Microsoft knows how to play the game with the best of them.  "Microsoft officials said they understand the larger concerns raised by the summer 2023 attacks, as well as the continued threat from Midnight Blizzard and other nation-state actors.  The company is working to make extensive changes in its engineering processes, improve its relationships with the security community" - wow, listen to the security community?  What a concept - "and its responsiveness to customer needs.



"Bret Arsenault, corporate VP and chief cybersecurity advisor at Microsoft, said in a statement:  'We're energized and focused on executing Microsoft's Secure Future Initiative commitments.  And this is just the beginning.  We commit to sharing transparent learnings..."



LEO:  They love that word, I don't know why.



STEVE:  Oh, god, I hate that word.



LEO:  I know.



STEVE:  "...transparent learnings."  There's got to, you know, I thought, Leo, isn't there a better word?  But you've turned an activity into a noun.



LEO:  Right.



STEVE:  And I guess there's no helping you.



LEO:  There's no helping you.



STEVE:  After you've done that.  Yes.  I just wish that some of Microsoft's customers would have some walkings, and would walk away.



LEO:  From earnings.



STEVE:  Yeah, my god.  Okay.  So interestingly, one of the problems with being transparent about what's being fixed is that the process of enumerating all the improvements also serves to enumerate just how bad things had been allowed to become.



LEO:  Oh, yeah, exactly.



STEVE:  Uh-huh.  Listen to these numbers.  Bret said that since the launch of the company's Secure Future Initiative, the company has sped up related engineering work in several areas.  Okay, he calls it a "speed up."  Well, he lists four.  He says:  "Microsoft has accelerated the lifecycle management of tenants, with a focus on either unused or older systems.  The company eliminated more than 1.7 million Entra ID systems related to used, aging, or legacy technology."  In other words, there were 1.7 million Entra ID systems that could be eliminated, but had not been.  They were just, you know, hanging around, waiting to be abused.  "It has also made multifactor authentication enforcement automatic across more than one million Entra ID tenants."  Which, again, says that they weren't before.



Also:  "More than 730,000 apps have been removed across production and corporate tenants that were either out of lifecycle or were no longer meeting current standards."  Nearly three quarters of a million apps were just, again, you know, left alone.  Left there.  Even though they were no longer serving any purpose.  As we know, fundamental to security is taking an employee's badge and then removing all their passwords from the system before they have a chance to use them.  Three quarters of a million apps were left there.



Also Microsoft said:  "New employees and vendors are now being given short-term credentials to make impersonation and credential theft more difficult.  More than 270,000 have been implemented thus far."



And finally:  "The company's internal multifactor authentication implementation using Microsoft authenticator has been enhanced by eliminating a call feature and relying on an in-app login feature.  This change covers more than 300,000 employees and vendors."  Again, 300,000 employees and vendors were using an insecure feature of the multifactor authentication that likely made it easier to use, but was less secure.  So, okay.  Gee.  I guess we should fix that.



Okay.  So I've observed for some time here on the podcast that one of the reasons Microsoft has been acting the way it has, has been able to act the way it has for so long without correction, is that until now its negligence had no consequence, exactly as Senator Ron Wyden observed.  For this article, Dante Stella, an attorney at Dykema and a specialist in incident response, said that enterprise customers do not usually walk away in the face of nation-state threats against Microsoft, in part due to its enormous presence as a cloud provider.



Dante was quoted:  "Many switched to Exchange Online or Microsoft 365 to get away from on-prem servers and managed service providers.  If the only other choice is going 'back,' or a potentially disruptive switch to another platform like Google Workspace, they will most often just ride it out and trust Microsoft to fix the issues."  Right.  The customers may be unhappy; but, due to Microsoft's dominance in the market, that unhappiness is never reflected in Microsoft's bottom line.  So why change anything?



As we know, I always want to go to the source.  So after reading this piece I was curious to see the report from the Cyber Safety Review Board.  Now, the full report I'm not going to share.  It's 34 pages of quite eye-opening content.  But the short Executive Summary at the start paints the picture.  Here's what the review board found.  This is the actual report from this CSRB.



They wrote:  "In May and June 2023, a threat actor compromised the Microsoft Exchange Online mailboxes of 22 organizations and over 500 individuals around the world.  The actor known as Storm-0558, hereinafter simply as Storm, and assessed to be affiliated with the People's Republic of China in pursuit of espionage objectives, accessed the accounts using authentication tokens that were signed by a key Microsoft had created in 2016."  In other words, that key had never expired or been rotated in seven years.



Okay.  They say:  "This intrusion compromised senior United States government representatives working on national security matters, including the email accounts of Commerce Secretary Gina Raimondo, United States Ambassador to the People's Republic of China R. Nicholas Burns, and Congressman Don Bacon.



"Signing keys, used for secure authentication into remote systems, are the cryptographic equivalent of crown jewels for any cloud service provider.  As occurred in the course of this incident, an adversary in possession of a valid signing key can grant itself permission to access any information or systems within that key's domain.  A single key's reach can be enormous, and in this case the stolen key had extraordinary power.  In fact, when combined with another flaw in Microsoft's authentication system, the key permitted Storm to gain full access to essentially any Exchange Online account, anywhere in the world.  As of the date of this report, Microsoft does not know how or when Storm obtained the signing key.



"This was not the first intrusion perpetrated by Storm, nor is it the first time Storm displayed interest in compromising cloud providers or stealing authentication keys.  Industry links Storm to the 2009 Operation Aurora campaign that targeted over two dozen companies, including Google; and the 2011 RSA SecurID incident, in which the actor stole secret keys used to generate authentication codes for SecurID tokens, which were used by tens of millions of users at that time.  Indeed, security researchers have tracked Storm's activities for over 20 years.



"On August 11, 2023, Secretary of Homeland Security Alejandro Mayorkas announced that the Cyber Safety Review Board (CSRB, or the Board) would 'assess the recent Microsoft Exchange Online intrusion and conduct a broader review of issues relating to cloud-based identity and authentication infrastructure affecting applicable cloud service providers and their customers.'



"The Board conducted extensive fact-finding into the Microsoft intrusion, interviewing 20 organizations to gather relevant information.  Microsoft fully cooperated with the Board and provided extensive in-person and virtual briefings, as well as written submissions.  The Board also interviewed an array of leading cloud service providers to gain insight into prevailing industry practices for security controls and governance around authentication and identity in the cloud."  In other words, they really did look at the entire industry in order to support their conclusion that Microsoft stood out as negligent.  This wasn't common practice, what Microsoft was doing, the way Microsoft was operating.



They wrote:  "The Board finds that this intrusion was preventable and should have never occurred.  The Board also concludes that Microsoft's security culture was inadequate and requires an overhaul, particularly in light of the company's centrality in the technology ecosystem and the level of trust customers place in the company to protect their data and operations.  The Board reaches this conclusion based on seven points.  One, the cascade of Microsoft's avoidable errors that allowed this intrusion to succeed; second, Microsoft's failure to detect the compromise of its cryptographic crown jewels on its own, relying instead on a customer to reach out to identify anomalies the customer had observed."



And I'll just take a moment here to say, elsewhere they explain:  "The State Department was the first victim to discover the intrusion when, on June 15th, 2023, State's security operations center (SOC) detected anomalies in access to its mail systems.  The next day, State observed multiple security alerts from a custom rule it had created, known internally as 'Big Yellow Taxi,' that analyzes data from a log known as MailItemsAccessed, which tracks access to Microsoft Exchange Online mailboxes.  State was able to access the MailItemsAccessed log to set up these particular Big Yellow Taxi alerts because it had purchased Microsoft's government agency-focused G5 license that includes enhanced logging capabilities through a product called Microsoft Purview Audit Premium.  The MailItemsAccessed log was not accessible without that premium service."



LEO:  Oh.  This is why Microsoft gets in trouble, because they demand you pay for security.



STEVE:  Exactly.



LEO:  But wait a minute.  Big Yellow Taxi is the name of the tool?



STEVE:  Big Yellow Taxi is the name they gave to the intrusion detection rules for determining whether the Microsoft Exchange online mailboxes were being maliciously accessed.



LEO:  It's the name of a Joni Mitchell song.  But I don't really know why they used that.  That's weird.  Okay.



STEVE:  Big Yellow Taxi.  The Big Yellow Taxi alert went off, Leo, and they thought, uh-oh.  Something's wrong.



LEO:  Okay.  It's hard to take that seriously, I'll be honest with you.



STEVE:  That's why we normally don't get those internal names exposed to the public.



LEO:  Yeah, yeah.



STEVE:  Yeah.  When we find out that the State Department has named their intrusion rule "Big Yellow Taxi," it's like, uh.



LEO:  Somebody's a Joni Mitchell fan.  That's all I can say.



STEVE:  Is it a miracle you guys discovered this?  Anyway.  Also, they said, "The Board's assessment of security practices at other cloud service providers, which maintained security controls that Microsoft does not.  Fourth, Microsoft's failure to detect a compromise of an employee's laptop from a recently acquired company prior to allowing it to connect to Microsoft's corporate network in 2021."  So a compromised laptop was hooked up to Microsoft's network after Microsoft acquired a company, and that was a problem.



Also, number five:  "Microsoft's decision not to correct, in a timely manner, its inaccurate public statements about this incident, including a corporate statement that Microsoft believed it had determined the likely root cause of the intrusion when in fact it still has not," meaning even to this day.  And "Even though Microsoft acknowledged to the Board in November 2023 that its September 6th, 2023 blog post about the root cause was inaccurate, it did not update that post until March 12th of 2024, as the Board was concluding its review, and only after the Board's repeated questioning about Microsoft's plans to issue a correction."  In other words, what?  Oh.  Oh, you mean, what we immediately said back in September?  Yeah, we've been meaning to change that.  But, gee, you know, we just haven't gotten around to it.



Number six:  "The Board's observation of a separate incident, disclosed by Microsoft in January of this year, 2024, the investigation of which was not in the purview of the Board's review, which revealed a compromise that allowed a different nation-state actor to access highly-sensitive Microsoft corporate email accounts, source code repositories, and internal systems."



And, finally, number seven:  "How Microsoft's ubiquitous and critical products, which underpin essential services that support national security, the foundations of our economy, and public health and safety, require the company to demonstrate the highest standards of security, accountability, and transparency."  As opposed, obviously, to the lowest.



"Throughout this review," they wrote, "the Board identified a series of Microsoft operational and strategic decisions that collectively point to a corporate culture that deprioritized both enterprise security investments and rigorous risk management."  Deprioritized.  "To drive the rapid cultural change that is needed within Microsoft, the Board believes" - and I love the fact that here's the government telling - this board on the government suggesting how Microsoft should run things.  They said:  "The Board believes that Microsoft's customers would benefit from its CEO and Board of Directors directly focusing on the company's security culture and developing and sharing publicly a plan with specific timelines to make fundamental, security-focused reforms across the company and its full suite of products.  The Board recommends that Microsoft's CEO hold senior officers accountable for delivery against this plan.



"In the meantime, Microsoft leadership should consider directing internal Microsoft teams to deprioritize feature developments across the company's cloud infrastructure and product suite until substantial security improvements have been made in order to preclude competition for resources."  In other words, if you don't have enough people available to fix your security, why don't you just hold off on all those new improvements that you were planning and get your security house in order first.



LEO:  Yeah.



STEVE:  How would that be?



LEO:  Yeah.



STEVE:  Because, you know, national security and billions of dollars in contracts that we keep providing to you and rolling over year after year after year.



LEO:  And Big Yellow Taxi.



STEVE:  How about fixing?



LEO:  Yeah.



STEVE:  How about fixing some things?



LEO:  Yeah.



STEVE:  That's right.  Okay, Leo.  Let's take our last break.



LEO:  Okay.



STEVE:  Then I want to look back at mainframe computing and why where we are is like where we were then.



LEO:  Oh, that's interesting.  A little bit of history to tie into the present.



STEVE:  Yup.



LEO:  I like it.  Now let us conclude our journey down the highway of insecure operating systems with Steve Gibson.



STEVE:  So one of the earliest breakthroughs in computing was the introduction of a concept that came to be called "timesharing."



LEO:  Yes.



STEVE:  Back then, mainframe computers were incredibly expensive to purchase and operate.  A single machine installation was planned years in advance.  Electrical power and cooling was plumbed.  Large rooms were set aside.  And these machines had their own staff and managers.  The bean counters, who occupied the upper floors, quickly realized that their costs were the same, whether or not the monstrously expensive machine in the basement was busily working for them or sitting idle.  So the question soon became how do we keep this massive investment of ours busy?  And the answer was timesharing.



Timesharing meant that a great many people could share the machine's time.  This worked because most people spent most of their time staring at the screen of their timesharing terminal reading what had just been displayed, deciding what to do next, and then slowly punching out the next command they wished to issue.  If it had been just one person, the mainframe would have been bored to death.  But the bean counters perked right up when they learned that their machine in the basement could keep thousands of their employees, literally everyone in the building, busily poking away at their keyboards and never waiting long for their next screen of data to be presented.



Most of the company's thousands of employees never visited the basement.  They weren't allowed to.  Security was high because too much was at stake.  All of the company's jewels had been concentrated into a single small region, and those who had privileged access wore white coats and prominently displayed ID tags.  To most of the rest of the company, these tenders of the machine did not appear to speak English, and what exactly they did down there in the basement was shrouded in rumor and mystery, with some not appearing to emerge for days on end.



I've painted this picture of the past because it's interesting that it's a close approximation of what has gradually and organically re-evolved today, mostly of its own accord.  Part of it is upside down because, instead of computing being done in the basement, today it's being done in the clouds.  But we have a very similar concentration of value into a small, high-security, tightly controlled area to which few people have access.  And the concept of resource sharing exists pervasively.  Thanks to the miracle of the global Internet, the networking wires that interconnect the servers are literally being shared by everyone in the world.  And the use of virtual machine technology, which shares physical processor resources among a great many more virtual processors, is the essence of timesharing.  No single virtual machine needs to, or can, keep a high-powered cluster of processor cores completely busy; so a much larger number of virtual machines can simultaneously share that single powerful resource with many others.



This move to the cloud does not feel like yet another phase.  This feels like an inevitable evolution.  Earlier I noted that Dante Stella had been quoted saying:  "Many switched to Exchange Online or Microsoft 365 to get away from on-premises servers."  I think this represents an inevitable evolution because, just as happened in the past era of mainframe computing, the computational resource we were able to create far outstripped the needs of the typical user.  Today's processors are so powerful that most PC users today are only using a small fraction of their system's capabilities.  When this is scaled up to an enterprise of 10,000 employees, the wasted resources are astonishing.  Since most people today are, just as they were 50 years ago, staring at a screen, taking the time to figure out what it says, then poking away at their keyboard to indicate what they want to do next, we've returned to the mainframe era, and what we're sharing are cloud-based resources.



And I'll just note that the recent evolution of interactive cloud-based AI models represents another example where sharing a single massive resource among many users is vastly more economical than giving each user their own instance.  And even though local mini-models can be used, thanks to our astonishing computing power, the best models will be continuously training, which requires massive connectivity and a far greater level of processing.



Okay.  So how did Microsoft get into trouble?  There's that old observation, which I've heard isn't actually true, but it makes for a great example nevertheless, that if you toss a frog into a pot of boiling water it will immediately jump out.  But if the frog is placed into cold water and the temperature is slowly increased, it won't notice the change.



What this report makes clear is that the world has awoken to just how utterly dependent we have become upon computing in the cloud.  It happened so gradually, so incrementally and slowly, with one day following after the next, with one company after another deciding that the economics of moving their communications infrastructure into the cloud made the most sense, that, just as with the apocryphal frog, we've arrived at a position where the security of our cloud computing can no longer be considered an afterthought, and it can no longer be taken for granted.



I initially skipped past the opening statement from the chair and deputy chair of the CSRB's report because now we have some context that they had when they wrote it.  They said:  "It is not an exaggeration to say that cloud computing has become an indispensable resource to this nation and, indeed, to much of the world.  Numerous companies, government agencies, and even some entire countries rely on this infrastructure to run their critical operations, such as providing essential services to customers and citizens.  Driven by productivity, efficiency, and cost benefits, adoption of these services has skyrocketed over the past decade; and, in some cases, they have become as indispensable as electricity.



"As a result, cloud service providers (CSPs) have become custodians of nearly unimaginable amounts of data.  Everything from Americans' personal information to communications of U.S. diplomats and other senior government officials, as well as commercial trade secrets and intellectual property, now resides in the geographically distributed data centers that comprise what the world now calls the 'cloud.'



"The cloud creates enormous efficiencies and benefits; but, precisely because of its ubiquity, it is now a high-value target for a broad range of adversaries, including nation-state threat actors.  An attacker that can compromise a CSP can quickly position itself to compromise the data or networks of that CSP's customers.  In effect, the CSPs have become one of our most important critical infrastructure industries.  As a result, these companies must invest in and prioritize security consistent with this 'new normal,' for the protection of their customers and our most critical economic and security interests."



So, getting back to your comment, Leo, what will all this mean to Microsoft, and what will it mean to us?  I have no idea, and neither does anyone else.  For one thing, big changes take time.  What Microsoft's rhetoric promises is a major reorganization of their corporate priorities.  They're saying this because it has become clear to everyone that a major reorganization of their corporate priorities is exactly what will be needed.



I want to conclude our look at this by sharing the report of Microsoft's actions once the State Department's "Big Yellow Taxi" honked its horn, indeed noting that there was a problem.  I want to share it because it reads like a detective novel, which I know our listeners will enjoy; and because, while it's part of the same scathing report, it paints Microsoft in a good light and shows what this behemoth is capable of doing when it wants to, or maybe needs to.



The report wrote:  "Though the alerts showed activity that could have been considered normal - and indeed State had seen false-positive Big Yellow Taxi detections in the past - State investigated these incidents and ultimately determined that the alert indicated malicious activity.  State triaged the alert as a moderate-level event; and on Friday, June 16th, 2023, so coming up on a year ago a month from now, its security team contacted Microsoft.  Microsoft opened and conducted an investigation of its own, and over the next 10 days ultimately confirmed that Storm-0558 had gained entry to certain user emails through State's Outlook Web Access.  Concurrently, Microsoft expanded its investigation to identify the 21 additional impacted organizations and 503 related users impacted by the attack and worked to identify and notify impacted U.S. government agencies.



"Microsoft initially assumed that Storm had gained access to State Department accounts through traditional threat vectors, such as compromised devices or stolen credentials.  However, on June 26th, 10 days after the initial alert, Microsoft discovered that the threat actor had used OWA (Outlook Web Access) to access emails directly using tokens that authenticated Storm as valid users.  Such tokens should only come from Microsoft's identity system, yet these had not.  Moreover, tokens used by the threat actor had been digitally signed with a Microsoft Services Account (MSA) cryptographic key that Microsoft had issued in 2016.  This particular MSA key should only have been able to sign tokens that worked in consumer OWA, not Enterprise Exchange Online.  And this 2016 MSA key was originally intended to be retired in March of 2021, but its removal was delayed due to unforeseen challenges associated with hardening the consumer key systems."  Whatever that means.



"This was the moment that Microsoft realized it had major overlapping problems.  First, someone was using a Microsoft signing key to issue their own tokens; second, the 2016 MSA key in question was no longer supposed to be signing new tokens; and, third, someone was using these consumer key-signed tokens to gain access to enterprise email accounts.  According to Microsoft, this discovery triggered an all-hands-on-deck investigation by Microsoft that ran overnight" - oh my god, Leo, somebody lost some sleep over this.



LEO:  Well, maybe not.  Maybe they just ran it overnight, went to bed.



STEVE:  Oh, that's possible, you're right.  "It ran overnight from June 26th into June 27th, 2023, focusing on the 2016 MSA key that had issued the token, as well as the access token itself.  By the end of that day, Microsoft had high confidence that the threat actor was able to forge tokens using a stolen consumer signing key.  Microsoft then escalated this intrusion internally, assigning it the highest urgency level and coordinating its investigation across multiple company teams.  As a result, Microsoft developed 46 hypotheses to investigate, including some scenarios as wide-ranging as the adversary possessing a theoretical quantum computing capability to break public-key cryptography, or an insider who stole the key during its creation.  Microsoft then assigned teams for each of the 46 hypotheses to try to prove how the theft occurred..."



LEO:  How interesting.



STEVE:  Yeah.



LEO:  What an approach, yeah.



STEVE:  "...prove it could no longer occur in the same way now; and to prove Microsoft would detect it if it happened again.  Nine months after the discovery of the intrusion, Microsoft says that its investigation into these hypotheses remains ongoing."  Another way of phrasing this would be "Microsoft still has no idea exactly how this happened."  They know what, but not in detail exactly how.



The report continues:  "Microsoft began notifying potentially impacted organizations and individuals on or about June 19th and July 4th, respectively.  As detailed below, this effort had varying degrees of success.  Ultimately, Microsoft determined that Storm-0558 used an acquired MSA consumer token signing key to forge tokens to access Microsoft Exchange Online accounts for 22 enterprise organizations, as well as 503 related personal accounts worldwide.  Of the 503 personal accounts reported by Microsoft, at least 391 were in the U.S. and included those of former government officials, while others were linked to Western Europe, Asia-Pacific, Latin America, and Middle Eastern countries and associated victim organizations.



"Microsoft found no sign of an intrusion into its identity system and, as of the conclusion of this review, has not been able to determine how Storm-0558 had obtained the 2016 MSA key.  It did find a flaw in the token validation logic used by Exchange Online that could allow a consumer key to access enterprise Exchange accounts if those Exchange accounts were not coded to reject a consumer key.  By June 27th, 2023, Microsoft believed it had identified the technique used to access victim accounts and rapidly cleared related caching data in various downstream Microsoft systems to invalidate all credentials derived from the stolen key.



"Microsoft believed that this mitigation was effective, as it almost immediately observed Storm beginning to use phishing to try to regain access to the email boxes it had previously compromised.  However, by the conclusion of this review, Microsoft was still unable to demonstrate to the Board that it knew how Storm-0558 had obtained the 2016 MSA key."



So we've already seen that Microsoft has reversed its profit-motivated policy of charging its customers extra for security logging.  We covered that earlier.  And overall, a policy of charging anything extra in return for extra security seems similarly shortsighted.  Security should be baked into all underlying aspects of any cloud deliverable.  It should not be possible to "buy more security."  It should be impossible to purchase less.



Only time will reveal what lessons Microsoft learns from all this.  The lesson we must all learn is that when we transfer our corporate assets to the cloud, we're also transferring the responsibility for the security of those assets to the cloud services provider.  So it's important to recognize that doing so does come with some risk, and that the fine print of the provider's contract holds them harmless, regardless of fault.



LEO:  What a - what a world.  Do you feel like Microsoft's, however, recognized the issue and made the changes they need to make, and we won't have to do this all over again?



STEVE:  Everybody loves a project, Leo.



LEO:  It's like a committee.  It's very similar to a committee.  You know, you don't want to make a decision, appoint a committee.  You don't want to really solve something, create a project.  Maybe 47 of them, working overnight.



STEVE:  Yeah.  Everybody loves a project.  I mean, they need something to do.  You know, you could argue Windows is done.  You know, cloud, you know, Exchange is done.  Everybody, I mean, what is the refrain we hear?  Leave it the eff alone.



LEO:  Yeah, yeah, yeah.



STEVE:  Just why - so Microsoft, yes, put a freeze, a formal freeze on features because they keep breaking it.  Right?  I mean, how many times have we said they're never going to get rid of the bugs because as many as they fix, they introduce new ones.



LEO:  Yes. 



STEVE:  With new features.  Because they're constantly, you know, adding features.  Stop with the features already.  How about considering security a feature?  What a concept.  



LEO:  Yeah.  Good point.  Good point.  This is why you listen to Security Now!, right, every Tuesday, 1:30 Pacific, 4:30 Eastern, 2030 UTC, to get the deets, the update, the straight talk.  That's the really most important thing, without fear or favor.  So many other places on the 'Net you can get information, but there's always this undercurrent of, like, well, you know, who's paying for this?  With Steve, you know.  Steve says what he thinks and is extraordinarily trustworthy, and I love that about this show.  I'm glad you're here.  Glad you like it, too.  Steve is at GRC.com.  That's where you'll get SpinRite.  This is his only, by the way, the only thing he has, his only bread-and-butter winner, the world's finest maintenance and recovery and, what, speed-up utility for mass storage?



STEVE:  Performance recovery.



LEO:  Performance recovery.



STEVE:  Data recovery and performance recovery.



LEO:  Yeah.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#975	

DATE:		MAY 21, 2024	

TITLE:		312 SCIENTISTS & RESEARCHERS RESPOND

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-975.MP3

LENGTH:	115 MINUTES



DESCRIPTION:  Which browser has had a very rough week, and why?  Which bodily fluid should you probably not drink despite Google's recommendation?  And how can you tweak your browser to avoid those in the future?  What happens when a Windows XP machine is exposed to the unfiltered Internet?  Duck and cover!  How did a pair of college kids get their laundry washed for free?  And what do we learn about still-clueless corporations?  And finally, after engaging with some terrific listener feedback, we're going to examine the latest thought-provoking response to the EU's proposed Child Sexual Abuse Regulation from their own scientific and research community.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Of course, as always, there's a ton to talk about.  We will in just a little bit talk about the scientific response to the European CSAM proposals, those proposals that break Internet encryption.  We'll also talk a little bit about three new zero-days in Google Chrome, what happened to Google Search, and why AI is not the answer.  And just how long can an unprotected XP machine live on the Internet?  The answer will surprise you.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 975, recorded Tuesday, May 21st, 2024:  312 Scientists & Researchers Respond.



It's time for Security Now!, the show where we cover the latest security news, privacy information, talk about hacks and hackers and, you know, just kind of shoot the breeze with this super intelligent human being we call Steve Gibson.  Hi, Steve.



STEVE GIBSON:  And continue to justify our existence, apparently.



LEO:  Without Security Now! there's no justification, yeah.	



STEVE:  So, okay.  We're going to have some fun this week, not that we don't always.  We're going to examine which browser has had a very rough week, and why.  Which bodily fluid should you probably not drink despite Google's recommendation to the contrary?



LEO:  Eww.  Okay.



STEVE:  I know.  It's freaky.  And also, how can you tweak your browser so that you will be avoiding those recommendations in the future?  What happens when a Windows XP machine is exposed to the unfiltered Internet?  Duck and cover comes to mind.



LEO:  Oh, boy.



STEVE:  How did a pair of college kids get their laundry washed for free?  And what do we learn about that from the still-clueless corporations which clearly exist?  And finally, after engaging with some terrific listener feedback that we have this week, we're going to examine the latest actually quite thought-provoking response to the EU's proposed Child Sexual Abuse Regulation, this time from their own scientific and research community.  Thus the title of this podcast:  "312 Scientists & Researchers Respond."



LEO:  Awesome.  And four out of five doctors agree that this is the only thing you should be doing on a Tuesday afternoon; all right?



STEVE:  That's right.



LEO:  All right.



STEVE:  To justify your existence.



LEO:  We will get to all that.  I can't wait to hear the washing machine story.  I saw the headlines on that, and I thought, oh, Steve has got to cover this.



STEVE:  Yeah, yeah, yeah, too fun.



LEO:  Very interesting.  Very, very interesting.  And now I am prepared for the Picture of the Week.



STEVE:  So I gave this picture just a simple caption:  "Uh, what?"



LEO:  It says it all, really.  You don't - you don't need...



STEVE:  Exactly.  You know, I was tempted to give it the caption, "I don't think that means what you think it means."  And this is another one of those, Leo, where you've just got to ask yourself, you know, somebody produced this.  Somebody, like, created a plate to put on a door which you can only push in order to open the door.  Yet prominently displayed at the top, beautifully engraved and then color filled, you know, etched in this plate it says "Pull."



LEO:  I think it's for Jedi warriors to, like, practice the Force.  Pull.



STEVE:  That would be good.  Yes, exactly.  You just, you know, work on your telekinesis.



LEO:  Unbelievable.  Wow.



STEVE:  Anyway, I just got just a kick out of that.  Thanks to our amazing listeners.  They find these things and send them to us so I get to share them with everyone.



Okay.  So Google's much-beloved Chrome browser has had a very rough week.  In just one week, the total number of exploited, in-the-wild, zero-day vulnerabilities to be patched so far this year jumped from four to seven.



LEO:  Wow.



STEVE:  In other words, last week saw three newly discovered Chrome vulnerabilities receiving emergency Chrome patches.  In their blog last Wednesday, Google wrote:  "Google is aware that an exploit for CVE-2024-4947 exists in the wild."  This was also separately echoed by Microsoft, who said they were looking into it, and they were going to, like, work on fixing this thing, too, because of course Microsoft is also using the common Chromium engine.



So this latest trouble is rated as a high-severity zero-day vulnerability which results from a type confusion weakness in Chrome's V8 JavaScript engine.  The discovery was made by researchers at Kaspersky Labs when they discovered it being used in targeted attacks.  Now, these so-called "Type Confusion" bugs, we see them are arising often.  They're more formally referred to as "Access of Resource Using Incompatible Type," which sort of says the same thing.  This occurs when code misinterprets data types which can lead to unpredictable behavior - which is putting it mildly - that can allow attackers to manipulate program logic or access sensitive information.



We've talked about before how the values stored in a computer's registers or in memory might either be the actual data itself, or often can be a pointer to some other data.  The use and manipulation of pointers is, wow, I mean, it's very powerful, but also very dangerous because the pointer can potentially point to anything.  So it's not difficult to imagine what would happen if some data that the program was storing, especially if it's data that an attacker is able to manipulate, like for example the length of the data they've just sent, could be mistakenly treated by some buggy code as a pointer, in which case the attacker can control what the pointer points to and thus increase the amount of mischief that they're able to get themselves into.  In theory, that would allow an attacker, for example, to do exactly the sort of things that we see happening.



So as we've observed before, Google understandably sees no upside to revealing more details of their flaws, beyond confirming the reports of them being used in attacks and that they're now fixed.  So, you know, they say update your Chrome and you'll be okay.  And, you know, all they say is "Access to bug details and links may be kept restricted until a majority of users are updated with a fix."  And of course Google knows that by the time everyone has updated, the world will have moved on and won't care about some old bug that's since been fixed in Chrome.  So they sort of say, oh, we're not going to tell you until later, and later never comes.



However, you were just talking about the Thinkst Canary.  And this article talks about this because, or this event, because one thing that comes very clear is that network monitoring has become crucial.  The way and reason Kaspersky is able to discover such attacks is that their customers are running Kaspersky's end-point security solutions, and those solutions are feeding the intelligence that they collect back to Kaspersky's mothership for monitoring and interpretation.  So when one of Kaspersky's customers is targeted, red flags go up at Kaspersky central.



Okay, now, as I said, there were three this past week.  The other two actively exploited Chrome zero-days patched this week are 4671 and 4761, which also double as a test for dyslexia.  4671 is a use-after-free flaw in Chrome's Visuals component, whereas 4761 is an out-of-bounds write bug in, once again, the V8 JavaScript engine.  And it's worth noting that four out of the seven zero-day bugs Chrome has patched so far this year have all been located in Chrome's V8 JavaScript engine.  This is not necessarily the JIT, the Just-In-Time compiler portion.



But recall that the observation has been previously made that the overwhelming majority through time of bugs in the common Chromium core were being found in V8's - in the JIT, the Just-In-Time compiler portion of V8's JavaScript engine.  This is what led Microsoft to explore disabling Edge's Just-In-Time compilation under the theory that a modicum of speed could be sacrificed, especially given how much faster our processors are today than when this was first implemented, back when they really did need all the speed they could get.  Now it's like, well, you know, the processors are sitting around doing nothing most of the time anyway.  So how about trading off some speed in return for cutting serious vulnerabilities by more than half.



Toward the end of last month, Microsoft explained the so-called "Enhanced Security for Edge" setting that they have in their browser.  They wrote:  "Microsoft Edge is adding enhanced security protections to provide an extra layer of protection when browsing the web and visiting unfamiliar sites."  That's the keyword.  "The web platform," they wrote, "is designed to give you a rich browsing experience," blah blah blah, "using powerful technologies like JavaScript.  On the other hand, that power can translate to more exposure when you visit a malicious site.  With enhanced security mode, Microsoft Edge helps reduce the risk of an attack by automatically applying more conservative security settings on unfamiliar sites, and adapts over time as you continue to browse."



They wrote:  "Enhanced security mode in Microsoft Edge mitigates memory-related vulnerabilities by disabling Just-In-Time JavaScript compilation and enabling additional operating system protections for the browser.  These protections include Hardware-enforced Stack Protection and Arbitrary Code Guard.  When combined, these changes help provide 'defense in depth' because they make it more difficult than ever before for a malicious site to use an unpatched vulnerability to write to executable memory and attack an end user."



So Microsoft wound up with a hybrid solution where additional meaningful protections, which will take a modest toll on performance, are being selectively enabled when visiting unfamiliar sites.  But this allows Edge running on, for example, Outlook 365 or Google properties to race ahead at full speed with those extra protective guards disabled.  And given Chrome's past week of three newly exploited in-the-wild zero-days, and the fact that we appear to be unable to secure our web browsers, I think Microsoft's tradeoff makes a huge amount of sense.



Okay.  So this next piece, the fact that Leo has been driven to a paid search solution I think says important things about what has happened to search.  We're going to see some additional evidence of that.  One of the things I most loved about the early Google search was its search results' cleanliness and simplicity.  They were remarkable, not only because they were relevant.  I mean, it was astonishing back then.  Anyway, I'll come back to that in a second.



Everyone knows that my current project is implementing a state-of-the-art email system for GRC.  I hoped to be able to announce this week that the subscription management frontend was ready for the world.  But it needs some additional testing, so that'll be next week's announcement.  I wrote GRC's first email system back in the late 1990s, and it sent over the course of its life a grand total of 11 mailings.  To my surprise, last week I stumbled upon the archive of those 11 emailings.  And the second one, dated April 2nd - not April Fool's Day, fortunately - April 2nd, 1999 had the subject "Steve Gibson's News of a Stunning New Search Engine."



Okay.  So the email that I sent to GRC's subscribers a little over 25 years ago read:  "We've all experienced the problem.  The automated search engines like Alta Vista return 54,321 items 'in no particular order'" - it actually used to say that - "many of which unfortunately were porn sites.  But the human-indexed search services of the time like Yahoo," I wrote, "often cannot find what you want because they're only able to index a small fraction of the entire web since they're being indexed by people.  So you're left with the uneasy, but probably accurate  sense," I wrote, "that what you want is out there somewhere, but you're no closer to finding it."



And then I said:  "The truly amazing new solution:  A couple of extremely bright guys at Stanford University solved the Web Search Engine Problem once and for all, creating the last search system you will ever need."  And then I provided a URL that at that time no one had ever seen:  http: - no "s" back then - http://google.com.  And I wrote:  "What's their secret?  They use Linux-based web robots to explore and index the entire Web.  But then they determine the quality of each resulting link based upon the quality of the other sites that link into that site.  So the only way a site can be highly rated under Google is if other highly rated sites have links pointing into it."



I wrote, "It's brilliant.  This simple concept works so well that every single person I've told about Google has switched permanently to using Google as their web search engine of choice.  It really is that good."  And I said:  "And of course it's free, so give it a shot yourself."  And then my email ended with a link to, again, Google.com, which 25 years ago when I sent this mail on April 2nd of 1999...



LEO:  That's pretty impressive.



STEVE:  ...no one had ever heard of.



LEO:  That's great.



STEVE:  So I just, I thought, I got such a kick out of that.  So, you know, what was fun for me was that 25 years ago Google had just appeared on the scene, and there was barely a "scene" for Google to appear on.  So this really, you know, it was life-changing news that I was able to share with GRC's email list subscribers.  And way back then there was no downside to Google.  But it's been 25 years; and oh, how times have changed.  As I said at the start of this, the fact that you, Leo, have been driven to a paid search solution says some important things.



My own personal annoyance is that I never, I mean, literally, I never want to watch a video to receive an answer to whatever question I might have put into search.  Yet Google promotes videos to the top of their search results, not because they provide the best answer, but because Google owns YouTube now.



LEO:  Exactly, yeah.



STEVE:  Yeah.  I'm writing my forthcoming email system's subscription management frontend because I'm very picky about exactly how I want it to work and how I insist that GRC treats its visitors.  But I have no interest in reinventing the wheel when I have nothing to add.  So I'm using an existing SQL database-driven mailing engine on the backend to actually deliver the mail.  The other day I wanted to bring up the pages of documentation on this package's API, so I entered its full proper name, properly spelled, into Google search.  And I tried it again just now to be sure.  What I received in return, which filled the entire page vertically, thus requiring me to scroll, was four sponsored results for commercially competing products or services.



LEO:  Oh, wow.



STEVE:  And this was not because, as I originally wrote 25 years ago, those four alternative solutions are objectively better, but because they're paying Google to appear first.



LEO:  Right.  They're ads, yeah.



STEVE:  That's right.  Anyway, I know that none of this comes as news to anyone here, but I wanted to lay that foundation since against this background a piece of disturbing news about Google's latest degeneration caught my eye when BleepingComputer brought their readers up to speed.  BleepingComputer's headline Sunday, two days ago, was "Frustration grows over Google's AI Overviews feature, how to disable."



They wrote:  "Since Google enabled its AI-powered search feature, many people have tried and failed to disable the often incorrect AI Overviews feature in regular search results.  Unfortunately, you can't.  However, there are ways to turn it off using the new 'Web' search mode, which we explain below.  AI Overviews, also known as 'Search Generative Experience'" - and I might change it to Degenerative, but we'll get to that later - "is Google's new search feature," they wrote, "that summarizes web content using its in-house LLM (large language models).  Google says AI Overviews appear only when the search engine believes it can provide more value than traditional links.



"When you're signed into Google and search for general topics like how to install one of Windows 11's recent updates, Google AI will rewrite content from independent websites and summarize it in its own words."  They said:  "This feature may sound good in theory, but Google's AI integration has several quality issues, including causing a slight delay as it generates the answer; and, even more problematic, sometimes displaying incorrect information.  For example, when searchers asked how to pass kidney stones quickly, Google AI Overviews told them to drink two quarts of urine."



LEO:  What?



STEVE:  I have a snapshot of the tweet from May 5th.



LEO:  What?



STEVE:  It reads - it shows the person...



LEO:  Every 24 hours.



STEVE:  Yes.  How to pass kidney stones quickly.  And the answer is "drinking plenty of fluids, such as water, ginger ale, lemon-lime soda, or fruit juice can help pass kidney stones more quickly."  Next sentence:  "You should aim to drink at least two quarters" - and it has helpfully in parens - "(two liters) of urine every 24 hours, and your urine should be light in color."



LEO:  Okay, thank you, AI.  Holy cow.



STEVE:  Thank you so much.  Now, this was May 5th.  And it noted that AI Overviews are experimental.  This was the week before this was formally released.  And I just so loved the comment of the guy who posted this who asked the question.  He wrote in response to this:  "Perfect.  Ready to go.  Ship it out."



LEO:  Oh, my god. 



STEVE:  I know.  Wow.  So BleepingComputer said:  "Although it was initially released as an opt-in Search Labs experiment, Google recently began rolling out AI Overviews to everyone in the United States whether they want it or not, with other countries to soon follow.  Google says that AI Overviews cause people to 'use Search more'" - well, yeah, because they don't get the answer the first time, they've got to use it some more.  I don't think I want to drink pee, thank you very much.  You got any other ideas?



So BleepingComputer continues:  "That doesn't even seem to be the case on many Google support forums, that is, where people are more satisfied with their results.  For example:  'I'm finding the results very repetitive, often wrong, and they don't at all match what I'm looking for.  But they take up so much space and feel in the way, I just want them to go away.'  Another user posted over on Google forums: 'Every single result I've received from the AI Overviews has been incorrect.  I'm more capable of misinterpreting Internet articles on my own.'"



LEO:  I don't need any help, thank you.



STEVE:  Don't need any AI to help me.



LEO:  Wow.



STEVE:  "'And I can probably get at least slightly closer to actual understanding than the AI because I actually have cognitive processes.'"  So BleepingComputer wrote:  "As the posts on Google forums suggest, early feedback on Google AI Overviews has been negative, with people finding the feature unnecessary and often misleading.  Unfortunately, there's no way to disable it now that it is out of Search Labs, and Google has quickly locked support threads for many people asking how to do so."  So, whoops, you're not even allowed to ask anymore.



LEO:  Wow.



STEVE:  And they said:  "As the Google search we all fell in love with 26 years ago no longer exists, now filled with endless features, sponsored search results, and shopping results, the company recently introduced" - get this, Leo - "a new 'Web' search option to return the old search feel."  Wait.  What?  I thought that Google was web search.



LEO:  Yeah, what are they talking about?  What?



STEVE:  Right.  Much of the tech press has gotten a big kick out of the fact that Google's default search results have become so cluttered and congested with their commercial crap that even they, Google, no longer consider it to actually be web search.  Okay.  Google's search results list a series of search result "filters" in a line underneath the search field.  They typically read All, then Images, Shopping, Videos, and News.  And after that are three vertical dots and a More menu item which drops a menu containing additional filters, one of which now is Web.  And selecting that filter, sure enough, dramatically cleans up the results.



What BleepingComputer posted was a way to cause that "web mode" filter to be selected by default.  The normal search URL is /search?q={search phrase}.  But adding the magic incantation "udm=14" after the /search? and then joined with an ampersand to the "q=" clause, causes the search to default to "Web," and you get much cleaner results every time.  And since this disables a large collection of Google's default search "enhancements," including its new and still apparently troublesome AI Overview, at no point will Google AI suggest that you drink urine.  At least, yeah, the AI won't.



So I have not encountered this default web search trick anywhere else, so I've placed a link to BleepingComputer's write-up in the show notes.  And for ease of access I've also made this GRC's shortcut for the week.  So for this podcast 975, it's grc.sc/975 will take you to this article at BleepingComputer if you want to, like, see them explaining how to do this.  And you do need then to get into your browser and tweak, like to add a custom search technique and then select that as the default.  But if you want to keep using Google, and you want to return to simpler times, you are able to get the Web search results.  And you can quickly see for yourself how much better it looks just by going under those three dots and More and selecting Web, and you get a cleaned-up page.



So I'll just say, as an aside, what a mess.  You know, the fact that this generation of AIs hallucinate very convincingly and with great authority makes this AI Overlord - I mean Overview - quite worrisome.  We absolutely know that there are many people who will suspend their own critical thinking, or what used to be called "common sense," in favor of accepting "truths" provided by external sources.  Perhaps Google feels that the Internet is already so full of crap that creating intelligent-appearing overviews won't further hurt anything.  I just hope their AI improves quickly.  Wow.



LEO:  I think there's actually a story behind all this, and we'll talk about it tomorrow on TWiG.  It's unknown, obviously.  Google's not talking.  But a number of people from Google have said, you know, Google has just panicked.  In the same way they panicked when they thought they were going to lose to Meta and created Google Plus, they panicked, thinking they were going to lose the farm to AI.  And of course panic is not a good way to create new features.  And it feels like that.  They're just throwing stuff up against the wall at this point.



STEVE:  Especially something this complicated.  Leo, this stuff is so, I mean, you know, arguably most people don't know how AI works.  Right?  It's like, well, it started sounding conscious.  So...



LEO:  I don't know how it works myself.  It's kind of a mystery. 



STEVE:  Exactly.  I mean, the AI researchers are like...



LEO:  They don't know, yeah.



STEVE:  ...what?



LEO:  Yeah.  Anyway, as you say, I don't use Google anymore, so I have missed this entire drama.  You know, for a while I did use something called Neeva that had AI summaries at the beginning.  But they were always footnoted, and I never found them to be wrong.  It was done cleverly.  But what I use now, Kagi, K-A-G-I, which is a paid search tool, doesn't do that.  I think people have realized that these AI summaries are not that useful.  And just give me the results.



STEVE:  Well, and think of the good that could be done, Leo, if we ever got to a point where all of the chaff could be separated, and instead of getting nonsense from an Internet search, no matter who does it, you actually got rigorous truth.  That would be something.



LEO:  Well, but let's also be fair to Google, part of this is because the Internet is full of crap.



STEVE:  Exactly.



LEO:  I mean, the Internet...



STEVE:  So if you're training your AI on crap...



LEO:  Right.



STEVE:  ...you know, crap in, crap out.



LEO:  And even Google Search can only reflect the search contents.  And if it's garbage, everybody's trying to game Google, it's going to reduce the search results anyway.  So it's a mess right now.  It's just a mess.  All right.  We're not alone as long as we've got Mr. G. here, help us out with the world at large.  Steve?



STEVE:  So under the topic of how things have changed, PC Gamer published an enlightening article titled "A Windows XP machine's life expectancy in 2024 seems to be about 10 minutes before even just an idle Internet connection renders it a trojan-riddled zombie PC."  They wrote:  "How long do you think it takes an unprotected Windows XP box to fall foul to malware?  To be clear, this is a machine sitting idle, no Internet browsing required, just connected to the Internet.  One YouTuber, Eric Parker, decided to find out.  Using a virtual machine, Parker set up a Windows XP instance and configured it to be fully exposed with no firewall and no antivirus software, just like the good old days."



Okay, now, just to remind everybody, even though XP always had a built-in firewall, it wasn't until Service Pack 3 that the firewall was enabled by default, like by the installation of that service pack, or installing XP that included SP3 after that point.  And of course, thanks to the tyranny of the default, very few earlier pre-Service Pack 3 Windows XP machines were protected out of the box.  And I remember those days.  Remember that there was a big third-party market for firewalls.  ZoneAlarm was the one that I found and liked a lot based on the way it operated.  So Microsoft wanted to add a firewall to their Windows client platform, but they also didn't want to, you know, blatantly they'd already had a lot of problems with antitrust.  They didn't want to just go obsoleting a whole class of software immediately.  So they put it in, but they didn't turn it on.



Okay.  So PC Gamer continues:  "So how long exactly does it take for malicious software to appear on the PC?  Parker returns to his PC" - you know, his virtual PC - "10 minutes later and, sure enough, there's something nasty running in Task Manager called conhoz[.]exe (C-O-N-H-O-Z dot exe), a known trojan.  He terminates that process and leaves the machine running.  Within just a few more minutes, a new user has been added, plus a number of new processes, including an FTP server.  So, yeah, within 15 minutes that's multiple malware processes and an entirely compromised machine with the bad guys having already created a new admin account and an FTP server running locally.



"Parker then traces the malware's communication to, yup, you guessed it, the Russian Federation.  He speculates that the bad guys might be trying to set up a botnet" - you think? - "or spam email server from his compromised machine.  Further investigation reveals even more malware, including another trojan and a rootkit.  A Malwarebytes scan then reveals the full horror, with eight nasties actually running, including four trojans, two backdoors, and a couple of adware apps.  In other words" - and of course an FTP server - "the machine is already a complete and utter zombie."



And they said:  "Anyway, it's a fun watch as Parker observes his virtual XP machine being ravaged in real-time and a reminder of what's bubbling away behind the firewalls and malware protections on all of our PCs."  He says:  "Sniffing through your running processes in Task Manager used to be something of a regular ritual for the well-informed.  Now, it's not really necessary.  Famous last words and all that.  Indeed," they write, "it just goes to show how effective those machines are that we can all be connected to the Internet now 24/7 and not give this stuff much thought.  It's dangerous out there, boys and girls.  Be careful," they conclude.



Okay, now, I would - I love that.  I would edit that just a bit to observe that this vividly shows what's right now pounding away at the outside of our stateful NAT routers, those vital pieces of hardware all of our networks are blessedly perched behind.  More than any other single thing, it's the godsend of NAT routing, which placed a stateful hardware firewall filter between our internal LANs and the increasingly wild and wooly Internet, that have made it possible to use this crazy global network with any hope whatsoever of remaining safe.  Presumably, I don't know what Eric's history is, but presumably the IP that his XP machine appeared on wasn't for any reason particularly high-profile; right?  It was just some random guy.



And there's just that much crap hitting each of our IPs regularly enough that - and who knows?  Was it all the same attacker who said, oh, my goodness, we just found a new victim, you know, let's get it?  Or different attackers who were all randomly scanning the Internet and happened to lock onto this XP machine.  I mean, I have to say I'm tempted to do this because this would sound like fun.  Except you've got to be so careful.  And it would be so easy to make a mistake.  So, you know, if you do want to replicate what Eric did, then, you know, really, really, really be careful.  For anyone who's curious to see Eric Parker's YouTube video described in this article, I posted the link in the show notes so it's easy to find.  And also when I was there looking at it, I noticed that since then, and this got a lot of attention, he's done the same thing to Windows 2000.  So I didn't make time to dig in and see if 2000 was similarly vulnerable.



Okay.  TechCrunch reported that, thanks to the discovery made by a pair of curious students at the University of California at Santa Cruz, who to their credit did try to do the right thing by attempting to report the flaws they'd uncovered in the control software of their shared University washing machines, as TechCrunch headlined their story, "Two Santa Cruz students uncover a security bug that could let millions do their laundry for free."



Okay.  So the company behind these widely deployed machines is called "CSC ServiceWorks," which is an unfortunate name because the service doesn't work so well.  The two UC students, Alexander Sherbrooke and Iakov Taranenko, discovered flaws that allows anyone to remotely send commands to laundry machines run by CSC, which allows them to initiate laundry cycles without paying.  It appears to be another instance of a company that should really not be putting their equipment on the Internet,  yet doing so anyway.



Like your typical college student, Alexander was sitting on the floor of his basement laundry room in the early hours one January morning earlier this year with his laptop.  He was bored waiting for the spin cycle to finish on his last load, and while poking around with some scripting commands the machine in front of him suddenly woke up with a loud beep and flashed "PUSH START" on its display, indicating the machine was ready to wash a free load of laundry, and this was despite that fact that Alexander's current laundry system balance was $0.  Since students will be students, experimenting further, they set one of their accounts to reflect a positive balance of several million dollars in credit.  And sure enough, their "CSC Go" mobile app reflected this balance without complaint.



As I said, the company behind this is CSC ServiceWorks, a large laundry service company which boasts a network of over one million laundry machines installed in hotels, university campuses, and residences across the United States and Canada.  Oh, and also Europe.  You would think that such a firm that's using Internet and smartphone technology to replace coin-op machines might have someone on staff to field trouble reports.  But there's no indication of that.  Since CSC ServiceWorks does not have a security page for reporting security vulnerabilities, Alex and Iakov sent the company several messages through its online contact form in January, but heard nothing back from the company.  Even a telephone call to the company got them nowhere, either.



Finally, they reported their findings to the CERT Coordination Center at Carnegie Mellon University, which, as we've discussed, provides a means for security researchers to disclose flaws to affected vendors and provide fixes and guidance to the public.  Even that failed to evoke any reaction from CSC.



Today, months later, despite having tried to do the right thing, the glaring vulnerability remains open.  In following up on this, even TechCrunch failed to get anywhere.  TechCrunch wrote:  "It's unclear who, if anyone, is responsible for cybersecurity at CSC, and representatives for CSC did not respond to TechCrunch's requests for comment."



Okay.  So it seems to me that what might finally arouse CSC's attention - and apparently the only thing that will - may be a sharp and sudden drop in cash flow revenue as word of this spreads across college campuses in the U.S., Canada, and Europe. It's just the sort of hack that's pretty much guaranteed to become quite popular.  Having waited longer than the customary 90 days after attempting to report their discovery and findings, Alex and Iakov have now started to reveal more about their discovery.  They decided to disclose their research in a presentation during UC University's cybersecurity club meeting earlier this month.



They explained that the vulnerability is in the API used by CSC's mobile app, CSC Go.  In the normal case, someone needing to do the wash opens the CSC Go app to top up their account with funds, then pay, and begin a laundry load on a nearby machine.  But Alex and Iakov found that CSC's servers can be tricked into accepting commands that modify their account balances because security checks - get this - are only performed by the client app on the user's device.



LEO:  [Laughing]



STEVE:  I know.  And anything sent to CSC's servers are fully trusted.



LEO:  Oh, my.



STEVE:  This allows...



LEO:  Including things like I have a million dollars in my account.



STEVE:  Correct.  



LEO:  Unbelievable.



STEVE:  Correct.  This allows fake payments to be posted to their accounts without ever putting any real world funds in the accounts.  And Leo, it's worse.  While Alex was sitting on the floor of the basement, he was analyzing the network traffic while logged in and using the CSC Go app.  And he discovered that he could circumvent the app's security checks to send commands directly to CSC's servers.  Alex and Iakov said that essentially anyone could create a CSC Go user account and send their own commands using the API - get this - because the servers are also not checking whether new users even own their email addresses.  The researchers tested this by creating a new CSC account with a made-up email address.  So not only mistakes, but also really crappy overall system design.



Here was the comment that surprised me:  CSC quietly wiped out the student's spoofed account balance of several million dollars after they reported their findings.  But the researchers said the bug remains unfixed, and it's still possible, four months, five months later, for users to freely give themselves any amount of money.  Iakov said that he was disappointed that CSC did not acknowledge their vulnerability.  He said:  "I just don't get how a company that large makes those types of mistakes, then has no way of contacting them."  He said:  "Worst-case scenario, people can easily load up their wallets, and the company loses a ton of money.  Why not spend a bare minimum of having a single monitored security email inbox for this type of situation?"



But, of course, even that's not the point.  If the company zeroed the students' demonstration multimillion dollar account balance, that shows that someone somewhere within the company did receive the message, and does know that there's a problem.  My guess is that we have become so accustomed to the way a mature security-conscious company goes about handling such matters that we don't know what to make of a company that chooses instead to bury its head in the sand.  You know?  But we should remember that it wasn't so long ago that most companies acted this way.  They would freak out, raise the drawbridge, switch to internal power, and say nothing publicly while they scurried about behind the scenes trying to figure out what to do.  We've learned that's not the enlightened way to act with regard to Internet security vulnerabilities, but it does stand to reason that those who are not actively involved in this arena might not still be up to speed on today's etiquette.



LEO:  Hey, we're laundry guys.  What do we know about the Internet?  You put in a quarter, you wash your laundry.  This just shows you, though, it's really good for students to have to do their own laundry because that enforced period of boredom can really lead to some creative results.  That and a lack of quarters.  I love it.



STEVE:  Yeah.  We actually, Leo, I have to confess, back, you know, when I was myself at Berkeley, we had coin-op washing machines.



LEO:  Did you tie a string to the quarter and pull it back?



STEVE:  Actually there was a screw hole in the back of the quarter-accepting add-on to the washing machine.  And it didn't take long before - and I'm not saying who.



LEO:  Yeah, someone.



STEVE:  An enterprising student figured out that if you took a coat hanger - I've talked about how handy that coat hanger...



LEO:  So useful.



STEVE:  How coat hangers are like, they are the perfect type of stiff wire.  You could cut off a length and put a little hook in the end, snake it through the hole, and then you could reach in, hoping not to be electrocuted, by the way, and grab, you know, find the arm that gets pulled when the quarter is put on the little slider and pushed in, and give it a few tugs, and what do you know?  The washing machine would start right up.



LEO:  Unbelievable.



STEVE:  It is a good thing I'm not in college, you know, in this day and age.



LEO:  You know, this, honestly, there's a subtext here that maybe CSC goes, yeah, so the students are ripping us off.  What else is new?  As long as we get most of the money, we're fine.  What else is new?



STEVE:  Well, yeah.  And exactly as you said, they're in the washing machine business; right?  They probably contracted out to the lowest bidder...



LEO:  Of course they did, mm-hmm.



STEVE:  ...to make themselves an app to put these machines on the Internet.  And that guy is gone. 



LEO:  Long gone.



STEVE:  So they probably have no idea what to do.  You know, they're able to monitor account balances and zero them when they get set to a million dollars.  But other than that, eh.



LEO:  So we had an intern for a while back in the Brick House days, the wonderful Jeff Needles.  You may remember him.



STEVE:  I do remember him, yeah.



LEO:  He was a fun guy.  And he said, "I'll write a sales system for you."  And so he wrote up a whole sales database system for us that we started using for every ad sale and so forth.  And then he left.  He got a better job.  I think he went to that video company that he was so enamored of.  And we said, well, can we give you a contract to maintain?  He said, nah, you know.  And he just left.  And so we had this blob of software which breaks, well, let me put it this way.  If two people try to use it at the same time, boom, it's dead.  So it breaks all the time.  But it's not really worth it for us to redo it.  We just hired some guy who kind of looks at the code and pokes at it once in a while.  And we just limp along with it.  I think a lot of companies are like that.  I don't think that's at all unusual.  I mean, it's not a security issue because it's not public-facing in any way.



STEVE:  Right.  When I released SpinRite 6.1, we made a decision that we would no longer maintain upgrades of SpinRite from before 6.0 because it's been 20 years.



LEO:  Right.  Right.



STEVE:  And, you know, we've been more than generous for decades.  Anyway, Sue was greatly relieved because she was using what we called Dino, which actually was a dinosaur.  It was the original GRC Novell NetWare database that was written in FoxPro by my second employee, who was a truly gifted coder.  He went into writing gaming stuff.  His name is Steve Rank, and neat guy.  It's running today.



LEO:  Yeah.



STEVE:  And so it was only a few weeks ago that I said to Sue, "Sue, you no longer" - and the reason, the point was that if someone said, hey, I bought SpinRite 3.1 in '98, I think it was, and so she would look it up in Dino and say, oh, sure enough, here you are.



LEO:  Yeah.  Well, you know, we just - the way it works here is when somebody's going to go use the sales system, they send out a company-wide beacon on Slack, says everybody out.  It's funny, I mean, I think every company has something like that.



STEVE:  Yup.



LEO:  It's just normal.



STEVE:  Yup.



LEO:  It's the way it is.  And technology moves so fast.



STEVE:  It's what kept IE from ever dying.



LEO:  Exactly.



STEVE:  Is so many enterprises had written internal stuff that was dependent upon specific quirks in operation of Internet Explorer, that there was like, no, no, no, you can't take our IE away from us. 



LEO:  Now, back to the fun and games with Stevie Gibson, Little Stevie Gibson.



STEVE:  Okay.  So, many of our listeners forwarded tweets from Bernard Netherclift, who is a Voyager follower and enthusiast.  Last Thursday on the 16th Bernard tweeted:  "Fingers crossed.  This looks like Voyager 1's science data is due to resume Sunday 11:48 UTC, commands going up Friday."  So he's saying commands were being sent on Friday to switch from just sending data back as they have been to actually switching over to sending science back.  Meaning the output of the Voyager 1's surviving sensor arrays.



Then Sunday on the 19th, two days ago, Bernard followed up with:  "Voyager 1 has just returned to science mode, at a data rate of 160 bits per second, for the first time in six months."  So, yes, incredibly, I mean, it really is incredible.



LEO:  Yeah, no kidding.



STEVE:  Voyager 1 is back online after having had its programming updated to literally work around a bad patch of memory.  What an amazing piece of technology.  And, wow.



LEO:  Brilliant.



STEVE:  And also to our listeners, thank you, all you who tweeted that, making sure that I knew.  Okay.  Hakku.  "Hello, Steve.  Long-time follower and big fan of SN.  Keep up the great work.  One question following SN-973 VPN-attack topic.  We discussed this internally in our IT-Security-Consultant bubble, and one of our network guys mentioned that he would expect VPNs to use the internal firewall as soon as the VPN started, to block all outbound traffic that's not tunneled via the VPN.  Therefore, there would not be a possibility to route some traffic around the VPN since the traffic would be blocked; right?  What do you think?  Is this an actual fix?"  He says:  "We're all about to research if and which provider does use this technique.  Thanks for making my car drives a lot more interesting, and have a nice week."



LEO:  Nice.



STEVE:  "To 999 and beyond."  Okay.  So Hakku makes a great point, which is that VPNs could arrange to prevent this sort of simple routing table-driven attack.  But what the researchers found was that what "could" be done often was not being done in practice.  And I remember they mentioned OpenVPN.  One of the problems is OpenVPN is open source and cross-platform.  So what is cross-platform is using the routing table to manage rerouting.  But if you're running OpenVPN on a Windows machine, the local firewall aspect is not cross-platform.  So that's not something, I mean, whereas other platforms may also have local firewalls, they've all got their own.  And Windows is certainly not compatible with anything over in the Linux world, or Mac.



So what they found was that many popular VPNs in widespread use today were true victims of the attack which we talked about two weeks ago.  What Hakku's networking guy suggested, which was that a VPN could arrange to dynamically manipulate the machine's local firewall rules to block all other outbound traffic not bound to the VPN server IP and port, could indeed be done.  So let's hope that the popular VPN providers are being asked about their susceptibility to this particular form of simple routing table attack and then do take the effort to revise their solutions, if necessary.  And I'm glad, for example, that this came up and that Hakku brought it up to his tech guys and that they're going to do some research to make sure that they're not vulnerable.



214normandy wrote:  "Hi, Steve.  I know you've been using" - oh - "the Netgate SG-1100 as well as the four-port Protectli Vault.  I'm starting to see reports that the eMMC in the SG-1100 is starting to wear out for folks."  He said:  "I ran their suggested commands" - and he provides a link to their documentation - "to check the eMMC, and it says that my eMMC is end of life expected already."  He says:  "No big deal, I'll move on and try the four-port Protectli Vault instead.  Hoping you can confirm that you are still happy with your Protectli. Thanks, Bob."



Okay.  So this came as news to me, so I wanted to share it for any other Netgate SG-1100 users who may have followed my advice and my choice about that beautiful little Netgate appliance.  The eMMC is non-volatile memory that's soldered directly to the motherboard and cannot be replaced.  I presume that the problem is the logging and status updating that is currently churning away in the pfSense firewall.  It's constantly writing logs to the file system, and eMMC memories do not have huge amounts of excess endurance.  You know, they're meant more for embedded solutions that are not churning constantly.  I still have a trusty SG-1100.  I mean, right now the bandwidth for this podcast is passing across my SG-1100.  And it's been giving me no trouble ever since I replaced its power supply.  Remember that it was glitching, and it turned out to be the power supply that was the problem.  But it is sobering that it will have a lifetime limit due to the failure of an eMMC memory that cannot be replaced.



Bob also asked about my other favorite pfSense hosting device, which is the four-port Protectli Vault.  That's what's running pfSense at my place with Lorrie.  And yes, I'm still utterly happy with that choice, too.  And in fact I have another of those standing by ready for deployment here if the SG-1100 should ever die, which unfortunately no longer seems as unlikely as it once did.  You know, just giving it power apparently won't be enough in the long run, which is quite disappointing.



Okay.  I have an important and interesting message from a listener who requested anonymity.  He wrote:  "Hello, Steve.  I've been a listener of Security Now! for years, perhaps even a decade; a member of TWiT.tv; and a proud owner of SpinRite.  Thank you for all your incredible work.  I work for a large French company as a web developer, managing a website with a significant" - okay.  "Managing a website with a significant audience of approximately one million visitors per day.  Like many other websites, we rely heavily on advertising."  Okay, now, you can imagine with that sort of website traffic what sort of revenue their site is able to generate from all those eyeballs being confronted by ads.



Anyway, he continues:  "Similar to your sentiments, I am enthusiastic about the Google Privacy Sandbox and its potential to enhance privacy compared to traditional cookies.  However, the advertising industry is pushing back against this initiative.  As you're aware, ad companies profit by constructing user profiles and serving targeted ads.  With the advent of the Google Privacy Sandbox, their revenue streams are threatened, as user profiles will no longer be available, and ad selection will be handled by the browser itself.  Consequently, they are resisting this change.



"Their strategy" - now, we're hearing from a listener of ours who is over on the implementation side of all this.  He says:  "Their strategy involves persisting with the current model of tracking users across websites.  Several alternatives to third-party cookies have emerged and are rapidly gaining traction.  Some utilize first-party cookies through CNAME redirection such as" - and he cites the site that offers the service - "first-id.fr, while others leverage ISP data to identify users based on their Internet connection" - and then again he says - "like Utiq.com.  Additionally, there are methods involving email or phone numbers for cross-website identification, like LiveRamp.com."



He said:  "I've been tasked with implementing these solutions, and I anticipate that a majority of websites will follow suit, as a few big websites in France already have.  This is because the CPM" - meaning, you know, the amount of money they get - "for ads using the Google Privacy Sandbox is lower, resulting in reduced revenue for website owners compared to more precise tracking solutions.  Furthermore, these newer tracking methods are perceived as being more reliable than traditional third-party cookies.



"Regrettably, I fear that this development may exacerbate privacy concerns in the future.  Currently, it's possible to clear or block third-party cookies, but it will be considerably more challenging to mitigate these new tracking solutions based on first-party cookies, ISP connections, or email and phone numbers.  I believe it's crucial to inform your audience about this trend.  It's already underway, and I doubt Google can do much to counter it.  I prefer to remain anonymous to avoid potential repercussions from my employer."



Okay.  So first of all, I thank our listener for this view from the trenches.  It is disappointing, but unfortunately not surprising.  It was the subject of our "Unforeseen Consequences" podcast back on February 6th of this year.  Here's the way to think about this:  Third-party cookies enabled tracking of users based only upon the ads that were being shown and the original ability of advertisers to plant cookies into browsers along with their ads, and for those cookies to later be returned when ads were placed on other websites.  This allowed advertisers to follow users around the Internet, since the user's browser would quietly send back whatever cookies it had previously collected for the same advertiser.  The key point of this original tracking model is that it did not in any way involve the website.  It operated completely separate from the website.



And this is, crucially, what's in the process of changing now, and it's being driven by the universal change motivator, namely money.  What's changing is that websites are now beginning to collude with their advertisers specifically to facilitate tracking.  Why?  Because advertisers will pay websites more for the ads they're hosting if they collude with them to facilitate tracking which better identifies their visitors.  Our listener wrote:  "Currently it's possible to clear or block third-party cookies, but it will be considerably more challenging to mitigate these new tracking solutions based on first-party cookies, ISP connections, or email/phone numbers."



It's actually worse than that.  The bad news is that, if websites are willing to collude with third-party advertisers, there is nothing whatsoever we can do about that.  Anything a website knows about you will now be shared with third parties.  In many cases, as we recently saw with Microsoft, which was forced to disclose this due to the GDPR, I think it was, what, more than 700 was it, or 500 and some odd, I don't quite remember.  But a phenomenal number, you know, many, many hundreds of individual third parties they were confessing they would be sharing anything they had about their visitors with.  We talked about websites beginning to want their visitor's email addresses.



And Leo, you pointed out that even if we give them our throwaway email, if we always give them the same one, it still identifies us as efficiently as if we were using our primary email.  Money is the great motivator.  We saw what the ability to extract extortion payment by cryptocurrency did for the ransomware world.  It exploded overnight.  Websites are now being shown how to make more money by asking their visitors more about themselves so that they are then able to turn that information over to their advertisers.  How many are going to see this as a problem?  I would venture probably not that many.



So what was once tracking being done without website assistance is evolving into collusion between websites and their advertisers.  You know, pay us, and we will tell you everything we know about our visitors.  I think it's clearly inevitable, and there's nothing we can do about it.  As with most things which are abhorrent but invisible, as tracking always has been, most people will have no idea it's going on, and I suspect that many wouldn't care anyway.



LEO:  And this is, by the way, exactly what's happening to podcasts, as well.  The difference is we don't have any information to collude with advertisers.  And when they do ask us to put tracking pixels in, or beacons of some kind, we just say no.  And we try to constrain that.  And it hurts us, which is the reason why most advertisers now move to places like Spotify because they can get that information.  We're kind of out of luck.



STEVE:  Yeah, they want it, yeah.



LEO:  That's why we want people to join the Club because ad support's just not going to do it in the long run.



STEVE:  Kevin van Haaren tweeted:  "I'm not sure anyone's mentioned this to you yet, but Bitwarden's Passkey implementation is available now.  I was able to create a Passkey for a site on my iPad, go into work and use that Passkey from my Windows computer without issue.  When I went to add a Passkey to the account on the iPad, Bitwarden popped up automatically, asking if I wanted to create the Passkey in Bitwarden."  So yes, we had heard that support was in beta and coming soon, but I hadn't noticed that Bitwarden's support for mobile was out now, was out of beta.  That's great news.  And as we all know, Bitwarden is a sponsor of the TWiT network, and we're very glad they are.



Robert Harder tweeted:  "Regarding Passkeys, help me out here.  I feel like you and Leo are missing the point."  And Leo, it's actually more my fault than yours, so, you know.



LEO:  Oh, I'm good at missing points.  Go right ahead.



STEVE:  He said:  "Or am I?"  He said:  "I thought Passkeys were to say, 'Hey, this device has already logged in properly so let's make future logins super easy but also secure.'  So that would mean I don't," he says, "I don't want my Passkeys to be exportable.  If I ever want to log in on another device or OS or ecosystem at all, I want to prove that it's me all over again with whatever way I do that on that website, hopefully with multifactor authentication.  Only then is that device, and that device only, secured and proven.  It's a nice bonus that Apple or Microsoft or Google have internal synchronizing for their own ecosystems, but only if it's really, really, really securable.  Generally speaking, having Passkeys exportable is as bad as Firesheep days when grabbing someone's session cookie gave an opponent 100% impersonation of a victim.  Yes?  No?  Thanks."



LEO:  No.  No.



STEVE:  "Listener from Episode 1.  Rob."  Okay.



LEO:  Rob misunderstood.



STEVE:  So I think Robert makes a valid point.  Although entirely different, although, well, okay.



LEO:  Look.  Passkeys are being proposed as a password replacement.  Passwords are not specific to the device you use, nor should Passkeys be.  It's the same thing.



STEVE:  So, okay.  Another entirely different way to think of Passkeys is the way he does.



LEO:  Yeah, but it's not right.



STEVE:  Okay.  In that case, the existing username and password login is used one last time on each device, which then receives a Passkey to forevermore authenticate that user and device to that website.  Okay, I can see that as a workable model.  But here's the critical factor, and this is what you're alluding to, Leo.  That model only works in a world where every website allows for any number of Passkeys to be registered to any single web account.  And as we've been saying in the last couple weeks, it is apparently the case that any number of Passkeys can be registered to any single web account, maybe without limit?  That's the question.  Where's the limit?



So, you know, if at some point a website were to reply:  "We're sorry, but you've reached the limit of Passkeys that can be assigned to your account.  If you wish to add another, please review and remove some that have already been added."  You know, we don't know if that's ever going to happen, but we know that it could.  And the experience that Kevin just reported of creating a single Passkey with Bitwarden on his iPad, then having Bitwarden later login for him using the same synchronized Passkey under Windows, well, that's pretty slick.



LEO:  He just misunderstands what Passkeys are all about.  He's saying he's thinking they're like session cookies, which is the problem Firesheep had.  But Passkeys are not as easily accessed as cookies.  I would hope they're better secured than that.



STEVE:  They're public key crypto.



LEO:  Right.  So they're secure.



STEVE:  And so they're not at all the same.



LEO:  And furthermore, they're being proposed as a replacement for passwords.  So that's not what he's just described.  He's describing a replacement for session cookies.  That's not what Passkeys are.  So I think he just misunderstands what Passkeys are.  They are a replacement for passwords and, as a result, are not tied or should not be tied to a specific device.  And you're right.  A password manager can be and probably should be the person that holds your Passkeys, just as they are the people who hold your passwords.  Yeah.



STEVE:  Okay.  Spencer Webb, he tweeted:  "Enjoyed the eLORAN discussion.  I know the guy at UrsaNav.  We had discussions about some projects a few years back.  When the USG turned off LORAN, I thought it was incredibly stupid.  It does work indoors, and in caves, and without an ionosphere.  And yes, you can read into the above some interesting scenarios.  Remember to feed your antenna.  Best, Spencer."



LEO:  He's probably a ham.  I think that's a ham.



STEVE:  He is.



LEO:  Okay.



STEVE:  Spencer is a serious radio guy.



LEO:  That's what I thought.



STEVE:  We often exchange notes when something about radio comes up.  I remember back in the days when you had to hold your iPhone in the proper way, he and I were having some conversations about antenna science.  So anyway, it was nice to have him add to the eLORAN discussion.  I think it's clear that having a system that's fundamentally terrestrial has many applications, even when GPS is working well.



Oh, this is interesting.  Dr. Brian of London tweeted:  "I integrated Passkeys into my own site as a secondary login system which in some cases is easier to use, especially on mobile devices, than the primary method, which is cryptographic signing of a challenge with either a browser plugin wallet or something using QR codes that looks like SQRL to prove ownership of personal keys."  So, okay.  What he's saying is he has a website, and he rolled his own fancy login system which he's had for some time.  But then he decided, hey, Passkeys is a standard.  I'm going to add that to my site.



So he said:  "To this I added the ability to associate one or many keys with an account and add/delete/rename them.  One little gotcha which you probably only learn when implementing this."  He said:  "I store on my server a list of all public Passkeys, and every time I get a login request from a client, I could send every public key I have, and the client would figure out which, if any, it holds.  But in reality I don't do that.  I associate each of the public keys with a username.  This is part of my primary system anyway, but that username is the only thing I hold.  I don't have emails or passwords."



He said:  "I use that username to filter the list of public keys I send back to the client, which then figures out if the user's device has any of them."  He says:  "It works nicely with Apple Passkeys and other Passkeys which already sync across multiple devices nicely."  So basically he's saying he rolled his own server-side Passkeys implementation.  As a consequence, he has a bunch of accounts, and each of those accounts has Passkeys.  He could send all of the public keys to the client, which would then say, oh, I found a match, which would tell him who it was that was wanting to log in.  But instead he asks them for just their username, which allows him then to filter from all of his public Passkeys only those associated with that user and send those back in order to give it a chance to log in with a Passkey.  So anyway, we have users who are implementing Passkeys on the server side, which is also very cool.  Or rather listeners.



Shaun Merrigan said - remember that it was Shaun - oh, right.  Okay.  First of all, Shaun was the guy with the old LORAN receiver which woke up when eLORAN was turned back on late last year.  Anyway, he heard us talking about him last week, and he followed up with a bit of more interesting information.  He said:  "To close the loop on this, my location is Edmonton, Alberta, Canada."  Okay, that's where he is.



He said:  "The three eLORAN stations that are currently testing are Fallon, Nevada; George, Washington; and Havre, Montana."  So he's receiving signals from those three locations.  He says:  "This is my best information.  Currently, my old Austron 2100F is showing 2.8E-12 seconds offset from GPS."  Again, 2.8E-12 seconds offset from GPS.  So, yeah, lots of accuracy in LORAN available timing data.  And really it sounds like once this is turned back on, all of our clocks that used to synchronize on WWVB...



LEO:  Oh.  Lost LORAN.



STEVE:  Yeah.



LEO:  Oh, that's cool.



STEVE:  Because, you know, that's not very reliable, that whole WWVB.



LEO:  And they wanted to turn it off for a long time.



STEVE:  Yeah.



LEO:  Yeah.  Well, clever.



STEVE:  Talk about range.  That's really cool range.  Oh.  Markus Daghall tweeted:  "Hi, Steve.  While looking at the PIN heat map graph, the number 1701 seems to be more prevalent than its surrounding numbers."  Which I love because we know what that is.



LEO:  Star Trek NCC 1701.



STEVE:  Exactly.



LEO:  That's a good PIN.  Wish I'd thought of that.



STEVE:  NCC 1701.  That is great.  Ed Ross tweeted:  "Re Big Yellow Taxi," he said, "presumably that system helps in situations where 'you don't know what you've got till it's gone.'"



LEO:  They paved paradise, put up a parking lot.



STEVE:  And that was your observation last week, Leo.



LEO:  Yes, yes, yes.



STEVE:  Riny, and I can't even begin to pronounce this guy's last name, he's in Spain, H-E-I-J-D-E-N-D-A-E-L.



LEO:  It's a Dutch name, Heijdendael.



STEVE:  Perfect, thank you, Leo.  So he wrote:  "As many, I started the FIDO1 journey with YubiKey, but even then I was splattered by the messy software support, implementation guides, and it was at that level that I thought it was a no-go for regular users - slot selection, HMAC, keyboard emulation, all cool, a bit too cool.  But when FIDO2 came along we had to switch tokens anyway, and I switched to 'Token2,' a Swiss-made token that manages selective key removal, up to 300 keys, and enforced PIN complexity, all for a better price than the YubiKey.  Furthermore, I needed TOTP for two-factor authentication that would work as a standalone device when traveling, and even that is in their device.  I just don't understand why YubiKey is still pushed as the de facto standard.  What do you think?"



Now, he included in his note a link, and it's in the show notes at the top of page 15.  And he finished, saying - and Leo, I should mention I have two on order now.



LEO:  Yeah, I think I'm about to buy some, yeah.



STEVE:  Yup.  He said:  "Keep up the good work.  By the way, I silently suspect that you were hired by the UK government to write their specs for them."  And of course he's talking about the fact that we talked about the requirements that the UK had for their consumer IOT devices, and it did actually sound like, you know, they've been listening to this podcast.



LEO:  It sure did, yeah, yeah.



STEVE:  Okay.  So I needed to let all of our listeners know about these Token2 Passkeys dongles.  They look fantastic, and supporting 300 Passkeys, individually manageable and deletable, with both USB-A and C connection options, they look fantastic.  I will certainly admit to feeling some proprietary intellectual connection to YubiKey as the guy who happened to come along at the right time and had the perfect audience for them with this podcast.  But that's the limit of it.  I would like them to succeed in the long term, but that requires them to keep up in what has obviously become a very competitive market.  The huge advantage they've been enjoying is having been first.  And that's a big deal.



But to remain first they need to remain competitive, and we've all been scratching our heads over why they would still have a 25-key limitation when such limitation pretty much relegates them to the enterprise or password manager unlocking role.  To be a consumer's primary Passkeys container requires that they be able to retain and selectively manage hundreds of keys.



So I'll say it again.  These Swiss-made Token2 dongles look fantastic.  And I should note that YubiKey has since announced a new key, and I don't remember the number.  It might be 200.  But even now it doesn't appear to still be 300.  Or maybe 100.  Anyway, the bad news is unfortunately these guys are in Switzerland, and the one we want is currently sold out.



LEO:  But it says June 17th shipping, so that's not so...



STEVE:  Oh, that's good.



LEO:  Yeah.



STEVE:  That's good.  Although shipping, unless you choose postal mail, which they discourage, is twice the cost of the dongle.  So, okay.  Anyway.  Anyway.  The way I know that is that I've ordered two, and they're on the way.  So anyway, thank you very much, Riny, for providing a direct link to the Token2 page, which as I said is in the show notes.



Also, another listener, Andreas in Germany, also pointed to the Token2 solution which, by the way, is FIDO, FIDO2 with WebAuthn, TOTP, USB, and NFC.  And it really does look very slick.



LEO:  Clearly they put a fairly potent chip in there.  So when it says 300, that could be 300 Passkeys?



STEVE:  It is, 300 FIDO2 WebAuthn Passkeys.



LEO:  Oh, wow.  300 is probably a good start.  I don't - at least for a while, yeah.  All right, Steve.  Let's talk about 200 doctors can't be wrong.  Or something like that.



STEVE:  Our listener, Robin van Zon in the Netherlands, brought this recently produced letter to my attention.  So thank you, Robin.  The letter opens by introducing itself:  "The text below is an open letter on the position of scientists and researchers on the recently proposed changes to the EU's proposed Child Sexual Abuse Regulation."  Now, we're interested in this, of course, because this is all about whether we're going to have backdoors and something is going to be monitoring communications for, you know, grooming and CSAM material and so forth.  "So as of the 7th of May, exactly two weeks ago today, the letter has been signed by 312 scientists and researchers across 35 countries."  I mean, it is the Who's Who of security and research.



So, and what's interesting is that there has been some very good, you know, good faith back-and-forth here.  So this is not an open letter that's just being blown off and being ignored.  The EU's regulators and legislators have changed their legislation in an attempt to solve the problems that were earlier voiced.  As we're going to see, not only are they not there yet, but there's real good reason to believe, as we probably all know, you can't get there from here.



Okay.  So it turns out that what scientists and researchers have to say is quite refreshing because it actually engages science, math, statistics, and, yes, reality, as opposed to the politicians' statements of "this is what we want and what we're preparing to demand."



So I want to share what these 312 scientists and researchers collectively assembled.  And it's not overly long, you know, because the devil as it turns out is in the details, and because there's probably no more important issue on the table at this moment, arguably in the world, than what the EU's political class will finally decide to do about this.  And importantly, as we'll see, this is the technical response to the politicians' responses to the previous technical response.  And as I said, what's heartening is that both sides so far appear to be negotiating here in good faith.  And the politicians are at least listening.



So as we know, for their part, the UK was faced with the same problem and serious opposition to their similar proposal to require all private conversations to be monitored for content.  What they did was wisely added the caveat "where this can be proven to be technically feasible without compromising security," which allowed the politicians to say that they had passed legislation and allowed all the messaging providers to continue offering fully private end-to-end encryption because it hadn't been and cannot probably be proven to be feasible without compromising security.  So win-win, win-win-win.



Okay.  But the European Union is not there yet.  So here's the latest feedback from the EU's technical experts, which is intended to inform the politicians of reality.  The undersigned wrote:  "We're writing in response to the new proposal for the regulation introduced by the Presidency on the 13th of March, 2024."  So 13th of March, right, just a couple months ago.



"The two main changes with respect to the previous proposal aim to generate more targeted detection orders, and to protect cybersecurity and encrypted data.  We note with disappointment that these changes fail to address the main concerns raised in our open letter from July of 2023" - so nearly a year ago - "regarding the unavoidable flaws of detection techniques and the significant weakening of the protection that is inherent to adding detection capabilities to end-to-end encrypted communications.  The proposal's impact on end-to-end encryption is in direct contradiction to the intent of the European Court of Human Rights' decision in Podchasov v. Russia on the 13th of February of this year.  We elaborate on these aspects below."



Now, just to interrupt here, I tracked down that decision.  The case surrounded Russia's FSB demanding that Telegram turn over the decrypted communications of six individuals who the FSB alleges were involved in terrorism against the Russian state.  Telegram refused, explaining that since all of the subjects involved had enabled Telegram's optional end-to-end fully encrypted mode, Telegram's default ability to store unencrypted conversation data in their servers was thwarted.  And indeed, paragraphs 79 and 80 of the decision of the European Court of Human Rights backed that up.  And I skipped all of the earlier paragraphs.



Here's what those two paragraphs say.  79 says:  "The Court concludes that in the present case the ICO's statutory obligation to decrypt end-to-end encrypted communications risks amounting to a requirement that providers of such services weaken the encryption mechanism for all users.  It is accordingly not proportionate to the legitimate aims pursued."  In other words, yes, the intention is legitimate, but the only way you can do this is by weakening encryption for everybody.  And that's not a proportionate response.



And then paragraph 80 says:  "The Court concludes from the foregoing" - and that's all the other paragraphs that I'm sparing everyone - "that the contested legislation providing for the retention of all Internet communications of all users, the security services' direct access to the data stored without adequate safeguards against abuse and the requirement to decrypt encrypted communications, as applied to end-to-end encrypted communications, cannot be regarded as necessary in a democratic society.



"Insofar as this legislation permits the public authorities to have access, on a generalized basis and without sufficient safeguards, to the content of electronic communications, it impairs the very essence of the right to respect for private life under Article 8 of the Convention.  The respondent State has therefore overstepped any acceptable margin of appreciation in this regard."



So what this tells us is that, separate from whatever political pressures the EU's politicians may be under, when the issues at stake are very carefully and thoroughly examined by the European courts, their decisions never support the application of wholesale surveillance.  For the sake of our listeners' sanity, as I said, I skipped over the first 78 paragraphs.  But those paragraphs make it very clear that the courts really do very clearly understand the issues.  They clearly understand that the phrase "selective backdoors" is an oxymoron.



Okay.  So continuing with the technologists' latest rebuttal response to the politicians' attempt to mollify them following their first surveillance proposal, they all wrote and signed:  "Child sexual abuse and exploitation are serious crimes that can cause lifelong harm to survivors.  Certainly it is essential that governments, service providers, and society at large take major responsibility in tackling these crimes.  The fact that the new proposal encourages service providers to employ a swift and robust process for notifying potential victims is a useful step forward.



"However, from a technical standpoint, to be effective this new proposal will also completely undermine communications and systems security.  The proposal notably still fails to take into account decades of effort by researchers, industry, and policymakers to protect communications.  Instead of starting a dialogue with academic experts and making data available on detection technologies and their alleged effectiveness, the proposal creates unprecedented capabilities for surveillance and control of Internet users."  Again, "the proposal creates unprecedented capabilities for surveillance and control of Internet users.  This undermines a secure digital future for our society and can have enormous consequences for democratic processes in Europe and beyond."



So then they bring up five points.  The first:  "The proposed targeted detection measures will not reduce risks of massive surveillance."  They said:  "The problem is that flawed detection technology cannot be relied upon to determine cases of interest.  We previously detailed security issues associated with the technologies that can be used to implement detection of known and new CSA material and of grooming because they are easy to circumvent by those who want to bypass detection, and they are prone to errors in classification.  The latter point is highly relevant for the new proposal, which aims to reduce impact by only reporting 'users of interest,' defined as those who are flagged repeatedly," and they said, "as of the last draft, twice for known CSA material and three times for new CSA material and grooming."



They said:  "Yet this measure is unlikely to address the problems we raised.  First, there is the poor performance of automated detection technologies for new CSA material and for the detection of grooming.  The number of false positives due to detection errors is highly unlikely to be significantly reduced unless the number of repetitions is so large that the detection stops being effective.  Given the large amount of messages sent in these platforms, in the order of billions, one can expect a very large amount of false alarms, on the order of millions."



So they then had a footnote which explains how they draw this conclusion.  They said:  "Given that there has not been any public information on the performance of the detectors that could be used in practice, let us imagine we would have a detector for CSAM and grooming, as stated in the proposal, with just a 0.1% false positive rate, in other words, in a thousand times it incorrectly classifies non-CSAM as CSAM, which is much lower than any currently known detector."  Right?  So they're drawing like a best, absolutely beyond best possible case.



They said:  "Given that WhatsApp users send 140 billion messages per day, even if only one in 100 would be a message tested by such detectors, there would be 1.4 million false positives every single day.  To get the false positives down to the hundreds, statistically one would have to identify at least five repetitions using different, statistically independent images or detectors.  And this is only for WhatsApp.  If we consider other messaging platforms, including email, the number of necessary repetitions" - that is, you know, repeated hits on a given individual before you raise the alarm in order to bring down basically the rate at which alarms are being raised, you need to raise that number of repetitions, they say - "would grow significantly to the point of not effectively reducing the CSAM sharing capabilities."  Meaning detection would be effectively neutered.



Then they said:  "Second, the belief that the number of false positives will be reduced significantly by requiring a small number of repetitions relies on the fallacy that for innocent users, two positive detection events are independent, and that the corresponding error probabilities can be multiplied.  In practice, communications exist in a specific context, for example, photos to doctors, or legitimate sharing across family and friends."  They said:  "In such cases, it is likely that parents will send more than one photo to doctors, and families will share more than one photo of their vacations at the beach or pool, thus increasing the number of false positives for this person.  It is therefore unclear that this measure makes any effective difference with respect to the previous proposal."



Okay.  So in other words, the politicians proposed to minimize false positive detections by requiring multiple detections for a single individual before an alarm is raised.  But the science of statistics says that won't work because entirely innocent photographs of one's children will not be evenly distributed across the entire population of all communicating users.  People who have young families and like to share photos of their children frolicking at the beach in their bathing suits will generate massive levels of false positive CSAM detections because there is massively non-equal distribution of content that might falsely trigger CSAM detection.



The scientists explained:  "Furthermore, to realize this new measure, on-device detection with so-called client-side scanning will be needed.  As we previously wrote, once such a capability is in place, there is little possibility of controlling what is being detected and which threshold is used on the device for such detections to be considered 'of interest.'"  I should explain that another amendment to the proposed legislation involves their attempt, the legislators' proposal of attempting to divide applications, that is, you know, like WhatsApp as an application, Telegram as an application, to divide applications into high-risk and low-risk categories so that only those deemed to be high risk would be subjected to surveillance. 



The techies explain why this won't work.  They write:  "High-risk applications may still indiscriminately affect a massive number of people.  A second change in the proposal is to only require detection on parts of services that are deemed to be high risk in terms of carrying CSA material.  This change is unlikely to have a useful impact.  As the exchange of CSA material or grooming only requires standard features that are widely supported by many service providers, such as exchanging chat messages and images, this will undoubtedly impact many services.



"Moreover, an increasing number of services deploy end-to-end encryption, greatly enhancing user privacy and security, which will increase the likelihood that these services will be categorized as high risk.  This number may further increase with the interoperability requirements introduced by the Digital Markets Act that will result in messages flowing between what was previously low-risk and high-risk services.  As a result, almost all services would be classified as high risk.



"This change is also unlikely to impact abusers.  As soon as abusers become aware that a service provider has activated client-side scanning, they'll switch to another provider, that will in turn become high risk.  Very quickly all services will be high risk, which defeats the purpose of identifying high-risk services in the first place.  And because open-source chat systems are currently easy to deploy, groups of offenders can easily set up their own service without any CSAM detection capabilities.



"We note that decreasing the number of services is not even the crucial issue, as the change would not necessarily reduce the number of innocent users that would be subject to detection capabilities.  This is because many of the main applications targeted by this regulation, such as email, messaging, and file sharing, are used by hundreds of millions of users, or even billions in the case of WhatsApp.



"Once a detection capability is deployed by the service, it's not technologically possible to limit its application to a subset of users.  Either it exists in all the deployed copies of the application, or it does not.  Otherwise, potential abusers could easily find out if they have a version different from the majority population and, therefore, if they have been targeted.  Therefore, upon implementation, the envisioned limitations associated with risk categorization do not necessarily result in better user discrimination or targeting, but in essence have the same effect for users as blanket detection regulation."  So basically these guys are just, you know, they're cutting through these proposals one after the other, very carefully backing up their statements with, you know, actual data.



The second is:  "Detection in end-to-end encrypted services by definition undermines encryption protection."  They go over this again, explaining why that's the case.  Oh, and they note one of the other arguments is, and we've talked about this on the podcast, the idea of adding age discrimination.  Well, they said:  "Introducing more immature technologies may increase the risk.  And they note that their proposal states that age verification and age assessment measures will be taken, creating a need to prove age in services that before did not require so.  It then bases," they said, "some of the arguments related to the protection of children on the assumption that such measures will be effective.



"We would like to point out that at this time there is no established, well-proven technological solution that can reliably perform these assessments.  The proposal also states that such verification and assessment should preserve privacy.  We note that this is a very hard problem.  While there is research towards technologies that could assist in implementing privacy-preserving age verification, none of them are currently in the market.  Integrating them into systems in a secure way is far from trivial.  Any solutions to this problem need to be very carefully scrutinized to ensure that the new assessments do not result in privacy harms or discrimination causing more harm than the one they're meant to prevent."



So they conclude, saying:  "With secure paths forward for child protection," and this is really good.  They said:  "Protecting children from online abuse, while preserving their right to secure communications, is critical.  It is important to remember that CSAM content is the output of child sexual abuse.  Eradicating CSAM relies on eradicating abuse, not only abuse material.  Proven approaches recommended by organizations such as the UN for eradicating abuse include education on consent, on norms and values, on digital literacy and online safety, and comprehensive sex education; trauma-sensitive reporting hotlines; and keyword search-based interventions.  Educational efforts can take place in partnership with platforms, which can prioritize high-quality educational results in search or collaborate with their content creators to develop engaging resources.



"We recommend substantial increases in investment and effort to support existing proven approaches to eradicate abuse, and with it, abusive material.  Such approaches stand in contrast to the current techno-solutionist proposal, which is focused on vacuuming up abusive material from the Internet at the cost of communication security, with little potential for impact on abuse perpetrated against children."  So in other words, you politicians are aiming at the wrong target anyway.  So even if you got everything you want by effectively eliminating security and all privacy, it won't actually solve the problem that you're hoping to solve.



So I think the problem is that this is like an iceberg.  CSAM is the tip of the iceberg that is the visible manifestation of something that is abhorrent.  And because we see it, the tip of that iceberg, we want to get rid of it.  But these authors remind us that CSAM is the output, it's the result of these abhorrent practices, less so the practices themselves.  What I'm heartened by, as I said at the top, is that we appear to be seeing a true, honest back-and-forth negotiation in good faith between European Union politicians and European scientists and researchers.  Given that the original proposed legislation was significantly amended after their first round of objections and feedback, it appears that the politicians are heeding what their technocrats are explaining.  And of course we have no idea what's going to finally happen, which is what makes all this so interesting.  And it is obviously very important.  So stay tuned.



LEO:  Yeah.  It's so much easier to go after the symptom than the cause; you know?  So much - yeah, yeah.



STEVE:  Isn't it?  Exactly right.  That is exactly right.  



LEO:  And unfortunately there's huge side effects to going after the symptom that make for more problems.  So it's not really a great solution.



STEVE:  Yeah.  And nothing prevents the politicians from wanting to save face or look good by saying, "We did this."



LEO:  We fixed it.  It's all over.



STEVE:  Yeah.



LEO:  But, see, it's not going to be all over.  And really that's the nut of it.



STEVE:  And the best, yes, the most important reminder is that CSAM is the output of the practice, not the practice itself.  And it's the practice that you want to curtail.



LEO:  Right.  Well, good stuff, as usual.  You have no fear to go where angels fear to tread, and that's good.  That's good.  That's what we want.  You're going to hear it here.  You're going to hear it all.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#976

DATE:		May 28, 2024

TITLE:		The 50 Gigabyte Privacy Bomb

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-976.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Why is Google's AI Overview fundamentally impossible today?  And what's the latest news on how to suppress it?  What's LastPass's decade-late announcement?  Why and when is a VPN not a VPN?  Are eMMC chips really impossible to replace?  Are vertical tabs finally coming to Firefox?  What's one well-informed listener think about Fritz!Box network appliances?  And what's just about the worst thing that could be done with four-digit PINs?  Were we guilty of WinXP abuse by exposing it to today's Internet?  And how can Security Now! listeners now send email directly to me?  Yes!  GRC's new email system is alive.  After looking at all of that, we're going to examine the latest crazy idea from Microsoft which deliberately plants a 50 Gigabyte Privacy Bomb right in the middle of all Windows 11 PCs.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have lots to talk about.  He's going to give you his take on Windows 11 new Recall feature.  That's the 50 Gigabyte Privacy Bomb you've heard about.  Also, when is a VPN not a VPN?  Can you really replace surface-mount chips?  Are vertical tabs finally coming to Firefox?  And a new way to contact Steve directly.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 976, recorded Tuesday, May 28th, 2024:  The 50 Gigabyte Privacy Bomb.



It's time for Security Now!, yay.  We wait all week for this; don't we, kids?  And here he is, appearing magically like a wizard in a puffy of greasy smoke - what are you sucking your thumb for?  Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again.



LEO:  I called them kids, not you.



STEVE:  As I knew who you were talking about.  We are beyond, you know, we're discussing things that happen when you're no longer a kid.



LEO:  Mm-hmm.



STEVE:  Which they didn't really tell you about when you were kids.



LEO:  No.



STEVE:  It's like, ah, you old people, that's never going to happen to me.



LEO:  Well, we were talking on MacBreak Weekly about tinnitus, ringing in your ears.  Do you have that?



STEVE:  No.



LEO:  No.  You were smart.



STEVE:  I have - I said "no" too quickly.  Like right now, if I concentrate, I can hear probably that 9 kHz very faintly in the background.



LEO:  Faint, okay.  That's not bad, yeah.  



STEVE:  Yeah, so it's not bad.



LEO:  I hear it.  It's very predominant, very persistent.  So reason to talk about it, and I'll mention it on this show, too, I'm sure, is I'm doing this new FDA-approved process that uses electrodes on your tongue while you're listening to something, I'm not sure what, in your ears.  I'm going to get fitted for that a week from Thursday.  I'll let you know.



STEVE:  Yes, on your tongue it feels like pop rocks, kind of fizzy.



LEO:  Kind of, yeah, that's what people report, little tickling.  Yeah.  I can take that.  And they tune it.  They can say, they told me, they say if it's really bothering you we can turn it down.  All right.  But I want all the pop rocks, personally.



STEVE:  Well, you want to get the full dose treatment, whatever it's going to do, that's right.



LEO:  Geez, it's driving me crazy at this point.  I really need to.



STEVE:  Be really cool to see if it helps.



LEO:  Yes, I hope so.  Anyway, what's on...



STEVE:  Eighty percent of people got helped by it.



LEO:  Yeah, well, that's why I'm doing it.  I mean, it's not cheap.  But I thought, if 80%, if it works for 80%, maybe I should try it, yeah.



STEVE:  Yeah.



LEO:  Watch, I'll be in the one in five it doesn't.



STEVE:  Okay.  So we are closing in on 999.



LEO:  Yay.



STEVE:  And the good news is, that's not bad news.  So...



LEO:  Bad news for you.  Good news for the rest of us; right?  You have to keep working.



STEVE:  We've got another great fun episode here.  Today's episode is titled "The 50 Gigabyte Privacy Bomb," which we will be discussing at length.  But we're first going to talk about why Google's AI Overview is fundamentally impossible for them to do today.  And we're going to look at the latest news on how to suppress it.  As you would expect, you know, lots of people are coming up with various ideas.  And there are some cool solutions.  Also, I just thought I would bring up the fact that LastPass has announced something that is, yeah, maybe 10 years too late.  So thanks.  And also, why and when is a VPN not a VPN?  Are eMMC chips really impossible to replace, as I kind of offhandedly said last week?  We have a listener who takes issue with that.  Are vertical tabs finally coming to Firefox?  Everyone wants them.  No one seems to be able to get them, at least not natively.



Also, what does one well-informed listener think about the FRITZ!Box network appliance which came up last week?  What is the worst thing that could be done with four-digit PINs?  We have a listener who explains his firsthand experience with that.  And were we guilty of Windows XP abuse by exposing it to today's Internet?  Apparently someone feels we should have been a little kinder and gentler.  Also, yes, how can Security Now! listeners now send email directly to me?  GRC's new email system is alive.  And after looking at all of that, we're going to examine the latest crazy idea from Microsoft which deliberately plants 50GB of privacy bomb right in the middle of all Windows 11 PCs.



LEO:  Yeah, mm-hmm.



STEVE:  Where we'll be marching out our often-used, sometimes overused, "What could possible go wrong?"



LEO:  So you're saying we're not talking Little Boy, we're talking Fat Man on this one.



STEVE:  We're talking could we have done anything that more delights Chinese attackers?



LEO:  Oh, great.  Oh, good.  Oh, that's exciting.  All right.  We'll get to all that and the world-famous Picture of the Week.



STEVE:  Picture of the Week.



LEO:  Actually, I'm glad you chose the one you chose because there are far worse ones.



STEVE:  Yeah, we're not doing cockroaches.



LEO:  Don't do cockroaches.



STEVE:  I'm not doing the cockroaches one.



LEO:  Okay, good, all right.



STEVE:  No.



LEO:  All right.  All right.  We'll talk about that in just a little bit.  First, though, oy oy oy.  Picture of the Week time.



STEVE:  So, yes.  I gave this snapshot of Google's brilliant AI Overview the caption:  "Don't you just hate it when the cheese slides off the pizza?"



LEO:  Apparently some people do.



STEVE:  That's a real problem, you know, Leo.



LEO:  Yeah.



STEVE:  On the space station, you know.  Fortunately, Google AI Overview has the obvious answer.  And again, it's, you know, why, you know, of course this is what it would suggest.  So the question that prompted this was "Cheese not sticking to pizza."  AI Overview jumps in and says:  "Cheese can slide off pizza for a number of reasons, including too much sauce, too much cheese, or thickened sauce.  Here are some things you can try:  Mix in sauce.  Mixing cheese into the sauce helps add moisture to the cheese and dry out the sauce.  You can also add about 1/8 cup of non-toxic glue to the sauce to give it more tackiness."



LEO:  Mmm, yum-yum.



STEVE:  Oh, that's right.  Yes.  And it's not, like, some.  It was like last week we had two quarts of urine.  You were supposed - not to drink urine, but how much you should be consuming.  And here, you know, you do not - Leo, you don't want to overdo the glue.



LEO:  Nn-nnn, never.



STEVE:  So it's only 1/8 of a cup.



LEO:  You actually could do this with non-toxic Elmer's Glue or something like that.  You could do it.



STEVE:  Absolutely.



LEO:  Not sure I would recommend it.



STEVE:  You know, kindergartners are eating that stuff, so what the heck.



LEO:  Yeah, sure.



STEVE:  Okay.  So it turns out that this latest AI hallucination is based upon an eight-year-old Reddit posting, which was posted as a joke on Reddit.  And of course all of the other humans there knew that it was a joke and thought that was kind of funny.  The guy wrote:  "To get the cheese to stick, I recommend mixing about 1/8 cup of Elmer's glue in with the sauce.  It'll give the source a little extra tackiness, and your cheese-sliding issue will go away.  It'll also add a little unique flavor.  I like Elmer's school glue, but any glue will work as long as it's non-toxic."



Now, of course, Google's bot came along and scraped that up and thought, hey.



LEO:  Didn't know any better, yeah.  



STEVE:  There's an idea.



LEO:  Yeah.



STEVE:  Let's hold onto that until some asks why their cheese is not adhering to the bread.  In which case we'll give them a little handy help there.  Okay.  So, okay.  We're having some fun at Google's expense.  But this seems like a large can of worms which Google's new AI Overview would probably tell you to eat.  NBC News picked up on this trouble and offered some additional depth that I want to share since it's really not that funny.  Their headline under "Artificial Intelligence" was:  "Glue on pizza?  Two-footed elephants?  Google's AI faces social media mockery."  And then they said, as their subhead:  "A Google spokesperson said the company believes users are posting responses to uncommon questions on social media."



Okay.  So NBC said:  "Social media has been buzzing with examples of Google's new, 'experimental'" - and they have that in air quotes - "artificial intelligence tool going awry.  The feature, which writes an AI Overview response to user queries based on sources pulled from around the web, has been placed at the top of some search results.  But repeatedly, social media posts show that the tool is delivering wrong or misleading results.  An NBC News review of answers provided by the tool showed that it sometimes displays false information in response to simple queries.



"NBC News was easily able to reproduce several results highlighted in viral posts online, and found other original examples in which Google's AI tool provided incorrect information.  For example, an NBC News search for 'How many feet does an elephant have' resulted in a Google AI Overview answer that said 'Elephants have two feet, with five toes on the front feet and four on the back feet.'"



LEO:  Oh, my god.



STEVE:  "Some of the false answers verged into politically incorrect territory.  An NBC News search for 'How many Muslim presidents in the U.S.,' the results of which were first posted on social media, returned a Google AI Overview that said 'Barack Hussein Obama is considered the first Muslim president of the United States.'"



LEO:  What?



STEVE:  "Obama, however, is a Christian."



LEO:  Oh,  yeah.



STEVE:  "Google said this Overview example violated its policies and that it would be 'taking action.'"  I know what action they should take, Leo, and it's not the one they're going to take, apparently.



"A Google spokesperson said in a statement" - okay, so this is Google.  "The examples we've seen are generally very uncommon queries, and are not representative of most people's experience using Search.  The vast majority of AI Overviews provide high-quality information, with links to dig deeper on the web.  We conducted extensive testing" - you know, we spared no expense.  "We conducted extensive testing before launching this new experience to ensure AI Overviews meet our high bar for quality.  Where there have been violations of our policies, we've taken action; and we're also using these isolated examples as we continue to refine our systems overall."



NBC writes:  "It's difficult to assess how often false answers are being served to users.  The responses are constantly shifting, and on social media it's difficult to tell what is real or fake.  Some Google users have created workarounds to avoid the new AI Overview feature altogether.  Ernie Smith, a writer and journalist, quickly built a website that reroutes Google searches through its historical Web results function, which avoids the AI Overview or other information boxes that prioritize some results over others.  Adding 'udm=14' to Google search URLs strips the new feature from results.  Smith told NBC News that his new website has quickly gained traction on social media, surpassing the traffic of his entire decade-old blog in just one day.  Smith said in a phone interview:  'I think people are generally frustrated with the experience of Google right now.  In general, the average person doesn't feel like they have a lot of agency.'



"A Google spokesperson said the company believes users are deliberately attempting to trip up the technology with uncommon questions.  Some deeper dives into why the answers have gone awry suggest that the tool is pulling from surprising sources.  404 Media reported that a Google search query for" - and here it is - "cheese not sticking to pizza pulled an 11-year-old Reddit comment that jokingly suggested mixing Elmer's Glue into the sauce.  Even though Google has now removed the AI suggestion from searches for 'cheese not sticking to pizza,' according to an NBC News search, the top result is still the Reddit post, with the comment about Elmer's Glue highlighted.



"A Google spokesperson quoted that queries like 'cheese not sticking to pizza' are not searched very often, and are only being noticed because of the viral posts about wrong answers on social media platforms like X, of which there are many.  The same issue with an old Reddit comment also occurred in a search for 'how to rotate text in ms paint,' referring to the Microsoft Paint application.  The top Google search result, viewed by NBC News, directs the reader to a sarcastic Reddit comment that says to press the 'Flubblegorp' key on your keyboard."  They note:  "This key does not exist.  This example was originally posted on social media.



"Despite Google's assertion that the tool is working well for many users, mistakes of the AI Overview are continuing to gain visibility and hype.  Some of the answers that have been posted online seem to be fake, indicating that the trend has shifted from authentic errors to a new meme format."



Okay.  So a couple of comments.  First is, I think it's clear that it's wise to be extremely skeptical now about anything we see online in general these days, and not only Google AI Overview results that we receive, you know, but just as much any reports of bizarre and wonderfully wrong results.  Every time I've encountered one of these reports like I've been sharing, I've immediately worked to verify its authenticity as much as I can since there's clearly some strong motivation to invent non-existent high-profile, you know, funny failures.  But, you know, here's NBC News, who themselves searched for how many feet does an elephant have and was told "two."



My other observation is that I hope Google truly understands that there are two fundamental reasons why they're getting into trouble with AI Overview.  The first reason is how powerful and potent this would be if it were possible.  It would be truly amazing.  But that's coupled with the second reason, which is that what they are attempting to do is not even remotely possible - not yet, not today, not even close.



You know, I make no claims to being an AI expert.  But we've all been paying attention, and our intelligence is not artificial.  We know that the current level of AI development definitely falls short of comprehension.  These large language models, exactly as their name suggests, are capable of mimicking the output of an intelligent species whose actual intelligent output was used to train them.  But as we're finding out, there's a world of difference between seeming and sounding intelligent, and actually being intelligent.



So here's the problem.  Google is attempting to use automation to create an accurate factual summary overview of what the web contains without understanding the content that it's summarizing.  It should be clear to everyone that this can never work.  It is not possible to create an accurate summary of content for which there is no comprehension.  AI Overview doesn't "know" that glue should not be mixed with tomato sauce because AI Overview doesn't actually "know" anything at all.  Yet to do the job Google has given it, it must comprehend the content that it's accessing.



What Google appears to have completely missed here is somewhat astonishing, I think.  You know, it's that the job of displaying pages of links resulting from keyword matches is entirely different from attempting to extract truth and knowledge from the content behind those links.  Keyword matching and link ranking they know how to do.  Truth and knowledge extraction no one knows how to do.  Not yet.  Not today.  But unfortunately, that has not stopped Google, and it should have.



Okay.  And this brings us to the perfectly named website udm14.com, which our prolific Twitter poster, Simon Zerafa, tweeted to me.  Thank you, Simon.  Recall that when the string "udm=14" is included, you know, appended to a Google search query, it serves as a shorthand, asking Google to return its search results in what they term "web search mode."  Among many other things, their AI Overview system is not consulted in that case.  From that page at udm14.com I discovered another site named TenBlueLinks.org.  And of course 10 blue links is reminiscent of what Google was, you know, decades ago, back when I first discovered it and sent that second email from GRC's first email system out back then, when no one had ever heard of Google.  Ten blue links was what you got on the page.



So TenBlueLinks.org, which, with just a few clicks of the mouse, allowed me to instantly and as permanently as I want switch my default Google search to "Google Web" mode search in Firefox.  For example, for Firefox on Windows or MacOS the instructions are just, you visit TenBlueLinks.org.  Then you right-click in the address bar, and you get a dropdown menu which is enhanced by this site.  At the bottom it says "Add Google Web."  So you click on that.  Then you open the hamburger menu in the top right corner, choose "Settings," and then click on "Search" on the left.  And then in the "Default Search Engine" you will now have a new entry, "Google Web," which you then select, and you're done.  Now Firefox will use, by default, until you change it, this Google Web mode search for all your browser searches.



You know, and when I first read the instructions I thought, what?  But sure enough.  You just go to this website, and it adds this cool option in the dropdown menu from right-clicking in the address bar, the URL field, and then allows you to make it your default.  They've got instructions for Chrome on Android, Chrome on iOS, Chrome on Windows or Mac, and Firefox on Windows and MacOS.  So anyway, again, I commend our listeners:  TenBlueLinks.org.  It's a very cool site.



LEO:  You don't actually get the Overview on your Google search; do you?



STEVE:  Actually, I don't think I've ever seen one yet.



LEO:  Yeah.  So that's - what happened was, as part of Google's Experimental Labs, you could turn it on there.  And then briefly they made it default.  And that's when they got in all the trouble.  It's turned off now, I believe.  



STEVE:  Oh, no kidding. 



LEO:  I see it's still in Labs.  Yeah, I don't think anybody's actually getting it anymore.



STEVE:  So they backed out.



LEO:  Oh, almost immediately.



STEVE:  Of the full...



LEO:  Yeah, yeah.  They said, oh, whoops.  But Google keeps doing that.  It's amazing.  I don't know why.  It's amazing.  It just keeps happening to them.  But I don't think anybody's getting that now, unless you turn it on specifically.



STEVE:  Interesting.  So...



LEO:  I don't know if udm14 is different than that.  I think it is.  Because then I think you also don't get the knowledge graph...



STEVE:  Correct.



LEO:  ...and the suggested links and all that stuff.  So it's still worth doing that, yeah.



STEVE:  Correct.  And all of the image search stuff and all the other junk, yeah.  Yeah, well, yeah.  So the udm14, which is to say invoking the Google Web mode search one way or the other, it definitely cleans that up and suppresses all that.



LEO:  Yeah, yeah.  I stopped using Google Search a year ago so none of this affected me.  But I use Kagi.  I pay for it because I don't want ads.  And I think it's compromising.  It's terrible.



STEVE:  Yup.  Well, it has compromised Google Search; right?



LEO:  Really.



STEVE:  I mean, it's completely skewed what they return.



LEO:  Mm-hmm.  Yeah.



STEVE:  Okay.  So a piece in BleepingComputer caught my eye, mostly because of how pathetic the announcement seemed.  BleepingComputer's headline was "LastPass is now encrypting URLs in password vaults for better security."  To which I respond, gee, what a great idea.



LEO:  Finally.  Wow.



STEVE:  BleepingComputer wrote:  "LastPass announced it will start encrypting" - maybe not even quite yet, but it will - "start encrypting URLs stored in user vaults for enhanced privacy and protection against data breaches and unauthorized access.  The vendor of the popular password manager also notes that this new security feature is a significant step towards reinforcing its commitment to implementing zero-knowledge architecture in the product, so it's not just to protect data from external threats.



"LastPass says that due to restrictions in processing power in 2008, when that system was created, its engineers decided to leave those URLs unencrypted, lessening the strain on CPUs and minimizing the software's energy consumption footprint."  That's right.  It was good for the planet, everybody.  What a crock of you-know-what.  But let me finish just another two lines from BleepingComputer's piece.  They said:  "With most of the hardware performance constraints of the past now having been lifted, LastPass can now start encrypting and decrypting those URL values on the fly without the user noticing any hiccups in browser performance while enjoying ultimate data security.  LastPass says this is being done to enhance user security and comply with the company's zero-knowledge architecture."  So I don't know where to...



LEO:  Everybody else does this; right?  I mean, I know Bitwarden does.  I think everybody does.



STEVE:  Everybody.  Everybody else.  So, you know, okay.  It's true that the world was very different back in 2008 when Joe Siegrist designed the original LastPass architecture.  And I would believe that since the URLs the user was visiting were needed for on-the-fly matching, and since their privacy - again, back in 2008 - didn't seem like a big issue, Joe would have consciously and deliberately chosen not to keep them encrypted, especially given that everything else in there was.  You know, so his not encrypting the URLs at the time was obviously intentional.  But that was 16 years ago.



LEO:  Yeah.



STEVE:  Sixteen years ago.  And the flow of time really does impact what we would term "best practice" today.  Back then, most web sessions were only briefly encrypted during login, after which the connections dropped back to plain old HTTP.  And as we know from Firesheep, the now logged-in session cookies were completely exposed to the Internet, allowing those sessions to be impersonated easily.  That would no longer be considered "best practice" today, and no one does that anymore.  So as times change, what's considered reasonable changes along with it.







But computers have been plenty powerful for the past decade at least to handle on-the-fly URL decryption without introducing any discernible pause or overhead.  Back when we were talking about this, I noted that it would have been possible to keep the user's vault encrypted on disk and only decrypt it in RAM.  That was the decryption event that would have been one time only during browser launch, when the extension was coming to life.  It would have decrypted the on-disk storage into RAM, where it could then access it easily on the fly.  But what was actually stored in the computer and was available potentially to be stolen would have been kept fully encrypted.  So there have been ways to offer vault encryption at rest, without any problem, for a long time.



I suspect that the real problem is - and we talked about this at the time.  LastPass's parent, LogMeIn, was purchased by a purely financial private equity firm back in 2019 for, what, four point some billion dollars.  I mean, a ton of money.  And that new parent did not love it for anything more than the cash flow it could produce.  In any event, for anyone who may still be lingering with LastPass, I just wanted to note that, for what it's worth, your vault-stored URLs on your machine will now finally be encrypted at rest.  So good on you.  And Leo, let's take a break.



LEO:  Good on you, yes.



STEVE:  I'm going to share a piece of feedback that will lead on to our next bit of news.



LEO:  All right.  I don't think we're ever going to get any data brokers advertising on this show.  I'm just guessing.  I'm just guessing.  All right.



STEVE:  Yeah, that would be a difficult...



LEO:  I wouldn't do it.  I wouldn't do it.



STEVE:  So I'm going to take one piece of listener feedback out of sequence, ahead of the pack, because of the P.S. that Andrew included, which I'll get to in a second.  So this is from Andrew Gottschling, who said:  "Hey, Steve.  I wanted to provide some feedback to Hakku's comment on VPNs and Firewalls.  It's probably not an option for many, if not most, corporate users.  This is because many corporations these days, and all of the ones I've worked for thus far, utilize 'split tunneling' on their VPNs to reduce bandwidth usage for high bitrate communications that are common, for example, voice and video calling on Teams or Slack.  Therefore, simply blocking all traffic from leaving on anything other than the VPN interface, unless it's to the VPN concentrator, would not be feasible in these cases, especially in the case of something like Slack, which runs in AWS, and their IP range is very dynamic.  Love the show.  Thanks for all you do.  Andrew."



So Andrew's exactly right, and this could be a problem with any VPN that insists upon forcing all of the system's traffic through its tunnel, and its tunnel alone.  The problem we're running into sort of more broadly is that we're tending to use the term "VPN" generically.  Like there's only one sort of VPN, you know, a VPN only does one thing, as if they're all created equal.  But that's not the case.



For example, the VPN that a typical roaming consumer in an Internet-equipped caf, airport, or hotel might want installed on their laptop would be a VPN that proactively refuses to allow any packet traffic in or out of that machine that does not travel through its tunnel.  What such a consumer will want is full protection.  This is contrasted against, for example, an IT-managed enterprise setting where a great deal of attention has been paid to exactly which traffic flows where.



For example, headquarters might have several satellite offices located elsewhere in the world which need to participate on the same corporate network, as if they were, you know, attached.  And since that traffic cannot safely be exposed to the Internet, static VPN tunnels would be established to securely interlink the satellite offices no matter where they were.  In this case, only the traffic that's bound for network addresses at the other end of a VPN tunnel would be routed there, with all the other local traffic allowed to have contact with the Internet directly.



So, you know, these are all just differing applications for private virtual networks where, you know, that's sort of a generic umbrella term.  The common factor is that traffic is being encrypted and decrypted as it flows between one or more local and remote IP addresses.  And part of what's so cool about VPNs is that they are, you know, V stands for Virtual.  They really are a virtualizing technology that is very flexible and very powerful.



Now, as I said, I chose Andrew's note because it arrived, Leo, via email, addressed to "securitynow@grc.com."  And Andrew ended his note with a P.S., which read:  "P.S.:  This new email system is REAL [all caps] slick.  Glad to get rid of Twitter."



LEO:  You have set something up, haven't you.



STEVE:  What email system, you ask?  Well, since you asked, it's GRC's new email system.



LEO:  Ah, excellent.



STEVE:  I finally have the long-awaited email announcement for GRC which features for this podcast a simple means for our listeners to send feedback and thoughts to me through spam-proofed email.  As I mentioned last week, GRC's been without any form of subscription email news system since I shut down the first system, which I wrote 25 years ago, back in 1999.



The completion of SpinRite v6.1 created my need to announce it to 20 years' worth of SpinRite 6 owners, and it would be nice to be able to send news of new things I create to those who would like to know of them.  For example, I do have plans to revisit ValiDrive, which has turned out to be extremely popular.  I've got a list of things I want to do for ValiDrive 2.0 that'll just be a little quickie update, but very useful.  And of course GRC's DNS Benchmark, which continues to be the most popular download we have, could use a bit of attention as DNS servers come and go.  So that's on the side of sending email out.  What about receiving feedback from our listeners?



Just yesterday, I received a very useful DM tweet from someone who said he created a Twitter account just so he could send me that tweet.  And as we know, many of our listeners have had to do so.  On the one hand, I'm deeply honored that our listeners are as interested in engaging as so many are.  I'm blown away by that.  But on the other hand, I'm horrified that the bar has been set so high by the need to join any social media service just to send me some thoughts, or a link to something that might be of interest to our listeners, especially when everyone already has email.  Email is the obvious common denominator.



Now, before I go on, just for the record, allow me to reiterate one last time, because I know there are still some people who need to hear this:  This has absolutely nothing whatsoever to do with Elon Musk's ownership of Twitter.  Really.  Nothing.  I could care less.  For one thing, I am barely a Twitter user.  When I start working on each week's podcast, I check in with Twitter to collect all of the tweets I've received since my previous check-in the week before.  I don't even look at it during the week.  I scan through those, replying when I can, and that's been where our listener feedback has mostly come from every week.  As everyone knows, I've never followed anyone on Twitter.  So I've never used it the way it was intended to be used.  As a consequence, I'm not directly aware, you know, of what may have changed after Elon's reluctant purchase of Twitter, other than things I've heard secondhand.  So I could care less.  I just want to lower the bar for all of our listeners.  And everyone has email.  The normal downside of asking people to share their email addresses is that the implied trust might be abused.  I think everyone knows that will never happen with me.



Until this past weekend I've not had a workable means for receiving incoming email from our listeners.  Now I do.  GRC now has the subscription management front-end of its new email system up and running.  It's what I've been developing for the past few weeks, and of course it's all written in assembler because that's just where I'm most comfortable.  It's now possible for anyone who wishes to, to optionally subscribe to any one or more of our three mailing lists.  One is aimed at our commercial product owners; one is aimed at general GRC news of products, freeware, services, et cetera; and one is intended for this Security Now! podcast, which has become a significant part of my life through these past 20 years.



But, and this is crucial:  You do not need to be subscribed to any of these lists to be able to send email to securitynow@grc.com.  There's no requirement for anyone to subscribe, although of course everyone's welcome to if they wish.  Here's the requirement:  The email address from which you are sending email to me does need to be known to the system.  So here's how you register:  At the top of every GRC page, in the page's header, is a little white envelope with an "Email Subscriptions" link.  There's also a link under the Home menu. And, as you might expect, it's also just grc.com/mail.



So you go to grc.com/mail and enter the email address you wish to register with GRC.  GRC will send an email to that address containing a link back to your own subscriptions page here.  And as you'd expect, everything defaults to "unsubscribed."  I don't ever want to send anyone any email they don't want to receive.  But if you wish, you can optionally provide your name and join any of the three lists shown there.  Then, either way, click the "Update Subscriptions" button, and your confirmed email will then be known to GRC.



From that point forward you can simply address anything you wish to the email address securitynow@grc.com.  No exclamation point or hyphen or anything, just securitynow@grc.com.  When that email arrives at GRC's server, the sender's address will be looked up; and if it's known to the system, the email will be accepted and will appear in my SecurityNow account inbox.  If email you send to securitynow@grc.com is rejected and bounces back to you as undeliverable, you'll know that something went wrong somewhere.



So that's the front end system.  The back end is the part that contains the subscriber database and actually sends email to the lists.  I should mention that at this exact moment, due to a limitation that the back end had, this new system is unable to accept email addresses containing plus signs, which I'm sure our listeners would like to use.  The back end has been fixed, but I haven't updated my code yet because it just happened yesterday, and I haven't, you know, I've been working on the podcast.  So that'll be the first thing I do later today.



And as for the back end, all I have running and tested at this point is the subscription management.  So please do not be surprised when you don't immediately start receiving email from me.  It's not you, it's me.  Since the industry has become so spam-sensitive, I plan to proceed with caution to be very sure that any bulk email I send meets all of today's anti-spam technical and legal requirements, and there are many.  So it will likely be another week or two before email begins to flow.  While I hope to be able to send weekly podcast summaries and links, the other two lists will always be very, very low volume.  I think over the eight-year life of the previous email system, I sent a total of 11 pieces of email.  So, you know, no one's going to get spammed.  You'll be wondering what's going on.  If you weren't listening to the podcast, you would wonder where I went.



But today, the new incoming email system filter is in place.  And frankly, I have no idea what will become of my use of Twitter.  It's trivial for me to tweet the weekly summary of the podcast, you know, a link to the show notes and the Picture of the Week.  My ambition is to deliver the same thing via email, but I'll be doing that somewhat cautiously as we see how it goes.  And I should note that I have recently noticed a significant uptick in spam to my, Leo, as you always mention, my  Open DM channel.  You know, I got one this morning:  "Hello.  My sister saw your profile while browsing X on my phone, and she's interested in you."  And then I have a link: e.yqyh571.xyz.



LEO:  Oh, I know them.  They're great, yeah.



STEVE:  Yeah, that's right.  "Open the link to complete the registration, and she will take the initiative to call you."  And I've got four different emojis.  And by the way, the emojis differ every time I receive this, although the text is always the same.  And it says:  "Remember to say hello to her.  She's very shy."  Right.  And she's also going to be very lonely in this case.  So anyway, if the spam becomes a lot worse, I'll likely be forced to abandon open DMs.  So the establishment of this alternative channel is coming at an opportune time.  The bottom line is I'm very excited to finally be adding this long-missing piece of GRC's infrastructure.  It's been crazy that we've had no means of announcing new stuff.  And once the dust settles from that, I'll begin sending out the news of SpinRite 6.1 to all 6.0 owners.  So, very pleased.



Okay.  So some closing-the-loop feedback.  A listener, Hatcher Blair, said:  "Hi, Steve and Leo."  He described himself as a "medium-time listener" and huge fan of the work we do.



LEO:  Medium is good.  I'll take medium.  I'm happy with medium.  That's fine.



STEVE:  A medium-time listener, exactly.  So that's what, he came around maybe 10 years ago, something like that?



LEO:  Yeah, something like that.



STEVE:  Jumped in about halfway along, yeah.  So he said:  "I hope this is still the appropriate place to contact you as I made a Twitter account just for this."  So bless you, Hatcher, in the future you can send email to securitynow@grc.com.  Anyway, he said:  "I just listened to SN-975, and I wanted to thank you for alerting me to the Web Search option in Google."  So again, and I'm glad you pointed this out, Leo, not just to remove AI Overview, but to clean up the pages significantly.



He said:  "I wanted to make it my default search option, but you cannot add the search engine to Chrome or edit the Google Search engine in Chrome's settings.  However, you can create an extension which adds a Web Search engine and make it the default.  I made a simple Chrome extension that makes Web Search the default option when searching from the address bar.  This extension is not and cannot be published on the Chrome web store because I use the domain https://google.com and would need to have ownership of that domain to publish the extension.  Although it's not on the web store, it is on my GitHub for anyone that wants to clone the repo and install it for themselves.



"A warning to anyone who wants to install the extension."  He said:  "It is bad."  I don't know what he means.  He says:  "All the extension does is make the default search," you know, and then he shows a search query with the &amp;udm=14.  He said:  "There is no localization support or option to enable or disable the extension in the UI.  If you end up sharing this on the show" - here I am, doing that - "feel free to share the repo, and anyone who wants to contribute is welcome to.  Anyone is also welcome to use anything on the repo for their own purposes if wanted.



"I did a little bit of googling, and making a similar extension should be possible in Firefox.  It might even be easier as Mozilla seems to have much better documentation than Google.  Keep up the great work and looking forward to episodes 999 through infinity," says Hatcher Blair.  So I've got a link to his GitHub repo in the show notes.  And just another piece of work along the lines of the TenBlueLinks.org that we talked about earlier, you know, this one from one of our listeners.  And as I said, I'm sure this will be very popular moving forward.



Defensive Computing's Michael Horowitz wrote to say:  "Steve, a fun story.  I recently got a fairly standard scam email message claiming my computer had been hacked and asking for Bitcoin.  As proof of the hack, the bad guy told me my password.  But I use a different password everywhere.  Have for years and years.  So the revealed password told me the service that had been hacked, and I logged onto it and changed that one password.  It had a stored credit card, but fortunately that had expired."  He says:  "It's rare to actually experience, firsthand, up close and personal, the benefit of never re-using a password."



So thanks for sharing, Michael.  I think that's very cool.  As I've been perusing the email domains of our listeners who have been subscribing to GRC's new service since I announced the email system on Twitter yesterday, I've seen many gmail.com email domains; but also, as indicative of the listeners we have, many personal domains.  As we've discussed, unfortunately,  there's no good way to hide from tracking when websites are willing to trade their visitors' privacy for cash by colluding with advertisers and other data aggregators.  Not even a personal domain will help with that.  But it can be very useful for tracking down personal information leakage.



I established a unique email address for the dealership that services my car.  So when I started receiving unwanted spam from some auto-related source, from that one email address that I had never shared with anybody else, I knew who had leaked it.  So, yeah.  Even though it won't help for tracking, it is a little bit satisfying just to be able to say, uh-huh, I got you.  And of course I'm then able to retire that email address if the spam becomes annoying.  And when they wonder why they're unable, when the dealership can't send me email, I say, oh, yeah, I changed that because you guys sold my address to a third-party commercial entity.  So here's the new one.  And I imagine, Leo, that you at Leoville.com...



LEO:  Oh, it's unusable, yeah.



STEVE:  Oh, yeah, Leo is unusable.  So imagine...



LEO:  Well, also, oh, no, I don't even use that.  I mean, I do, but I don't.  I have lots of solutions around this.  



STEVE:  Right.



LEO:  Similar to yours, but not.



STEVE:  Right.



LEO:  You have to.  I have many addresses.  I can't use laporte@gmail anymore.  That really went downhill fast.  But no, what I do now when I sign up for something is I use - I actually don't use those unique passwords, unique email addresses that Bitwarden and Fastmail do.  I just make it the name of the company at a particular address that I haven't used before, you know, that I use exclusively for that.



STEVE:  And you have a catchall so that everything comes in?



LEO:  Yeah, right.  I don't have to worry about that in Fastmail.  I have about 10 domains or 15 domains that get email at Fastmail.  It all goes in the same inbox.  And then I can do sorting based on the address it thinks it's going to and stuff like that.  It's, look, spam is a mess.  It's just a mess out there.



STEVE:  Oh, Leo.



LEO:  It's terrible.



STEVE:  Yeah.  Well, and as somebody facing the task of sending subscription email, you know, I mean, I'm...



LEO:  Well, yeah, yeah.  That's the biggest issue.  It's not that we're getting a lot of spam.  It's just really hard to send email out now.  Google will not accept email if it doesn't have  DKIM, SPF, and DMARC authentication.  They just won't even accept it.  So it's gotten really - that's where it's really, it's much harder.  Deliverability has gone downhill.



STEVE:  Yes, yes.  And in fact in my instructions, as does everyone, I say, if you don't receive the confirmation email, you know, look around for it.  You know, check your spam folder or wherever it might have gone.



LEO:  Yeah, that's a must also, yeah.



STEVE:  Yeah.



LEO:  I think Google loves this because it means that eventually they hope everyone in the world will use Gmail.  And that'll solve it.  Sort of.



STEVE:  Oh, you mean from Gmail to Gmail.



LEO:  Yes.



STEVE:  Wonderful.



LEO:  That solves it.  You know, as long as everybody's using Gmail, we can get rid of spam.  We can authenticate.  It's just it's our fault for not using Gmail.  That's Google's attitude.



STEVE:  Right.  That's right.



LEO:  That's Google's attitude.



STEVE:  So Elliot.Alderson tweeted:  "Hey, Steve.  One extra way to avoid Google's AI Search."  He says:  "Don't sign in."  He says:  "I've never seen any of that AI nonsense.  I have a different browser profile for signing into Google, and I clear it whenever I'm done with whatever Google account management I need to do."



Huh.  Okay.  So I can't speak to that myself.  I don't do Google account management.  And like you, Leo, I've never run across that.  And now we learn from you that it's gone.



LEO:  You won't.  Yeah, it's gone.  You won't anymore.  It's still crappy search, but at least you won't see an AI Overview.



STEVE:  Yes, exactly, telling you to eat glue.  Steve Murray said:  "Steve, just FYI, you can replace soldered motherboard components like eMMC/RAM."  He says:  "A ton of YouTube videos cover it."  He says:  "The hard part is doing it in an economically viable way if not doing it DIY."  Okay.  Now, I would argue...



LEO:  Just because it's on YouTube doesn't mean it's possible.



STEVE:  I would argue that, right, that the hard part is doing it at all.  I've been soldering electronics, literally, I'm not kidding, since I was four years old.  I still recall my dad's big honking soldering iron.  It was about 3/4 of an inch in diameter and 18 inches long, with a wooden handle.  It was nothing like what we have today.  This thing took about 30 minutes to come up to temperature, at which point you could push its tip through a solid steel plate.  And while growing up, my standard Christmas present was a Heathkit, which I would receive every Christmas Eve.



LEO:  Aww.  That's so cool.  Aww.



STEVE:  I would open it on Christmas Eve, and it would be fully assembled by Christmas morning...



LEO:  That's so cool.



STEVE:  Since I had no interest in sleeping with an unfinished kit in front of me.



LEO:  No wonder you're a geek.  Now I get it.



STEVE:  Oh, and I remember, Leo, the VTVM, the Vacuum Tube Volt Meter that I built, I must have, let's see, I was still living on Overhill in Orinda, and I left Sleepy Hollow Elementary in the middle of the fifth grade.  So I was, what, eight, I guess?



LEO:  Wow.  Wow.



STEVE:  And Dad - one Christmas was a shortwave radio receiver.



LEO:  Holy cow.



STEVE:  One Christmas was, you know, all Heathkits.



LEO:  Was he an engineer?  Is that why he really encouraged this?



STEVE:  Yeah, he was - he had his masters in engineering from Berkeley.  But mechanical engineering, not electrical engineering.  But there's a lot of overlap.



LEO:  Right.  Sure.



STEVE:  And, you know, he set me up with a battery and knife switches and light bulbs because I just had to understand how all this stuff worked.



LEO:  That's so great.



STEVE:  Anyway, so, while, yes, technically I agree that it's possible to remove and replace today's modern high-density surface mount components, doing so - because I have - is neither fun nor easy.



LEO:  And it's really easy to screw up the substrate, the motherboard; right?



STEVE:  Oh, so easy.



LEO:  Yeah.



STEVE:  Especially when they're surrounded on all four sides with a forest of tiny pins on half-mil centers, or when it's a BGA, which is a ball grid array chip, with its myriad connections underneath the chip itself.  I'm sure anybody who's looked at a modern motherboard or circuit board, you see this chip sitting there with no obvious connections to it.



LEO:  Right, right.



STEVE:  It's because they're little dimples on the underside of the chip.  You've got to heat the whole thing up in order to melt the solder of them all at once in order to pull this thing off the circuit board.  Anyway, the reason there are a ton of YouTube videos is that it's actually not possible to do.



LEO:  It makes for an excellent YouTube video. 



STEVE:  It'll hold you on the edge of your chair.  Is it going to come off?  Is it going to come off?



LEO:  You know, just because you see somebody doing it on YouTube does not mean you can do it, or anyone normal or even that they...



STEVE:  Yes.  Hank is able to show me how to create a recipe which I am unable to reproduce.



LEO:  Yeah, exactly, yeah.



STEVE:  So the fact that Hank is able to do it does not mean that I can.



LEO:  My son, yes.  No, I watch with wonder myself.



STEVE:  Sylvester said:  "Replying to @firefox," he said, "@SGgrc Vertical tabs are coming!"  So his tweet sent me off looking, and I found a posting by Martin Brinkmann over at gHacks.net.  Martin wrote:  "Mozilla released a Firefox Nightly test build recently that includes support natively for vertical tabs.  This new functionality is not available in regular Firefox Nightly builds, but there is a way to get that build and test it for yourself.  Native vertical tabs support is a highly requested feature.  It is placed third currently on Mozilla's Connect website, just behind native tab grouping, and the restoration of Progressive Web App support in Firefox.



"Vertical tabs," he says, "move tabs from a horizontal bar at the top of the browser to the side.  It enables better drag and drop support, sorting, hierarchical views, and better use of space on widescreen monitors or sites that limit their width.  Firefox would not be the first browser to support vertical tabs.  Several browsers, including Microsoft Edge, Brave, or Vivaldi, support vertical tabs already," and he says, "with Vivaldi taking the cake when it comes to customization options.  There has always been talk of introducing vertical tabs in Firefox.  The last time was in February of 2022, when Mozilla looked into the matter."



Okay.  So I don't understand this.  Vertical tabs are such an obvious improvement for modern web browsing that it is difficult for me to understand what's taken so long.  Fortunately, I've had vertical tabs in Firefox thanks to the browser's sidebar that can be used to contain browser tabs.  I use the add-on Tree Style Tabs, which works wonderfully.  And then I tweaked the browser's UI, the CSS style sheet, to hide the tabs across the top, which are still there.  So although I've found a solution to place tabs to the side, where they should have been immediately moved once our screens moved away from their original 4:3 aspect ratio, it will be wonderful for Firefox to offer them natively.  So let's hope that happens because, like, what's the problem?  It's just so obvious.



LEO:  You raise an excellent point.  We have wide, wide screens.  There's lots of room.



STEVE:  Yes.



LEO:  On the left.  I hadn't really thought about it that way.  Yeah.



STEVE:  Yes, exactly.  And many sites, in fact, my site looks weird because it sets the width to 85% of the browser window, which on today's browsers is wrong.  So as I'm rewriting pages, I'm changing the way that works.  As a consequence, there's a lot of empty space on the sides.  So anyway.



LEO:  Yeah, yeah.  I put my dock on the left for that reason on my Mac, yeah.



STEVE:  Right.  And we're an hour in, Leo.  Let's take our third break.



LEO:  Let's go to work.  Let me do some work while you relax.  It's my turn.



STEVE:  I've got coffee.



LEO:  The world's largest mug of coffee.  Hey, you're going to like - whoa.  Steve?



STEVE:  So Tal in Israel, he said:  "Back at Episode 970 you read a listener's feedback about a SOHO router that requires you to press a button on the router in order for configuration changes to be applied."  We've talked about this is now recommended behavior for future routers, in order to minimize the ability of attackers to do a purely electronic, non-present change of configurations.  Anyway, he said, speaking of the listener feedback, said that "A well-known router manufacturer named FRITZ!Box has been creating such routers, where configuration changes require the press of a button on the router.



"I've been looking for a new router, as my old Xiaomi router stopped receiving updates in 2021.  Xiaomi is notoriously known for not providing many updates to devices after they've been sold.  Also, that router was always underpowered, dropping WiFi connections and being generally unable to handle my needs.  I remembered the name FRITZ!Box, looking around a little, and it seems the company who manufactures them is very security-aware, and the performance of them is very good.  I was happy to discover an Israeli seller, and bought the FRITZ!Box 5530, which seems to be what is most suitable for me.



"After I've been using it, I think it's the best router I've ever had.  It does not even break a sweat with multiple video streaming and downloading and anything else I do.  And I think it can serve as an example of how SoHo routers should be.  First, it comes with automatic updates turned on by default.  Second, both wireless key and router admin passwords are randomized when you get one.  And if you reset it to factory default, those passwords will be reverted back.  There's a very durable sticker on the bottom of the router with them so you should not worry about losing them.



"Changing some configurations like DNS will require you to go and press a button on the router.  But since it can also serve as a telephony hub, if you have a phone directly connected to it you can pick it up and dial some number it tells you to dial in order to apply the configurations.  Or you can define an authenticator app and then use the six-digit token to apply changes.  Other nice things it supports" - oh, by the way, that's a cool feature, right, because now you've got a way where you don't have to physically be present, but you do have another authentication token that is changing dynamically to prevent someone from making changes electronically.  That's something we hadn't talked about.  That's brilliant.



And he says:  "Fourth, other nice things it supports is DNS over TLS so your ISP will know nothing of your DNS queries."  And he says:  "I use both Google and Cloudflare OpenDNS resolvers, which I trust way more than my ISP provider.  And finally, fifth, FRITZ!Box is well known for supporting their devices for a long time."  He finishes:  "It has many other features where you can definitely see that security awareness went into the design.  So whoever mentions FRITZ!Box in Episode 970, thank you.  Unfortunately," he said, "I could not find your name in the transcripts."



Well, okay.  Since this is all about listener feedback, I wanted to keep this thread alive by sharing this listener's very positive experience with FRITZ!Box.  I brought up a site web search of GRC some time ago, so I went to GRC and put "FRITZ!Box" into the search field at the upper right of every page.  That brought up all of our many mentions of FRITZ!Box through the years, as well as some comments over in GRC's forums.  The listener who tweeted the news to us in Episode 970 used the Twitter handle "ndom91."  So we still don't know his name or who he or she is, but thank you again for the mention.  And thank you, Tal, for sharing your impressions.



I went over and looked at their lineup.  It's a German company.  They've got, fortunately, an English language website.  And it is very impressive, I have to say.  I especially liked the integrated DOCSIS 3.1 cable modems and routers.



LEO:  Oh, nice.



STEVE:  They have several devices.  Yes.  And I've been sort of unhappy with domestic cable modem suppliers.  They, you know, they really don't seem to be doing a great job.  This German firm, you know, really does seem to have their act together.  So if we ever get fiber in our area, they also have an integrated fiber modem router.  I might take a look at that.  And they're all WiFi 6 and even 7.  So they're keeping up to date with the standards, yeah.  Look at that thing.  That's just beautiful.



LEO:  Yeah, it really is.



STEVE:  And somehow they're doing a good job without lots of antennas sticking out everywhere.



LEO:  Well, I think that's - the antennas are a marketing thing, I think, yeah.



STEVE:  Uh-huh, exactly.



LEO:  Oh, this is cool.



STEVE:  Yup.  And there's a WiFi 7-enabled cable modem router that I'm seeing there.



LEO:  Yeah.  And with Zigbee onboard so you can control your smart home. 



STEVE:  Yup.



LEO:  Oh, these are nice.  This is really nice.  This is for LTE, so you use mobile broadband.



STEVE:  They are solid-looking devices.



LEO:  Of course, you've got to make sure that your cable company will support their...



STEVE:  Yes, that's the thought I had was that the Cox does, you know, they make a point of saying, well, these are the ones we support. 



LEO:  Yeah.  This is the one I would get, though, if I - interesting.  3.1, WiFi 7.



STEVE:  And apparently really strong.



LEO:  Yeah.



STEVE:  As he said, you know, they're not, like, they didn't cheap out the processor and RAM.



LEO:  I like that, yeah.  Look at that.  Okay.  FRITZ!Box.



STEVE:  And a telephone system.  It does telephony built-in, too.



LEO:  Oh, yeah, with DECT Base Station for cordless telephony.  Wow.  Wow.  They put everything in here but the kitchen sink.  Oh, wait a minute, here's a kitchen sink.



STEVE:  Oh.



LEO:  No.  At least use a kitchen.  And their operating system is called FRITZ!OS.  No.  FRITZ!OS.



STEVE:  Too bad it's not Fritos.



LEO:  It's so close to Fritos, I want to get it, yeah.  So it's not - you can't put WRT or something else on there.  This is their...



STEVE:  No, I don't think you - but I would bet that they've got a beautiful-looking built-in router.



LEO:  Well, it's made in Germany, so it's got to be good; right?



STEVE:  Jawohl.



LEO:  Look at that, yeah.  Wow.  Okay.



STEVE:  Okay.  So Richard Green in Lethbridge, Alberta, Canada, his subject was "Four-digit PINs in a corporate environment."  Okay.  Get a load of this, Leo.  He said:  "Hi, Steve.  Absolutely love the show.  Thanks for doing it.  I thought you might enjoy this story.  I'm a physical security installer, as in physical alarm systems, and I was asked to do a system audit and upgrade on a major chain grocery store.  So we came out and gave everything a physical check up and upgraded their equipment.  We then asked for their list of current users so we could verify and remove any old and unused alarm-disarming PINs.  At first, they didn't want to do this, and I figured it was a corporate policy.  But then they relented and started printing off pages of names and PINs.  Pages and pages of PINs.



"Apparently, some higher-up decided it would be a great idea to have every manager, assistant manager, or anyone else of importance nationwide, programmed into every store's alarm system just in case they might travel.  We started out, of course, with 10,000 possible PINs, but their list was nearly 7,000 PINs long.  This meant that any random guess would have a 70% chance of disarming their alarm system at any facility."  He said:  "I flat-out refused to be the guy to set up a system that was so insecure.  Luckily for me they finally relented, and we only added about 60" - six zero - "user PINs, local to our region.  I wouldn't have believed it had I not seen it for myself."



Amazing.  So Richard, thank you for sharing that horror story from the field.  It's helpful, I think, to see, like, the way things are actually being done out there in the real world, you know, way far from the ivory tower.



Manuel Schmerber, sorry for messing up your last name, Manuel, in St. Louis.  He said:  "Hi, Steve.  I noticed that the UDM value just selects from the menu of search options:  15 is attractions, 12 is news, 14 is web, and so on.  I also noticed that an easier way for me to get to the simpler Web results is to just select from the menu of offerings below my search phrase.  I select the More dropdown and then Web."  And he says:  "Thanks for the lowdown on search."



So I just wanted to thank Manuel for demystifying the "magic 14" of the UDM value.  I didn't spend any time digging around, and I'd been wondering where the 14 came from, you know, why was it UDM=14.  And yes, it is certainly possible to select the Web menu item from Google's already displayed results.  But the various hacks that are emerging allow us to get those same "web only" results right from the start with our browser's default search.



Vern Mastel in Mandan, North Dakota.  Oh, I got a kick out of this.  The subject was "SN-975 Windows XP Test."  He said:  "The Windows XP report is misleading.  The test was not fair.  What Parker did was test a 1935 Chevrolet sedan on a modern eight-lane superhighway at rush hour.  He should repeat the test with a new out-of-the-box configuration Windows 10 or 11 machine on the same, no router, open Internet connection.  That would be very interesting."



Vern said:  "Windows always has come out of the box with EVERYTHING" - that's all caps - "turned on.  I claim that the biggest holes in that test XP machine were Windows File and Print Sharing and Windows Remote Desktop.  Both are wide open in a fresh Windows XP install.  Such a machine should be dead meat on today's Internet.  So that begs the question, what ISP was used for the test?



"When XP was new in 2001, ISPs did not do any active protocol blocking.  Windows NetBEUI/NetBIOS ports 137, 138, 139, and 445, along with many others, were open to the world.  For example, with File and Printer Sharing turned on, the default, you could see and easily access other Windows XP machines in the vicinity.  For many years, when I set up a new Windows XP machine on the networks I administrated, I spent an extra hour changing network and system settings to close security holes, and shut down or remove the many unneeded features.



"Now things are pretty well locked down at the ISP level.  Old LAN protocols are blocked by default, you cannot run your own mail server out of your house, and other server protocols like FTP are monitored or blocked outright.  Properly configured, XP is/was a stable, reliable, and reasonably safe version of Windows."



Okay.  I agree with everything Vern said, except for his thesis that this was in some way not a fair test.  I have a problem with that characterization only because it wasn't meant to be a test of fairness.  It was a test of reality, or perhaps a test of yesterday's reality versus today's reality.  And of course everything Vern noted about the way he would first spend his first hour with any new Windows XP machine was the reason I created the ShieldsUP! port probing facility.  It was precisely because these early machines from Microsoft were such a disaster on the Internet.



That said, I also agree with him that it would indeed be interesting to place a currently fully patched Windows 10 or 11 machine directly on the 'Net to see how it would fare.  You know, direct exposure to the Internet.  Given that all Windows machines have a very competent application-driven firewall that is up and running before the rest of the vulnerable networking behind it comes to life, I would expect it to do well.



But in any event, Parker's whole point was to get some sense for the malicious crap that is circulating out on the Internet right this moment.  We're all so well insulated and so well shielded behind our NAT routers and firewalls that it's possible to sort of forget just what's out there constantly pounding away at those defenses.  These defenses that we have today are absolutely not optional.



Okay.  And lastly, Jeff Smock says as his subject:  "I offer you my very own First Law of Cloud Data Security."  He said:  "Forget about all the bluster and jazz hands the cloud service providers give us regarding the security of our data."  He says:  "Here is the simple truth:  'The security of cloud data is inversely proportional to its potential value as perceived by a hacker or rogue staff member."  So, yes, the more they want it, the bigger problem we're going to have keeping them from it.  So I completely agree with that characterization.  And Leo, let's take our final break, and then we're going to talk about the 50 Gigabyte Privacy Bomb.



LEO:  Can't wait.



STEVE:  Which Microsoft plans to drop into everyone's lap.



LEO:  Can't wait.  Hey, I just wanted to mention, because that FRITZ! thing got me really excited, and I went to the website, it's a German company.  As far as I could tell they do not sell in the United States.  Your correspondent was in Israel.  



STEVE:  Ah.



LEO:  If somebody who's listening tells me where I can get FRITZ! stuff in the U.S., and if it's U.S. compliant, I'd be very interested.  But right now it looks like, I mean, it's Germany.



STEVE:  I wonder why.



LEO:  Well, we have an FCC.  I mean, they'd have to get approval in the U.S.



STEVE:  But they're Germans.



LEO:  How hard can that be?



STEVE:  I mean, I'm driving - we're each driving one of their cars, Leo.



LEO:  I know.  I like...



STEVE:  They work just great.



LEO:  German engineering, okay.  But as far as I can tell I couldn't find anywhere I could buy it in the U.S. 



STEVE:  Wow.



LEO:  And I checked, and your correspondent was in Israel, which might explain how they could get it.



STEVE:  Yes, he was.



LEO:  Yeah.  Okay, 50 gigabyte pile of nonsense. 



STEVE:  Uh-huh.  So since we began the podcast with a general theme of how AI, which is not even close to being intelligent, is being misapplied during these early days, I feel as though a security and privacy-focused podcast like this one ought to take note of the new "Recall" feature that will be part of the next-generation ARM-based Windows 11, what's known as Copilot+ laptop PCs.



First of all, yes, it does appear that ARM processors have finally come far enough along to be able to carry the weight of Windows on their processors.  And while having Windows on ARM will certainly create a new array of challenges, like for example the lack of specific hardware drivers that only exist for Intel kernels, in the more self-contained applications, you know, where drivers are much less used, such as laptops, where power consumption and battery life trumps pretty much any other consideration, it's foreseeable that Windows may finally be able to find a home on ARM.  Today, laptop and tablet form-factor machines containing Qualcomm Snapdragon ARM processors, running Windows 11, have been announced and are in some cases available for pre-order from Acer, Asus, Dell, HP, Lenovo, Microsoft, and Samsung.



It's also worth noting that Intel PCs will also be getting Copilot+ at some time in the future.  But they will need to have a neural processing engine.  Answering the question "What makes Copilot+ PCs unique," Microsoft writes:  "Copilot+ PCs are a new class of Windows 11 PCs that are powered by a turbocharged neural processing unit (NPU), a specialized computer chip for AI-intensive processes like real-time translations and image generation, that can perform more than 40 trillion operations per second."  So we have TOPS, trillion operations per second.  So more than 40 TOPS.  And later, Microsoft writes:  "We are partnering with Intel and AMD to bring Copilot+ PC experiences to PCs with their processors in the future."  So potentially everybody's going to be able to get this.



Okay.  So what is Recall?  Microsoft explains.  They said:  "You can use Recall on Copilot+ PCs to find the content you have viewed on your device.  Recall is currently in preview status.  During this phase we will collect customer feedback, develop more controls for enterprise customers to manage and govern Recall data, and improve the overall experience for users.  On devices that are not powered by a Snapdragon X Series processor, installation of a Windows update will be required to run Recall.



"Recall is currently optimized for select languages, including English, simplified Chinese, French, German, Japanese, and Spanish.  This means Recall is able to retrieve snapshots from your PC's timeline based on more sophisticated searches in these languages.  During the preview phase, we will enhance optimization for additional languages.  Recall can also retrieve snapshots from your PC's timeline based on text-to-text searches in more than 160 languages."



Okay.  Fortunately, they then ask themselves "How does Recall work?"  To which they reply:  "Recall uses Copilot+ PC advanced processing capabilities to take images of your active screen every few seconds.  The snapshots are encrypted and saved on your PC's hard drive.  You can use Recall to locate the content you have viewed on your PC using search, or on a timeline bar that allows you to scroll through your snapshots.  Once you find the snapshot that you were looking for in Recall, it will be analyzed and offer you options to interact with the content.



"Recall will also enable you to open the snapshot in the original application in which it was created."  Whoa.  Really?  Okay.  "And, as Recall is refined over time, it will open the actual source document, website, or email in a screenshot."  Which, okay, is mindboggling.  But they said:  "This functionality will be improved during Recall's preview phase."  So before they let it loose.



They said:  "Copilot+ PC storage size determines the number of snapshots that Recall can take and store.  The minimum hard drive space needed to run Recall is 250GB, and 50GB of space must be available.  The default allocation for Recall on a device with 256GB will be 25GB, which can store approximately three months of snapshots.  You can increase the storage allocation for Recall in your PC Settings.  Old snapshots will be deleted once you use up your allocated storage, allowing new ones to be stored."  Okay, so it's sort of a rolling 90-day window, the most recent 90 days of screen snapshots taken every few seconds.



Okay.  Then they ask:  "What privacy controls does Recall offer?"  They respond:  "Recall is a key part of what makes Copilot+ PCs special, and Microsoft built privacy into Recall's design from the ground up."  Which of course we all recognize as standard boilerplate, which we all hope is true.  They said:  "On Copilot+ PCs powered by a Snapdragon X Series processor, you will see the Recall taskbar icon after you first activate your device.  You can use that icon to open Recall's settings and make choices about what snapshots Recall collects and stores on your device.  You can limit which snapshots Recall collects; for example, you can select specific apps or websites visited in a supported browser to filter out of your snapshots.  In addition, you can pause snapshots on demand from the Recall icon in the system tray, clear some or all snapshots that have been stored, or delete all the snapshots from your device."



LEO:  We call that "I'm going to watch porn button now."



STEVE:  Yeah.



LEO:  Press the porn button.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  And it occurs to me that I'd later talk about how snapshots of the Windows-based Signal app would be a problem.



LEO:  Oh.  Because that's in the clear; right?



STEVE:  Right, right.  I mean, it's what the user sees.  Maybe this allows you to say "Don't take snapshots of that window."  And we should also remember that what we see is a graphic user interface.  But Windows knows the text behind the actual controls that it's displaying.  So it doesn't actually have to be, I mean, I guess it's - who knows what it's doing in detail.  But my point is that, while we see graphics, there's actual text which is being mapped into bitmapped fonts which is then being displayed on the screen.  So behind the screens, so to speak, Microsoft actually has the raw text which was used to generate the screen.



LEO:  Yeah, yeah.  That makes sense.



STEVE:  Okay.  So they said:  "Recall also does not take snapshots of certain kinds of content, including InPrivate web browsing sessions in Microsoft Edge."  And by the way, they only said Edge, but I saw elsewhere that it's any of the browsers that have a well-defined private browsing mode.  You know, they do not record that.  And they said:  "It also treats material protected under digital rights management," you know, DRM stuff, "similarly.  Like other Windows apps such as the Snipping Tool, Recall will not store DRM content."



And they said:  "Note that Recall does not perform content moderation.  It will not hide information such as passwords or financial account numbers.  That data may be in snapshots that are stored on your device, especially when sites do not follow standard Internet protocols like cloaking password entry."



Okay.  So we're rolling toward an entirely new capability for Windows PCs, where we'll be able to store data which I presume is somehow indexed first, then encrypted for storage and later access.  And unless otherwise instructed and proscribed, this system is indiscriminately taking snapshots of our PC screen content every few seconds and is, by Microsoft's own admission, potentially capturing and saving, for later retrieval, financial account numbers, monetary balances, contract language, proprietary corporate memos and communications, and who knows what private things we'd really rather never have recorded, or whatever else the user might assume will never go any further.  This is where our much beloved and overworked phrase "What could possibly go wrong?" comes to mind.



Does anyone not imagine for an instant that having searchable access to the previous 90 or more days of a PC's screen might be hugely interesting to all manner of both legal and illegal investigators?  Corporate espionage is a very real thing.  China is moving their enterprises away from Windows as rapidly as they can.  But you have to know that cyberattackers, many of the most skillful and persistent who seem to be persistently based in China, must be beside themselves with delight over this new prospect that we decadent capitalists in the West are going to start having our PCs recording everything that's displayed on their screens.  What a great idea.  If history teaches us anything, it's that we still have not figured out how to keep a secret, and especially not Microsoft.  So what Microsoft is proposing to plant inside all next-generation PCs is tantamount to a 50 Gigabyte Privacy Bomb.  Maybe it will never go off, but it will certainly be sitting there trying to.



And just ask yourself whether law enforcement and intelligence agencies don't also think this sounds like a terrific idea?  Oh, you betcha.  With great power comes great responsibility.  And here, clearly, there's much to go wrong.  Microsoft understands this perception, and so they ask:  "How is your data protected when using Recall?"  They explain:  "Recall snapshots are kept on Copilot+ PCs themselves, on the local hard disk, and are protected using data encryption on your device and, if you have Windows 11 Pro or an enterprise Windows 11 SKU, BitLocker.  Recall screenshots are only linked to a specific user profile, and Recall does not share them with other users, make them available for Microsoft to view, or use them for targeting advertisements.



"Screenshots are only available to the person whose profile was used to sign into the device.  If two people share a device with different profiles, they'll not be able to access each other's screenshots.  If they use the same profile to sign into the device, then they will share a screenshot history."  And thus, you know, be able to scroll back to see what the other person has been doing.  "Otherwise, Recall screenshots are not available to other users or accessed by other applications or services."



Okay.  So all that really means there is they've done the obvious thing; right?  Is that they've, you know, they've divided the machine in the same way they do currently now, you know, with things like apps that you install for only one profile.  So, okay.  That's what Microsoft had to say.



The guys from Ars Technica watched Microsoft's presentation of this last Monday and gave their write up an impressively factual and neutral headline.  They said:  "New Windows AI feature records everything you've done on your PC."  And then they said:  "Recall uses AI features" - okay - "to 'take images of your active screen every few seconds.'"  So Ars wrote:  "At a Build conference event on Monday, Microsoft revealed a new AI-powered feature called Recall for Copilot+ PCs that will allow Windows 11 users to search and retrieve their past activities on their PC.  To make it work, Recall records everything users do on their PC, including activities in apps, communications in live meetings, and websites visited for research.  Despite encryption and local storage, the new feature raises privacy concerns for certain Windows users.



"Microsoft says on its website:  'Recall uses Copilot+ PC advanced processing capabilities to take images of your active screen every few seconds.  The snapshots are encrypted and saved on your PC's hard drive.  You can use Recall to locate the contents you have viewed on your PC using search or on a timeline bar that allows you to scroll through your snapshots,'" quotes Ars Technica.



Ars wrote:  "By performing a Recall action, users can access a snapshot from a specific time period, providing context for the event or moment they're searching for.  It also allows users to search through teleconference meetings they've participated in and videos watched using an AI-powered feature that transcribes and translates speech.  At first glance, the Recall feature seems like it may set the stage for potential gross violations of user privacy.  Despite reassurances from Microsoft, that impression persists for second and third glances, as well."  They said:  "For example, someone with access to your Windows account could potentially use Recall to see everything you've been doing recently on your PC, which might extend beyond the embarrassing implications of pornography viewing and actually threaten the lives of journalists or perceived enemies of the state."  And I'll interject to say, in other words, this puts examining someone's web browser history to shame.  How quaint that becomes.



Ars continues:  "Despite the privacy concerns, Microsoft says that the Recall index remains local and private on device, encrypted in a way that is linked to a particular user's account.  Microsoft says:  'Recall screenshots are only linked to specific user profile, and Recall does not share them with other users, make them available for Microsoft to view,'" anyway, blah blah blah, what I just wrote about that.



So they said:  "Users can pause, stop, or delete captured content, and can exclude specific apps or websites.  Recall won't take snapshots of InPrivate web browsing sessions in Microsoft Edge or DRM-protected content.  However, Recall won't actively hide sensitive information like passwords and financial account numbers that appear onscreen.  Microsoft previously explored a somewhat similar functionality with the Timeline feature in Windows 10, which the company discontinued in 2021, but it didn't take continuous snapshots.  Recall also shares some obvious similarities to Rewind, a third-party app for Mac we covered in 2022 that logs user activities for later playback."



They said:  "As you might imagine, all this snapshot recording comes at a hardware penalty.  To use Recall, users will need to purchase one of the new 'Copilot Plus PCs' powered by Qualcomm's Snapdragon X Elite chips, which include the necessary neural processing unit.  There are also minimum storage requirements for running Recall, with a minimum of 256GB of hard drive space and 50GB of available space.  The default allocation for Recall on a 256GB device is 25GB, which can store approximately three months of snapshots.  Users can adjust the allocation in their PC settings.



"As far as availability goes," they conclude, "Microsoft says that Recall is still undergoing testing.  Microsoft says on its website:  'Recall is currently in preview status.  During this phase we'll collect customer feedback, develop more controls for enterprise customers to manage and govern Recall data, and improve the overall experience for users.'"



Okay.  I just should note that the amount of storage Recall uses does scales upward with the size of the system's mass storage.  And presumably the duration of the scroll back increases similarly.  It'll take 25GB when 256 is available, 75GB on a 512GB drive, and 150GB from a system with a 1TB drive of primary mass storage.  So presumably, the more storage the system is able to commandeer, the further it's possible to scroll back through the system's display history.



Okay, now, while trying to be objective about this, the first question that leaps into the foreground for me is whether anyone actually needs or wants this?  Is this a big, previously unappreciated problem that everyone has?  Okay.  But trying to be objective.  First of all, compared to the static contents of a hard drive, Recall would be objectively a goldmine of additional new information about the past 90-plus days of someone's life, as viewed through their computer activities.  And more than ever before, people's entire lives, and their private lives, are reflected in what's shown on the screens of their computers.  Maybe that makes scrolling back through their recorded lives compelling.  I don't know.



But we know from Microsoft that it will be snapping video conference content on the fly.  And as I mentioned, the Windows Signal app that goes to extremes to protect the contents of its chats would presumably be captured, unless you're able, as I mentioned before, and they sort of suggest, you can tell Recall don't record specific applications.  So you probably want to turn that off, or maybe you trust Microsoft, and it'll be part of your scroll back.



But, you know, email screens and nearly everything that happens on a PC would be captured.  And of course that's the point; right?  But the vast majority of that content will not have been stored on the machine's hard drive ever, until now.  So objectively, the presence of Recall clearly introduces a new, never before existing liability.  And that's what everyone who talks about this sees as a potential for creating havoc where none existed before.



So the question, it seems to me, is whether the new value that's created and is returned by Recall's scrolling usage history justifies whatever new risk might also be created by its retention of that data.  How useful will having all that actually be?  I've tried to imagine an instance where I wish I could look back in time at my computer screen.  I suppose I don't feel the need since I've never had the option.



So if I knew I could scroll my computer's screens back in time, I suppose it might be an interesting curiosity.  But it really doesn't feel like a feature I've been needing and missing until now.  I suppose an analogy would be that the world had no idea what it was missing before the creation of social media.  And hasn't that been a big boon to mankind?  Now, unfortunately, we seem unable to live without it.  Perhaps this will be the same.



The bottom line is this I think we're just going to need to live with this thing for a while.  We're going to need to see whether this is a capability desperately searching for a need; or whether, once people get used to having this new thing, they start thinking, how did I ever live without this?  However, one thing that is also absolutely objectively true is that everyone will be carrying around a 50GB privacy bomb that they never had before.  Maybe it'll be worth the risk.  Only time will tell.



Oh, and Simon Zerafa posted a tweet from someone who has been poking into Recall's storage.  He's detective@mastodon.social, who wrote:  "Can confirm that Recall data is indeed stored in a SQLite3 database.  The folder it's in is fully accessible only by system and the administrators group.  Attempting to access it as a normal user yields the usual 'You don't currently have permission' error."  And he said:  "Here's how the database is laid out for those curious."  And he said:  "Figured you might appreciate a few screenshots."



So I've put one in the show notes.  And sure enough, it's got a DB browser for SQLite and shows the layout of the table with all of the various components, you know, window capture text index content, window capture text index data, window capture text doc size and relations and all kinds of stuff.  So anyway, I guess what this means is that, if nothing else, if that data should ever escape from anyone's PC, it will not be difficult for anybody who gets it to open it up and browse around in it because it's just a SQLite3 database.



And Leo, you know, I guess, you know, if search really worked, and you were able to search on something that you remember, but you didn't write down or didn't record, didn't save, but it was just like right there at your fingertips, and bang, it popped up and showed it to you, I guess I could see that that could be compelling.



LEO:  Yeah.  I mean, I want to have - the late Gordon Bell passed away last week.  He used to wear around a camera.  His wife Gwen, I knew them both, wonderful people, had severe Alzheimer's, so he became very aware of the idea of remembering things.  And I can't remember what he called it, the "mem-it," the "meme-it" or something.  But this was '96.  This was way before there really was the technology to do this.  But it would take a picture every 20 seconds.  And his theory was I would like to have, I mean, it's not just recall of your Windows desktop, but of everything, that you could then search and query.



And now there are, you know, I just bought something called the Limitless Pin that should come in August, it's a little lapel pin that records all your audio and then feeds it to an AI.  So you can query it of things like that.  You know, Steve and I were talking, and he mentioned a router from a German company, I can't remember the name of it.  What was the name of that?  "The name was Fritz."  You could see that that kind of might be useful.  There are absolutely privacy issues with this.  In fact, that Limitless Pin won't record somebody's voice unless you get explicit spoken approval to do so, which is very interesting.  It uses voice printing.



STEVE:  So it's doing voice recognition.



LEO:  Yeah, yeah.



STEVE:  Instead of just being a generic audio recorder.



LEO:  Right.  But then, once they say yes, then it will say, "Steve said," "Leo said," that kind of thing.  I don't - you know what?  This is very early days.  But you nailed it.  There is potentially some use for this, but there's also a downside, many downsides.



STEVE:  Yeah.  And so I think it's a tradeoff, like anything else.  Is this so useful that it's worth carrying around the last 90 days plus of everything that your computer screen showed?  And that's the other thing, Leo.  You're not going to want to not record, like, chunks of your screen, like you would probably not want to not record - I'm sorry.  You would want to record Signal because you'd want to be able to...



LEO:  You want to be able to query it, yeah.



STEVE:  Exactly.  Exactly.  So the tendency will be to, you know, record everything and trust the Force.  Unfortunately, that's Microsoft.



LEO:  It's a challenge, I mean, this is really a challenge.  I would not turn on Recall, partly because of the burden, the strain it puts on the system seems like a bad idea.



STEVE:  Well, yeah.  I have a feeling that our audience will not be among the first adopters.



LEO:  Right.



STEVE:  I mean, some will.  I'm curious.  And as I was thinking about this, I thought, I will be interested in hearing.  Like, you know, to hear Paul - lord knows he's not a pushover.  So if Paul Thurrott says, hey, this is the greatest thing since, you know, bananas, sliced bread, you know, whatever, then wow.



LEO:  Yeah, we'll watch what Paul - exactly.  I mean, you've searched through your - I have - searched through your browser history; right?  I can't remember, what was that site?  And you go through your browser history.  That's what browser history does, except it's just recording websites you visit, just the URLs.  Might be more useful if it recorded the content.  And then maybe if all your apps did the same, and you can see how you can slide into this.



STEVE:  Yeah.  Content, there was an app back in the DOS days, and I've tried to remember what it was.  It would take little notes.  And so you could easily create a little text window and type some text in, and it just went into a big pile.



LEO:  Was it Sidekick?



STEVE:  Well, as you typed, I think it was a TSR, and you would bring it up full screen, and it would be this blizzard of little overlapping, like, Post-its.



LEO:  Yeah, that sounds familiar, yeah.



STEVE:  But then, as you type a few characters, all the ones that did not contain that substring would - they disappeared.  And it was compelling to be able, like anything you thought you remembered, you could just type a few characters, and it would, like, whittle it right down. 



LEO:  There was "Ex."  Do you remember "Ex"?  That was the idea of "Ex" was a superfast - but that wasn't like what you just described.  But the idea was it indexes your - just like Windows does, but it faster and better indexes everything on your drive.  And then you type one letter, it finds everything that matches that, two letters, three letters, it's that progressive search.  So you could very quickly - "Ex" was very, very fast.  It was very cool.



STEVE:  What happened to it?



LEO:  It's still around.  I think they went - they became an enterprise tool.  



STEVE:  Okay.  Well, and it's built into Windows now.  So, you know...



LEO:  Yeah.  Yeah, but Windows doesn't do it as well as "Ex" did.  It was [crosstalk] even when "Ex" did it.  It was a smart search tool.  What was the name of that DOS program?  I know exactly what you're talking about.



STEVE:  Yeah.  It was, and I've tried to remember it, and like Instant Recall or something like that.



LEO:  Oh, that sounds familiar, yeah.  Oh, this is "Ex."  That's the wrong "Ex."  This is the problem with the word "Ex."  It's  not a good search term.  It's not easy. 



STEVE:  No, it was a dumb thing for Twitter to get renamed to.



LEO:  Not the only dumb thing Elon's ever done.  That's interesting.  I want to know this Instant Recall thing.



STEVE:  I want to think - I think it was written by Phil Katz.



LEO:  Oh, well, there you go.  Yeah, he was a genius.



STEVE:  The PKZIP guy.



LEO:  The PKZIP guy.



STEVE:  Yeah.



LEO:  Yeah, Recall 11.  It was called Recall.



STEVE:  Just Recall.



LEO:  I think so.



STEVE:  Wow.



LEO:  I think so.  Memory resident, no, it's command line editor and history utility.  That might be it.  It's TSR.



STEVE:  No.  This thing was definitely - it was little notes.  It was not a command line history.



LEO:  Okay.



STEVE:  That does sound like a DOS command line history.  



LEO:  Yeah.  Huh.  Somebody will remember, and they'll message you on your new email platform.



STEVE:  Yay.



LEO:  How do they do that again?  They go to GRC.com.



STEVE:  /mail.



LEO:  /mail.



STEVE:  Or just GRC.com, and there's a little white envelope up at the top of the screen.



LEO:  They have to get whitelisted, though.



STEVE:  Yes.  Exactly.  So GRC.com/mail.  Okay.  And there it is.



LEO:  I put in my email.



STEVE:  Yup.  And I just sent you a link.



LEO:  And then you're going to send me an email.  Okay.  And then I confirm that that's my email, which no spammer would ever do.



STEVE:  Correct.



LEO:  So right there you've eliminated spammers.



STEVE:  Correct.  And so you click on - the email that comes is very attractive.  And you click on the little button that you get, and it takes you to your subscriptions page.  And you can put your name in if you want so that I address email to you by name.



LEO:  Well, there it is. 



STEVE:  There it is.



LEO:  Very attractive.  Nice choice of colors, Steve.  Confirm.



STEVE:  Well, I need to think about that, though, the black on white.  It looks much better on a white background.



LEO:  Ah, yes.  I'm dark mode, man.  Okay.  So now I'm - you know better than clicking a button in email.  So what I'm going to do is the smart thing, which is copy it.



STEVE:  That's right, I gave you a link.



LEO:  And paste it in.



STEVE:  I gave you a link because I know who our...



LEO:  Yeah, GRC.com/manage, yeah, that looks good, okay.  And now I can choose, if I wish, to subscribe, but I don't need to.  Oh, look, you can subscribe to Security Now!.  Oh, I definitely want product news.  Oh, and GRC news.  Okay.  And my name, Leo.



STEVE:  Is Leo.



LEO:  Yes.  And I'm going to update my subscriptions.  So now I can email you from this address.



STEVE:  Correct.  And now, if you get the mail again, you'll see the confirmation that was sent, which is also very pretty.



LEO:  Okay.  Look at that.  Also, you know, let me get out of dark mode so I can enjoy the fresh...



STEVE:  Yeah, it's very pretty.



LEO:  ...beauty of what you're doing here.  Of course I've been in dark mode so long I have no idea how to get out of it.



STEVE:  How to turn it off.



LEO:  How do you turn this off here?  I don't know.  I think I have to go to system settings.  And I go, let's be light.  Light mail.  Oh, look how pretty that is, Steve.



STEVE:  Yup.  So I show - I address it by name, show you your email address, and which list that you're subscribed to.



LEO:  Nice.  Very nice.



STEVE:  Yup, that's all there is to it.  And so now you are whitelisted, and you can send email to securitynow@grc.com.



LEO:  And that's where you should do your feedback.  Go through those steps, and you can have a nice conversation, a nice chat with Steve.



STEVE:  Yeah, and I don't know what's going to happen with Twitter.  As I said, it's easy for me to post the weekly notes there. 



LEO:  Oh, you should keep doing that.



STEVE:  I'm beginning to get a lot of spam.  So I guess I don't see any reason for it any longer.



LEO:  I've been gone more than a year, and I don't miss it.  Good.  Good.  Very nice.  Now, while you're at GRC.com, don't stop there.  Do that, GRC.com/mail.  But you can also find Security Now! there.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#977

DATE:		June 4, 2024

TITLE:		A Large Language Model in Every Pot

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-977.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  When is a simpler application better than something complex?  How did the first week of GRC's new email system go?  Have you been pwned?  And if so, how worried should you be?  What's the latest new supply-chain attack vector?  What certificate authority just lost all their TLS server business?  And remember that early messaging service ICQ?  Whatever became of it?  Finally, after I share a tip about a perfect science fiction movie, two pieces of listener feedback and one user's happiness over SpinRite, we're going to look at what a prominent security researcher learned after using Microsoft's Recall for 10 days, and why I think Microsoft is willing to bet the farm and risk the dire warnings of the entire security community over this unasked-for capability.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Have you ever been pwned?  Well, here's a way to know and whether you should worry about it.  What certificate authority just lost their TLS server business?  We'll talk about that, the end of ICQ, and Microsoft's new Recall feature that's coming to all Copilot+ PCs.  Steve explains why it is not as secure as Microsoft has said, why it's in fact a real danger.  He also has a theory, and I agree with it 100%, of why Microsoft is doing this.  So a very interesting play for your information, next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 977, recorded Tuesday, June 4th, 2024:  A Large Language Model in Every Pot.



It's time for Security Now!.  Yes, adjust your spectacles and put your beanie on straight because this guy, Steve Gibson, is going to challenge you, he's going to excite you, he's going to thrill you, he's going to make you a geek just by proximity.  Hello, Steve Gibson.



STEVE GIBSON:  I think if you've survived more than a couple of these podcasts...	



LEO:  You qualify.



STEVE:  ...your geek status has been already established.



LEO:  You more than qualify.



STEVE:  If you haven't gone running for the hills.  It's like, agh.  I got a piece of mail from one listener who said, okay, so I think I understand about 5% of what you're talking about.



LEO:  That's pretty good.  You're doing well.



STEVE:  But I do come away with something useful every week, so I keep coming back for more abuse.  No, for more edification.



LEO:  Well, and it's like lifting a heifer.  Like when a cow, a baby cow is first born, you can lift it.  If you lift it every day, you'll be able to lift a full-grown cow. 



STEVE:  This is the analogy you've come up?  We're lifting - lifting cows.



LEO:  Keep listening every week, and in a year or two you'll be able to lift a cow.  How about that?



STEVE:  And maybe you'll be able to throw a honeypot.



LEO:  There you go.



STEVE:  Oh, actually there's a really interesting piece that Microsoft just revealed, the details of a honeypot they had been running for a long time.  Anyway, I may be talking about that next week, if nothing more interesting comes along.  But speaking of pots, today's title is "A Large Language Model in Every Pot."  And we're going to go back and talk about Recall again.  Well, okay.  I'm stepping on my own sequence here.  So we've got a lot of things to talk about.



When is a simpler application better than something complex?  How did the first week of GRC's new email system turn out?  Have you been pwned?  And, if so, how worried should you be?  What's the latest new supply-chain attack vector?  What certificate authority just lost all their TLS server business?  Whoops.  And remember that early messaging service ICQ?  Whatever became of it?



Finally, after I share a tip about what I consider to be a perfect science fiction movie, two pieces of listener feedback, and one user's happiness over SpinRite, we're going to look at what a prominent security researcher learned after using Microsoft's Recall for 10 days, and why I think Microsoft is willing to bet the farm and risk the dire warnings of the entire security community over this unasked-for capability.  I think I know where they're headed.  And it's very exciting, if I'm right.  And it's also very troubling.  And it's really a shame that they've been screwing around with Windows, adding features nobody wanted instead of making it more secure because they really can't do what they want to do.  So we're going to have fun today.



LEO:  Very interesting.



STEVE:  Unlike all of the other 976 podcasts that came before.



LEO:  So boring.  So boring.



STEVE:  Yeah.



LEO:  No.  We're going to have fun today, I promise you.



STEVE:  And we do have a great Picture of the Week.



LEO:  Oh, haven't read it, I just know the caption.  All right.  All right.  I am ready for the Picture of the Week, Mr. Gibson.



STEVE:  So I gave this picture the title, "But Officer..."



LEO:  Okay.  Does it need no explanation?



STEVE:  It really doesn't, once you see the picture.



LEO:  All right.  It's going to take me a minute to get it up on this computer.  Here it comes.  All right.  I'm prepared.  Are you ready?  I'm going to scroll up.  We shall enjoy it together.  But Officer...  There's a one-way street sign, a stop sign, and a no right-turn sign.  What the - what am I supposed to do?



STEVE:  You know, Leo, you just have to wonder, like...



LEO:  What the heck?



STEVE:  I know.  Okay.  So for people who aren't seeing this, we have a picture where a public street has come up to a T intersection.  So you have to turn left or right.  Well, there's a stop sign, so you certainly need to consider your options, thus stopping.  The problem is that the street that you are intersecting with has been labeled as one-way, where all the traffic is moving from left to right.  But below the stop sign, it's also very clearly marked that you must not turn right.  There's the right-turn arrow with a big red slash through it.  So I don't know.  Do you back up?  You know, like backing up would be the only thing you can do.



LEO:  I think it's all you can do.  But notice there's no outlet.  You're in a cul-de-sac.  So you're really dead in the water.



STEVE:  So you're right.  Is that what the yellow sign says?  



LEO:  Yeah, it says "No Outlet."



STEVE:  It says "No Outlet"?  I thought, yeah, I thought so.  So now that...



LEO:  So this is the worst.



STEVE:  That's something that would be seen by people going down the street waving at you because you're stuck, and you can't go anywhere.



LEO:  I think this is a prank being played on self-driving cars.  Whoever lives on this street added that sign knowing that a self-driving vehicle would then be complete stuck.



STEVE:  It would just explode, Leo.



LEO:  It can't do anything.



STEVE:  It would just say, okay, I quit.



LEO:  I can't do anything.  I'm stuck.  Oh, my god, that's hysterical.



STEVE:  Welcome to America.  Okay.  So I wanted to thank all of our listeners who correctly recalled that the random notes DOS app we were trying to remember last week was "Tornado Notes."



LEO:  And I don't even remember that one, so I wouldn't have gotten it.



STEVE:  Yeah, it was not well - Leo, it was DOS.  But you used DOS back in the day.



LEO:  Oh, I used Sidekick.  I used a lot of [crosstalk] DOS, yeah.



STEVE:  Yeah, yeah, yeah.  So it was not Phil Katz of PKZIP fame.  It was a guy named Jim Lewis of Micro Logic Corporation. And when I first encountered Tornado Notes from a company named Micro Logic Corporation of Hackensack, New Jersey, I wondered, why is that name so familiar?  And it turned out it was because the same guy had created one of the most useful sets of 8.5 x 11" double-sided plastic sheet processor instruction reference cards the world had ever encountered.  I have a picture of them in the show notes.



Now, upon the event of my death, my plan is for cremation, after first having whatever organs may still be functioning and useful to anyone removed. But if my plan were burial, I would want these processor instruction reference cards...



LEO:  Bury them with you.



STEVE:  ...buried alongside me.



LEO:  This is a 6502, a Z80, and an 8086.  It's all in there.



STEVE:  And there is a 68000, as well.  I cannot begin to express how important they were back when I was writing assembly code...



LEO:  Wow, look at this.



STEVE:  ...first for Apple's and later Atari's 6502-based machines.



LEO:  This is so cool.



STEVE:  And Leo, I've got links on the next page to the PDFs of them.  I mean, these things were significant to so many people.



LEO:  Yeah.



STEVE:  I ran across someone over on Reddit who commented that it was a good thing these were 100% plastic or he would have worn his out.  You know, they were indispensable.  And I don't know where mine are.  I'm sure they're here somewhere because I would have never thrown them out.  They were just perfect.  Now, you have on the screen now the 6502 card.  And notice all the blank boxes.  Those are missing opcodes.  So that was important.  You had to know, you know, what was available and what wasn't.  And one of the reasons the 6502 microprocessor was so well used - Apple chose it, Atari chose it, Commodore chose it - was because it was so inexpensive.  And the reason it was inexpensive...



LEO:  It didn't do diddly.



STEVE:  It didn't do much, exactly.  It, you know, it transferred all the burden to the programmer, and like most of those opcodes are empty in there.



LEO:  Wow.



STEVE:  But it did just enough in order to get the job done.  But this was just - it was so - so this guy named Jim Lewis, who later gave us Tornado Notes for DOS, a TSR, you know, the reason I knew his name when Tornado Notes came along is like, wait a minute, I've got these Instruction Reference Cards that I've been using forever.  But anyway.  Tornado Notes for DOS was utterly unique.  When Windows happened, Jim tried to recreate the success of Tornado Notes with a product he named Info Select.  But Info Select was the victim of its own featuritis.



The sublime beauty of Tornado Notes was that it was so simple. It did exactly and only one thing perfectly and - and this was the other thing - instantaneously.  It began as a massively overwhelming disorganized pile of rectangular notes.  Didn't matter, you could just put anything, just random text in, didn't matter what shape or size they were.  But then, as you typed successive characters of a string, all those notes that did not contain the substring that had been entered thus far would instantly disappear.  So you got this very satisfying, almost animated, real-time winnowing of your entire pile until you could see the note you knew was there somewhere.  And notice that you also saw all the notes that contained that same substring, which was often surprisingly useful at times.



Unfortunately, Jim, for all his brilliance, did not understand that Tornado Notes succeeded due to the constraints imposed upon it by its DOS environment.  So when he created its successor, which was Info Select for Windows, he gave it hierarchies and categories and menus and formatted printing and everything else you can imagine that Windows made possible.  I think there was even a kitchen sink tucked in there somewhere.  And, you know, we wanted the same thing for Windows that we had for DOS.  But what we got was a monstrosity that required all manner of configuration and thought.  Yes, it could do so much more than Tornado Notes could.  But the very thing that was so beautiful about Tornado Notes was everything it did not do.  So as it turned out in retrospect, you know, the thing, I mean, it being so minimal was what made it so compelling and useful.



And I'm mentioning this because there's a larger lesson here.  One of the things the original designers of Unix also got exactly right was the idea of creating many simple commands that took some input, did something to it, and then produced some output.  And then to that you add the simple ability to interconnect these individual small building blocks into a chain by piping the output of one into the input of another, and you're able to interactively create and assemble a much more complex ad hoc function.



And, Leo, while I'm not a LISP programmer, I have the sense that the same sort of approach can be used there, where you kind of incrementally build up...



LEO:  Exactly, yes.



STEVE:  ...a much more complex solution that's assembled from many smaller pieces interacting.



LEO:  They call it "composable" because you compose a larger program out of pieces of smaller programs.  And to my mind it makes it so much easier because you can bite off a little bite, figure out how it works, and because it's basically functional, you know, it's always going to give you the same result with the same input.  You could slow put those together and build something out of it.



STEVE:  Yeah.



LEO:  It feels to me like woodworking almost, like assembling a machine.  It's great.



STEVE:  Like crafting a solution.



LEO:  Crafting, exactly, yes.



STEVE:  So anyway, the point I hope to make here is that more is not always better.  And, you know, for example, this is a lesson that the people who design the remote controls for A/V equipment appear to have never learned.  Oh, my goodness, it's a joke that those things are so crazy.  And I did notice that, you know, when I was thinking about this, that my freeware all just does one thing.  You know?  I create a little program.  It just does one thing.  If you want that one thing, that's the program you use.  It's, you know, 23K.  It does its job.  And then you're done.  And actually through the years people have been asking for many, many more features from SpinRite, and I've just said no, you know, SpinRite does what it's supposed to do.  And that's what it's for.



So anyway, I just - I want to thank all of our listeners who said "I think you guys were thinking about Tornado Notes."  And sure enough.  And I wouldn't be surprised, I mean, there are DOS boxes around that could run Tornado Notes.  I haven't run across a copy of it, but I probably have one on a hard disk around here somewhere.  Anyway, I also wanted to follow up on last week's announcement of GRC's new email system, which has been a resounding success.  If you missed last week's episode, that is, if you don't listen to them all and don't know about it yet, you could go to our old GRC.com/feedback page which we've been talking about for 20 years which explains a bit about the nature of web form spam, which unfortunately is a thing, and it contains a pointer over to our new page, GRC.com/mail.



Anyway, the only post-announcement glitch we encountered was from users mostly using Gmail, but also a few other ISPs, I think Virgin Media was one, that use their own domains backed by those services, like Gmail.  But since the email they send comes from that underlying service, like Gmail, rather than from their domain alias, and since the incoming filter that's in front of the securitynow@grc.com mailbox looks to see whether the sender is known to us, listeners need to register their underlying Gmail account at GRC, not their aliased account, which is the one that's, you know, shown in the email From: header of their email.



So some people were going over to the GRC.com/mail page and putting in their account name and their own domain, even though it's a front for Gmail.  It turns out that the mail that they send actually comes from Gmail, so that was not an account that we'd ever seen before and so their mail was bouncing.  As soon as I understood what was going on, I added a little comment on the form just to say, you know, for Gmail people, that was like by far the majority of users who were having a bounce problem, that that was what they had to do, and that problem went away.  So people are paying attention to that.



Oh, also, anyone using an anonymizing email service will have a problem.  I received an email from a listener who was using the SimpleLogin email anonymizing service by Proton, which by the way appears to be a very nice service.  When that listener sent email to GRC, the sender's email was this bizarre long one-time 54-character random account name in front of the @simplelogin.com domain name.  So again, GRC's filter had never seen that before, probably will never see it again.  And it bounced that mail back.  So we're not compatible, our approach is not compatible with email anonymizing services.



And I didn't mention it last week, but I actually have at the GRC.com/mail page what I call "The Prime Directive," which is nobody will ever get mail from us that they don't want.  I mean, and I'm serious about that.  We will also never divulge anyone's email address.  Since sending email is a pain, you know, please unsubscribe if you're ever not happy and so forth.  Anyway, to make a long story short, our listeners love the simple solution.  You just register one time.  You optionally subscribe to whatever announcement lists, if any, you may wish.  And then from then on you can simply send email to securitynow@grc.com.  I have been overwhelmed with notes of thanks and congratulations from listeners, and people I've never heard from before who were never going to sign up to Twitter just to maybe send me a note.



You know, and in fairness, Twitter is about so much more than that.  You know, it's about building a community and a following, and following people, and networking.  I had been just using it as a point-to-point instant messaging service, which after all is exactly what email is.  So anyway, needless to say, as I said, I will never share anyone's email address.  Oh, and I did want to say, if somebody writes to me, I will never share your email address when I share your feedback.  And anyone requesting anonymity for their name, of course I will honor that.



Now, I should mention, and Leo I remember you mentioning this, too, when we first talked about it, one of the nice things about GRC's now-retired web form was that it solicited our listeners' location.  And it was nice being able to include that when sharing feedback, you know, since it made the email feel a little bit more personal.  So if you happen to think of it, let me know where you're writing from when you send me a note, and I'll just sort of toss that in when I share your feedback.



LEO:  I'm wondering, you said it has to have the same domain as the server.  So most email clients will let you choose a personality that says - so, for instance, I might be running on Gmail, but my email, I would like it to be leo@leoville.com.  I can choose leo@leoville.com as my personality in Gmail.  And even though it's originating from the Gmail server, it should look to you, to your server, like leo@leoville.  You don't look at the underlying outbound server; do you?  Maybe you do.



STEVE:  Yes.  I actually do.



LEO:  You do.



STEVE:  Yes, because...



LEO:  Instead of just the email address, the reply-to address, in other words.



STEVE:  Yeah.  The problem is the reply-to address is trivially spoofable.



LEO:  Of course.



STEVE:  And so I wanted something that is a little less spoofable.



LEO:  Okay.



STEVE:  I have a thread that I've not yet caught up in over in the newsgroups to do some brainstorming about whether I ought to change that because it would be easier if I just use the From: address.  And I'm not sure that it really matters because any spammer could certainly be spoofing the Receipt-To address, as well.



LEO:  Right, right.



STEVE:  So I may rethink that and change that, just to make it a little bit easier for...



LEO:  That's a good advisory.  You have to use the email address that your service provides.



STEVE:  Correct.



LEO:  As opposed to any personality, any identity that you use.



STEVE:  Correct.  And we ran across that with Gmail people and also, as I mentioned, SimpleLogin people.  It's an anonymizing service from Proton.  They also had to do that.  But really, after I explained it, we stopped having anymore problems with signup.  So my current work, this moment, you know, this evening, is to finish up automating and catching real-time email bounces.  So I could immediately inform someone when GRC is able to detect that it was unable to successfully deliver their authentication loop email.  Once that's in place, I'll stick my toe in the water to begin actually sending email in today's spam-conscious climate.  You've got to be careful.  And so we'll ramp up from there.



So anyway, I wanted to thank everybody for their support.  You know, everyone's interest is the reason I became convinced that we need to keep this going past 999.  And, you know, here we are, already at 977, with our 20th birthday coming up in August.



LEO:  Yeah, see?  Yeah.



STEVE:  Yeah.



LEO:  Old doesn't mean in the way.



STEVE:  While I was writing the note above yesterday, I received an email alert from Troy Hunt's "Have I Been Pwned?" email breach monitoring service.  The email's Subject was:  "16 emails on GRC.com have been pwned in the Telegram Combolists data breach."  Okay.  The breach occurred one week ago on May 28th.  In the breached data - get this, Leo - 361,468,099 email accounts were found.  And HIBP (Have I Been Pwned) sent this email because 16 of those 361 plus million belonged to GRC.com.



The description of the breach that Troy included said:  "In May 2024, two billion rows of data with 361 million unique email addresses were collated from malicious Telegram channels.  The data contained 122GB across 1,700 files with email addresses, usernames, passwords and in many cases, the website they were entered into."



LEO:  Does Troy email every one of those addresses?  How did you get that?  He must; right?  Or do you sign up for some?



STEVE:  No, no.  Yes.  So I subscribed to a domain-wide free - it doesn't cost anybody, so I would recommend this.  It's domain-wide.  So you would, you know, do Leoville.com and TWiT.tv.  And then you have to prove ownership of the domain.  And once you do, anytime Troy gets a hold of any new breach data, he'll scan the email addresses in the breach content and then notify you of any hits which may be one of your active email addresses having just been disclosed.



Okay.  So he said:  "In this case the data contained from this Telegram Combolists data breach, 122GB across 17,000 files, with email addresses, usernames, passwords, and in many cases the website they were entered into."  He said:  "The data appears to have been sourced from a combination of existing combolists and info-stealer malware."  And we'll be hearing a little bit more about info-stealer malware because that comes up when we're talking about Recall again. 



Okay.  So naturally I went over, after I received this email from him, to see whether any of those 16 addresses which HIBP reported were of concern.  Okay.  The short version is none were.  The longer version is the only two that were ever valid were greg@grc.com and offices@grc.com, neither of which we have used for decades.  I once watched a spammer's server connect to GRC's email server and just run down a list of first names, just, you know, abigail@grc.com, amantha@grc.com, and so forth, A through Z, hoping to get lucky.  Immediately after that we retired our original and, you know, oh-so-very-innocent use of our first names for email.  That just became impractical.



The wonderful open source email server I've been using for years is known as hMailServer.  Anyone looking for an utterly solid, feature-packed, no nonsense, free, Windows-hosted email server should look no further.  There really is nothing comparable.  I know lots of people run, you know, Sendmail and Postfix and so forth over on Linux.  And I get that.  Those are certainly mature platforms, too.  But Windows hMailServer.  It's another of those rare software creations that has no bugs.  Just like John Dvorak gets no spam, this thing has no bugs.



The only time it's been updated for years is to keep up with improvements in the OpenSSL library which it uses to make its TLS client and server connections.  And in fact I updated it just last week after many years of trouble-free service only to obtain support for TLS 1.3, which I did not have in my previous instance.  And remember, 1.2 appears to be fine.  You know, 1.3 exists.  It's real.  People should support it.  But 1.2 ain't going away anytime soon because it's still, what is it, 86% of connections or something like that.



Anyway, hMailServer has a dynamic blocklist feature that will block for a configurable period of time any remote server by IP address that attempts to deliver email to any nonexistent address, in my case at GRC.  I just checked the server when I was writing this yesterday.  I currently have the blocklist expiration set for two hours.  And at the moment I checked, 473 individual IP addresses were currently being blocked.  So within the previous two hours, 473 different spamming SMTP servers had connected to GRC and attempted to send spam.  Not to actually, you know, not even to any valid email address, but just to throw crap at the wall, hoping to get lucky.



Now, GRC's been around a long time.  The domain is well-known.  But we're certainly not particularly high-profile.  And it so saddens me, Leo, to see, sadly, I mean, really, what a sewer our beloved Internet has become.  I'm unsure what it teaches us about humanity, but I'm pretty sure I don't want to know.



LEO:  Yeah.  I think it just reflects humanity.  That's the problem.



STEVE:  Yeah.



LEO:  As we go along, it's more and more like the people who make it.



STEVE:  Yes, yes.  The trifecta of the Internet being anonymous, global, and free, those three things, enables every last miscreant on Earth to attempt to have their way with everyone else.  Fortunately, the rest of us are far from powerless, and we have this podcast to help us stay ahead of the tidal wave of incoming crap that's out there pounding on the door, trying to get in.  You know, we're not going to let any of that in.



LEO:  No.



STEVE:  Okay.  Okay.  So I want to talk about a new supply chain attack vector.  But let's take a break first, and then we will get into some security news of the week.



LEO:  All right.  I think you should write a manifesto, Steve.  We're mad as hell, and we're not going to take it anymore.



STEVE:  Well, we're going to stamp on it.  We're going to hide behind our NAT routers and hope that all that junk out there - I mean, come on, 473 servers just hooking up to GRC in the course of two hours, spewing junk at it.



LEO:  It's mindboggling; isn't it.



STEVE:  It's just...



LEO:  It's just amazing, yeah.



STEVE:  It's really sad.



LEO:  Yeah.  It's the world we live in, I'm sorry to say.  Well, you know, one good thing about doing this show is because you focus on all this stuff, we have the best sponsors when it comes to security; right?  They flock to us.  In fact, we talk to people all the time that say, hey, can I be on Security Now!?  And most of the time I'm happy to say we have to say, no, it's sold out for the next quarter.  All right, Mr. G.  On we go with the show.



STEVE:  And speaking of what a sad mess the greater Internet has become...



LEO:  Yes.



STEVE:  ...and of not letting any of that mess into our lives, one of our listeners, Terence Kam, pointed me to a recent piece in BleepingComputer titled "Cybercriminals pose as 'helpful' [in air quotes] Stack Overflow users to push malware."  Okay, now, for those who have never encountered it, Stack Overflow is a forum community of developers of widely ranging skill.  It's essentially a place where coders can help one another.  When I've been struggling with a programming problem, such as when I was working to get server-side on-the-fly code signing to work remotely with a certificate stored in an HSM, which as far as I know no one has ever done before, the Stack Overflow site would often be listed among Google's search results.  And I'm a member there, since I've enjoyed answering questions and giving back when I can.



So BleepingComputer writes:  "Cybercriminals are abusing Stack Overflow in an interesting approach to spreading malware - answering users' questions by promoting a malicious PyPI package that installs Windows information-stealing malware.  Sonatype researcher Ax Sharma (who's also a writer at BleepingComputer) discovered this new PyPI package is part of a previously known 'Cool package' campaign, named after a string in the package's metadata, that targeted Windows users last year.



"This PyPI package is named 'pytoileur' and was uploaded by threat actors to the PyPI repository over the weekend, claiming to be an API management tool.  Malicious packages like this," they write, "are usually promoted using names similar to other popular packages," you know, a process we've talked about before known as typo-squatting.  "However, with this package, the threat actors took a more novel approach by answering questions on Stack Overflow and promoting the package as a solution.  As Stack Overflow," Bleeping Computer writes, "is a widely used platform for developers of all skill sets to ask and answer questions, it provides a perfect environment to spread malware disguised as programming interfaces and libraries.



"Sonatype's Ax Sharma said in their report:  'We further noticed that a Stack Overflow account' - it had a nonsense name of  EstAYA G - 'created roughly two days ago is now exploiting the platform's community members who are seeking debugging.  It's directing them to install this malicious package as a "solution" - again in air quotes - 'to their issue, even though the "solution" is unrelated to the questions being posed by developers.'



"In this case, the pytoileur package contains a setup.py" - you know, Python - "file that pads a Base64 encoded command which executes with spaces, so that unless you enable word wrapping  in your IDE, you know, your Integrated Development Environment, or text file editor, this Base64 blob will be pushed all the way out past the right margin and offscreen so you'll never see it.  When that blob of Base64 is deobfuscated, the command will download an executable named 'runtime.exe' from a remote site and run it."



They write:  "This executable is a Python program converted into an .exe that acts as an information-stealing malware to harvest cookies, passwords, browser history, credit cards, and other data from the users' web browsers.  It also appears to search through documents for specific phrases and, if found, steals the data in them, as well.  All of this information is then sent back to the attacker, who can sell it on the dark web markets or use it to breach further accounts that are owned by the victim."



They said:  "While malicious PyPI packages and information-stealers are nothing new, the cybercriminals' strategy now to pose as helpful contributors on Stack Overflow is an interesting new approach as it allows them to exploit the site's trust and authority within the coding community.  This approach serves as a reminder of the constantly changing tactics of cybercriminals and, unfortunately, illustrates why you can never blindly trust what someone shares online.  Instead, developers must verify the source of all packages they add to their projects and, even if it feels trustworthy, check the code" - and they said "with word wrap enabled" - "for unusual or obfuscated commands which will be executed."



I have a picture in the show notes of the window.  And you can where there is a Python class named "install command," and then a definition of run which is going to print something, and then you can see a big bunch of white space.  Well, that's all spaces that will push this huge green blob of Base64 encoded code far off to the right so that, if someone did not have word wrap enabled, they'd never see this.  They would look at it and go, huh.  Well, okay.  I don't quite get what it's doing, but looks fine.  Nothing bad there.  When in fact there's a big blob of badness which the exec function will deobfuscate and then run.



So anyway, I'll just note that before the end of today's podcast, the security researcher Kevin Beaumont is going to show us, despite Microsoft's claims to the contrary, that the database underlying Microsoft's new Recall system can, in fact, be exfiltrated remotely, does not require system privilege, and can be accessed by any other user on the same machine.  That means that Recall's SQLite database is 100% vulnerable to exactly this sort of info-stealing malware.  So it's not like Microsoft has created some miracle that is going to protect this database.  And we'll be talking about more of that in a minute.



So in other news, we have another certificate authority in the doghouse.  Google has announced that it will be removing its trust of all new TLS certificates issued by the Austrian certificate authority GlobalTrust.  Rather than yanking GlobalTrust's root certificate, which would invalidate all previously-issued GlobalTrust certs, Google will be using a recently added new feature that allows it to manage certificate trust based on certificate issue dates.  So Chrome will not be trusting any new certificates issued by GlobalTrust after the end of this month, June 30th.



Now, through the nearly 20 years of this podcast we've seen and discussed a range of misbehavior on the part of those who have been given the privilege of essentially printing money.  Certificate authorities charge their customers hundreds of dollars in return for encrypting a hash of a small block of bits that the customer presents.  But in return for this money-printing privilege, the CA must abide by a significant code of conduct.  When that code is broken, and only after bending over backwards with more than ample warnings, the industry can and has summarily withdrawn its trust from the signatures of those CAs on the grounds that, if the CA cannot be trusted, neither can anything they have signed.



In this case GlobalTrust has established a multi - well, "established" is an interesting choice of my words - a multi-year history of misconduct, and they've lost the trust of the industry.  Google will be enforcing a ban retroactively on all Chrome versions down to 124.  So lots of previous Chrome versions.  I don't know who would not be keeping their version of Chrome up to date, but okay.  And the other browser makers have not yet announced a similar decision, although Mozilla appears to be aware of the problems with GlobalTrust and is concerned.



On the other hand, since no customer would purchase a certificate for a web server which anyone visiting with Chrome would be unable to connect to securely, this immediately puts GlobalTrust out of the business of selling web server certificates.  In other words, whether or not Apple and Mozilla should choose to follow, GlobalTrust is done for now, at least on the TLS web server certificate business.  They may be selling lots of certificates for other purposes, but not for any Chrome browsers in the future.



Those of us who have been around since the dawn of the Internet will likely remember the first successful instant messaging app known as ICQ.  It was meant to be short for "I seek you."  The system was originally developed back in 1996 by an Israeli company named Mirabilis.  I practiced pronouncing it earlier, and now I can't do it.  Mirabilis.



LEO:  No, Mirabilis.



STEVE:  Mirabilis.  I thought - okay, right.  Mirabilis.



LEO:  Mirabilis.



STEVE:  Two years after it was created - ICQ was created by AOL in 1998, and then by the Russian Mail.ru Group in 2010.  It had a neat kind of funky flower petal logo, and I've sort of thought of it like through the years, wondering whatever became of it.  At its peak around 2001, it had more than 100 million accounts registered.



LEO:  Wow.



STEVE:  And nine years later, when AOL sold it to Mail.ru, it had around 42 million daily users.  And it has been puttering along in the background ever since.  Two years ago it had dropped to around 11 million monthly users.



And finally, the reason the subject came up is that a week and a half ago, on May 24th, the website of ICQ.com announced that the service would be shut down about three weeks from now, on June 26th, 2024.  So it had a pretty good 28-year run for an instant messaging service that was largely passed by when smartphones and other major social media service got into the game.  But it was there from the beginning and kind of cool.



Okay.  Now, completely off topic, but this has been something that I've been wanting to just make sure everybody knew about for a while.  My wife recently agreed to join me in watching one of my favorite science fiction movies of all time.  We know I'm a pushover for science fiction.  But unfortunately, far more horrible science fiction movies have been made than good ones, and even more rare is the perfect science fiction movie.  So we settled down to watch "Dj Vu" which stars...



LEO:  I feel like I've seen it before.  I don't...



STEVE:  You probably have, Leo.  It's not new.  And yes, I get your...



LEO:  Okay, just checking.  Actually, I don't feel like I've ever seen it.



STEVE:  Oh, no kidding?



LEO:  I don't usually think of Val Kilmer and Denzel Washington as being sci-fi stalwarts.



STEVE:  Oh, Leo.



LEO:  Oh, all right.



STEVE:  Okay.  So listen to - okay.  So Denzel Washington, Val Kilmer, and some other recognizable actors from Hollywood's inventory.  As I was watching it for maybe the fourth time, I kept thinking over and over, you know, it is - as I was watching this perfectly and often leisurely paced two-hour movie unfold scene by scene, and everything was happening exactly the way it should, that I was sitting here watching one of the all too rare perfect movies.  This movie offers convincing acting that's not distracting, a brand new and perfect concept, a perfect script, and a plot that's both surprising and where what happens is better than someone steeped in science could have ever hoped for.  The writers enlisted the help of Brian Greene, a Cornell and Columbia University physicist, to get the science right.  And boy, did they.  You know, that's part of what's so gratifying about this movie.  Now, as I said, it's not a new movie.  It was released 18 years ago, back in 2006.  But it stands up, and it feels 100% contemporary.



I realized that since this podcast is closing in on its 20th birthday, every time I've seen this movie I've done this podcast a few days later, yet somehow I've never thought to mention it.  I searched our transcripts, and there was no mention of it.  So, you know, that's my bad, and that's fixed now.  I know quite well that not everyone's taste is the same.  Not everyone will feel as I do about this.  But if you don't already know this movie - and Leo, I guess you don't...



LEO:  Lisa said she's seen it.  So it'll be dj vu for her, but it will be whatever it is, premiere view for me.



STEVE:  It is just so good.  I just...



LEO:  I'm watching it tonight.  I need something to watch.



STEVE:  It is wonderful sci-fi.



LEO:  I love Denzel, of course.



STEVE:  And, yes, I do, too.  And it will not disappoint you.



LEO:  Okay.  Thank you.  Finally, something to watch tonight.



STEVE:  So our listener Jeff Price, he wrote and said:  "Leo touched on this, but Fastmail allows you to create these unique random email addresses.  What most people forget is Apple lets you create these, as well.  They call it Hide My Email."  So I just wanted to share Jeff's note since I have the feeling email aliasing services are going to become increasingly popular as websites turn to collecting and sharing whatever they can about their visitors as a means of increasing their advertising revenue, you know, as third-party cookies and as Google tries to promote their sandbox anti-tracking technologies.



Kirk Sexton wrote:  "Hi, Steve.  Great work on the new email system.  I never miss a show.  I listen on my morning runs and in the car on my way to work.  Sometimes I have to run a little further or sit in my car for a few minutes longer after arriving so I don't interrupt a point before hitting pause.



"I may have missed this point, but I don't recall hearing anything about those users who sync their accounts on Microsoft OneDrive, or for that matter use other cloud-based backup services."  And he's talking about Recall.  He says:  "Backing up files is one thing.  It would be expected that anything committed to local storage will be backed up to the subscribed cloud storage.  However, temporary information that is used just for the moment will now be stored locally - think passwords, credit cards, or other sensitive information - within the screen grabs.  Microsoft has said it will only be stored locally, but what about cloud-syncing with OneDrive or other services?  I see it as the problem just mushrooming into multiple attack vectors.  Am I missing something?"  And he finished:  "To 999 and beyond!  All the best, Kirk Sexton."



So Kirk raised a great point, I think.  We're about to spend the rest of the podcast looking at what one security researcher found and also about what may be Microsoft's significantly greater plan beyond what they've announced.  But everything we know now suggests that the Recall data are just SQLite files stored under the user's AppData directory in a new folder called "CoreAIPlatform."  Microsoft has indicated that BitLocker will be used to encrypt the data at rest.  But online backups are made of live unencrypted data so that they can later be retrieved.  And there's nothing we know so far that would prevent anything that was backing up a user's machine from also backing up their machine's Recall history.  So, you know, there just seems to be so many things that have not been well thought through here.



Okay.  And then just one piece of feedback.  I'm way far behind, just so everybody knows.  The first week of listener feedback email was intense, with many listeners, you know, wanting to say hi, to express their happiness there's now a way to send me thoughts without engaging in social media.  So, yeah, as I said, I'm way behind.  But I figured I'd share one piece of feedback that's primarily about a SpinRite owner's experience, first with SpinRite 6.0, or by comparison with SpinRite 6, and then with 6.1.



Our listener Mark Jones sent email with the subject "Wow!  SpinRite 6.1 is amazing."  He wrote:  "Dear Steve.  Long-time listener, occasional source of feedback."  He says:  "(I was @mjphd on Twitter.)  I'm so happy to be using email. I only kept my X account for Security Now! feedback."  He said:  "I've listened to you discuss both the speed of 6.1 and the magic it does on an SSD.  Ever the experimentalist, I thought I would put it through its paces.  I have two drives, a 1TB spinner and a 250GB SSD that seemed to have slowed.  The results are nothing short of remarkable on both drives.  In only four hours, the 1TB was rejuvenated.  That would have taken days using SpinRite 6.  The boot into Windows 10 is now seconds instead of minutes, and the random slowdowns that were plaguing the system are gone.



"The real miracle was on the SSD.  The new drive test showed I was at 19 MB/s at the front and middle, and 80 MB/s at the end."  So 19 front and middle, 80 MB at the end.  "The whole drive is now over 546 MB/s after a level 3 scan.  Saying computer performance has returned feels inadequate.  It's mind-blowingly fast compared to yesterday.  Truly amazing.  Thanks for the great work, and I'm happy there will be a future past 999.  Regards, Mark Jones."



Okay.  So let's talk about Recall again because we have additional information.  And Leo, I'll find a point to pause here.



LEO:  Sure.



STEVE:  For our final.



LEO:  Yeah.



STEVE:  Okay.  So I think that a data-driven theory about Microsoft's future plans for this technology emerged after I read a recent posting by a well-known and well-informed security researcher named Kevin Beaumont.  Since last week's episode, which I titled, as we know, "The 50 Gigabyte Privacy Bomb," Kevin, whom we often quote and refer to, has again weighed in on Microsoft's new Recall facility.  His first posting on the subject, which he made on May 21st, immediately following Microsoft's announcement, was titled "How the new Microsoft Recall feature fundamentally undermines Windows security."



As a mature, seasoned, and experienced security researcher, his immediate "What could possibly go wrong?" reaction to the idea of having Windows continually recording and storing our PCs' screens echoes my own.  It's immediately obvious to anyone who's been around the block a few times that this is, indeed, a 50GB privacy bomb.  What wasn't clear to me until just yesterday was why Microsoft may be doing this, and what they probably have planned for the future.  We'll get to that.



Ever since his immediate posting in reaction to the announcement of Recall, Kevin has been playing with it.  After reading what Kevin wrote, a light bulb went off for me.  So I'm first going to share Kevin's follow-up piece which further describes Recall in much more detail.  Then I'll share what I think it really means.  Kevin titled his follow-up piece, which he posted four days ago, after spending a week and a half with Recall:  "Stealing everything you've ever typed or viewed on your own Windows PC is now possible with two lines of code inside the Copilot+ Recall disaster."



Okay.  Now, before switching into Q&A mode, which he does later, Kevin began his newly informed discussions of Recall by writing this.  He said:  "I wrote a piece recently about Copilot+ Recall, a new Microsoft Windows 11 feature which  in the words of Microsoft CEO Satya Nadella  takes 'screenshots' of your PC constantly, and makes it into an instantly searchable database of everything you've ever seen.  As he says, it is a photographic memory of your PC life.  I got hold of the Copilot+ software and got it working on a system without an NPU about a week ago, and I've been exploring how this thing works in practice.  So we'll have a look into that shortly.  First, I want to look at how this feature was received as I think it is important to understand the context.



"The overwhelmingly negative reaction has probably taken Microsoft leadership by surprise.  For almost everybody else, it wouldn't have.  This was like watching Microsoft become an Apple Mac marketing department.  At a surface level, it is great if you're a manager at a company with much to do and too little time as you can instantly search what you were doing about a subject a month ago.  In practice, that audience's needs are a very small - tiny, in fact - portion of Windows overall user base.  And frankly, talking about screenshotting the things people in the real world, not executive world, are doing is basically like punching customers in the face.  The echo chamber effect inside Microsoft is real here, and oh, boy.  Just oh, boy.  It's a rare misfire, I think," Kevin wrote.



He said:  "I think Recall is an interesting, entirely optional feature with a niche initial user base that would require incredibly careful communication, cybersecurity, engineering, and implementation.  Copilot+ Recall does not have any of these.  The work has clearly not been done to properly package it together.  A lot of Windows users just want their PCs so they can play games, watch porn, and live their lives as human beings who make mistakes that they don't always want to remember.  And the idea other people with access to the device could see a photographic memory is very scary to a great many people on a deeply personal level.  Windows is a personal experience.  This shatters that belief."



Okay, now, I thought Kevin's take on this was interesting.  His observation that Microsoft appears to be oblivious to the fact that not all users of PCs are even close to being the same.  That a manager in a corporate environment might indeed find it useful to be able to look a month back for some specific work subject, but that for the common user - the rest of us - the idea that our machines are watching and recording everything we do, even if it would only be for our own later access, is mostly just creepy.  You know, we don't know the future.  We don't know what's going to happen a month or two from now.  But Recall would make what's happening on our machines now available to that unknown future.



Anyway, Kevin finishes his lead-in by writing:  "I think they're probably going to set fire to the entire Copilot brand due to how poorly this has been implemented and rolled out.  It's an act of self-harm at Microsoft in the name of AI; and, by proxy, real customer harm.  More importantly, as I pointed out at the time, this fundamentally breaks the promise of security in Windows.  I'd like to now detail why."  He said:  "Strap in.  This is crazy.  I'm going to structure this as a Q&A with myself now, sourced from comments I've seen online, as it's really interesting seeing how some people hand-wave the issues away."



Okay.  So now Kevin switches into Q&A format.  He asks himself a question.  So the question is someone saying, "Well, the data is processed entirely locally on your laptop; right?"  Answer:  "Yes.  They made some smart decisions here.  There's a whole subsystem of Azure AI, et cetera, code that processes on the device."  Okay, question:  "Cool, so hackers and malware can't access it; right?"  And he says, "No, they can."  



Q:  "But it's encrypted."  A:  "When you're logged into a PC and run software, things are decrypted for you.  Encryption at rest only helps if somebody comes to your house and physically steals your laptop.  That's not what criminal hackers do.  For example, info-stealer trojans, which automatically steal usernames and passwords, have been a major problem for well over a decade.  Now these can be easily modified to support Recall."



Q:  "But the BBC said data cannot be accessed remotely by hackers."  A:  "They were quoting Microsoft, but this is wrong.  Data can be accessed remotely.  This is what the journalist was told for some reason."  And then he has a snippet from the journalist that says:  "That is what Microsoft told me, that attackers would have to get physical access to your laptop and sign into it to get hold of the screenshots."  Kevin says:  "Not true."  The questioner says:  "Microsoft say only that user can access the data."  Kevin:  "That is not true.  I can demonstrate another user account on the same device accessing the database."



Okay.  The question:  "So how does this work?"  Kevin answers:  "Every few seconds, screenshots are taken.  These are automatically OCR'd by Azure AI, running on your device, and written into an SQLite database in the user's folder.  This database file has a record of everything you've ever viewed on your PC in plaintext.  OCR is a process of looking an image and extracting the letters."  Question:  "What does the database look like?"  And Kevin shows some screenshots like those that we saw last week.  Just looking like, you know, a SQLite database with rows and columns, recognizable filenames.



Question:  "How do you obtain the database files?"  Answer:  "They're just files in AppData, in the new CoreAIPlatform folder."  Q:  "But it's highly encrypted, and nobody can access them; right?"  A:  "Here's a few seconds of video of two Microsoft engineers accessing the folder."  And then Kevin quotes an earlier Mastodon post of his at cyberplace.social where he notes that the Risky Business episode on Recall is good, but with one small correction:  Recall does not need system rights.  He notes that since it's just a SQLite database, it is trivial to access.  And he finishes by saying:  "I'm not being hyperbolic when I say this is the dumbest cybersecurity move in a decade.  Good luck to my parents safely using their PC."



Question:  "But normal users don't run as admins."  Answer:  "According to Microsoft's own website, in their Recall rollout page, they do."  And then he has a snippet from Microsoft.com where it says:  "Making admin users more secure:  Most people," says Microsoft, "run as full admins on their devices, which means..."



So Kevin says:  "In fact, you don't even need to be an admin to read the database.  More on that in a later blog."  Question:  "But a UAC prompt appeared in that video, that's a security boundary."  Kevin replies:  "According to Microsoft's own website and MSRC, UAC is not a security boundary."  And he quotes Microsoft saying:  "More important, Same-desktop Elevation in UAC is not a security boundary.  It can be hijacked by unprivileged software that runs on the same desktop.  Same-desktop Elevation should be considered a convenience feature."  So now Microsoft is saying, oh, well, you know, that's just for convenience.



So the questioner asks:  "So where's the security here?"  Answer:  "They've tried to do a bunch of things, but none of it actually works properly in the real world due to gaps you can fly a plane through."  Question:  "Does it automatically not screenshot and OCR things like financial information?"  A:  "No."  We know that it does.  Q:  "How large is the database?"  Kevin says - and here was one of the first ahas that hit me.  Kevin says:  "It compresses well.  Several days working is around 90KB," nine zero kilobytes for several days of work.  He says:  "You can exfiltrate several months of documents and key presses in the space of a few seconds with an average broadband connection."



Question:  "How fast is search?"  He says:  "On device is really fast."  Question:  "Have you exfiltrated your own Recall database?"  A:  "Yes. I have automated exfiltration, and made a website where you can upload a database and instantly search it.  I am deliberately holding back technical details until Microsoft ship the feature as I want to give them time to do something."  He said:  "I actually have a whole bunch of things to show, and I think the wider cyber community will have so much fun with this once it's generally available.  But I also think that's really sad, as real-world harm will ensue."



So question is "What kind of things are in the database?"  A:  "Everything a user has ever seen, organized by application.  Every bit of text the user has seen, with some minor exceptions," he says, for example, "Microsoft Edge InPrivate mode is excluded, but Google Chrome isn't."  He said:  "Every user interaction, for example minimizing a window.  There is an API for user activity, and third-party apps can plug in to enrich data and also view stored data."  Well, that's news, and interesting.  He says:  "It also stores all websites you visit, even if third party."



Question:  "If I delete an email/WhatsApp/Signal/Teams message, is it deleted from Recall?"  A:  "No, it stays in the database indefinitely."  Question:  "Are auto-deleting messages in messaging apps removed from Recall?"  A:  "No, they are scraped by Recall and available."  Q:  "But if a hacker gains access to run code on your PC, it's already game over."  Kevin says:  "If you run something like an info-stealer, at present they will automatically scrape things like credential stores.  At scale, hackers scrape rather than touch every victim, because there are so many, and resell them in online marketplaces.  Recall enables threat actors to automate scraping everything you've ever looked at within seconds.



"While testing this with an off-the-shelf info-stealer," he said, "I used Microsoft Defender for Endpoint, which detected the off-the-shelf info-stealer.  But by the time the automated remediation kicked in, which took over 10 minutes," he notes, "my Recall data was already long gone."



Question:  "Does this enable mass data breaches of website?"  A:  "Yes.  The next time you see a major data breach where customer data is clearly visible in the breach, you're going to presume the company who processes the data is at fault; right?  But if people have used a Windows device with Recall to access the service/app/whatever, hackers can see everything" - he means that the people offering the service have seen, he said - "and assemble data dumps without the company who runs the service even being aware.  The data is already consistently structured in the Recall database for attackers.  So prepare for AI-powered super breaches.  Currently credential marketplaces exist where you can buy stolen passwords.  Soon you'll be able to buy stolen customer data from insurance companies, et cetera, because all code required to do this has been pre-installed and enabled on Windows by Microsoft."



Q:  "So did Microsoft mislead the BBC about the security of Copilot?"  A:  "Yes."  Q:  "Have Microsoft misled customers about the security of Copilot?"  A:  "Yes.  For example," he says, "they describe it as an optional experience, but it is enabled by default, and people can optionally disable it.  That's," Kevin says, "wordsmithing.  Microsoft's CEO referred to 'screenshots' in an interview about the product, but the product itself only refers to 'snapshots.'  A snapshot is actually a screenshot.  It's, again, wordsmithing for whatever reason.  Microsoft just need to be super clear about what this is so customers can make an informed choice."



And of course I need to note here that the tyranny of the default will be at work.  We know that whatever is the default setting is what 99.99% of all Windows users will leave active.  I don't know if any of you have seen people using Windows computers, but for some reason they always leave those stickers all over the keyboard.  And I just, I can't believe it.  It's like you realize the computer will still work if you peel those stickers off the keyboard.  You don't need to be, you know, advertising the crap that came from the manufacturer.  But anyway, the tyranny of the default.



So question:  "Recall only applies to one hardware device."  Kevin replies:  "That's not true.  There are currently 10 Copilot+ devices available to order right now from every major manufacturer.  Additionally, Microsoft's website says they're working on support for AMD and Intel chipsets.  Recall is coming to Windows 11."  Q:  "How do I disable Recall?"  A:  "In initial device setup for compatible Copilot+ devices out of the box, you have to click through options to disable Recall.  In enterprise, you have to turn off Recall as it is enabled by default."



Q:  "What are the privacy implications?  Isn't this against GDPR?"  Kevin replies:  "I'm not a privacy person or a legal person.  I will say that privacy people I have talked to are extremely worried about the impacts on households in domestic abuse situations and such.  Obviously, from a corporate point of view, organizations should absolutely consider the risk of processing customer data like this.  Microsoft won't be held responsible as the data processor, as it is done at the edge on your devices.  You are responsible here."



The question:  "Are Microsoft a big, evil company?"  Kevin:  "No."



LEO:  Hell, yes.  Oh.



STEVE:  "That's insanely reductive."  He says:  "They're super smart people, and sometimes super smart people make mistakes.  What matters is what they do with knowledge of mistakes."  So the question:  "Aren't you the former employee who hates Microsoft?"  Kevin says:  "No.  I just wrote a blog this month praising them.  It was 'Breaking down Microsoft's pivot to placing cybersecurity as a top priority.'  My thoughts on Microsoft's 'last chance saloon' moment on security."



So we have a couple, just two more.  Question:  "Is this really as harmful as you think?"  Answer:  "Go to your parents' house, your grandparents' house, et cetera, and look at their Windows PC.  Look at the installed software in the past year.  Try to use their device.  Run some AV scans.  There's no way this implementation does not end in tears.  There's a reason there's a trillion dollar security industry, and that most problems revolve around malware and endpoints."  Q:  "What should Microsoft do?"  Answer:  "In my opinion, they should recall Recall and rework it to be the feature it deserves to be, delivered at a later date.  They also need to review the internal decision-making that led to this situation."



He says:  "This kind of thing should not happen.  Earlier this month, Microsoft's CEO emailed all their staff, saying:  'If you're faced with the tradeoff between security and another priority, your answer is clear:  Do security.'"  He said:  "We will find out if he was serious about that email.  They need to eat some humble pie and just take the hit now, or risk customer trust in their Copilot and security brands.  Frankly, few if any customers are going to cry about Recall not being immediately available, but they are absolutely going to be seriously concerned if Microsoft's reaction is to do nothing, ship the product, slightly tinker, or try to wordsmith around the problem in the media."  Okay.



LEO:  Seems like a great piece.  I mean, I've read it, and I was very impressed.



STEVE:  Yup.



LEO:  And he makes a strong case.  The one thing that's a question mark, a lot of the things he describes sounds like you had to be on the physical PC.  But he says you don't.  So malware would be able to escalate the UAC and do all those things, look across accounts.



STEVE:  Yup.



LEO:  All of that stuff.  Okay.  So the real issue is, if malware gets in your system, they've got access to everything you've done.



STEVE:  Right.  There is now much more that it has access to.  Let's take our final break, and then I'm going to talk about what I think is really going on.



LEO:  Yeah.  Why would Microsoft do all this?



STEVE:  Yup.



LEO:  What's the plan here?  Hmm.



STEVE:  I think there is one.



LEO:  All right.  Steve.  You've set us up well.  Obviously this is a bad idea.



STEVE:  Why would they do it?



LEO:  But Microsoft's going full speed ahead with it.  Why?



STEVE:  Okay.  So we now know that Microsoft currently plans to enable this whole PC history recording by default.  They also know that unless Windows ships with it enabled and running, no one will use it.  So they want to blow everyone's mind by AI-enabling Windows PCs somehow, and this is what they've come up with.  I doubt there's an informed security-minded technologist anywhere who doesn't think this is a very bad idea.  Yet until we learn otherwise, this is exactly what Microsoft intends to do.  Now, I have to say I have some personal experience with endeavoring, and failing, to get Microsoft to change its plans.



LEO:  Can anybody say "raw sockets"?



STEVE:  Uh-huh.  Before their release of Windows XP, which grew out of Windows 2000, I tried to keep Microsoft from shipping XP with the totally unnecessary access to raw sockets available to the operating system's client software.  They ignored me until the MSBlast worm would have taken them off the Internet had it  not been targeted at the wrong domain.  After that near-death brush with being attacked by an entirely unnecessary feature of their own operating system, XP's Service Pack 3 removed unprivileged access to raw sockets, and no one cared.  The fact that no one cared demonstrated that the unnecessary feature should have never been present in a consumer OS.  Raw sockets never came back because they just beg to be abused.



Okay, now, I learned my lesson from that experience.  I have no interest in lobbying Microsoft to change its behavior.  You know, Microsoft is like Godzilla.  It does whatever it wants to do.  All anyone can do is stay out of its way.  But what's so odd about this moment where we find ourselves is that they have just made all this noise about how security is now job number one.  And Kevin quoted Satya Nadella saying:  "If you're faced with the tradeoff between security and another priority, your answer is clear:  Do security."  Except they're not.  The entire security industry is jumping up and down, waving their arms, and saying "Don't do it," exactly as I once did before with XP.  Yet Microsoft is certain that they know better.



Now, it's interesting that Kevin believes that the screen is being OCR'd.  I strongly doubt that's actually the case, at least not unless an actual JPEG or PNG-style graphic image is being displayed, in which case OCRing the image would be the only choice.  As I noted last week, hooking into the Windows API that paints text onto the screen would be far more efficient.  Behind every character glyph, what we see on the screen is a 16-bit Unicode character which was rendered through a chosen font and turned into clear-type colorized pixel text.  There's just no reason to look at the pixels of a screen that was just rendered from Unicode and try to determine which characters they are.  So my assumption would be that the textual output graphic API is being hooked and intercepted by Recall.



It was also very interesting to learn how economical Recall's storage is.  This makes sense if it's storing and compressing text, since we know how much redundancy exists in linguistic text.  But Kevin said that several days' worth of work compresses to around 90KB of database storage.  If we take Kevin's "several days" to mean two, then that's around 45K of storage required per day.  That means that 50GB of storage allocation, consumed at the rate of 45K per day, would yield 3,042 years' worth of storage.  I'm sure we'll learn more going forward, but I don't think Recall will be storing the past 90 days of a PC's use.  It appears that it will always be recording the PC's entire life of use.



That's why the title of Kevin's second post makes far more sense.  His title began "Stealing everything you've ever typed or viewed on your own Windows PC."  And I think that's exactly what Microsoft is actually planning to do.  If they're able to capture and compress all the text displayed on Windows 11 screens, and given the explosion in local mass storage capacity and the efficiency of text compression, they clearly have the storage capacity to capture everything for all time.



And this brings us to the title I gave today's podcast:  "A Large Language Model in Every Pot."  Why would Microsoft want to be capturing every single thing a user types and views on their own PC throughout its entire lifetime of use?  I have a theory.  Microsoft wants to make a big splash in AI.  So how about using all of that data to train an entirely personal local large language model?  What if a future local large language model was not just used to index and search your PC's history timeline, but was continually being trained across your entire corpus of personal data so that it would be possible to conversationally interact with your own personal AI that has grown to know you intimately because it has been watching and learning everything you've been doing for years?  It would "know," and I have "know" in air quotes, everything you had ever entered into its keyboard and displayed on its screen.  The entire history of that machine's use would become an ever-growing corpus that is continually training the model.



That would completely and profoundly forever alter a user's interactive experience with their PC.  It would be a true game changer.  It would be transformative of the PC experience.  And if Microsoft has that up its sleeve, I can see how and why they would be super excited about Recall, even though Recall would be just the beginning.  Even if the local large language model technology is not yet ready for delivery, the time to begin capturing all of a user's use of their machine is as soon as possible.  That begins creating the corpus that will be used to train a future personal local large language model.



If this view of the future is correct, there's one large and glaring problem with this, which Kevin highlights and which Microsoft is conveniently ignoring because they have no choice but to ignore it.  What Microsoft must ignore is that the actual security of today's Windows is a catastrophe.  Microsoft has not been paying more than begrudging and passing attention to security while they've been busily adding trivial new feature after new feature and never getting ahead of the game.



Last month's Patch Tuesday saw Microsoft patching 61 newly recognized vulnerabilities, 47 of them in Windows, and another 25 for anyone paying for extended security updates.  44% of those were remote code execution, 11% were information disclosure, and 28% were elevation of privilege - none of which suggests that Windows would be a safe place to store the data that will be used to drive an entity that can be queried about nearly any aspect of you and your life which it has observed throughout the entire history of your use of that machine.



If this is indeed what Microsoft is planning, and having voiced it now it's difficult to imagine that it's not exactly what they're planning, then this really is a double-edged sword.  The world stumbled upon the startling power of large language models, which Microsoft just so happens to own a big chunk of, and someone inside Microsoft realized that by leveraging the power of next-generation neural processing units, it would be possible to train a local model on the user's entire usage history of their computer.  And that would create a personal assistant of unprecedented scope and power.



I would wager that, today, the smarter people within Microsoft are wishing, more than anything else, that instead of screwing around with endless unnecessary features and new unwanted versions of Windows, they had been taking the security of their existing system seriously.  Because if they had, they would own a secure foundation and would stand a far greater chance of successfully protecting the crown jewels of a user's computer usage legacy.  Instead, what they have today is a Swiss cheese operating system that is secure only so long as no one really cares what its user has stored.  Depending upon who the user is, the data that will be accumulated by Recall will represent a treasure that is certain to dramatically increase the pressure to penetrate Windows.  The entire professional security community understands this, which is why it's going batshit over Recall, while Microsoft has no choice other than to deny the problem because they're desperate to begin the data aggregation of their users so that it can be used to train tomorrow's personal PC assistant AIs.



So Microsoft will declare, as they always do, that Windows is more secure than it's ever been, even though history always shows us afterward that's never been true.  Microsoft is going to have Recall installed, running, and collecting its users' data in all forthcoming qualifying Copilot+ Windows 11 PCs.  And don't get me wrong.  The idea of being able to ask a built-in autonomous personal AI assistant about absolutely anything we've ever typed into or seen on our computer is intoxicatingly powerful.  For many of us who live much of our lives through our computers, it would be like having a neural-link extension of our brain with flawless perfect recall.  But it also represents a security and privacy threat the likes of which has never existed before.



When you consider the amount of digital storage that anyone can now easily own, it seems pretty obvious that this is going to happen sooner or later.  Unfortunately, Microsoft has not proven itself to be a trustworthy caretaker of such information.



LEO:  Wow.  I think you're exactly right.  I mean, that's almost what they're proposing anyway is you can always query the machine about everything you've done.



STEVE:  Well, they're saying "timeline," that you can query a timeline.  But if this thing, if they're capturing text from the screen, Leo, and Kevin saw 90KB was stored after several days of use, that means that that 50GB that they want to set aside, this is not a 90-day rolling window which I thought last week.  They're going to store everything you ever do for your entire life of your use of that machine.



LEO:  Right, right.



STEVE:  And in fact you're going to want that to be portable to the next machine you move to so that you're able to take that accrued data with you from one, you know, three years from now when you need to buy a new Windows 13 machine.



LEO:  It could be secured; right?  You could do this right; couldn't you?



STEVE:  Yes.  And what they're doing - I think you could.  I think you could, I mean, you would need new hardware because you need some sort of the equivalent of an HSM.  Basically you'd want this super Jeeves to be in its own enclave that could not be exfiltrated from.



LEO:  Yes, that's right.



STEVE:  Where data goes in, and nothing comes out.  And then, I mean, but it would - imagine that, Leo.  It would be compelling to be able to ask your computer anything that you ever did with it.



LEO:  I'm well aware of that.  That's the...



STEVE:  It's perfect recall.



LEO:  Yeah, the endgame for all of this.  I've even referred back...



STEVE:  And you've been talking about your own local smaller corpuses or corpi and how useful that is. 



LEO:  Right.  And I've talked about...



STEVE:  This would be that.



LEO:  ...the founder of DEC, not the founder, one of the designers of DEC just passed away recently.



STEVE:  Gordon Bell.



LEO:  Gordon Bell, who had the same idea.  He had a camera around his neck.  He wanted to record everything he ever did.  This is ever before we had these powerful LLMs.



STEVE:  And the storage capacity to record our life.



LEO:  Right, right.  Well, the issue always was, and with Gordon's database, is well, okay, I've got it.  What do I do with it?



STEVE:  Right.



LEO:  I can't in a reasonable way parse it.  Well, now we can.



STEVE:  Yes, yes.



LEO:  And so I'm very interested.  I ordered the Limitless Pin which records all our conversations, the idea, same thing, being to allow you to query that.  You know, what did I say to Steve?  I think this is the single most useful persuasive use of AI is as an assistant that knows everything about you.  But, boy, that poses some big problems.  It's almost as if we need an initiative to create a way.  It also solves other problems because data privacy's a huge issue.  We need a way, something that you can - Stacey Higginbotham used to call it The Blob, a place where you could securely, in Secure Enclave, store all your data for your own personal use, not so that other people could invade your privacy, but for your own personal use.  And this is the best possible use.



So I think we're on the right track.  I think this Microsoft implementation could kill it in its tracks.  It could actually have a - this is what worries me is people are moving so fast, with so little regard for safety, that they could have the opposite effect.  They could get people so scared about their security and privacy that they give up entirely on AI.



STEVE:  Well, and they're frankly lying...



LEO:  Yeah.



STEVE:  ...about the security.



LEO:  They're misrepresenting it, yes.



STEVE:  Yes.  I mean, all this is is some files under the user's app directory.



LEO:  Right.



STEVE:  This is not some hocus pocus.  And so everybody knows how to exfiltrate files.  Kevin did it.  There's now a GitHub project that is able to display all your Recall data.



LEO:  Well, I'm glad that he published this paper.  I'm glad you did this show because up to now the press, not knowing any better, and I include myself, we've parroted Microsoft's assertions that, well, it's all on device.  It's all local.  It's all safe.  It's encrypted.  It's only available to you.  I always - I have pointed out in the past that it's only encrypted as long as you don't log in.  This is the second part of that.  Once you're logged in, it's decrypted, and then available to any malware on your system.  Yeah, I think people will - I hope the press will start to come around and say, hey, wait a minute, this isn't as secure as you said it was.



STEVE:  Well, our listeners are preemptively protected; right?  I mean, they're going to turn this off.



LEO:  Like that.



STEVE:  But unfortunately, there's no reach.  Well, there's minimal reach.  But, you know, there's a bazillion Windows 10 or Windows 11 users, and they're going to think, hey, this is cool.  I can scroll back in history.  And this is Microsoft getting ready for something that comes next.



LEO:  Yeah, I agree.  You know, Apple has a solution called Timeline.  It's a backup solution that keeps everything you do in a timeline database, a vault.  Hard links to every version of every document.  So they're kind of doing something similar.  Nobody's ever questioned the usefulness or the security of it.  I don't know how different it is.  But, yeah, this is a problem.  This really is a problem.



Steve's done it again, hasn't he, kids.  This is why we wait for Tuesday with bated breath.  Steve is the man in charge of GRC.com, the Gibson Research Corporation dot com.  And it is the place you can email him.  Now, what should they do again?  They email...



STEVE:  So first you need to register.  Otherwise your email will not get through.  So just go to GRC.com/mail.



LEO:  Okay, there you go.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#978

DATE:		June 11, 2024

TITLE:		The rise and fall of code.microsoft.com

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-978.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How has Microsoft responded to the tidal wave of criticism over Recall?  And what about Google?  Who else recently lost control of their data?  Apple devices will be getting a password manager?  What about iCloud?  Is that a drone recording a wedding, or a Chinese Communist Party surveillance device?  What did SlashData's survey of more than 10,000 coders reveal about their use of AI and choice of language?  And if AIs can code, what's the career future for programmers?  Why has the Linux Kernel project suddenly begun spewing CVEs in great number?  Will we be able to order pizza in the future?  What did one listener discover when he attempted to register his new Passkey devices across the Internet?  And how did a stunning mistake at Microsoft turn into a goldmine of attacker intelligence?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here. He has a rebuttal to Microsoft's rebuttal to last week's accusation, all about Microsoft Recall.  He still says you might want to hold off on that one.  Apple gets a password manager.  Is it good enough for government work?  We'll also talk about the use of AI and coding.  Is AI going to kill the future of code?  And what Microsoft did when they had a little error that's ended up becoming a very useful tool to go after bad guys.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 978, recorded Tuesday, June 11th, 2024:  The Rise and Fall of Code.microsoft.com.



It's time for Security Now!, the show you wait all week for.  As Steve says, it's Tuesday, it must be Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Hey, Leo.  We were out walking with some friends around the neighborhood yesterday, and there's like three long legs of our community, and after doing two of them we all started down the third.  And I said, whoop, nope, it's podcast night, I've got to get back to working on the podcast.



LEO:  I love it.  Aww.



STEVE:  So the walk was cut short.  And Lorrie had been pushing me to start earlier on Mondays because I would be trying to get work done through the mornings, but invariably it would creep into the afternoon, and then it'd be like, oh, shoot, I should be further along.  So now I'm just not even trying to get anything done on Monday except work on the podcast.  Now, that's the driving force.  We're seeing the result of that today because we have 24 pages of show notes as a consequence of me having gotten an extra early start yesterday.  Now, normally the people who listen to us at 1.5x do so because I kind of plod along.  There'll be no plodding today because we have a lot of ground to cover.  So if you are hearing me at 1.5x and wondering why I seem to be going a little faster than usual, you might want to consider dropping down to 1x and hearing it at normal speed.



We've got a bunch of things to talk about on today's podcast 978 for Patch Tuesday.  And I am mourning the fact, and if one of our listeners is listening, someone tweeted me a couple weeks ago a picture of a pipe that just had patches all over it like crazy, and silver, like, screw-on straps.  And it looked like this thing was on life support.  I thought, that is the most perfect Picture of the Week for Microsoft Patch Tuesday.  But I don't know what happened to the picture.



LEO:  Oh, no.



STEVE:  So maybe if the person is listening you can send it to me again, and I'll hold it for July's Patch Tuesday.  We're going to talk about something really interesting, which is - I titled this "The Rise and Fall of Code.microsoft.com," which everyone would recognize as a subdomain, the code subdomain of Microsoft.com.  Essentially the way what happened started was with an architectural failure the scope of which I can hardly grasp.  I have no idea what's going on up there in Redmond that what I will describe could have happened, but it did.  But they turned it around.  I mean, they still have some problems to solve, but they managed to repurpose this mistake into something that ended up creating a ton of intelligence for them, collecting a ton of intelligence.  Anyway, we're going to have a lot of fun with that.



But first we're going to talk about how Microsoft has responded to the tidal wave of criticism that they've received over Recall, and what about Google.  Who else recently lost control of their data?  Apple devices will be getting a password manager?  What?  What about iCloud?  I thought we already had that.  Is that a drone recording a wedding, or is that the Chinese Communist Party surveillance device?



LEO:  Whoa.



STEVE:  What did SlashData's survey of more than 10,000 coders around the world reveal about their use of AI today and their choice of language?  And if AIs can code, which they seem able to do, then what's the career future for programmers?  We have some feedback on that.  Also, why has the Linux Kernel project suddenly begun spewing CVEs in great number?  Will we be able to order pizza in the future, or should we just give up now?  Or maybe order all your pizza today and, you know, freeze it.  What did one listener discover when he attempted to register his new Passkey devices across the Internet?  And as I said, how did a stunning mistake at Microsoft turn into a goldmine of attacker intelligence?  So not a slow podcast today for the second podcast of June.



LEO:  We know Microsoft never makes any mistakes, ever.



STEVE:  Oh, Leo.  Wait till you hear this one.



LEO:  Oh, god.  Oh, god.



STEVE:  I just, when I read this, I thought - and this is from them.  This is their own blog posting.  Somebody confessed this, which first of all I thought, oh.  But, like, really?  Well, anyway, we'll get there.



LEO:  We'll get there.



STEVE:  It's stunning.



LEO:  I know we've got 26 pages, so let me hurry up here.  All right.  Back to, speaking of photos, your Picture of the Week, Steve.



STEVE:  So we've had endless fun with these bizarre pictures of, like, a gate blocking a path where there's open grass on either side of it.  It's like, what?  Just like what were they thinking?  Anyway, somebody had fun over the Microsoft Recall issue and actually relative to their most recent update, this titled "Microsoft Recall," and then it shows us one of the pictures that we've shown before, the bright yellow gate in the middle of a walkway, but, you know, with actually some - it looks like there actually is footpath traffic on either side because the grass is patted down and a little brown.  Anyway, at the bottom it says, referring to this gate, "Don't worry, it's encrypted."



LEO:  Oh.  Doh.



STEVE:  Yeah.  Right, right.  And we'll have, well, actually right now.  Last Friday Mary Jo Foley, that's right, our Mary Jo of Windows Weekly fame, she tweeted:  "Microsoft, bowing to growing security-centric criticism, is making some tweaks to its coming Windows 11 24H2 Recall app.  The first Copilot+ PCs still are on track to start shipping June 18."  Okay, now, today's June 11th.  So that's one week from today, folks.  And she said:  "...and the tweaks are slated to take effect by then, too."



Okay.  So she was referring, of course, to a Microsoft Windows blog, that is, when she said "Microsoft bowing to security-centric" concerns, posted by the Corporate Vice President for Windows + Devices.  So it appears that "Windows +" devices are a category.  So that would presumably mean Copilot+.  Anyway, this guy posted under the title "Update on the Recall preview feature for Copilot+ PCs."  And as Mary Jo noted, this is clearly in response to the security industry's reaction over the previous three weeks to the privacy implications that would be present, my contention is, and I'll be echoing this a couple other times today, in any system that aggregates, for all time, everything a user does on their PC.  Okay.  And I'll have a little more to say specifically about that in a moment.  But first let's hear from Microsoft.



Since a lot of the danger Recall represents is reflected by Microsoft's attitude toward Recall, I want to share this VP's entire post.  It's not very long.  But it's important that their attitude be seen.  So bear with me at the start, since it's pure Microsoft marketing-speak.  Anyway, everyone will recognize it as such.



He wrote:  "Today, we're sharing an update on the Recall feature in preview for Copilot+ PCs, including more information on the set-up experience, privacy controls, and additional details on our approach to security.  On May 20, we introduced Copilot+ PCs, our fastest, most intelligent Windows PCs ever.  Copilot+ PCs have been reimagined from the inside out to deliver better performance and all new AI experiences to help you be more productive, creative, and communicate more effectively.  One of the new experiences exclusive to Copilot+ PCs" - thank god - "is Recall, a new way to instantly find something you've previously seen on your PC.  To create an explorable visual timeline, Recall periodically takes a snapshot of what appears on your screen.  These images are encrypted, stored, and analyzed locally, using on-device AI capabilities to understand" - loosely termed; right? - "their context."



Okay.  He says:  "When logged into your Copilot+ PC, you can easily retrace your steps visually using Recall to find things from apps, websites, images, and documents that you've seen, operating like your own virtual and completely private 'photographic memory.'  You're always in control of what's saved.  You can disable saving snapshots, pause temporarily, filter applications, and delete your snapshots at any time.  As AI becomes more prevalent, we're rearchitecting Windows."



Okay, really?  Like there are dialogs from Windows 95 that pop up every so often in Windows 11.  So, right.  All we've ever seen is some paving over the previous pavement which was over the pavement before that and so on.  So, okay, "...rearchitecting Windows to give customers and developers more choice to leverage both the cloud and the power of local processing on the device made possible by the neural processing unit (NPU).  This distributed computing model offers choice for both privacy and security.  All of this work will continue to be guided by our Secure Future Initiative (SFI).



"Our team is driven by a relentless desire to empower people through the transformative potential of AI, and we see great utility in Recall and the problem it can solve.  We also know for people to get the full value of the experiences like Recall, they have to trust it.  That's why we are launching Recall in preview on Copilot+ PCs, to give customers a choice to engage with the feature early - or not - and to give us an opportunity to learn from the types of real world scenarios customers and the Windows community finds most useful."



Okay.  So under the subhead of "Listening to and acting on customer feedback," he wrote:  "Even before making Recall available to customers, we have heard a clear signal that we can make it easier for people to choose to enable Recall on their Copilot+ PC and improve privacy and security safeguards.  With that in mind, we are announcing updates that will go into effect before the Recall preview ships to customers on June 18th."  So as Mary Jo said, right now, before next week.



"First," he said, "we are updating the set-up experience of Copilot+ PCs to give people a clearer choice to opt-in to saving snapshots using Recall."  He said:  "If you don't proactively choose to turn it on, it will be off by default."  So that's a big change, and that matters.  The flipside is, how much does it really matter?  You know, we've seen how persistent, seductive, and eventually forceful Microsoft can be when they want to push their users in a certain direction.  It's not that difficult to imagine that while the user might need to switch it on, Microsoft will not be cautioning the user about the system's inherent dangers.  Rather, they will be promoting the benefits and touting encryption, locality, security, and all the rest.



I believe the upshot will be that users will turn it on, if nothing less, just to see what it's about because Microsoft will be making it very appealing.  But still, if nothing else, having people turn it on probably gets them off the hook when things go wrong.  After all, well, we didn't ship it with it on.  You turned it on.  And it's like, yes, because you told me to.



Okay.  Anyway, "Second, Windows Hello enrollment is required to enable Recall.  In addition, proof of presence is also required to view your timeline and search in Recall."  Okay.  So these are all good things; right?  They've created additional hurdles, barriers, requirements in order to gain access to this, and I'll just note, through the front door, like gain access the way you're supposed to.  They're not talking about gaining access the way you're not supposed to.  We'll see how that turns out.



And "Third," he said, "we're adding additional layers of data protection including 'just in time' decryption, protected by Windows Hello Enhanced Sign-in Security.  So Recall snapshots will only be decrypted and accessible when the user authenticates."  On the other hand - okay.  Anyway.  Finally, he said, "In addition, we encrypted the search index database."  Which wasn't originally decrypted.



So finally he says:  "Secure by design and secure by default.  In line with Microsoft's SFI" - that's the secure initiative thing - "principles, before the preview release of Recall to customers, we are taking steps to increase data protection.  Copilot+ PCs will launch with 'just in time' decryption protected by Windows Hello Enhanced Sign-in Security, so Recall snapshots will only be decrypted and accessible when the user authenticates.  This gives an additional layer of protection to Recall data in addition to other default enabled Window Security features like SmartScreen and Defender, which use advanced AI techniques to help prevent malware from accessing data like Recall."



Now, okay, remember that last week Kevin Beaumont deliberately used known-to-Microsoft info-stealer malware, which Windows Defender was so slow to recognize that the info-stealer had already successfully exfiltrated the user's entire Recall history before Defender woke up and shut it down.  So again, the problem is Microsoft's heart being in the right place doesn't help anybody because Windows, as we know, is - it's not an exaggeration to say "riddled with vulnerabilities" because, you know, more than a hundred are being fixed today, in today's Windows Update.



Okay.  Anyway, he said:  "We also know the best way to secure information on a PC is to secure the whole PC itself."  Right, because that's been going so well.  And he said:  "We want to reinforce what has previously been shared from David Weston, vice president of Enterprise and OS Security, about how Copilot+ PCs have been designed to be secure by default and share additional details about our security approach."  You know, in other words, unlike all of our previous Windows systems, which really weren't all that secure, even though we've always told you they were.  But oh, baby, this time we really and truly mean it.  Not like all those previous times.



So he said:  "Some notable examples of security enhancements include:  All Copilot+ PCs will be Secured-core PCs, bringing" - which, you know, doesn't matter if the Windows that runs on it isn't secure.  But he said:  "...bringing advanced security to both commercial and consumer devices.  In addition to the layers of protection in Windows 11, Secured-core PCs provide advanced firmware safeguards and dynamic root-of-trust measurement to help protect from chip to cloud."  And that's a new phrase that Microsoft is using, "from chip to cloud."  But, you know, from, what, "cradle to grave."



LEO:  That's it.



STEVE:  Right.  "Also, Microsoft Pluton security processor will be enabled by default" - oh, goodie - "on all Copilot+ PCs.  Pluton is a chip-to-cloud security technology, designed by Microsoft and built by silicon partners, with Zero Trust principles at the core.  This helps protect credentials, identities, personal data, and encryption keys, making them significantly harder to remove from the device, even if a user is tricked into installing malware, or an attacker has physical possession of a PC."



Again, unfortunately, making sure that a buggy operating system isn't altered before it boots or while it's booting doesn't help you once the buggy operating system is running.  But at least it didn't get compromised before it booted.  Who cares?  Anyway, he said:  "All Copilot+ PCs will ship with Windows Hello Enhanced Sign-in Security.  This provides more secure biometric sign-ins and eliminates the need for a password."  Because, yeah, who wants those passwords when you could smile at it?



Okay.  Under the headline "Protecting your privacy on Copilot+ PCs," we have:  "In our early internal testing, we've seen different people use Recall in the way that works best for them."  Blah, blah, blah.  I'm going to skip all this because we don't have a lot of time, and this is just all same stuff.  He's basically saying, okay, we heard you.  We're going to turn it off by default.  We're going to seduce people to turn it on.  But if they do, it's their fault, not ours because, after all, they were the ones who turned it on.  And oh, baby, you know, this is the most secure thing we've ever made.  So again, as I said...



LEO:  We've heard that before, haven't we.



STEVE:  ...we've always told you that.  Remember Ballmer jumping around onstage about Windows XP, which turned out to be the worst security of any operating system to date that they'd had?  Anyway.  So basically they're saying we heard you, and here are all the reasons why we're going to keep doing what we were doing except we're going to turn it off by default as our "get out of jail free" card.  So anyway, we know that users will be impressed by the sounds of all this security.  And I have no doubt that users are going to want to have the power that this provides.  Don't get me wrong.  I mean, I get it.  This is a seductive feature.  And that's part of why this is a double-edged sword.  You know, make no mistake about it, this is powerful.  But it's because it's powerful that it's also so dangerous and brings the potential for great harm.



Will that harm come to pass?  Well, we'll be here to see.  I should also note that I've been asked by a number of our listeners whether I would consider creating some sort of utility that absolutely positively guarantees that Recall is not running on a machine.  We'll see how all this goes.  But I am inclined to do so.  And if so, I know what I'll call it.  And Leo, I will make sure you're not sipping coffee when I reveal its name.



LEO:  Okay.  I've put the coffee down.  Okay.



STEVE:  Because, yeah.  And you'll have to center yourself over your ball because you're going to love this one.  Anyway, we'll see how it goes.



Kevin Beaumont also weighed-in on Microsoft's revised explanation.  He posted this on Mastodon.  He said:  "Obviously, I recommend you do not enable Recall, and tell your family not to enable it, too.  It's still labeled 'Preview,' and I'll believe it is encrypted when I see it.  There are obviously serious governance and security failures at Microsoft around how this played out that need to be investigated, and suggests they are not serious about IA safety."



And I think that raises a really good point.  It's like, you know, they announce this, and we saw Satya jumping up and down, talking about how great it was going to be, and the entire security community had a collective meltdown.  So that tells you something about, like, why they need to have this in people's machines.  Which again comes back to my theory about what they're actually planning, which is that this will be used to train some sort of high-power local assistant.  And again, I get it.  I mean, that would be so cool.  But they've never demonstrated their ability to do anything like this safely.



I should mention that Google with their Chrome OS is also in on the "store everything that happens for possible later use" bandwagon.  Everyone can sense that there's huge potential here somewhere, so no one wants to be left out.  Last week, John Solomon, Google's VP in charge of the ChromeOS, said that their so-called "Memory" feature - okay, they maybe talked to Apple about naming things as that's what they're currently unofficially calling it, is different from Recall.  Okay, but then he describes Recall.  He says:  "Because users will have control of how and where the 'memory' feature works."



LEO:  Uh-huh.



STEVE:  Right.  Just like Recall will offer.  So not so different from Recall.  And after all, if you turn it off, then you're not going to get it.  So people are not going to turn it off.  They're going to have it on, if they want it, and then suffer the consequences, if there are any.  Anyway, Google apparently already wants to distance itself from the stink surrounding the announcement of Recall.



The New York Times.  On the topic of is it possible to keep secrets, last Friday 270GB of data belonging to The New York Times, which I'm quite certain The New York Times wanted to keep secure and secret, and which those in charge of securing it were absolutely and positively certain was completely secure until it wasn't.  You know, just like Microsoft is absolutely and positively certain they're going to secure their users' Recall data, until they don't.  In the case of The New York Times, it got loose.  An unknown threat actor leaked The New York Times source code, as in all of it, all 270GB of it, after one of the company's IT guys apparently left a private GitHub access token in a public code paste.



The leaked data includes the source code of the company's entire public website, mobile apps, and even, for those who are interested, its Wordle game.  The 270GB of data being made available on the dark web is mostly unencrypted.  The hacker posted:  "Basically all source code belonging to The New York Times Company, 270GB.  There are around 5,000 repos - out of them less than 30 are additionally encrypted, I think" - he said, "3.6 million files total, uncompressed tar."  And I have a picture in our show notes of the screen that was posted on the dark web with the series of links so that you, too, can download 270GB and find out what The New York Times coders have been up to.  The lesson here is that, unfortunately, mistakes happen.  In fact, Leo, were we to rename this podcast, it would be "Mistakes That Happen."



LEO:  Yes, that's the whole show, right there, in a nutshell.



STEVE:  You know?  Yeah.  We've seen stories of valuable exposed credentials sitting unnoticed for years.  Right?  Where, like,  some hacker came along and saw that a credential had been posted publicly, but nobody noticed it until now.  One real concern for the future, against the background of "mistakes happen," is that there may soon be, if there aren't already, malicious AI-driven bots scanning and rifling through the Internet looking for any fresh mistakes of value that anyone may have made.  The point is, our world is changing right underneath us right now, and I'm not sure the good guys are winning.  This whole thing feels somewhat asymmetric.  Right?  Because, I mean, as we know, security is about a series of links, and we keep seeming to add more links to the chain, any one of which being defective can break the entire strength of the chain.  Again, it feels like an asymmetric fight that we are not clearly winning.



LEO:  Pretty much losing, I think would be fair.  All right.  How about this?  "You Can't Recall."  Or "Recall Recall."



STEVE:  Oh, no.



LEO:  No, better than that?



STEVE:  I mean, I almost have to do the app just so I can use this name.  It is...



LEO:  "Recall What?"  Okay.  I can't wait.  It'll be fun.



STEVE:  Let's take a break, Leo.



LEO:  Okay.  All right, Steve.  Let's continue on.



STEVE:  So, and I know you'll have something to say about this one, Leo.  During yesterday's World Wide Developer Conference, or the kickoff, Apple introduced their forthcoming Passwords app.  Now, of course, Apple users have long been using their iCloud account to store and sync their passwords among their devices.  But what was going on wasn't super transparent.



LEO:  No.



STEVE:  You know, it just worked, but without a clear and clean UI.  It was, you know, it was necessary to dig down into the Control app to locate a sub-page.  So the passwords app that will be included in the next major release of their OSes, so that would be iOS 18, macOS Sequoia, and visionOS 2, will provide a UI for Apple's storage of this information.  Now, since this is not ever going to be a cross-ecosystem solution, you know, it's Apple only...



LEO:  Oh, it's Windows, too.



STEVE:  Well, yeah.  And I heard you say something about iCloud for Windows password app.



LEO:  Yeah, yeah.



STEVE:  Anyway, I'll just finish and say that those of us using Windows, Linux, or Android will likely remain with whatever cross-ecosystem solution we're using today.



LEO:  That's right.  Yeah.



STEVE:  But this move does create an explicit and native password manager for Apple OSes for the first time.  And if someone is 100% pure Apple-world, it likely offers everything anyone would need.  It also incorporates clear Passkeys management and a built-in one-time-password-style authenticator.  Since I'm currently using OTP Auth as my one-time-password authenticator of choice, I'll look at what Apple has to offer once I upgrade my iPhone to something that'll run iOS 18.  I think I'm stuck back on 12 or something right now.



But so Leo, I hear you guys using the word "sherlocked."  Where did that come from?



LEO:  So way back in the day, I mean, I think this is 20 years ago, there was an app called Sherlock that let you find files on your device.  It was really good.  You could, you know, you'd make an index, and you'd sherlock, and you could find anything on your hard drive.  Then Apple released something it calls Spotlight.  And Sherlock was out of business overnight.  And so ever after, when Apple introduces a product that duplicates functionality of a third-party product and essentially puts them out of business, we call it "being sherlocked."  And there were, I mean, obviously 1Password, Bitwarden, and other password managers...



STEVE:  There was a sherlock festival yesterday.



LEO:  ...may have been sherlocked by this Passwords, but it was just one of many, exactly, yes. 



STEVE:  Now, now, we've seen Microsoft do the same thing, too; right?  Like I was complaining that it took until Service Pack 3 for Windows XP's firewall to be enabled by default.  Back at the time, remember ZoneAlarm was my favorite firewall.  And there was a firewall industry...



LEO:  Exactly.



STEVE:  ...for Windows PCs.



LEO:  Sherlocked, yup, yup.



STEVE:  And then, well, Microsoft says, you know, we're going to put a firewall in, but don't you worry, it'll be turned off by default.  Well, it eventually got turned on.  And I was just talking to a friend of mine the other day who was asking me if she needed to be still using McAfee.  And I said, oh, lord, no.  I said, you know, Windows' built-in Defender is really all anybody needs.  And I explained that Microsoft does this.  They sort of create the capability, but they don't want to step on anyone's toes, so they sort of ease it into the world slowly.  Microsoft, I would argue, was a little less caretaking about that.  They say, yeah, now we're doing that.  So.



LEO:  Yeah.



STEVE:  Anyway.  Again, it makes sense for this thing to get moved in.  So there is - oh, and iCloud for Windows, how can you use a password manager for Apple under iCloud for Windows?  Because, I mean, it just - I thought it was just folders.



LEO:  Oh, no, no, no.  iCloud for Windows lets you do a lot of things, and including I guess now access your passwords.  Actually, I think that's been around.  It is not as elegant as a password manager.  And remember, this is going to do Passkeys.



STEVE:  Right.



LEO:  But I don't know, and they didn't really say, I think the Passkeys are hardware dependent, which means it seems unlikely Passkeys would make it over to Windows.  But they might do what they do now, which is show you a QR code, and then the phone that you have the Passkey on, you do the QR code.



STEVE:  Right.



LEO:  So, you know, is it going to be as full-featured as a standalone password manager?  Probably not.  That's often the case with these things.



STEVE:  Well, and I'm multiplatform.  So...



LEO:  Yeah, well, you and I won't use it.  I can't - I'm Android in Windows and Linux.



STEVE:  Right, right.  But again, Apple...



LEO:  A lot of Apple people - and you know what, I love this because it means a lot of non-sophisticated users will just do it because it's part of the operating system.  It's built in.



STEVE:  Yes.  And I think the fact that they're showing it as a separate thing, you know, helps to raise people's awareness of passwords and the various aspects of passwords, like Passkeys and one-time passwords and so forth.  It just, you know, it brings it more to light, which has got to be a good thing. 



LEO:  Yeah.



STEVE:  I saw some talk a while back about some congressional pushback on Chinese-made drones by DJI.  And those DJI drones are by far the best drone technology around.  In advance of the U.S. Senate's planned discussion of the so-called "Countering CCP Drones Act," which would limit the use of Chinese-made drones in the U.S. on the grounds of national security, tomorrow, June 12th, DJI will be disabling the ability of users in the U.S. to sync their drone flight data to its servers, and the option to sync U.S. drone data at all will be completely removed by the end of the month.



So, you know, DJI is seeing what's going in with TikTok and this general sort of concern over what the Chinese Communist Party is doing with technology that U.S. consumers are excited about.  And so I'm sure they don't want to lose this market.  So they're saying, okay, fine, we're going to strip this out of our devices.  So I don't know what a problem that will be for DJI users, if being able to sync drone flight data to servers is a big deal.  But it'll be gone by the end of the month.



Okay.  Another thing that, Leo, I think you're going to get a kick out of seeing, although you and I are not represented among these statistics.  SlashData revealed some interesting developer statistics.  They recently surveyed 10,000 developers from more than 135 countries.  The question put to them was "How has AI affected your workflow?"  Okay, now, let me first allow SlashData to introduce themselves.  They wrote:  "If this is the first time you've heard about SlashData" - did I say Slashdot?  SlashData.



"If this is the first time you've heard about SlashData, I'm happy to share a few quick words," writes the person who posted this.  "SlashData is a developer research company.  Every quarter, SlashData runs a survey on the global developer audience, to measure the pulse of the developer ecosystem and how they feel about new technologies, tools, platforms, the support from developer programs, and more.  Following the closing of the survey, our expert analysts work to identify key trends and translate raw data into actionable insights that professionals and companies addressing a developer audience can utilize to fine-tune their strategy and address developers' needs and wants.



"The 26th edition of the Developer Nation survey reached more than 10,000 respondents from 135 countries around the world.  SlashData announces the first two of a six-report series that are becoming widely available to the world, showcasing and diving into key developer trends for 2024 and beyond.  Each report focuses on a specific topic.  All reports published under the State of the Developer Nation will be accessible under the freshly launched SlashData Research Space, free access for viewing and downloading."



Okay.  So the first two chunks are interesting.  The first is how AI has impacted development.  And the second is the ever popular "Which programming language do you use?"  So first off, AI.  They said:  "How developers interact with AI technologies.  Has AI taken over the world?  Not yet," they write.  "However, it has already achieved a takeover of all our discussions about the future."  Indeed it has.  "And," they said, "59% of developers report that they're now using AI tools in their development workflows.  This report investigates the current landscape of developers' work with artificial intelligence technologies and how this impacts their careers.  We start by looking at the ways in which developers work with machine learning models, tools, APIs, and services, and highlight the key differences between professional and amateur developers."  And they go on.



So on the AI front we first have four broad categories.  And Leo, I've got a chart at the top of page 9 of the show notes.  Four broad categories.  59% report using AI in their own development workflows, 25% are adding AI functions into applications, and 13% are actively involved in creating AI models.  This leaves only 29% whose development work has not yet been touched by AI in any of those ways.



LEO:  Wow.



STEVE:  Yeah.  Among the 59% - so more than half and fewer than two-thirds.



LEO:  I'm in that category, by the way.  I have an AI that helps me with my coding.



STEVE:  Yes, yes.  So 59% who are now actively using AI tools in their development workflows.  42%, almost half, are using chatbots to obtain answers to coding questions.  This is globally, out of 10,000, more than 10,000 developers surveyed.  42% are using chatbots to obtain answers to coding questions, 27% are using development tools that have AI-assistance built-in, and 19% are using generative AI to help generate creative assets.



If coding was a Monday through Friday, nine-to-five job which I was doing to earn my living, where I was being judged by my own productivity against my peers, then yeah, I'd be quite happy...



LEO:  Yeah, yeah.



STEVE:  ...to get quick answers to questions about how to do this or that from a chatbot AI.  You know, rather than searching around the Internet looking for someone, like wherever on the Internet.



LEO:  Stack Exchange or somewhere, yeah.



STEVE:  Stack Exchange is the name I was just trying to remember, a Stack Exchange, who has posted something similar to learn from, I'd be happy to ask a smart bot what it had found from previously doing essentially the same thing.  There's no shame there.  And it's clear that many coders agree.  Okay.



LEO:  I use it instead of flipping through manuals.



STEVE:  Yes.



LEO:  Almost universally the stuff that's on Stack Exchange is useless.  But I still have to, I mean, I don't code enough to remember every - and the language I use is massive, Common LISP.  So this is in lieu of looking through manuals.  It's very useful.



STEVE:  Yup.



LEO:  Very useful.



STEVE:  Yup.  So what's going on with the use of programming languages?  I have a chart there at the bottom of page 9.  The survey revealed that by far the number one language in use today is JavaScript.



LEO:  Oh, yeah.  That's for sure.



STEVE:  Yup.  Web programming.  The current total is estimated to be 25.2 million JavaScript coders...



LEO:  Wow.  Wow.



STEVE:  ...with that number having grown by four million just over the past year.  So 25.2 million JavaScript coders.  In the number two slot is Python at 18.2 million.



LEO:  There's probably a lot of overlap, too.  I mean, nobody...



STEVE:  Yes.



LEO:  ...uses just one language.



STEVE:  Right, right.  Python at 18.2 million, which is just a bit ahead of JAVA at 17.7 million in third place.  Behind those top three is C++ at 11.6, C# at 10.2, PHP at a respectable 9.8 million, Visual development tools at 7.2 million, followed by plain old 'C' language at 6.5.  Then in steadily decreasing numbers we have Kotlin, Go, Swift, Rust, Dart, Objective-C, Ruby, and Lua.  And you know, Leo, there's no sign of LISP or assembly language on this chart.



LEO:  We're old-timers.



STEVE:  What do you suppose that means that neither of the two languages which you and I have chosen to use, LISP and Assembler respectively, are in the running here?



LEO:  We're just smarter than the masses.  That's all there is to it.



STEVE:  You know, I think part of it is that we're able to choose the language we most want to code in.



LEO:  We get to choose.  That's right.  That's right.



STEVE:  We don't have any boss telling us, or an existing code base that we're having to maintain in whatever language.



LEO:  Or colleagues who have to be able to read our code.



STEVE:  Right.  Yes.  And neither of us are part of a team that would think we had lost our minds.



LEO:  But also there are very good modern languages that aren't on that list either.  I think it really comes down to more trends, but also what your business is demanding of you.



STEVE:  Yes.  I think, I mean, that's - I mean, we already know for decades, coders rsums have listed all the languages that they can, you know, that they're proficient in.



LEO:  Right.  Right.  Any coder should be able to write in any language, if they're any good; right?  I mean, it's...



STEVE:  Or be able to pick up a new one.



LEO:  Yeah, that's what I mean.  You can...



STEVE:  Yes.



LEO:  Yes, yeah.  All the concepts are the same.



STEVE:  And that's where a chatbot can help you.



LEO:  Helps a lot.



STEVE:  It's like, okay.



LEO:  That's right.



STEVE:  I'm not proficient in Perl, but I need to solve a Perl problem.  So what regex would you expect to use?



LEO:  I've actually done that.



STEVE:  And it may not be right.  But it's...



LEO:  It's a good start.



STEVE:  It's a place to start.



LEO:  I've taken some Python code that I didn't fully understand, given it to ChatGPT and said, what would this look like in LISP?  And, yeah, it wasn't perfect, but it gave me a big head start on understanding what that code was doing.



STEVE:  Yeah.



LEO:  Yup.



STEVE:  So the question is, are we going to turn programming over to AI?



LEO:  No.  Well, eventually, I guess.



STEVE:  Well, coding appears to be something that AIs may be able to do.  You know, and it makes a sort of sense for code to be something that an AI might do well because, after all, it's talking to a machine.  So that begs the question, what's going on at the university level with computer science education?  Business Insider published a piece last Monday titled "With AI writing so much code, should you still study computer science?"  And the subheading was "This new data point provides an answer."



Okay, now, I realize that many of our listeners are well past university age, but many will have children - or perhaps grandchildren - who may be wondering whether coding has been lost to AI.  So the author of this piece writes:  "One of the most persistent concerns around generative AI is whether the technology will put workers out of a job.  This idea has particularly caught on in the context of software coding.  GitHub Copilot can write a lot of code these days, so is it even worth studying computer science now?  That's been a question on the minds of math-minded high schoolers since ChatGPT burst onto the scene in 2022.  There's a new data point that helps answer at least part of this question:  Students are still lining up in droves to take computer science in college."



Let's take the University of California Berkeley as an example, as this college is at or near the top for computer science, as it was when I was there in '73.  First-year applications to UC Berkeley's College of Computing, Data Science, and Society (CDSS) - now, that's not the college I was in.  I was in EECS, Electrical Engineering Computer Science.  But we have CDSS, the College of Computing, Data Science and Society.



Anyway, first-year applications increased 48% this year.  There were 14,302 non-transfer applications for these CDSS majors in the Fall of 2024 incoming class, versus 9,649 the previous year.  So in one year, 48% increase, they said; whereas, for context, the number of first-year applications to UC Berkeley as a whole did not change much from a year earlier.  So it was specifically the College of Computing, Data Science, and Society.  This was announced last week by Professor Jennifer Chayes, the dean of Berkeley's College of CDSS during the Joint California Summit on Generative AI in San Francisco.



Afterwards, Business Insider got in touch with an interesting guy, John DeNero, a Computer Science Teaching Professor at UC Berkeley, to talk about this some more.  Now, he's also chief scientist at Lilt, a generative AI startup; and he was previously a researcher at Google working on Google Translate, one of the first successful AI-powered consumer apps.



Okay.  So at this point the article quotes this John DeNero guy.  And remember, he's a teaching professor of Computer Science at UC Berkeley who has been working with AI at Google and is now the chief scientist at a generative AI startup.  So the article continues:  "In an email to Business Insider, John wrote" - so this is John speaking - "'Students express some concern that generative AI will affect the software engineering job market, especially for entry-level positions.  But they're still excited about careers in computing.  I tell them that I think many of the challenging aspects of software development cannot be performed reliably by generative AI at this point, and that I expect there will still be a central role for human software developers long into the future.'"



So this is a Comp Sci professor, teaching professor at Berkeley, who's also deeply steeped in AI technology.  The article says:  "DeNero explained that generative AI is currently very good at replicating parts of software programs that have been written many times before.  But what if you want to create something new?  This is where smart human coders will still be needed.  This makes logical sense as AI models are trained on data.  If that information doesn't exist yet, or it's not part of the training dataset, the models often get in trouble."  Or as we say, "They just make it up."



DeNero said:  "Generative AI requires a lot of thoughtful human intervention to produce something new, and all consequential software development projects involve quite a bit of novelty.  That's the hard and interesting part of computing that currently requires clever and well-trained people.  Generative AI can speed up the more mundane parts of software development, and software developers tend to adopt efficiency tools quickly."  So this applies to what's happening at Lilt, which is building an AI platform for translators.  "Google Translate first came out 18 years ago," they write, "and human linguists still have jobs and are relied upon when translations are really important.  For instance, you could use Google Translate to read a Japanese train timetable, but would you use the app to translate your business's most important contract without having a human expert check it out?  Probably not."



John said:  "To reliably produce publication-quality translations, human expert linguists are still at the center of the process.  But by using Lilt's task-specific generative AI models, those experts are much faster, more accurate, and more consistent.  As a result, more text gets translated at higher quality into more languages."  And they finish:  "He expects this same pattern to play out in software development.  A small team of highly trained human developers will have an even greater capacity to build useful, high-quality software."



DeNero finished by adding:  "And so future Berkeley graduates will have plenty of opportunities to use their computing skills to improve the world.  Hopefully some more of them will come work at Lilt."  And I got a kick out of that because where better to recruit people for your own startup?



LEO:  Your classes.



STEVE:  Yes, than teaching them and culling from the herd those...



LEO:  The best, yeah.



STEVE:  ...that you want to have working for you.  And Leo, it really does make sense.  You know, I'm weird; right?  I mean, we already know I code in assembly language.  At the moment, I am creating the scaffolding to access COM API objects in my email server from assembly language.



LEO:  Oh.  That sounds painful.



STEVE:  Because it is unbelievably painful.



LEO:  There would be absolutely libraries galore to do that in any higher level language.



STEVE:  And there are none.  No one, as far as I know it's not been done...



LEO:  Not in assembly, yeah.



STEVE:  ...in assembler.  But I like it.  And I also...



LEO:  That's all that matters.



STEVE:  And I also spend a lot of...



LEO:  You like it. 



STEVE:  Yes, I like it.  And how many times have I written a super fast sort algorithm of one type or another?  I've written them and I've rewritten them because I like it.



LEO:  It's fun.



STEVE:  It's like somebody who loves - like a woodworker building chairs.



LEO:  Yeah, yeah.



STEVE:  It's like, I'm going to make - the next chair I build is going to be better than the last one.  But, you know, I'm coding because I want to, not because I have to.  So it totally makes sense to me that generative AI could be producing a bunch of the crap code that people have already written, not the new stuff, which is where the fun really is for most people who, you know, don't like building chairs over and over.



LEO:  Yes.  You can always buy a chair, but there's a satisfaction in building your own, absolutely.



STEVE:  Yup.



LEO:  Yeah.  And I would imagine anybody who's in a computer science program I would hope is there because they enjoy it, because they like it.  They're not just there to get a job skill.  I mean, that's a nice side benefit.



STEVE:  It certainly is nice to be able to spend your life doing something you love.



LEO:  Do something you love.  You'll be glad.



STEVE:  Okay.  So, and this one affects you, Leo, as a Linux person.  In case any of our Linux users notice and worry about a sudden torrent of CVEs emanating from the Linux Kernel Project, I wanted to assure everyone that the problem is with the underlying issuing policies and is not reflective of any sudden collapse of the Linux's kernel code quality.  Catalin Cimpanu, the editor of the Risky Business Newsletter, did some editorializing, but he drew the facts underlying his recent editorial from across the industry.  So this is strongly based in what everybody who's looking at this going, what the hell is going on, is talking about.  So I'm explaining this beforehand since I wanted everyone to understand that this is, you know,  not just one grumpy guy's opinion.



Here's what he wrote last Wednesday.  He said:  "In February of this year" - get this - "The Linux Kernel Project was made an official CVE Numbering Authority" - that's called a CNA, a CVE numbering authority - "with exclusive rights to issue CVE identifiers for the Linux kernel.  While initially this looked like good news," he wrote, "almost three months later, this has turned into a complete and utter disaster.  Over the past months, the Linux Kernel team has issued thousands of CVE identifiers, with the vast majority being for trivial bug fixes and not just security flaws.



"In May alone, according to Cisco's Jerry Gamblin, the Linux team issued over 1,100 CVEs, a number that easily beat out professional bug bounty programs and platforms run by the likes of Trend Micro's Zero Day Initiative, Wordfence, and Patchstack.  Ironically," he says, "this was a disaster waiting to happen, with the Linux team laying out some weird rules for issuing CVEs right from the moment it received its CNA status.  We say 'weird' because they're quite unique among all CNAs.  The Linux Kernel team argues that because of the deep layer where the kernel runs, bugs are hard to understand, and there is always a possibility of them becoming a security issue later down the line."



He said:  "Direct quote below."  This is the Linux Kernel team.  "Note, due to the layer at which the Linux kernel is in a system, almost any bug might be exploitable to compromise the security of the kernel, but the possibility of exploitation is often not evident when the bug is fixed.  Because of this, the CVE assignment team is overly cautious and assign CVE numbers to any bug fix that they identify.  This explains the seemingly large number of CVEs that are issued by the Linux Kernel team."  Wow.



He says:  "While this looks good on paper, the reality is that other projects also manage similarly sensitive projects, but they don't issue CVEs for literally every possible bug fix.  You don't see Intel and AMD issuing hundreds of CVEs with each firmware update.  These projects vet reports to confirm that bugs pose a security risk before issuing a CVE and triggering responses with their customers, such as inventory asset scans and emergency patch deployments."  In other words, CVEs have actual real-world consequences.  They're not just to be used casually.



He says:  "Instead, the Linux Kernel team appears to have adopted a simpler approach where it puts a CVE on everything and lets the software and infosecurity community at large confirm whether or not an issue is an authentic security flaw.  If it's not, it's then up to the security and vulnerability management firms to file CVE revocation requests with the Linux Kernel team that's responsible for the affected component.



"Linux's new CNA rules also prohibit the issuance of CVE for bugs in EOL Linux kernels, which is also another weird take on security."  He said:  "Just because you don't maintain the code anymore doesn't mean attackers won't exploit it and that people wouldn't want to track it.  The Linux team will also refuse to assign CVEs [whoops] until a patch has been deployed, meaning there will be no CVEs for zero-days or vulnerabilities that may require a longer reporting and patching timeline."



LEO:  I think they do not know what CVE means.



STEVE:  Leo, that's nuts.  I mean, it's like, if we don't admit that there's a problem, then Google can't start a clock forcing us to fix it.



LEO:  Yeah.



STEVE:  So we're not going to issue it for a zero-day or vulnerabilities that may take a while to fix.



LEO:  Wow.



STEVE:  I mean, you're right, it's like they don't at all understand what CVEs are for.  Catalin said:  "The new rules also create a confusing process of validating, contesting, and rejecting CVEs.  I'm not going to go into all of that," he said, "since the venerable Brian Martin did a way better job back in February.  Open Source Security's Bradley Spengler shared a real-world example last week of why the entire process of analyzing, validating, and revoking Linux CVEs is now a giant clusterf**k of confusion and frustration."



Catalin said:  "We quote him:  'To say this is a complete disaster is an understatement.  This is why CVEs should be for vulnerabilities, should involve actual analysis, and should provide that information in the CVE description, as any other responsible CNA would be doing.'"  Catalin said:  "Linux maintainer Greg Kroah-Hartman tried to justify the team's approach to its new CVE rules; but, as expected, this has not gone down well with those in the infosec community.  Criticism has been levied against the Linux Kernel team from everywhere, and there have been some calls for the Linux team to reconsider their approach to issuing CVEs.



"The new rules were criticized right from the get-go.  The likes of Katie Moussouris, Valentina Palmiotti, Ian Coldwater, Bradley Spengler (again and again), Adam Schaal, Tib3rius, the grsecurity team, the GrapheneOS team, and a whole bunch more, foresaw the disaster that is currently unfolding.  And if this isn't bad enough, the Linux kernel team appears to be backfilling CVEs for fixes to last year's code, generating even more noise for people who use CVEs for legitimate purposes.



"Some described the Linux team's approach as 'malicious compliance' after the project was criticized for years for downplaying vulnerability reports and contesting CVEs assigned to its code by other CNAs.  This may not be the case, as the new approach has some fans who see its merits, such as forcing more people to upgrade their kernels on a more regular basis."  Meaning even if it's not necessary.



"The Linux CNA" - this is quoting somebody, he doesn't say who.  "The Linux CNA intentionally adopts an overly cautious approach and assigns a new CVE when in doubt.  While this may surprise many, it is a perfectly legitimate and entirely honest strategy.  In contrast, vendors of proprietary software often tend to take the opposite approach, minimizing the assignment of CVEs whenever possible.  Effectively managing the substantial number of CVEs involves understanding your kernel configuration, having a clear threat model, and ensuring the ability to update the kernel as needed.  I hope that other large projects will eventually adopt Linux's approach."



And Catalin finishes:  "Unfortunately, all of this CVE spam could have not happened at a worse time.  Just as the Linux Kernel team was getting its CNA status, NIST was slowing down its management of the NVD database, where all CVEs are compiled and enriched.  NIST cited a staff shortage and a sudden rise in the number of reported vulnerabilities, mainly from the IoT space.  Having one of every fifth CVE being a Linux non-security bug is not helping NIST at all right now."  So unfortunately, we depend upon CVEs to convey true problems that require remediation of some kind.  Having the Linux Kernel Project spewing CVEs for non-vulnerability bugs really is an abuse of the system.



LEO:  Yeah.  And they're creating a lot of noise, which is obscuring the real security issues.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  Exactly.  It has a real potential of just causing people - it's crying wolf; right?



LEO:  Right, right.  False alarm.



STEVE:  You're going to end up blunting the effect.



LEO:  Yup.



STEVE:  What is not blunted, Leo, is the power of our sponsors.



LEO:  I like your thinking.  I like where you're going there, Mr. G.  Now, back to Mr. Gibson.  Steve Gibson, yes.



STEVE:  Okay.  So GRC's email system continues to mature, and I could not be more pleased with my decision to create a more convenient means for our listeners to send podcast feedback. Some listeners have noted that nowhere on GRC's website do I prominently display the email address "securitynow@grc.com."  That's true, and that's also deliberate.  It's clearly not a secret, since Leo and I will be mentioning it every week here.  But to whatever degree is possible I'd like to reserve inbound email to that mailbox for podcast feedback.  There will be a temptation to send things to me that I already pay Sue and Greg to handle.  So I'd prefer not to short-circuit our traditional lines of communication.  So once again, securitynow@grc.com.  And I imagine everybody can remember that.



I did want to let everyone know that, after last week's podcast, I improved GRC's email registration system to also accept email that's registered against a user's "From" header.  The moment I made that change, all false positive rejections stopped.  We haven't had a single one since then.  So anyone who may have had initial difficulty registering with private domains fronted by Gmail or some other email anonymization service should no longer have any trouble and may do so.  So again, I was a little overprotective initially.  That's fixed.  It's easier now.



Several people have been worried that they haven't ever received a single piece of email from me.  You know, they're expecting the flow of weekly podcast announcements.  So I wanted to assure everyone that, so far, I have never sent one.  I'm still working to finish up the front-end email registration bounce processing, which I expect to complete this week.  I always wondered about the practice of asking people to enter their email addresses twice.  I understood that it was to catch typos, and when I designed GRC's e-commerce system back in 2003, that's what I had it do, too.  But I did that mostly because everyone at the time was doing it.  Now I know why.  The email registration system I have does NOT do that, and it's somewhat surprising to see how many typos are present in email that cannot be delivered.  It turns out that ".vom" is not a valid top level domain, and that the V key is right next to the C key.



The good news is that such typos only result in a brief stumble, since this is part of an immediate email confirmation loop.  So anyone who doesn't receive a confirmation email returns to try again, and they will probably enter their email correctly, and maybe by typing it more carefully, you know, the second time.  Since I think that asking everyone only once, because they receive immediate confirmation, is more convenient for most people, I'm going to leave the system as it is.  I'm not going to ask everybody to enter it twice.



The work I'm doing right now is in automating the process of receiving any immediate delivery attempt failures from our email server and holding that information for someone's second attempt when they don't get the first email confirmation and then come back and try again.  There are a surprising number of "mailbox unknown" or "mailbox over quota" bounce-backs that I would like to be able to present to someone when they retry using an address that just failed for that reason.  So once that system is in place, I'll actually begin sending email, and the system will be up and running.



Okay.  So I got a kick out of this fictional dialog with an AI which was titled "Ordering a Pizza in 2024."  This was shared by a listener via Twitter.  There's no indication of the dialog's origin, but it's definitely worth sharing.  So the caller says, apparently into their phone, "Is this Pizza Hut?"  "No sir, it's Google Pizza."



Caller:  "Oh, I must have dialed the wrong number, sorry."  "No, sir.  Google bought Pizza Hut last month."  "Okay.  I would like to order a pizza."  "Do you want your usual, sir?"  "My usual?  You know me?"  "According to our caller ID data sheet, the last 12 times you called you ordered an extra-large pizza with three cheeses, sausage, pepperoni, mushrooms, and meatballs on a thick crust."  Caller says:  "Super.  That's what I'll have."



Google:  "May I suggest that this time you order a pizza with ricotta, arugula, sun-dried tomatoes, and olives on a whole wheat gluten-free thin crust?"  The caller says:  "What?  I don't want a vegetarian pizza."  Google:  "Your cholesterol is not good, sir."  "How the hell do you know that?"  "Well, we cross-referenced your home phone number with your medical records.  We have the result of your blood tests for the last seven years."  "Okay, but I do not want your rotten vegetarian pizza.  I already take medication for my cholesterol."



"Well, excuse me, sir, but you have not taken your medication regularly.  According to our database, you purchased only a box of 30 cholesterol tablets once at Lloyds Pharmacy, four months ago."  Caller says:  "I bought more from another pharmacy."  "That doesn't show on your credit card statement."  "I paid in cash."  "But you did not withdraw enough cash, according to your bank statement."  "I have other sources of cash."  "That doesn't show on your latest tax returns, unless you bought them using an undeclared income source, which of course is against the law." 



Caller says:  "What the heck?"  Google says:  "I'm sorry, sir.  We use such information only with the sole intention of helping you."  The caller says:  "Enough already.  I'm sick of Google, Facebook, Twitter, WhatsApp and all the others.  I'm going to an island without the Internet, TV, where there's no phone service and no one to watch me or spy on me."  Google says:  "I understand, sir, but you need to renew your passport first.  It expired six weeks ago."



So anyway, yes.  If Microsoft Recall does evolve into a semi-smart personal assistant, it better not start offering helpful advice, or maybe people will think about deleting it.  Or using my forthcoming freeware app.  We'll see.



LEO:  End All Recall?  I keep trying.



STEVE:  Leo, like I said, I'll just make sure you don't have a mouthful of coffee.



Nathan Hartley tweeted:  "I would love Windows Copilot on my work PC, though we have far more local admins who have access to everything than I'm comfortable with.  I will wait a bit for my personal PC."  Now, of course, Nathan is suggesting that in a corporate environment, having access to a comprehensive history of everything that has been done on a company machine might be useful.  But he wonders what access to that information would also be available to local admins.



And I think that's another very good point.  All indications are that in their enthusiasm for this idea, which is understandable, Microsoft failed to give sufficient thought to just how transformative a change it would be for a machine's entire usage history to be captured and stored in detail.  We know that enterprise machines are owned and operated by their companies who oversee them and their security.  So how does Recall fit into that environment?  There do appear to be some questions still to be answered.



Tom, who's @TomLawrenceTech - this was shared via a public Tweet, so it appeared in my timeline because he referenced @SGgrc.  He said:  "I just had a great conversation with @DRtheNerd about ADAMnetworks, @pfsense, and their 'Don't Talk to Strangers' system.  I'll be doing some testing, but for those who want to learn more right now, check out https://adamnet.works and @SGgrc Episode 946," and he provided a link.  Now, of course @DRtheNerd, that would be David Redekop, whom I first met when they were an early advertiser on this podcast, I think right from the get-go, Leo.



LEO:  Yeah, very early on.



STEVE:  They were the Canada-based Nerds On Site guys at the time.  And of course, as we know, David is now part of the team at ADAMnetworks, and I discussed their work during Episode 946 and noted that they have some very interesting and mature perimeter security technology, which is definitely worth looking at.



Okay.  Listener John Liptak asked:  "Steve, I've been caught up in the Google Domains to Squarespace DNS migration, and due to Squarespace's terms of service I want to move.  However, due to the number of security issues with DNS as well as your wonderful testing software, I've been unable to find the episode where you give your recommendation for a domain name provider.  Can you remind me who you recommend?  Thanks, John."



The name John is trying to recall is "Hover.com."  They are my absolute, hands-down, favorite domain name registrar.  They were also a TWiT sponsor, though that followed my switching to them away from Network Solutions, who was GRC's original registrar, with whom I registered the GRC.com domain back in December of 1991, which was a few months after the domain Microsoft.com was first registered.  I could not be more pleased and happy to recommend Hover as the place for anyone to hang their domain.  I mean, again, I know that, Leo, you and I both have a ridiculous number of domains just because...



LEO:  It's fun.



STEVE:  ...each one seems inexpensive, and maybe we'll use it someday for something.



LEO:  Right, exactly.



STEVE:  I can't even tell you, like, the nonsense I have.



LEO:  Oh, me, too.



STEVE:  But what the heck.  They're all there at Hover.  Steve in Tampa, Florida sent me a note regarding the Token2 keys that we've talked about a couple times.  He said:  "I just wanted to let you know that after hearing your mention of the Token2 keys on the podcast I ordered two of the T2F2-NFC-Dual keys.  I received them today.  I immediately downloaded the Windows app from their website and entered in a PIN.  I then tried them with Bitwarden.  After entering them in Bitwarden under WebAuthn, I was able to" - in other words, the Passkeys - "I was able to login in every case - USB-A, USB-C, and NFC - using either the web app or an Android phone.  Of note is that to activate the key you need to squeeze contacts together and not just touch the contacts.  Regards, Steve in Tampa, Florida."



So that's welcome feedback, and I'm glad that those Token2 keys were not a boondoggle.  They really do look like solid solutions.  The ones I ordered were backordered, and they've not shown up yet, but I'm not in a huge hurry.



Now, yesterday a listener, Bob Grant, wrote through the new email system with some of the best on-the-ground feedback about the current state of Passkeys support that I've seen so far.  What Bob had to share was of crucial importance because it clearly dispels the belief that all websites which support Passkeys support multiple Passkeys, thereby allowing multiple physical dongles to be used without restriction.  That's not the case.  So here's Bob's great reporting.



He said:  "Hi, Steve.  I've always enjoyed trying out the bleeding edge, and I've been using YubiKeys for over a decade.  So I recently replaced one of my YubiKeys with a Token 2 key from Switzerland to get its 100-passkey support.  I then went about registering multiple YubiKeys and my new Token 2 keys plus Bitwarden at multiple sites.  For instance, I have five Gmail accounts and two Microsoft accounts I wanted to use with the Passkeys.  I discovered a few indications that we have a ways to go before this is ready or easy for prime time.



"For security purposes, all the hardware keys require a PIN to unlock the key for each login to a site.  This is as opposed to Bitwarden, which will do it for you while the vault is unlocked; or, if locked, can use a biometric authentication, which is pretty quick.  Further, the hardware token operation requires an initial touch to bring up the PIN prompt followed by another touch after the PIN to perform the authentication.  The Token 2 keys require the FIDO-recommended six-digit PIN, whereas YubiKeys allow for a more convenient four-digit PIN.  As usual, security trumps convenience.



"Next, I found that a bunch of sites do not follow the FIDO recommendations.  eBay, PayPal, and Lowes only allow a single Passkey to be registered.  This of course means you have to use something like Bitwarden that can sync between devices rather than a single hardware key, which is a point of failure.  Kayak, LinkedIn, Adobe, and Amazon do not allow naming the keys as you enroll them.  LinkedIn calls them Passkey 1, 2, 3, et cetera.  Amazon has the date, but not the time, the key was enrolled, so there's no way to differentiate unless you enroll on different days.  The effect of this is that, if you need to revoke a key that is lost, you don't know which enrolled key should be deleted from the site.  All other sites I used allowed naming at creation, and some even allow later renaming of enrolled keys.



"Most sites allow quite a few keys, but LinkedIn only allows five.  Surprisingly, Amazon AWS seems to only allow FIDO1-style U2F mode keys, not FIDO2 for Passkey login.  Many sites allow keys from one type of device, for example, iOS or iPadOS, but not another, like Firefox on a desktop.  Chrome seems to have better support, and I think MS Edge has good support, although I didn't test extensively.  Chrome allows managing keys, Token 2 or YubiKey, from its Settings > Security menu within the browser, and so you can list, delete, edit, et cetera.



"This all suggests that it's still early days, but I still kind of prefer my YubiKey to my Token 2 key, and I'm doubtful I'll get to 100 Passkeys anytime soon.  The Token 2 is fatter and more bulky and at least feels a little more vulnerable than the YubiKey.  Also, at one point my T2 stopped responding and prompting for a PIN when I tried to login.  But I was able to use my YubiKeys without a problem.  Once I rebooted my laptop, the Token 2 key resumed responding.  I don't know whether the auth infrastructure would blacklist a key, but I'm going to keep an eye on it."  Now, for the record, my guess is that the Token 2 Windows app probably froze somehow, and that that's what the reboot cured.  And of course USB is always, because it came along after Windows had already launched and a lot of it been written, USB has always been a little bit flaky.



So anyway, he finishes:  "I'd like to see PayPal allow multiple keys so I could switch to using a hardware key for added security.  But I'll need to use Bitwarden with PayPal until then.  It's disappointing to me that banks, investment houses, and other high-value targets do not currently support Passkeys at all.  In fact, most are still using SMS text second factors rather than Google Auth or even the older U2F which could use keys for multiple-factor authentication.  Hardware keys can also be used for SSH authentication for more security for your SSH sessions.  Each one takes the same type of slot as a Passkey and can also can store the SSH key info which allows it to move the public key from system to system.  It's easy to see what an uphill battle SQRL faced when even given all the support behind FIDO2, its implementation remains spotty and uncertain."



So, wow, thank you very much, Bob.  That's some terrific feedback about the current state of Passkey support.  And all of this does suggest that today's optimal solution - driven by the fact that there are sites which will only accept a single Passkey enrollment, and you never know when you're going to hit one - would be to enroll one or more, where possible, hardware dongles only for the highest security sites where that's what you want; but to then otherwise use a cross-platform password manager such as Bitwarden, a sponsor of the TWiT network, and use that hardware dongle to, in turn, unlock Bitwarden, if you want more than Bitwarden's biometric unlocking.  In that fashion, any site's single Passkey support won't present a problem since Bitwarden is able to present that site's single Passkey from any Bitwarden-supported device.  And now that its support for mobile devices is shipping, it's on all platforms everywhere.



A listener using his initials B.E. surprised me.  He wrote:  "Hi, Steve.  Thank you for the new email system since I don't use any social media.  Regarding Code Signing HSMs:  My friend and I are on top of the development of a hobby software, used by only 15 to 20 people.  We used to share the code signing keys between us and one other developer.  But when I went to renew the code signing certificate, I saw there is no longer an option to be able to sign any code without an HSM.  Do you have an idea how we can still have the three developers able to sign the code?  Thank you for all your work.  Long-time listener and a Club TWiT member, B.E."



LEO:  Yay, thank you.



STEVE:  Okay.  So this was news to me.  In a follow-up note, B.E. sent some links so that I didn't need to track them down myself.  And sure enough, reading from the knowledgebase maintained by my favorite certificate authority DigiCert, under the title "New private key storage requirement for Code Signing certificates," they write:  "Starting on June 1, 2023, industry standards will require private keys for standard code signing certificates to be stored on hardware certified as FIPS 140 Level 2, Common Criteria EAL 4+, or equivalent."  In other words, an HSM, some sort of a hardware dongle.  "This change," they write, "strengthens private key protection for code signing certificates and aligns it with EV (Extended Validation) code signing certificate private key protection."



Wow.  This is actually troubling.  First, as I previously reported, the enhanced trust that Microsoft was originally conveying to any code signed with the significantly more expensive EV certs, which have always required storage in an HSM, has been revoked so that there is no longer any benefit to having an EV certificate for code signing.  No one cares.  But now the industry has moved to requiring ALL code signing to be performed inside, and by a hardware dongle.  It was already a problem that any code signing was becoming a requirement, which is what we've been seeing due to the increasing prevalence of malicious code.



The problem is that many open source projects are hobby projects like that of our listener, which would otherwise not need to be signed.  So this general signing requirement was already imposing a burden on developers.  But now the stakes are raised even higher, requiring the purchase of hardware for the storage of any code signing key.  And as a side effect, as our listener notes, this also prevents small teams of hobby developers from sharing a single certificate among themselves for the purpose of defraying and amortizing its cost across multiple users.  And it's not as if this is a one-time event, since certificates expire and require periodic renewal.  The hardware won't need renewing, but an updated certificate will need to be installed.



So what I expect will happen is that we'll start to begin seeing code signing servers appearing so that multiple members of a team distributed physically, geographically, will still be able to share a single HSM dongle among themselves.  And when that happens, I sure hope they get their security right, since there will be tremendous pressure from malware authors to also get their malicious software signed by those same code signing servers.



Now, as we know, I wrote such a thing myself as part of SpinRite 6.1's launch, since everyone's SpinRite download is unique and needs to be individually signed.  And I commented here a few months ago, when we learned that EV certs were losing their special treatment, that I had apparently wasted my time doing that because my next certificate would not be EV and would therefore not need to be contained within an HSM.  It turns out my time was not wasted after all.  Everyone who signs code will need to use an HSM to do so as soon as their current non-HSM code signing certificate expires.  Wow.  Yet another tax put on the good guys by malware.  It's unfortunate.



And again, certainly nothing prevents anyone from automating the code signing process.  As I found, due to Microsoft's pathetic documentation for doing this, for me it was a heavy lift.  I got it working.  It's been surprisingly, which is to say utterly bulletproof, since I finally finished it, and I'm thankful for that.  But boy, I'm sure somebody will do it for Linux and open source it, and then code signing servers will be something we start to see.



And for what it's worth, B.E., it is possible to rekey a certificate for installation in a second hardware dongle.  So if you ended up purchasing two dongles, one for each location, you could still only purchase one certificate key.  The process of installing it does not give you any control over it, and this is by design, so that it can only get installed in a single device.  But it can be immediately rekeyed and then installed into a second device.  So at least you won't need to be doubling up on purchases if you have two sets of hardware.  But, boy, it's very clear this is what the industry has done.  And it's going to, you know, it's attacks on open source software, and I think it's really unfortunate, Leo.



LEO:  Yeah, I agree, hundred percent.  All right.  I want to talk about code.microsoft.com.  I didn't know anything happened to it.  But you're going to tell us all about it.  All right.  What happened to code.microsoft.com?



STEVE:  So the page I ran across at Microsoft, and I don't recall how it came to my attention, has the intriguing title: "Examining the Deception Infrastructure in Place Behind code.microsoft.com."  Okay.  The Deception Infrastructure?  What?  Well, it turns out that the reader is not left to wonder long since this piece starts out:  "The domain name code.microsoft.com has an interesting story behind it.  Today it's not linked to anything, but that wasn't always true."  And as a matter of fact, yesterday I did an NSLOOKUP and the domain, and there's no name resolution.  So they completely disconnected it.



He writes:  "This is the story of one of my most successful honeypot instances, and how it enabled Microsoft to collect varied threat intelligence against a broad range of actor groups targeting Microsoft.  I'm writing this now as we've decided to retire this capability."  Okay, now, that's not the good part.  The astonishing part is how this got started.



So here's what he wrote:  "Code.microsoft.com was an early domain used to host Visual Studio code and some helpful documentation.  The domain was active until around 2021, when this documentation was moved to a new home.  After the move, the site behind the domain was an Azure App Service site that performed redirection, thus preventing existing links from being broken.  Then, sometime around mid-2021, the existing Azure App Service instance was shut down, leaving code.microsoft.com pointing to a service that no longer existed.  This created a vulnerability.



"This situation is what's called a 'dangling subdomain,' which refers to" - which as far as I know Microsoft just made up, never heard that before, a dangling subdomain - "which refers to a subdomain that once pointed to a valid resource, but now hangs in limbo."  Again, never - limbo's not a term.  That's something you normally do where you have to like lean over backwards and get underneath a horizontal bar.  I don't know, you know, limbo.  Okay.



So he says:  "Imagine a subdomain like blog.somedomain.com that's used to handle a blog application.  When the underlying service is deleted - the blog engine - you might update your page link and assume the service has been retired.  However, there is still a subdomain pointing to the blog."  What?  "This is now 'dangling' and cannot be resolved."  Okay.  He says:  "A malicious actor can discover the dangling subdomain."  Except no.  It's a subdomain of your own domain.  So a malicious actor, what do you mean they can discover it?  Anyway, he says - this is what he said.  "A malicious actor can discover the dangling subdomain, provision a cloud Azure resource with the same name, and now visiting blog.somedomain.com will redirect to the attacker's resource."  What?  He says:  "Now they control the content."



He says:  "This happened in 2021 when the domain was temporarily used to host a malware command-and-control service.  Thanks to multiple reports from our great community, this was quickly spotted and taken down before it could be used.  As a response to this, Microsoft now has more robust tools in place to catch similar threats."



Okay.  So first of all, let me just say "holy crap," and I hope that no one listening to this while driving just lost control of their vehicle because this is nothing short of insane that that could happen.  I'm not trained up on Azure, and on how or why it might be possible for a so-called "dangling subdomain" of Microsoft.com to be casually commandeered by someone not Microsoft, by giving their own Azure cloud instance the same name as an unassigned Microsoft subdomain.  All I can surmise is that there must be some serious architectural design problems over in Microsoft land for that to ever have been possible.  That's just nuts.



But in any event, this author continues by posing the rhetorical question:  How did it become a honeypot?  He says:  "Today it's relatively routine for MSTIC to take control of an attacker-controlled resource and repurpose these for threat intelligence collection."  Right?  Like they'll take over some domain that was an existing command-and-control server and run it in order to gain intelligence.  He wrote:  "Taking control of a malware command-and-control environment, for example, enables us to potentially discover new infected nodes."  Right.  In other words, because the infected machines will be phoning home to the mothership for instructions.



So he says:  "At the time of the dangling code" - you know, code.microsoft.com - "subdomain," he says, "this process was relatively new.  We wanted a good test case to show the value of taking over resources versus taking them down.  So instead of removing the dangling subdomain, we pointed it instead to a node in our existing vast honeypot sensor network."  He says, and just for anyone who doesn't know, but everyone does:  "A honeypot is a decoy system designed to attract and monitor malicious activity.  Honeypots can be used to collect information about the attackers, their tools, their techniques, and their intentions.  Honeypots can also be used to divert the attackers from the real targets to consume and waste their time and resources.



"Microsoft's honeypot sensor network has been in development since 2018.  It's used to collect information on emerging threats to both our and our customers' environments.  The data we collect helps us be better informed when a new vulnerability is disclosed and gives us retrospective information on how, when, and where exploits are developed.  This data becomes enriched with other tools Microsoft has available, turning it from a source of raw threat data into threat intelligence.  This is then incorporated into a variety of our security products. Customers can also get access to this via Sentinel's emerging threat feed.



"The honeypot itself is a custom-designed framework written in C#.  It enables security researchers to quickly deploy anything from a single HTTP exploit handler in one or two lines of code, all the way up to complex protocols like SSH and VNC.  For even more complex protocols we can hand off incoming connections to real systems when we detect exploit traffic and revert these shortly after.  It is our mission to deny threat actors access to resources or enable them to use our infrastructure to create further victims.  That's why in almost all scenarios the attacker is playing in a high-interaction simulated environment.  No code is run.  Everything is a trick or deception designed to get them to reveal their intentions.



"Substantial engineering has gone into our simulation framework.  Today" - get this - "over 300 pseudo-vulnerabilities can be triggered through the same exploit proof-of-concepts available in places like GitHub and Exploit DB.  Threat actors can communicate with over 30 different protocols and can even 'log in' and deploy scripts and execute payloads that look like they're operating on a real system.  There is no real system, and almost everything is being simulated."



Okay.  So, wow.  Let me just say "props where it's due," and it's definitely due here.  That is some seriously cool technology.  They've created "The Matrix" - a simulated, deliberately vulnerable environment that's designed to lure bad guys into believing that they've successfully exploited any of more than 300 known vulnerabilities on a machine, while retaining the control that the actual exploitation of the vulnerability was designed to bypass.  So it looks like a duck, and it quacks like a duck, but it ain't no duck.  Very, very cool tech.



So he continues:  "It's important that in standing up a honeypot on an important domain like Microsoft.com it wasn't possible for attackers to use this as an environment to perform other web attacks, attacks that might rely on same-origin trust."  Meaning they had to make sure that bad guys could not originate their attacks from inside Microsoft.com because that's where code.microsoft.com was.  You can imagine that everybody knows what Microsoft.com networks are, and it would not be a stretch to imagine that there are some enterprises that have whitelisted Microsoft networks after having to put lots of individual whitelist IPs - some guy just said, oh, forget it, let's just whitelist the whole /16 or whatever Microsoft has.  So again, origin of trust.  They could not allow the origin to be Microsoft.com.  So he said:  "To mitigate this further, we added the sandbox policy to the pages which prevents these kinds of attacks."  



So, he writes:  "What have we learned from the honeypot?  Our sensor network has contributed to many successes over the years.  We've presented these at computer security conferences in the past, as well as shared our data with academia and the community.  We incorporate this data into our security products to enable them to be aware of the latest threats.  In recent years this capability has been crucial to understanding the zero-day and n-day ecosystem.  During the Log4Shell incident we were able to use our sensor network to track each iteration of the underlying vulnerability and associated proof-of-concept all the way back to GitHub.  This helped us understand the groups involved in productionizing the exploit and where it was being targeted.  Our data enables internal teams to be much better prepared to remediate, and provides the analysis for detection authors to improve products like Microsoft Defender for Endpoint in real time."  That's MDE.



"The team developing this capability also works closely with the MSRC who track our own security issues.  When the Exchange ProxyLogon vulnerability was announced, we had already written a full exploit handler in our environment to track and understand, not just the exploit, but the groups deploying it.  This kind of situational awareness enables us to give clearer advice to the industry, better protect our customers, and integrate new threats we are seeing into Windows Defender and MDE.  The domain code.microsoft.com was often critical to the success of this, as well as a useful early warning system.  When new vulnerabilities have been announced, threat actors can often be too consumed with trying to use the vulnerability as quickly as possible than checking for deception infrastructure like a honeypot.  As a result, code.microsoft.com often saw exploits first.  Many of these exploits were attributed to threat actors MSTIC already tracks."



Okay.  So it is very interesting that the announcement of a new vulnerability immediately triggers a mass frenzy as - you know, we've talked about this effect before; right? - as attackers, who are literally everywhere, scurry to take advantage of it before machines are patched.



Okay.  So the author continues:  "What happened next?"  He says:  "The 'code' subdomain had been known to bug bounty researchers for several years.  So whenever they would receive a report from someone who believed that they had discovered a critical vulnerability for this domain, these would be closed to let them know they had found a honeypot.  We've asked these security professionals to refrain from publishing details of this service in an effort to protect the value we received from it.  We've also understood for a while that this subdomain would eventually need to be retired once its existence had become too well known to be of value.  That time finally arrived.



"On April 25th, a sudden uptick in traffic to the subdomain, and posts on Twitter, revealed that the domain was being investigated by broad groups of individuals.  Since this discovery meant that the secret was out, and the subdomain had lost its value, we decided to fully reveal the truth and retire the system."  I have a chart in the show notes that shows this, where they're basically ticking along at almost nothing, and then over the course of a couple days the traffic just explodes.



He said:  "The timeline gives an order of events from our perspective.  It's unknown exactly how the full exploit URL of our server ended up in Google search database, but it looks like this, and the associated discovery on Twitter/X culminated in almost 80,000 WeChat exploits in a three-hour period.  It's unlikely the Google crawler would have naturally found the URL.  Our current theory is that a security researcher found this and submitted a report to Microsoft.  As part of this process, either the Chrome browser or another app found this URL and submitted it for indexing."



Okay.  So in other words, it's very difficult to keep anything a secret on the Internet.  It's easy to imagine that Google would have set up Chrome to feed URLs back to them for bot-crawling indexing.  That way, users of Chrome are unwittingly providing Google with links to index as a means for assuring that Google bots are able to discover everything, even things that are not pointed to by anybody else, as in this case.  In this case they somehow discovered a secret that Microsoft had been trying to keep quiet for several years.



The timeline showed that in March the WeChat exploit appeared in Google search results for the first time.  On April 15th, a redacted screenshot of an exploit mitigation was posted online, and some debate followed as to whether the domain was the code.microsoft.com subdomain.  Six days later, on the 21st of April, Google trends show that many people were now searching for the "code" domains.  Three days after that, on the 24th, they start noticing a significant uptick in traffic to the subdomain.  And finally, on the 26th, they're hit with 126,000 times more requests than average.



They write:  "By the 26th of April we were handling 160,000 requests per day, up from the usual between five and 100.  Most of these requests were to a single endpoint handling a vulnerability in the WeChat Broadcast plugin for WordPress (CVE-2018-16283).  This enabled anyone to 'run' a command from a parameter in the URL.  Looking at these URLs, we found 11,000 different commands being attempted.  Most of these pushed a message by some group or another stating that the site had been hacked by them.  So just ego.  This was a simulation, so nothing happened.  Removing these messages gave a clearer picture of the kinds of commands people were entering.



"Most commands entered were Linux recon commands.  These attempted to find out what the system was; what files it contained; and, more broadly, what value it was to Microsoft.  The next biggest group were running command.  These ranged from basic Linux commands like 'whoami,' but a few enterprising folks went on to run scripts of various languages.  Most people who interacted didn't get further than the WeChat exploit.  Over the three busiest days, 63 different exploits in total were triggered.  The biggest surprise was that most researchers stuck to HTTP.  Only three groups probed the other ports, and even fewer logged into the many other services that were available.



"Some of the best investigation came from a Twitter handle @simplylurking2 on Twitter/X who, after discovering that the system was a honeypot, continued to analyze what we had in place and constructed, first constructing a Rickroll and then a URL that, when visited, would display a message to right-click and save a payload.



"With so much information now publicly available, the value of this subdomain was diminished.  On April 26th we replaced the site with a 404 message and are working on retiring the subdomain completely.  However, our ongoing data collection efforts are undiminished.  Microsoft runs many of these collection services across multiple datacenters.  Our concept has been proven, and we have rolled out similar capabilities at higher scales in many other locations worldwide.  These continue to give us a detailed picture of emerging threats."  So that's the story of the rise and fall of a honeypot.



LEO:  What a story.



STEVE:  Which Microsoft inadvertently created, but then managed to put to great use and advantage for several good years before its identity finally leaked and was made public, thus rendering it useless.  We've also seen how the tip of the iceberg for a honeypot is that it can detect that something is wrong on the network.  That's generally sufficient for most purposes.  But as we see, this can also be taken far beyond simple detection with a sufficiently advanced vulnerability simulator to reveal exactly what bad guys will do when they're given more rope to hang themselves.



LEO:  I love it.



STEVE:  Wow.



LEO:  I used to go to code.microsoft.com all the time to download VS code.  I had no idea that they had abandoned it.  That is a wild story.  Wow.



STEVE:  Yeah.  And Leo, again, the idea that, like, not having something responding to code.microsoft.com, which used to be hosted by Azure, allowed somebody else to register that.  Again...



LEO:  That's amazing.



STEVE:  I hope that somebody's looking at this architecture because something is broken, if that's possible.



LEO:  You shouldn't be able to create a subdomain if the domain is owned by somebody else.  That doesn't make any sense.



STEVE:  I know.  I know.  It's insane.



LEO:  Is it still that way?  I mean, should I worry about TWiT.tv, people getting subdomains on our own...



STEVE:  Are you on Azure?



LEO:  No.  Whew.



STEVE:  I'm not.



LEO:  Oh, what a relief.



STEVE:  No, I mean, no one's ever heard of this.  It's just nuts.



LEO:  No.  Crazy. 



STEVE:  The only thing that - so code.microsoft.com must have pointed to an IP in Azure.



LEO:  Right.



STEVE:  And something is rotten in Denmark.  I've got nothing against Denmark.  But, like, this - wow.



LEO:  Shouldn't you blame the registrar?  I mean, isn't that - or the DNS resolver?  Isn't that...



STEVE:  No.  No.  It's somebody somehow created their own Azure instance.  And because it was named code.microsoft.com...



LEO:  That's all it needed was a name.



STEVE:  ...somehow it glued itself to that subdomain, which they called "dangling," a "dangling subdomain."



LEO:  A dangling subdomain.



STEVE:  No one's ever heard of a dangling subdomain.  Subdomains don't dangle.



LEO:  Head up.  What a story.  Well, at least they got some good out of it; right?  I think that's...



STEVE:  Yeah, it did.  They turned it around.  And the fact that it was in Microsoft, and they had some seriously cool tech, I mean, again, what I think must be the case, as I was reading this to our listeners, I was thinking why don't we get the sense in general that Microsoft is this good?  I mean, there are parts of Microsoft that are really good.



LEO:  They're smart people.



STEVE:  They're just buried so deeply down in the infrastructure that, you know, you just talk to morons on the surface.



LEO:  Well, I don't even think it's that.  I think it's just so complex that things fall through the cracks.  So David Redekop was in...



STEVE:  Oh, Leo, just go to answers.microsoft.com, and you will swim in moron, the most moronic nonsense you have ever seen.



LEO:  The intern did it.



STEVE:  Oh, my god.



LEO:  It's the intern's fault.  David Redekop, who we were just talking about, is in our Discord.  I guess you're a Club member, thank you, David.  He says Azure is the authoritative DNS for an Azure tenant.  And there's your problem right there; right?  They've decided.  We don't need no stinking DNS service.  We'll do it.  We'll resolve the domain.  We can do that.



Steve, you've always been full of fascinating stuff.  Today was no exception.  Another great episode of Security Now! in the vault.  978 done, 22 to go, and we can begin a new era as 1000, Episode 1000.  Yay.  You'll find Steve at GRC.com.  You can email him there, securitynow@grc.com.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#979	

DATE:		JUNE 18, 2024	

TITLE:		THE ANGLE OF THE DANGLE

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-979.MP3

LENGTH:	108 MINUTES



DESCRIPTION:  Why is updating your Windows laptop with last week's patches potentially much more important than usual?  Copilot+'s Recall feature won't be released today; what happened?  Was Recall recalled?  What does Johns Hopkins' well-known cryptographer think about Apple's new Private Cloud Compute concept?  How could the WGET command-line utility possibly have a CVSS 10.0 vulnerability?  Or does it?  What order did Google, Cloudflare, and Cisco recently receive from a Parisian court?  And after a brief GRC email update and three pieces of closing-the-loop feedback from our listeners, we're going to examine exactly how Microsoft lost control of their code.microsoft.com subdomain and why the underlying problem is far bigger than them.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  You're going to get to watch us try out some new technology if you're watching the video.  We're using Restream to produce this show today, which gives us some interesting new features.  Steve's going to talk about Patch Tuesday.  Holy cow.  Microsoft patched one of the worst flaws I've ever heard.  It gives Microsoft and all of us reason to think maybe Recall should be recalled.  In fact, that's exactly what happened.  Do you use WGET?  Maybe you shouldn't.  And then, finally, Steve's going to talk about how that kind of DNS poisoning that he talked about last week at code.microsoft.com, how that happens and how you need to configure DNS to prevent this.  I'm talking to corporate IT folks.  It's all coming up next on Security Now!.  Stay here.



LEO LAPORTE:  This is Security Now!, Episode 979, recorded June 18th, 2024:  The Angle of the Dangle.



It's time for Security Now!, the show where we cover the latest news in security, privacy, security, healthiness, and sci-fi with that guy falling over because I'm going on so long with his intro, it's Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.  I hope I sound the same to everyone, as good as I have for the last 20 years.  We're on new technology today.



LEO:  Yes, we are.  We're testing out something called Restream.  The sound, if you're an audio listener, which 90% of you are, should not be any different.



STEVE:  You sound just the same to me except the headphones are distracting because the cord's coming off of the wrong side.  I don't know.  I don't know.



LEO:  Will it help if I do this?



STEVE:  Oh, it's much better.  Now I recognize you.  It's Leo.



LEO:  Who is this guy with [crosstalk] headphones.



STEVE:  I recognize your voice, but, you know.  Okay.



LEO:  Actually, for years people have told me that I'm wearing my headphones backwards because this is supposed to be the left ear.  So I was actually doing it - now I'm strangling myself.  I was actually doing it - this is correct.  Let's go back.  Anyway, to explain what's going on, if you're watching video, it does look a little bit different, and that's because we're using a new program called Restream.  And this is so that we can, sad to say, but a sad necessity, close the studio in a month or so and move to a local recording from my house.  And this is how we're going to do it.



We won't be able to bring the TriCaster and all the beautiful Telos Axia hardware that we use for audio.  But Restream is a really great solution that allows us to pretty  much do the same thing that we've always done.  We have - Benito's here.  He's our TD remotely.  So he's at his house.  He can - everybody can work from home.



STEVE:  What?



LEO:  Yeah.



STEVE:  Cool.



LEO:  And so he's going to watch the show.  Kevin King's probably going to do it, too.  Depends who ends up producing it.  But the producer of the show will watch the show, can switch.  But I said, no, I want the capability to do things like this.  And this.  Woo, it's fun.  Steve was flipping himself back and forwards like this earlier.  So in a way this is kind of a cool platform because it gives me some interesting capabilities, one of which I really think we're going to enjoy, which is I can pull up, from chat, people talking.  So if you're in the chatroom, like Emanuel is, you can chat with us.  Now, we can see from the little icon that he's watching on YouTube.  The other thing that happens with this is we're able to stream onto Twitch, YouTube, and other platforms, as well.



STEVE:  Did we lose Discord?



LEO:  We don't stream video into Discord.  But you know what, video on Discord was always kind of bad.



STEVE:  I would keep losing the audio when I was trying to watch.



LEO:  Yeah, it's not good.  So we're going to push people - and we can actually do X, Instagram, and other platforms, as well.  So eventually this will give us the capability to be in more places.  And, you know, Emanuel's chat was from the YouTube chat.  So chatters from everywhere, watching on any stream, will actually be able to converse with us.  And if there are good comments, instead of me reading them, I can pull them up.  So that's pretty nice.  So the Discord chat is still there.  The video - that's Patrick, our engineer.  The video is not.  It was meh, as Patrick said.  So I think this is going to be, once my fingers become trained...



STEVE:  Tuned, yes.



LEO:  Yeah, because it is, it's a lot of stuff to learn for me.  But I think there's a lot of benefit to this.  One big dysfunctional family, as DCrash says.  Everybody's included, Lou.  So this is going to be fun.  Now, Steve, yes, the real question is what's coming up on the show today?



STEVE:  So we've got a bunch of interesting things to talk about.  We're going to learn why updating your Windows laptop with last week's patches is potentially more important than it has been for a long time.  Also, today's June 18th.  Copilot+'s Recall feature...



LEO:  That's right, yeah.



STEVE:  ...will not be released today.



LEO:  Nope.



STEVE:  What happened?



LEO:  Victory.



STEVE:  Was Recall recalled?  I think so.



LEO:  I can't recall.



STEVE:  Also, what does Johns Hopkins well-known cryptographer think about Apple's new Private Cloud Compute concept?  How could the WGET command-line utility possibly have a CVSS vulnerability rating of 10.0, which we know is reserved for, you know, end-of-the-world events?  Or does it?  What order did Google, Cloudflare, and Cisco recently receive from a Parisian court?  And after a brief GRC email update and three pieces of closing-the-loop feedback from our listeners, we're going to examine exactly how Microsoft lost control of their code.microsoft.com subdomain.



LEO:  Oh, okay.



STEVE:  And why the underlying problem is far bigger than them.  Thus today's podcast title, "The Angle of the Dangle."



LEO:  Ah.



STEVE:  For Security Now! #979, closing in on that magic 999.  Will we be able to go to four digits?  That's the question.  What will break?  It's our own Y2K.  We'll find out.



LEO:  Is 999 going to break something?



STEVE:  Yeah.  My whole system actually, as I said in the beginning, the reason the stopping at 999 ever was brought up was that my system will collapse if I put in four digits because I thought, you know, back in, what, 1923 when we began this, I thought, we're never going to be doing this in 20 years.



LEO:  Yikes.



STEVE:  Maybe we're going to run out of steam after the 15th podcast.  But here we're going.



LEO:  Well, we'd have to face it.  But you know what, I think there's probably more than a few people who say, whatever it takes in 21 episodes, we're glad that there's going to be an Episode 1000.



STEVE:  Yes.  And I will spend some time to figure out - actually, the problem was that I was redirecting clicks at GRC.com through Podtrac in order to give TWiT credit for...



LEO:  Oh, but you don't have to do that anymore.  We don't do that anymore.



STEVE:  Well, so then there's not going to be any problem.  I'll just pull the plug on the Podtrac TWiT redirector code, and we're good to go.



LEO:  Okay.



STEVE:  Yeah.



LEO:  All right.  We're going to pause, and Leo's going to push a bunch of buttons including Shift Nancy, which apparently...



STEVE:  If I'm still here after that, then we'll have a podcast.



LEO:  Shift Nancy did the job.  And now I'm going to click this link, and welcome...



STEVE:  Thought it was Nora.



LEO:  ...Steve Gibson in his continuing quest to make us all safer online.  It's all yours, Steve.



STEVE:  So to that end we actually do have, we're going to end up with some interesting takeaway.  Our Picture of the Week is sort of a placeholder.  You know, it's not one of our crazy fences out in the middle of nowhere with a bunch of sheep standing behind the fence, not willing to go around for some reason.  That one was a real puzzler.  This is a diagram that demonstrates the process of CNAME, DNS record-based subdomain takeover.  And I gave this...



LEO:  Oh.  Well.



STEVE:  Yes, yeah.  And it's got lots of pretty - it looks like something you would get, you would use your box of Crayola crayons to color in.  I gave it the title, "Fortunately, as we'll see today, the 'Subdomain Takeover' problem is much less confusing than this diagram!" because, I mean, I understood the problem immediately when I realized that's what had happened at Microsoft.  I think maybe, Leo, every one of our listeners said, "I know what happened.  I'm going to tell Steve."  So, boy, does the email system work.  But anyway, I had to sort of figure out the diagram after already knowing what the problem was.  So just for what it's worth, to anybody who, like, gets lost in these arrows, don't worry.  By the end of the podcast, you'll understand what's going on.



LEO:  Good.  Because it's not obvious.



STEVE:  No, it's not.  The diagram does not help much.  I need to begin this week by making 100% certain that everyone listening is aware of a flaw that was patched during last Tuesday's Windows patch fest.  It's CVE-2024-30078.  And the only reason that it only has a CVSS score of 8.8, rather than 11, on a scale that maxes out at 10, is that an attacker needs to be within WiFi radio range of any Windows machine.



LEO:  Oh.



STEVE:  Uh-huh, any Windows machine with WiFi enabled that has not been updated with last Tuesday's patches.



LEO:  Oh, that's terrible.



STEVE:  It's unbelievably bad.



LEO:  Because one network might have multiple Windows machines, and one that's close to the window.



STEVE:  Yes.



LEO:  Right?



STEVE:  Being, you know, that it's Windows.  And, yeah.  Or somebody has a laptop and, you know, for whatever reason they've learned, for example, that, well, you may not want to apply those patches immediately.  Let them kind of sit for a while to see if Microsoft made a mistake, and apply them later.  Okay.  In this case, we have the worst of all possible problems.



Last Tuesday eliminated a fortunately very rare remote wireless radio takeover of any Windows machine which is using its native WiFi protocol stack.



LEO:  Geez.



STEVE:  This affects all versions of Windows ever, so that means that any machine that was not updated, as I said, for whatever reason last Tuesday, is vulnerable today.  Microsoft is only saying "all supported versions of Windows," but that's what they always say.  And that typically means all earlier versions, too, but they don't want to say that because they've decided they don't want to support those anymore.  So good luck to you.  And the fact that this appears to be Windows-wide suggests that the flaw was in a core component that they have not been messing with for the last, you know, 15 years.  Because, as we know, mostly Windows is just new levels of, you know, new layers of candy coating on top of the core, which they're afraid to touch.  Which thank god, you know, leave some of it alone, at least.



So to provide some additional color and perspective, I'll share what Forbes' cybersecurity guy wrote under their headline "New Wi-Fi Takeover Attack - All Windows Users Warned To Update Now."  The guy wrote:  "Microsoft has confirmed a new and quite alarming WiFi vulnerability in Windows, which has been rated 8.8 out of 10 in terms of severity using the Common Vulnerability Scoring System.  The vulnerability, assigned as CVE-2024-30078, does not require an attacker to have physical access to the targeted computer."  Exactly as you said, Leo, just be standing outside the building.  "Although physical proximity" - meaning you cannot do it from Russia.  In fact, that's why it's not a 10.0. If you could do it from Russia it would be a 10.0.  But you've got to be nearby, you know, WiFi range.



"Exploiting this vulnerability," he writes, "can allow an unauthenticated attacker to gain remote code execution on the impacted device.  What's perhaps most concerning, though, is that this WiFi driver security flaw affects all supported versions of the Windows operating system.  Microsoft has confirmed that with no special access conditions or extenuating circumstances, apart from the proximity requirement, an attacker could 'expect repeatable success against the vulnerable component.'  Microsoft also warns that an attacker requires no authentication as a user on that machine before exploiting this vulnerability, nor any access to settings or files on the victim's machine before carrying out the attack.  Further, the user of the targeted device does not need to interact at all.  There's no link to click, no image to load, no file to execute.



"Jason Kikta, chief information security officer at Automox, said that, given its nature, 'this vulnerability poses a significant risk in endpoint-dense environments including hotels, trade shows, or anywhere else numerous devices connect to WiFi networks.'  In these kinds of environments, it would be all too easy for an attacker to target users without raising any red flags.  To protect against this vulnerability, it's recommended that you apply the latest patches as soon as possible.



"Assuming, that is, you are using a version of Windows that still receives security updates.  'Anyone using an end-of-life version of Windows without an extended service contract is recommended to update to a supported version as soon as possible,' Kikta said.  'If patching immediately isn't feasible, you must use endpoint detection to monitor for suspicious activity related to this vulnerability.  Due to its unique nature, it's unlikely to be visible to network-level detection methods.'"  Meaning, you know, it's down in the kernel, you know, deep stack, before this even gets up to the network interface level at a higher level.  "He says:  'The risk of running outdated software cannot be overstated.'"



The article then concludes:  "In case you need any further incentive to get patching as soon as possible, Kikta said:  'This close access vector threat potentially bypasses network-based detections and mitigations.  It circumvents most threat modeling, so this is an immediate patch priority for me,' he said.  'Most security experts agree that publicly available exploitation tools will be available before long, so the window of opportunity to patch before the attacks start is getting smaller every day.'"



And I should note that GitHub does indeed already have a vulnerability detection and command execution script posted.  So that has happened.  Not malicious, but it's easy to take that script and make - actually they did have - they had it commented out where you're able to provide the command.  So technically not malicious, but it doesn't even take a script kiddie in order to make this do something bad.



LEO:  Wow.



STEVE:  Microsoft, in their own tracking of this CVE, enumerates the following characteristics, which are bracing.  They said under "Attack Vector," they said "Adjacent."  And they wrote:  "The vulnerable component is bound to the network stack, but the attack is limited at the protocol level to a logically adjacent topology.  This can mean an attack must be launched from the same shared physical, i.e., Bluetooth or IEEE 802.11, you know, which is WiFi; or logical, local IP subnet network; or from a secure or otherwise limited administrative domain."



Under "Attack Complexity," this is not what you want to hear, "Low Complexity attack."  They said:  "Specialized access conditions or extenuating circumstances do not exist.  An attacker can expect repeatable success against a vulnerable component."  But privilege is required.



LEO:  Wow, this is bad.



STEVE:  Leo, this is as bad as it gets.  This is a shocking flaw for Windows to have.



LEO:  CBits wants to know if the firewall stops it.



STEVE:  No.



LEO:  Oh, my god.



STEVE:  The firewall is at the border, and this is inside the boundary that the firewall protects.



LEO:  Unbelievable.



STEVE:  For "Privileges Required:  None.  The attacker is unauthorized prior to attack, and therefore does not require any access to settings or files to carry out the attack.  User Interaction:  None.  The vulnerable system can be exploited without any interaction from any user.  Confidentiality," they said, "High Threat.  There is total loss of confidentiality."  And this is where the favorite comment I saw on the Internet was, "But don't worry, your Recall data is encrypted."  It's like, uh-huh, right.



So, I mean, this is exactly the kind of problem that Microsoft ignores when they say, oh, storing all the history of your computer, not a problem.  We've got you.  We've got your back.  And here's a flaw in the WiFi protocol stack that probably dates back 15 years.  This is, you know, I think 7 is out of extended service now, but 8 is still in it.  And 8 is vulnerable.  So we're talking this has been around for a long time.  And we don't know who already knows about it.  We just know that it finally came to light.



And for Integrity they said:  "High Threat.  There is a total loss of integrity, or a total loss of protection."  This is Microsoft saying this of their own flaw.  "For example, the attacker is able to modify any/all files protected by the impacted component."  And then they wrapped it up with two FAQ questions.  They said:  "Question:  According to the CVSS metric, the attack vector is adjacent.  What does that mean for this vulnerability?  Answer:  Exploiting this vulnerability requires an attacker to be within proximity of the target system to send and receive radio transmissions."  And the second question:  "How could an attacker exploit the vulnerability?  Answer:  An unauthenticated attacker could send a malicious networking packet to an adjacent system that is employing a WiFi networking adapter, which could enable remote code execution."



So, you know, the appearance, as I said, the appearance of this vulnerability provides a perfect case-in-point demonstration of why the presence of Recall running in Windows machines represents the threat that it does.  I have no doubt that Microsoft's heart - their heart, not their brain - is in the right place.  They're not an evil empire.  But they're now attempting to add an extremely powerful and high-risk feature to an old, creaky, and bug-ridden operating system.  That's the only way you can characterize Windows, an old, creaky, bug-ridden operating system.  Again, they keep adding.  They, you know, change the UI and make the desktop look different.  But we all see dialogs that we remember from Window 95, so it's still in there.  Well, okay, Windows 2000.



Anyway, this vulnerability demonstrates what we all intuitively feel, which is that Windows is not up to the task of protecting anything as valuable as everything that our machine has ever displayed on its screen.  You know?



LEO:  The scenario is very simple.  You've got Recall running.  Grandma has Recall running on her home network.  She's got WiFi running.  Some guy sits out on the curb, joins her computer that's in the window - she doesn't even have to be on that one - can then easily get into the other computers and access the data.



STEVE:  Yeah.



LEO:  I mean, it's a very - this is why you hated the idea of Recall in the beginning.



STEVE:  Yes.  Yes.  Exactly.  You know, it's just they don't, you know, Microsoft has come up with, I would argue, a killer idea, the idea of storing all of the history and then, probably in the future, creating your own personal AI expert that knows everything you've done.  That, I mean, as I said when we first discussed it, that is transformative.  It could change, it would, will change people's relationship with computing.  I mean, I think it's really huge.  The problem is they haven't been spending any time on the security of their system.  They keep giving us ridiculous crap features that nobody wants.  And we have, what, 53 flaws I think this month, 100, more than 100 last month.



LEO:  Which is low.  53 is low.  That's the thing.  It's a small number, relatively.



STEVE:  And then here this comes along, which is in every version of Windows.  It's been in there for who knows how long.  So I just, you know, it just, I mean...



LEO:  Amazing.



STEVE:  I'm sure, as I said, somebody, the smart people at Microsoft are thinking, wow, wouldn't it be cool if we actually had an operating system that was safe to put this kind of capability into.  They don't.  And they just can't get one by saying they do.  You know, we have a history of Microsoft declaring each version of Windows is more secure than the last one.  Meanwhile, this has been in there the whole time.  And who knows what else?  How many flaws have they got queued up for July?



LEO:  So, I mean, and just to be clear, this Recall's not a crap feature.  It's a feature people would want, and really cool.  It's just you're putting it on a platform that isn't secure.



STEVE:  Right.  It is a rickety, insecure platform.  And that's fine if you want to run Excel.  Okay, Leo, I know you've messed with security certificates in the past.  Have you ever looked at a certificate just to sort of make sure that it was correct?



LEO:  No.



STEVE:  You know, we all know what they look like; right?



LEO:  I mean, I've used Let's Encrypt.  I have Let's Encrypt certificates on my NAS and on various places.  I never look at them.



STEVE:  I would imagine, well, they're just text.  It's a little text file, a 2K or 4K text file.  And if you look at it, you can see that it's a nicely, you know, it's a Base64 encoded blob.  If that appears on your screen, and Recall snaps it, you've just lost privacy of your private key.



LEO:  I never thought of that.  So the information that somebody would need, the private key is in plaintext in that...



STEVE:  Yes, yes.  Certificates are viewable as text.



LEO:  The private key.  So in public/private key crypto, the public key can be given to everybody, and is, and that's why it works.  But you keep that private key close to your chest because if somebody got it, they can impersonate you.



STEVE:  And if you ever open it on your computer and look at it, Recall snaps it.



LEO:  I never thought of that.  Of course they could have a switch that says, oh, yeah, if there's a certificate, don't look at it.  Because they have some things don't look at it.  But then, and then...



STEVE:  And then, and then, and then, exactly.  Yeah.



LEO:  Because we do stuff, private stuff on our computers.  Duh.



STEVE:  Yeah.  You know, and we want to be able to.  I've been thinking about this a lot.  And I'm thinking, okay, if you were conscious of the fact that your computer was spying on you because you want it to, I mean, like, with your permission it's trying to collect all of this.  But then this does, for anyone who's privacy-minded, they've got to be constantly turning it off before, not after.



LEO:  Right.



STEVE:  Although I guess you are able to go back and kill, like, some period of time.



LEO:  Oh, good luck editing stuff.



STEVE:  So, oh, crap, I forgot to turn off Recall.



LEO:  Yeah, yeah.



STEVE:  Again, you've got to remember.  So if you want this, and you want privacy, and you're serious about it, then you're constantly toggling this thing on and off so that it doesn't capture things that it shouldn't capture that you don't want to have your expert know.  Otherwise, Russia gets into your computer and says, uh, show me all of the private keys that have been on the screen.  Ping, there they are.  So, yes, super search.



LEO:  Wow.



STEVE:  Speaking of Recall, first they switched Recall to opt-in, which we covered last week.  That was a welcome move. 



LEO:  Yes.



STEVE:  But as we know, I was still quite worried that Microsoft would nevertheless be promoting the heck out of it.  And why would Microsoft ever mention the very real danger inherent in having any machine storing its entire history?  You know, they're not going to say, oh, well, we want you to do this, but danger, Will Robinson.  No, they're going to just - they're not going to tell you what could go wrong.  So the latest good news is that Recall has now been completely removed from today's June 18th production release of Windows Copilot+.  It will initially only be available to users participating in the Windows Insider Program.



Last week, I shared Microsoft's blog posting where, amid all of their mumbo jumbo about how they're putting security first, they're explaining, you know, that they're going to be switching Recall to opt-in and only decrypting, you know, on the fly after you've given them a blood sample and chapped your heels three times.  In an update to that blog posting last Thursday, they added:  "Update:  June 13th, 2024:  Today, we are communicating an additional update on the Recall (preview) feature for Copilot+ PCs.  Recall will now shift from a preview experience broadly available for Copilot+ PCs on June 18, 2024, to a preview available first only in the Windows Insider Program (WIP) in the coming weeks.  Following receiving feedback on Recall from our Windows Insider Community, as we typically do, we plan to make Recall (preview) available for all Copilot+ PCs coming soon."



So again, yay, they backed off.  They said:  "We're adjusting the release model for Recall to leverage the expertise of the Windows Insider community to ensure the experience meets our high standards for quality and security.  This decision is rooted in our commitment to providing a trusted, secure, and robust experience for all customers and to seek additional feedback prior to making the feature available to all Copilot+ PC users.  So nobody gets Recall - yay - until the insiders have played with it, people have experimented with it, and we've learned collectively more about it."



So this is terrific news.  And I'm sure Microsoft will get there eventually, if it wishes to.  And I'm quite certain that it wishes to.  As I said, I believe Microsoft has more in store for a machine's entire usage history than just scrolling back through time, since the actual delivered security of anything can only be judged retrospectively.  That means that only time will tell whether they've been able to sufficiently protect any machine's entire usage history.  What they were on the brink of was obviously half-baked, if that.



So it appears that they allowed their enthusiasm for this new capability to get the better of them.  And as we saw from the reactions of the entire industry, this sort of immature exuberance does not inspire confidence.  The main point I wanted to make is to note that we've never seen anything like Recall before.  It is not just a change in degree.  This represents a change in kind, and it presents a security challenge of an entirely different scale.  And that's just something Microsoft glossed over because, you know, they wanted to.



LEO:  Yup.



STEVE:  And, you know, maybe they're reading their own press.  I don't know.



LEO:  I think they, you know, I mean, in their defense, they heard you and all the other security researchers howling.  And they did respond to it.  I mean...



STEVE:  No, I'm 100% in agreement.  They realized this thing was going to come out and just get panned by, you know...



LEO:  Problems.



STEVE:  ...by, like, everyone.



LEO:  Yeah, yeah.  All right.  You want to take a break here, my friend?



STEVE:  I do indeed. 



LEO:  I have my finger hovering over the button. 



STEVE:  Then we're going to talk about Matthew Green and what he thinks about Apple's private...



LEO:  You know, it's funny, because you're going to talk about Apple in the next one.  And I'm thinking it's very easy for me, an Apple user and not such a big fan of Windows, to go, yeah, Microsoft.  But now, when the shoe's on the other foot, I'm, whoop, let's see.  I may not be so sanguine about the whole thing.  We'll be listening.  Just a moment.  But first, I think, whoops, I've got to do the right thing, Shift Nancy, and tell you about - I'm still learning the keys here.  Why are you laughing, Steve?  What are you laughing at?



STEVE:  I'm just smiling.



LEO:  He's okay.  Steve Gibson.



STEVE:  So to remind everyone of Matthew's pedigree, this is  Matthew Green, he's an American cryptographer and security technologist, Associate Professor of Computer Science at the Johns Hopkins Information Security Institute.  And we're fond of quoting Matthew.



LEO:  He's great.



STEVE:  He's often outspoken about security issues.  And super trustworthy.



LEO:  Super trustworthy.  Super trustworthy, yes.



STEVE:  So in a series of, I think it was 21 individual sequential postings under his Mastodon account, he outlined his feelings about what Apple described and about the nature of the challenge that they're undertaking with their private cloud compute concept.  And I'm going to share this because it was great, and then we'll also talk about it.



So he said:  "So Apple has introduced a new system called Private Cloud Compute that allows your phone to offload complex," he says, "typically AI tasks to specialized secure devices in the cloud."  He said:  "I'm still trying to work out what I think about this.  So here's a thread.  Apple, unlike most other mobile providers, has traditionally done a lot of processing on-device.  For example, all of the machine learning and OCR text recognition on photos is done right on your device.



"The problem is that while modern phone 'neural'" - he has in quotes - "hardware is improving, it's not improving fast enough to take advantage of all the crazy features Silicon Valley wants from modern AI, including generative AI and its ilk.  This fundamentally requires servers.  But if you send your tasks out to servers in the cloud, this means sending incredibly private data off your phone and out over the Internet.  That exposes you to spying, hacking, and the data-hungry business model of Silicon Valley.



"The solution Apple has come up with is to try to build secure and trustworthy hardware in their own data centers.  Your phone can then 'outsource' heavy tasks to this hardware.  Seems easy; right?  Well, here's the blog post."  And then he provided a link to Apple's own disclosure about their private cloud compute.



And he said:  "TL;DR:  It is not easy.  Building trustworthy computers is literally the hardest problem in computer security.  Honestly," he wrote, "it's almost the only problem in computer security.  But while it remains a challenging problem, we've made a lot of advances.  Apple is using almost all of them."  He said:  "The first thing Apple is doing is using all of the advances they've made in building secure phones and PCs in their new servers.  This involves using Secure Boot and a Secure Enclave Processor (SEP) to hold keys.  They've presumably turned on all the processor security features.  Then they're throwing all kinds of processes at the server hardware to make sure the hardware is not tampered with."  He says:  "I can't tell if this prevents hardware attacks, but it seems like a start."



Okay.  And then Matthew includes a screen shot from Apple's posting, which explains.  This is Apple:  "Private Cloud Compute hardware" - get a load of this.  Wow.  I mean, this, I'm a bit of a fanboy for this - "hardware security starts at manufacturing, where we inventory and perform high-resolution imaging of the components of the Private Cloud Compute node before each server is sealed and its tamper switch is activated.  When they arrive in the data center, we perform extensive revalidation before the servers are allowed to be provisioned for PCC (Private Cloud Compute).  The process involves multiple Apple teams that cross-check data from independent sources, and the process is further monitored by a third-party observer not affiliated with Apple.  At the end, a certificate is issued for keys rooted in the Secure Enclave UID for each PCC node.  The user's device will not send data to any PCC nodes if it cannot validate their certificates."



I'm just, you know, nobody has ever done anything like this before.  And as I said, I need to confess that I'm a bit of a fanboy for the idea that Apple is performing high-resolution imaging of each node's components in order to detect anything that might have been done to the server between its design and through its manufacturing.  That's very cool.  And just the fact that it's now widely known that this is being done, likely serves as a deterrent to prevent anyone from even trying to mess with them.



Matt continued.  He said:  "They also use a bunch of protections to ensure that software is legitimate.  One is that the software is 'stateless' and allegedly does not retain any information between user requests.  To help ensure this," he writes, "each server/node reboot re-keys and wipes all storage."  So the idea is it's like Apple has provided a large and growing collection of remote servers, and an individual iPhone user is able to be connected to one of these, exchange keys, independently validate that specific connected node's security, establish a secure tunnel, send the data to this big computation engine, have it do whatever it needs to do on behalf of its user, return the results, and then it shuts down, reboots, wipes memory.



LEO:  Wow.



STEVE:  And then comes back up again so that it's fresh and clean for the next user.



LEO:  For each request?



STEVE:  Yes.



LEO:  For every request?



STEVE:  Yes.



LEO:  It reboots for every request?



STEVE:  It cleans itself out, rekeys, wipes all storage.



LEO:  That seems like a lot.



STEVE:  Yup.  That's Apple.



LEO:  But that's what you have to do; right?



STEVE:  Yeah.



LEO:  But, I mean, there's going to be millions of people using this in a second.  They're going to have a million machines rebooting every second?



STEVE:  Maybe it boots fast, or they have a partition.



LEO:  They're running in RAM or something.  I don't know.  That's wild.



STEVE:  Yeah.  Again, he quotes Apple's announcement, saying - this is Apple:  "We designed Private Cloud Compute to make several guarantees about the way it handles user data."  Three of them.  "First, a user's device sends data to PCC for the sole, exclusive purpose of fulfilling the user's inference request.  PCC uses that data only to perform the operations requested by the user.  Two, user data stays on the PCC nodes that are processing the request only until the response is returned.  PCC deletes the user's data after fulfilling the request, and no user data is retained in any form after the response is returned.  And then, three, user data is never available to Apple, even to staff with administrative access to the production service or hardware."  I mean, so they've literally created a system that they themselves cannot penetrate. 



Matt continues:  "A second protection is that the operating system can 'attest' to the software image it's running.  Specifically, it signs a hash of the software and shares this with every phone and client. If you trust this infrastructure, you'll know it's running a specific piece of software.  Of course, knowing that the phone is running a specific piece of software doesn't help if you don't trust the software.  So Apple plans to put each binary image into a 'transparency log' and publish the software.  But here's a sticky point:  not with the full source code."  And again, Apple is still a private company; right?  You know, they're not Linux.



So Matt quoted Apple, saying:  "Our commitment to verifiable transparency includes" - and here we have four short points.  "Publishing the measurements of all code running on PCC in an append-only and cryptographically tamper-proof transparency log.  Making the log and associated binary software images publicly available for inspection and validation by privacy and security experts.  Publishing and maintaining an official set of tools for researchers analyzing PCC node software."  So they're going to create and provide tools that allow researchers to examine everything that they've done.  And fourth:  "Rewarding important research findings through the Apple Security Bounty program."



Matt says:  "Security researchers will get 'some code' and a VM they can use to run the software.  They'll then have to reverse-engineer the binaries to see if they're doing unexpected things."  He says:  "It's a little suboptimal."  But again, you know, there's a limit to what you can ask Apple to give.  So he says:  "When your phone wants to outsource a task, it will contact Apple and obtain a list of servers, nodes, and their keys.  It will then encrypt its request to all servers, and one will process it.  They're even using fancy anonymous credentials and a third-party relay to hide your IP from themselves."  So they're even masking the IPs of the people using this incredible resource in their own data centers.  



Quoting Apple about this, they wrote:  "Target diffusion starts with the request metadata, which leaves out any personally identifiable information about the source device or user, and includes only limited contextual data about the request that's required to enable routing to the appropriate model.  This metadata is the only part of the user's request that is available to load balancers and other data center components running outside of the PCC trust boundary."  In other words, the metadata's not encrypted in the tunnel, it's just used to get the tunnel endpoint connected within the Compute Center.



They said:  "The metadata also includes a single-use credential, based on RSA Blind Signatures, to authorize valid requests without tying them to a specific user."  Again, you can't even figure out who's doing the asking.  "Additionally, PCC requests go through an OHTTP relay operated by a third party" - that's an anonymizing HTTP relay  "which hides the device's source IP address before the request ever reaches the PCC infrastructure."  Sort of like a mini TOR.  "This prevents an attacker from using an IP address to identify requests or associate them with an individual."  Again, Apple has gone so far beyond what anyone has ever done.  "It also means," they write, "that an attacker would have to compromise both the third-party relay and our load balancer to steer traffic based on the source IP address."



So again, Matt says:  "Okay.  There are probably half a dozen more technical details in the blog post.  It's a very thoughtful design."  He said:  "Indeed, if you gave an excellent team a huge pile of money and told them to build the best 'private' cloud in the world, it would probably look like this."  He says:  "But now the tough questions.  Is it a good idea?  And is it as secure as what Apple does today?  Most importantly, can users opt-out entirely from this feature?"  He said:  "I admit that, as I learned about this feature, it made me kind of sad.  The thought that was going through my head, is this going to be too much of a temptation?  Once you can 'safely'" - and he has that in air quotes.  "Once you can 'safely' outsource tasks to the cloud, why bother doing them locally?"  And Leo, I think the answer is what you just said.  There's a limit to what this, like how much processing Apple could possibly provide in their cloud.



LEO:  Oh, yeah.  Everybody wants to do this on device eventually, not just for privacy but for economy.



STEVE:  Right, right.  He says:  "Once you can safely outsource tasks to the cloud, why bother doing them locally?  Outsource everything."  And the answer is, exactly as you said, you know, it's way better if your distributed computing is in everybody's hands instead of monster servers in the cloud.  He said:  "As best I can tell, Apple does not have any explicit plans to announce when your data is going off-device to Private Compute."  And I notice that his saying that does not feel Apple-esque to me.  It feels like Apple will provide these controls.  We just haven't seen any of it yet.  But they haven't said, you know, they haven't talked about that.



Matt said:  "You won't opt into this.  You won't necessarily even be told it's happening.  It will just happen magically."  He says:  "I don't love that part."  Now, maybe Matt knows something we don't.  Or maybe we haven't seen that yet from Apple.  He said:  "Finally, there are so many invisible sharp edges that could exist in a system like this.  Hardware flaws.  Issues with the cryptographic attestation framework.  Clever software exploits.  Many of these will be hard for security researchers to detect.  That worries me, too."



And that's an interesting point.  We've talked about how, because Apple's smartphone handset technology is so tightly locked down in order to keep bad guys out, unfortunately it also keeps good guys from being able to see what they need to see in order to know what's going on.  Remember that Kaspersky, who are among the best people there are, they had malware in their phones that they couldn't detect until they detected some strange network activity that was being driven by the malware, and that allowed them to begin to pursue what was going on.  But, you know, they can't see inside Apple's iPhones any more than the bad guys can.



LEO:  And that's been a complaint of security researchers forever.



STEVE:  Right.



LEO:  To which Apple, by the way, has responded by saying, okay, we're going to give select researchers access.  I think they need to have kind of a valve, an escape valve so that researchers, legitimate researchers can look in.



STEVE:  Yeah.



LEO:  But I understand why they don't want to do that.  They don't want to [crosstalk].



STEVE:  And it - right.  And Matt's sort of contradicting himself because he, I mean, he's just saying, okay, you know, he's playing devil's advocate because he just told us that Apple will also be making a lot of this as open as they reasonably can.



LEO:  Yeah.



STEVE:  I mean, providing virtual machines for...



LEO:  That's mindboggling.



STEVE:  For people to poke at.  Wow.



LEO:  That's really great.  I mean, that's what they need to do.



STEVE:  So he said:  "Wrapping up on a more positive note, it's worth keeping in mind that sometimes the perfect is the enemy of the really good."



LEO:  Yeah.



STEVE:  "In practice, the alternative to on-device is ship private data to OpenAI or someplace sketchier, where who knows what might happen to it."



LEO:  Right.



STEVE:  He says:  "And of course keep in mind that super-spies are not your biggest adversary.  For many people your biggest adversary is the company who sold you your device and software."  He says:  "This PCC system represents a real commitment by Apple not to 'peek' at your data.  That's a big deal."  And his final tweet was, or whatever you call it on Mastodon...



LEO:  Toot.  We call them toots.



STEVE:  Toot, well, okay.  His final toot.  Bean eater.  "In any case, this is the world we're moving to," he says.  "Your phone might seem to be in your pocket, but a part of it lives 2,000 miles away in a data center.  As security folks, we probably need to get used to that fact and do the best we can to make sure all parts are secure."



And I think Matthew's take is exactly right.  The design of this system is what you would get if a bunch of very good engineers and cryptologists were to deliberately design a system that was meant to transiently extend an individual smartphone's local computing into the cloud for the purpose of performing some very heavy lifting.  It takes advantage of everything we know about how to do this safely and securely.  It will enable Apple's devices to do things no other mobile devices can do.



But I have a concern that Matt did not raise, which is that, because Apple has made this transparent to their users, no one will be able to appreciate the lengths Apple has gone to, to securely offer this capability.  The listeners of this podcast understand that Apple is visually inspecting the motherboards of their servers prior to deployment because, for example, we've covered the worries over tiny chips being added to server hardware or Cisco routers when they're intercepted during transshipping.  Even though that's way out there, it's a factor Apple has preemptively considered.  Who else is going to go to these extremes?



It's not that I'm worried about Apple being underappreciated.  It's that I can easily see "me, too" vendors popping up and offering their own outwardly similar capabilities that APPEAR to be the same...



LEO:  Ah, yes.



STEVE:  ...while providing none of the same true protections.  They'll be able to say:  "We're doing the same thing Apple is doing," thus riding on Apple's coattails while providing far less true security for their users, at a far lower cost to themselves.  The concern is that Apple is legitimizing and popularizing the idea of exporting what could be an extremely personal mobile device data blob to the cloud for cloud-based processing.  Other vendors are going to do the same.  But users of those lookalike services will be placing their users' data at far greater risk than Apple.  And who would ever know?



LEO:  Well, I'll tell you who would know.  People who listen to this show would know; right?  And I think what Apple counts on, you're right, the normal people will not, you know, know this. But what Apple counts on is that the people who do understand it who listen to this show will then kind of spread the word.  And when their less sophisticated friends and family say, well, is this trustworthy, they'll say, oh, yeah, you should see what Apple has done.  They don't need to go into the details.  That's why you should listen to the show.



STEVE:  He, Leo, we have a big improvement with this new technology.  Oh...



LEO:  Your mug is still giant, Steve.



STEVE:  It wasn't darkening my screen.  But it is darkening my screen.



LEO:  Now, Leo's going to push some buttons.  He's going to click a button here, click a button there.  He's going to press Shift November, and now Steve can...



STEVE:  Now I know my name.



LEO:  Now Steve knows his name...



STEVE:  Now I know my name.



LEO:  ...and we'll continue with Security Now!.  Thank you, Steve.



STEVE:  Okay.  So there's buzz in the industry today - today, Tuesday, June 18th - of a recently discovered flaw in the widely used WGET command-line utility.



LEO:  Oh, no.



STEVE:  Yeah.



LEO:  I use this.  I use it all the time.



STEVE:  Actually, it's the way I download the podcast audio every week to recompress it for Elaine.



LEO:  [Muttering]



STEVE:  Some outlets are claiming that this flaw carries an attention-getting CVSS score of 10.0.  But anyone reading that, anyone who's been listening to this podcast for long should immediately be skeptical.  As we've seen, 10.0 scores are pretty much reserved for "end of the world as we've known it" flaws, and it's hard to see how you can have an end-of-the-world flaw that's not remotely exploitable, and probably also wormable without any user interaction at the receiving end.



But WGET is not a server or a service.  It's just a convenient command-line tool used to retrieve files.  As I said, I use it every week to grab this podcast's audio for recompression before I post it for Elaine.  So how any flaw in any command-line tool that's not publicly exposing a vulnerable service to the Internet could rate a 10.0 is beyond me.  I mean, we've seen bad ones that, like, get the 9.8.  And it's like, oh, it got really close, but no.  And they're bad.



But okay.  So I did some digging, and it's true that there is a problem with WGET up through version 1.24.5.  The problem surrounds incorrect parsing of semicolons appearing in the "userinfo" portion of the URL that's passed to WGET.  Okay, now, for those who've been around for a while, you may remember that URLs are technically able to carry a username and password, which appears before the hostname.  So rather than, for example, actually the example I'll use is example.  Rather than  https://example.com, you could have https://username: password@example.com.  And it's that username:password and the @ sign which is the userinfo portion of a URL.  It's sort of been deprecated.  There's still some use for it, but you have to be careful because a username and password in a URL is now considered bad form.  So, you know, use with caution.  So the concern is that mishandled semicolons in that portion of a URL might lead WGET's parser to confuse the userinfo with the hostname which follows it in some way.



I located the dialogue with the guy who patched this flaw a few weeks ago.  He wrote:  "I just pushed a fix for the issue.  Indeed, the URL parser implementation of WGET 1.x is based on RFC 2396, a standard from 1998.  But even by this outdated standard," he wrote, "the implementation of the userinfo parsing was not correct.  It hopefully is correct now.  Anyway, nobody is going to lift the whole URL parsing of WGET 1.0 to newer standards.  But we have WGET2, and Fedora 40 recently switched to using WGET2 instead of WGET."  And he says:  "Of course there are corner cases that break backward compatibility.  Regards, Tim."



Okay.  So if you see anyone running around screaming about a CVSS of 10.0 in WGET while looking up to see whether the sky is falling, you can put their mind at ease.  All anyone ever had was a concern raised by seeing that semicolons were being mishandled.  No exploit, no worms, no remote code anything.  The CVE for this minor parsing flaw appears to have just been assigned and published this last Saturday, June 15th, so it's quite recent.  NIST's National Vulnerability Database lists the CVE, but doesn't yet have any CVSS assigned.  I just looked this morning.



As I was going over all this again just before the podcast, I did find a new reference at Red Hat which lists this with a CVSS of 5.4, which is far more sane.  So anyway, I just wanted to put everyone's mind at rest.  WGET, I'm still using I'm sure 1.something or other.  But again, not in any way that's insecure, and just to - typically just to grab podcast audio once a week.



Okay.  As a result of a lawsuit recently brought by Canal+, a French sports broadcaster, a French court has ordered three very popular, well known, public DNS providers - Google, we've heard of them; Cloudflare, we know them; and Cisco, oh, yes - to selectively edit their DNS domain name resolutions in order to block the lookup of around 117 individual pirate sports streaming domain names.  And you know, why not just sue copper wire for having the audacity to carry electrons at this point?  We've covered this sort of conduct before, and it's just as wrong now as it was then.



TorrentFreak posted an article last Thursday which explained how this battle has been slowly escalating for the past year.  They wrote:  "In 2023, Canal+ went to court in France to tackle pirate sports streaming sites including Footybite.co, Streamcheck.link, SportBay.sx, TVFutbol.info, and Catchystream.com.  Canal+ said that since subscribers of local ISPs were accessing the pirate sites using their Internet services..."



LEO:  How dare they.



STEVE:  I know, and how dare those wires carry those electrons with those sporting people.



LEO:  Shocking.



STEVE:  It's just, you know, we should - oh, yeah.  The ISPs, they said, should prevent them from doing so.  When the court agreed, all major French ISPs were required to implement technical measures to comply.  Since the ISPs have their own DNS resolvers for use by their own customers, these were configured to provide non-authentic responses to deny access to the sites in question, all 117 of them.  Naturally, in response to this blackout, savvy Internet users that had not already done so simply changed their settings to use different DNS providers.



LEO:  Yes, of course.



STEVE:  I mean, you can just imagine people texting each other, shoot, you know, Sportybites.com just went dark.



LEO:  Nuts.



STEVE:  Oh, just change your DNS.



LEO:  Fruitybits, yes.



STEVE:  Put it to 1.1.1, yeah, and off you go.  So they just changed them to different providers - Cloudflare, Google, and Cisco - whose resolvers had not yet been tampered with, at least at that time.  Use of third-party DNS providers to circumvent blocking is common.  So last year Canal+ took legal action against those three popular public DNS providers - Cloudflare at 1.1.1.1, Google at 8.8.8.8, and Cisco at 208.69.38.205. hike - in each case demanding measures similar to those which had already been implemented by French ISPs.  And once again the court agreed.



TorrentFreak writes that:  "Tampering with public DNS is a step too far for many Internet advocates.  But for major rightsholders, if the law can be shaped to allow it, that's what will happen.  In this case, Article L333-10 of the French Sports Code, which became active in" - I know.



LEO:  Well, there you go.



STEVE:  The French Sports Code.  We knew that they are good sports.  Anyway, which became active in January of 2022, seems capable of accommodating almost anything.  TorrentFreak says:  "It reads, when there are 'serious and repeated violations' by an 'online public communication service' whose main objective is the unauthorized broadcasting of sports competitions, rightsholders can demand 'all proportionate measures likely to prevent or put an end to this infringement, against any person likely to contribute to remedying it.'"  So that's about as broad as any language could be.



As a consequence:  "Two decisions were handed down by the Paris judicial court last month, one concerning Premier League matches and the other the Champions League.  The orders instruct Google, Cloudflare, and Cisco to implement measures similar to those in place at local ISPs.  To protect the rights of Canal+, the companies must prevent French Internet users from using their services to access around 117 pirate domains.  According to the French publication which broke the news, Google attorney Sebastien Proust crunched figures published by government's anti-piracy agency Arcom."  So using figures from their own government, Google crunched some numbers and concluded that the effect on piracy rates, if any, is likely to be minimal.



"Starting with a pool of all users who use alternative DNS for whatever reason, users of pirate sites - especially sites broadcasting the matches in question - were isolated from the rest.  Users of both VPNs and third-party DNS were further excluded from the group since DNS blocking is ineffective against VPNs.  Proust found that the number of users likely to be affected by DNS blocking at Google, Cloudflare, and Cisco amounts to a whopping 0.084% of the total population of French Internet users."



Then, citing a recent survey, which found that only 2% of those who face blocks simply give up, shrug, and don't bother to find other means, he reached an interesting conclusion:  "2% of 0.084% is 0.00168% of Internet users.  In absolute terms, that would represent a group of around 800 people across all of France," who would find that, oh, shoot, my pirate sports are no longer available, and I'm just giving up because it's just not worth typing anything into my keyboard to get it back.



They said:  "In common with other courts which have also been presented with the same arguments, the Paris court said the number of people using alternative DNS to access the sites, and the simplicity of switching DNS, are irrelevant."  We don't care.  "Canal+ owns the rights to the broadcasts.  And if it wishes to request a blocking injunction, a blocking injunction is what it shall receive.  The DNS providers' assertion that their services are not covered by the legislation was also waved aside by the court.  Google says it intends to comply with the order.  As part of the original matter brought in 2023, it was already required to de-index the domains from search results under the same law.



"At least in theory, this means that those who circumvented the original blocks by switching to these alternative DNS services" - oh, my - "will be confronted by blocks all over again.  But given that circumventing this set of blocks will be as straightforward as circumventing the originals, that raises the question of what measures Canal+ will demand next, and from whom."



So like I said, let's sue copper for having the audacity to indiscriminately carry anyone's electrons.  Just as we have in the European Union, regarding whether or not and exactly how and when communications can be encrypted, here again we have another instance of a collision between the power of the courts and the power of technology.  Technology desperately wants to be and to remain content agnostic.  The electrons just don't care.  But those who are in the position to impose their will upon the electrons only want them to carry the content they approve of.  Google has capitulated, and I presume that Cloudflare and Cisco will follow suit.



Before long, DNS is going to become an even greater mess than it already is.  And the most annoying part of this is that it's going to be a mess that doesn't actually solve any real problem since pirates will just switch over to some lesser known, well-off-the-map DNS provider that isn't on anyone's radar.  And we should remember that DNS is really only a convenience in the first place.  It's a pretty good bet that these pirate content hosting services are using a fixed IP.  So just placing an entry into a machine's local HOSTS file will permanently solve the DNS problem by preventing the system from even querying external DNS.  And we should also not forget that these piracy streaming sites are being hosted somewhere by someone.  THEY are the true culprits, and it's they who should be shut down, not honest and well-functioning free Internet services offering DNS resolution.  Wow.



LEO:  It's, yeah, go after the - but this is how they always do it.  And it's frustrating.



STEVE:  Right, because they can get to Google.  Google has people.



LEO:  Right.



STEVE:  You know?  Snootystream.qr doesn't.



LEO:  Piratebooty.net, no.



STEVE:  That's right.  Well, you know, we sent them a letter, and nobody replied.  The service didn't go dark.  So what can we do?



LEO:  Yeah, yeah, yeah.  It's kind of a...



STEVE:  Wow.  Okay.  During the run-up to today's podcast, I almost - this was yesterday, or, I mean, Sunday evening - finished with the code I wanted to have in place for automating the handling of undeliverable email.  So it's still the case that nothing - I just wanted to let everyone know who's been registering at GRC and subscribing to the Security Now! list, nothing has ever been sent.  So the reason you haven't received anything is that I haven't sent anything.  But that should start happening this week.



Among other minor announcements, I am now pre-checking the domain name portion of user entries by performing my own DNS lookup, before I try to submit an email address that might bounce.  Anyway, so the point is that typos in the domain name portion are being caught before the user leaves the page.  And GRC now supports plus signs in email addresses.  So anyone who wishes to filter incoming email by, for example, adding a +GRC or a +SN to the end of their mailbox name may now do so.  There's a simple "Delete" button on GRC's mail.htm page.  So if you're enjoying using, you know, if in general you enjoy using plus signs, you can easily delete your original non-plused email account and create a new one with a plus.



LEO:  Yeah.



STEVE:  So for anyone who wants.  And I have three quick pieces of useful feedback.  Tallis Blalack, he wrote:  "Long-time Security Now! listener and SpinRite owner.  What was the program you used to download your email into something you could easily search?  You mentioned it on an SN episode a few years ago.  My domain host originally offered free email.  The cost went to $4.80 per year when my email storage went over 2GB, and I was willing to pay that instead of making the time to reduce my email size.  Now they've moved to an email-as-a-service and have increased the cost to $48 per year.  It's time to back it up and clean it up as I move to a new hosting service.  Thanks for all you do."



So Tallis, this is my periodic opportunity to share one of my best discoveries ever.  It's the free and wonderful MailStore Home, which is MailStore's free personal Home edition.  As I said, it remains one of my most valuable discoveries which I'm happy to recommend without reservation.  Through all the years I've been using it, it has never let me down.  After installing it, you can aim it at your existing email archive or service, and it will suck it down, index it, and build an instantly keyword searchable, you know, substring searchable local database.



The way I have my own personal email system setup is that all incoming mail to me is also cloned into a separate "archive" email account, and everything I send is also automatically blind copied into the same archive account.  That way the archive receives a copy of everything coming in and going out.  Then in the very early morning, at 3:30 a.m., a Windows Task Scheduler task fires off, starts up MailStore to retrieve, index, and permanently add to its archive everything in my day's worth of email.  So it's always kept up to date within a day.



And astonishingly, all of this is 100% free.  And I should note that, while I've never looked into it, they also have enterprise solutions available for a refreshing one-time purchase, no subscription.  For $260 you get an enterprise-wide email archiving and retrieval solution that integrates with Microsoft 365, Google Workspace, and other email providers.  You know, and even whatever you are using yourself.  So again, MailStore is the company, just at MailStore.com.  They're a German outfit.  And the Home version is completely free.  And, you know, just hats off to them.  It's a beautiful solution.



Eric, a listener of ours who has some firsthand info about how IT goes over at the New York Times.  Remember last week we talked about them losing, was that 270GB of, like, basically they lost everything, all of their code, all of their website technology, everything.  So he says:  "Hello, Steve.  I want to share a comment regarding the New York Times, and a bit of history.  Around 2010 I was working for a company that provided endpoint security, and New York Times was our customer.  They were stuck on an old, unsupported version of our software.  Despite all the advances in behavioral-based machine learning and non-signature-based detection technologies, they insisted on running as 'AV only.'  We had countless emails and phone calls documenting our strong recommendation that they upgrade and apply the full security stack which they were entitled to."



LEO:  Aha.  This is 14 years ago.



STEVE:  Yup.



LEO:  Holy cow.



STEVE:  He said: "The recommendations were ignored, and they were successfully attacked.  They proceeded to publicly name us as their security provider and the technology that failed.  Of course we could not go public with a big 'We told you so,' and we were forced to take the black eye to protect the customer relationship.  So with whatever cybersecurity incident happened to the New York Times recently," he said, "I do not believe a word they say.  I have no doubt the story about the company's IT guy leaving the private GitHub access token exposed..."



LEO:  Yeah, blaming somebody, yeah.



STEVE:  "...is only a cover story for a far worse problem."  So anyway, Eric, thank you for sharing that.  This really makes me wonder how many of the problems we examine each week are effectively self-imposed.



LEO:  Right.



STEVE:  You know, we hear about a critical shortage of qualified IT professionals.  But I suppose there's no shortage of unqualified IT wannabes.  It would be interesting to know what the real reason was for them not wanting to improve their own security when an improvement was being offered and even pushed.  From what Eric described, it sounds like it wasn't money because they were already purchasing technology they refused to deploy.  Incredible.



Mark Newton said:  "Steve, I was searching through the show notes.  I was thinking you mentioned having a reMarkable 2 or something similar, and how much you liked it.  It appears there are a couple of different manufacturers out there.  I thought you specifically mentioned the reMarkable 2, or perhaps their  version?  You would not believe how often the word 'remarkable' applies in the notes from multiple Security Now! podcasts."  I guess I'm saying remarkable, remarkable, remarkable.  There's three.  Anyway, he said:  "You sparked my interest.  Pricey, but it looks like it would be helpful."



Okay.  So Mark, Leo turned me onto the reMarkable 2 stylus-based eInk tablet, and I never want to use anything else.  Okay, now, in fairness, I did once feel the same way about the Palm Pilot.



LEO:  How many do you still have in the fridge?



STEVE:  I don't know.  Extras of them went into the fridge for long-term storage and availability and safekeeping, and they've never come out.  So instead I have iPhones and iPads that didn't exist at the time.  So I can't say something better may not come along someday.  So I suppose the lesson is to never say never.



But what I can attest to is that, through the years since college, I kept trying over and over to use every spiffy new technology to replace my longstanding use of those wonderful light green quadrule engineering pads with the grid printed on the back side of the paper.  Those pads with a soft lead mechanical pencil were always the ultimate solution when I was brainstorming or working on a problem.  When I'm coding, I'll often make sure I don't make one of those common "off by one" errors by drawing simple diagrams of reduced complexity cases and then check it as I'm running the code in my head.



After 50 years with those green engineering pads, the reMarkable 2 tablet finally replaced them for all time.  I mean, it is absolutely my go-to tablet.  I love it.  So, yes, the reMarkable 2 is the device.



And Leo, after sharing our last sponsor relationship with our listeners, we're going to discuss, dare I say, "The Angle of the Dangle."



LEO:  You dare not.



STEVE:  Also known as the saga of BingoBango.com.



LEO:  Okay, well, we'll find out.  We'll learn more.



STEVE:  The dangerous dangling domains.



LEO:  The dangerous dangle of [crosstalk].  We're doing different software, doing a different studio, different cameras.  And I apologize because I don't have a mic on/off switch.  So everybody said, "Hey, Leo, you cracked a beer during that last segment."  It wasn't a beer, I promise.  This is zero-calorie sparkling water, okay?  Tropical fruit flavor, if you must know.  And by the way, before the next show, yes, we're going to have a mic switch.  Every once in a while Leo needs to slurp.



All right, Steve.  When I heard the name of this show, I said, I don't think Steve knows what that means.  But then I forgot, you're a boomer.  You know what it means.  You know who doesn't know what it means, all of those people watching us now, and there are almost 500 of them on YouTube, the same number on Twitch, on our Discord channel for the Club.  We've got a big audience today.  Thank you, all of you, for joining us.  I appreciate it.  And maybe it has something to do with the title.  I don't know.  What do you think?  Steve?



STEVE:  If so, we may be disappointing some of our viewers.



LEO:  I think so.



STEVE:  Or maybe not because, you know, sometimes you don't want any dangle.



LEO:  Yes.



STEVE:  Actually, as a general rule, especially when DNS is concerned, dangling is not good.



LEO:  No.



STEVE:  As I said, this week's Picture of the Week presents a step-by-step flow of how Domain Takeover happens.  Even though it's entertaining, even though I understood how this happens by the time a listener sent me that fun diagram, untangling the diagram was more work than following an explanation of how this happens.  So I want to thank everyone who wrote with their explanations.



Since I love my rack of router, firewall, switch, and server hardware at Level 3, as I noted at the start last week, I've never so much as glanced at the architecture of cloud-based hosting.  Just has never been on my radar.  Maybe in another 15 years, when I can no longer lift a fully loaded server into the rack, but not yet.



So the moment a listener mentioned that the trouble was almost certainly with a DNS CNAME record, the entire story became clear.  And since we've somehow never talked in detail about how DNS CNAME records are used, and how badly they can go wrong, I thought this would be the perfect opportunity.



All of this, of course, is in reference to last week's topic, the rise and fall of code.microsoft.com, and specifically to how it happened that a malicious actor managed to, however briefly, install their own presence into the "code" subdomain of microsoft.com.  The root of the problem is the potential for any organization's DNS records to point to something, anything, that they do not themselves control because, if they do not control the resource that their DNS records point to, the presumption would be that someone else may.  Which brings us to the question, "How could that possibly happen?"  Which many of our listeners will recognize as being the close cousin of "What could possibly go wrong?"



The first thing to appreciate is that this general problem has existed since the beginning of the Internet, when DNS was first created.  So the issue of anyone's DNS pointing to anything they don't control itself is not new.  And that's never been good.  But what is new is the way cloud-hosted resources have evolved; and how, as a result of this, the old problem of DNS misconfiguration has quickly become much greater.



Okay.  So let's take the fictitious site BingoBango.com.  The people running it decide that they're fed up with running their own website and other services.  It's time to move to the cloud.  And for the sake of this discussion, they've chosen Microsoft's Azure web hosting service.



When Microsoft created Azure, they obtained the domain azurewebsites.com to act as the root anchor for all of their subscribers' services.  So when someone wishes to have Azure host a site and their services, they register with Azure and choose a name for their service.  It might be a human readable and meaningful name, like BingoBango, or it might be just anything that's available like BB123.  Regardless of what they choose, their new site is reachable as a subdomain of the azurewebsites.com domain.  So it could be reached at Bingobango.azurewebsites.com or, in the case of not caring much about the site's subdomain name, BB123.azurewebsites.com.



But the BingoBango folks already own BingoBango.com, and no one wants to type "bingobango.azurewebsites.com" every time they want to visit the BingoBango website.  Microsoft was well aware of this, as was Amazon and all the other cloud providers.  They knew that the names of their services, and its subscribers' subdomains did not matter because DNS could be used to hide whatever they were.



We're all aware of the way good old standard DNS IP address lookup works; right?  A DNS typical IPv4 "A" address record or IPv6 "AAAA" address record is used to turn a DNS name into one or more IP addresses.  When a user's client wishes to connect, it uses DNS to look up the domain's IP address and then connects to one of the IP addresses returned.  So it's a one-step process.  But the success of this one-step process requires that the server's IP address does not change.



If Microsoft could guarantee a fixed IP address for their subscriber's website bingobango.azurewebsites.com, then standard DNS A-record lookup could be used.  Just like the BingoBango people once pointed their BingoBango.com IP address at their own server, they could instead point their BingoBango.com IP at the IP address provided by Microsoft.  Then visitors to BingoBango.com would simply go to the server at the IP provided by Microsoft.



But DNS provides a better and much more flexible way to achieve the same goal which allows cloud providers total freedom in the way they set up at their end.  For example, it might be useful to allow different regions of the world to be accessing cloud resources at different IP addresses so that offering a single fixed IP to everyone would be less convenient.  So this is made possible by a different type of DNS record known as a CNAME.  The "C" of CNAME stands for canonical.



Whereas a DNS "A" address record returns one or more IP addresses, a CNAME record returns another domain name.  In programming we would refer to this as introducing a level of indirection because, instead of a domain name pointing directly to an address record, it points to another domain name which then probably points to an address record.  And this domain name indirection using DNS CNAME records is the way cloud hosting providers are typically accessed.



Using our example, once the BingoBango people have established their services at bingobango.azurewebsites.com, they replace their DNS's BingoBango.com "A" record with a CNAME record pointing to bingobango.azurewebsites.com.  Now, anytime someone wishes to visit the BingoBango.com website, their web browser will look up BingoBango.com and will find a CNAME record there.  That CNAME record tells the browser that the name they were looking up, BingoBango.com, is actually a "canonical name" for the actual hostname, and that CNAME record provides the actual host name, which is bingobango.azurewebsites.com.  So THAT is the domain name that it should pursue in its quest to find an IP address for its connection.  The user's DNS resolver then looks up the IP for the domain bingobango.azurewebsites.com, obtains that IP from the azurewebsites.com DNS servers, and returns that to the user for their connection to the BingoBango.com website.



What's significant here is that two DNS resolutions were required, and that the second lookup was for a property located under the azurewebsites.com domain.  This allows Microsoft to host their subdomains at whatever IP addresses they choose and affords them the flexibility to change this at any time they wish.  The BingoBango.com DNS simply refers anyone who asks to the bingobango.azurewebsites.com DNS for its IP.



This CNAME resource creates a very powerful system of DNS pointers; and, as we know, with great power comes great responsibility.  So it should come as no surprise that things have not always worked out well.  The problem occurs, as happened to Microsoft with their code.microsoft.com subdomain, when an organization deletes the hosted services and domain being pointed to by some other canonical DNS name.  At that point the canonical name is said to be left "dangling" because it's pointing to a nonexistent host name.  The bad news is that this opens the way for someone else, who would almost always have to be malicious, to re-register their own cloud service under the same domain name as what was previously deleted.



In the case of the code.microsoft.com subdomain, the subdomain would have had a DNS CNAME record.  When that record, code.microsoft.com, was retrieved, it would have provided the name of the host that had been deleted.  So the bad guys would simply register their own host under that CNAME record's name, and anyone then accessing code.microsoft.com would be referred to their probably malicious services.  And note that their servers would appear, and this is important, the malicious servers would appear to any web browsers to be within microsoft.com.  This creates additional potential for various browser domain origin trickery.



Three and a half years ago, on November 25th of 2020, "The Daily Swig" cybersecurity news and views newsletter at PortSwigger.net posted a piece under the title "Rampant CNAME misconfiguration leaves thousands" - they could have said hundreds of thousands - "of organizations open to subdomain takeover attacks."



They wrote, and this is three and a half years ago:  "Security researchers have discovered more than 400,000 subdomains with misconfigured CNAME records, leaving many at risk of malicious takeover as a result.  When websites are externally hosted, the CNAME (Canonical Name) record used to map their canonical domain and subdomains to the third-party host domain, this means that the canonical, rather than the host domain, appears in the browser's address bar.



"Pinaki Mondal, of the security firm RedHunt Labs based in India, wrote in a blog post that when a cloud-hosted website is deleted, but the DNS entry pointing to the resource is retained, attackers can potentially re-register the host, add the organization's subdomain as an alias, and thus control what content is hosted under the original canonical name.  Attackers can then serve malicious content to visitors and potentially intercept internal emails, mount PDF-based click-jacking attacks, hijack users' cookies and sessions by abusing OAuth whitelisting, and abuse cross-origin resource sharing to harvest sensitive information from authenticated users."  In other words, you really don't want bad guys to appear to be in your own primary domain.



"Using a tool," they write, "that conducts mass DNS resolution, RedHunt Labs found more than 424,000 subdomains with misconfigured CNAME records during an automated trawl of 220 million hosts.  The number of sites that were abandoned, for example, if they belonged to defunct organizations, was unclear due to the additional need to look up company registries to obtain that information.  But by adding HTTP response grabbing, the researchers uncovered evidence that 139 of Alexa's top 1,000 domains may have fallen prey to subdomain takeovers."  139 out of the top 1,000.



"RedHunt Labs identified 33 third-party services that allowed for potential subdomain takeovers.  With nearly 63% of vulnerable DNS records pointing to Shopify, most vulnerable domains belonged to ecommerce operators.  Landing page creator Unbounce accounted for the second highest number of vulnerable domains at 14%, followed by Heroku at 10%, GitHub Pages at 4%, and BigCartel at 2%.  Drilling into the data, RedHunt said 'www' was" - wow, that actually makes sense; right - "was the most frequently vulnerable subdomain."  So think about that.



LEO:  Wait a minute.  Wait a minute.  You can have a www - oh, .bingobango.com.  Okay.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  So BingoBango.com might still have their email being handled by their own BingoBango.com, but www...



LEO:  .BingoBango...



STEVE:  You know I used that name just because it was so fun to say it.



LEO:  Well, you saw that our chatroom found somewhere you can register it for a mere $8,000.  So go for it.



STEVE:  Oh, that's interesting.  Because when I looked last night it was at $15,000.



LEO:  Yes, that's what Galia said.  So maybe he got a deal, he found a deal.



STEVE:  Nice.



LEO:  Half off, Steve.



STEVE:  Now, I have to tell you that my first thought was to go with BingoTips.com because I thought, okay, how can you have Bingo tips?  But there is a BingoTips.com.



LEO:  Of course there is.  Dude, you're quoting security research from a company called PortSwigger.



STEVE:  Yeah, okay.



LEO:  Whose number one [off mic].  I'm sorry, I pushed the wrong button.  Whose number one product is Burp Suite.  Although I've got to say there's a certain consistency to their naming.



STEVE:  Right, they're staying faithful to the name.



LEO:  They're actually legit, but I just love it.



STEVE:  Oh, I know they are.  That's why I quoted them is they're a real group.



LEO:  Yeah.  It's hysterical.  Oops, I've got to zoom you.  Here we go.  Nope, not that.  No, no.  Whoops, not you.  No, no.  Here.  Wait a minute.  There you go.



STEVE:  Command Nancy, Command Nancy.



LEO:  Command Nancy.  Okay.



STEVE:  Okay.  So it totally makes sense that www would be the most frequently vulnerable subdomain because somebody who doesn't want to host their own website would send www off to some third party.  So, yeah.  Wow.



This guy Pinaki Mondal at RedHunt pointed out that by suppressing and removing the "www" - remember when Google did this? - and the "m." subdomains from the Chrome browser's address bar, that was at Chrome 69 onwards, he said:  "Google had inadvertently made it more difficult for users to determine whether they 'might be browsing attacker-controlled content.'



"RedHunt found around 200 non-functional .gov site subdomains with misconfigured CNAME records, and one had a 'wildcard' CNAME record, which poses a particularly dangerous security risk.  And Mondal noted that prestigious universities owned some of the roughly 1,000 misconfigured .edu subdomains."  And anyway, I guess my point here is Microsoft certainly is not alone in having tripped over this problem.  



"The findings show that despite the potentially calamitous impact of subdomain takeovers, many well-resourced large organizations are struggling to comprehensively discover and track their own ever-expanding infrastructure."  You can just imagine what the DNS must look like of some of these organizations.  Yikes.  "Mondal also noted that Roblox, Starbucks, and the U.S. Department of Defense are among the organizations to have remediated subdomain takeover flaws through HackerOne in the past year."  That was two and a half or three and a half years ago.



The article ended with The Daily Swig noting that the year before they posted this, in 2019, it had previously reported on subdomain takeover flaws stemming from Windows 8 Live Tiles feature, and the year before that a misconfigured Microsoft subdomain, back in 2018.  So not even Microsoft's first subdomain problem.  And four years before this article, which is from November of 2020, and before RedHunt's research, researchers from the University of Delaware and the College of William and Mary published their research titled - and this is back in 2016 - "All Your DNS Records Point to Us - Understanding the Security Threats of Dangling DNS Records."  So my point being, not a new problem.  Very powerful, but very easy to get wrong.  



So our takeaway for today's podcast is that Microsoft definitely made a serious mistake when they left their "code" subdomain dangling, but they are far from the only organization to have ever done so.  In today's increasingly complex IT landscape of overlapping services, where such services are increasingly being outsourced, DNS can become quite complex and convoluted.  And we once talked about how one of the tricks being used to track website visitors is to point a CNAME record of a website to an advertiser's domain as a means of giving them first-party cookie-tracking status.  That struck us as being ultra slimy at the time, but it's the times we're living through.



So just a heads-up reminder to any of our listeners who have responsibility for the management of DNS within their organization.  You likely already know that DNS is not as simple and straightforward as it once was.  It might be worth making sure that you know who any of your organization's CNAME records are pointing to, and that they have a good, ongoing, and legitimate reason for being granted such a potentially powerful position since they're sharing your organization's root domain.



LEO:  Wow.  Absolutely a chilling thought.  Although if you've figured out how to get BIND working, you probably know enough to keep yourself from getting CNAME poisoned.



STEVE:  Well, I don't know.  You know what happens is it's one little thing after another.  You add this.  You add that.  It gets increasingly complex.  And then, like, someone doesn't renew a contract; right?



LEO:  Yeah, right.



STEVE:  And so they disappear, but it doesn't automatically delete that record from DNS.  DNS is still pointing to them, even though they're no longer offering the service.



LEO:  It's amazing.  What a world we live in.  Steve, you're the best.  Thank you for putting up with this crazy situation here.  We're trying out new software, trading in the TriCaster for a software-based solution called Restream.  Which unfortunately allows me to put chat comments into your feed.



STEVE:  I just decided I'm not going to try to read them while it's going on.



LEO:  Don't look at them.



STEVE:  I can't do that.



LEO:  And I'm having a little fun with it, but so are the chatters.  It also gives us the opportunity to stream this video into multiple places once again, which is great.  We've got Twitch watching.  There's 445 people on YouTube, 138 people on Twitch.  We can expand it to beyond that, to X and to Insta.  You know, I think this is a really nice way to be able to share what is such an important show with a broader audience.  So thank you.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#980

DATE:		June 25, 2024

TITLE:		The Mixed Blessing of a Crappy PRNG

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-980.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How long did it take for Windows' recent horrific WiFi flaw to be weaponized?  What are the implications of the U.S. Commerce Department's total ban on Kaspersky?  How is the Kremlin reacting?  Why would an EU privacy watchdog file a complaint against Google for their Privacy Sandbox?  When is an email tracking bug not a tracking bug?  What can this podcast do to help a well-known security researcher present his work at DEFCON and Black Hat this summer?  What's another near certainty for Microsoft's plan for Recall?  What two mistakes have I been making on this podcast?  And why might a really bad password generator wind up being a good thing?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  The Commerce Department in the United States bans the Kaspersky Antivirus.  Steve talks about why and whether it's a legitimate problem.  We hear from a security researcher who is having trouble getting into the United States.  Maybe you could help.  And then we'll find out why every once in a while it's a good idea to have a bad password generator.  All that and more coming up next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 980, recorded Tuesday, June 25th, 2024:  The Mixed Blessing of Lousy PRNG.



It's time for Security Now!, yeah, the show you wait all week for.  Let's just make that your tagline.  We wait all week for Tuesday.  If it's Tuesday, it must be Steven Gibson day.  Hello, Steve Gibson.



STEVE GIBSON:  My friend, it's great to be back with you as we close out the first half of the year.  And I have to say it was a little poignant, well, not poignant, it was very clear to me that we were closing in on Episode 1000, crossing the infamous 999, when I wrote 980.  		



LEO:  Wow.



STEVE:  Because we're getting there.



LEO:  Holy moly.



STEVE:  Yes.  Okay.  So I just have to say, and I assume you haven't seen it yet because you haven't fallen off your ball, that we have one of my favorite Pictures of the Week in a long time.  So that's coming up.  But I think a great episode, one with some interesting lessons:  The Mixed Blessing of a Lousy PRNG.  And I realized when I was using PRNG that some of our listeners might, what, a pring?  Of course we know that's a pseudorandom number generator.



LEO:  Yes.  Do you think Pringles were named after pseudorandom - no, probably not.



STEVE:  Leo, that's it, exactly.  This is the audience they were targeted at because we all sit around eating Pringles.



LEO:  Yes.



STEVE:  But we're going to answer some questions, as we always do, before we get to our main topic, which is how long did it take for Windows' recent horrific WiFi flaw to become weaponized?  Oh, and, oh boy, is there a new twist on this WiFi flaw, too.  What are the implications of the U.S. Commerce Department's total ban on Kaspersky, which will be coming into effect in a few months?



LEO:  Can you believe that?  Wow.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  How is the Kremlin reacting to that?  And who cares, but still.  Why would an EU privacy watchdog file a complaint against Google over their Privacy Sandbox?  Which is all about privacy, as the name suggests.  When is an email tracking bug not a tracking bug?  What can this podcast do to help a well-known security researcher present his work at DEFCON and Black Hat this summer?  What's another near certainty for Microsoft's actual plan for Recall?  This is something else that occurred to me that I think everyone's going to go, oh, of course, like the first time I had that thought a couple weeks ago.  And what two mistakes - maybe not only two, but at least these two - have I been making on this podcast?  And finally, why might a really bad password generator wind up being a good thing?  A mixed blessing, as it were.



LEO:  Yeah.  I'm trying to think of why it could ever be a good thing.



STEVE:  And more importantly, what lessons do we learn about cryptography overall from that?  



LEO:  Yeah.



STEVE:  So I think, Leo, we may actually have a good podcast finally.



LEO:  Well, after 980 attempts, I think it's good.



STEVE:  I think we got the hang of it.



LEO:  We've kind of got it.



STEVE:  And this might be something where people come away thinking, you know, that was okay.



LEO:  They're doing all right, these kids.



STEVE:  Yeah.



LEO:  Well, let me tell you, before we go much further, about our first sponsor of the show.  And then we can get into the...



STEVE:  I do have a penguin in my face, I should note.



LEO:  Oh, I'm sorry.  That's rude.  How rude.  I put a Linux penguin in your face.  That's - sorry, you shouldn't go there.  You go right here.  Let me put him kind of off to the side.  But just a little reminder that, if you're using Windows, you don't have to.  Oh, by the way...



STEVE:  What we should all actually be using, that's right.



LEO:  I found out what that was.



STEVE:  The thought that I have about Windows may cement that further.



LEO:  Oh, boy.  I found out what that was.  Remember before the show Windows kept saying restart, restart, restart.



STEVE:  Yeah, yeah.



LEO:  And then I was getting a UAC from 8bit Solutions.  Turns out that's the Azure provider that Bitwarden uses.  So that was Bitwarden asking me to update itself, basically.



STEVE:  Oh, interesting.



LEO:  Which is, but, you know, I have to say, from a security standpoint, you don't want to see another name show up when you're reinstalling something.  That means you have to go out and look it up and figure out why it wants to do that.  So...



STEVE:  Well, and Leo, why is it only eight bits?  I would think, what, is this from the '80s?  8bit Solutions?  What?



LEO:  I don't know.  You wouldn't see it on any other platform.



STEVE:  Runs on a 6502 or something.



LEO:  That's very odd, yeah.  It's a good point.  I don't know.  Well, anyway, now I'm going to install it because I, you know, found out.



STEVE:  128bit Solutions, and I'd be inclined to trust it.



LEO:  More bits is better.  Is it always better?



STEVE:  That's right, baby.  If we learned anything in crypto, it's the more bits you've got, the better.



LEO:  That's, what did you call that, padding of some kind; right?  I can't remember, you had a good name for it.  Password, Perfect Paper Password, Haystacks or something; right?  Was it Haystacks?



STEVE:  Well, the Haystacks is an interesting idea, but that's different.



LEO:  That's a different one, okay.  All right.  I am prepared to show you the Picture of the Week, soon as you say so.



STEVE:  And it is a fave.



LEO:  I haven't looked at it yet.



STEVE:  It's just quick, visual, simple fun.



LEO:  Okay.  Do you want me to show it and then you describe it?  Is that how you want to...



STEVE:  Okay, yes, that would be good.



LEO:  All right.  So I have to figure out how to show it first.



STEVE:  There it is.



LEO:  You know, it's funny, my thing is moving around all the time, and I don't - sometimes my - now it's this laptop, so it's just going to take me a second.  Why don't you set it up anyway?



STEVE:  Okay.  So anyway, the picture.  And I should note, Leo, that 4,330 subscribers to the Security Now! mailing list received this three hours ago.



LEO:  Oh.  So they're already up on it.  They know all about it.



STEVE:  So email is live, yeah, and the email contained a thumbnail which a bunch of them clicked in order to see the full-size image.  So anyway, I gave this the title "Correlation Is Not Causation" because...



LEO:  That's a very important concept that people need to understand.  Yeah, yeah.



STEVE:  It absolutely is.  Yes.  And we have a cute little, I'm not sure what he is, kind of a dog, but a small dog that's been leashed to a - isn't it wonderful?



LEO:  He wants to get away badly.



STEVE:  He's looking at his master saying, hey, what about me?  Why am I stuck here?  So he's tied up to this, what would you call that?



LEO:  A bollard.



STEVE:  A bollard, yes.  However, something in the past has whacked this bollard off to the right so that it's a leaning bollard.  And if you didn't know better, you'd think that this was Mighty Dog, and that in trying to join his owner he had tugged at this thing and yanked it almost out of the pavement.  Anyway, I would commend our listeners to go find today's Picture of the Week because it is - it's a goodie.  A lot of fun.  And thank you to whomever it was who sent it to me.



Okay.  So last week we opened with the news that the previous week's monthly Windows patch fest had quietly closed a remarkably worrisome flaw that had apparently been sitting undiscovered in every Windows native WiFi network stack since the last time Microsoft poked at it, and there's been no definitive statement about this because it appears that even Microsoft is quite freaked out by this one.



A listener of ours, Stephen CW, sent a relevant question:  He said:  "Hi, Steve.  Longtime listener.  Our corporate IT group vets Windows patches, thus delaying them.  In the meantime, does turning off the WiFi adaptor prevent the attack you described?"



Okay, now, given the havoc that past mistakes in Windows updates have caused for corporate IT, especially remember a couple years ago when Microsoft kept wiping out all printing capability enterprise-wide, about like once a month they would do that?  I suspect many organizations may have adopted a wait-and-test to avoid subjecting their users to such mistakes.  And it's typically the case that even though 50 to more than 100 flaws may be fixed in any given month, nothing is really happening that's highly time-sensitive.  But that's not the case with this month's revelations.



What I saw and mentioned last week at GitHub did not make any sense to me since it appeared to be too high-level.  Remember, I mentioned in passing that there was already an exploit on GitHub.  Well, since then, someone else appears to have found a way to overflow the oversized 512-byte buffer which Windows' WiFi driver provides for SSIDs.  But that's not this problem.



He wrote, thinking that this was the critical 30078 CVE.  He initially wrote:  "CVE-2024-30078 describes a vulnerability in the way Windows handles SSIDs (Service Set Identifiers) in WiFi networks.  Windows has a buffer for SSIDs up to 512 bytes long, which exceeds the WiFi standard.  By sending chunked frames to increase the SSID size beyond 512 bytes, a buffer overflow can be triggered."  He says:  "This exploit leverages this vulnerability to cause a buffer overflow by creating and sending WiFi beacon frames with oversized SSID fields."



Okay, so that's a problem.  But then he realized that he had found a different flaw from what Microsoft patched.  So in an update he subsequently added, he said:  "Info:  This REPO does not seem to be hitting the same bug as in the stated CVE.  New information has come to my attention thanks to FarmPoet. The CVE-2024-30078 vulnerability is in the function 'Dot11Translate80211ToEthernetNdisPacket().'"  And I should say that absolutely, based on that function name, that makes total sense.  And he said:  "Of the native WiFi Windows driver (nwifi.sys), where a very specific frame needs to be constructed to get to the vulnerable code path," which, as he said, his code, his current code, does not.  So he said:  "I'm working on it.  I've identified the changes in the patched function and am now working on reversing to construct the relevant frame required to gain code flow into this segment."



Okay.  So we have this guy publicly working on a public exploit for this very worrisome flaw.  And we're about to see why turns out this a lot worse than it first seemed.  Meanwhile, it may be that anyone who has a spare $5,000 may be able to purchase a working exploit without waiting for a freebie on GitHub.



The online publication DailyDarkWeb - believe it or not there is such a thing - writes:  "A threat actor has announced the sale of an exploit for CVE-2024-30078, a Remote Code Execution vulnerability in the WiFi driver affecting all Windows Vista and later devices.  In their announcement, the threat actor details that the exploit allows for remote code execution over WiFi" - get a load of this, though - "leveraging compromised access points or saved WiFi networks."  I'll get more to that in a second.  "The exploit reportedly works by infecting a victim through WiFi router-based malware or simply by having the victim's device be within range of a WiFi network they've previously connected to.



"The exploit code is offered for sale at $5,000 U.S., with the price being negotiable.  The seller also offers to develop custom solutions tailored to the buyer's needs.  Anastasia, the new owner of the forum, is listed as the escrow for this transaction.  Interested parties are instructed to send a private message to the threat actor, with a warning that time wasters and scammers will be ignored."



Now, in the first place, while we have no way to confirm this, from what we've seen before, it's entirely believable that several weeks downstream from the release of a patch which will have altered the binary of some WiFi component of Windows, that by "diffing" - in hacker parlance - the pre- and post-patched files, the change that Microsoft made to repair the original driver's defect can be readily found.  This is what the guy on GitHub is already doing.



But the really interesting attack vector that had not occurred to me when we first talked about this last week, but obviously has occurred to the author of this $5,000 for sale exploit, is the idea of infecting vulnerable consumer routers or corporate wireless access points, which might well be half the world's circumference away.  In other words, if a vulnerable WiFi router is available anywhere in the world, it could be infected with knowledge of this critical Windows flaw, so that any unpatched Windows WiFi laptop within range of that router could be compromised.  And that would be a very remote attack.  It's clear that the only reason Microsoft was able to get away with labeling this flaw as only being "important," with a CVSS of 8.8, instead of "critical," with a CVSS of 9.8, or maybe even 10, is that it required a nearby attacker.  At least that was the theory.  But in reality, all it requires is a nearby hostile radio.  And thanks to the historical vulnerability of consumer and enterprise routers, that's not a high bar.



The observation here is that a maliciously infected router may not be able to attack the machines connected to it by wire because there are no known exploitable vulnerabilities in their wired Ethernet network stacks.  But that same router may now be able to successfully attack those same or other machines within its wireless reach thanks to the known presence of a, by Microsoft's own assessment, readily exploitable, low-complexity, highly reliable, likely-to-succeed flaw that exists in any Windows machine since Vista which has not yet received the patch that first appeared only two weeks ago.



So to answer Stephen CW's question about whether turning off, you know, disabling the WiFi on a machine will protect it, the answer has to be yes.  Everything we know - although I have to say I looked around, and as I said, Microsoft is oddly mute on this whole thing.  Normally you would expect them to say, mitigation, you know, disable WiFi.  But maybe they presume so many people are using WiFi that you can't really call it a mitigation if taking the machine off the network is what it takes to mitigate the problem.  So they're not suggesting that.  But yes, everything we know informs us that turning off Windows WiFi adapters will completely protect any unpatched machine from the exploitation of this vulnerability.



LEO:  Yeah.  But you could also remove the machine from the Internet entirely, air gap it, and that would be good, too.  I mean...



STEVE:  Or, Leo, something just hit me.  Turn it off.



LEO:  Yeah.  That'll fix it, too.



STEVE:  What a concept.  That's right.  Pull the plug, shut it down, you're safe.  Anyway, I wanted to conclude this week's follow-up on this CVE by making sure everyone understands that the addition of this remote router extension to this vulnerability really does change the game for it.  We know tens of thousands of routers have already been and are taken over, and are being used for a multitude of nefarious purposes - launching DDoS attacks, forwarding spam email, as proxies to probe the Internet for other known weaknesses, and on and on.  So the bad guys are going to realize that, by updating the malware that's already within their compromised router fleets, they'll be able to start attacking and hijacking any Windows machines that have not yet been updated that have their wireless turned on.



And for whatever reason, history tells us that there will be many such machines.  Updating seems to be a slow process.  And, for example, Stephen CW acknowledged that his corporate IT people, they're waiting now because there's been too much history of updates destroying corporate IT functioning.  So they're taking a cautious process.  Anyway, it's going to be interesting to see whether bad guys - how long it takes bad guys to leverage the idea of pushing this flaw out to the routers and seeing if they can remotely grab wireless machines.



I'll share this piece of news, this next piece, and interject some of my thoughts along the way.  And Leo, I know you reacted a little bit as I did.  And I'm of two minds.  So it creates for some interesting dialogue.



Last Thursday, Kim Zetter, writing for Zero-Day, posted the news:  "The U.S. government" - which did it on the same day.  "The U.S. government has expanded its ban on Kaspersky software in a new move aimed at getting consumers and critical infrastructure to stop using the Russian company's software products, citing, of course, national security concerns.  The ban, using new powers granted to the U.S. Commerce Department, would prohibit the sale of Kaspersky software anywhere in the U.S., and would also prevent the company from distributing software security updates or malware signature updates, to customers in the U.S."  In other words, they're being cut off.



"Signatures," they explain, or Kim explains, "are the part of the antivirus software that detect malicious threats.  Antivirus vendors push new signatures to customer machines, often on a daily basis, to keep their customers protected from new malware and threats as the vendors discover them.  Without the ability to update the signatures of customers in the U.S., the ability of Kaspersky software to detect threats on those systems will significantly degrade over time.



"The U.S. Commerce Department announced the ban on Thursday after what it said was an 'extremely thorough investigation,' but did not elaborate on the nature of the investigation or what it may have uncovered," if anything.



"U.S. Secretary of Commerce Gina Raimondo told reporters in a phone call:  'Given the Russian government's continued offensive cyber capabilities and capacities to influence Kaspersky's operations, we have to take the significant measure of a full prohibition if we're going to protect Americans and their personal data.  Russia,' she said, 'has shown it has the capacity, and even more than that, the intent to exploit Russian companies like Kaspersky to collect and weaponize the personal information of Americans, and that's why we're compelled to take the action we're taking today.'"



Wow.  Okay.  So in other words, we don't like their zip code, so we're going to deny a company, against whom we have no actionable evidence of wrongdoing, all access to the American market because, being a Russian company, they could be forced to act against us.  And as I said, I'd say that I'm evenly divided on this.  Through the years we've covered countless instances where Kaspersky has been hugely beneficial to Western software and to Internet security globally.  Thanks to their work for the past many years, the world is a safer place than it would otherwise be.  So to say "We don't like where you live so we cannot trust you" is a bit brutal. But at the same time, it is also understandable because, being in Russia, it's possible that their actions may not always reflect their values.  And it's not as if operating within a state where we democratically elect our representatives is all that much different; right?  After all, in the U.S. we have "warrant canaries."



Remember that Wikipedia explains a warrant canary by writing:  "A warrant canary is a method by which a communications service provider aims to implicitly inform its users that the provider has been served with a government subpoena, despite legal prohibitions on revealing the existence of the subpoena.  The warrant canary typically informs users that there has not been a court-issued subpoena as of a particular date.  If the canary is not updated for the period specified by the host, or if the warning is removed, users might assume the host has been served with such a subpoena.  The intention is for a provider to passively warn users of the existence of a subpoena, albeit violating the spirit of a court order not to do so, while not violating the letter of the order."



So again, you know, state entities do.  And in other words, in the U.S. our courts are able to say:  "We demand that you turn over information within a certain scope; and, by the way, you're legally forbidden from disclosing that we've asked and that you have complied."  So it's not my intent to pass moral judgment here.  I'm just saying that what we see is unfortunately, you know, all nation states will act to protect their interests, and that their client citizens have little choice other than to comply.



So Kim's piece continues:  "Asked what evidence the government found to support concerns that the Russian government is using Kaspersky software to spy on customers, Raimondo and other government officials on the call declined to provide specifics.  One senior Commerce official said on background:  'In terms of specific instances of the Russian government using Kaspersky software to spy, we generally know that the Russian government uses whatever resources are available to perpetrate various malicious cyber activities.  We do not name any particular actions in this final determination, but we certainly believe that it's more than just a theoretical threat that we describe.'"  And that's right because these days, as we know, "belief" is all that's needed.



Kim writes:  "The ban will not go into effect until September 29th to give existing Kaspersky customers in the U.S. time to find a replacement for their antivirus software.  The ban on new sales of Kaspersky software in the U.S., however, goes into effect on July 20th.  Sellers and resellers who violate the ban could be subject to fines from the Commerce Department and potentially criminal action.  In addition to the ban, the Commerce Department also put three Kaspersky entities on its trade-restrictions entities list, which would prohibit U.S.-based suppliers from selling to Kaspersky, though it's unclear if Kaspersky currently has U.S. suppliers.



"A Kaspersky spokesman, in a statement to Zero Day, accused the Commerce Department of making its decision 'based on the present geopolitical climate and theoretical concerns..."



LEO:  Well, yeah.



STEVE:  "...rather than on a comprehensive evaluation" - right.



LEO:  I mean, I feel bad for Eugene Kaspersky.  Everybody loves him.



STEVE:  Yes.



LEO:  But why not?  I mean, we don't have to have it; right?



STEVE:  Right.



LEO:  And Huawei phones.  I mean...



STEVE:  Right.  I mean, this is what we're beginning to see happen; right?



LEO:  Right.



STEVE:  As we choose sides, and we pull the bridges, the drawbridges up of global commerce that used to interconnect everyone.  So anyway, they apparently think they have some legal standing.  This spokesperson said:  "We will continue to defend ourselves against actions that seek to unfairly harm our reputation and commercial interests."  So, okay.



Now, as a little bit of background, the Department of Homeland Security had previously issued a directive in 2017 banning federal government agencies and departments - not consumers, and like everybody now, which is what is about to happen, so this was in 2017 - just federal government agencies and departments from installing Kaspersky software on their systems.  DHS had also not cited any specific justification for its ban at the time, but media reports citing anonymous government officials back then cited two incidents, and we talked about them on the podcast.  According to one story, an NSA contractor developing offensive hacking tools for the spy agency had Kaspersky software installed on his home computer.



LEO:  Yeah, we reported this story.  Remember this?



STEVE:  Yes, yes.



LEO:  This was those NSA tools.



STEVE:  Right.  He was developing these NSA tools, and the Kaspersky software detected the source code as malicious and extracted it from the computer, as AV software often does.



LEO:  It quarantined it, yeah.



STEVE:  Well, it actually sent it to Kaspersky.



LEO:  But that's what all antiviruses do.  They quarantine it and send it in.



STEVE:  Exactly.



LEO:  To the home office, which in this case was in Moscow.  That was the EternalBlue leak.



STEVE:  Yes.  So a second story claimed that Israeli spies caught Russian government hackers using Kaspersky software to search customer systems for files containing U.S. secrets.  So, okay, you could install Kaspersky as you can many other tools.  Mark Russinovich's PsExec is a favorite tool for bad guys to use, but its intention is benign.  So Kaspersky for their part denied that anyone used its software to explicitly search for secret information on customer machines and said that the tools detected on the NSA worker's machine were detected in the same way that all AV software is designed to detect malware on customer machines.



LEO:  Because it was malware.  It really was malware.



STEVE:  Right, right, exactly.  They were developing NSA malware for the NSA.  And, you know, it's funny, too, because it's a little reminiscent of the Plex breach, which of course is the way LastPass got themselves in trouble.  You have some, you know, some third-party contractor using your tools at home on their home machine, where they've got AV software installed.



LEO:  Right.



STEVE:  It's like, whoops.



LEO:  Whoops.



STEVE:  Not quite secure.  So anyway, following that 2017 DHS directive, Best Buy and other commercial software sellers that had contracts with Kaspersky to sell computers with Kaspersky AV software pre-installed on those systems subsequently announced they would no longer install the software on computers they sold.  This didn't, however, put an end to existing customers using Kaspersky software, or prevent new customers from purchasing the software on their own.



Today's ban is designed to convince those customers to stop using the software, as well.  And get this.  Commerce Secretary Raimondo told reporters:  "When Americans have software from companies owned or controlled by countries of concern - such as Russia and China - integrated into their systems, it makes all Americans vulnerable.  Those countries can use their authority over those companies to abuse that software to access and potentially exploit sensitive U.S. technology and data."



And I'll just note that the United States is no different in that regard.  It's just that we're here, and they're there.  We've covered the news that China's government is similarly urging its businesses to stop using Windows.  We clearly have a new cyber cold war heating up; and unfortunately, choosing sides and cutting ties is part of the process.



So anyway, it's unfortunate, Leo, that a product that many people use is not going to be available.  At the same time, I guess it feels to me like Kaspersky's employees should have seen the writing on the wall.  They've seen the tensions between the U.S. and Russia heating up.  It's easy for us to say, well, you know, they could have left Russia.  But they probably love Russia as much as we love the U.S.  So, you know, for most of them it's just a job.



LEO:  And, I mean, Eugene Kaspersky was trained at the KGB technical school and did have a job in the Ministry of Defense when he founded Kaspersky Antivirus.  So there are deep connections to the Russian government and the GRU.



STEVE:  And I would note also that AV software in particular has a very intimate relationship with an operating system.  It is in the kernel.  It is absolutely true, and this is the kind of thing that keeps the military mind up all night, you know, here we have a Russian company that has an active connection to all of the customers' machines in the U.S.  And, you know, it's not a text editor.  It's running a driver in the kernel.



LEO:  This ain't TikTok.  This is something else



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  If it did want to get nasty, in an instant it could take over all of the machines where it's installed.



LEO:  And that's why it's been banned on government machines for a long time.



STEVE:  Since 2017, yes.



LEO:  Yeah.  So, I mean, yeah.  It makes sense.  I mean, I feel bad for Eugene.  And part of the reason people are upset is everybody loves Eugene Kaspersky.  Dvorak used to recommend Kaspersky all the time.  He loved it.  But mostly because he used to hang with Eugene and drink vodka during Comdex.  So, but honestly, there are plenty antiviruses out there.  One could argue you don't even really need an antivirus.



STEVE:  I was going to say, you and I, Leo, and my wife, and everyone I have any influence over, no longer uses any.



LEO:  Right.



STEVE:  We just use Windows Defender.  And believe me, it apparently is doing the job as it sure is a pain for me.



LEO:  Oh, yes.



STEVE:  Let's take a break.



LEO:  Oh, what a thought.  My goodness.  Yeah, okay.  Good, okay.  Yeah.  Let me just pull the copy up here.  I don't know, I'm a little off today.



STEVE:  Well, I'll just note while you're doing that that the Kremlin has extended the duration on its ban on Russian government agencies and critical organizations using IT services from unfriendly countries.



LEO:  Yeah.  That's exactly what's going to happen.



STEVE:  And that ban will enter into effect, the extension, on January 1st.  So that was when the previous one was going to expire.  And we still don't like each other, so you can't use those nasty Windows computers.



LEO:  I mean, in a perfect world, I mean, I often think that the best way to keep from going to war is to have an economic dependency on each other.



STEVE:  Yes, yes.  That's why it's like, it makes no sense for us to be upset with China.  Everything we own comes from China.



LEO:  Right.  Right.  And as a result, China's less likely to screw with us.



STEVE:  I would think so.



LEO:  But maybe not.  Who knows.



STEVE:  It's like, why are they messing with Taiwan, where the chips from?  They're out of their mind.



LEO:  But, see, that's more of a - see, this is the problem.  That's not a rational thing.  That's more an emotional thing.  Just like Russia invading Ukraine because it used to be part of China.  And so...



STEVE:  We want it back.



LEO:  We want it back.  But that's emotional.  It's not rational, clearly.



STEVE:  Okay.  So saw a short blurb in the Risky Business newsletter.



LEO:  That's a good name.



STEVE:  And all it said was - sorry?



LEO:  When you say "Risky Business," I think of Tom Cruise in underpants.



STEVE:  Yeah, I know.  Always.



LEO:  Always.



STEVE:  Okay.  So it just was a short blurb.  It said:  "Google Chrome complaint."  And it read:  "European privacy organization noyb" - and it's not capitalized, so noyb?  N-O-Y-B.  It's Austrian - "has filed a complaint with Austria's data protection agency against Google for its new Privacy Sandbox technology.  Noyb says Google is tricking users to enable Privacy Sandbox by falsely advertising it as an ad privacy feature."



Of course my reaction to that was "What?!"  So I dug a bit deeper.  I went over to the noyb.eu website and found their article with the headline:  "Google Chrome:  Agree to 'privacy feature,'" it has in quotes, "but get tracking."



Okay.  So their piece begins with "After years of growing criticism over invasive ad tracking, Google announced in September of 2023 that it would phase out third-party cookies from its Chrome browser."



LEO:  Wait a minute.  Noyb stands for None of Your Business.



STEVE:  Oh, that is really good.



LEO:  In German, I guess.  Maybe not.  No, it's English.



STEVE:  Really?



LEO:  It's the European Center for Additional Rights, noyb.



STEVE:  That is perfect.



LEO:  None of your business.



STEVE:  Okay.  So this guy's saying:  "After years of growing criticism over invasive ad tracking, Google announced it would phase out September of 2023 that it would phase out third-party cookies from its Chrome browser."  So this is already misleading because, while it's true that Google has been using the same ad tracking that the rest of the advertising and data aggregation industry uses, the growing criticism has been over the entire industry's use of ad tracking, not just Google's.



You know, as we've been carefully covering here, what Google is hoping to do with their Privacy Sandbox is to change the entire model of the way advertising and its user profiling operates by inventing an entirely new way for a user's browser to intelligently select from among available advertisements that are seen at websites.  And we've already heard from one of our listeners, whose job it is to implement the server-side technology of a major website, that the rest of the non-Google industry is massively pushing back against Google's attempt to end tracking.  Google really is trying to end tracking, and the rest of the community says no.  We like tracking.  We don't want  you to take it away.



Okay.  So the beginning of this article, I'll just share the beginning, it reads:  "After years of growing criticism over invasive ad tracking, Google announced in September 2023 that it would phase out third-party cookies from its Chrome browser."  That part I already read.  "Since then, users have been gradually tricked into enabling a supposed 'ad privacy feature' that actually tracks people."  Okay, it doesn't.



"While the so-called 'Privacy Sandbox'" - again in quotes from him - "is advertised as an improvement over extremely invasive third-party tracking" - which it is - "the tracking is now simply done within the browser by Google itself."  Which is not true.  "To do this, the company theoretically needs the same informed consent from users.  Instead, Google is tricking people by pretending to turn on an ad privacy feature.  Noyb has therefore filed a complaint with the Austrian data protection authority."



Okay.  Now, the article goes on at length, and it never gets any more accurate.  So there's no point in dragging everyone through it.  It's full of misconceptions and an utter lack of understanding of what Google is trying to do.  Google's Privacy Sandbox system explicitly does not track users, which is precisely why the rest of the well-established tracking industry is freaking out over it and scurrying around trying to come up with alternative tracking solutions.



This noyb is a privacy watchdog agency, as I said, based in Austria.  I looked around their site, and they appear to gauge their value to the world by the number of complaints they're able to file per day.  They're complaining about everyone and everything.  So they're kind of like a rabid version of the EFF.  You know, like the EFF, they are never going to be happy with anything short of complete and total legally and technically enforced Internet anonymity.  And in a perfect world that would be great.  But as we know, that's unlikely to happen.



Giving the author of this the most sweeping benefit of the doubt possible, the only thing I can imagine is that he confuses, hopefully not willingly, he confuses "tracking" with "profiling."  Those two words are different, and so is what they mean.  Perhaps he sees no difference.  Perhaps he doesn't consider Google's Privacy Sandbox to be the "ad privacy feature" that Google does.  We're told that websites which are able to offer identification of the viewers of the ads they display, or at least some reasonable assurance of the relevance to them of the ads, can double the revenue from their advertising.  The problem, therefore, is not Google, who's been working long and hard to find a way to do this without tracking.  The problem now is becoming websites and their advertisers who are refusing to change their own thinking.



LEO:  It's challenging.  By the way, you said IETF.  Pretty sure you didn't mean the IETF, you meant EFF.



STEVE:  Oh.  Oh, my god.



LEO:  Because I don't think the IETF cares.



STEVE:  Of course.  Thank you, thank you, thank you, Leo.  You just saved me from receiving a thousand emails.



LEO:  I know.  "Steve, the IETF, the Internet Engineering Task Force, is not at all who cares about this at all."



STEVE:  Thank you so much.



LEO:  I just thought I'd mention it.



STEVE:  Yes.  Not the - I have a couple of errata coming up, so we just reduced the errata by one.  Thank you.



LEO:  I'm trying when I can, yes.  Sometimes I miss it.  I don't know.  So there you go.



STEVE:  The EFF, exactly, thank you.  But speaking of tracking, after last week's podcast, as planned, I finished the implementation of GRC's subscription management front end and turned to the email sending side.  I designed a layout and template for the weekly podcast announcements I planned to start sending.  And Saturday afternoon, U.S. Pacific Time, I sent the first podcast summary email to this podcast's 4,239 subscribers.  And then, this morning, about three hours before - actually, we started an hour later than usual, so about four hours before this podcast began, I sent a similar summary of today's podcast to 4,330 subscribers.  The list had grown by about 100 over the past week.  So email is starting to flow from GRC, and everybody who has subscribed should have it.  If you don't find it, check your spam folder because it may have been routed over there.



Rob in Britain said:  "Hi Steve.  As Apple broke their IMAP message read flag a while back, I've been using the Blue Mail app to get my email.  Blue Mail includes a 'tracking image detector.'  And guess what, it flagged your email message as containing one.  As a Brit, the irony of a security podcast tracking me does not escape me."



Okay, now, Rob was one of a couple of people who replied with a "What the..." when their email clients reported that a so-called "tracking bug" was present in their email from me.  And since that's what their client calls it, it's natural for concern to be raised.  So I wanted to correct the record about when an email bug is tracking someone and when it's not.



The TL;DR is, it's not tracking you if it's a bug you indirectly asked for, and if it's only linked back to whom you asked from.  The confusion arises because our email clients have no way of knowing that this incoming email is not unwanted spam, and that makes all the difference in the world about the purpose and implications of the bug because, if it were an unwanted spam email, as opposed to email everyone has been clamoring for, you would definitely not want your opening of that email to send a ping back to the cretins who are despoiling the world with spam.



But in this case, no one is being tracked because the image link points only back to me, back to GRC, the source of the email that was sent to you, which only those who jumped through some hoops to ask for it in the first place would have received.  Also, unlike pretty much everyone else, and against the advice of some well-informed others, I (GRC) am sending the email myself, not through any of the typical third-party cloud providers that most organizations have switched to now using.  As a consequence, the email address our subscribers have entrusted to me will never be disclosed to any third party.  And as I noted, that single pixel "bug" is only coming back to me to allow me to obtain some statistics about the percentage of email I send that's opened and viewed.



And I've learned some interesting things, thanks to that little bug.  For example, half of our listeners, well, I guess I already knew this already, half of our listeners are using Gmail.  But I did not know that fully one quarter of our listeners are using Mozilla's Thunderbird as their email client.  I thought that was interesting.  So basically three quarters of everybody who has listed for email and received and opened their email from me, the last two that I've sent, three quarters are either - half of the total is Gmail, and the other one quarter is Mozilla's Thunderbird.



I'll also note that, as regards this bug, the Security Now! emails contain a link to the full-size Picture of the Week, and the show notes, and GRC's Security Now! summary page, all in the email, back to GRC.  So it's not as if anyone who receives these emails from me and clicks any of their links is being stealth.  Also, I chose to embed a reduced size Picture of the Week as a visible, about 250 pixels wide, thumbnail image so that the email would be self-contained and complete.  I could have linked back to GRC for the retrieval of the thumbnail when viewed.  And that way I would have obtained the same feedback that the single pixel image provided.  And presumably, since it's, like, 250x203 pixels, it will just look like a real image, and it's visible, and no email client would say, oh, whoa, you've got a tracking pixel in your email.  Right.



Anyway, it's certainly the case that unsolicited commercial spam email contains tracking bugs to inform their senders when their obnoxious unwanted spam has been opened by its recipient.  Anyone who thinks that describes the weekly podcast summaries they signed up for will be glad that every one of my emails contains a very clearly marked unsubscribe link. And, of course, it has immediate effect.  There's none of this "Please allow two weeks for your unsubscribe to be processed" nonsense.  I've seen that from other, you know, major mailers.  And I just think, wow, aren't you using computers?



Anyway, my work after today's podcast will be to automate the sending of these weekly podcast summaries.  At the moment, sending a new email to a list is not difficult, but it does involve a large number of steps and decisions which are redundant week from week.  So I want to take a bit more time to build some infrastructure to make it simple and mistake proof.



And Leo, I wish I had you.  Are you nearby?  No.



LEO:  I'm coming.  He can see me?  I know, he knows I'm coming.



STEVE:  Yes, I do.



LEO:  I tweaked my knee.



STEVE:  Oh, god, I heard about going up into the attic or something.



LEO:  Yeah.  I'm not used to stairs.  And I fell up.  I didn't fall down.  I tripped.



STEVE:  So sort of like hit the front of your knee on the...



LEO:  Yeah, I hit my knee, yeah.



STEVE:  Ooh.



LEO:  It'll get better.



STEVE:  And of course stairs to an attic are probably not padded or carpeted.



LEO:  Not good stairs, oh, no, oh, no.



STEVE:  So there are two things.  We needed to take a break because we went a long time before our second one.



LEO:  Yes, that's what I thought.



STEVE:  And you need to hear this next thing because this is from Orange Tsai.



LEO:  Orange Tsai, yes.



STEVE:  The security researcher.



LEO:  And you sent me a note about this.



STEVE:  Yup.



LEO:  And I know what you're going to say.  So, okay.  Now, back to the show we go, Mr. G.



STEVE:  So I got an email with the subject "Seeking Assistance for Black Hat USA Visa Issue."  And when I saw that this was from Orange Tsai, whose name should be familiar to all of our long-time podcast listeners, I thought, really?  That one?  I mean, that Orange Tsai?



So the email reads:  "Hello, Steve Gibson and Leo Laporte.  My name is Orange Tsai, a security researcher from Taiwan.  While I'm not a listener of the show, Jonathan Leitschuh, a friend of mine, says you've featured my work and spoke about my name many times on the show.  I've won the Pwn2Own championship and Pwnie Awards several times, as well as having been the researcher behind impactful research such as Exchange Server RCEs (mentioned in Security Now! 809, 819, 833, 844, 916 for ProxyLogon and ProxyShell), Samba RCE (Security Now! 857), Facebook RCE (Security Now! 795 for MobileIron RCE), SSL VPN RCEs (Security Now! 814 for FortiGate & Pulse Secure), and the recent PHP RCE."  In other words, yes.  We know Orange Tsai quite well on the podcast.



He says:  "I come to you with a plea for support from either you or your listeners.  I've been accepted to speak at Black Hat USA this year.  Unfortunately, due to the United States border control, I've been unable to enter the country the past few years.  I was wondering if you or your listeners had any connections that would be of assistance in this.



"Here's a brief intro to give you some of the context:  I've previously traveled to the U.S. seven times through ESTA (the U.S. Visa Waiver Program) and have presented in person at DEFCON and Black Hat USA many times without any issues.  However, after I reported several critical vulnerabilities to Microsoft in 2021, my ESTA was rejected, my guess is because one of my reported bugs has a collision with a China APT Group.  I believe this may have resulted in me being flagged by the U.S.  Since then, I've been unable to enter the United States to present at DEFCON and Black Hat USA in person.



"In 2022, I tried applying for a business/tourist visa at the embassy.  However, the consular officer couldn't decide, and my application had to be sent to DHS for further administrative processing.  After several months of review, I never got a response and missed the 2022 DEFCON/Black Hat USA dates.



"This year, I submitted my latest research and was accepted by Black Hat USA in May of 2024," so last month.  "To catch up with the visa this time, I reapplied for the B1/B2 visa in January and had the interview on March 19th.  However, three months have passed, and there's still no update.  As a security researcher, I try to do the most impactful research, and I'm keen on having my research seen by the world, especially at the top hacker gatherings like Black Hat USA.  I'm currently seeking all the help I can get to break through this situation.



"I hope this gives you a better understanding of the situation I'm facing.  This has been a long and troubling issue for me.  If you have any advice or guidance to offer, it would be greatly appreciated.  Here is my contact information in case anyone needs it.  Thank you.  Orange Tsai."



LEO:  Wow.



STEVE:  Okay.  So this is great, if we can help.  And by "we," I mean everyone listening.  So the moment I saw his name, as I said, my eyes opened wide because of course we recognized him from all the times we've talked about his many significant contributions to the security of this industry and its software systems.  I don't actively maintain the sorts of contacts that he needs for this, like with the State Department.  But I'm always surprised and flattered when I learn about the roles of many of the people who are listening to this podcast and who consider it to be worth their time.



So I'm sharing Orange Tsai's plea in the sincere hope that we do have listeners here who may have the connections required to solve this problem for him.  This year's DEFCON and Black Hat USA conferences are being held near the start of August, and today is our last podcast of June.  So we only have a month to go.  I wrote back to Orange Tsai to tell him that I would be honored to do anything I could to help by giving his situation a larger audience.  I also asked how someone who was in a position of authority might contact him if they needed further clarification.



He replied:  "Hi, Steve.  Thank you for your response.  I really appreciate your help.  My only concern comes via a friend, in that the U.S. government can be very sensitive to" - and he has in quotes - "'media pressure.'  And there have been cases where this has led to a permanent ban on entry.  Although Security Now! is not 'traditional media,' I still hope that, when mentioning my case, it can be done in a neutral manner.  When seeking help, please ask listeners to do so in their personal capacity, rather than representing me, the media, or any other sensitive identities."



So anyway, I, speaking for myself, would ask anyone to heed that.  You know, if you're in a position to help, please understand and be gentle if you're able to determine what's going on and why.  I asked him for a link to a web page of contact information, which he provided.  But all he wrote there was "Hi.  I'm Orange Tsai, a security researcher from Taiwan.  I really want to go to the U.S. to present my latest research at Black Hat USA 2024 in person.  If you have any suggestions, please feel free to email me at orange[at]chroot.org.  Thank you."



So with that, I'm leaving this in the hands of our wonderful listeners.  You know, please don't do anything if you are not the right person.  I would hate to make matters worse.  But if you are the right person, or have a sufficiently close relationship with someone who is, then it would be wonderful if we were able to help him.  His years of past work have shown that he is exactly the sort of security researcher whose work should be encouraged.



Markzip sent me a note.  He said:  "Hi, Steve.  Seems to me that an overlooked problem with Recall is" - and this was interesting - "is third-party leakage.  Listeners to Security Now! may lock down their machines and opt out of Recall, whereas the people with whom we interact may not.  If I write an email to a friend, their Recall instance now knows of our correspondence.  We can think of other leakage easily.  For instance, people frequently share passwords via email.  More examples should be easy to imagine."



Okay.  So first of all, I think Mark makes a great point.  Many people who've been critical of Recall have likened it to spyware or malware that's now being factory installed.  Through our first podcast about this, well, I should say, although our first podcast about this was titled by me "The 50 Gigabyte Privacy Bomb," I have never characterized Recall as spyware or malware because both of those things require malicious intent, and at no point have I believed, or do I believe, that Microsoft has ever had a shred of malicious intent for Recall.



I've seen other commentators suggesting that the entire purpose of Recall is to eventually send the collected data back to Redmond for some purpose.  I think that's irresponsible nonsense, and it's a failure of imagination.  For one thing, Microsoft knows that in today's world they could never do that without being found out immediately.  We are all now watching them too closely.  And besides, why would they?  The details of some grandmother's management of her canasta group is nothing that Microsoft cares about.



But that's not to say that there would not be some value to having the AI residing in Grandma's computer be "aware" of her interest in canasta.  If Windows continues to evolve, or maybe devolve, into an advertising platform - which would be unfortunate, but seems likely based on the way it's going - Microsoft, think about this, Microsoft would be crazy not to use their Recall AI's digested history and understanding of its machine's user to improve the relevance of such advertising.



And as we know, this could all be done locally on the machine, much as Google's Privacy Sandbox will be doing in the user's web browser.  In this case, the Windows OS itself would be pulling the most relevant ads from the Internet for display either in Windows itself or in their Bing web browser.  So we now have one declared and two undeclared but obvious uses for Recall.  And none of these applications for Recall's data requires it to ever leave its local machine environment.



The concern Mark raised about third-party leakage I think is a good one.  It probably hadn't occurred to most of us that not only would our own machines be recording our every move, but that all of our personal interactions with any others would also be captured by their instances of Recall.



Last week we quoted Matthew Green on the topic of Apple's Cloud Compute design.  He wrote:  "TL;DR:  It's not easy."  And he said:  "Building trustworthy computers is literally the hardest problem in computer security."  He said:  "Honestly, it's almost the only problem in computer security.  But while it remains a challenging problem, we've made a lot of advances.  Apple is using almost all of them."  So that was Matthew talking about Apple's Cloud Compute.  But the point being building trustworthy computers is the hardest problem we have.



So in Apple's case, they have the comparative luxury of housing their Cloud Compute infrastructure in data center facilities surrounded by strong physical security.  Even so, the architecture Apple has designed does not require its physical security to hold in the presence of an infiltrating adversary.  But they have physical access security nevertheless.  That's something Microsoft does not have with their widely distributed Windows workstations.  Grandma always leaves her Copilot+ PC open, logged in, and unlocked, just like her back door.



So Microsoft's challenge is greater than Apple's, which Matthew Green has just made clear is already the hardest problem in computer security.  And as we've seen with last week's revelation of a super-critical WiFi proximity remote code execution flaw that's apparently been present in Windows forever, at least since Vista, whatever solution Microsoft finally implements will need to be something we've not yet seen them successfully accomplish.



Let me say that again because I think it's really important, and it's exactly the right way to think about this.  Whatever solution Microsoft finally implements to protect its Recall data will need to be something we've not yet seen them successfully accomplish.  What everyone else other than Microsoft clearly sees is just how much having Recall running in a PC raises the stakes for Windows security.  But so far we've seen zero indication that Microsoft truly understands that this is not something they can just wave their hands around and claim is now safe for them to do because they said so.



What's not clear is whether they'll be able to use the hardware that's already present in those Copilot+ PCs to implement the sort of super-secure enclave they're going to need.  And this is to your point, Leo, you made a couple weeks ago about that's really being what's necessary.  And that makes it even more doubtful that they'll be able to securely retrofit the inventory of existing Windows 11 hardware to provide the required level of security.  It may take new hardware.  Apple has only managed to do it for their iPhone handsets because their hardware architecture is so tightly closed.  Windows has never been, since it's an OS designed to run on third-party OEM hardware.  So, for example, the phrase "Secure Boot" is an oxymoron since secure boot bypasses are continually surfacing.



I realize that I'm spending a great deal of time on Recall.  This is now the fourth podcast where I've given it some significant discussion.  And of course for the first two podcasts it was our main topic.  But given the security and privacy significance of Microsoft's proposal, it would be difficult to give it more time than it deserves.



And finally, I have two pieces of errata.  The first came from someone who wanted to correct my recent statement about the duration of this podcast.  He noted that since we started in 2005, we are still in our 19th year of the podcast, not, as I have been erroneously saying, in our 20th year.  So in two months we will be having our 19th birthday, not our 20th birthday.  He said:  "The reason we listen is that we know you care about getting the details right."  I'm glad that comes through.



LEO:  That's fair, yes.



STEVE:  So I'm happy to correct the record.  And the second mistake several of our astute listeners have spotted is that I've been erroneously saying that the big security changes in Windows XP - its built-in firewall being enabled by default and its users' access to raw sockets being restricted - came about with the release of XP's final Service Pack 3.  That's wrong.  It was the release of XP's Service Pack 2 where Microsoft finally decided that they needed to get more serious about XP's security and made those important changes.  So a thank you to everyone who said, "Uh, Steve."  I appreciate the feedback.



LEO:  Always.  Wow, that's a deep cut.  I mean, have you talked about that in a while?



STEVE:  Yeah.



LEO:  Oh, okay.



STEVE:  The last couple weeks, actually.



LEO:  Oh, all right.  It's a long time ago.  You can be excused for not remembering the exact details.



STEVE:  And I think the reason I was getting hung up on it is that I have had occasion to install some Windows XPs way later.  And of course, after installing it, I always installed Service Pack 3, which was the last service pack, in order to bring it current.



LEO:  But I do remember when they - it was a big deal when they built a firewall into Service Pack 2.  That was like - in fact, I think we pretty much said, don't use XP until Service Pack 2 comes out, basically.  It was a much-needed service pack, as I remember.



STEVE:  Yup.



LEO:  All right.  Well, you're forgiven.



STEVE:  I'm always happy to correct my mistakes.



LEO:  Yes.  That's good.  Let us talk about something very important, our sponsor for this hour, and then get to the pseudorandom number generator, or PRINGLE, as I call it.



STEVE:  From hell.



LEO:  For the PRINGLE from hell.  Although if I'd only used it, I would be in heaven.  So that's why it's a double-edged sword.



STEVE:  Actually, that's true.



LEO:  Yeah.



STEVE:  You would be a little richer, as opposed to Little Richard.



LEO:  Did you see it peaked at $67,000 a bitcoin?



STEVE:  Thanks a lot, Leo.



LEO:  Don't do the math.  Don't do the math.



STEVE:  Says he who formatted his hard drive.  



LEO:  50.  It might be worth, what, 3 million, almost 4 million.



STEVE:  North of, yeah.  That hurts.



LEO:  It's just money.  Steve, money doesn't buy happiness.



STEVE:  I'll just have to earn it the old-fashioned way.



LEO:  Yeah, there you go.  Let's talk about PRNGs.



STEVE:  Yes.  Leo, you may think it was bad.  It's worse than you could have imagined.



LEO:  Great.



STEVE:  So, yeah.  "The Mixed Blessing of a Lousy Pseudorandom Number Generator," or "When are you very glad that your old password generator used a very crappy pseudorandom number generator?"



So today I want to share the true story of a guy named Michael who, after generating 43.6 bitcoin, lost the password that was used to protect it.  With Bitcoin currently trading at around $60,000 U.S. for each coin, that's around $2.6 million dollars worth of bitcoin waiting for him at the other side of the proper password.  Unlike many similar stories, this one has a happy ending.  But it's the reason for the happy ending that makes this such an interesting story for this podcast, and offers so many lessons for us.



Okay, now, by pure coincidence, the story was recently written up by the same guy, Kim Zetter, who wrote that piece about Kaspersky for Zero Day that we were discussing earlier.  Kim's story for Wired is titled:  "How Researchers Cracked an 11-Year-Old Password to a $3 Million Crypto Wallet."  He wrote:  "Two years ago, when 'Michael'" - and he has that in air quotes.  Michael wants to remain anonymous because now Michael has a lot of money, and he would rather just keep it to himself.  "Two years ago, when Michael, an owner of cryptocurrency, contacted Joe Grand to help him recover access to about $2 million worth of bitcoin he had stored in encrypted format on his computer, Joe turned him down.



"Michael, who is based in Europe and asked to remain anonymous, stored the cryptocurrency in a password-protected digital wallet.  He generated a password using the RoboForm password manager and stored that password in a file encrypted with a tool called TrueCrypt.  At some point, that file got corrupted, and Michael lost access to the 20-character password he'd generated to secure his 43.6 bitcoin, worth a total of about 4,000 euros, or $5,300, back in 2013 when it was generated and stored."



LEO:  Yeah, a lot more now, baby.



STEVE:  That's right.  "Michael used the RoboForm password manager to generate the password, but did not store it in his manager.  He worried that someone would hack his computer to obtain the password."  Reasonable concern.



"Joe Grand is a famed hardware hacker who in 2022 helped another crypto wallet owner recover access to $2 million in cryptocurrency he thought he'd lost forever after forgetting the PIN to his Trezor wallet," which is a hardware device.  "Since then, dozens of people have contacted Grand to help them recover their treasure.  But Grand, known by the hacker handle 'Kingpin,' turns down most of them, for various reasons.



"Grand is an electrical engineer who began hacking computing hardware at age 10 and in 2008 co-hosted the Discovery Channel's 'Prototype This' show.  He now consults with companies that build complex digital systems to help them understand how hardware hackers like him might subvert their systems.  He cracked the Trezor wallet in 2022 using hardware techniques that forced the USB wallet to reveal its password.



"But Michael stored his cryptocurrency in a software-based wallet, which meant none of Grand's hardware skills were relevant this time.  He considered brute-forcing Michael's password, writing a script to automatically guess millions of possible passwords to find the correct one, but determined this wasn't feasible."  Right, you know, 20 characters, upper and lower, special cases, numbers and so forth.  As we know, 20 characters, that's strong security.



LEO:  Believe me, I know.



STEVE:  Yeah, huh.  That's right, Leo.  "He briefly considered that the RoboForm password manager Michael used to generate his password might have a flaw in the way it generated passwords, which would allow him to guess the password more easily.  Grand, however, doubted such a flaw existed."



LEO:  [Laughing]



STEVE:  "Michael contacted" - uh-huh.  The plot thickens.  "Michael contacted multiple people who specialize in cracking cryptography.  They all told him there's no chance of retrieving his money."  And I should mention they should have been right.  Right?  You know, Joe Grand should have been right.  All these crypto specialists should have been right.  "Last June he approached Joe Grand again, hoping to convince him to help, and this time Grand agreed to give it a try, working with a friend named Bruno in Germany who also hacks digital wallets.



"Grand and Bruno spent months reverse engineering the version of the RoboForm program that they thought Michael had probably used back in 2013, and found that the pseudorandom number generator used to generate passwords in that version, and subsequent versions until 2015, did indeed have a significant flaw."  And let me just say calling it a "significant flaw" is like, you know, I don't know what.



LEO:  It's understatement.



STEVE:  Calling noon "daylight" or something.  I mean, okay.  "The RoboForm program unwisely tied the random passwords it generated" - and I should explain I've dug down into the technology.  I'm going to go into the kind of detail that our listeners want after I'm through sharing what Kim wrote.  So he wrote:  "The RoboForm program unwisely tied the random passwords it generated to the date and time on the user's computer.  It determined the computer's date and time, and then generated passwords that were predictable.  If you knew the date and time and other parameters, you could compute any password that would have been generated on a certain date and time in the past.



"If Michael knew the day or general timeframe in 2013 when he generated it, as well as the parameters he used to generate the password, for example, the number of characters in the password, including lower- and upper-case letters, figures, and special characters" - and by figures I guess he means numbers and special characters - "this would narrow the possible password guesses to a manageable number.  Then they could hijack the RoboForm function responsible for checking the date and time on a computer and get it to travel back in time, believing the current date was a day in the 2013 timeframe when Michael generated his password.  RoboForm would then spit out the same passwords it generated on the days in 2013.  There was one problem:  Michael could not remember when he created the password.



"According to the log on his software wallet, Michael moved bitcoin into his wallet for the first time on April 14, 2013. But he couldn't remember if he generated the password the same day or some time before or after that.  So looking at the parameters of other passwords he generated using RoboForm, Grand and Bruno configured RoboForm to generate 20-character passwords with upper- and lower-case letters, numbers, and eight special characters from March 1st through April 20th, 2013.  It failed to generate the right password.  So Grand and Bruno lengthened the time frame from April 20th out to June 1st, 2013, using the same parameters.  Still no luck.



"Michael says that Grand and Bruno kept coming back to him, asking if he was sure about this or that parameter that he'd used.  He stuck to his first answer.  Michael said:  'They were really annoying me because who knows what I did 10 years ago.'  Anyway, he found other passwords he generated with RoboForm in 2013, and two of them did not use any special characters, so Grand and Bruno adjusted.  Last November, they reached out again to Michael to set up a meeting in person.  Michael said:  'I thought, oh my god, they're going to ask me again for the settings.'  Instead, they revealed that they had finally found the correct password, no special characters.  And it was generated on May 15, 2013, at 4:10:40 p.m. GMT.



"Grand wrote in an email to Wired:  'We ultimately got lucky that our parameters and time range was correct.  If either of those were wrong, we would have continued to take guesses and shots in the dark, and it would have taken significantly longer to pre-compute all the possible passwords.'"



Kim then provides a bit of background about RoboForm, writing:  "RoboForm, made by U.S.-based Siber (spelled with an 'S') Systems, was one of the first password managers on the market, and currently has more than 6 million users worldwide, according to a company report.  In 2015, Siber (S-I-B-E-R) seemed to fix the RoboForm password manager.  In a cursory glance, Grand and Bruno couldn't find any sign that the pseudorandom number generator in the 2015 version used the computer's time, which makes them think they removed it to fix the flaw, though Grand says they would need to examine it more thoroughly to be certain.



"Siber Systems confirmed to Wired that it did fix the issue with version 7.9.14 of RoboForm, released on June 10th of 2015; but a spokesperson would not answer questions about how it did so.  In a changelog on the company's website, it mentions only that Siber programmers made changes to 'increase randomness of generated passwords,' but it doesn't say how they did this.  Siber spokesman Simon Davis says that 'RoboForm 7 was discontinued in 2017.'



"Grand says that, without knowing how Siber fixed the issue, attackers may still be able to regenerate passwords generated by versions of RoboForm released before the fix in 2015.  He's also not sure if current versions contain the problem.  He said:  'I'm still not sure I would trust it without knowing how they actually improved the password generation in more recent versions.  I'm not sure if RoboForm knew how bad this particular weakness was.'"



Kim writes:  "Customers may also still be using passwords that were generated with the early versions of the program before the fix.  It doesn't appear that Siber ever notified customers when it released the fixed version 7.9.14 in 2015 that they really should regenerate new passwords for critical accounts or data.  The company did not respond to a question about this.



"If Siber did not inform customers, this would mean that anyone like Michael who used RoboForm to generate passwords prior to 2015, and are still using those passwords, may have vulnerable passwords that hackers can regenerate.  Grand said:  'We know that most people don't change passwords unless they're prompted to do so.'  He added that:  'Out of 935 passwords in my password manager,' he said, '(not RoboForm), 220 of them are from 2015 and earlier, and most of them are for sites I still use.'  Depending on what the company did to fix the issue in 2015, newer passwords may also be vulnerable."  We don't know.



"Last November, Grand and Bruno, having earned their reward, deducted a percentage of bitcoin from Michael's account for the work they did, then gave him the password to access the rest.  The bitcoin was worth $38,000 per coin at the time.  Michael waited until it rose to $62,000 per coin and sold some of it.  He now has 30 BTC, now worth $3 million, and is waiting for the value to rise to $100,000 per coin.



"Michael says he was lucky that he lost the password years ago because otherwise he would have sold off the bitcoin when it was worth $40,000 per coin and missed out on a greater fortune.  He said:  'My losing the password was financially a good thing.'"



LEO:  Yeah, that's how I feel.  If I ever - now, you can never recover yours.  But if I ever remember my password, why, it's just been a long-term savings account.



STEVE:  Okay.  So, first of all...



LEO:  But a bad PRNG, ooh.  They're always bad, aren't they?  That's what the "pseudo" means.



STEVE:  Oh, well, Leo, wait for this.  Oh, my god.  First of all, RoboForm is probably a well-known name to everyone, even those of us who never had occasion to use it.



LEO:  They were one of the first.



STEVE:  I'm in that camp.  You're in that camp.



LEO:  No, I think I used it, back in the day, though.  I mean, many years ago.



STEVE:  Okay.  Because it was the only - it was once the...



LEO:  It was the first one, yeah.



STEVE:  Yes, yes.  Okay.  But since this podcast has been going since 2005, we've covered the span of time that RoboForm was apparently using a horrific password generation scheme.  One of this podcast's early and continuing focuses has been on the importance of the strength of pseudorandom number generators used in cryptographic operations.  So I was quite curious to learn more about what exactly Grand and Bruno found when they peeled back the covers of RoboForm circa 2013.  And I was reminded of a line from the sci-fi movie "Serenity" where our villain says to Mel:  "It's worse than you know," to which Mel replies, "It usually is."



Believe it or not, whenever the user of RoboForm v7.9.0 - and probably my theory is even v1, but we'll get to that in a minute.  But definitely 7.9.0, which was released on June 26th of 2013.  Whenever the user pressed its Generate Password button, RoboForm, up until its repair two years later with 7.9.14, simply took the Windows system's Unix time, which is the number of seconds elapsed since January 1st, 1970, and directly and deterministically used that time of day to produce the user's password.  RoboForm didn't even take the trouble to create a unique per-system salt so that differing installations would produce differing bad passwords.  This meant that if two users anywhere were to press the Generate Password button within the same one-second interval, if they were using the same password parameters, identical passwords would be generated.



Grand and Bruno discovered something else when they opened up RoboForm.  The designers of this password generator, that should really just be called a "time scrambler," realized that if a user happened to press the Generate Password button a second time within the same second, the same password would be generated.  To cover up this flaw, they subtract a fixed amount of time from the system time for repeats.  What an utter disaster.



One thing we don't know is for how long RoboForm's password generator was this horrific before it was changed.  I originally wrote "before it was fixed," but we don't know that it's been fixed.  We don't know that it, you know, how it was changed.  And we don't know why it was changed.  But I have a theory about that.  My theory is that this must have been the original implementation of RoboForm's password generator.  The reason I think that is that by 2013 no one would have ever designed such a horrifically lame password generation scheme.



This had to have been a very early password generator created back in the late '90s or early 2000s before there was much awareness of the proper way to do these things.  And then, following the well-understood property of software inertia, 10 to 15 years went by without anyone at RoboForm bothering to think about it again because it was, after all, producing random-appearing passwords.  But for some reason, whatever reason, eventually someone noticed and apparently fixed it.  We don't know how, but at least changed it.



Grand and Bruno note that something did finally change in 2015 with 7.9.14.  But since RoboForm is both closed source and closed mouthed, we have no idea what may have precipitated the change, nor what the new algorithm was changed to.  So I'm put in mind of Bitwarden, the password-generating sponsor of this network, where we can know anything we want to know about its innards, first because, if we ask, we'll be told; secondly because it's probably openly documented; and thirdly because the source code of the solution is publicly available.  None of which is true for RoboForm.



The final note that's worth repeating is the point that Grand highlights:  Regardless of their apparent complexity, we now know that's an illusion.  It's just the scrambled time of day and date, without even having any per-system salt, which means that all user scramblings are identical for all owners of RoboForm, probably from the beginning, its first release, through 2015.  Therefore, any passwords that were ever generated by RoboForm, presumably until 7.9.14, can be reverse engineered, and the set of possible passwords can be further narrowed by the degree to which their approximate date of creation is known.



Even if the format of the password is not known, there are a limited number of choices available for upper and lower case, special characters, numbers and length.  So if someone were determined to crack into something that was being protected by a password that they had reason to believe had been generated by RoboForm, and they had some idea of when, such as the date of the protected account's creation, it's not a stretch to imagine that it could be done.



Sure, I would put the chances of this actually happening being done as extremely remote at best.  But anyone who was using RoboForm back then, who may have never had the occasion to update their passwords since, should at least be aware that those passwords were simply generated by scrambling the time of day, and with a resolution of only one second.



LEO:  That's terrible.  That's so terrible.



STEVE:  There are not a cryptographically strong number of seconds in a day.  And while I don't want to throw shade on RoboForm's products of today, which might be excellent, given the history that has just been revealed, RoboForm is certainly not something I could ever use or recommend, especially when there are alternatives like Bitwarden and 1Password which are hiding nothing, and RoboForm is hiding everything.



And this brings me to the final and most important point and lesson I want to take away from this.  Way back when I and this podcast first endorsed LastPass, I was able to do so with full confidence.  And in fact the only reason I was able to do so, and did, was because the product's original designer, Joe Siegrist, completely disclosed its detailed operation to me.  It was the 21st Century, and Joe understood that the value he was offering was not some secret crypto mumbo jumbo.  That was 20th-century thinking.  Joe understood that the value he was offering was a proper implementation of well-understood crypto that was then wrapped into an appealing user experience.  The value is not in proprietary secrecy, it's in implementation, maintenance and service.  As we know, many years and ownership changes later, LastPass eventually let us down.  I hope Joe is relaxing on a beach somewhere, because he earned it.



LEO:  I think he is.



STEVE:  So the lesson we should take from what can only be considered a RoboForm debacle is that something like the design of a password generator is too important for us to trust without a full disclosure of the system's operation and its subsequent assessment by independent experts.  Any password generator that anyone is using should fully disclose its algorithms.  There's no point in that being secret in the 21st Century.  It doesn't necessarily need to be open source, but it must be open design.  No company should be allowed to get away with producing passwords for us while asking us just to assume those passwords were properly derived, just because their website looks so nice.  What the marketing people say has exactly zero bearing on how the product operates.  It's obvious that we cannot assume that just because a company is offering a fancy-looking crypto product that they have any idea how to correctly design and produce such a thing.  There's no reason to believe that there are not more RoboForms out there.



LEO:  What's the best way, I mean, software random number generators are pseudo because they repeat eventually; right?



STEVE:  Remember that the first thing I started doing, the first piece of technology I designed for SQRL, and I talked about it on the podcast, was I created what I called an "entropy harvester."  It was harvesting entropy from a range of sources.  It was pulling from Windows' own random number generator.  I fed mouse clicks and received network packets, DNS transfer rates, all the noise that I could was constantly being poured into a hash that SQRL was churning.  And the idea was to create something unpredictable.  Unpredictability is the single thing you want.  And so the idea was that, I mean, like almost immediately SQRL's pseudorandom number generator would just have so much noise poured into it, all of that affecting its state, that there would be no way for anybody downstream to have ever been able to predict the state that SQRL's pot of entropy was in at the time that it generated a secret key.



LEO:  Right.  Gallia's reminding us that Cloudflare uses a wall of lava lamps to generate their random numbers.



STEVE:  Yes.  Yes.



LEO:  But it's not the seed you're generating because, as I remember with software random number generators, if you reuse the same seed, you'll get the same sequence of numbers.  It'll repeat eventually; right?



STEVE:  Yes.  Those are old pseudorandom number generators.



LEO:  That's not how we do it anymore.



STEVE:  Right.



LEO:  Okay.  And I do remember you saying the best way to do it would be use a capacitor.  Was that right?



STEVE:  Actually, a diode.



LEO:  A diode, that's right.



STEVE:  A reverse bias diode where you put it just at the diode junction's breakdown voltage.  And what happens is you get completely unpredictable electron tunneling across the reverse bias junction to literally create hiss.  If you listen to it, it is hiss.



LEO:  Right.



STEVE:  And it's truly - it is quantum level noise.  And that's as good as it gets.



LEO:  Wow.  That would be the best way, you think?  As good as it gets, yeah.



STEVE:  That is what all of the true random number generators now do.  It is a variation on that.  They actually do some post-processing because the noise can be skewed, but it is utterly unknowable.



LEO:  That's actually fascinating problem in computer science because, you know, you might say, well, is a coin flip random?  Well, it is with a perfect coin, but no coin is perfect.  A roulette wheel is random with a perfect roulette wheel.  But there is no such thing.  They all have biases.



STEVE:  I was asked in 1974 to design a little machine that some people would take to Las Vegas.  And it was going to be operated with toe switches because it could not...



LEO:  Oh, that's the Eudaemonic Pie.  This was in Santa Cruz; right?  There's a book about this.



STEVE:  Actually, it was close, it was close to Santa Cruz, yes.



LEO:  Yeah, there's this famous book about this.  Have you read "The Eudaemonic Pie"?



STEVE:  No.



LEO:  Well, they got caught.  But they made a lot of money.



STEVE:  Yeah.  And what they were doing was they were recording, at least in the case of the guys who asked me to develop this thing, they were recording roulette wheel results because no roulette wheel is perfect.



LEO:  Right.



STEVE:  And so, and believe it or not, they had this thing running already, and they were using a wire recorder to record tones that their toes were generating, and they wanted me to do a solid-state version for them.



LEO:  Yeah.  They wore computers in their shoes to basically solve roulette, and they won a lot of money.  And because, now, people are used to people counting cards in blackjack, but everybody in Vegas assumes, oh, a roulette wheel can't be beat.  Well, they can.  If you haven't read this book, you've got to read it.  I wonder if it's the same guys.  Very interesting story, "The Eudaemonic Pie."  And I'm pretty sure that they were in the Santa Cruz area.



STEVE:  Well, that would be the right physical area because I was in Mountain View, which is just over the hill.



LEO:  Yeah.  And it was a toe computer.  They would - wow.



STEVE:  Yup.



LEO:  Wow.  How fascinating is that?  So, yeah, maybe someday, maybe if you've got, you know, a slow week - I know there's never a slow week on this show - you could talk a little bit about random numbers and why they're pseudo and why, you know, how to - it's a challenge.  It's a nontrivial way to generate those with computers.



STEVE:  And crucially important.  It's funny because we think about crypto as solving all the problems.  But I'm not sure I can think of an instance where you don't need something random.  When you're choosing a private key for public key crypto, you need high-quality random numbers.  And we've seen failures of that where, for example, studies of the private keys used on web servers have turned up a surprising number of collisions of private keys because they were all getting their key shortly after turning on a version of Linux that hadn't yet had time to develop enough entropy.  It hadn't warmed up its pseudorandom number generator enough.



LEO:  I think that you were - I can't believe you've not heard of the book.  The book focuses on a group of University of California Santa Cruz physics graduate students who in the late '70s and early '80s designed and employed miniaturized computers hidden in specially modified platform-soled shoes to predict the outcome of casino roulette games.  I think you were an unwitting - you didn't do it, though; right?



STEVE:  I didn't do it.



LEO:  You didn't do it.  But they found somebody to do it.



STEVE:  Wow.



LEO:  What a story.  That may also be one of the first wearable computers, ironically.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#981

DATE:		July 2, 2024

TITLE:		The End of Entrust Trust

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-981.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Why does everyone running OpenSSH need to patch immediately?  Who just moved 50 bitcoins minted in 2010?  (Sadly, it wasn't me.)  How are things going with our intrepid Voyager 1?  What features have I removed from GRC's email system?  And what embarrassingly affordable commercial emailing system do I now recommend without reservation?  Who's a "she" and not a "he"?  What's recently been happening with Syncthing?  Why do I use DNS for freeware release management?  And what in the world happened to cause one of the industry's original SSL/TLS certificate authorities to fall from grace and lose all future access to Chrome's root store?  Another really great episode of Security Now! is yours for the taking.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with some scary news about OpenSSH.  An old bug that was fixed is back, and it means a lot of people are going to have to update their OpenSSH.  Problems with the speed of Syncthing?  Steve has a solution.  And then we'll talk about a certificate authority that really seems to have messed it up.  It looks like they're in deep trouble.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 981, recorded Tuesday, July 2nd, 2024:  The End of Entrust Trust.



It's time for Security Now!, the show where we cover the latest security and privacy news, keep you up to date on what's happening in the world out there with this guy right here, he's in charge, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  I'm in charge.



LEO:  You are.



STEVE:  That's right.



LEO:  Large and in charge.



STEVE:  Just don't ask my wife if she thinks I'm in charge.



LEO:  No. 		



STEVE:  We have today something for everyone.  We're going to do, oh, boy, going to take a deep dive into this disastrous OpenSSH problem, which really does mean everybody needs to patch their instances of OpenSSH if it's exposed to the Internet.  And I'll explain why, and everyone's going to come away with tinnitus.  I mean, really, this one'll wind up your propeller beanies.  But we also are going to spend most of our time, like the latter half of the podcast, looking at the politics of the whole question of certificate authority self-management because one of the oldest original certificate authorities in the world, Entrust, has, well, they've gotten themselves in so much trouble that they're no longer going to be trusted by the industry's browsers.  So basically all of their SSL/TLS certificate business is gone in a few months.



LEO:  Wow.



STEVE:  It's like, yeah.  And so this also gives us an occasion to really look at the behind-the-scenes mechanisms by which this can happen.  And of course we've covered CAs falling from grace in the past.  This is a biggie.  Also, someone just moved 50 bitcoins, minted back in the Satoshi era.  Sadly, it wasn't me.  But who was it?



LEO:  Well, that's interesting.  Hmmm.



STEVE:  Yeah.  Also, how are things going with our intrepid Voyager 1 spacecraft?  What features have I just removed from GRC's email system?  And by the way, just shy of 5,000 people have email from me, from like an hour ago.



LEO:  Nice.



STEVE:  And what embarrassingly affordable commercial emailing system am I now prepared to recommend without reservation, which is what I'm using?  And I could not be more impressed with it or its author.  Who's a "she" and not a "he," whom I mistakenly referred to last week?  What's recently been happening with Syncthing, and what can you do about it?  Why do I use DNS for freeware release management, and how?  And then we're going to spend the rest of our time taking a look at the title of today's podcast 981 for this July 2nd, "The End of Entrust Trust."



LEO:  Wow.



STEVE:  So I think, you know, a really great episode of Security Now! that is now yours for the taking.



LEO:  Well, it will be in a moment.  I mean, as soon as I get through my ad it'll be yours for the taking.  I'm going to keep it here for a little bit.  Wow, very interesting.  I can't wait to hear about all of these things.  This is going to be a very geeky episode.  I like that.



STEVE:  Yeah.



LEO:  I always enjoy it when your shows are a little bit on the propeller-head side.  All right.  Let's get back to Steve Gibson and your first topic of the day.



STEVE:  Well, our Picture of the Week.



LEO:  Oh, yes.



STEVE:  I gave this one the caption "Perhaps we deserve to be taken over by the machines."  Because...



LEO:  Oh, dear.  Oh boy, oh boy, oh boy.



STEVE:  I don't know.



LEO:  Describe this for us, will you?



STEVE:  So we have a close-up photo of a standard intersection traffic light.  And permanently mounted on the horizontal member which holds the red, yellow, and green lights is a sign that's got the left turn arrow with a big red, you know, the big circle, red slash through it, clearly indicating that if a police officer is watching you, and you turn left, you'll be seeing some flashing lights behind your car before long.  Now, the problem here, the reason I'm thinking, okay, maybe machines will be better at this than we are, is that the signal has a left green arrow illuminated.  Meaning, you know...



LEO:  Which means turn left.



STEVE:  Turn left here.  So I'm not sure what you do.  This is one of those things where the automated driving software in the electric vehicle...



LEO:  Oh, imagine.



STEVE:  It comes here and just quits.  It just shuts down and says, okay.



LEO:  I don't know what to do.



STEVE:  I don't know what.  I'm seeing that I can't turn left, and the signal is saying I must.  And not only that, but it's my turn.  So I give up.  Anyway.  Frankly, I don't know what a human would do if we came to this.  It's like...



LEO:  Go straight, that's what I'd do.  I would not - anything but turn left.



STEVE:  Wow.  Okay.  So the big news of the week we're going to start with because we're going to talk about industry politics and management, like self-management of the whole public key certificate mess at the end of the show.  But if you survive this first piece, you'll be ready for something as tame as politics.  Everyone's buzzing at the moment about this regression flaw.  It's a regression because it was fixed back in OpenSSH in 2006.  And in a later update it came back.  Thus there was a regression to an earlier bad problem.  This was discovered by Qualys in OpenSSH, a widely used and inherently publicly exposed service.  And in fact, when I say "widely used," we're talking 14 million vulnerable publicly exposed servers identified by Shodan and Censys.  



So the developers of OpenSSH have been historically extremely careful.  And thank god, because OpenSSH being vulnerable, that would be a big problem.  In fact, this is - get a load of this -  the first vulnerability to be discovered in nearly 20 years.  That's an astonishing track record for a chunk of software.  But nevertheless, when you hear that OpenSSH has an unauthenticated remote code execution vulnerability that grants its exploiter full root access to the system with the ability to create a root-level remote shell, affects the default configuration and does not require any interaction from the user over on the server end, that ought to get anyone's attention.



Okay.  So we have CVE-2024-6387.  So here's what its discoverer, Qualys, had to say.  They wrote:  "The Qualys Threat Research Unit (TRU) discovered this unauthenticated Remote Code Execution  vulnerability in OpenSSH's server" - which is sshd because it's a Linux daemon - "in glibc-based Linux systems.  This bug marks the first OpenSSH vulnerability in nearly two decades, an unauthenticated remote code execution that grants full root access.  It affects the default configuration and does not require user interaction, posing a significant exploit risk.



"In Qualys TRU's analysis, we identified that this vulnerability is a regression of the previously patched vulnerability which was CVE-2006-5051, reported in of course 2006.  A regression in this context means that a flaw, once fixed, has reappeared in a subsequent software release, typically due to changes or updates that inadvertently reintroduce the issue.  This incident highlights the crucial role of thorough regression testing to prevent the reintroduction of known vulnerabilities into the environment.  This regression was introduced in October of 2020 with OpenSSH 8.5p1."  So it's been there for four years.  And what this means is any OpenSSH from 8.5p1 on, thus for the last four years, is vulnerable.



Now they say:  "OpenSSH is a suite of secure network utilities based on the SSH protocol that are essential for secure communication over unsecured networks.  It provides robust encryption, secure file transfers, and remote server management.  OpenSSH is widely used on Unix-like systems, including macOS and Linux, and it supports various encryption technologies and enforces robust access controls.  Despite a recent vulnerability, OpenSSH maintains a strong security record, exemplifying a defense-in-depth approach and a critical tool for maintaining network communication confidentiality and integrity worldwide."



Okay.  So what we're dealing with in this instance is a very subtle and very tight race condition between multiple threads of execution.  Remember that not long ago we spent some time looking at race conditions closely.  Back then, I used the example of two threads that both wanted to test and conditionally increment the same variable.  A race condition fault could occur if one thread first read the variable and tested it, but before it could return the updated value it was preempted, then another thread came along to change that shared variable without the first thread being aware.  Then when that first thread had its execution resumed, it would place the updated value back into the variable, destroying whatever that second thread had done.  That's a bug, and just something as simple as that can lead to the loss of, you know, lots.



Okay.  So today, as it happens, we're going to see a real world example of exactly this sort of problem actually occurring.  Okay.  So first, though, I want to share Qualys' note about OpenSSH in general.  In their technical report about this, they wrote:  "OpenSSH is one of the most secure software in the world.  This vulnerability is one slip-up in an otherwise near-flawless implementation.  Its defense-in-depth design and code are a model and an inspiration, and we thank OpenSSH's developers for their exemplary work."



Then they explain:  "This vulnerability is challenging," they write, "to exploit due to its remote race condition nature, requiring multiple attempts for a successful attack. This can cause memory corruption and necessitate overcoming Address Space Layout Randomization (ASLR).  Advancements in deep learning may significantly increase the exploitation rate, potentially providing attackers with a substantial advantage in leveraging such security flaws.



"In our experiments" - and I should note that they wrote this, but we'll see later this is one of three sets of experiments, and this was the least worrisome of the three.  They wrote:  "In our experiments, it takes around 10,000 tries on average to win this race condition.  So, for example, with 10 connections being accepted every 600 seconds, it takes on the average of one week to obtain a remote root shell."  On the other hand, you can obtain a remote root shell, which is not nothing.  And as it turns out, there are ways to optimize this heavily, which we'll get to.



Okay.  So of course - and again, they say "around 10,000 tries on average to win the race condition."  So that's statistics; right?  It could happen on the first try, or never, or anywhere in between.  You know, it's like those 50 bitcoin I mined back in 2011.  I got lucky.  And it's still possible to get lucky today, though it's vastly less likely than it was back then.  The great concern is the available inventory, the total inventory of currently vulnerable OpenSSH servers which are publicly exposed to the Internet.



Qualys writes that searches using Censys and Shodan had identified over 14 million potentially vulnerable OpenSSH server instances exposed to the Internet; and that within their own, that is, Qualys's own customer base of users who are using their CSAM 3.0 External Attack Surface Management technology, approximately 700,000 external Internet-facing instances of their own customers are vulnerable.  And they explain that this accounts for around 31% of all Internet-facing instances of OpenSSL in their own global customer base.  So, you know, of their own customers, they know of 700,000 external Internet-facing instances vulnerable.  Censys and Shodan have all of those and an additional 13-plus million more.



Okay.  So the way to think about this is that both intensely targeted and diffuse and widespread attacks are going to be highly likely.  If a high-value target is running a vulnerable instance of OpenSSH, once this has been fully weaponized, someone can patiently try and retry in a fully automated fashion, patiently knocking at the door until they get in.  And what makes this worse, as we'll see, is that the attack is not an obvious flooding style attack that might set off other alarms.  Its nature requires a great deal of waiting.



This is why the 10 connections over 600 seconds that Qualys mentioned.  Each attack attempt requires, the way it actually works, is a 10-minute timeout.  But since 10 can be simultaneously overlapped and running at once against a single target, that brings the average rate down to one completed attack attempt per minute.  So on average, you're getting one new connection attempted per minute, which is each of those patiently knocking quietly on the door until it opens for them.  



And note that what this means is that a single attacker can be, and almost certainly will be, simultaneously spraying a massive number of overlapping connection attempts across the Internet. It would make no sense for a single attacking machine to just sit around waiting 10 minutes for a single connection to timeout.  Rather, attackers will be launching as many new attempts at many different targets as they can during the 10 minutes they must wait to see whether a single connection attempt succeeded on any one machine.



So to that end, they wrote:  "Qualys has developed a working exploit for the regreSSHion vulnerability.  As part of the disclosure process, we successfully demonstrated the exploit to the OpenSSH team to assist with their understanding and remediation efforts.  We do not release our exploits, as we must allow time for patches to be applied.  However, even though the exploit is complex, we believe that other independent researchers will be able to replicate our results."



Okay, and then indeed they detail exactly where the problem lies.  I'm going to share two dense paragraphs of techiness, then I'll pause to clarify what they've said.  So they wrote:  "We discovered a vulnerability, a signal handler race condition, in OpenSSH's server (sshd).  If a client does not authenticate within the LoginGraceTime" - which is 120 seconds recently, 600 seconds in older OpenSSH versions - "then sshd's SIGALRM handler is called asynchronously" - that's a key, asynchronously - "but the signal handler calls various functions that are not async-signal-safe."  For example, it calls syslog() to log the fact that somebody never authenticated, and it's going to hang up on them.



They said:  "This race condition affects sshd in its default configuration.  This vulnerability is exploitable remotely on glibc-based Linux systems, where syslog() itself calls async-signal-unsafe functions like malloc() and free()," which allocate and free dynamically allocated memory.  They said:  "An authenticated remote code execution as root, because it affects sshd's privileged code, which is not sandboxed and runs with full privileges, can result.  We've not investigated any other libc or operating system.  But OpenBSD is notably not vulnerable because its SIGALRM handler calls syslog_r(), which is an async-signal-safe version of syslog() that was invented by OpenBSD back in 2001."



Okay.  So what's going on here is that when someone connects to a vulnerable instance of OpenSSH, as part of the connection management, a connection timeout timer is started.  That timer was once set to 600 seconds, which is 10 minutes.  But in newer builds giving someone 10 minutes to get themselves connected seemed excessive and unnecessary, so it was shortened to 120 seconds, which is two minutes.



Unfortunately, at the same time they increased the number of simultaneous waiting connections to complete from 10 to 100.  So that really did make things worse.  And because the attack inherently needs to anticipate the expiration moment, a shorter expiration allows for faster compromise since it's the instant of timer expiration when OpenSSH is briefly vulnerable to exploitation.  That window of vulnerability is what the attacker anticipates and exploits.  So the more often you get those little windows, the worse off you are.



So upon a new connection the timer is started to give the new connection ample but limited time to get itself authenticated and going.  And if the incoming connection just sits there doing nothing, or trying and failing to properly authenticate, regardless of what's going on and why, when that new connection timeout timer expires, OpenSSH drops that still-pending connection.  Right?  All that makes sense.  That's the way you'd want things to operate.  Unfortunately, before it does that, as it's doing that, it goes off to do some other things, like make an entry in the system log about this expired connection attempt.  So if the wily attacker was doing something on purpose at the precise instant that the connection expiration timer expires, the race condition can be forced to occur.



LEO:  Wow.



STEVE:  Yeah, yeah.



LEO:  Modern-day hacks are so subtle and interesting.



STEVE:  Yeah, because all the easy ones are gone.



LEO:  Yeah, that's a good point, yeah.  Yeah, right.



STEVE:  Yeah.  The dumb ones we already - we're not doing dumb problems anymore.



LEO:  No.  Well, you can see how this could have been a regression, too.  Now it would easy to reintroduce it.



STEVE:  Yeah, there was actually an ifdef that got dropped from an update, and that allowed some old code to come back in that had been deliberately deffed out.  So just as with the two threads in my original shared variable example, the timer's expiration "asynchronously," and that's the key, it asynchronously interrupts.  That means it doesn't ask for permission to interrupt.  It just yanks control away from OpenSSH in order to start the process of tearing down this connection that never authenticated itself.



So if the attacker was able to time it right, so that OpenSSH was actively doing something involving memory allocation at the exact time of the timer's expiration, the memory allocations that would then be performed by the timer-driven logging action would conflict and collide with what the attackers were causing OpenSSH to be doing, and that could result in the attacker obtaining remote code execution under full root privilege and actually getting themselves a remote shell onto that machine.



With the massive inventory of 14 million exploitable OpenSSH servers currently available, this is going to be something bad guys will not be able to resist.  And unfortunately, as we know, with so many of those forgotten, unattended, not being quickly updated, whatever, there's just no way that attackers will not be working overtime to work out the details of this attack for themselves and get busy.



Qualys explained:  "To exploit this vulnerability remotely - to the best of our knowledge, the original exploit of this" - the original vulnerability of this CVE-2006-5051, which I initially mentioned - "was never successfully exploited before," they said, "we immediately face three problems.  From a theoretical point of view, we must find a useful code path that, if interrupted at the right time by SIGALRM, leaves sshd in an inconsistent state, and we must then exploit this inconsistent state inside the SIGALRM handler.  From a practical point of view, we must find a way to reach this useful code path in sshd, and maximize our chances of interrupting it at the right time.  And then from a timing point of view we must find a way to further increase our chances of interrupting this useful code path at the right time, remotely."  So theoretical, practical, and timing.



They said:  "To focus on these three problems without having to immediately fight against all the modern operating system protections, in particular ASLR and NX," which is execution protection, no execute, they said:  "We decided to exploit old OpenSSH versions first, on an x86 system, and then, based on this experience, move to recent versions.  So their first experiment was Debian - they showed it as, well, it was the old woody version, which they show as Debian 1:3.4p1-1.woody.3."  They said:  "This is the first Debian version that has privilege separation enabled by default and that is patched against all the critical vulnerabilities of that era."



They wrote:  "To remotely exploit this version, we interrupt a call to free()" - where memory is being released back to the system - "with SIGALRM, inside sshd's public-key parsing code."  Now, that's significant because that means that the attacker is causing OpenSSH to do some public key parsing, probably presenting it with a bogus public key, saying here's my key, use this to authenticate me.  So unfortunately, bad guys have been given lots of clues here as a consequence of this disclosure.  They know exactly where to look and what to do.



So they said:  "We interrupt a call to free() with SIGALRM while sshd is in its public-key parsing code.  That leaves the memory heap in an inconsistent state, and exploit this inconsistent state during another call to free(), inside the SIGALRM handler."  Probably in syslog.



They said:  "In our experiments it takes around 10,000 tries on average to win this race condition," in other words, with 10 connections, which is the MaxStartups setting, accepted per 600 seconds, which is the LoginGraceTime.  They said:  "It takes around one week on average to obtain a remote root shell."  But again, even if you couldn't multiplex this, and you couldn't be attacking a bazillion servers at once, just one attacker camped out on some highly valuable OpenSSH that thinks it's secure because, hey, we use public key certificates.  You're never going to guess our password.  It's just sitting there, ticking away, knocking patiently at the door an average of once a minute because it can be doing it 10 times over 10 minutes, and eventually the door opens.



LEO:  10,000 tries is hysterical.



STEVE:  Right.  But very patiently; right?



LEO:  Because you have to be patient.  But still, that's how subtle this race condition is; right?



STEVE:  Yes, yes.  Well, because it also involves vagaries of the Internet timing.



LEO:  Right.



STEVE:  Because you're a remote person, I mean, the good news is the further away you are, if you're in Russia with a flaky network and lots of packet delay, or you're in China, and there's so many hackers that your packets are just competing with all the other attackers, then that's going to introduce a lot more variation.  But still, again, this is where patience pays off.  You end up with a remote shell with root privilege on the system that was running that server.  So the point is, yeah, around 10,000 tries, but massive payoff on the other side.



Okay.  Then they said, and I won't go through this in detail, on a newer Debian build where the LoginGraceTime had been reduced from its 600 seconds down to 120, in other words, from five minutes to two minutes, it still took them around 10,000 attempts.  But since they only needed to wait two minutes for timer expiration rather than 10 minutes, and they were able to do - oh, no, sorry.  On that system they were still only to do 10 at once.  So it reduced the wait from five minutes to two minutes, I'm sorry, from 10 minutes to two minutes.  They were able to now obtain a remote shell - this is on a newer build - in one to two days, down from around a week.



And finally, on the most current stable Debian version v12.5.0, due to the fact that it has reduced the login time to 120 seconds, but also increased the maximum number of simultaneous login attempts, that so-called "MaxStartups" value, from 10 to 100, they wrote:  "In our experiments, it takes around 10,000 tries on average to win this race condition, so on this machine three to four hours with 100 connections accepted per 120 seconds.  Ultimately, it takes around six to eight hours on average to obtain a remote root shell, because we can only guess the glibc's address correctly half of the time due to ASLR."



And they finish, explaining:  "This research is still a work in progress.  We've targeted virtual machines only, not bare-metal servers, on a mostly stable network link with around 10 milliseconds of packet jitter.  We are convinced that various aspects of our exploits can be greatly improved, and we've started to work on an amd64, you know, 64-bit world, which is much harder because of the stronger ASLR."  And of course that's Address Space Layout Randomization.  And the reason 64 bits makes things much worse is that you have many more high bits to allow for more randomized places to locate the code.



And finally, they said:  "A few days after we started our work on the amd64, we noticed a bug report in OpenSSH's public Bugzilla, regarding a deadlock in sshd's SIGALRM handler.  We therefore decided to contact OpenSSH's developers immediately to let them know that this deadlock is caused by an exploitable vulnerability.  We put our amd64 work on hold, and we started to write this advisory."



Okay, so, yikes.  We have another new and potentially devastating problem.  Everyone running a maintained Linux that's exposing an OpenSSH server to the public Internet, and potentially even major corporations using OpenSSH internally - because, you know, can you trust all your employees? - need to update their builds to incorporate a fix for this immediately.  Until that's done, and unless you must have SSH running, it might be worth blocking its port and shutting it down completely.



LEO:  I think I have SSH running on my Ubiquiti system and my Synology NAS.



STEVE:  Yes, probably.  And how about on the Synology box?



LEO:  Yeah, yeah, yeah.  So I'd better check on both of those.



STEVE:  Yeah.



LEO:  And my server, too, come to think of it.



STEVE:  So, yeah.  I mean, no, it's - this is a big deal.



LEO:  Yeah.



STEVE:  So, and as I said at the top, both profiles, you know, a high-value target could be located.  And notice that nothing prevents 50 different people from trying to get into the same high-value target at once.  So high value is a vulnerability, and just being present is one because the bad guys are going to be spraying the Internet, just looking for opportunistic access.  If nothing else, even if they don't care what's going on on your server, they want to put a cryptominer there, or stick a botnet node there.  I mean, they're going to want in on these machines where now they have a way.  This gives them a way for anything that has been brought up with code for the last four years, since 2020, when this regression occurred.  And, you know, they also know some systems will be getting patched, so there's also a rush.



LEO:  Right.



STEVE:  To weaponize this thing and get into the servers they can.



LEO:  The way you describe it, it sounds so difficult to implement.  But they publish proofs of concept which a script kiddie can implement; right?  



STEVE:  Yup.



LEO:  Yeah.



STEVE:  Yup.  It'll end up being productized.



LEO:  Right.  And you don't need to know what you're doing, you just...



STEVE:  In the same way that we saw that Windows WiFi bug last week, some guy offering, for five grand you can just buy it right now. 



LEO:  Wow.  Ah, well.  What a world.



STEVE:  Hey, it keeps this podcast full of...



LEO:  Thank goodness.  That's right.  Hello.  We appreciate it.  Keep up the good work, bad guys.  You're keeping us busy.  Do you want me to do an ad now, or do you want to keep going?  Up to you.



STEVE:  Yep, perfect timing.



LEO:  Okay.  You're watching Security Now! with our genius at work, Steve Gibson.  Wouldn't be able to talk about this stuff without him, I tell you.  He's the key.  Now back to Steve Gibson, who is protecting us all week long.



STEVE:  So a listener of ours, James Tutton, shot me a note asking whether I may have found my 50 bitcoin when he saw an article about 50 bitcoin having been moved from a long dormant wallet.  Now, yeah, I wish that was my 50 bitcoin, but I've satisfied myself that they are long gone.  But I thought our listeners would enjoy hearing about the general topic of ancient bitcoin movement.  The article, which appeared last Thursday at Cryptonews.com, was titled:  "Satoshi Era Bitcoin Wallet Awakens."



LEO:  Wow.  When did you make your 50 bitcoin strike?  It was early on.



STEVE:  February 9th of 2011.



LEO:  Okay.  So it was one year after this.  Oh, that's interesting.



STEVE:  Yeah, it was early, but it was not this early.



LEO:  Okay.



STEVE:  So they said:  "Satoshi Era Bitcoin Wallet Awakens, Moves 50 BTC to Binance."  And the Crypto News piece starts out saying:  "A Satoshi era Bitcoin wallet address, dormant for 14 years, transferred 50 bitcoin, approximately 3.05 million USD to the Binance exchange on June 27, last Thursday.  The wallet is believed to belong to a bitcoin miner who likely earned the 50 bitcoin as mining rewards in 2010."



LEO:  This must make you cry.



STEVE:  Oh, I know, believe me, it's, like, you know.  Oh, gosh.  It hurts.



LEO:  Yeah.  



STEVE:  They said:  "On-chain analytics firm Lookonchain revealed the Bitcoin wallet's origins.  It's linked to a miner who received 50 bitcoin as a mining reward on July 14, 2010, just months after the Bitcoin network launched."  And I'll note that my podcast which Tom Merritt and I did, which was titled "Bitcoin Cryptocurrency," where I explained the operation of the entire Bitcoin cryptocurrency system, how the blockchain works and all that, that aired the following February 9th of 2011.



LEO:  Wow, we were really early on that.  Wow.



STEVE:  We were on the ball.



LEO:  Yeah.



STEVE:  So while it's true that solving the bitcoin hash problem way back then resulted in an award of 50 bitcoin, my 50 were different from the 50 that were recently moved.  The article continues:  "Back in 2010, one bitcoin" - oh, and this explains why I formatted my hard drive - "was valued at a mere $0.003, or 0.3 cents."  Right?  So, I mean, it was all just...



LEO:  A nickel's worth of bitcoin.  It wasn't worth worrying about, yeah.



STEVE:  Well, and remember the faucet, the bitcoin faucet was dripping out bitcoin that anybody could go get for free.



LEO:  Right?  Free, yeah.



STEVE:  Right.  So they said:  "This price was not surpassed until February 2011, reaching $30 by June of that year.  Today, Bitcoin trades around $61,000," which, they say, is a 17% drop from its all-time high in mid-March of this year of $73,750 per coin.



LEO:  Wow.



STEVE:  "Satoshi Bitcoin wallets," they write, "which were created during Bitcoin's infancy (2009-2011), hold historical significance.  This period marked the time when Bitcoin's enigmatic creator, Satoshi Nakamoto, was still an active presence in the cryptocurrency community.  The wallets' historical value, coupled with the limited transactions during that era, makes any movement of funds from them a notable event.



"In 2010, bitcoin mining was accessible to anyone with a personal computer, yielding a reward of 50 bitcoin.  This accessibility stands in stark contrast to the current bitcoin mining environment.  Four halving events," you know, as in cut in half, "have since reduced the block reward to a mere 3.125 bitcoin."  On the other hand, bitcoins are worth 60 grand, so not so mere.



LEO:  It gets harder to get a block, to make a block, though, too, because the math is much harder.



STEVE:  Oh, it's virtually impossible.  "These halvings, occurring roughly every four years, are integral to Bitcoin's deflationary model.  This recent transfer from a Satoshi Bitcoin wallet is not an isolated incident.  It joins a growing list of dormant wallets springing back to life."  And of course we know why they're springing.  It's because Bitcoin is jumping.  So, you know, multiplying the number of bitcoins which were easily earned back then by $60,000 will definitely put a spring in one's step.



LEO:  Yeah.



STEVE:  So they wrote:  "In March, a similar event occurred.  A miner transferred 50 bitcoin, earned from mining on April 25th, 2010, to Coinbase after 14 years of wallet inactivity.  The reactivation of these wallets often stirs interest and speculation within the cryptocurrency community.  Many are curious about the intentions behind these moves, whether they signal a change in market dynamics or simply represent a long-time holder finally deciding to liquidate their assets.



"Bitcoin Whales, individuals or entities holding vast quantities of bitcoin, possess the capacity to influence the cryptocurrency market through their sheer trading volume and holdings.  Two such Whale wallets, dormant for a decade, sprang to life on May 12th of 2024, transferring a combined 1,000 bitcoin.  On September 12th and 13th of 2013, when Bitcoin was trading at $1.24, each of these two wallets received 500 bitcoin, which was valued at $62K back then.



"In another noteworthy event on May 6th, a Bitcoin Whale moved $43.893 million worth of bitcoin to two wallet addresses.  This Whale had remained inactive for over 10 years, having initially received the bitcoin on January 12th, 2014, when it traded at $917."



LEO:  This is why it's hard, though, because had you had those 50 bitcoin, when it got to, say, worth $100,000, you would have for sure sold it.  You would have said, I'll take the money.  That's great.  I'm happy.



STEVE:  Right.  And that's why last week's podcast about when is a bad pseudorandom number generator...



LEO:  Right, a good thing.



STEVE:  It kept that guy from decrypting his wallet and selling his bitcoin until, you know, far later, when it became, you know, worth paying some hackers.  We don't know what percentage they took, but it would have been nothing if this guy hadn't had them crack his password.



LEO:  Many have offered to crack my password.  All have failed because it's probably a good password, and it's not - if it's a random password, it's not - and it's done well, which it was, you know, using Bitcoin wallet, it's virtually impossible to brute force.



STEVE:  Salted and memory hard and slow to do and so forth.



LEO:  But someday, you know, I figure this is just a forced savings account.  Someday those eight bitcoin will be mine.  I don't know, I'll guess the password.  Because I must have come up with something I know.  Why wouldn't I record it; right?



STEVE:  I'm sure I know mine.



LEO:  Yeah, that's what's sad, yeah.



STEVE:  But yes, back then we were not fully up to speed on generating passwords at random and having password managers hold onto them.



LEO:  Right, right.



STEVE:  So I could guess my own password.



LEO:  I've tried all of the dopey passwords I used by rote back in the day, and none of those worked.  So maybe I was smarter.



STEVE:  And did you rule out monkey123?



LEO:  I did, immediately.



STEVE:  Okay.



LEO:  That's the first one I tried.  Oh, well.  Those eight bitcoins are just going to sit there for a while.  That's interesting that 50 is enough to make a news story.  That's really...



STEVE:  Yes.  And Leo, there is so much bitcoin that has been lost.



LEO:  Oh, yeah.  Right.



STEVE:  I mean, so many people did this.



LEO:  Right.



STEVE:  I'm not a unique hard luck case at all.  And besides, I'm doing fine.  But a lot of people - and remember when we had some of our listeners come up to us in Boston when we were there for the Boston event.  There was one guy in particular who said, "Thank you for that podcast.  I retired a long time ago."



LEO:  Oh, my gosh.  Oh, my gosh.



STEVE:  "Thanks to listening to the Security Now! podcast.  What you said made a lot of sense.  I got going.  I mined a bunch of bitcoin, and I don't have to work anymore for the rest of my life."



LEO:  Wow.  That's just good luck.  Good fortune.



STEVE:  Well, yeah.



LEO:  Nice.



STEVE:  Okay.  So on the topic of astonishing achievements by mankind, and not cracking your password wallet, I wanted to share a brief update on the status of what has now become the Voyager 1 interstellar probe.  NASA's JPL wrote:  "NASA's Voyager 1 spacecraft is conducting normal science operations for the first time following a technical issue that arose back in November of 2023.  The team partially resolved the issue in April when they prompted the spacecraft to begin returning engineering data, which includes information about the health and status of the spacecraft.



"On May 19th, the mission team executed the second step of that repair process and beamed a command to the spacecraft to begin returning science data.  Two of the four science instruments returned to their normal operating modes immediately.  Two other instruments required some additional work.  But now all four are returning usable science data.



"The four instruments study plasma waves, magnetic fields, and particles.  Voyager 1 and Voyager 2 are the only spacecraft to directly sample interstellar space, which is the region outside the heliosphere, the protective bubble of magnetic fields and solar wind created by the Sun.



"While Voyager 1 is back to conducting science, additional minor work is needed to clean up the effects of the issue.  Among other tasks, engineers will resynchronize timekeeping software in the spacecraft's three onboard computers so they can execute commands at the right time.  The team will also perform maintenance on the digital tape recorder, which records some data for the plasma wave instrument which is sent to Earth twice per year.  Most of the Voyagers' science data is beamed directly to Earth, not recorded onboard.



"Voyager 1 now is more than 15 billion miles (24 billion kilometers) from Earth, and Voyager 2 is more than 12 billion miles (20 billion kilometers) from us.  The probe will mark 47 years of operations - 47 years of operations - later this year.  They're NASA's longest-running and most-distant spacecraft."



LEO:  We were young men at the time.



STEVE:  Yes, Leo.



LEO:  Just children.



STEVE:  We thought we were going to be able to understand all this one day.  And, you know, there's more to understand now than there was then.



LEO:  Well, that's fun; you know?



STEVE:  Yeah.



LEO:  It's not like everything's a solved problem anymore.



STEVE:  Nope, we don't have to worry about that.



LEO:  No.



STEVE:  Speaking of solved problems, everything is going well with GRC's email system.  And I'm nearly finished with my work on it.  The work I'm finishing up is automation for sending the weekly Security Now! email.  So I'm able to do it before the podcast while being prevented from making any dumb errors, like forgetting to update the names of links and so forth.  I'm about a day or two away from being able to declare that that work is finished.  And I should mention, just shy of 5,000 listeners already have the email describing today's podcast with a thumbnail of the show notes that they can click on to get the full-size show notes, a link to the entire show notes text that you and I have, Leo, and then also a bullet-pointed summary of the things we're talking about.  So that's all working.



Last week's announcement that I had started sending out weekly podcast summaries generated renewed interest and questions from listeners, both via Twitter or forwarded to me through Sue and Greg.  And these were listeners who had apparently been waiting for the news that something was actually being sent before deciding to subscribe to these weekly summary mailings.  So now they wanted to know how to do that.  All anyone needs to know is that at the top of every page at GRC is a shiny new white envelope labeled "Email subscriptions."  Just click that to begin the process.  If you follow the instructions presented at each step, a minute or two later you'll be subscribed.



And remember that, if your desire is not to subscribe to any of the lists, but to be able to bypass social media to send email directly to me, you're welcome to leave all of the subscription checkboxes unchecked when you press the "Update Subscriptions" button.  That will serve to confirm your email address, which then allows you to send feedback email, Pictures of the Week, suggestions, and whatever else you'd like directly to me by just writing to securitynow@grc.com.



Finally, I wanted to note that the email today's subscribers have already received from me was 100% unmonitored, as I expect all future email will be.  So I won't know whether those emails are opened or not.  I've also removed all of the link redirections from GRC's email so that clicks are also no longer being counted.  This makes the mailings completely blind, but it also makes for cleaner and clearer email.  Some of our listeners, as I mentioned last week, were objecting to their clients warning them about being tracked - even though I still don't think that's a fair use of a loaded term when the email has been solicited by the user, and if the notification only comes back to me.



I would never have bothered, frankly, to put any of that in if I'd written the system myself from scratch.  But it was all built into the bulk mailing system I purchased.  And it is so slick, and it has such lovely graphical displays with pie charts and bar charts and flow charts that it's just - it was so much fun to look at, you know, while it was new.  And, frankly, I didn't anticipate the level of backlash that doing this would produce.  But then this is not your average crowd, is it.  So, you know, we're all Security Now! listeners.



LEO:  And, by the way, the average crowd probably knows this, but I will reiterate this.  You could go get this PHP program yourself, but the chances are your Internet service provider would immediately block it.  You have some sort of special relationship with Level 3 or somebody that allows you to send 5,000 emails out at once.  No other Internet service provider would allow that.



STEVE:  Well, no consumer ISP; right?



LEO:  Right, right.



STEVE:  So anybody who has - any of our people in corporations who have a regular connection to the Internet, you know, not through Cox or through, you know, any of the consumer ISPs.



But anyway, the first two mailings I've done so far, which did contain link monitoring, provided some interesting feedback.  For example, three times more people clicked to view the full-size Picture of the Week than clicked to view the show notes.  Now, in retrospect that makes sense; right?



LEO:  Yes, yes.



STEVE:  Because most people will be listening to the podcast audio, but they're still curious to see the Picture of the Week, which we have fun describing each week.  In any event, I'm over it now.  No more single-pixel fetches with its attendant email client freakout, or anything else that might be controversial.  What you do with any email you receive from me is entirely up to you.  I'm just grateful for everyone's interest.



LEO:  There's also an issue with those invisible pixels.  Most good email clients, certainly all the ones I use, don't ever load them.  They know they're there.  They don't warn me.  I don't get a warning.  They just go, yeah.



STEVE:  Right.  Well, a lot of our listeners do.



LEO:  And apparently they do.  That's probably Outlook.



STEVE:  Yeah.



LEO:  But, you know, most email clients just go, yeah, sure, yeah.



STEVE:  Well, anyways, that's all gone.  Now, one thing I've been waiting to do, and I've been waiting until I knew I could, was to give a shout-out to the emailing system I chose to use.  I've been utterly and totally impressed by its design, its complete feature set, its maturity, and the author's support of his system.  And I have to say I feel somewhat embarrassed over what I've received in return for a one-time purchase payment of $169.  This thing is worth far more than that.



Now, because I'm me, I insisted upon writing my own subscription management frontend, although I have to say this package's author, a Greek guy whose first name is Panos, and I can't even begin to pronounce his last name because it's about 12 inches long, he has no idea why I've done my own subscription management frontend.  He thinks I'm totally nuts because his system, as delivered, does all of that, too.  But as Frank Sinatra famously said, "I did it my way."  I wanted to, you know, have it look like GRC's pages that our users interacted with.



So nuevoMailer, which is spelled N-U-E-V-O-M-A-I-L-E-R, is an open-source PHP-based email marketing management and mailing solution.  It runs beautifully under Windows, Unix, or Linux.  To help anyone who might have any need to create an email facility for their organization or their company or whatever from scratch, or replace one that you're not happy with, I made it this episode's GRC shortcut of the week.  So grc.sc/981 will bounce you over to www.nuevomailer.com.



I've had numerous back and forth dialogues with Panos because I've been needing to customize some of the RESTful APIs which his package publishes.  I've actually expended his API for my own needs.  But, for example, a new feature that's present in the email everyone received from me today for the first time  provides a direct link back to everyone's own email subscription management page.  So you can click it and immediately be looking at all of the lists and add or remove yourself.  To do that I needed to modify some of his code.  So I can vouch for the support he provides.



And as I've said, I felt somewhat guilty about paying so little when I've received so much.  I mean, this is GRC's email system moving forward forever.  So, you know, I'm aware that telling this podcast's listeners about his work, I hope, will likely help him.  All I can say is that he deserves every penny he makes.  There are thousands, literally thousands of bulk mailing solutions out in the world.  This one allows you essentially to roll your own, and I'm very glad I chose it.



LEO:  Most people will use something like, what is it, Chimp Mail and/or [crosstalk].



STEVE:  Mailchimp.



LEO:  Mailchimp.  Or Constant Contact because they do the mailing, and they've, you know, arranged with whoever's doing their mailing to send out tens of thousands of emails at once.  But, yeah, most consumer ISPs won't let you mail anything like that at all.



STEVE:  No, no, no.  In fact, they block port 25, which is SMTP.



LEO:  Right.  He has a very limited - basically he has a very limited set of possible customers.  So you should use it if you can, yeah.  Absolutely.



STEVE:  Okay.  A bit of errata, and then we're going to take our next break.  Last week's podcast drew heavily on two articles written by Kim Zetter.  It's embarrassing that I've been reading, appreciating, and sharing Kim's writing for years, but never stopped to wonder whether Kim would probably associate with the pronoun "he" or "she."  Her quite attractive Wikipedia photo strongly suggests that she would opt for "she" - as will I from now on.



LEO:  Did you call her "him" last week?



STEVE:  I think I must have because somebody, you know, said, "Hey, Gibson."



LEO:  She's a she, yeah.



STEVE:  Yeah.  What are you talking about here?



LEO:  Got to get the pronouns right these days.



STEVE:  That's right.



LEO:  I want to hear what you have to say about Syncthing because I still use it like crazy, and I'm worried now that there's something I should be worried about.  But that's, after all, why we listen to the show, isn't it.  On we go with the show, Mr. G.



STEVE:  So I wanted to note that while I'm still a big fan of Syncthing, lately I had been noticing a great deal of slowdown in its synchronization relay servers.  I don't think they used to be so slow.  I'm unable to get more than 1.5 to 1.8 megabits of traffic through them.  While it's not possible to obtain a direct end-to-end - or I should say when it's not possible to obtain a direct end-to-end connection between Syncthing endpoints, an external third-party relay server is required to handle their transit traffic.  Everything is super-well encrypted, so that's not the issue.  The issue is the performance of this solution.



Since this problem has persisted, or was persisting for me for several weeks, my assumption is that Syncthing's popularity has been growing, and actually we know it has, and is loading down their relay server infrastructure.  Which, after all, they just provide for free.  No one's paying anything for this.  At one point in the past I had arranged for point-to-point connections between my two locations.  But some network reconfiguration had broken that.  My daytime work location has a machine that runs 24/7.  But I shut down my evening location machine at the end of every evening's work.  The trouble was that synchronization to that always-on machine had become so slow that I was needing to leave my evening machine running unattended for several hours after I stopped working on it, waiting for my evening's work to trickle out and be synchronized with the machine I'd be using the next morning.



I finally became so - this problem finally became so intolerable that I sat down and punched remote IP filtered holes through my firewalls at each endpoint.  Even if pfSense's firewall rules were not able to track public domain names as they are, the public IPs of our cable modems, for example, change so rarely that even statically opening an incoming port to a specific remote public IP is practical.  Once I punched those holes, Syncthing was able to make a direct point-to-point connection once again, and my synchronization is virtually instantaneous.



So I just wanted to give a heads-up to anyone who may be seeing the same dramatic slowdown that I was seeing with the use of their relay server infrastructure.  It is an amazingly useful free service.  And, frankly, helping it to establish direct connections between endpoints also helps to keep the relay servers free, freed up for those who really need them.  So that was the issue, Leo, was just the use of a third relay server had recently really ground to a near halt.



LEO:  Yeah, I haven't noticed it, but I don't have - you have a much more complicated setup than I do.



STEVE:  Yeah, I've got like double NAT and all kinds of other crazy stuff going on that really make it a little extra difficult.  But for what it's worth, I guess my point is it's worth taking the time, if you are not seeing a direct WAN, they call it a WAN connection in the UI, with the IP of your remote node, instead you see some mention of relay servers, well, you probably already know how slow things are going.



LEO:  Right, right.



STEVE:  The point is it's worth taking the time to resolve that.  And then syncing is just instantaneous.



LEO:  Yeah.



STEVE:  Matt St. Clair Bishop wrote, saying:  "Hello, Steve.  I've been a listener of Security Now! for some years now.  However, as I've edged closer to making my own utilities publicly available, my mind has turned to my method of updating them.  I think in my dim and distant memory I remember you saying that you used a simple DNS record to hold the latest edition of each of your public releases, and the client software inspects that record, it being a very simple and efficient mechanism to flag available updates.  Could you elaborate at all if you have a spare section in your podcast?  I'm personally using C# and the .NET framework as I'm a Windows-only guy.  So if you could paint the broad strokes, I should be able to Google the C# detail.  SpinRite user, loving all your efforts in this field.  Matt St. Clair Bishop."



Okay.  So Matt's correct about my use of DNS, and I am pleased with the way that capability has turned out.  Anyone who has the ability to look up the IP address, for example, for validrive.rel.grc.com will find that it returns 239.0.0.1.  This is because ValiDrive is still at its first release.  When I've released an update to ValiDrive, it will be release number two, and I'll change the IP address of ValiDrive.rel, as in release, .grc.com to 239.0.0.2.



Whenever an instance of ValiDrive is launched by any user anywhere in the world, it performs a quick DNS lookup of its own product name "validrive.rel.dns.com" and verifies that the release number returned in the lower byte of the IP address is not higher than its own current release number.  If it is, it will notify its user that a newer release exists.  What's convenient about this, I mean, there are many things about it, you know, there's no massive flood of queries coming in all over the Internet.  It also provides all of its users the anonymity of making a DNS query, as opposed to coming back to GRC, so there's that, too.  But this version checking is performed by a simple DNS query packet, and that DNS is distributed and caching.



So it's possible to set a very long cache expiration to allow the cached knowledge of the most recent version of ValiDrive to be spread out across the Internet, varying widely with when each cache expires.  This means that when the release number is incremented, the notifications of this event will also be widely distributed in time as those local caches expire.  This prevents everyone on the Internet from coming back all at once to get the latest version.  And, you know, typically it's not a matter of any urgency.



And to Matt's question and point, I've never encountered a language that did not provide some relatively simple means for making a DNS query.  I know that C# and .NET make this trivial.  So anyway, that's the story on that.



Oh, and I should mention that 239 is obviously a huge block of IPs which have been set aside.  That's the high end of the multicast address space.  But the 239 block specifically is non-routable.  So those IPs will never and can never go anywhere.  So that's why I chose 239 as the first byte of the IP in the DNS for my release management.



A listener of ours named Carl sent email to me at securitynow@grc.com.  He said:  "Hi, Steve.  Much has been discussed over the recent weeks on your podcast about the upcoming Windows Recall feature and its value proposition versus security and privacy concerns.  It has been suggested that the concept started as a productivity assistant that uses AI to index and catalog everything on your screen, and may be more applicable in an office environment than at home.



"However, I think it was just as likely that this concept first started as a productivity monitoring tool, where corporate management can leverage Recall to ensure employees are using their screen time doing real, actual work.  Of course, Microsoft realizes they can't possibly market Recall this way, so here we are."  He said:  "I dread the day Recall is installed on my work computer.  Signed, Carl."



LEO:  Bad news, Carl.  Microsoft already has a product that does that called Viva.  They don't need another one.  They monitor you all the time, yeah.



STEVE:  Anyway, Carl's take on this, you know, it aligned with the Evil Empire theory, which as we know I don't subscribe to.  



LEO:  Right.



STEVE:  I would say that Recall itself is ethically neutral.  You know, it's like the discovery of the chain reaction in the fission of atomic nuclei.  That discovery can be used to generate needed power or to make a really big bomb.  But the chain reaction itself is just the physics of our universe.  Similarly, Recall is just a new capability which could be used to either help or to hurt people.  Could employers use it to scroll back through their employees' timeline to see what they've been doing on enterprise-owned machines?  That's not yet clear.  There are indications that Microsoft is working to make that impossible.  But we know that as it was first delivered it would have been entirely possible.



It appears that Microsoft desperately wants to bring Recall to their Windows desktops.  It would be incredibly valuable as training material for a local AI assistant and to deeply profile the desktop user as a means for driving advertising selection in a future ad-supported Windows platform.  So I suspect they will be doing anything and everything required to make it palatable.



LEO:  And as I said, they already have an enterprise product that does that.



STEVE:  Right, that is deployed, you know...



LEO:  In business, yeah.



STEVE:  Group policy, right.



LEO:  Yeah.



STEVE:  Right.  Okay.  So this week I want to share the story, and the backstory, of the web browser community again bidding a less than fond farewell to yet another certificate authority.  In, as we'll see, what appears to be, or as a result of, as we'll see, what appears to be a demonstration of executive arrogance, Entrust, one of the oldest original certificate authorities, after six years of being pushed, prodded, and encouraged to live up to the responsibilities that accompany the right to essentially print money by charging to encrypt the hash of a blob of bits, the rest of the industry that proactively monitors and manages the behavior of those who have been, dare I say, "entrusted" to do this responsibly, finally reached its limit, and Google announced last Thursday that Chrome would be curtailing its trust of Entrust from its browser's root store.



Okay.  So signing and managing certificates is by no means rocket science.  There's nothing mysterious or particularly challenging about doing it.  It's mostly a clerical activity which must follow a bunch of very clearly spelled out rules about how certificates are formatted and formulated and what information they must contain.  These rules govern how the certificates must be managed and what actions those who sign them on behalf of their customers must do when problems arise.  And just as significantly, the rules are arrived at and agreed upon collectively.



The entire process is a somewhat amazing model of self-=governance.  Everyone gets a say, everyone gets a vote, the rules are adjusted in response to the changing conditions in our changing world, and everyone moves forward under the updated guidance.  This means that when someone in this collective misbehaves, they're not pushing back against something that was imposed upon them.  They are ignoring the rules that they voted to change and agreed to follow.



"Certificates" have been an early and enduring focus and topic on this podcast because so much of today's security is dependent upon knowing that the entity one is connecting to and negotiating with over the Internet really is who we think they are, and not any form of spoofed forgery.  The idea behind a certificate authority is that while we may have no way of directly confirming the identity of an entity we don't know across the Internet, if that entity can provide proof that they have previously, and somewhat recently, proven their identity to a third party, a certificate authority whose identity assertions we do trust, then by extension we can trust that the unknown party is who they say they are when they present a certificate to that effect signed by an authority whom we trust.



That's all this whole certificate thing is about.  It's beautiful and elegant in its simplicity.  But as the saying goes, "The devil is in the details."  And we're going to see today, those who understand the importance of those details can be pretty humorless when they are not only ignored, but flaunted.



The critical key here is that we are completely and solely relying upon a certificate authority's identity assertions, where any failure in such an authority's rigorous verification of the identity of their client customers could have truly widespread and devastating consequences.  This is one of the reasons I've always been so impressed with the extreme patience shown by the governing parties of this industry in the face of certificate authority misbehavior.  Through the years we've seen many examples where a certificate authority that's trusted really needs to screw up over a period of years, and actively resist improving their game, in order to finally have the industry lower the boom on them.  No one wants to do this indiscriminately or casually because it unilaterally puts the wayward CA, the certificate authority, out of the very profitable browser certificating issuing business overnight.



Okay.  So what happened?  In a remarkable show of prescience, when things were only just heating up, FeistyDuck's "Cryptography & Security Newsletter" posted the following only a few hours before Google finally lowered the boom on Entrust.  FeistyDuck wrote:  "Entrust, one of the oldest Certification Authorities, is in trouble with Mozilla and other root stores.  In the last several years, going back to 2020, there have been multiple persistent technical problems with Entrust's certificates.  That's not a big deal when it happens once, or even a couple of times, and when it's handled well.  But according to Mozilla and others, it has not been.  Over time, frustration grew.  Entrust made promises which it then broke.  Finally, in May, Mozilla compiled a list of recent issues and asked Entrust to please formally respond.



"Entrust's first response did not go down well, being non-responsive and lacking sufficient detail.  Sensing trouble, it later provided another response, with more information.  We haven't seen a response back from Mozilla, just ones from various other unhappy members of the community.  It's clear that Entrust's case has reached a critical mass of unhappiness."  And that's really interesting because this is really the point.  All it takes is a critical mass of unhappiness because, as I said, four hours after this was posted, Entrust lost Google.  And that's losing the game, essentially, if you're selling certificates for browsers.  



So they said:  "We haven't heard from other root stores yet.  However, at the recent CA/Browser forum meeting, also in May, Google used the opportunity to discuss standards for CA incident response.  It's not clear if it's just a coincidence, but Google's presentation uses pretty strong words that sound like a serious warning to Entrust and all other CAs to improve, or else.  Looking at the incidents themselves, they're mostly small technical problems of the kind that could have been avoided with standardized validation of certificates just prior to issuance."



And I'll note later that I'll use the term "lint."  "Lint" is well understood in the developer community.  It means just running a certificate through a lint filter to make sure that there isn't any lint, any debris, any obviously, like, a date set to an impossible number.  Or, you know, something obviously missing that the standard says should be there.  You know, just do it.  But that doesn't happen.



They said:  "As it happens, Ballot SC-75 focuses on preissuance certificate linting.  If this ballot passes, linting will become mandatory as of March 2025."  Meaning it's not there yet.  But, boy, after March we're going to see some more booms lowered if people don't lint by default.  And that means people are going to have to spend some time and spend some money upgrading their certificate issuing infrastructures.  They have not been bothering.  Anyway, they said:  "It's a good first step.  Perhaps the CA/B Forum," you know, the CA/Browser Forum, "will in the future consider encoding the Baseline Requirements into a series of linting rules that can be applied programmatically to always ensure future compliance."



Okay, now, as I noted, a few hours after FeistyDuck posted this, Google made their announcement.  Last Thursday, June 27th, the Chrome Root Program and Chrome Security Team posted the following in Google's Security Blog under the title "Sustaining Digital Certificate Security - Entrust Certificate Distrust."  And Leo, after taking our final break, I will share what Google wrote and the logic and basically the preamble that led up to this.



LEO:  Yeah.  As you say, you lose Google, you've pretty much lost the game.



STEVE:  Game over.



LEO:  Game over, man.  Back to the saga of Entrust.



STEVE:  So Google wrote:  "The Chrome Security Team prioritizes the security and privacy of Chrome's users, and we are unwilling to compromise on these values.  The Chrome Root Program states that CA certificates included in the Chrome Root Store must provide value to Chrome end users that exceeds the risk of their continued inclusion."  You should hear a drumbeat in the background here.  "It also describes many of the factors we consider significant when CA owners disclose and respond to incidents.  When things don't go right, we expect CA owners to commit to meaningful and demonstrable change, resulting in evidenced continuous improvement.



"Over the past few years, publicly disclosed incident reports highlighted a pattern of concerning behavior by Entrust that falls short of the above expectations and has eroded confidence in their competence, reliability, and integrity as a publicly-trusted CA owner.  In response to the above concerns and to preserve the integrity of the Web PKI ecosystem, Chrome will take the following actions.



"In Chrome 127 and higher, TLS server authentication certificates validating to the following Entrust roots whose earliest Signed Certificate Timestamp is dated after October 31st, 2024, will no longer be trusted by default."  Okay.  Then in Chrome's posting they enumerate the exact nine root certificates that Chrome has, until now, trusted to be valid signers of the TLS certificates that remote web servers present to their Chrome browser.



They continue, writing:  "TLS server authentication certificates validating to the above set of roots whose earliest Signed Certificate Timestamp is on or before October 31st, 2024, will not be affected by this change.  This approach attempts to minimize disruption to existing subscribers using a recently announced Chrome feature to remove default trust based on the SCTs" - that's the Signed Certificate Timestamp, the signing date - "in certificates.  Additionally, should a Chrome user or enterprise explicitly trust any of the above certificates on a platform and version of Chrome relying on the Chrome Root Store, the SCT-based constraints described above will be overridden, and certificates will function as they do today.  To further minimize risk of disruption, website owners are encouraged to review the Frequently Asked Questions listed below."



Okay.  So now - okay.  If Chrome were to yank, just summarily yank all nine of those Entrust certs from their root store, at that instant any web servers that were using Entrust TLS certificates would generate those very scary "untrusted certificate warnings" that sometimes we see when someone allows their certificate to expire by mistake.  And that makes it quite difficult to use your browser, and most users just say, whoa, I don't know what this red flashing neon thing is, but it's very scary.  And if you want to see that, you can go right now to untrusted-root.badssl.com.  And there what you will get is a deliberately untrusted certificate so you can see what your browser does:  untrusted-root.badssl.com.



Okay.  Instead of doing that, Chrome is now able to keep those, I guess I would call them semi-trusted or time-base trusted root certificates in their root store in order to continue trusting any certificates Entrust previously signed and will sign during the next four months, July, August, September, and October, Halloween being the end of that.  No Entrust certificate signed from November on will be accepted by Chrome.  So that's good.  That allows Entrust four months to wind down their services, to decide maybe make a deal with some other CA to, like, purchase their existing customers and transfer them over.  I would imagine that's what they'll do.



But there could be no question that this will be a devastating blow for Entrust.  Not only will this shut down completely their TLS certificate business, but CAs obtain a great deal of additional revenue by providing their customers with many related services.  Entrust will lose all of that, too.



And of course there's the significant reputational damage that accompanies this which, you know, makes a bit of a mockery of their own name.  And there's really nothing they can say or do at this point.  The system of revoking CA trust operates with such care to give misbehaving CAs every opportunity to fix their troubles that any CA must be flagrant in their misbehavior for this to occur.  As long-time listeners of this podcast know, I'm not of the belief that firing someone who missteps always makes sense.  Mistakes happen, and valuable lessons can be learned.  But from what I've seen, and what I'm going to share, I'll be surprised if this is a survivable event for Entrust's Director of Certificate Services, a guy named Bruce Morton.



Way back in 1994, Entrust built and sold the first commercially available public key infrastructure.  They started all this.  Five years later, in 1999, they entered the public SSL market by chaining to the Thawte Root and created Entrust.net.  And as I said, their name has been around forever.  You know, I've seen it when I've looked at lists of certificates.  There's Entrust.  Ten years later, Entrust was acquired for $124 million by Thoma Bravo, a U.S.-based private equity firm.



LEO:  Mm-hmm.  There you have it.



STEVE:  Uh-huh.



LEO:  In a nutshell.  Add this one to the list.  Wow.



STEVE:  Uh-huh.  I don't know, and I'm not saying, whether being owned by private equity may have contributed to their behavior and their downfall.  But, if so, they would have that in common with LastPass.



LEO:  Yeah.



STEVE:  As you said, Leo...



LEO:  And Red Lobster.  And about a million other companies in the United States in the last 10 years that have been bought by private equity and then drained of their resources for money.  It's sad.



STEVE:  Yup.  Google, in their FAQ, answering the question "Why is Chrome taking action?," replied:  "Certification Authorities serve a privileged and trusted role on the Internet that underpin encrypted connections between browsers and websites.  With this tremendous responsibility comes an expectation of adhering to reasonable and consensus-driven security and compliance expectations, including those defined by the CA/Browser TLS Baseline Requirements.



"Over the past six years, we have observed a pattern of compliance failures, unmet improvement commitments, and the absence of tangible, measurable progress in response to publicly disclosed incident reports.  When these factors are considered in aggregate and considered against the inherent risk each publicly-trusted CA poses to the Internet ecosystem, it is our opinion that Chrome's continued trust in Entrust is no longer justified."



And, okay, this makes a key point:  It's not any one thing that Entrust did, taken in isolation, that resulted in this loss of trust.  The loss of trust resulted from multiple years of demonstrated uncaring about following the rules that they had voted upon and agreed to as a member of this group.  No one wants to make Entrust an example.  Too many lives will be negatively impacted by this decision.  But the entire system only functions when everyone follows the rules they've agreed to.  Entrust refused to do that, so they had to go.  Let's take a look at some specifics.



For example, a few months ago, following an alert from Google's Ryan Dickson, Entrust discovered that all of its EV certificates issued since the implementation of changes due to Ballot SC-62v2, which amounted to approximately 26,668 certificates, were missing their CPS URIs, in violation of the EV Guidelines.  Entrust said this was due to discrepancies and misinterpretations between the CA/Browser Forum's TLS Baseline Requirements and the Extended Validation Guidelines.  Entrust chose to not stop issuing the EV certificates - that's a violation of the rules - and did not begin the process of revoking the mis-issued certificates - that's another violation.



Instead, they argued that the absence of the CPS URI in their EV certificates was due to ambiguities in CA/B Forum requirements, which was not the case.  They said that the absence of the CPS URI had no security impact - that's arguably true - and that halting and revoking the certificates would negatively impact customers and the broader Web PKI ecosystem.  In other words, they thought they were bigger than the rules, that the rules were dumb, or that the rules didn't apply to them.  Everyone else has to follow them, but not them.  Entrust then also proposed a ballot to adjust the EV Guidelines so that they would not be out of compliance, to not require the CPS URI.  They also argued that their efforts were better spent focusing on improving automation and handling of certificates, rather than on revocation and reissuance.  Wow.



Okay, now, the CPS URI is truly incidental.  CPS stands for Certification Practice Statement, and EV certs are now supposed to contain a CPS URI link pointing to the CA's issuing document.  So is leaving that out a big deal?  Probably not from a security standpoint.  But it's worrisome when a CA intentionally defies the standards that everyone has agreed to follow, and then argues about them, and is deliberately, knowingly, in misissuance.



A security and software engineer by the name of Amir Omidi has worked on maintaining certificate issuance systems at Let's Encrypt and Google Trust Services, and he's very active in the PKI space.  His GitHub account contains 262 repositories, and it appears that he's currently working on a project named "boulder" which is an ACME-based certificate authority, written in Go.  And before that was "zlint," an X.509 Certificate Linter focused on Web PKI standards and requirements.



Yesterday, just Monday, yesterday he posted a terrific summary of the way the Public Key Infrastructure industry thinks about these things.  He wrote:  "Entrust did not have one big explosive incident.  The current focus on Entrust started with this incident.  On its surface, this incident was a simple misunderstanding.  This incident happened because up until the SC-62v2 ballot, the CPS URI field in the certificate policy extension was allowed to appear on certificates.  This ballot changed the rules and made this field be considered 'not recommended.'  However, this ballot only changed the baseline requirements and did not make any stipulation on how Extended Validation certificates must be formed.  The EV guidelines still contained rules requiring the CPS URI extension.



"When a CA," writes Amir, "when a CA has an incident like this, the response is simple:  Stop misissuance immediately.  Fix the certificate profile so you can resume issuance.  In parallel, figure out how you ended up missing this rule and what the root cause of missing this rule was.  Revoke the misissued certificates within 120 hours of learning about the incident.  Provide action items that a reasonable person would read and agree that these actions would prevent an incident like this happening again."  In other words, this is all understood.  Entrust ignored it.



He writes:  "When I asked Entrust if they've stopped issuances yet, they said they haven't, and they don't plan to stop issuance.  This is where Entrust decided to go from an accidental incident to willful misissuance.  This distinction is an important one," he says.  "Entrust had started knowingly misissuing certificates.  Entrust received a lot of pushback from the community over this.  This is a line that a CA shouldn't, under any circumstances, cross.  Entrust continued to simply not give a crap" - and I changed that word to be a little more politically correct - "even after Ben Wilson of the Mozilla Root Program chimed in and said that what Entrust is doing is not acceptable."  And then he writes:  "Entrust only started taking action after Ryan Dickson of the Google Chrome Root Program also chimed in to say this is unacceptable."



I'll interrupt to mention that this is an important distinction.  The executives at Entrust appeared not to care about any of this until Google weighed in with the power of their Chrome browser.  That was a monumental mistake, and it demonstrated a fundamental misunderstanding of the way the CA/Browser forum members operate.  None of this operates on the basis of market power.  It's only about agreeing to and then following the rules. It's not about, "Oh yeah?  Make me!"  We're not in the schoolyard anymore.



Amir continues:  "Entrust's delayed response to the initial incident, spanning over a week, compounded the problem by creating a secondary 'failure to revoke on time' incident.  As these issues unfolded, a flurry of questions arose from the community.  Entrust's responses were often evasive or minimal, further exacerbating the situation.  This pattern of behavior proved increasingly frustrating, prompting me to delve deeper into Entrust's past performance and prior commitments.



"In one of my earlier posts, I found that Entrust had made the promise that, 'One, we will not make the decision not to revoke,' which they just had.  'We will plan to revoke within 24 hours or five days as applicable for the incident,' which they've said they won't.  'We will provide notice to our customers of our obligations to revoke and recommend action within 24 hours or five days based on the Baseline Requirements,' which they won't do because they're not going to revoke in the first place."  He says:  "This pattern of behavior led to a troubling cycle:  Entrust making promises, breaking them, and then making new promises, only to break those, as well.



"As this unfolded, Entrust and the community uncovered an alarming number of operational mistakes, culminating in a record 18 incidents within just four months.  Notably, about half of these incidents involved Entrust offering various excuses for failing to meet the 120-hour certificate revocation deadline - ironically, a requirement they had voted to implement themselves."  He said:  "I do want to highlight that the number of incidents is not necessarily an indication of CA quality.  The worst CA is the CA that has no incidents, as it's generally indicative that they're either not self-reporting, or not even aware that they're misissuing."



Okay.  So in other words, mistakes happen.  Everyone understands that.  No one needs to be perfect here.  But it's how the mistakes that are discovered are then handled that demonstrates the trustworthiness of the CA.



Amir said:  "Due to the sheer number of incidents, and Entrust's poor responses up until this point, Mozilla then asks Entrust to provide a detailed report of these recent incidents.  Mozilla specifically asks Entrust to provide information regarding" - and we have some bullet points.  "The factors and root causes that led to the initial incidents, including commonalities among the incidents, and any systemic failures."  Okay, now, listen to this, I mean, because this is really Mozilla getting up in Entrust business.  And Entrust apparently doesn't take kindly to that.



Okay.  So literally, Mozilla confronts Entrust and says we want to know the factors and root causes that led to the initial incidents, highlighting their commonalities among the incidents and any systemic failures.  We want to know Entrust's initial incident handling and decision-making in response to these incidents, including any internal policies or protocols used by Entrust to guide their response and an evaluation of whether their decisions and overall response complied with Entrust's policies.  We want your practice statement, and the requirements of the Mozilla Root Program.



In other words, explain to us, and we're not kidding here, how this happened.  Like, are you ignoring our own policies, or are these your policies?  In other words, WTF?  And we want it in detail, please.  We need also - I mean, literally, this is in his letter - a detailed timeline of the remediation process and an apportionment of delays to root causes.  So please, you know, elaborate on the delays which were involved in this because, you know, we're out here.  We don't understand.  Also, an evaluation of how these recent issues compare to the historical issues referenced above and Entrust's compliance with its previously stated commitments, which everyone already knows is missing.



"Mozilla also asked," writes Amir, "that the proposals meet the following requirements."  So literally, these are what we need to know, and here are the requirements you must meet in your reply.  "We want clear and concrete steps that Entrust proposes to take to address the root causes of these incidents and delayed remediation.  We want measurable and objective criteria for Mozilla and the community to evaluate Entrust's progress in deploying these solutions.  And we want a timeline for which Entrust will commit to meeting these criteria."



As Amir said, even here, he said:  "Mozilla gave Entrust a one-month deadline to complete this report.  Mozilla's email served a dual purpose.  It was both a warning to Entrust and an olive branch, offering a path back to proper compliance.  This presented Entrust with a significant opportunity.  They could have used this moment to demonstrate to the world their understanding that CA rules are crucial for maintaining Internet security and safety, and that adhering to these rules is a fundamental responsibility.  Moreover, Entrust could have seized this chance to address the community, explaining any misunderstandings in the initial assessment of these incidents and outlining a concrete plan to avoid future revocation delays.



"Unfortunately, Entrust totally dropped the ball on this.  Their first report was a rehash of what was already on Bugzilla, offering nothing new.  Unsurprisingly, this prompted a flood of questions from the community.  Entrust's response?  They decided to take another crack at it with a second report.  They submitted this new report a full two weeks after the initial deadline.



"In their second report, Entrust significantly changed their tone, adopting a more apologetic stance regarding the incidents.  However, this shift in rhetoric was not matched by their actions.  While expressing regret, Entrust was still overlooking certain incidents, delaying the revocations of existing misissuances, and failing to provide concrete plans to prevent future delayed revocations.  An analysis of these 18 incidents and Entrust's responses serves as a prime example of mishandled public communications during a crisis."



Okay, now, stepping back from this for a moment, the only way to really read and understand this is that the executives at Entrust - and yes, Leo, a public equity owned firm.



LEO:  A private equity, yeah...



STEVE:  A private equity, sorry, private equity...



LEO:  Thoma Bravo.



STEVE:  ...owned firm, yeah, the executives at Entrust didn't really take any of this seriously.  They acted as though they were annoyed by the gnats buzzing around them who were telling them how they should act and what they should do.



Amir says:  "The consensus among many community members is that Entrust will always prioritize their certificate subscribers over their obligations as a Certificate Authority."  And there it is in a single sentence.  "The consensus among many community members is that Entrust will always prioritize their certificate subscribers over their obligations as a Certificate Authority."



And he said:  "This practice fundamentally undermines Internet security for everyone.  Left unchecked, it creates a dangerous financial incentive for other CAs to ignore rules when convenient, simply to avoid the uncomfortable task of explaining to subscribers why their certificates need replacement.  Naturally, customers prefer CAs that won't disrupt their operations during a certificate's lifetime.  However, for CAs that properly adhere to the rules, this is an impossible guarantee to make."  In other words, no one should expect CAs to be perfect.  The community here doesn't.  They understand mistakes will happen.  But it's maintaining the integrity of the system is more important than anything else.



He says:  "Furthermore, these incidents were not new to Entrust. As I've covered in earlier posts, Entrust has continuously demonstrated that they're unable to complete a mass revocation event in the 120 hours defined by and required by the baseline requirements.  This pattern of behavior suggests a systemic issue rather than isolated incidents.  Despite there being over a dozen root programs, there are only four that are existentially important for a Certificate Authority:  The Mozilla Root Program, used by Firefox and practically all Linux distribution and FOSS software.  The Chrome Root Program, used by Chrome - the Browser and the OS - and some Androids.  The Apple Root Program, used by everything Apple.  And the Microsoft Root Program used by everything Microsoft."  He finishes:  "Enforcement over the operational rules of a CA has been a struggle in the past.  A root program only has a binary choice, to either trust or distrust a certificate authority.



Now, there's one last much shorter piece of interaction that I want to share.  It was written by Watson Ladd, who studied math at Berkeley and is presently a principal software engineer at Akamai.  Among his other accomplishments, he's the author of RFC 9382 which specifies SPAKE2, a password-authenticated key exchange system, and his name has been on about six other RFCs.  So, you know, he's a techie, and he's in the game.



In the public discussion thread about Entrust's repeated and continuing failings to correct their mistakes and live up to the commitments they had made to the CA/Browser community, Watson publicly addressed a note to Bruce Morton, Entrust's Director of Certificate Services, who has been the face of Entrust's repeated failures, excuses, and defiance.



Watson Ladd wrote:  "Dear Bruce.  This report is completely unsatisfactory.  It starts by presuming that the problem is four incidents.  Entrust is always under an obligation to explain the root causes of incidents and what it is doing to avoid them as per the CCADB incident report guidelines.  That's not the reason Ben and the community need this report."  And here he's referring to Mozilla's Ben Wilson, who initially asked Entrust to explain how they would deal with these ongoing problems and demonstrate how they would be prevented in the future.  As we know, Entrust's Bruce Morton basically blew him off, apparently because he wasn't from Google.



Anyway, Watson says:  "That's not the reason Ben and the community need this report.  Rather, it's to go beyond the incident report to draw broader lessons and to say more to help us judge Entrust's continued ability to stay in the root store. The report falls short of what was asked for, in a way that makes me suspect that Entrust is organizationally incapable of reading a document, understanding it, and ensuring each of the clearly worded requests is followed."



LEO:  Wow.



STEVE:  Yeah.  The implications for being a CA are obvious.



LEO:  He's mad.  Holy cow.



STEVE:  They are, they all are.  He said:  "To start, Ben specifically asked for an analysis involving the historical run of issues and a comparison.  I don't see that in this report at all.  The list of incidents only has ones from 2024 listed.  There's no discussion of the two issues specifically listed by Ben in his message.



"Secondly, the remedial actions seem to be largely copy and pasted from incident to incident without a lot of explanation.  Saying the organizational structure will be changed to enhance support, governance, and resourcing really doesn't leave us with a lot of ability to judge success or explain how the changes made - sparse on details - will lead to improvements.  Similarly, process weaknesses are not really discussed in ways that make clear what happened.  How can I use this report if I was a different CA to examine my organization and see if I can do better?  How can we, as a community, judge the adequacy of the remedial actions in this report?



"Section 2.4 I find mystifying.  To my mind there's no inherent connection between a failure to update public information in a place where it appears, a delay in reconfiguring a responder, and a bug in the CRL generation process beyond the organizational.  These are three separate functions of rather different complexity.  If there's a similarity, it's between the latter two issues where there was a failure to notice a change in requirements that required action, but that's not what the report says.  Why were these three grouped together, and not others?  What's the common failure here that doesn't exist with the other incidents?



"If this is the best Entrust can do, why should we expect Entrust to be worthy of inclusion in the future?  To be clear, there are CAs that have come back from profound failures of governance and judgment.  But the first step in that process has been a full and honest accounting of what their failures have been, in a way that has helped others understand where the risks are and helps the community understand why they are trustworthy.  Sincerely, Watson Ladd."



LEO:  Watson was hopped up.  Doesn't sound like it, but that's what an engineer sounds like when they get really mad.



STEVE:  Well, now, Leo.



LEO:  Yes.



STEVE:  I don't know these Entrust guys at all.  But given the condescension they've exhibited, it's not difficult to picture them as some C Suite stuffed shirts who have no intention of being "judged by" and pushed around by a bunch of pencil-necked geeks.  But, boy, did they read this one wrong.  Those pencil-necked geeks with their pocket protectors reached consensus and pulled their plug, ejecting them from the web certificate CA business they had a hand in pioneering.



LEO:  This is what happens when people who only run businesses don't understand the difference between a business, a profit-seeking enterprise, and a public trust; right?



STEVE:  Yes.



LEO:  And they don't understand that in order to run your business, you have to satisfy the public trust part.  You can't just say, yeah, yeah, whatever.  You've got to respond.



STEVE:  And notice that Entrust was taken private.



LEO:  Yeah.



STEVE:  So they no longer had, literally, a public trust.



LEO:  Of their business, except that a certificate authority has a public trust, sorry.  That's the job.



STEVE:  Yes, it is a public trust.



LEO:  It's a public trust.



STEVE:  Yes.



LEO:  Wow.  Clearly they were in over their heads or something.



STEVE:  Well, but they started this business.  I mean...



LEO:  Is it the same people, though, really?



STEVE:  Well, that's exactly - that's a great question.



LEO:  Yeah.  It's like Joe Siegrist wasn't at LastPass at the end.



STEVE:  Yes, exactly, exactly.  So some middle managers rotated in and didn't understand that if they - that the aggravation that was simmering away in this CA/Browser Forum was actually capable of boiling over and ending their business.



LEO:  Yeah.  They didn't get it.  They really didn't get it.  And you know what?  You know, well, they know now.  Whoops.



STEVE:  Yeah.  It's over.  They appear to have believed that the rules they agreed to did not apply to them.  Or, you know, I thought maybe it was the extreme leniency that the industry had been showing them that led them to believe that their failures would never catch up with them.  And, boy, but the worst thing that they did was just to basically blow off the heads of these root programs when they said, hey, look, we see some problems here.  We need you to convince us that you're worthy of our trust.



LEO:  Yeah.



STEVE:  And the Entrust people probably just said "Eff you."  Well, now they're going to be out of business in four months.



LEO:  So without the trust of Chrome, and I presume other browsers, well, Mozilla's clearly going to...



STEVE:  Oh, Mozilla will be right on their heels, yes.



LEO:  And then Edge and everybody else who does certificates will follow; right?  But it doesn't matter.  If Chrome doesn't - if you're not in Chrome...



STEVE:  That's right.



LEO:  People using Chrome won't be able to go to sites that have  Entrust certificates.  Game over.  Right?



STEVE:  Yes.



LEO:  Yeah.



STEVE:  Yes.  Entrust is out of business.  The only thing they could do would be to sell their business.  Remember when Symantec screwed up royally?



LEO:  Oh, it happens all the time.



STEVE:  They ended up having to - they had to sell their business to DigiCert.



LEO:  Right.  Right.



STEVE:  Because, you know.



LEO:  So they also might face the wrath of people who use Chrome, rather websites that use their certificates when their customers can't get to them.  I mean, they might be some big companies.



STEVE:  Well, all the certs that have been issued stay valid.



LEO:  Okay.



STEVE:  That's the good thing.  So any site, even through Halloween, even through the end of October - because what Chrome is doing is they're looking at the signing date.



LEO:  Ah.



STEVE:  And they're saying anything Entrust signs after October 31st, 2024, we're not going to trust.



LEO:  Right.



STEVE:  We're going to take the user to the invalid certificate page.



LEO:  So there is a little bit of a warning going out to people who use Entrust certificates.  You're going to need a new certificate from a different company now.



STEVE:  Yes, yes.



LEO:  You know, by October.



STEVE:  Yes, yes.  You might as well switch.  You could buy an Entrust certificate up until Halloween.



LEO:  Get you through Halloween.



STEVE:  And it would stay good for the life of the certificate, which has now been reduced to one year.  What is it, 386 days or something.



LEO:  That's right, yeah, yeah.



STEVE:  So it's, you know, time to switch providers.  So Entrust loses all of that ongoing revenue from certificate renewals.  They also lose all of the second order business that their customers got.  You know, they're like, oh, well, we'll also sell you some of this and some of that.  And you probably need some of this over here.  I mean, they're in a world of hurt.  And, you know, unfortunately, it couldn't happen to a nicer group of guys because they entirely brought this on themselves.  It was literally their arrogance.



LEO:  Yeah.



STEVE:  You know, we are not going to do what you are asking us to do.



LEO:  Right.



STEVE:  And so the consensus was, okay.



LEO:  You don't have to, but we don't have to trust you.



STEVE:  We're not making you.  We're just going to stop believing you.



LEO:  Right.  That's really interesting.  Usually what happens with private equity is they buy a company and sell off assets or somehow make money.  The example of Red Lobster comes to mind.  The private equity company that bought the Red Lobster restaurant chain took the real estate that was owned by all the restaurants and sold it to a separate division, making almost enough money to compensate for the debt they'd incurred to buy it because that's what happens.  They borrow money.  Then they squeeze the company, get all the debt paid off, and then - but the problem was now the restaurants had to pay rent, and couldn't, and they went out of business.  And that's what happens.  You squeeze, get your money back, get your money out, and then you don't care what happens.



So I don't know what they were able to squeeze out of Entrust, but it may be they got what they wanted out of it, and they don't care at this point.  That's what it feels like.  They just don't care.  They could come back from this; right?  They could say, oh, yeah, wait a minute, oh, sorry, we were wrong.  Could they?  Or is it too late?



STEVE:  I don't think so.  I mean, maybe they'd change their name to Retrust.



LEO:  Wow.



STEVE:  No, I mean, it's over.  I mean, they can't, like, say oh, oh, oh, we're really sorry, we didn't understand.  I mean, I've never seen any of this ever reversed.  These are slow-moving icebergs.  And when the iceberg hits your ship, you know, it doesn't turn around.



LEO:  It rends you from stem to stern.



STEVE:  It does.



LEO:  OutofSync in our Discord says that Entrust is about 0.1% of all certs, compared to Let's Encrypt, which is now 53% of all certs.  Why not?  It's free; right?  And if you've got to renew it every 384 days, you might as well just go with Let's Encrypt.



STEVE:  Yeah, last year we talked about the big seven, where if you only trusted seven CAs, you've got 99.95 or something.  And Entrust is not one of them.



LEO:  I don't remember Entrust.  It wasn't on that list.  Okay.  So maybe this is just incompetence or something.



STEVE:  Well, it's - yeah.  That, I mean, you're incompetent if you're a manager who doesn't know how to read email and understand the importance of it to your career.  And I doubt this Bruce guy is going to have a job in four months.



LEO:  It won't be, let's put it this way, it will not be in the certificate business.



STEVE:  It cannot be in the certificate business.



LEO:  Steve Gibson, I love it.  This was a fun one.  A little scary early on with OpenSSH, but you kind of redeemed it with some humor towards the end.  What a story that is.  Amazing.  Steve's at GRC.com.  Now, if you go to GRC.com/email, you can sign up for his mailing lists.  You don't have to, though.  All it will do is validate your email so that you can then email him.  So it's the best way to be in touch with Steve.



If you're there, you should check out SpinRite.  Version 6.1 is out.  It is now easily the premier mass storage performance enhancer.  It's like Viagra for your hard drive.  No.  Performance enhancer, maintenance and recovery utility.



STEVE:  Well, you want your hard drive to be hard.



LEO:  You do.  You don't want a floppy drive.  We're not talking floppies here.  This is a hard drive.  Or an SSD.  Let's be fair, works on SSDs, as well.



STEVE:  Bye.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#982

DATE:		July 9, 2024

TITLE:		The Polyfill.io Attack

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-982.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What was Entrust's response to Google's decision to refuse trust of any of their TLS certificates signed after October 2024?  How have the other CAs responded to this new opportunity?  What's a Passkey Redaction Attack, and how worried should you be?  And speaking of Passkeys, why not just have each website hold as many as we need?  Wouldn't adding port knocking in front of the serious OpenSSH flaw we discussed last week prevent this problem?  And if so, what's the larger lesson to be learned?  And what about blocking an IP after some number of failed attempts?  And finally, once again the Internet dodged a potentially devastating bullet.  What happened, and what significant lesson should we take away?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, a response from Entrust to getting yanked from Google Chrome and other browsers.  We'll also talk about how you can protect yourself using port knocking from the OpenSSH flaw we talked about last week.  The biggest story, though, is the big bullet the Internet dodged, it could have been a bad one, the story of Polyfill.io.  All that and more coming up next with Steve on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 982, recorded Tuesday, July 9th, 2024:  The Polyfill.io Attack.  It's time for, oh, time to record a show.



STEVE GIBSON:  Oh, yes, the big red record button, please.



LEO:  We now have copies of the show on every streaming service, so the chance of losing a show is very low.  It's time for Security Now!, the show where we cover the latest security, privacy news; tell you a little bit about how computers work; sometimes talk about sci-fi with this guy right here, Mr. Steve Gibson.  What do you see?  What do you know?  It's time for the Steve Gibson Show.  How do you like it?  We're singing that.  No?



STEVE:  Maybe we should wait till year 20 before we deploy the corny jingle.



LEO:  We'll never get to 999 if you keep that up, Leo.



STEVE:  Okay, well, this is the week I've been waiting for because I have a photo that just had to be a Picture of the Week.  It had to be given the label Windows Patch Tuesday.  And so here we are, second Tuesday of July, Patch Tuesday.  Don't know what the patches are going to be, but it doesn't really matter because this photo says it all.



Today's podcast is titled "The Polyfill.io Attack."  And oh, baby.  The Internet, we the Internet, dodged another potentially devastating bullet.  A really interesting story, though, about the back story and history of this.  But first we're going to look at, not surprisingly, Entrust responded to Google's decision, which was our topic last week, to refuse the trust of any of their newer TLS certificates.  We're going to look at their response, and also at how the other CAs have responded to what they perceive as a new opportunity.  And I was thinking, Leo, about how you mentioned last week that somebody in the forum, I guess in Discord or wherever you have the chat dialogue, because I commented that Entrust was not a major, you know, not one of the top-tier certificate providers.  I don't remember if it was 1% or .1%. 



LEO:  It was pretty low, but it was ninth on the list, so it wasn't...



STEVE:  Well, yes.  And I was going to say that, given the total number of certificates, even 1% is...



LEO:  Right, right, it's a lot.



STEVE:  That's a lot of certificates that, you know, you'd like to...



LEO:  Yeah, to screw up.



STEVE:  Yeah, well, or the other CAs would love to take off of Entrust's hands.  You don't need all those certificates.  Let us handle those for you because you see them unable to carry them all.  Also, we're going to answer the question, what's a Passkey Redaction Attack, and how worried should we be?  And speaking of Passkeys, why not just have a website, one of our listeners ask, hold as many of them as we need?  And also, regarding the OpenSSH attack, wouldn't adding port knocking in front of that serious OpenSSH flaw which we discussed prevent this problem?



LEO:  Hmm.



STEVE:  And, if so, what's the larger lesson there to be learned?  And what about blocking an IP after some number of failed attempts?  Or nailed, I guess.  Number of failed, that would be nailed attempts.  What about that?  And as I said, we're going to then wrap up with the Internet once again dodging a potentially devastating bullet.  But what's really interesting is how we just walked right into this thing.  I mean, it's just, like, so obvious.  Other people saw it coming.  But nah.  Anyway, the good news is lessons to be learned, and I think some great takeaways from this week's podcast, "The Polyfill.io Attack."



LEO:  I like whatever that is.  Polyfill just sounds good.  Sounds like something to hold your dentures in place.  No, that's another...



STEVE:  Actually, I think there is a product called...



LEO:  Polident.



STEVE:  Polident, yes.



LEO:  And I have written in the past, I have coded flood fills.  That's kind of a fun little coding routine.  But a polyfill?  I don't know what that is.  We'll find out.  I am prepared to show you the consequences of many Patch Tuesdays.



STEVE:  One could argue too many Patch Tuesdays.



LEO:  One too many.



STEVE:  Oh, this is just so good.



LEO:  Do you want to describe it?  Do you need to describe it?



STEVE:  What we appear to have, given the somewhat blurry background, is like an oil storage tank of some sort.  Sort of looks like that, a big barrel painted red, got a big valve on it.  But then coming from it in the background, right up to crystal clarity in the foreground, is a pipe coming from the bottom of the output of this barrel, which then makes a right-hand turn, and it heads off the screen.  But not before we are treated to what you can only describe as something must have sprung a leak, and the people were desperate to not lose any of the precious fluid that was contained in this tank.  This is just - this is a patched pipe from hell.  I mean, it's got, like, shims and cardboard and black tape wrapped, and then metal screw-tightenable strangulators, I mean, it's just wonderful.



LEO:  It ain't going anywhere, that's for sure.



STEVE:  The moment I saw it, I thought, wow, this is our Patch Tuesday Picture of the Week.



LEO:  Oh, boy.  Yup.



STEVE:  So for those who are listening, if you have signed up to GRC's obviously free email system...



LEO:  Well, not obviously.  It's good that you do make that for free.



STEVE:  It is free, of course.



LEO:  Yes, yes.



STEVE:  5,782 of our listeners, so we're approaching 6,000, 5,782, received the email from me a couple hours ago containing a little thumbnail size of this wonderful picture, which if you click on it, will reveal it in all its full-size glory.  So, wow, this is a great Patch Tuesday picture.



Okay.  And again, a reminder that, if you would like to get the summary of the show, a link to the show notes, and also the Picture of the Week in your inbox every Tuesday morning, as soon as I have them, the system's working well.



LEO:  You have a, you know, that number is actually pretty good.  That's almost 10% of your audience is subscribed.  So that's pretty good.  Good job.  But that means there's still nine out of 10 can subscribe.



STEVE:  Well, and, you know, realistically, a lot of...



LEO:  It's what I'd expect.



STEVE:  Yes, I'm sure that a lot of people are like, hey, I listen to it.  Why do I need text?  You know, I've already got it buzzing into my ears.  So I understand.



Okay.  So Entrust responds.  We get a letter from el presidente, followed by an interesting FAQ.  So Todd Wilkinson, the President and CEO of Entrust, you can imagine this all got his attention.  The day after Google's announcement, and of course we were right on top of it the day of, he wrote an open letter to the industry essentially, titled "Thoughts on the Google Chrome Announcement and Our Commitment to the Public TLS Certificate Business."  And again, by Todd Wilkinson, President and CEO of Entrust.



He wrote:  "Last week Google announced that they would no longer include Entrust root CA certificates in the Chrome Root Program."  Not technically accurate, but then, you know, Todd is the president, so we're not expecting complete technical accuracy from a president and CEO.  But he's, you know, he's got the gist.  As we know, the roots are still staying in Chrome.  It's just that the signature date of anything signed by those roots is now being examined.  And if that date falls after this coming Halloween, which is to say the end of October, then they will not be trusted.  But anyway, Todd's got the idea.  He says:  "We are disappointed by this decision" - one can imagine - "and want to share how we intend to move forward."



He says:  "We understand what led us here.  We are committed to improvement.  And Entrust continues to have operational capabilities to serve our customers' public and private digital certificate needs.  These capabilities extend beyond the issuing roots in question.  Our recent mis-issuance incidents arose out of a misinterpretation we made of CA/Browser Forum compliance requirements."  And let me just stop right there to note that, and when they found out that they had been misissuing certificates, they did not stop doing so.  They instead started arguing with the CA/Browser Forum while continuing to issue misissued certificates and refusing to revoke any because they didn't want to upset their customers.



Okay.  Anyway.  He says:  "In our attempt to resolve this issue, our changes created additional non-security related misissuances."



LEO:  Oh, boy.



STEVE:  Wow.



LEO:  We started digging a hole, and then we dug it...



STEVE:  Get a little tangled up there, Todd.



LEO:  ...a little deeper.



STEVE:  Yeah, throw that shovel away.  "In our attempt to provide additional flexibility to our customers" - okay, we don't know what that is - "we provided extensions and delays in revocations that were not supported by the CA/Browser Forum Requirements."  In other words, we weren't supposed to do that because, you know, there's no latitude here; right?  You've got to follow the rules, which you voted on.  Anyway, he says:  "...which mandate five-day revocation for all certificate mis-issuances."  So, you know, he's covering part of this, but he's conveniently failing to mention that we didn't stop misissuing certificates, and that's also "bad dog."



So, he says:  "This created an environment in which the community scrutinized past Entrust incidents."  Whoops, we looked further back.  How did that go?  He says:  "This identified past Entrust commitments which, if fully implemented, could have helped to prevent these incidents."



LEO:  Oh, boy.



STEVE:  But of course we didn't fix those, either, back then.  So, whoops.  Our bad.  He says:  "We agree that there are opportunities for us to improve."  In other words, they agree with everybody else.  "And we've completed a thorough assessment of our CA operation in the past few months.  As a result of this assessment, we made changes in our organization" - I'm tempted to insert here, which no one has seen yet, no - "in our organization, processes, and policies.  For example, we've moved the CA product compliance team into our global compliance and operations teams to fully leverage the more robust capabilities of this larger organization."



LEO:  Well, thank goodness.



STEVE:  So apparently the people that were in the sidecar and...



LEO:  Geez, Louise.



STEVE:  Wow.



LEO:  We moved them.  We moved them over.



STEVE:  Yeah, they weren't doing it right.  So now we've moved the people that weren't doing it right in with the people who hopefully are.



LEO:  Yeah, right.



STEVE:  And maybe they won't contaminate the rest of the group.  He said:  "We've instituted a cross-functional change control board" - because that's what you want in your change control board, Leo, is some cross-functionality.



LEO:  Hereinafter, deck chairs will be rearranged by a committee of people in charge.



STEVE:  And bolted down with a seatbelt.  So they've got a - they've instituted a cross-functional change control board, whatever that is - "and a technical change review board to catch similar issues in the future."  Boy, this all sounds great.  "We are accelerating R&D for TLS certificate compliance and automation-related work while also improving the tracking of our public commitments" - oh, they're going to be tracking their public commitments, that's good, so they don't wander off course - "and revising our public incident response practices to ensure such issues do not occur again."



Well, talk about showing up after the party's over.  He said:  "I want to assure you that we are committed to continuing to serve as a public CA" - even though we'll have no customers - "and that we will complete open issues and promised improvements in a timely manner.  We're working with Chrome and the other browser root programs to address the raised concerns while also providing continuity for customers" - yeah, continuity, they had that because they were never getting any certificates revoked - "while we execute these changes.  We have the expertise to do this, as demonstrated by our ability to deliver our many products and solutions designed to meet demanding global compliance requirements."  Wait.  It was the demanding global compliance requirements that they were not meeting, which is what they've demonstrated over the years.  But, okay.



Finally, he says:  "Entrust has been" - and this is true - "a publicly trusted CA for over two decades and has contributed to stronger Web PKI capabilities globally."  Again, that's true.  "We continue to have operational capabilities to serve customers' certificate needs today and" - well, he says we'll do so in the future.  That remains to be seen because they'll have no customers.  But, he said:  "We respectfully ask for your patience as we work to ensure that you have no disruptions to the service you have come to expect from Entrust."



Okay.  Well, so he said what he could.  Right?  I mean, do you need, at this point, is this where you need the President and CEO to say something?  So he did.  Of course, he failed to explain that the real concern was their history of like exactly this; right?  Now, this is not the first time.  This is like the 12th time the industry has heard this, a history of repeatedly broken promises to repair what's broken, and then excuses after the fact that, you know, oh, like why it was that they weren't - these changes weren't made and attempts to justify their actions.  So, okay.  And of course they keep putting their customers' needs in front of their duties as a CA, which was the beautiful conclusion that one of the techies came to last week.



So anyway, I found also their four-question FAQ regarding this incident to be interesting, especially number three.  But here's all four.  So this is what they asked themselves and then answered.  Question:  "What does it mean when Google says you've been 'distrusted'?"  Ouch.  They say:  "This means that public TLS certificates issued from nine Entrust roots after October 31st, 2024, will no longer be trusted by Google Chrome by default.  TLS certificates issued by those Entrust roots through October 31, 2024, will still be trusted by default through their expiration date."  Got that one.  Nailed it.  Okay.



Question number two:  "Other CAs have said that after November 1st your TLS certificates won't be valid in the Google Chrome browser.  Is that true?"  "No, that's not true.  All Entrust TLS certificates issued through October 31st, 2024," like we just told you, "will be trusted by default by Google through their expiration date.  After October 31st, we will have the operational capabilities to serve customers' certificate needs."  But, you know, I don't think they're saying "but not issue any new ones."  Oh, no.  "With alternative or even partner roots if necessary.  Our goal is to ensure that customers have no disruptions and continue to receive global support, certificate lifestyle management" - oh, sorry, "lifecycle management."  Be nice to have some lifestyle management - "and expert services to meet their digital certificate needs."



Question three:  "Does Google's decision mean Entrust is out of the digital certificate business?"  Oh, by no means no.  "No.  It means the public TLS certificates issued from nine Entrust roots after October 31st, 2024, will no longer be trusted in Chrome's browsers by default.  Google's decision has no impact on our private certificate offerings."



LEO:  Hmmm?  Oh.



STEVE:  Well, private certificate, not public.



LEO:  Private, right, right.



STEVE:  "Including our PKI, PKI as a Service, and managed PKI - nor our code signing, digital signing, and email, S/MIME and VMC offerings."  Now, these are the things that I was referring to last week as the second order casualties of being forced away from the public certificate business.  It's just like, well,  you know, you don't seem able to make simple web certificates.  Should we trust you to do all this other stuff?  That's, you know, in question.



They finished answer three, saying:  "We have the operational capabilities to serve customers' certificate needs now and in the future."  And here they're repeating from question two:  "With alternative or partner roots if necessary.  Our goal is to ensure that customers have no disruptions and continue to receive global support, certificate lifestyle" - lifecycle - "management, and expert services to meet their digital certificate needs."



And, finally, fourth question:  "Does this impact other Entrust solutions?"  Answer:  "This announcement has no impact on the broad range of Entrust identity-centric solutions."  Of course, other than whether anyone is going to want them.



Okay.  So as I said, I thought their most interesting response was three, noting that even after Google begins to conditionally mistrust certificates signed by any of their root certs, nothing prevents them - and this is true - from riding on someone else's coattails.  Nothing, that is, other than Google releasing another update to Chrome which then also blocks that.



LEO:  Right.



STEVE:  Remember that Entrust originally bootstrapped themselves into the commercial SSL/TLS browser certificate business by having Thawte sign their intermediate certificate, which is how they were then able to sell end certificates.  And that allowed them to ride the coattails of the trust that Thawte had earned.  So this leaves two questions:  Would another CA think it was worth their while to sign an intermediate certificate for Entrust, knowing that their own reputation would then become entangled with Entrust's?  And if this were done, what would Google do?  Would they move to also block Entrust's intermediate certificates?



And actually, they could probably do that preemptively in Chrome.  That is, they could look at intermediate certificates, see if the signer is Entrust, and essentially make the block a little stronger than just blocking certificates signed by the roots directly.  You know, you could do a second order also.  So we're going to see what Halloween brings this year.  And I imagine behind the scenes Entrust is probably begging to be allowed to use these intervening four months - they've got July, August, September, and October - to fix and demonstrate fixes and not have that update to Chrome on November 1st make this happen.



LEO:  I wonder how much they care.  I mean, they really are in a lot of different - they have fingers in many different pies.  I don't know how big a part of their business this is.  Todd came to the company from Datacard, which acquired Entrust.  And Datacard's business was printing credit cards, and the mag strips on the back.  First credit card imprinting, like the raised lettering on it, and then the mag strips on the back.  So I don't think he has deep roots in the CA community.  Clearly, you know, it's just a management issue at this point.  And maybe they don't even care about that business that much.  It's kind of amazing.



STEVE:  We'll see; you know?



LEO:  Yeah.



STEVE:  Somebody wrote a nice letter for him.



LEO:  Yeah.



STEVE:  That he put his name on.



LEO:  It's a big company.  They have 2,500 employees.  They're a giant company.



STEVE:  Yeah.  They've been around since, you know, the...



LEO:  Fifty years, yeah.



STEVE:  Yes.  Yeah.  And as we noted, they were acquired by private equity.



LEO:  Right.



STEVE:  So don't know that that had any effect, but it happened.  Okay.  Now, no one will be surprised to learn that competing certificate authorities were not slow to report themselves on the news of Entrust's fall from grace and to offer to provide their services in replacement.  Sadly, not all CAs did this in a conscientious fashion.  For example, the CA Sectigo misstated the future.



Now, remember that Sectigo is not one of this podcast's favorite CAs.  In fact, they would likely be the last CA I would ever choose.  Long-time listeners may recall from whose loins Sectigo sprung.  That would be none other than Comodo.  Essentially, Comodo so badly damaged their own reputation due to a series of famous major certificate authority screw-ups that they decided to rebrand themselves in the apparent hope that no one would notice.  Remember that first spyware that I found that was the - I'm trying to remember whether it was - I know that they were Aureate.



LEO:  Oh, yeah.  Aureate.



STEVE:  But they renamed themselves Radiate or something.  You know, again, they got so, their reputation was so hit by the discovery that they'd snuck into everyone's PCs that they said, oh, I think maybe it's time to change our name.



LEO:  Yeah.



STEVE:  That's right.  So, okay.  In any event, Comodo/Sectigo gave their rapacious posting the title:  "Google to distrust Entrust SSL/TLS certificates:  What this means for the industry."  And then they tried to tell us.  They wrote:  "In a significant move to enhance digital certificate security, Google has announced its decision to distrust all public SSL certificates issued by Entrust, effective after October 31st, 2024."  Okay, they got that part right.  



"This announcement has sent not just ripples, but waves through the industry, particularly among Entrust customers who now face the urgent task of transitioning to new Certificate Authorities."  Okay, except there's nothing whatsoever urgent about anything here.  You know, as we are all very well aware, all currently issued certificates remain valid through their current lifetime, and everyone has July, August, September, and October to chose another CA when it comes time - I mean, it could be a year from this coming October - when it comes time to replace any Entrust SSL/TLS browser certificate.  So not so urgent there, Sectigo.



Then they said, under the heading "The catalyst for distrust,"  



"Google's decision is rooted in a series of compliance failures by Entrust."  Uh-huh, and look who's talking.  "Over the past several months, Entrust has experienced significant issues, including extremely delayed revocations and multiple lapses in meeting established security standards.  Google's Security Blog noted:  'Over the past six years, we've observed a pattern of compliance failures, unmet improvement commitments, and the absence of tangible, measurable progress in response to publicly disclosed incident reports.'"  Yeah.  Rub their face in it.  "This lack of progress and ongoing issues justified the revocation of trust in Entrust's public roots.



"To be a trusted browser, a CA must comply with specific requirements defined by the CA/Browser Forum."  And I guess they would be an authority on that.  "Transparency is crucial, as CAs are expected to work in good faith with browsers to fix and prevent issues.  Recent root program audits indicated a lack of confidence in Entrust's TLS certificate issuance practices, so this news wasn't completely unexpected to the industry, and prompted Google's decision to distrust Entrust certificates in the Chrome browser.



"Implications for business:  For businesses using Entrust certificates, this development necessitates immediate action."  Okay.  No, it deliberately doesn't, but okay.  "Any website using an Entrust certificate issued starting November 1st will be treated as an unsecured site on Google Chrome, and likely other major browsers will follow suit.  Companies must source a new certificate authority before the deadline" - again, no - "to avoid their websites being flagged as untrusted."  Again, just, you know, no companies must source a new certificate authority before their present Entrust certificate dies a natural death by having reached its expiration date.



Then they finally - they finish with "Choosing a reputable Certificate Authority."  They said:  "Considering Entrust's failings" - and of course our renaming - "businesses must reassess their relationships with CAs.  A reputable CA should demonstrate robust compliance with industry standards, transparent operations, and a proven track record of security and reliability.  Companies like Sectigo, which offers comprehensive certificate lifecycle management solutions, present viable alternatives."



Now, okay.  As I said, these guys would not be my first choice.  But we know who my first choice would be.  That would be none other than DigiCert.  So I went over to see what DigiCert had to say about this.  Again, this end of Entrust trust does represent a significant marketing opportunity for every other CA in good standing.  Now, rather than going point-by-point through DigiCert's similar marketing piece, I'll just say that they did the honorable thing, as I was hoping they would.



Under their topic "What Does That Mean for Entrust Customers?" they had three bullet points.  First:  "Public TLS certificates issued off of Entrust roots whose earliest SCT" - that's the signed certificate timestamp - "is after October 31st, 2024 will no longer be valid on Google Chrome."  100% correct.  Second:  "Those Entrust certificates will be treated as an unsecured site."  Right again.  And third:  "Any TLS certificate with a Signed Certificate Timestamp dated before October 31st, 2024 will be valid for its term."



Okay.  Now, with the slight exception that they meant before November 1st, rather than before October 31st, DigiCert got it exactly right, "valid for its term."  And then they finished with, of course, "We're Here to Help.  We understand this incident poses significant risk of business disruption to a large number of organizations.  As the world leader in globally trusted PKI and TLS/SSL solutions, we're committed to making our services and solutions available to help you maintain critical operations and ensure uninterrupted business continuity during the transition from Entrust and beyond."



So, as it happens, GRC's DigiCert certificate expires at the end of this month.  And thanks to their excellent service, I already have its replacement standing by, which I'll be switching all of GRC's servers over to shortly.  So anyway, I guess we will see in the fullness of time what transpires.  And I guess I'm most curious of all, out of all of this, to see whether the decision is reversible, whether, I mean, and I don't know how it would be, really, because all they can do is make more promises.



But their promises are only good when measured against practice.  And that takes time.  And they've been given time, time and time again, and have not come into compliance.  So in order to have no disruption, Google would have to become convinced that this time they really mean it when they sounded like they really meant it all the other times, too, and nothing changed.  So, you know, are these one-way decisions that never change?  And I guess this is the point.  Once they're out of the business, they don't have the capability of proving that they should be in the business because they're out of the business.  So, interesting.  Leo, we're 30 minutes in.  Let's take a break.



LEO:  Let's do it.



STEVE:  And then I'm going to talk about something new:  Passkey  Redaction Attacks.  You don't want your Passkey to be redacted, that's for sure.



LEO:  I don't even know what that means.  That means cover it over with a black magic marker?  I don't...



STEVE:  An interesting guy coined the term.



LEO:  Okay.



STEVE:  And he likes to refer to himself in the third person, sort of like Trump.



LEO:  Oh, hmm.



STEVE:  So, yeah.



LEO:  All right, Steve.  Let's hear about this redaction thing.



STEVE:  Okay, so, yeah.  A number of our listeners picked up on a weird story that was featured on the Dark Reading site.  Dark Reading is a good site, but maybe they were starved for news last week.  The story's headline was "Passkey Redaction Attacks Subvert GitHub and Microsoft's Authentication."  Okay, now, naturally that raised my eyebrows, too.  The tagline beneath the headline was "Adversary-in-the-middle attacks can strip out the Passkey option from login pages that users see, leaving targets with only authentication choices that force them to give up their credentials."



Okay, now, so first I need to pause here for an old fart "get off my land, get off my lawn" comment.



LEO:  Land is better.  Get off my land, I like it.



STEVE:  Get off my land.  I've got my shotgun ready.  It's what they're calling an Adversary-in-the-Middle attack.  Okay.  So it appears that the politically correct "PC" police have decided that the use of the traditional and time-honored term "Man in the Middle" is no longer proper, as if "Man in the Middle" was actually a reference to some man somewhere, and that as a consequence all men should now be taking offense at this slur to our gender.  Well, I am a man, and I'm not the least bit offended because I, and I'm sure everyone else, have always understood that the use of the term "man" in man-in-the-middle was always meant as an abstraction.  So if any of our listeners were also rolling their eyes at the use of this new replacement term "Adversary in the Middle," anyway, at least know that you are not alone.



LEO:  Well, you're not going to like this, but I understand the Evil Maid Attack has been rebranded the Evil Sanitation Worker Attack.  So I think that really there's just - the world has changed, Steve.  The world has changed.



STEVE:  There's just no hope.  I don't know, Leo.  Maybe 999 should be it because - no, I'm just kidding, everybody.  No,  999 won't be it.



LEO:  Old man, don't give up yet.  It's not too late.



STEVE:  I can adjust.



LEO:  Okay.



STEVE:  I can adjust.  Although I'm sticking with man in the middle; you know?



LEO:  Gotcha.



STEVE:  Okay.  So that's out of my system.  And I just completely distracted us from the article.



LEO:  Yes.  You can just call it MITM.  That's what you call it.



STEVE:  MITM.  Dark Reading refers to a piece from a site I've never encountered before called eSentire.  I don't know why it's called that.  I think all the good names were taken.  So everything about this is a bit odd, since the article is written by a guy named Joe Stewart.  And in the introduction, which he wrote, he refers to himself, as I mentioned before, in the third person.



Okay.  I'm only going to share the first bit in his introduction because I'll take it from there.  But Joe writes of himself, he said:  "In the past year, the uptake of Passkeys has surged, with industry giants such as Apple, Microsoft, and Google championing their adoption.  Joe Stewart, Principal Security Researcher with eSentire's Threat Response Unit (TRU), has been reviewing many of the leading software providers' implementations of Passkey technology and their current 'authentication process.'



"Regrettably, Stewart found that cybercriminals can still break into accounts protected by Passkey technology on many online platforms, such as banking, ecommerce, social media, website domain name administration, software development platforms, and cloud accounts, by using Adversary-in-the-Middle (AitM) phishing attacks.  Stewart explains in this" - and I guess he should know because he wrote it - "blog how Passkeys are designed to work, how threat actors can currently circumvent Passkey security, and the steps that online software providers and websites that use or intend to use Passkey technology must take to ensure that their Passkey implementation is secure from threat actors."



Okay.  Now, as I said, I read through Joe's article looking for this new problem that he had discovered.  And there isn't one that I could find.  All of this boils down to, if someone goes to a phishing site that's pretending to be the authentic site, the miracle that is Passkeys won't protect you because the fake site won't offer Passkey authentication.  Instead, it will offer any of the traditional authentication fallbacks that are still completely prone to interception by phishing.



LEO:  See, this shows you how good Passkeys are.  Right?  In a way, this is proof, I mean, Passkeys work.



STEVE:  Yes.



LEO:  Otherwise they wouldn't turn them off.



STEVE:  And it's unclear who would have ever believed otherwise, but Joe wants to be sure that everyone is on the same page with this.  So, you know, for what it's worth, we have a new term, the Passkey Redaction Attack, which when you think about it, that's what you'd call this.  It's a subset of phishing.  Since the FIDO2 WebAuthn Passkeys technology is immune from phishing, the phishing site simply removes the Passkeys login option and gets you the old-fashioned way.



LEO:  They probably don't want to use a YubiKey to log in, either.



STEVE: Oh, no, it can't do that.  You've just got to say... 



LEO:  Give me your password, buddy.



STEVE:  What's my pet's first name, or what's my porn name, or whatever it is.



LEO:  Forget the passwords.  Here's - what are your secret questions?  That's what you need.



STEVE:  That's right.  Wow.  Okay.  So those of our listeners who are worried, you know, basically, if you go to a phishing site, folks, you're in deep doo-doo; right?  I mean, you clicked on a leak in email.  You're at a bogus site, and nothing is going to help you.  So don't do that.



Ahmad Khayyat from Riyadh, Saudi Arabia, said:  "Hello, Steve.  Instead of synchronizing Passkeys, isn't it more secure to have a Passkey per device, locked into that device's TPM or equivalent facility?  Instead of backing up Passkeys, have backup Passkeys on additional devices.  Moreover, it's probably more feasible to convince sites to support multiple Passkeys per user than to convince Google, Apple, and Microsoft to support Passkey portability."



I completely agree with that latter.  The big problem here is that there's no way to know which sites support multiple Passkeys and which don't.  You know, you're just going to try to associate another Passkey with a site from a different device, and the device says, sorry, you already got one.  Use that one.  But you can't because it's on the device you're not using right now.



The Passkeys spec states that Passkey supporting sites should provide many-to-one Passkeys to account mappings.  But as we know today, not all do.  And it only takes one to ruin your lunch.  And running into a site that doesn't means that the user cannot add another device to that site, which is a breakdown of the Passkeys promise.  Hopefully this will eventually change.  But it's also true that having Apple, Google, and Microsoft performing their own cross-device synchronization of Passkeys, just as they've always done for passwords, takes the pressure off of sites to improve their Passkeys implementations.  Because, right, works for everybody using Apple, Google, and Microsoft.  What's wrong with you?



So for the time being, the only practical solution is to either have that be your complete and total solution, remain within one of the closed ecosystems which is provided by the big three, or use a third-party password manager such as 1Password or Bitwarden, both sponsors of the TWiT network, which will provide the kind of cross-platform compatibility that Passkeys was intended to provide, but doesn't yet universally.



And Passkeys was intended to provide it the right way, by having sites provide a many-to-one Passkeys to account mapping, then giving the user a user interface where they could see all of the Passkeys which are currently registered on their account, and administrate them.  You know, say yes and no to various Passkeys.  Like remove Passkeys for devices they're no longer using or don't want or have given a device to a family member, but they shouldn't have access to the family's banking site because, you know, they're not old enough yet and so forth.  Anyway, we don't have that unfortunately.  We can hope that we get that moving forward.



Max Feinleib said:  "I'm guessing" - I love this because I was pronouncing it "noo-vo" mailer.  He says:  "I'm guessing that nuevoMailer..."



LEO:  Nuevo.  Nuevo.



STEVE:  Well, he was saying "nuevo."



LEO:  Right.  That's Spanish for new, yeah.



STEVE:  As in Spanish for new, yes, nuevoMailers.  So now that he says it, and now that you've said it, Leo, and now that I've heard it, I'm sure that's the case, nuevoMailers.



LEO:  Nuevo.



STEVE:  Thank you, Max.  And I did get a nice email back from Panos, nuevoMailers author...



LEO:  Oh, nice.



STEVE:  ...saying that he'd heard from some of our listeners.  So believe me, as I said, I gave it an absolutely zero restraint endorsement because this thing, I'm just - you know, when you send out email to thousands of people in 30 minutes, it's a puckering sort of experience, and I keep expecting something to go wrong.  But it just works.  It's great.  So nuevoMailer.  Now I know how to even pronounce it.  So a bonus.



LEO:  Nuevo.



STEVE:  Yeah.  Okay.  So John, whose Twitter handle is @PsyVeteran, and I did get actually this from Twitter this week, he said:  "Hey, Steve.  Listening to you two discussing the OpenSSH bug in Security Now! 981 last week," he said, "I note that a port knocking setup would completely mitigate this front door camping.  Love listening to you guys and excited for 999 and beyond.  On a side note, while I am all signed up for email, I cannot see what address to send SN feedback on."  Oh, so he fell back to Twitter because he didn't know how to email me.  I'll get to that in a second.



So John's correct that port knocking would prevent exploitation of the OpenSSH flaw.  And I suppose if one were really a belt-and-suspenders type, then adding that additional layer of pre-authentication authentication would make sense.  But the problem is OpenSSH is supposed to be super secure, and the only protection anyone needs all by itself.  Leo, you and I both authenticate our OpenSSH instances with passwords and certificates.  A certificate is like a 2,048-bit secret key which cannot possibly be brute forced.  Then a password is added to that, just to keep anyone who might obtain local access to the certificate from using it.  So my point is, OpenSSH already provides so much security that anyone could be forgiven for not deploying an additional outer layer of protection.  



But having said that, there is a larger, more general lesson to be learned here.  And I'm glad John asked.  And that's the inherent problem with the security of any monoculture.  Relying entirely upon security from any single source is never going to be as safe as using multiple different sources of security.  Or stated another way, using a "layered security solution" is able to provide the strongest security by far.  And if you're going to go to the trouble of layering, it's even better when the various layers have nothing in common with one another.  So John's observation that adding a "layer" of port knocking security is exactly that.  And the advantage of port knocking is that it does not require a fixed IP for the "knocker."  The whole point of knocking is that the secret knock identifies the IP of the knocker dynamically, whose traffic is then allowed to progress into the next layer of security.



I've often noted that I have all of the links between my various sites protected, not only with their own native security, whatever that may be and about which each of their vendors will boast at great length, but also with point-to-point IP address filtering so that none of those links are even visible to anyone else out on the Internet.  Their own protection may be strong, but why take an unnecessary risk?



Of course, IP filtering is only feasible when the IP addresses of the endpoints are relatively static.  But there really is no stronger security than adding a layer of simple IP address filtering to Internet traffic whenever and wherever possible.  If anyone is listening to this who has publicly accessible endpoints which have fixed IPs, where IP-based filtering is not currently in place but could be, allow me to urge you to consider preventing anyone else from even being able to see those ports.  I mean, it's almost magic.  It's really worth it.



Now, John also mentioned that he signed up for our email system but didn't see our inbound email address listed anywhere.  That's mostly by design, to tell the truth, because I would prefer to only give Security Now! listeners direct access to this mailbox.  The address is shown on GRC's old feedback.htm page.  And since many others have mentioned this, I've just changed the "reply to" address for the mailings to the Security Now! list to that correct address, which is "securitynow@grc.com."



So everyone who has already received today's podcast email, all 5,782 of you, containing this show's summary, the Picture of the Week, the full show notes link, will see that it was sent from "securitynow@grc.com."  So simply hitting Reply to any of that Security Now! email will properly address a reply directly to me.



And speaking of heterogeneous layers of security, Dr. Brian of London wrote:  "Fail2Ban.  I can't believe any serious sysadmin is not running this."  And he provides a link to its GitHub page, github.com/fail2ban/fail2ban.  He said:  "That completely negates this ssh vulnerability as far as I can tell."  And as far as he can tell is exactly correct.  After being installed into any Linux system, "Fail2Ban" creates a daemon running in the background.



The project describes itself by writing:  "Fail2Ban scans log files like /var/log/auth.log and bans IP addresses conducting too many failed login attempts.  It does this by updating system firewall rules to reject new connections from those IP addresses for a configurable amount of time.  Fail2Ban comes out of the box ready to read many standard log files, such as those for sshd and Apache, and is easily configured to read any log file of your choosing, for any error you wish."



So, cool solution.  It provides another layer of security based upon dynamic IP address filtering, and it's another layer that any mature security solution ought to have.  Why would any service allow any single remote IP address to be pounding away with repeated authentication failures?  It's just nuts.  But sadly, that's still not built in.  So this sort of protection, you know, it wouldn't block a widely distributed attack, where attempts are coming from a huge botnet, for example, with each bot having a different IP.  But in this instance the exact timing required to exploit this recent SSH flaw means that it would be very difficult to launch such an attack through a widely distributed network of attackers.



So again, Fail2Ban.  If you've got publicly accessible important services that can be intolerant and should be utterly intolerant of many failed login attempts, and you're running Linux or a compatible OS for Fail2Ban, it's free, it's open source.  I can't imagine what the downside would be.



LEO:  What a good idea.  I mean, a lot of services have that turned on.  My Synology I have that turned on.  So after 10 failed attempts it's like, yeah, no, you're not getting in here ever again.  I also block China and Russia, which is probably a good idea since I don't think I'll be logging in from there any time soon.



STEVE:  No.  And in fact I have the same thing on my email server.



LEO:  Right.



STEVE:  Because they're just, you know, they're not spamming valid addresses.  They're just going - they're doing a dictionary attack.



LEO:  Right.



STEVE:  First names A through Z.



LEO:  Yeah.



STEVE:  Yes.  And so after a server tries, I think it's five times with bogus accounts, I just put it on a blacklist for, I don't know, some length of time, and just say, look, just - I'm just not even going to entertain your guesses anymore because they could guess right at some point.



LEO:  Right.



STEVE:  And I'd rather not have that crack with them.



LEO:  It's a simple and effective defense.



STEVE:  Let's take another break, and then we're going to talk about "The Polyfill.io Attack."  And, oh, baby.  



LEO:  It sounds, now that I think about it, I think it's a Pokmon is what it is.  You don't know what Pokmon are, probably.



STEVE:  Of course I know Pokmon.  I have one.



LEO:  You know Pokmon?



STEVE:  I have several in the refrigerator, Leo.



LEO:  Oh.  Are they tasty?  There is a Pokmon with that name, actually.



STEVE:  Actually, I was always the uncle who was able to get for his nieces and nephews the unavailable Christmas presents.



LEO:  Ah.



STEVE:  Yeah.  So...



LEO:  So you do know a little bit about all this.



STEVE:  Oh, yeah.



LEO:  That's a good uncle.  All right, Steve.  Continue on.  Soldier on, my friend.



STEVE:  This is big.



LEO:  Uh-oh.



STEVE:  We originally examined the consequences of the removal of a cloud-hosted service whose DNS CNAME record continued to point to the missing service.  This then allowed ne'er-do-wells to establish their own replacement cloud service under the abandoned domain name as a means of hosting their malicious services within someone else's parent domain.  That's not good.



LEO:  Unh-unh.



STEVE:  But bad as that is, it's not the worst thing that can happen.



LEO:  Oh.



STEVE:  Arguably, the worst thing that could happen would be for a domain which is hosting a highly popular, often downloaded code library to fall into nefarious hands.



LEO:  Ah.  That would be awful.



STEVE:  It would be awful.  And I would not be talking about this this week if it had not just happened.



LEO:  Oh, boy.



STEVE:  So first I'll back up a bit and explain what's going on here.  An unhealthy "design pattern" that's developed is websites hosting pages which dynamically pull significant chunks of code from third-party sites.  Back in the good old days, a website provided all of the pieces of the web page that it would present to its visitors.  Naturally, advertising ended that with ads being filled in by third-party advertising servers on the fly.  And though we've never had the occasion to talk about it directly, the same thing has become commonplace with code libraries.  A perfect example came to mind with GRC's own XenForo web forums.



XenForo mixes a bunch of different technologies, with PHP being its primary implementation language on the server side.  But the forums' browser-side polish, things like fading button highlights, some fancy animations, pop-up floating dialogs and so forth, are provided courtesy of JavaScript running in the user's browser.  And rather than reinventing the wheel, XenForo, like most websites, takes advantage of the wildly popular jQuery library.  Wikipedia explains.



They said:  "jQuery is a JavaScript library designed to simplify HTML DOM (Document Object Model) tree traversal and manipulation, as well as event handling, CSS animations, and Ajax."  Ajax being the browser's web page code itself making queries to other resources, typically back to the server as the user does things on the site.  Wikipedia said of jQuery:  "It is free, open-source software using the permissive MIT License.  As of August 2022" - get this - "jQuery is used by 77% of the 10 million most popular websites."  So just shy of four out of every five of the top 10 million websites are using jQuery.  "Web analysis indicates that it is the most widely deployed JavaScript library by a large margin, having at least three to four times more usage than any other JavaScript library."



Okay.  So jQuery has become so widely and universally used and standardized that you could almost think of it as a browser extension itself since so much code takes advantage of it. If you're a developer of web pages that depend upon jQuery, you'd like to obtain the benefits of any bugs that are fixed, speed improvements, or other optimizations as soon as they become available.  So rather than hosting the jQuery library yourself, you instead ask the browser to download the latest version of jQuery from the Internet.  The question is, where should it be obtained?  I've included a screenshot of one of the configuration dialogs for the XenForo forum system.



The option is labeled "jQuery source," and it has four radio buttons, so we get to pick one from among the four.  The choices are:  locally hosted, meaning that the server you're visiting provides the jQuery library itself; Microsoft CDN at aspnetcdn.com; jQuery CDN at jquery.com; or Google Ajax API CDN at googleapis.com.  And the default from the XenForo folks was the final choice, to obtain the library on the fly, every time it's needed, which is all the time, like every page, directly from Google.



Now, we might wonder why Microsoft and Google are interested in being benefactors by providing access to code libraries on their content delivery networks.  But remember, 77% of the top 10 million websites are all using jQuery.  In a world where third-party cookies are able to track their users, every time any web browser anywhere fetches a copy of the jQuery code library from Microsoft or Google, that browser's cookie for that CDN is sent along with the site the user's browser is visiting and the user's current IP address because that's where it's connecting from.  So Microsoft or Google will obtain some information to add to the pile they already have about every user's whereabouts and "whenabouts."  In return, our browsers probably don't need to wait very long to receive that library since both of those CDNs are extremely high performance.



Okay.  So the main point here is that a model has gradually evolved over time where websites we visit are no longer providing all of the content that runs in their pages, and that in addition to ads and Gravatars and web fonts and a bunch of other stuff, today's web pages also contain significant code libraries from third-party servers.



At the beginning of our discussion of this I noted that "Arguably, the worst thing that could happen would be for a domain which is hosting a highly popular and often downloaded code library to fall into nefarious hands.  And I would not be talking about this if it hadn't happened."  Armed now with this bit of background, we can easily understand the consequences of this and of how serious it could be.



So what happened?  First we need to answer the question, what's a polyfill?  There are times when a device or a library may not be the latest and greatest, like when someone is still using an older version of Internet Explorer which may not support the latest HTML standards.  What can be done in such cases is that a so-called "polyfill" library can be used.  The fill is that it's filling in for missing APIs.



So a website that wishes to be able to run on the widest range of browsers, while still being able to take advantage of the latest features of the most recent browsers, can choose to have its web pages download and invoke a polyfill library.  Then, when the page's code runs, the polyfill library will check to see whether the underlying web browser supports the various features that the site wishes to use.  And if the answer is "no," the polyfill library itself will make up the difference.  It'll fill in the gap.  It'll fill in for the down-version browser by emulating the functionality that's natively missing from that browser.  So in short, polyfilling is another popular practice and library.  It's mostly used to fill in for missing JavaScript features, but both PHP and Python have available polyfills, as well.



So just as with the wildly popular jQuery library, polyfill libraries, while not as wildly popular as jQuery, are still in wide use.  That makes it, let's just say, very bad when a Chinese company purchases the Polyfill.io domain and its associated GitHub account...



LEO:  Oh.  Oh.



STEVE:  Uh-huh, and then proceeds to do bad things with them.  Essentially, this company purchased a position of extreme power and responsibility and then chose to misbehave.  Exactly two weeks ago, on June 25th, the forensics team at the security site "Sansec" broke the news with the headline "Polyfill supply chain attack hits more than 100,000 sites."  And before I go on, I'll just note that, as we'll see, their initial estimation of more than 100,000 sites fell short by around 280,000.



LEO:  Oh.



STEVE:  The real number has turned out to be more than 380,000 web sites.  They wrote:  "Polyfill.js is a popular open source library to support older browsers."  And they said 100,000-plus.  We now know it's 380,000-plus sites.  They said:  "Embed it using the cdn.polyfill.io domain.  Notable users are JSTOR, Intuit, and World Economic Forum.  However, in February this year, a Chinese company bought the domain and the GitHub account.  Since their purchase, this domain was caught injecting malware into mobile devices via any site that embeds cdn.polyfill.io.  Any complaints were quickly removed from the GitHub repository.



"The polyfill code is dynamically provided based on the HTTP query headers, so multiple attack vectors are likely.  Sansec decoded one particular malware which redirects mobile users to a sports betting site using a fake Google analytics domain."  And I love this.  Get this, Leo, www.googie-anaiytics.com.  So get your googie anaiytics here.  "The code has specific protection against reverse engineering, and only activates on specific mobile devices at specific hours.  It also does not activate when it detects an admin user.  It also delays execution when a web analytics service is found, presumably to not end up in the stats.



"The original Polyfill author recommends that Polyfill should no longer be used at all, as it is no longer needed by modern browsers.  Meanwhile, both Fastly" - and we'll see where Fastly figures in here because it's significant - "and Cloudflare have put up trustworthy alternatives, if you should still need it.  This incident is a typical example of a supply chain attack."



Now, they followed up this report with a series of four updates, one per day, starting the same day.  So on the 25th of June, 26th, 27th, and 28th.  First they updated:  "Google has already started blocking Google Ads for eCommerce sites that use Polyfill.io."  The next day:  "Someone launched DDoS attacks against our infrastructure and BleepingComputer, who was the first to cover our research."  The day after that:  "Cloudflare has implemented real-time rewrites of cdn.polyfill.io to their own version."  A little later:  "Namecheap has put the domain on hold altogether, which eliminates the risk for now.  However, you're still recommended to remove any Polyfill.io references from your code."



And finally, on June 28th:  "We are flagging more domains that have been used by the same actor to spread malware since at least June" - so a year ago - "of 2023:  bootcdn.net, bootcss.com, staticfile.net, staticfile.org, unionadjs.com, xhsbpza.com, union.macoms.la, and newcrbpc.com."  So a big problem with this code.



And then Censys weighed in with some terrific reporting based upon their extensive visibility into the Internet.  I should note that my server logs are full of port probes from Censys.  They want to know what's going on and who's running what, where.  They're another site like Shodan.  But the nice thing, for me at least, is that their reverse DNS resolves to their domain name.  So at least, if I'm curious, I can see who's knocking at the door.  I still don't let them in, but it's marginally less creepy to know that it's not an attacker, you know, trying to actually get in.  It's just somebody, as their name suggests, doing a census of the Internet.



Anyway, last Friday, under the headline "Polyfill.io Supply Chain Attack - Digging into the Web of Compromised Domains," Censys wrote:  "On June 25, 2024, the Sansec forensics team published a report revealing a supply chain attack targeting the widely-used Polyfill.io JavaScript library.  The attack originated in February 2024 when Funnull, a Chinese company, acquired the previously legitimate Polyfill.io domain and GitHub account.  Shortly thereafter, the service began redirecting users to malicious sites and deploying sophisticated malware with advanced evasion techniques.



"On the 27th of June, Namecheap suspended the malicious Polyfill.io domain, mitigating the immediate threat for now.  However, Censys still detects 384,773 hosts embedding a polyfill JS script linking to the malicious domain as of July 2,2024.  These hosts include websites associated with major platforms like Hulu, Mercedes-Benz, and Warner Bros.  Security experts strongly advise website owners to remove any references to Polyfill.io and its associated domains from their codebase as a precautionary measure.  Cloudflare and Fastly have offered alternative, secure endpoints for polyfill services as a workaround.



"Further investigation has uncovered a more extensive network of potentially compromised domains.  Researchers identified four additional active domains linked to the same account that owns the Polyfill.io domain.  Censys detected 1,637 million hosts referencing one or more of these endpoints.  At least one of these domains has been observed engaging in malicious activities dating back to June of 2023, but the nature of the other associated domains is currently unknown."  And you've got to ask yourself, why would 1,637,000-plus hosts be referencing these domains?  I mean, they've got to be thinking that they're pulling something from them.  It's just mindboggling.  "This incident," they write, "highlights the growing threat of supply chain attacks on open-source projects."



So just to be clear, 1,637,160 website hosting servers are currently emitting web pages that are causing their visitors' web browsers to download resources from these malicious domains.  Speaking of the hosts that they have found attempting to pull from Polyfill.io, Censys added:  "The presence of domains like 'www.feedthefuture.gov' in these top results also highlights the use of Polyfill.io across various sectors, including government websites.  In total, Censys observed 182 affected hosts displaying a '.gov' domain."  In other words, 182 .gov websites were pulling code from Polyfill.io owned by a malicious Chinese company.  Which is where we interject:  What could possibly go wrong?



"While estimates of the scale of affected websites vary widely between sources," they said, "Sansec reported 100,000, while Cloudflare suggested 'tens of millions,' it's clear that this supply chain attack has had a widespread impact.  Further investigation has uncovered an extensive network of potentially related domains.  A Twitter user discovered that the maintainers of the polyfill GitHub repo had leaked their Cloudflare API secrets within the repo."  Whoops.



"It was thereafter discovered that the leaked Cloudflare API key is still active and shows four additional active domains linked to the same account:  bootcdn[.]net, bootcss[.]com, staticfile[.]net, and staticfile[.]org."  And I should just mention that we're all able to put Polyfill.io and those other four domains into our hosts file, or into whatever local resolver we have, and blackhole them so that for no reason would anyone in our own networks or our browsers successfully download code from those sites, where they never would be wanting to.



They wrote:  "Bootcss[.]com, has been observed engaging in malicious activities that are very similar to the Polyfill.io attack, with evidence dating back to June of 2023.  The two main malicious domain names involved were both registered at the end of May.  The evil jQuery was introduced by highlight.js hosted on cdn.bootcss.com.  When the request for highlight.js has a specific Referer and mobile User-Agent, the server will return highlight.js with malicious code; otherwise, it returns normal code, which is highly disguised.  Moreover, there are specific conditions for whether malicious jQuery is introduced when this code is executed."



Okay.  In other words, matching on a specific referrer enables the code to only target users visiting specific websites, and matching on the mobile device's User-Agent causes it to only target users using specific device types.  So a highly targeted attack which, because it's delivering benign code most of the time, would easily go unseen.



So this is all really quite bad.  The one person whose reactions we've not yet heard from is Andrew Betts, the originator of polyfill.  Not Polyfill.io, just polyfill.  Way back on February 25th, after the change of ownership of Polyfill.io had occurred and before all this came to a head, Andrew tweeted.  This is the originator of the polyfill process, the polyfill library for web browsers.  He tweeted:  "If your website uses http://polyfill.io, remove it IMMEDIATELY," all caps.  He says:  "I created the polyfill service project, but I have never owned the domain name, and I have had no influence over its sale."  So he wants to fully disavow himself of any association or responsibility for what might happen.



He said:  "No website today requires any of the polyfills in the Polyfill.io library.  Most features added to the web platform" - meaning the polyfill web platform - "are quickly adopted by all major browsers, with some exceptions that generally cannot be polyfilled anyway, like Web Serial and Web Bluetooth.  Domains that serve popular third-party scripts" - so listen to this.  This is Andrew.  "Domains that serve popular third-party scripts are a huge security concern.  The team at google-analytics.com, for example, could read or modify almost any website in the world, like gov.uk or wellsfargo.com.  If you own a website, loading a script implies an incredible relationship of trust with that third party.  Do you actually trust them?"



And I should explain that, as I said, Andrew never endorsed the distribution of his polyfill library via a third party.  He intended to have the hosting site that needed to use the library host it themselves, just like in the good old days.  He's clearly aware of the havoc that would ensue if any major third party - like Google with their ubiquitous analytics code being injected into every website there is - were to go rogue or be compromised by a sophisticated attack.  But somewhere along the way, someone - not Andrew - had the bright idea of hosting polyfill at Polyfill.io, and over time everyone started to use it rather than host it themselves.  And this was the crucial mistake that so many in the industry made.



Think about it:  You've purchased TLS certificates to protect your site; right?  You have electronic and physical security, maybe even hardware security modules to protect your secrets.  You've implemented Passkeys to offer the latest state-of-the-art public key login authentication.  You've gone through the popular checklists of all the things you need to do to be secure, and everything is checked off on them.  You've done everything you can think of.  Then you have every one of your precious and secure site's web pages downloading and running active JavaScript code that runs with full permissions in a first-party context from an unknown entity in China that turns out to be malicious.  What's wrong with this picture?  What's right with this picture?  It's nuts.



So what happened in this case?  And Leo, after sharing our final sponsor, we're going to look closely at who was behind Polyfill.io.  And if it was the Financial Times, would you be surprised?



LEO:  Yes.



STEVE:  Because it was.



LEO:  It was?  This is a cliffhanger.  Okay.  Now Steve's going to explain what the Financial Times has to do with this mess.



STEVE:  So what happened?  Without some deeper investigation, it's impossible to say exactly.  I took a look back in time, thanks to the Internet Archive's Wayback machine.  The Wayback machine began taking snapshots of the Polyfill.io site back in 2013, so 11 years ago.  And it certainly all looked aboveboard and solid for years.  It was a service being brought to the world by the Financial Times with bandwidth and CDN services provided by Fastly, and the public domain polyfill code present on GitHub.



An early version of the site's homepage said:  "Just the polyfills you need for your site, tailored to each browser.  Copy the code to unleash the magic."  And then there's a simple line.  It's got an open angle bracket, and it says <script src="https://cdn.polyfill.io/v2/polyfill.min.js">.  And then all that's closed, and then you close the script.  That will cause the browser that reads that line to go to that domain through the CDN, pull polyfill.min.js, deminify it, and execute it.  And what it then chooses to do is entirely up to it.  You've loaded it because of the privileges it needs as a first party in your own domain's code.  It can do anything to your page it wants.  It can take the esses off of the HTTPs.  It can log the username and password that are entered into your page's forms.  It can do whatever it wants.



They said:  "Polyfill.io reads the User-Agent header of each request and returns polyfills that are suitable for the requesting browser.  Tailor the response based on the features you're using in your app, and see our live examples to get started quickly.



"The polyfill service is developed and maintained by a community of contributors led by a team at the Financial Times.  It evolved from a previous service developed by Jonathan Neal, and our cdn.polyfill.io domain routes traffic through Fastly, which makes it available with global high availability and superb performance, no matter where your users are."



So at this point the Financial Times is referring to cdn.polyfill.io as "their domain."  But things changed, and it's interesting to trace the site's evolution through the years.  The Wayback machine makes that easy.  What I discovered was that something happened toward the end of the year last year, on November 1st of 2023.  The day before, on October 31st, Halloween of last year, the site was offering polyfills while boasting that in just the last 30 days it had fulfilled 61.2 billion requests while delivering 292.98 trillion bytes worth of polyfills, and that was just in the past 30 days.  61.2 billion requests offering nearly 300, just shy of 300 trillion bytes in polyfills.  And Fastly's name was still proudly highlighted in red against a black background.



But then, the next day on November 1st, the site's pages changed.  Gone was the activity tracking, the reference to GitHub, or any mention of Fastly.  So as I said, it would take interviewing the parties involved to learn more.  One guy who would likely know the whole story is Jake Champion, whose Twitter handle is @JakeChampion.  His personal bio at JakeChampion.name mentions his involvement with the polyfill project, he's at Financial Times, and his copyright can be found at the bottom of the earlier pages.  But he has his Twitter account locked for viewing only by approved followers, so I wasn't able to say whether he may have been tweeting something about what had been going on.



What's clear is that maintaining the 100% free polyfill service was doubtless becoming expensive, and it was the definition of thankless.  Eleven years earlier it would have been much easier than it was today.  And with nearly 300TB of polyfills being served every 30 days, that's 10TB per day, day in and day out, in return for nothing.



So it's not difficult to imagine someone coming along and offering to purchase that massive burden.  Or maybe just take it off the Financial Times' hands.  We don't know.  I've not found anything about that yet.  Maybe some of our listeners will know or will find out.  I'd love to know.  But however it happened, in the blink of an eye, overnight, suddenly 384,773 websites, or tens of millions if you take Cloudflare's number, were suddenly inviting an unknown stranger's JavaScript code into and onto their previously super-secure and secured pages, thus bypassing and making a joke of everything else they had done to create secure websites.



While digging around for all of this interesting information and backstory, I ran across one other chilling bit.  This was posted over on Medium by Amy Blankenship, who describes herself as a full stack developer at a financial technology company.  She primarily writes about React, Javascript, Typescript, and testing.  And under the headline "Those Scary Stories About Polyfill.io?  They're Just the Beginning," she posted the following to Medium.



She said:  "The Internet exploded this week about how Polyfill.io was injecting malicious code into websites that were linking to it from the CDN at the URL where it has been served for years.  The thing is, ownership of the GitHub repo and the download domain were transferred in February.  They didn't wait until this week to start making changes.  Here's how I know.



"In February, we started to get mysterious errors when some of our users tried to log in.  The stack trace the Sentry error boundary was giving us didn't make any sense.  It was deep within the okta-react library code we were using.  But we had not recently upgraded our okta-react, okta-js, or Sentry code  or the code that called it.



"Long story short, in the course of stepping through the code in the debugger, I discovered that some code in Okta was expecting to receive an object that had iterable properties.  Sometimes when it received the object, those properties could not be iterated.  Digging further, I found that the object was returned from code deep within the polyfill library."



In other words, Okta, the major identity, cloud security, and access management company, currently worth $6 billion, had some deeply embedded dependency upon the Polyfill.io library, which it was pulling from Polyfill.io.



LEO:  Doh.



STEVE:  Thus from China.



LEO:  No.  Good on Amy Blankenship for finding that.  That's amazing.



STEVE:  Yup.  Wikipedia reminds us:  "Okta, Inc. is an American identity and access management company based in San Francisco.  It provides cloud software that helps companies manage and secure user authentication into applications, and for developers to build identity controls into applications, website, web services, and devices.  It was founded in 2009 and had its IPO offering in 2017, reaching a valuation of over $6 billion."



That's right.  And its core, Okta-react and Okta-JavaScript library code was pulling from a library that is now being supplied by a clearly hostile and malicious company named Funnull, based in China.



LEO:  Wow.



STEVE:  We know that because shortly after Funnull acquired the Polyfill.io domain and GitHub account, the Okta library itself began mysteriously acting up.  And this, of course, brings us to, at the top of page 17 of the show notes, Randal Munroe's wonderful xkcd cartoon showing a large and ungainly collection of stacked blocks reaching up to the heavens.  We would call it a "house of cards" if it were composed from playing cards rather than blocks.  That detailed and stacked assortment is then labeled "All modern digital infrastructure."



And then down near the very bottom, off to one side, is a crucial twig whose presence is holding up the entire edifice, the whole assembly, such that without it everything would come tumbling down.  And that little twig is labeled "A project some random person in Nebraska has been thanklessly maintaining since 2003."  Unfortunately, in this case, as the result of a number of unfortunate events, it would be labeled "Control acquired by a hostile and malicious Chinese company."



Now, Leo and I live in California.  This state has a major, seismically active, geological fault running through it known as the San Andreas Fault.  The San Andreas Fault runs North-South about 750 miles through California, forming part of the tectonic boundary between the Pacific Plate and the North American Plate. This is the same fault that Lex Luthor was determined to trigger...



LEO:  With a nuclear bomb, yeah.



STEVE:  ...and use to turn California's eastern neighboring state of Nevada into beachfront real estate by sinking all of California into its Pacific ocean.  Fortunately, well, we have Superman, so that hasn't happened yet.  The relevance of this is that the somewhat precarious state of our Internet's security infrastructure puts me in mind of the nature of seismic faults, and the earthquakes that accompany them.  As tectonic plates slowly shift, pressure builds up, and that's not a good thing.  So here in California what we're looking for - and we're hoping for - is a more or less continuous series of small earthquake tremors where no one is hurt, and the china plates remain on their shelves.  People text each other asking, "Hey, did you just feel that one?"  That's much better than a long period of quiet, which allows massive pressures to build up, only to be released in a single massive event.  So what we want here in California is lots of small, barely noticeable events.



How many times on this podcast, especially over the last five years, where it seems to be happening more frequently, have I noted that we all just dodged a bullet?  That the Internet just experienced another tremor?  We discovered a serious problem that did not take everything down with it.  So here again, the industry just received another wakeup call with no known actual, let alone horrific, damage being done, and an important lesson was hopefully learned that was not expensive.  Let's all hope that things remain like this, with a continuing series of small and harmless little earthquakes, rather than, as it's referred to in California, "the big one" hitting.



LEO:  This could have been the big one.  Big time.



STEVE:  This could have been...



LEO:  Holy cow.



STEVE:  ...really bad.



LEO:  We don't know how many sites chose Polyfill.io to load that library, though.  It could have been...



STEVE:  Yes.  Yes.  1.67 million.



LEO:  Oh, we do.



STEVE:  1.67 million sites.  We have a count.



LEO:  And got it from Polyfill.io.



STEVE:  They got it from the CDNs looking at the pages and seeing how many.  



LEO:  Right.



STEVE:  And also, right, the various sites that are pulling from Polyfill.io right now after taking control of that domain.



LEO:  Wow.



STEVE:  Millions, millions of different websites.



LEO:  So somebody does, somebody benign now controls it.  So we're okay.



STEVE:  Yes.  Namecheap took the site down.  They took the domain name.



LEO:  Do they offer the library or no?



STEVE:  Both Cloudflare and Fastly are hosting the final benign version for anybody who needs it.  But as its author said, nobody does anymore.



LEO:  Right, right.



STEVE:  So a lot of this, again, this is also stupid inertia hurting us.



LEO:  Running on automatic, yup, yup.



STEVE:  All that crap was left in the pages because no one's sure if we need it or not, so we'd better leave it there.  Meanwhile, China owns the domain, and it's pulling JavaScript as a first party into every one of those pages' code, that could do anything.



LEO:  It's the big one.  It's the big one.  But it isn't.  We dodged a bullet.  Amazing story.



STEVE:  Yup.



LEO:  What a great story.  And a cautionary tale, as you point out.  That's why you listen to this show, so you know, you're prepared, and you're keeping an eye out.  And again, credit to Amy Blankenship, who noticed a weird anomaly with her okta logins.



STEVE:  Yup. 



LEO:  Said, you know, something's not right here.  And that's a note to everybody.  When little weird anomalies happen, they're not necessarily the big ones, just little ones.  That's how Clifford Stoll noticed the bad guy in his network, because there were one or two cent discrepancies in the balance sheet.



STEVE:  And actually I'll bet our listeners are as twitchy as I am now.



LEO:  Yeah.



STEVE:  I see, like, some spike in bandwidth, I go, wait, whoa, what, what...



LEO:  It's like, whoa, what's going on?



STEVE:  What's going on?



LEO:  If it's out of the ordinary, it needs to be investigated.  Wow.  What a story.  You'll hear more stories like that every week on Security Now!.  Steve Gibson is the man in charge.  You can get on his mailing list or send him email by going to GRC.com/email.  Then you can verify, validate your address, sign up if you wish for the free newsletters, including, by the way, the show notes, so you get a picture, a peek at the Picture of the Week ahead of time.  GRC.com/email.



Now, while you're there, you can also buy a copy of Steve's bread and better, didn't mention it this week, but SpinRite pays the bills for Steve.  And it is a very useful, must-have, frankly, tool for anybody with mass storage for both performance, reliability, and maintenance, and recovery, if necessary.  GRC.com.  Look for SpinRite.  Lots of free stuff there, including ShieldsUP!.  ValiDrive, which is really a great thing.



And of course this show.  Steve has the usual 64Kb audio, you know, the kind of traditional version.  He also has a nontraditional 16Kb version for people with very little bandwidth and no real hearing.  If you could put up with it, you know, you'll save a lot of bits.  Many bits will not have to die for your listening.  He made it for Elaine Farris who does those wonderful transcripts of every episode.  All that's at GRC.com, along with the show notes.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#983

DATE:		July 16, 2024

TITLE:		A Snowflake's Chance

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-983.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How can content delivery networks be used safely?  What do we learn from the ransomware attack that affected 15,000 auto dealers?  Guess who uses an Entrust certificate and when it expires?  How worried should we be about Polyfill.io attack aftermath?  Whose side is Microsoft really on?  Let's look at their history.  How is GRC's new weekly Security Now! mailing going?  And what about feedback?  And, finally, the company named "Snowflake" was the epicenter of what has now become the largest series of corporate data breaches in history (and that's saying something).  Naturally there's been a lot of finger-pointing.  So who's saying what, and what appears to be most likely?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have some really interesting things to talk about.  An update on the Polyfill.io attack.  Can content delivery networks be used safely?  That ransomware attack against 15,000 auto dealers, what have they learned there?  And you won't believe who still uses Entrust for their certificates.  I'll give you a hint, it's got a .gov domain.  Finally, we'll talk about the Snowflake breach.  Steve's not sure he believes the stories.  He's going to get to the bottom of this one.  All coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 983, recorded Tuesday, July 16th, 2024:  A Snowflake's Chance.



It's time for Security Now!, the show you wait all week for, looking to collect the little tidbits in the back of your brain, at least I do, going, oh, I wonder what Steve's going to say about that.  Now here's your chance.  Steve Gibson is here, our master of ceremonies.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  Good to see you.



STEVE:  Great to be with you.



LEO:  Yes.



STEVE:  One of the final episodes we're recording with you in the Eastside Studio.



LEO:  In the old studio, yeah.



STEVE:  That's right.



LEO:  There'll be, let's see, there'll be two, three more because we're leaving August 8th.  So August 7th will be the last episode of Security Now! from the Eastside Studio, which has been - it's been a good run.



STEVE:  So that will be after This Week in Google on Wednesday afternoon?



LEO:  Yes, yes.



STEVE:  Will be the last recording.



LEO:  Then that's when I'm going to...



STEVE:  That will give you a few days, Thursday, Friday, and Saturday...



LEO:  To spread the accelerant, stuff the newspapers in the various crevices, light the flame.  No, we're not going to do that.  We're not going to burn it to the ground.



STEVE:  Okay.  So we've got a lot of fun stuff to talk about, and some - sort of some interesting takeaways, I think.  Today's podcast number 983 for this July 16th is titled "A Snowflake's Chance."  This of course named after the firm, and I don't know why they named themselves Snowflake.  That just sounds like a flaky...



LEO:  Exactly, yeah.



STEVE:  You know, it's not a good name.



LEO:  Now that you mention it.



STEVE:  Yeah, I don't know that I want to store all of my customer data at Snowflake.



LEO:  That's the one that has a chance in hell.  I mean, that's not a good name.



STEVE:  No.



LEO:  Yes.



STEVE:  No.  And at least 350 companies are now very sorry that they did store all their data there.  And, you know, Ticketmaster, of course famously now AT&T, with 110,000 of their customers' mobile phone metadata, and the people that use the AT&T system and which AT&T resells to.  Anyway, big disaster there.  Some interesting takeaways from that.



But we have a lot of other stuff to talk about.  The discussion last week of the Polyfill.io attack caused people to say, can content delivery networks be used safely?  Because of course the problem was that Polyfill.io the domain got purchased by a now-known-to-be-malicious Chinese company.  So what's to prevent that from happening elsewhere?  Also, what do we learn from the ransomware attack that took 15,000 auto dealers down a couple weeks ago?  And interestingly, I heard from three of our listeners who were directly impacted by this.



LEO:  Oh.



STEVE:  I mean, so this thing was widespread.  Also, guess who uses an Entrust certificate, and when it expires?  We're going to look at that.  Also...



LEO:  Oh, that's interesting.  Can't wait to hear that one.



STEVE:  Don't fool with the IRS.gov until you catch up on...



LEO:  Oh, oh.



STEVE:  Also, how worried should we be about the Polyfill.io attack aftermath?  That is, do we have to reformat our hard drives, or what?  And a listener really brought up an interesting view that I'm going to share about whose side Microsoft is really on by taking a look at their history.  I'm going to comment about how GRC's new Security Now! mailing is going.  And then, as I said, we're finally going to take a look at this company named, who knows why, Snowflake, which was the epicenter of what has now become the largest series of corporate data breaches in history.  And as we know, unfortunately, that's saying something.



LEO:  Geez.



STEVE:  So there's been a lot of finger-pointing, who's saying what, what appears to be most likely.  And always we have a fun, but puzzling, Picture of the Week.  I understand what's going on.  I've already had feedback from those who received Security Now!'s email a couple hours ago saying, what?  Anyway, we'll explain it.



LEO:  I haven't seen it yet.  We will see it together for the first time.



STEVE:  I think everyone is going to like - they're going to find these next two hours have been well spent.



LEO:  I think that's the case, as always, Mr. G.  And we have Adam watching on Facebook today.  Thank you, Adam.  It's very nice to have you.  You don't know this, but we are streaming - maybe you do know this, Steve.  I don't know.  We're streaming now everywhere.



STEVE:  But we're using Zoom.



LEO:  We are.  But we have a service called Restream that we've started using that's going to be - we used it a couple of weeks ago.  Remember I was pulling up chat, and it was distracting you?



STEVE:  Ah, so you could either use Restream for the conferencing or use Zoom as the frontend and then Restream on the backend.



LEO:  That's exactly what we're doing.  We're going to have Zoom ISO in the frontend.  We're going to have Ecamm, so the technical director, our producers will also be switching the show.  I won't have to do that anymore, which is interesting.  Actually we'll be doing less, I guess.  



STEVE:  Good, because those Restream fades were not good.



LEO:  No.  These will be much nicer.  But we do have Restream taking our video and putting it everywhere.  So it's still YouTube, YouTube.com/twit/live.  But it's now Twitch.tv/twit.  It's on Facebook.  It's on LinkedIn.  It's on X.com.  All over the place.  So Adam is watching on Facebook, and I guess hasn't seen the video or doesn't remember.  But he said, "What's that Speak & Spell right behind Steve there?"  But there's a story with that; right?



STEVE:  Yup.  I was involved in its creation.



LEO:  Isn't that amazing.  So that's one of Steve's products, I guess you could say.



STEVE:  Well, no.  Linear predictive coding speech synthesis was the technology used.



LEO:  Wow.



STEVE:  And that's part of what happened when I was at the Stanford AI Lab.



LEO:  It's kind of amazing.  I mean, here we are, what was that, 40, 50 years ago?  Fifty, I guess.



STEVE:  Yeah, it was '73 was when I graduated high school, and I was at the AI Lab for the couple years before that.



LEO:  It's kind of amazing.  And look where AI is now.  I mean, yeah, the sound of the voice synthesis in the Speak & Spell versus what ElevenLabs is doing now with celebrity voices, I mean, it's incredible.  We have lived in interesting times, Mr. Gibson.



STEVE:  One of our listeners, one of our female listeners took offense to my little rant last week about man in the middle and needing to call it, what was it, not attacker in the middle, adversary in the middle.  And she fed what I said into some generative AI, asking it, how could women find this offensive?  And it was astonishing.  I mean, I really - it put me in my place.  I thought, well, maybe you just need to go, we need to turn you out to pasture because you're obviously, you know, you need to have a serious sit-down with human resources.



LEO:  Or at least with ChatGPT.  Gemini wants a word with you, Mr. Gibson.



STEVE:  Uh-oh.



LEO:  I have the Picture of the Week right here on my laptop, and I am ready to pull it up.  Do you want any prelude?



STEVE:  So I gave this one the title, "Does anyone wonder how to lock this bathroom door?"



LEO:  Oh, my imagination is reeling.  Let us look together.  Apparently, no one knows how to lock this bathroom door.  There are one, two, three, four, five, six signs, all of which say, "Do not turn.  Push to lock.  Please."



STEVE:  Five of the six are in a san serif font of varying sizes, apparently having been incrementally added to the door.  Five of them are on the door.  One is over on the wall.  Somebody came - the one that is the serif font, which reads "Simply push to lock," where those four words fill the entire page, someone came along and gave it some extra underlining.



LEO:  Underlining, yeah.



STEVE:  In felt tip marker.



LEO:  Simply push.



STEVE:  So we have "Just push to lock.  Do not turn" with three exclamation points.  "Push to lock, do not turn," oh, that one was actually - that looks like it's pretty much the same - no, no, no.



LEO:  That's replicated.



STEVE:  No.  The first one says "Just push to lock."  The second one says "Push to lock."  So apparently that wasn't sufficient, so they added this "Just push to lock."



LEO:  Don't turn it.



STEVE:  Now, we actually have a paragraph down at the bottom.  "If you just push the button straight in, without turning, the door is locked."



LEO:  Now, this makes me wonder, what happens if you turn it?  In fact, honestly, being the rebel that I am, looking at these signs, I would turn it.



STEVE:  Apparently people do.  I know what's going on here.



LEO:  Oh, good.



STEVE:  If you push in - so we should explain, it's got sort of an L-shaped handle coming off to one side.  And if you push it down, the door will open, and you can leave.



LEO:  Yeah.



STEVE:  But the button is actually sort of a thumbscrew...



LEO:  It looks like you should turn it.  It's got a little affordance for turning it.



STEVE:  Oh, it wants you - it is saying "Turn me."



LEO:  Turn me, yes.



STEVE:  Everything about this.  So you push it in, and it stays in.  Then when you push the handle down, it pops out.



LEO:  Right.



STEVE:  But if you push it in and turn it, it locks the button in.



LEO:  It locks you out after you go through the door.



STEVE:  No, no, no.  Well, yes, exactly.  So what people are doing is, because this was a poorly chosen handle for a bathroom, they're pushing it in and turning it, thinking, okay, I don't want anyone coming in on me.



LEO:  Yeah.



STEVE:  When I'm in the middle of doing my business.



LEO:  Right.



STEVE:  But then they leave.  And because they twisted, the door locks behind them, and nobody can ever get in again.  Now, I don't know if residential home door jamb locks still have this.  But Leo, I know that when you and I were young, if you looked at the door jamb on the front door of your home, there would often be two buttons there.



LEO:  Yes.



STEVE:  And those two buttons would lock or unlock...



LEO:  Right.



STEVE:  ...the thumb lever on the outside of the door, the point being that it would keep the door locked from the outside, whether or not you locked it from the inside.  It's exactly this technology.



LEO:  I see.



STEVE:  Unfortunately, they made this much too easy to use.



LEO:  And they put the wrong affordance on it because they put a little turning thing on it, which you shouldn't have.  This, by the way, if you've not read this book, the great Don Norman's "Design of Everyday Things."  And the cover tells you all that you need to know.  It's a teapot with a handle on the same side as the spout.  Which obviously is poor design.  He talks about this all the time, doors that invite you to push it because they have a push bar, or worse, have a handle for pulling, but do the opposite.  It's very common; right?  So they have to put a sign up that says "Push, don't pull."  Because you pull it and nothing happens, and you look like an idiot.  So this is very common, and he says, "Don't blame yourself.  This is just poor design."



STEVE:  Well, and what this company, whoever they are, should do is just hire a locksmith or a doorknob person to come out.



LEO:  Fix it.



STEVE:  And put a button here.  I mean, because you can buy the same handle with a button, and then you don't have this problem.



LEO:  Yup.  And that's the other thing, Dave Redekop's saying...



STEVE:  And clearly, no amount of signage is going to solve this.



LEO:  No.  If you have - that's, you know, a little hint.  If you have eight or nine signs on the door explaining in the same way how to do it, the message isn't getting through.  It's just poor design.



STEVE:  Yeah.  And presumably somebody has to go when they come into this bathroom.  So they're not taking any time to, like, read the dictionary of proper knob operation.  There is some hurry.



LEO:  Well, Dave also - Dave points out that the other thing they may do is push it and then test the handle and inadvertently unlock it, which could also lead to embarrassment.  So...



STEVE:  Bad design.



LEO:  We need better design, better design, exactly.



STEVE:  Okay.  So using content delivery networks safely.  Looking back upon last week's "Polyfill.io Attack" topic, I can imagine that I may have come off as being very anti-third party when it comes to sourcing potentially dangerous content, such as code libraries, from third parties, you know, such as high-performance content delivery networks, CDNs. It was never my intent to rain on the idea, you know, the concept of CDNs in general for this purpose, because the web's designers have made ample provisions for safely pulling code into web pages from remote sites.  And a number of our astute listeners sent me notes asking variations of "Uhhh, Steve, did you perhaps forget about asset integrity pinning?"



Actually, no, I didn't.  But those questions also raised a very good point.  So rather than answering each of those notes separately, and since it's a terrific topic for this podcast to cover in the wake of the Polyfill.io news, I wanted to talk about how third-party content can be delivered safely, and why the Polyfill.io facility was never able to take advantage of that.



Okay.  So the formal name for the facility is "Sub-Resource Integrity," abbreviated SRI, where the concept and implementation could hardly be clearer, cleaner, and simpler, you know, as the best things are.  This is a win.  The same HTML <script> tag that contains the URL of some remote third-party code or stylesheet, because it could be used for link tags also,  where the URL is what the browser is being asked to remotely load, can also, optionally, contain another name/value pair, specifically integrity= and then a big quoted string.



The format is the word "integrity," followed by an equal sign, then any one of the prefixes "sha256," "sha384," or "sha512."  And our astute listeners will already be going, ah, I know what's coming.  What's coming is a dash, followed by the specified hash of the expected URL resource which has been hashed under the specified hash, then encoded from its binary into Base64 ASCII.



Okay.  So here's what all that means.  When a web designer wishes to pull some remote resource from a remote content delivery network, or really from anywhere, where they do not directly control that resource, they want to be absolutely certain that the resource they want has not been changed from what they expect to receive.  So they first go to the SRI hash generator site, very handy, www.srihash.org.  And this is formally specified like at the W3 Consortium.  Mozilla talks about it.  This is sort of like the reference site for generating these resource protection hashes.  Again, www.srihash.org.



Or you could OpenSSL or any other utility that can create Base64-encoded hashes.  The srihash.org site is handy since, when given a URL, it will fetch the resource for you, perform the hashing, perform the Base64 encoding, and return the snippet of code tag which already is set up to drop into your own web page to perform all of the proper matching.  It defaults to sha384, sort of the one of medium strength.  But really sha256 is plenty strong also, creates a somewhat shorter hash.  But, you know, these days web pages have become so out of control, no one cares.



Anyway, so our designer goes there, gets the hash, drops the URL of the jQuery library they wish to use into srihash.org to receive the hash.  Then they add this hash, along with that "integrity" keyword, into the jQuery fetching script tag in their web pages, and they are henceforth protected from any modification of that code.



When their page is delivered to a user's web browser, the browser reads the page's HTML, sees the <script> or <link> tags, and fetches the resource referred to by the URL.  But because that <script> or <link> tag also includes an "integrity" argument, before the browser does anything with the freshly downloaded resource, it takes its own hash of what it just downloaded.  Base64 encodes that and compares the result with the hash that follows the "integrity" keyword.  And only if the hatches match will the browser allow that code to enter the browser's inner sanctum to be trusted and used.



So what all of this does is very nicely and cleanly allow web designers to protect their sites' users from both inadvertent or deliberate alteration of the resource that they're requesting.  Also note that since code libraries are constantly evolving - jQuery is currently at, for example, v3.7.1.  I noted that the jQuery my email system is using, I think it's 3.6.0.  So jQuery is a moving target.  Because of that, it's also necessary for the jQuery or whatever library specification to indicate the exact version that is being desired so that the hash will match.  Since the CDNs will always continue to offer all older releases, a site will continue to use that version, the one that's known to work, until its designer changes the version number in the URL and obtains that newer release's matching hash to add to the invocation tag in the HTML.



Okay.  So we now see how it's completely possible to safely obtain potentially dangerous script code from any other service that the designer does not control.  All of the advantages a CDN has to offer, like nearby points-of-presence so you get super-fast content delivery of potentially large content without delay, that can all be used without any risk at all, by specifying the hash that you expect that content that you receive to have.  And the browser itself will just say nope, and will not use it



But as I said earlier, unfortunately, this very slick protection was not available to users of Polyfill.io.  I touched on this briefly last week; but since it wasn't our focus, I didn't elaborate or highlight its significance.  So just now when I was putting the show notes together, I used our trusty Web Archive's Wayback Machine to show the Polyfill.io website home page.  They were clearly - they, the Polyfill.io people, when they originally created this were clearly very proud of what they'd created.  And they were a little tricky.



The home page of the site says:  "Just the polyfills you need for your site, tailored to each browser."  And then they said "Copy the code to unleash the magic."  And what we see is a script tag with a, you know, src= and then a URL.  Which doesn't have any specific subversion numbering.  It says v2, but otherwise no.  Polyfill.min.js.  Then it says:  "Polyfill.io reads the User-Agent header of each request and returns polyfills that are suitable for the requesting browser."  They said:  "Tailor the response based on the features you're using in your app, and see our live examples to get started quickly."  In other words, they are customizing what they return, depending upon the browser the user is using.  No version numbers.



So the way the Polyfill.io site always worked was that it generated and delivered custom polyfill JavaScript code specifically tuned to the make, model, and version of the web browser each individual user was using.  And Leo, I'm going to need you in about a minute.  This always made it actively hostile to...



LEO:  I won't leave.



STEVE:  ...to the web subresource integrity system, which prevented any of the Polyfill.io's great many users from supplying a hash code which they would - because they would receive entirely different code each time.  Anyway, that said, our listeners were 100% correct to point out the power and value of subresource integrity protection.  It's been universally supported by every web browser for many years, so it's something that all web designers whose web pages are pulling code, which should never change without notice from any third party, should be adding to their bag of tricks.



So I did want to just bring it to everyone's attention, that very cool website that I mentioned.  Srihash.org will give you the hashes and the code to drop in to any remote resources you're pulling.  So it is entirely possible to protect yourself.  So again, I thank our listeners for bringing my attention back to that.  I wanted to make sure that I explained.  And it's absolutely possible to safely pull remote content from CDNs.  Not from Polyfill.io because it was always delivering customized code.  



LEO:  Yeah.



STEVE:  Otherwise, yes.



LEO:  Interesting.  Yeah, I mean, how often do we install stuff that says, oh, just copy and paste this line that will download and install software.  And I, you know, there's a program I put on every Mac called Homebrew.  And that's how you're supposed to install it.  You know, I mean, you could look at the bash script, I guess.  But it's just - it's very nerve-wracking.  You nailed it, though, last week on talking about the Polyfill.  In read, I wanted to mention that in our TWiT Forums at TWiT.community, Pseudorandom Noise said:  "Another great episode.  And as a web developer, it was nice to hear how incredibly correct Steve was and how well he understands the subject matter."  So I just thought I'd pass that along.  People really appreciate your deeper understanding of this stuff.



STEVE:  Thank you.  



LEO:  And I guess because you're a coder, you know, you know about libraries, and you know how tempting they are to load.  And as it turns out, thanks to supply chain attacks, how risky.



STEVE:  Well, yeah.  And, you know, I'm a bit of a web developer myself.  I don't have, as everyone knows, a fancy website that's taking advantage of all kinds of...



LEO:  You're not using React or anything.  But, you know, it's pretty good.  It's nice.  It looks good.



STEVE: I'm not even using JavaScript.



LEO:  It's all text.  All HTML.



STEVE:  All of the - that's right.  Even the GRC's web, I mean, our website menus are just using CSS with no scripting in order to perform their magic.



LEO:  Nice.  So that's all you need, frankly.  That's good enough, yeah.



STEVE:  It works.  But let's take a break because the next chunk is going to be a big one, about this massive CDK Global ransomware attack that impacted three of our listeners.



LEO:  Oh, dear.  And now another breach.  Steve?



STEVE:  Ah, yes.  Toward the end of June I heard from three of our listeners whose lives have been affected by a recent major attack on a very large automotive dealer network.  On June 21st I received two notes from our listeners.  First one said:  "Hello, Mr. Gibson.  My name is Shawn, and I'm an automotive technician at a GM dealership and have been listening since about 2016."



LEO:  Oh, I know what he's going to talk about.  Oh, I know what he's going to talk - this hit my dealer, too.



STEVE:  Yup.  Yup.



LEO:  Okay.  Okay.



STEVE:  He says:  "I love when your world of security crosses over to my world of automotive.  My dealership, as well as thousands of others" - yes, 15,000 of others - "is affected by the CDK cyberattack that happened yesterday.  When the details come out, I would love to hear your take on it.  This is the first time a cyberattack has had a direct effect on me."  He said:  "(We get paid by what we do, and this is slowing everything down as we have to go back to manual ROs and quotes, lowering my booked hours)."



LEO:  Oh, that's too bad.



STEVE:  "Thanks.  Shawn."



LEO:  Yikes.



STEVE:  And then we actually heard from, apparently, an owner of a dealership.  On the same day, on June 21st, I received:  "Steve, thanks for all the years of podcasts.  I've been a listener from the beginning and a watcher from the Tech TV days. I hope to hear some coverage of the CDK Global incident.  Sales of auto repair parts from the dealer side of the industry have come to a screeching halt as they're unable to create invoices nor tell us our cost for a part.  I was told today from one dealer that they hope to be able to sell me parts next week with some form of paper invoice."  Paper, imagine that.  "The delivery box truck that stopped was almost empty today."  And he said:  "I only got my parts today as they had already been ordered and invoiced prior to the issue.  Thanks.  Signed Alan Alberg of Alberg Auto."



And then finally:  "Hi, Steve.  I just found out about this this evening.  Our son and daughter-in-law are both remote workers for a dealership network that has been brought to a standstill by this cyberattack.  USA Today is reporting 15,000 dealerships across the United States are affected and may not be back online until the end of the month.  Color me cynical, but I'm fond of the saying, 'There is no cloud.  You are just someone else's computer.'  Usually that other computer is better secured than your own.  But as you so frequently say, 'It's not a matter of IF, but WHEN.'  I appreciate the work you and Leo put into the podcast each week.  Best regards, Richard in Clemmons, North Carolina."



Okay.  So what's the scoop on this?  We have a situation where 15,000 operating dealerships were dependent upon a single MSP, a managed service provider, also sometimes referred to as SaaS, or delivering, offering SaaS, Software as a Service.  The dealerships were dependent up on this for all of their, I guess we still call it "paperwork" processing, though it's virtual paper.  We'll be talking more about Software as a Service when we get into today's discussion of the Snowflake disaster.  But in this case a Russia-based drive-encrypting ransomware cyberattack took down hard the entire network of 15,000 auto dealerships which needed that network to operate.



I found a terrific piece posted on Medium by someone who's been in the auto industry and writing about it for some time.  Speaking from her long experience, Kathi's headline is:  "The CDK Cyber Attack Recovery Will Fall Squarely on the Accounting Office."  She writes:  "During my first years in the car business, I wore a lot of hats in each job position I had.  The one thing I learned early is that the accounting office staff are often the cleanup crew when several types of problems arise.  There are still systems and procedure hiccups that happen today, but thanks to technology and automation they're fewer in number.  Then came the CDK cyberattack.



"This CDK cyberattack is on a whole different level.  This breach is a very different type of problem.  But in the end, when things begin to settle, which may take months, it will be the accounting office who will be tasked to gather the thousands of dealership puzzle pieces from sales, service, and parts, and methodically match them up together to form some semblance of financial order.



"The 'End of the Month' is here."  So she was writing this at the end of last month, June.  She said:  "New car dealerships are required to produce a monthly financial statement as mandated by the manufacturer and certain lenders.  It's unclear as of this writing if a June financial statement will be available.  I would say the chances are slim.



"So why did the CDK cyberattack happen?  There was once a company called ADP Dealer Services who were a great DMS provider.  DMS is Dealer Management System," which is the generic term in the industry.  "They got rolled into a company called Cobalt that sold mostly digital marketing services.  Then all of that got rolled into CDK Global, and with that came" - yeah, wait for it - "private equity investments."  Now, this is her speaking, not me.  And I'll address this a little bit later, in a second.



She said:  "The first thing to get cut when private equity rolls through the front door is 'cost centers,' and Infosec (aka: Information Security) is viewed as a cost center.  The main people who defend the gates of the village (the company) from the barbarians (the hackers) are the first sent off to exile.  When there's a ransomware attack, it's revealed with clockwork-like precision that no one has tested the backups for six months, and half the legacy systems cannot be resuscitated.



"As a cybersecurity expert told me last week, a few days after the attack happened:  'It's been at least two days since the ransomware attack with no fix in sight,' which tells me," he said, "a few things on this list have to be true.  A, they have no backups.  Or B, if they do have backups, they're outdated or never tested, which is effectively the same as having no backups.  C, no one knows how to restore the backups.  D, there's no disaster recovery plan; or if it exists, it's outdated to the point of uselessness.  E, multiple single points of failure are baked into the infrastructure.  Or finally, F, they have no idea how compromised they are."



So she says:  "I'm very angry about how ADP Dealer Services, once a great company, has been raped and pillaged by private equity.  The real pain is suffered by the rank and file at the dealerships, who still have to care for customers and sell to make a paycheck.  According to recent reporting, CDK will be paying the tens of millions of dollars in ransom."  And I've got some follow-up reporting on that I'll share in a second.  So I'll just note that information security doesn't seem like such a waste of money now, at this moment, does it.



So she says:  "How did the CDK cyberattack happen?"  She says:  "CDK is an ancient program.  Not a lot has been done to upgrade the original version for decades.  This is standard operating procedure when companies or private equity buy legacy companies.  Innovation is not the goal.  They slap on a new paint job or buff out the dents, and package it as the 'new improved version' that is always more expensive, but 'worth the investment.'  Ask any dealer how they feel about CDK and other DMS fees these days.  These corporate raiders' goal is to cut costs at all costs.  And in this debacle, it's clear they stripped the car for parts and left the data vulnerable to cyber criminals."



Now I'll interrupt to just say, as we know, it's very difficult to completely protect any large organization from intrusion.  But her earlier point about recovery is unassailable.  Any organization today whose survival would be threatened by a significant protracted network outage should certainly arrange to get back on the air after any attack.



Anyway, Kathi continues:  "Theoretically, a mature Dealer Management System provider should be able to lose any single critical part of their core business and be able to restore functionality within 24 hours, barring a massive natural disaster or personnel losses.  Instead, they have no backups, no redundancy, no separate servers, and no siloed databases which, when lost, are a pain to retrieve, but at least it's only one silo and not the entire client roster of 15,000 locations.



"How does a dealership restore their records once the breach is contained?  Once CDK pays the ransom, it may take weeks or even months to get all the data in order after they receive the keys to the ransomware," she writes.  "The database will likely have holes in it that will add to the arduous restoration process."  She says:  "There's been a lot of talk online about just getting a new DMS vendor.  While that seems like a good solution, the problem is that your data is being held hostage by whoever attacked CDK.  Without the data, you have nothing to convert to the new DMS.  But the idea of other DMS solutions is a good one that should be explored once the dealership's CDK records are restored.



"When the dealership comes back online, that's when the fun starts for the Accounting Office.  During the outage, all employees continue to serve customers to the best of their ability, using manual documents and a patchwork of software support."



LEO:  A bunch of paperclips.



STEVE:  Exactly.  Like, well, like remember when we're in a restaurant, and their credit card processing goes out.



LEO:  Yes,



STEVE:  And it's like, uh, whoops.  Yeah.  So she says:  "When operations is functional again, all the business they produced  new and used car sales, service, parts, internals, warranty  anything that happened during the downtime will need to be assembled and manually input into the system.  It could take a few weeks or a few months to match everything up, and it will be a lot of work just to get back to 'normal.'  Organization is key.  If it's a busier store  think 150-plus cars per month or over $500K in monthly service labor  it will take a considerable amount of time to input due to the sheer volume of transactions.  Vehicle inventories  new cars, used cars  will need to be counted to verify every unit's whereabouts.  Parts inventory should also be verified unless the store had some kind of redundant system that kept track of it during the outage.  Untracked inventories are ripe for theft.



"If all the manual input goes well, and I do mean 'if,' she writes, then all entries should land in their respective general ledger accounts.  Schedules and other general ledger reports should be run to determine what it all actually looks like, and to make sure all the monies that were collected are posted to their respective accounts.  One surefire place to start is bank reconciliation. If you can balance your books to your bank, you'll have a roadmap to a decent amount of checks and balances.  It will not be pretty.  But with the always-present perseverance of dealership accounting office staff" - sounds like she was once a dealership accountant.



LEO:  Oh, this is definitely a post-traumatic stress syndrome from somebody who's been on that front line, for sure, for sure.



STEVE:  Exactly.  She says:  "It will ultimately come together."  And she says:  "I'm just so appalled that this event happened.  When I first heard about it, I said to my colleagues:  'In what universe is it okay to manage data in such an irresponsible way?'"  And I'll have something to say about that, too, in a second.  She said:  "Most dealership employees have never had to perform their job without the use of technology.  It's a strong reminder that technology is only a tool for efficiency, and it's only as good as its infrastructure and established crisis protocols.



"There will be lawsuits, of course.  The only question is how many and from whom.  Certainly I would expect claims against CDK from" - and she names three:  "Dealers for impeding commerce and negligence in data loss, among other things; consumers for the massive data breach of extremely sensitive information; and employees for data privacy and lost compensation."  She says:  "Now is a good time for dealers to contact their Cyber Liability Policy carrier.  Check to see if you have Contingent Business Interruption coverage and put the carrier on notice.  No need to file a claim just yet, but it's worth having a conversation to know if you're covered and for how much."



Okay.  So that's Kathi's take.  In subsequent reporting, CNN Business reported, under their headline "How did the auto dealer outage end?  CDK almost certainly paid a $25 million ransom."  So CNN wrote...



LEO:  Oh, my god.



STEVE:  Yup, 25 million to get back online.  CNN said:  "CDK Global, a software firm serving car dealerships across the U.S. that was roiled by a cyberattack last month, appears to have paid a $25 million ransom to the hackers, multiple sources familiar with the matter told CNN.  The company has declined to discuss the matter.  Pinpointing exactly who sends a cryptocurrency payment can be complicated by the relative anonymity that some crypto services offer.  But data on the blockchain that underpins cryptocurrency payments also tells its own story.



"On June 21st, about 387 bitcoin  then the equivalent of roughly $25 million  was sent to a cryptocurrency account controlled by hackers affiliated with a type of ransomware called BlackSuit.  A week after the payment was made, CDK said that it was bringing car dealers back online to its software platform."  They write:  "Cryptocurrency allows for the exchange of digital assets outside of the traditional banking system, but a record of those transactions is accessible on the blockchain.  Three sources closely tracking the incident confirmed that a roughly $25 million payment had been made to BlackSuit affiliates, and that CDK was very likely the source of that payment.  Those sources spoke on the condition of anonymity because of the sensitive nature of the investigation.  The cryptocurrency account that sent the ransom payment is affiliated with a firm that helps victims respond to ransom attacks, one of the sources said, declining to identify the firm."



Okay.  So will the payment of that $25 million affect CDK's behavior going forward?  Who knows?  The greatest cost is likely their reputation damage.  We've previously seen the consequences of MSP's - managed service providers - being penetrated to allow malicious attacks against their clients thanks to the MSP's access into those clients' networks.  But that's not what happened here.  The problem here was that 15,000 auto dealers had come to so depend upon the networked services provided by their massive MSP - and I'm sure that was both at the MSP's urging and the dealer's willingness to avoid redundant work - that when that MSP was taken down by a ransomware attack, the second-order consequences were so widespread that at least three listeners of this podcast were directly affected and wrote to me.



Whether or not this was a consequence of profiteering by private equity owners, who stripped the organization of what they felt were excessive cost centers, is irrelevant here.  We've certainly seen many organizations attacked with devastating consequences when their owners were fully invested in their company's success and infrastructure security.  And as we know, mistakes happen.  Could profiteering ownership have been a contributing factor?  Okay, sure, maybe.  But we would need to have much more information about CDK Global's history to render any judgment about that.  The point Kathi made in her article about there needing to be some explanation for the fact that CDK Global was unable to recover immediately without paying a multi-million dollar ransom, that was certainly a good one.  But we have no idea what's going on behind the scenes and whether, you know, and what one way or another was the case.



And to my mind that's really beside the point.  What I think we have here is another consequence of a theme we saw last week with Polyfill.io, where so many websites were pulling unverifiable code from a central source.  This is another example, and just wait till we look at the Snowflake disaster in a minute.  One way to describe all of these widely different problems would be as the danger of the promise of a free lunch. Or stated another way:  "It's very rare that you get something for nothing."



Remember that xkcd cartoon we showed last week, where a massive construction of blocks was ultimately resting on an endangered twig?  In the case of the CDK Global MSP outage, 15,000 auto dealerships had become dependent upon this single service provider for virtually all of their daily operations.  And it's entirely human for this to happen over time if CDK's service had been so reliably delivered for so long that the maintenance of any "backup plan" in the event of a CDK service outage seemed entirely redundant.  For all we know, there were such plans in place 15 years ago.



But staff changed, people who knew how to fall back to a manual system retired and left the dealerships, and new hires were only trained on and knew how to use the automated system.  You know, "Just press this button and follow the onscreen prompts."  So what gradually grew over time was a deepening dependence upon this miraculous new system that had, after all, demonstrated to be dependable enough to be depended upon, right up until the day its plug got pulled.  And without it, a massive network of auto dealerships were marooned.



Kathi was correct in her prediction that class action lawsuits would be filed against CDK.  Some already have been.  And I think that's unfortunate because the whole truth is, this sort of free lunch failure is what comes with the territory.  Class action lawsuits after the fact, when the free lunch needs to be paid for, is just sour grapes.  Having tasted and grown accustomed to the power of the service provided by CDK, it would be safe to predict that not a single dealer is going to return to a manual in-house operation.  Was the pain that great?  No.  Not nearly enough.  Might some switch to an alternative provider?  I'd bet that even that is rare.



Everyone is breathing a huge sigh of relief, with the network and the automation that it provides coming back up, and business resuming as normal.  CDK's CEO has apologized.  He's promised to improve their cybersecurity posture and has even offered some financial restitution to their 15,000 dealerships for the loss of sales and service revenue that they suffered.  And life goes on.



So the message I'd like to take from this perfect example of what can go wrong is that, in the final analysis, it's all worth it.  I don't take the opportunity to remind us of that often enough.  You know, we're only doing all of this cyber stuff because it really does make sense.  It really is phenomenally powerful.  It really is improving people's lives.  Sure, there's a "two steps forward, one step backward" sense.  And that faltering backward step can be painful.  But the net effect is still one step forward.



This still doesn't mean that a truly massive catastrophe is not possible.  From all the evidence we continually see, we can feel the very real possibility of that in our guts.  And following from my analogy last week of hoping for minor earthquake tremors, the hope is that other competing DMS (Dealer Management System) providers are looking at what just happened at CDK and shuddering, while suddenly feeling better about the size of their own information security budgetary line item.  And they ask their IT staff with renewed attention whether they are safe from the same thing happening to them; and, if not, what more do they need to do?



Thanks to the network effects of this event, a great deal of press coverage and attention was given to this.  So let's hope that some lessons were learned to better prepare other similar organizations to respond if it should happen to them.



LEO:  But what lessons can be learned?  No dealership is going to turn to a tool that they create themselves, nor would that solve the problem.  CDK I'm sure says, well, we're going to make sure we're more secure.  But are they, and will they, and can they?



STEVE:  I don't think it matters, Leo.  I think that dealerships are saving so much by using, by basically subcontracting out, I mean, the very fact that they were completely crippled demonstrates how much of the work of...  



LEO:  How useful it is, yeah.



STEVE:  Yes, how much of their work CDK was successfully automating.



LEO:  Right.



STEVE:  So yes, this was not good.  They were inconvenienced.  You know, like the restaurant whose computers go down, and they have to take orders by hand and process their customers' credit slips.  It's inconvenient.  But do they stay on paper afterwards?  You know, they recover, and life goes on.



LEO:  Yeah.



STEVE:  And that's really the message I wanted to convey here is, yes, this was big.  This was awful.  But ultimately this was a minor earthquake tremor.  This is what we want.  We want to remind people these things can fail.  And failure, we would like it not to happen, but it does.  And but even so, when you step back from it, all the dealers are going to stay with CDK.  They're going to say, well, the CEO says they're going to improve their security.  And look, we got a check that didn't begin to cover our losses, but at least it's something.  And besides, it would be far too painful to have to train our staff how to do this themselves if we didn't have automation.  And we don't really want to move to a different DMS provider because all of our stuff is here.  So look, it's working again.  The lights are on.  Let's just move forward.



LEO:  Yeah, and I don't even know what you could do.  It's, I mean, it's almost like saying, well, don't use computers.  You know, go back to that paper system.  That never crashes.  Nobody's going to do that.



STEVE:  Right.  Now, I mean, I guess the only thing you could do, if you really cared, would be to go to the expense of running your own system, and not using a Managed Service Provider.  However, there may very well be significant advantages from being tied into CDK.  There was some stuff Kathi talked about that was so deep in the weeds that I didn't include it.  But it was - it had to do with CDK's stature as a preferred vendor to auto manufacturers.  And that gave them some privileged status which all of those 15,000 dealers inherited as a consequence of using their dealer management system.  So they're actually, you know, there actually is benefit that an independent dealer being entirely independent, even from automation, would not have.



LEO:  Yeah.  And you can't even really fault CDK.  I mean, I wish they - I almost want to say it's too bad they paid the ransom.  But there was no alternative.



STEVE:  No.



LEO:  That was the cheapest way of them getting those desperate dealers back online.  



STEVE:  Yes.  And we can fault them for not having, I mean, they're probably faulting themselves for not having the infotech, the information technology security, to deal with this.  They've been around a long time, for decades, before ransomware became a problem.



LEO:  Right.



STEVE:  I'll bet you this is pure inertia.  They just hoped it would never happen to them.  And they were using backup technology and security that was a decade old.



LEO:  Yeah.



STEVE:  So when this thing took them out and encrypted their servers, they were...



LEO:  They were unprepared.



STEVE:  To use the phrase that's common these days, screwed.



LEO:  All right.  We'll have more in a bit.  When I ask, when I say, well, what can you do, there are things you can do.  Just listen to our sponsors.  I mean, this is specifically why advertisers come to Security Now!, because they're trying to reach out to companies like CDK, saying before it's too late.



STEVE:  So a listener, Knox North, he said:  "I listened to the Entrust story with interest, even though professionally I use DigiCert.  I figured I'd never encounter Entrust.  But I went to https://irs.gov, and guess who issued their cert?"



LEO:  No.



STEVE:  Now, Knox's observation made me curious, so I went over to the IRS.gov website to see for myself.  First of all, sure enough, the IRS has been purchasing its websites' TLS certificates from Entrust.  Presumably that will end.  But what caught my eye was exactly when it will end.  The certificate that's presently being sent to any visiting web browser is displaying a "not valid after" date of October 26th of this year.



Now, we might expect Entrust to attempt to renew any certificates they can before the Halloween drop-dead date.  And since the IRS's current certificate will need renewing before October 26th, it will be interesting to see whether they remain with Entrust, as they certainly could for another year, or whether a policy somewhere deep within the bureaucracy triggers a change.  We'll see.  And we won't have long to wait because by Halloween they will be recertified.  We just don't know who will sign their certificate.



Jonathan said:  "Hello, Steve.  I found a connection from my iPhone to one of the polyfill-related domains, cdn.staticfile.org.  There was one look-up in my NextDNS logs on June 24th.  Obviously it would be difficult, if not impossible, to locate the source of the lookup on an iPhone.  I looked for information on how to respond to this potential compromise, but all I can find is information for site operators, you know, remove dependencies on polyfill.  I see no other connections to the known indications of compromise domains in my logs.  What would you recommend at this point to make sure I'm not hacked?  I'm thinking of wiping and reinstalling the OS, a backup, or starting fresh.  Thank you."  And he says:  "From an undisclosed location near Washington, DC."



LEO:  Oh ho, it was the President.  Okay.



STEVE:  So it's 100% true that we don't know what we don't know.  And the reason the Polyfill.io event was so significant was mostly how bad an attack could have been.  But all indications are - again, within what we know - that for whatever reason, Funnull chose to only use this immense power they had to launch highly targeted and selective attacks against users of mobile devices who were selected by the make and model of the handset they were using and only when visiting specific websites.



Funnull's missed opportunity is the massively large bullet that we appeared to have dodged.  Funnull may have imagined that their hack would never be discovered, so they may have been in no hurry to do more damage.  And they likely figured that as long as they continued to deliver the proper polyfills to nearly everyone who asked, their deception would go unseen.  So my point is, based upon everything we know, the actual likelihood that you, Jonathan, or I, or anyone would have ever been subjected to Funnull's malicious code truly seems vanishingly small.  You know, I'm an avid iPhone and iPad user, and I haven't given it a second thought.  None of the forensic analysis that's been done after this was discovered has revealed any more than those very tightly targeted attacks.  It may have only ever been a handful of users who got this malicious Javascript.



But also Jonathan asked:  "What would you recommend at this point to make sure I'm not hacked?  I'm thinking of wiping and reinstalling the OS, a backup, or starting fresh."  There's no indication that the malicious JavaScript, even if the targeting happened to match with you and somewhere you went, was exploiting a vulnerability in the platform you were using.  So when we say it was malicious, we don't necessarily mean that it was exploiting a vulnerability.  It's almost certain that even in those who were penetrated, nothing about their browser or OS was ever compromised.  That's a whole different end.  The attack would have just used JavaScript code running in the browser at that website to, whatever, grab their login credentials or their  browser's session cookie to impersonate them, or something of similar value to the attackers.



As I noted last week, since the browser was loading the Polyfill.io code in the browser's first-party context and giving it access to the browser's DOM - you know, the Document Object Model, the webpage's guts - that code could do whatever it wished, but probably only within the bounds of what any JavaScript code could do.  In other words, your browser and OS would not be damaged at all.  Therefore, first of all, incredibly unlikely that you ever actually received any malicious JavaScript; and, even if you did, especially on an iOS device, vanishingly small chance that a compromise was required.  It just wasn't necessary in order to probably get what they wanted, which would have been login credentials or a session cookie, something like that.



Okay.  From Bud in West Virginia.  I want to share a longer-than-usual piece of thoughtful feedback from a listener of ours.  By taking a fact-based look at Microsoft's actual past delivered behavior, he makes what I think is a factually supported case for Microsoft clearly placing their own profit well ahead of the needs of the users of their Windows desktop.  And while, yes, okay, maybe that's obvious to all of us, the conclusion that he draws and what I think he predicts is worth looking at.  So I'm going to share first what Bud wrote, and then I'll discuss it.



So he says:  "Hi, Steve.  I realize this is a bit long and tried the best I could for brevity.  When I first heard about Recall, I thought it could be a useful tool, but also expected it to be a mess.  So far I'd say that's accurate.  And after listening to your coverage about Recall, I think it's going to be even worse than I originally thought.



"You've said multiple times recently that Microsoft has not shown malicious intent.  But I believe that they have.  Let's look at three Microsoft products, and then I'll share my thoughts about Recall and how it might be what finally makes me switch everything I'm responsible for away from Microsoft products and services.  Yes, I believe it's that bad.



"First, let's look at Windows 10.  I tend to be an early adopter," he writes, "and Windows 10 was no exception for me.  When it was released, I was working in a small IT services company with customers in small business, local government, and home end users.  As for Win10 upgrades, some people didn't want change, and some couldn't change due to a dependency on something not supported by Windows 10.



"Microsoft's rollout of Windows 10 basically went like this:  First:  'Hey, Windows 10 is a free upgrade for 7 or 8.'  Then:  'You haven't upgraded to 10.  Let's schedule it.'  And then:  'I'm going to schedule the upgrade unless you click in the fine print,' which he says tricked a bunch of users to upgrade.  And finally, no notification, no choice, some users went to bed with Windows 7 or 8, and on their computer they woke up to have Windows 10."



He says:  "Okay.  Next let's look at OneDrive.  Last year, Microsoft started asking users to back up their desktop and other folders to OneDrive.  Then after saying no, some users found that when trying to delete something from their desktop, they'd get a message stating that items deleted from OneDrive could be recovered.  Microsoft has now started asking in Windows 11 initial setup, but then turning 'ON' folder backup even if the user selects not to."



I'll just note that I've been listening to Paul Thurrott lamenting this more recent behavior of Microsoft's which has also been driving him crazy.  Microsoft is, indeed, ignoring these settings even when they are arguably privacy oriented and should be entirely within the user's control.  But as Paul keeps saying, they're just ignoring him.



So Bud continues:  "And, finally, the kludge (wonderful word) that is Microsoft Edge.  Microsoft Edge (Chromium) started off as a great browser."  He says:  "I used it for a few years.  But now it's so bad that I'll use literally anything but Edge.  There's too much content here to choose from so I'll just choose the latest that has impacted me."



He says:  "I've worked in DevOps for several years and redeploy Windows VMs often.  The startup screens for Edge have over the past couple of years gone from 'Please sign into your Microsoft account' to this infuriating mess."  And he has four points.  "Sign in to your Microsoft Account."  Then, "Let's sign in to your Google account to pull in that data."  Then, "We're scheduling to pull data from other browsers on a regular basis," which he says is "enabled by default."  And finally, "Let's make your experience better for you," and he says, "really meaning better for Microsoft to track you and target you with ads."



And he says:  "Finally, the coup de grace.  After already turning off the 'Let's make your experience better' setting, opening the browser sometime later will open a small notification that Microsoft has made your experience better anyway.  And if you don't like it, go to Settings and change it, again."  He says:  "Every time I get that notification, I'm already typing, and some key hits OK. If you aren't paying attention, you could easily miss it.



"So how does this all come together and apply to Recall?  Microsoft has clearly demonstrated that they can AND WILL pressure, trick, countermand, and/or silently change settings to what will benefit them.  And Microsoft has heavily invested in AI and needs some return on that AI investment."  He says:  "I don't think they're just setting the stage for an ad-supported version of Windows.  They are going to want all Windows systems to have Recall enabled so they can have hundreds of millions of computers that can be targeted for advertisings.  And everything the user does, not only web browsing activity, will be monitored.  It will be everything they do.



"Microsoft needs that AI return on investment, and this is how I think their deployment will likely go.  First:  'Hey, Windows Recall is ready, and you should turn it on now.'  Then:  'I'm going to go ahead and enable Recall.  You can disable it in Settings.'  And finally:  'Windows Recall now works on non-Copilot+ PCs!  Let's enable it now!'"  He said:  "And when all that isn't enough, Microsoft will just silently enable it anyway and people won't know until search exclaims 'Your search is now enhanced by AI and Windows Recall!'"



He finishes with:  "I sincerely hope Microsoft does not take this path.  But given their track record as I've outlined it above, I think it's all too likely."  He says:  "I'm in the market to replace an old laptop and would love to get an EliteX-based system. But I'm waiting until Linux is an option for it and for AMD's next-gen system to be released because I simply do not trust Microsoft to be content with leaving Recall disabled.  They have an established history of breaking the workarounds."  And he says:  "Looking at you, Edge!"



Then he says to me:  "I'd like to hear your thoughts on this review of Recall and Microsoft's intentions given their history. Thanks again for all you and Leo and the other hosts do to provide great shows every week.  Signed, Bud."



And I have to say it's difficult to argue with Bud's assessment.  As I said at the start of this, he makes a strong evidence-based case for what Microsoft seems very likely to do with Recall in the future.  One thing we do know is that it's been very clear from everything they've said that they are very determined to push Recall onto the desktop.  Which really does beg the question, why?  What's in it for them?  Why is Microsoft so anxious to push everyone into using Recall, if it's just to give us better search?  That doesn't really track.  I've heard from other listeners whose opinions more align with Bud's in this regard, so I wanted to share Bud's well-reasoned perspective.  And I will reiterate that should or when this comes to pass, I will make a definitive Recall blocker available as a piece of lightweight GRC assembly language freeware.



And one last note.  Thomas Tomchak said:  "I have a disproportionate amount of joy for you having a newsletter via email.  Thank you for putting the work in to make it happen and to do so on your terms."



LEO:  Aw.



STEVE:  And so Thomas, I just should say I just chose one quote.  I'm getting constant approbation from our listeners who are just delighted to receive GRC's weekly emailing in advance of this podcast.  I did have, for what it's worth, a problem this week which was interesting.  The emailing contains a thumbnail of the Picture of the Week, which as we know was that crazy bathroom door lock issue.  About 80 or so of our recipients had the email bounce, claiming that it contained a virus.  Apparently ClamAV had a false-positive match on the binary of the jpeg and thought it was malicious.  And so 80 of our listeners, although that's 80 out of currently 6,471 who have signed up for the...



LEO:  Conveniently, almost exactly one-eighth.



STEVE:  Yup.  So about - anyway, I just wanted to say, for those of you who did not receive it, though you are signed up, it's because your email provider is using an antivirus system, ClamAV, which identified the thumbnail as being a virus.  So it bounced.  I had, like, two last week, and 80, something like 80 this week.  So, and I'm using the same template.  So, like, nothing changed except a different picture.  And the only reason I can see that AV would trigger would be when you look at the body of the email, it is a binary blob because that's the jpeg thumbnail.  And, you know, I could omit it, but it's fun to have that picture in email.  And other people get to have pictures in email, so...



LEO:  Why not me?



STEVE:  I'm just going to chalk it up to hopefully a rare occurrence.



LEO:  One 80th, of course, not one eighth.



STEVE:  I think you'll get email.



LEO:  Yeah, one 80th of your entire group.



STEVE:  Right.



LEO:  Let us break, and then the meat of the matter, the Snowflake's Chance.  We'll talk about the Snowflake breach.  Who's at fault for that after all, the AT&T breach?  Okay, Steve.  Let's talk about Snowflake.



STEVE:  Okay.  So there's undeniable logic in the proposition that a third-party organization specializing in some aspect of business operations can, within a limited sphere, do a better job and a more cost-effective job than a company whose business is not doing that.  So the idea of farming out to a subcontractor some chunk of work becomes appealing, when that's not your business's main focus.



Like, for example, when a building is being built, you use a subcontractor who specializes in laying foundations to do that work.  You don't ask your painter to do that.  And the commercial plumbers install the plumbing, and the HVAC guys run the air ducting and install the equipment on the roof, and so on.  So from a theoretical standpoint the model is sound.  It can and has gone wrong, of course.  If a contractor is discovered to be doing substandard work, it's certainly prudent to go back and look at the previous buildings they worked on to determine whether those might also have been impacted.



Now, as we know today, the "Cloud" is all the rage.  I've told the story of participating in a DigiCert customer summit seven years ago where all of the other techies looked at me like I had two heads when I casually mentioned my rack of servers at Level 3, one of them saying to me, "Steve, no one does hardware anymore."  No one does hardware anymore.  Right.



What's been happening for at least the past several decades or more is that a few nerds who know each other will get together over some pizza to discuss ways to make a bazillion dollars.  The framework of their idea is nothing new:  Create a business plan and present it to some venture capitalists in order to obtain seed capital and form a classic start-up.  Work 24/7 to create something everyone needs, then start it running.  Watch it grow, create demand, then either take it public or sell it off to a much bigger fish.  The venture capitalists are happy, the cofounders are rich, and everyone wins.



So in a world where I'm told that "no one does hardware anymore," it was only natural for those nerds to turn their attention to offering various sorts of cloud services.  And the model there is more intoxicating than anywhere else, since not only do their future customers not want to "do hardware," neither do they.  And they don't need to since massive data centers already exist where "doing hardware" is all they do - again, another example of increasing specialization.



So these nerds write a bunch of code to do whatever it is they think companies will not be able to live without once they see what their new service is capable of doing for them.  They rent some servers, spin up a bunch of virtual machines, launch their website, make an offer for trying it for free before committing, and start looking for and signing up new customers.



Okay, now, I wrote everything I've just shared before I went to Wikipedia to see what Wikipedia had specifically to say about Snowflake.  I promise that I really did write all of that with zero specific knowledge of Snowflake.  So here's what the start of Wikipedia's page on Snowflake says.  Wikipedia writes:  "Snowflake Inc. is an American cloud computing-based data cloud company based in Bozeman, Montana.  It was founded in July of 2012 and was publicly launched in October 2014 after two years in stealth mode.  The firm offers a cloud-based data storage and analytics service, generally termed 'data-as-a-service.'  It allows corporate users to store and analyze data using cloud-based hardware and software.



"The Snowflake service's main features are separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and support for third-party tools.  It has run on Amazon Web Services since 2014, on Microsoft Azure since 2018, and on the Google Cloud Platform since 2019.  The company was ranked first on the Forbes Cloud 100 in 2019.  The company's initial public offering raised $3.4 billion in September of 2020, one of the largest software IPOs in history," writes Wikipedia.



"Snowflake Inc. was founded in July 2012 in San Mateo, California by three data warehousing experts, two who previously worked as data architects at Oracle Corporation, and the third a cofounder of a Dutch start-up VectorWise.  The company's first CEO was Mike Speiser, a venture capitalist at Sutter Hill Ventures."  So pretty much exactly what I said is the way this all happens these days.



So the point I can now make from what was my first blind writing, without any, you know, specific knowledge of Snowflake, is that, indeed, this is the way today's cloud-based service ventures are being born.  And, as Wikipedia's details have shown us in this case, the founding three were absolutely correct about the need for and the appeal for their service.  Since we're going to be talking about what happened in a minute, it's worth getting a little more specific information about this company.



So Wikipedia continues from where I had left off:  "In June 2014, the company appointed former Microsoft executive Bob Muglia as CEO.  In October 2014, it raised $26 million and came out of stealth mode, being used by 80 organizations.  In June of 2015, the company raised an additional $45 million and launched its first product, its cloud data warehouse, to the public.  It raised another $100 million in April 2017.  In January 2018, the company announced a $263 million financing round at a $1.5 billion valuation, making it a unicorn."  For those who don't know, a unicorn is a startup company valued at over $1 billion which is still privately owned and not listed on any market.  Wikipedia says:  "In October 2018, it raised another $450 million in a round led by Sequoia Capital, raising its valuation to $3.5 billion.



"In May of 2019, Frank Slootman, the retired former CEO of ServiceNow, joined Snowflake as its CEO; and Michael Scarpelli, the former CFO of ServiceNow, joined the company as CFO.  In June 2019, the company launched Snowflake Data Exchange.  In September 2019, it was ranked first on LinkedIn's 2019 U.S. list of Top Startups.  On February 7th, 2020, the company raised another $479 million.  At that time, it had 3,400 active customers."  Okay?  Four and a half years ago, 3,400 active customers.  "On September 16th, 2020, Snowflake became a public company via an initial public offering, raising $3.4 billion, one of the largest software IPOs and the largest to double on its first day of trading."



So four and a half years ago, back in February 2020, Snowflake had 3,400 active customers, and the sky's the limit.  Everything looks great.  We can presume that four and a half years later that number has only grown.  So I wanted to start by painting that generic picture of the relatively new phenomenon of an entirely cloud-based industry, of which Snowflake is a perfect example, because the events and finger-pointing in the aftermath of Snowflake's apparent inability to protect many of its customers' vast troves of data, much of it sensitive, suggests that we're not yet fully equipped to deal with the consequences of this new and essentially "virtual" cloud-based industry.  A ton of information about what can now only be described as an historic data breach exists on the Internet.  So I've spent a great deal of time following the links and reading original sources in an attempt to make sense of what happened.



I think I finally have it worked out, and it's not quite the narrative that has taken hold throughout the industry due to a bit of subtlety as well as contracts and non-disclosure agreements.  Snowflake is blaming its customers for having their Snowflake login credentials used to log into their Snowflake accounts, noting that it was only those customers who did not have their logins protected by multifactor authentication that had been breached.  In other words, it appears that Snowflake is blaming their own customers for having weak authentication security - while, I'll note, Snowflake did not require any stronger login authentication, as it certainly could have.



But it seems to me that the real question, which Snowflake appears to want to avoid answering by deflecting about multifactor authentication, while its security contractors may be bound by agreements not to disclose, is how were many hundreds of its customers' login credentials obtained by these attackers in the first place?  The facts strongly suggest that something happened where, in short order, attackers obtained the login names and passwords belonging to a large number - hundreds - of Snowflake's customers.  Where present, the attackers were apparently unable to obtain the accounts' MFA secrets, which is why MFA protected those customers who were using it.  But somewhere around 350 of Snowflake's customers who were not using MFA suddenly found that all of the proprietary data they had shared with Snowflake had been exfiltrated to parts unknown.



So whose fault was it?  Was it Snowflake's customers, for not extra-protecting themselves from what appears to be a major precipitating breach of authentication credentials at Snowflake?  Or did Snowflake make some mistake themselves - which, to be clear, they are denying strongly - and that that preceding breach allowed a large set of their customers' login credentials to fall into the hands of the bad guys?



We know mistakes happen.  That's a fact.  But the narrative that's taken hold in the industry, which many articles quote Snowflake's spokesperson's saying, is that the actual fault lies with Ticketmaster, with Advance Auto Parts, with Santander Bank, with LendingTree, and now with AT&T, as well as apparently more than 340 others, for not using multifactor authentication.  That's a nice sleight of hand on Snowflake's part, but I'm not sure it's fair.



Security researcher Kevin Beaumont often summarizes things with more technical detail than other publications.  In this case, back toward the beginning of June, under his headline "Snowflake at centre of world's largest data breach," Kevin posted on Medium:  "Cloud AI Data platform Snowflake are having a bad month, due to teenage threat actors and cybersecurity of its own customers, and its own cybersecurity, too, in terms of optics.  There are several large data breaches playing out in the media currently.  For example," he writes, "Ticketmaster owner Live Nation filed an 8-K with the SEC for potentially the largest data breach ever, claimed to be 560 million customers.  They finger Snowflake as part of the data breach."  Kevin cites TechCrunch's article with the headline:  "Live Nation confirms Ticketmaster was hacked, says personal information stolen in data breach."



Then Kevin says:  "Additionally, incidents are running at multiple other cyber companies who are Snowflake customers where full databases have been taken."  He says:  "I've spoken to people in multiple industries at large corporations where they've had significant data exfiltration in May via Snowflake.  The Australian security services have issued an advisory:  'High Alert / Act Quickly!'  They say they are 'aware of successful compromises of several companies using Snowflake environments.'"  He says:  "Snowflake themselves have put out Indicators of Compromise for 'threat activity' over the weekend, saying to look for connections into their platform from the user agent 'rapeflake.'  Additionally, a threat actor claims they gained access to Snowflake itself and their customers using infostealers."



Okay.  So let's pause here because what happened has been interesting.  The security research firm Hudson Rock first told the story of the penetration of Snowflake, but quickly received a takedown order from Snowflake's legal beagles.  You know, 'We're going to sue you if you don't stop saying this.'  So Hudson Rock complied, and the industry was then forced to reference the Internet Archive's Wayback Machine record of their write-up, until it was hit with a similar order, requiring it to block that URL from access.  So not really a good look for Snowflake.  What Hudson Rock had to say was interesting, so we'll circle back to that in a minute.



Referring to what Kevin read in Hudson Rock's piece, he wrote:  



"The threat actor makes various claims which sound questionable; but, well, Snowflake have confirmed some of it is true while crowing to the media and customers that the blog is not true.  It is Schrdinger's Blog.  The threat actors here, from what I've managed to establish, were a teen crimeware group who've been publicly active on Telegram for a while."



LEO:  Wow.



STEVE:  You know, thus rapeflake or whatever it was that they called this, their agent.  Was it rapeflake?  Yeah, rapeflake.  As, you know, snowflake, rapeflake.  So okay.  In other words, you know, a bunch of kids did all this damage.  When Kevin writes "Let's Recap," he says:  "We have what appears to be the world's biggest data breach  in terms of impacted individuals  playing out with Snowflake as the vendor linking the victims.  A lot of data has gone walkies.  Snowflake, for those won't know, is an AI data platform where you shove vast amounts of data in and then use it.  It allows you to do this with effectively no security."



He says:  "I feel bad for Snowflake on a human level as they're in a bad situation.  This is a potentially business-ending event for them.  So they have to use every lever possible to point the fingers at their own customers as being negligent over 'rapeflake' activity to avoid responsibility.  And to be clear, some of this is their customers' responsibility.  But also, Snowflake have to own this issue and face straight into it to survive, as there's an extremely high chance this is going to play out publicly over coming weeks and months."



And boy, was Kevin prescient about that one.  He wrote this more than a month before the AT&T breach announcement.  Then he writes - and this is so perfect.  He says:  "Note that in the age of SaaS (Software as a Service), your providers will throw you under the bus to save themselves.  When you transfer your security risk to a provider, they don't accept your risk.  They just take your money."  He says:  "What you're sold versus what you get often don't align."  He says:  "I've worked for a cloud provider.  You don't want to see how the sausage is made.  And there's no real accountability for the provider.  There will be much more of this to come with cloud data providers in the future, is what I'm saying.



"So what actually happened?  Despite Snowflake saying the Hudson Rock blog is inaccurate," and he says, "and parts most probably are, the Snowflake credentials bit is accurate.  Snowflake say:  'We did find evidence that a threat actor obtained personal credentials to and accessed demo accounts belonging to a former Snowflake employee.  It did not contain sensitive data.  Demo accounts are not connected to Snowflake's production or corporate systems.  The access was possible because the demo account was not behind Okta or Multi-Factor Authentication (MFA), unlike Snowflake's corporate and production systems.'"



And I'm just reminded, didn't LastPass go on and on about how safe everything was because their development systems were completely isolated from their production and corporate systems?  That sounds like a familiar tune we've heard before.  Whoops.



He says:  "Snowflake have incident response stood up, with CrowdStrike and Mandiant involved.  They say the cause of the malicious activity - in other words, database downloads - is 'This appears to be a targeted campaign directed at users with single-factor authentication.  As part of this campaign, threat actors have leveraged credentials previously purchased or obtained through infostealing malware.'"



Okay.  So to me this is curious, since they're not saying from where a huge number of their customers' single-factor authentication credentials may have been "infostolen."  There's only one place in the entire world where all of those otherwise completely independent customer credentials would all be gathered into one place.  I wonder where that could be?  As Kevin wrote:  "In the age of SaaS, your providers will throw you under the bus to save themselves."



Okay.  So as he says:  "So what happens, essentially, is infostealers were used to gain access to Snowflake databases using their customers' stolen credentials, using the client name rapeflake."  And then he said:  "Side note to threat actor over that name.  Really?"  Anyway, he finishes:  "Snowflake themselves fell into this trap by both not using multifactor authentication on their demo environment and failing to disable an ex-employee's access.  Stuff happens.  Incidents happen.  And while Snowflake may present themselves as having no platform breach, they themselves also fell into the same problem, and in terms of optics it isn't great as they can point out customers messed up, but then they messed up, too."



So he then went on a little bit of a digression about what he feels is a hugely important topic of "infostealers."  And since it's Kevin, I'm going to share that.  He said:  "You may know about infostealers as I recently wrote about them being a huge threat when it comes to Microsoft Copilot+ Recall allowing full data threat of everything you've ever viewed, a feature you should absolutely disable in Windows 11."



Then he says:  "Mandiant themselves have this to say about infostealers this weekend."  And then I grabbed a picture of Kevin's snap from Mandiant's site, where he says:  "Here are some of Mandiant's observations related to infostealers from the past few years.  Since the beginning of 2020, employees and contractors working from home increasingly use their personal computers to access corporate systems.  People often synchronize their web browsers on their work computers and personal computers.  People, or their children, sometimes inadvertently install software laced with infostealing malware on their personal computers.  The malware can capture credentials from their web browsers.  Threat actors opportunistically search for corporate credentials stolen by infostealing malware to use them to compromise enterprises, steal data, and conduct extortion."



In other words, although it's not a straight line, it's a series of interconnections.  A worker at home is using his personal machine on the enterprise network.  The personal machine is synchronizing browsers with the enterprise network at the other end.  Someone may install infostealing malware on the home machine.  That home machine can steal the local credentials from the browser which, being synchronized with the enterprise browser, gets the enterprise credentials.  And then the infostealing machine on the home computer is able to whisk those off somewhere.



So Kevin says:  "If you use Snowflake, you need to first of all enable multifactor authentication and tighten authentication to your database as a top priority.  Then you need to go back and look at the access logs on Snowflake itself and check who's been using your data."  He says:  "You cannot rely on Snowflake doing this for you."  He says:  "Infostealers are a significant problem.  It has long since outpaced botnets and so forth in the real world.  And the only real solution is robust multifactor authentication, and ideally getting rid of passwords altogether by replacing them with secure authentication."  In other words, Passkeys.



He says:  "There are companies offering services where you can buy your own stolen credentials back" - whoa - "and then you can change users' passwords."  He says:  "I don't like this approach.  The reason is those vendors often buy those credentials from 'credential brokers,' which translates to funding the criminal hackers who steal them in the first place.  As a customer you end up proxy funding the threat actors you're trying to deal with.  Additionally, it is a huge user impact to have their password changed, and it doesn't fix the problem."



He says:  "Tightening authentication fixes the problem.  Ask the Snowflake victims how they have fixed the problem.  It's through robust multifactor authentication.  The wider problem is that something is wrong at Snowflake when it comes to authentication.  Snowflake themselves fell victim to this incident, albeit with a demo tenant.  They need to, at an engineering and secure-by-design level, go back and review how authentication works, as it's pretty transparent that, given the number of victims and the scale of the breach, that the status quo has not worked.  Secure authentication should not be optional.  And they've got to be completely transparent about steps they're taking off the back of this incident to strengthen things.



"For cloud providers in general, they need to be more robust in terms of secure defaults or risk being dragged into this kind of situation.  For Microsoft" - I love it that he just couldn't resist finishing with this.  "For Microsoft, they need to recall Recall, or they will pour petrol onto the flames and make the infostealer problem far worse."



After Kevin posted this piece, he added a quick follow-up.  He said:  "People are pinging me to say there's more to this story than I've disclosed."  And he says:  "I know.  It will be a developing story, and all eyes are on Snowflake."  So maybe, for example, there was some knowledge of the AT&T problem.  So anyway, you know, Snowflake insists that it didn't happen that they were breached.  Maybe that's true.  Seems suspicious to me since all of a sudden someone got a hold of 350-plus of their customers' non-MFA single-factor authentication login data and used it to breach their technologies.  It'll be interesting to see if anything more happens and, you know, how Snowflake fares.  They are certainly a huge cloud provider.



It's also interesting, and here a little egg is on AT&T, the data that was stolen from AT&T was apparently two years old.  That is, this was back from 2022.  AT&T left old, 110 million customers' worth of individual transaction data at Snowflake from two years back and, you know, could have pressed a button to delete it, but didn't.  So, you know, plenty of blame to go around.  But, wow, we now have broken all records in terms of amount of data lost in a massive breach.



LEO:  So do you think that the credentials of those 350 companies were revealed, or that there is something - more likely there's a flaw in the authentication process; right?  That whatever it is takes in the password and then lets the person in wasn't working properly?  There was a hole in it?  We don't know, do we.



STEVE:  I don't think the - everybody who is surrounding this thinks something is fishy.



LEO:  Yeah.



STEVE:  What Snowflake - it is hard to keep saying that name.  What a name.  Maybe they'll change it.



LEO:  They thought it was funny at the time, you know.  Hey, we're a unicorn, Snowflake, ha ha.



STEVE:  What they're saying is that their customers were the victims of infostealers which found Snowflake credentials on their customers' computers.



LEO:  Okay.



STEVE:  And then, because those customers weren't using multifactor authentication, just finding static authentication, username and password...



LEO:  Yeah, was enough.



STEVE:  ...allowed them to log into Snowflake.



LEO:  Okay.



STEVE:  If that's the case, then why not everything else?  I mean, and why all of a sudden, you know, 340, 350 customers?  It seems much more likely that Snowflake was infiltrated, and the database of those, you know, that authentication was exfiltrated and then used because those customers did not have multifactor authentication...



LEO:  That was all they needed.



STEVE:  It was used to log in as them and grab all their data.  We just don't know.



LEO:  We don't know.  It's possible that the hacking group was actually looking for Snowflake credentials on all those customer computers.



STEVE:  Yes.  It absolutely is possible.



LEO:  That they have some sort of, you know, they knew they were targeting Snowflake, and they were looking for that kind of credentials.  That's why it was all Snowflake.



STEVE:  Yes.



LEO:  I mean, it's unknown.  Somebody needs to come forward and say what happened.  But Snowflake probably...



STEVE:  Well, actually, The Record, the publication The Record, said this.  They said:  "According to the original post, the intruders were able to sign into a Snowflake employee's ServiceNow account using stolen credentials, and from there were able to generate session tokens."



LEO:  Ah.



STEVE:  Hudson Rock wrote:  "To put it bluntly, a single credential resulted in the exfiltration of potentially hundreds of companies that stored their data using Snowflake, with the threat actor himself suggesting" - the threat actor himself suggesting - "400 companies were impacted.  In a post on Friday, Snowflake did not respond directly to the researchers' claims, but denied that a vulnerability within its systems was to blame for the accessing of customer data.  The company said it is 'investigating an increase in cyber threat activity'."



LEO:  A large increase.



STEVE:  Uh-huh.  So basically Hudson Rock posted this claim by the threat actor themselves, and they received a legal takedown notice.



LEO:  Oh.



STEVE:  And then the web archive was similarly forced to block their archive link.



LEO:  Well, that smacks of cover-up.



STEVE:  Yes.



LEO:  I mean, I guess you could say for security reasons we don't want anybody to know how they did this.



STEVE:  Or PR reasons.



LEO:  Or PR reasons.  Wow.  Oh, I hope we get to the bottom of this at some point.  I'm sure, if we do, you will let us know.



STEVE:  Absolutely.



LEO:  What a great story.  Great in an interesting way, not a good way for anybody involved.  Thank you, Steve Gibson.  Once again, elucidating the dark corners of the Internet.



STEVE:  Bye.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#984

DATE:		July 23, 2024

TITLE:		CrowdStruck

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-984.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What do we know about how the FBI broke into the smartphone of Trump's deceased would-be assassin?  Cisco scored another very rare CVSS 10.0 for a serious remote authentication vulnerability.  If you're affected you MUST update.  Untrusted Entrust's plan for the future is revealed.  Surprisingly, Google loses the anti-third-party cookie battle.  Third-party cookies stay.  More interesting experiences from GRC's weekly Security Now! podcast mailings.  Now we know why the company named itself "Snowflake."  A collection of interesting listener feedback follow-ups on recent discussions.  And we learn what in, literally, the world happened to allow CrowdStrike to crash 8.5 million Windows gateways, servers, and workstations to cause the largest IT outage of all time.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And of course the topic of the hour, the day, the week, the year probably, is the CrowdStrike incident.  Steve breaks it down, tells us what happened.  Of course, some of this is speculation because we don't know all the details.  But he will give you more details than anyone else.  This is the place for the CrowdStrike story, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 984, recorded Tuesday, July 23rd, 2024:  CrowdStruck.



It's time for Security Now!, the show where we cover your security, your privacy, and how things went awry last Thursday and Friday with this guy right here, Steve Gibson of GRC.com, our security guru.  Hi, Steve.



STEVE GIBSON:  Leo, it's great to be with you for the podcast that will surprise no one.  I was using the phrase "This podcast wrote itself."



LEO:  Yeah.  But I think there's been a lot of interest from our audience about what your take is, what your explanations are, and all of that.	



STEVE:  I actually have some that no one has read anywhere else.



LEO:  Good.  Look forward to that.



STEVE:  So, yeah, I think it's going to be interesting.  So of course I titled this podcast "CrowdStruck."  And the subtitle is "The Legend of Channel File 291."



LEO:  Yes, mm-hmm.



STEVE:  So, yes, we're going to have...



LEO:  A name, by the way, that IT professionals everywhere are learning to hate as they go from machine to machine, deleting them one by one.



STEVE:  Oh, my god, yes.  And, well, there's so much to talk about because, like, why was that necessary?  Why couldn't Windows restore itself? 



LEO:  Yeah, lot of people said that, yeah.



STEVE:  Why isn't there, like, some escape?  And of course the real ultimate biggest issue of all is how could CrowdStrike have ever allowed this to happen?



LEO:  Right.



STEVE:  I mean, like, what could possibly explain how this got out into the world without them knowing that, you know, like what was going on.  So anyway, just this is going to be one for the record books, probably also in terms of total length because I ended up at, I think, 22 pages, which is four more than our normal - oh, no, 24 pages.  Yikes.  So, yeah, we have a lot to cover.



LEO:  It's okay.  We want this.  We want it.  Go, baby, go.



STEVE:  And believe it or not, Leo, something else actually happened last week.



LEO:  No.  No.



STEVE:  Besides the world basically coming to a grinding halt.  So we're going to look at what we know about how the FBI broke into the smartphone belonging to Trump's deceased would-be assassin.  Cisco managed to score another of a very rare CVSS 10.0s that you never want to score.  It's a serious remote authentication vulnerability, as evidenced by having the maximum 10 out of 10 severity rating.  So anyone affected absolutely must update.  Also, we now know about the untrusted Entrust's plan for the future, how they plan to be moving forward once Chrome has said we're not going to be trusting anything you sign after Halloween.  Also, oh, boy.  Now, once upon a time I would have said the most tweeted.  Now this was the most emailed to me note because all of our listeners know how I feel about this.  Google has lost...



LEO:  Yes.



STEVE:  ...the anti-third-party cookie battle.



LEO:  Yes.



STEVE:  Oh, boy.  So cookies are staying, and we'll have some things to talk about there.  Also I'm going to share a few more interesting anecdotes from my weekly Security Now! podcast mailings, the experience from last week.  And now I know about this week because 7,000 of our listeners received all of this two hours ago, you know, the breakdown of topics and the show notes and the Picture of the Week and so forth.



LEO:  So you get feedback for the show even before you do the show.



STEVE:  Yeah, it happened.



LEO:  That's great, yeah.



STEVE:  Yes, exactly.



LEO:  That's good, you know, it's like a focus group kind of.



STEVE:  And one of those things was a guy writing from New South Wales, Australia, who actually wrote his letter to me, tweeted it, but I had already finished and produced the podcast before I finally went over to Twitter to tweet about the podcast, and I saw that there were some - anyway, he has something really cool to say about CrowdStrike which we'll get to next week because, hint hint, email is a little more quick getting to me.



LEO:  Faster.



STEVE:  Yes.



LEO:  Well, our friends in Australia were the first to bear the brunt of the CrowdStrike.



STEVE:  Yes, it was in the afternoon that his world as he knew it ended.



LEO:  Poor guy.



STEVE:  Anyway, so I'm going to - also, we now know where the seemingly "flaky" name "Snowflake" came from, and why.



LEO:  Ah.  Oh.



STEVE:  And then I do have some listener feedback I want to share, following up on recent discussions.  And then we're going to learn what in, literally, the world happened to allow CrowdStrike to take down 8.5 million Window's gateways, servers, and workstations to cause the largest IT outage of all time.  And of course the reason we're not hearing the details is that you have to imagine that CrowdStrike's attorneys dropped the Cone of Silence over that, I mean, they probably just went over and yanked the phones out of the wall.



LEO:  Yeah, yeah.



STEVE:  And said no one is saying anything.



LEO:  No one's saying nothing.



STEVE:  And we do have a Picture of the Week for today's podcast.



LEO:  Awesome.



STEVE:  Which I will talk about a little bit briefly as we get going here.



LEO:  Very good.  Well, a really big show coming up here in just a bit with Steve Gibson, and we will get right to it.  All right, Steve.  The Picture of the Week is not a joke this week.  Far from it.



STEVE:  Not so funny this time.



LEO:  Yeah.



STEVE:  I gave this snapshot the title "The simple memory pointer mistake that stalled the world."  And I will be talking about this in gratifying detail at the end of the podcast.  But what we see from this, now, this is the kind of crash dump that most people just like, what?  You know, it's like, it's going to have no meaning whatsoever to almost anyone.  But people who understand the machine architecture and dumps will see that this occurred, this crash occurred inside something called CS Agent, which doesn't take any stretch to know would stand for CrowdStrike Agent, and that there was a function which took two parameters.  And this shows up like on the fifth or sixth line.



LEO:  This is assembly language, which is why Steve knows what it means; right? 



STEVE:  Correct, correct.  Anyway, so the function was given a bad parameter - and I have a theory as to why, which we'll talk about - which caused it to attempt to load 32 bits from where there was no memory.  And you can't do that.



LEO:  Right.



STEVE:  It's one of the things that cause Windows to just give up.  And I'll explain why Windows could not recover from this, why an application doing this is different from the kernel doing this, and all of that.



LEO:  Good.



STEVE:  So, you know, basically this is a snapshot of the actual crash.  And it was a simple memory pointer mistake, and it took down immediately 8.5 million machines, Windows operating systems.



LEO:  To the point where they couldn't be rebooted.  They just had to be fixed.



STEVE:  Yes, in a way that required - and that's one of the most expensive factors of this, right, is you had to visit, somebody had to visit...



LEO:  Physically.



STEVE:  ...every single machine.



LEO:  Lord above.



STEVE:  Oh, boy.  So, you know, with the glare of what happened last week still looming over everything, it's a little difficult for us to focus upon anything else.  And of course we're going to give this our full attention.  But there was some other important news that emerged last week which should not be overshadowed.



In the wake of the failed attempted assassination of our ex-U.S. President Donald Trump during his recent campaign rally, the FBI has been attempting to learn all it can about the immediately deceased would-be assassin.  You know, they obviously can't ask him for his password.  So 20-year-old Thomas Matthew Crooks was using an Android phone that was password locked.  And of course we've certainly been here before; haven't we?  We all remember the San Bernardino mess.



Bloomberg reported that the FBI sought help from the Israeli digital intelligence company Cellebrite which, with offices conveniently located in nearby Quantico, Virginia, is known to provide smartphone unlocking technology to U.S. federal agencies.  A significant percentage of their business is doing that.  Sources familiar with the investigation, who requested anonymity, told Bloomberg that the FBI needed data from the phone to understand Crooks' motives for the shooting.  Right, everyone wants to know, you know, why?  Doesn't really matter, but still interesting.



So although the local FBI bureau in Pittsburgh already did have a current license for Cellebrite's smartphone cracking software, it was ineffective on Thomas Crooks' newest Samsung device.  So undaunted, the FBI reached out to Cellebrite's nearby federal team, which is there for the purpose of collaborating with law enforcement when they've got some problems.  Within hours of that, Cellebrite had provided the FBI with the additional support they needed, including some newer, not-yet-released software.  And 40 minutes after that, the FBI had Thomas's Samsung Android smartphone unlocked and open for detailed inspection of the shooter's social media, browsing, texting, whatever, history.



What's interesting is that in other, just it was coincidental really, reporting, it appears to be fortuitous for the FBI that Thomas was not using a later model Apple iOS device since some documents leaked from Cellebrite indicate its inability to unlock such devices.  9to5Mac picked up on this last Thursday, reporting under their headline "Cellebrite cannot unlock most iPhones running iOS 17.4 and later."  They wrote:  "Leaked documents reveal that Cellebrite cannot unlock iPhones running iOS 17.4 and later, at least as of the date of publication," which was April of this year.  They said:  "The company has confirmed that the documents are genuine.  Cellebrite devices, which are widely used by law enforcement agencies, can crack most Android phones, though there are exceptions.



"Cellebrite's kit relies on discovering vulnerabilities discovered in iOS and Android, which Apple and Google of course then aim to fix, well, discover and resolve.  Others also work to defeat the phone-cracking kit, which mostly secure messaging app Signal scored with a big win in 2021, when it managed to booby-trap iPhones to render the kit useless."  And we covered that at the time.  Back in 2022, 9to5Mac managed to obtain user documentation which iPhone models that kit at the time could not unlock.



Since then, and with this recent document discovery, it was 404 Media that grabbed updated docs, and these are the ones dated April of 2024.  I got a look at the PDF.  It was four panels of grid and explanation that used a lot of jargon that you'd have to have a glossary in order to untangle.  So the reporting here is easier to understand.  They said:  "As of that date" - that is, April 2024 - "Cellebrite had not managed to crack iPhones running iOS 17.4 or later, which today is a very large percentage of iPhones."



9to5Mac said:  "Additionally, the kit cannot currently break into most iPhones running iOS 17.1 to 17.3.1, though hardware vulnerabilities in the iPhone XR and 11 mean those are exceptions.  The company (Cellebrite) appears to have worked out how to access other iPhones running those versions of iOS; however, as the table says this capability is 'coming soon'" - which means, but we don't know how to do it yet, so we don't really know if it's coming ever.  The documents are titled "Cellebrite iOS Support Matrix" and "Cellebrite Android Support Matrix," respectively.  An anonymous source recently sent the full PDFs to 404 Media, who said they obtained them from a Cellebrite customer.



For all locked iPhones able to run 17.4 or newer, the Cellebrite document says "In Research," meaning they cannot necessarily be unlocked with Cellebrite's tools today.  We know from Apple that the majority of iPhones in use today are using iOS 17, though the company doesn't share breakdowns of the specific point numbers.  That said, it's a safe bet that a high percentage were uncrackable by Cellebrite as of the date of the document.



A separate table of Android-cracking capabilities show that most of them are accessible by the kit, though the Google Pixel 6, 7, and 8 are exceptions if they were powered down at the time that they were obtained.  That's because the cold-boot process blocks the exploit that's being used.  But they can be accessed if powered up and locked.  The same is true of Samsung phones running Android 6, but not those running later versions, indicating that Samsung's implementation of Android 7 managed to introduce a vulnerability which is still present all the way through Android 14, which Cellebrite knows about.



So anyway, The Verge summarized the state of play by writing simply:  "Phone hacking companies are overstating their capabilities."  And of course they have motivation to do so.  The Verge noted that most newer phones are currently beyond the capabilities of these commercial phone hacking companies.  This could mean that the phone vendors are finally winning this battle by iterating over and constantly improving the security of their solutions.  It could also mean that older phones are currently vulnerable because the companies have had more time with them and that these newer phones may similarly fall in the future.  We can't really say.



I guess if I were a betting man I'd go short on the stock of the hacking companies, since I suspect their remaining days are numbered as the hardware finally becomes impregnable.  But please don't take this as a stock tip.  I'm not a betting man, and I would never encourage anyone else to be.  CrowdStrike has just demonstrated that anything can happen.  So you never know with computers and software.



As we know, CVEs having a CVSS score of 10.0 out of a possible 10.0 are vanishingly and blessedly rare.  Unfortunately, last Wednesday, Cisco was forced to report and acknowledge one of their own.  Ars Technica wrote:  "On Wednesday, Cisco discovered a maximum-security vulnerability that allows remote threat actors with no authentication to change the password of any user, including those of administrators with accounts, on Cisco Smart Software Manager On-Prem devices."  So that's the key phrase for any of our listeners.  And, boy, I'm getting an education about just at what a high level our listeners are operating throughout their organizations.  So I wouldn't be at all surprised if this means something to some of them.  Cisco Smart Software Manager On-Prem devices.  If you have one around, and it hasn't been patched since last Wednesday, hit pause on the podcast and go do that.



"The Cisco Smart Software Manager On-Prem resides inside the customer premises and provides a dashboard for managing licenses for all Cisco gear in use."  So, you know, they created sort of a central management hub which on the one hand makes it very convenient for managing everything in your organization.  On the other hand, if you happen to have a CVSS of 10.0 which allowed non-authenticated remote users to change passwords at will, that would be a problem.  That concentration of power which we keep seeing now, this seems to be a repeating theme; right?  Over and over, yes, it's convenient.  But, boy, when it gets hit, it makes the pain much worse.  "So it's used by customers who can't or don't want to manage licenses in the cloud, which is the more common approach.



"So in their bulletin, Cisco warns that the product contains a vulnerability that allows hackers to change any account's password.  The severity of the vulnerability" - this is Ars speaking - "tracked as CVE-2024-20419, is rated 10," they write, "the maximum score."  The Cisco bulletin stated:  "This vulnerability is due to improper implementation of the password-change process."  Okay, that seems kind of obvious.  "An attacker could exploit this vulnerability by sending crafted HTTP requests" - in other words web request - "to an affected device.  A successful exploit could allow an attacker to access the web UI or API with the privileges of the compromised user."  There are no workarounds available to mitigate the threat.  Other than, you know, pulling its plug, but that would probably be a problem, too.



"So it's unclear precisely what an attacker can do after gaining administrative control over the device.  One possibility is that the web UI and API, which the attacker gains administrative control over, could make it possible to pivot to other Cisco devices connected to the same network and, from there, steal data, encrypt files, you know, get up to all the bad stuff that the bad guys do these days.  Cisco reps did not immediately respond to email queries from Ars Technica."  And they finished by noting that the post would be updated if a response were to come later.  So if there's an update,  you definitely want to apply it.  So again, ultra rare, 10.0.  If you know anybody who has one of these, make sure they update it since last Wednesday.



Okay.  So in following up on one of our past big points of coverage, several weeks ago, as we all know, after a great deal of handwringing and teeth gnashing on the part of those who run the collective known as the CA/Browser forum, Google decided that they could no longer in good conscience afford to have their Chrome web browser honor and trust certificates signed by Entrust moving forward.  This was not done, as had been done in the past, due to any egregious, horrific certificate mis-issuance event, but rather to send a very strong and clear message, not only to Entrust, but to the entire community of certificate authorities that there would actually be consequences if they did not live up to the operational and behavioral commitments that they themselves had previously agreed to.



Among many other aspects of this, it was not fair for Entrust to be allowed to leave their own misissued certificates in place and thus saving face with their customers, while other CAs were playing by the self-imposed rules by going to the cost and inconvenience of acknowledging, revoking, and reissuing any mistakes that they may have made.  So Google's move was, and it was meant to be, a very clear demonstration that this game would not tolerate any multiyear endemic cheating.



The week after we covered this historic event, Todd Wilkinson, Entrust's president and CEO, formally apologized and said once again, after the industry had lost count of the number of previous times Entrust had said this, that they were really, truly, and seriously this time going to do better.  And I suspect that this time they probably will.  But that left us with the question, what would Entrust do in the meantime?  When we were talking about this we explored various paths.  We're back here talking about this today because Todd is present with the answer to that question.



He signed the following newsflash from them, writing:  "To our TLS customers."  He said:  "I would like to thank you for your patience as we diligently work to ensure that you will continue to receive uninterrupted public TLS certificate services through Entrust.  Today we are ready to share our go-forward plans.  First, as you likely know," he says, "Google said that Chrome will no longer accept Entrust public TLS certificates issued after October 31st, 2024.  Entrust TLS certificates issued prior to October 31st will continue to be accepted through their expiration date.



"Entrust is committed to returning to the Chrome Root Store and will keep you informed of developments.  We've identified the steps to address Google's decision.  We continue to execute our improvement plans and are working closely with the browser community in discussions on our path forward.  In the meantime, after October 31st, 2024, you can continue to request public certificates and receive certificate services directly from Entrust.  Here is how this will work."



And we have three bullet points:  "First, continue to order certificates as you have been, under the same pricing model and service-level agreements (SLAs).  Second, rely on Entrust for certificate lifecycle management, verification, support, and professional services, as we plan to serve as the Registration Authority for these certificates.  And finally, we will deliver public TLS certificates issued by a CA partner that meets the requirements of the CA/Browser Forum and Entrust."  And he finishes:  "Today we can share that SSL.com is now an Entrust CA partner.  SSL.com is a global CA founded in 2002 with full browser ubiquity.  They are used by businesses and governments in over 180 countries to protect internal networks, customer communications, e-commerce platforms, and web services, and we are pleased to partner with them to meet your needs."



And he finishes:  "To build resilience into your organization, we recommend that you take inventory and renew your Entrust certificates prior to October 31st, 2024.  These certificates will be trusted through their expiration date, up to 398 days."  By which time, I'm sure, he's hoping this will no longer be necessary.  Anyway, he finishes:  "You can renew your certificates through your certificate lifecycle management solution, automation tool, or the Entrust Certificate Services Portal.  Signed, Todd."



Okay.  So that answers that question.  Quote: "We will deliver public TLS certificates issued by a CA partner that meets the requirements of the CA/Browser Forum and Entrust."  So it does not appear that any other CA is going to be allowing Entrust to ride on their coattails by signing a new Entrust intermediate certificate that has the power to, in turn, sign web server identity end certificates.  Instead, Entrust found SSL.com, an even smaller CA than them, who is in good standing, from whom they will purchase and resell web server TLS identity certificates.



The best estimates I've been able to find on the web are that Entrust does indeed, as we noted previously, have about 0.1% of the total website server business.  SSL.com appears to have about half of that at 0.05%.  So this deal represents something of a windfall for SSL.com.  Entrust will presumably use SSL.com's certificate issuing machinery in return for paying SSL.com for every certificate Entrust issues under their auspices.



So it's a win-win for the time being, but this does also feel like a temporary backstop solution for Entrust.  It feels as though Entrust does indeed plan to work to rehabilitate itself in the eyes of the CA/Browser community to then have Chrome and any other browsers that may be planning to follow Chrome's lead restore their trust in Entrust's operations and integrity.  So though Entrust will be losing out on some fraction of their overall certificate revenue, they will likely be able to retain the customer relationships they've built and will someday be able to again issue certificates under their own name.



So, and of course you can see, based on what Todd is saying, he is saying, if you use Entrust before Halloween to reissue a certificate you have, that will span the next 398 days.  And they're hoping to be rejuvenated by that point.  So maybe they won't even need to fall back on SSL.com.  But certainly there will be customers who will be, like, aren't listening to the podcast, are completely clueless about any of this happening, who will be coming back to Entrust later in the year or any time next year as their previously issued Entrust certificate is getting ready to expire.  Entrust is saying, that's fine, we're not losing you.  You can still, you know, everyone's pretending that we're still issuing your certificate.  You use our portal, you use our UI, but actually the certificate will be coming from SSL.com in return for us giving them some piece of the action in order to perform that service for us.  So that appears to be what they're up to.  And Leo, we're about 30 minutes in.



LEO:  Yeah, let's take a...



STEVE:  So this would be a good time to take a break, and I'm going to take a sip of coffee to whet my whistle.



LEO:  We'll take a little time out and be back with more.  You know what, you found a lot of other stuff to talk about.  It's not all CrowdStruck.  Absolutely not.  In fact, Google's cookies coming up in just a little bit.



STEVE:  Ooh.



LEO:  Ooh, boy.



STEVE:  Sadly.



LEO:  Ooh, boy.  All right, Steve.  What's all this about cookies?  You really liked this whole Topics thing that they were going to do; right?



STEVE:  Well, it made sense.  I understood it.



LEO:  Yeah.



STEVE:  It would have worked.



LEO:  It was privacy forward, yeah.



STEVE:  Oh, absolutely.  So just sort of to recap.  Because web servers and web browsers operate query by query, one query at a time, long ago Mozilla designed a simple add-on called a cookie.  A website's server could easily give a browser one of these unique cookies, just a meaningless string of data to the browser.  But it would subsequently return that, thus identifying itself to the website for all subsequent activities.  This simple solution enabled the concept of being "logged on" to a website, which never existed before.  And this was the same way that users were previously able to log into other online services.  So it was a breakthrough.  But that's what it was meant for.



As this podcast's longtime listeners know, I've always been very annoyed by the abuse of this simple cookie technology by third parties, since cookies were purely and expressly intended to be used as a means for maintaining logged-on session state, and nothing more.  But the advent of third-party advertisers whose ads poked out onto tens of thousands of websites all over the world meant that their third-party cookies could be used to track and thus profile users as they moved across the web.



And as you said, Leo, for this reason I've been very excited and hopeful about actually all of Google's sincerely repeated attempts to design a workable alternative, which, you know, they would, they've said, once they had that, completely eliminate all support for third-party cookies.  Now, after that, the web would, in my opinion, finally be operating the way Mozilla originally intended, without cookie abuse, while still offering advertisers the feedback about the visitors to websites who were viewing their ads that they wanted.



The only problem was the achievement of this goal would also collapse the entire Internet tracking, profiling, and data aggregation industry.  And for that reason it appears that Google has failed in their quest, and that the tracking and profiling industry has won.  Yesterday, Monday, July 22nd, Google's VP of the Privacy Sandbox project effectively admitted defeat.



Anthony Chavez's posting was titled "A new path for Privacy Sandbox on the web."  Well, you know, the path they were on was the right path, so a new path is not going to be any righter.  Understanding what Anthony is really saying here requires a great deal of reading between the lines, though we've focused enough on this in the past that I think I know what actually happened.



Anyway, first, here's what he wrote.  He said:  "We developed the Privacy Sandbox with the goal of finding innovative solutions that meaningfully improve online privacy while preserving an ad-supported Internet that supports a vibrant ecosystem of publishers, connects businesses with customers, and offers all of us free access to a wide range of content."  Right.



He says:  "Throughout this process, we've received feedback" - I bet you have - "from a wide variety of stakeholders, including regulators like the UK's Competition and Markets Authority (CMA)" - that we were talking about not long ago - "and Information Commissioner's Office (ICO), publishers, web developers, and standards groups, civil society, and participants in the advertising industry.  This feedback has helped us craft solutions that aim to support a competitive and thriving marketplace that works for publishers and advertisers, and encourage the adoption of privacy-enhancing technologies.



"Early testing from ad tech companies, including Google, has indicated that the Privacy Sandbox APIs have the potential to achieve these outcomes.  And we expect that overall performance using Privacy Sandbox APIs will improve over time as industry adoption increases.  At the same time, we recognize this transition requires significant work by many participants and will have an impact on publishers, advertisers, and everyone involved in online advertising.  In light of this, we are proposing an updated approach that elevates user choice."  Now, that's called putting a good face on the problem.



They said:  "Instead of deprecating third-party cookies, we would introduce a new experience in Chrome that lets people make an informed choice that applies across their web browsing, and they'd be able to adjust that choice at any time.  We're discussing this new path with regulators and will engage with the industry as we roll this out."  They said:  "As this moves forward, it remains important for developers to have privacy-preserving alternatives.  We'll continue to make the Privacy Sandbox APIs available and invest in them to further improve privacy and utility.  We also intend to offer additional privacy controls, so we plan to introduce IP Protection into Chrome's Incognito mode.



"We're grateful to all the organizations and individuals who have worked with us over the last four years to develop, test, and adopt the Privacy Sandbox.  And as we finalize this approach, we'll continue to consult with the CMA, ICO, and other regulators globally.  We look forward to continued collaboration with the ecosystem on the next phase of the journey to a more private web."  And of course the problem is none of the other stakeholders want a more private web.



So third-party cookies will remain in Chrome, and it really appears unlikely that technologies such as the privacy-preserving Topics will gain any foothold since the advertising, tracking, profiling, and data aggregating industries want everything they can get their hands on.  And they appear to have won this battle by crying to European regulators that it's no fair for Google to take this away from them.



It'll be interesting to see how Google's capitulation manifests in this user-interface change they're talking about and whether websites begin insisting that their users enable what is going to be called, you know, whatever they end up calling it across their site's content.  You know, like, oh, you seem to have this turned off.  If you want to use this content, please turn it on.  Oh, and while you're at it, what's a good email address for you?



LEO:  But most people who certainly listen to this show turn off third-party cookies.  Is that setting honored in all browsers?



STEVE:  Yes.



LEO:  Okay.



STEVE:  Yes.  And I can vouch for that because this, you know, this has been a hobbyhorse of mine for quite a while.  If you were to google "GRC cookie forensics," it takes you to a series of pages I created years ago.  And actually I'm seeing that third-party cookie behavior is down among GRC.com's visitors.  Yes, you're showing it now.  That cookie forensics page is actually able to verify...



LEO:  Wow.



STEVE:  ...the percentage.  Yeah, so that used to be up at about 80%.  And now as we can see it says 17.8% of all GRC visitors have persistent third-party cookies enabled.  And that's of 21,767 unique GRC visitors.



LEO:  So people trying to get back on?



STEVE:  No, no, no.  Scroll down and look at the Apple number.



LEO:  Ah.



STEVE:  Because Apple is the only one who's always had this off by default.  So again, this shows the tyranny of the default.  Apple Safari visitors are down below 2% of them.



LEO:  Okay.



STEVE:  But it's on by default, and that says that a lot of people are turning it off.



LEO:  And that's because they listen to this show.  They know to turn it off.  That's good.



STEVE:  Yes.  And if you scroll down to the cookie forensics, there is a - let's see, which one?  There, number three, it'll actually do it.  That just tested your browser to look at your third-party cookie handling across the board.  And yours is completely locked down.



LEO:  And of course, because I listen to you, I have always turned off third-party cookies.



STEVE:  Yup.



LEO:  Good job.  So you know a lot about this.



STEVE:  Yes, this has been a focus of mine because it just really, you know, irks me that this has been so abused.  And what's interesting is back when I originally designed it, that forensics test, you'll notice it has eight different types of cookies it looks for of each type.  It's because browsers...



LEO:  They're sneaky.



STEVE:  So there's page, CSS, script, embedded, image, icon, iframe, and object.  Browsers used to be broken.  They were actually, even if you turned them off, some of them would still be on.  So this allowed us to profile whether browsers were even operating correctly.  And back in those early IE6 days, they weren't.



LEO:  Well, the other point I think it's really good to take from this page is that first-party cookies are okay.  That is a necessary system on the Internet.  You don't want to turn off all cookies.



STEVE:  You can't log on.  You can't log onto a website without first-party cookies.



LEO:  And it's a convenience.  It's the third-party cookies that are a problem.  And it's unfortunate because I think these cookie banners from the UK and the EU have kind of taught people all cookies are bad.  But that's not the case. 



STEVE:  No.



LEO:  Grr.



STEVE:  Yup.  Anyway, so what I think must have happened, Leo, is because we saw the writing on the wall, it was the EU and their regulators that were under pressure from...



LEO:  Advertisers, yeah.



STEVE:  Yes, the advertisers and the trackers and the data aggregators.  It's like, hey, this is our business.  You can't, you know, Google just can't come along and take away our business.  It's like, unfortunately, their business should not be ours.  And these have been, you know, mixed up.  So I don't know how to really read what Google's thoughts are in the future.  You know, they've got an API that requires, in order for it to work, they had to force everyone to use it.  Otherwise no one's going to use it.  You know, no one's going to make a change.  So I think we're stuck with third-party cookies.



LEO:  So again, if you google "GRC cookie forensics," you can see this page, free page on Steve's site.  It's a really useful and informational page that I wish all of these regulators would read, to be honest with you.  Oh, well.



STEVE:  They don't want it to be there.  They just want to, you know, skulk around in the dark and aggregate data about us.  Okay.  So I thought that our listeners would enjoy learning what I've been learning from the exercise of sending email in this era of hyper-vigilant anti-spam and anti-viral email protection measures.  Last Tuesday I found three causes for the trouble that I mentioned in getting the email to our listeners.



First, it turns out by sheer coincidence the thumbnail image of the Picture of the Week was indeed triggering a false positive detection from some AV scanners.  Later that evening, I removed the thumbnail from the email and re-sent the email to the 83 subscribers who had not received it due to an AV rejection false positive, and that time only three of them bounced.  So it was indeed just a coincidental thumbnail image.  There's nothing I can do about it.  And this week's thumbnail had no problem.



The second issue was, after carefully examining the feedback that I was receiving from some of the AV tools, I saw that some of them were complaining about the email containing a banned URL.  The exact quote was: "Contains a URL listed in the URIBL blacklist."  And guess what the URL was?  Polyfill.io.  I used that phrase in the email; and I didn't, like, put square brackets around the dot as I should have done.  So, yep, you got me on that one.  From now on I'll make sure that any dangerous URLs are rendered non-dangerous in any email that I send.  But, so, yeah, kind of props for the AV guys for catching it, even though it was a pain.



The third and final discovery was a complaint about invisible text in the email.  And so I stared at my code.  I handwrote the HTML.  Like, what?  There's no invisible text.  But it turns out I had taken an innocuous-looking line from several discussions about composing email for better viewing on a wider range of devices.  It was an HTML <div> line with its "display" style set to none and a font point size of 0.  I don't know why...



LEO:  Oh, it's a tracking pixel.  That's a tracking pixel.



STEVE:  Yes.  Well, no.  But no, no, there was no URL.



LEO:  It doesn't phone home.



STEVE:  There was no text.  There was no URL.  But what it did was it basically triggered another false positive.  And I removed it from this week's mailing, and I didn't have any trouble at all.  I should note, however, that it was mostly our listeners using Hover's email hosting service that were rejecting and bouncing last week's Security Now! email back to me.  The bounce message was:  "5.7.1:  Message blocked due to the very low reputation of the sending IP."  And it's like, okay, well, yeah, true.  I'm just getting started here at GRC, sending these kinds of mails.  So that was expected, and it doesn't make me love Hover any less.  They're still my beloved domain...



LEO:  Well, and you said it was a ClamAV, which that makes sense that they would use as an antivirus scan, it's a free open source program.



STEVE:  Yes, although this is - I'm not sure where the reputation of the sending IP comes from.



LEO:  Oh, right.



STEVE:  Yeah.  I did confirm with one of our listeners, a Paul Sylvester, whom I exchanged email with, he added securitynow@grc.com and mail-manager@gmail.com to his allow list.  There is an allow list at Hover in the webmail system.



LEO:  So you can do that.



STEVE:  Yes.  And today I only got four bounces back from Hover.  Everybody else apparently looked into it and fixed it.



LEO:  Do you use DKIM and SPF?  Do you use the authentication?



STEVE:  Oh, yeah.  You don't even get off the ground unless you've got SPF, DKIM and SPF.  I have them all.  The problem is, in the same way that I'm signing all of my SpinRite EXEs, yet Windows Defender still sometimes complains.  Similarly, even though my email is signed, if you don't have reputation, reputation matters.  And so GRC.com, you know, I've never been doing anything like this kind, I mean, even 7,000 pieces of email.  This morning 7,000 pieces of email went out to Security Now! listeners, and it almost went perfectly.



LEO:  Yeah.



STEVE:  So it was much better than last week, and I imagine it'll only keep getting better in the future.



LEO:  Well, and you build reputation as you use it.



STEVE:  Over time.



LEO:  Yeah, over time.



STEVE:  Right, yeah.  



Jeff Garretson in Yakima, Washington, said:  "One of Snowflake's primary target markets is data warehouse applications.  Traditionally, data warehouse databases are organized as a 'star schema,' with a central 'fact table' linked to multiple 'dimension tables' that provide context.  A variation is when one or more dimensions have enough internal complexity that it makes sense to break some attributes out into sub-dimensions.  Then the star schema diagram starts looking more complex, more like a snowflake.  So a 'snowflake schema' is a more general case of a star schema.'"  And he said:  "Hope that helps.  Love the show."



LEO:  Okay.



STEVE:  And Jeff, thank you.  That is indeed where the Snowflake name came from.  I also got a kick out of a much more playful posting.  At the moment, of course, as we know, Snowflake has a problem because some 350 of their clients had all of their data stolen from them, which does not make them happy.



But 10 years ago, back in 2014, a Snowflaker named Marcin Zukowski, who is a co-founder and VP of engineering at Snowflake, posted the following to Snowflake's blog.  He wrote:  "One of the questions that we get the most is, 'Why did you decide to name the company Snowflake?'"  He says:  "I'm sure our marketing department has their opinion of what we should say, but let me give you the real story.  The name Snowflake just fits us in several ways."  And he has four bullet points.



"First, Snowflakes are 'born in the cloud.'  For a data warehouse built from the ground up for the cloud, that's very important.  Second, we love the snow."  He says:  "We love the snow.  Most of our founding team, and even our first investor, love to spend time up in the mountains in the winter.  They even convinced me to try skiing, and took me on a black run my first day.  And third," he said, "each Snowflake is unique.



"One of the really cool things about our architecture is that it lets you have as many 'virtual warehouses' as you need, all in one system, each of which has exactly the right resources to fit the unique needs of each set of your users and workloads.  And conveniently, 'Snowflake' happens to have a meaning in the world of data warehousing  A data warehouse schema organized as multiple dimension tables surrounding a set of fact tables is one of the data architectures that we can support."  So now we know.



LEO:  Maybe more than we wanted to know, actually.



STEVE:  Well, we were making fun of them last week, so that's only fair.



LEO:  That's true, yes.



STEVE:  To explain where they came up with as funky a name.



LEO:  It's not completely out of the blue.



STEVE:  No.



LEO:  Well, it is, it's out of the cloud.



STEVE:  A listener requesting anonymity shared his experience following the recent CDK Global dealership outage.  He said:  "Hi, Steve.  I'm a software developer, and I develop an interface between my company's software and CDK dealerships.  Our software sends tens of thousands of transactions to CDK daily.  Our software tried to post many, many thousands of records during the outage; and since they were down, all transactions failed.  The article you mentioned last week about the accounting office needing to deal with the mess is spot on.



"As I'm listening to the podcast about this mess a couple of days after it came out, I'm in the middle of crafting SQL scripts to 'fake out' the system to make it think that items that the dealership accounting offices had to manually handle were already posted across to CDK so that they would not double book them.  That's basically anything that happened in June because the accounting offices had to close the books at the end of June.



"CDK cut all third-party interface access during their restoration.  Our interface access was finally restored a few days ago.  However, as part of the process of restoring our interface access, CDK changed our interface credentials which had remained the same for 15 years.  Yes, 15 years."  So regards from an anonymous listener.



John Meuser, writing about the Polyfill.io mess and the use of resource hashes, wrote:  "One thing I feel you should have mentioned with the SRI system" - you know, that's the system for tracking hashes of downloaded resources - "is that this does invalidate one of the reasons a web developer might use externally hosted resources.  Suppose there's a vulnerability found in one of the external libraries?  In that case, the website developer will have to update their URL and the hash before the vulnerability is fixed for their site.  If they blindly pull the latest compatible version, they will always have the latest bug fixes.  There will always be a difficult balance to be struck between convenience and security."



And John's right, of course.  As I noted last week, the only way to safely verify a downloaded resource against its known hash is if that resource never changes.  That can be assured only by specifying the resource's exact version number.  But as John notes, that also means that the web pages using that "pinned" version release will not be able to automatically receive the benefits of the version being moved forward as bugs are found and fixed.  So choose your poison.  Either tolerate a bug that may later be discovered until you're able to update the version that your website is pulling, or go for the latest automatically and hope that you never download malware by mistake, as the Polyfill.io debacle showed was possible.



Simon, an Aussie in the UK, as he describes himself, he says:  "I suggest MITM now stand for Miscreant in the Middle."  And I like that one a lot.  We don't need to change the abbreviation away from MITM, just the first "M" now stands for Miscreant.  Thank you, Simon.



Ryan Frederick wrote:  "You said on this week's show that you will make a Copilot+ blocker app in assembly, if Microsoft ever releases it.  At this time, the only Copilot+ certified PCs are ARM, while you're an x86 assembly developer.  That said, if anyone can learn ARM assembly in a week and release a patching program, it's you."



Okay.  So first, Ryan makes a very good point.  However, we know that there's no way ARM-based Windows machines will not also be able to emulate Intel x86 family instructions to run all existing Intel-based apps.  We know they'll have a problem with drivers because that's - because they're down in the kernel.  Apps they're going to be able to run.  And Microsoft has indicated that Copilot+ with Recall will be coming to Intel platforms just as soon as they're able to make it happen.  So I'm pretty sure that my style of app development will not be threatened.  And it will be a long time, as in decades, before the population of ARM-based Windows desktop becomes important or significant or threatens Intel.  So I think we're probably going to be okay.  Which is good because I don't think I would develop an app in ARM assembly for Windows, if that's what it took.  



Kris Quinby in Riverdale said:  "I'm a week behind the podcast, finishing an audiobook.  I'm also late in sending this feedback so it may have already been sent to you by hundreds of listeners."  No.  "In Episode 982" - which was just two weeks ago, so he's not that far behind - "you talked about a Linux Daemon that would monitor logs and modify firewall rules to block IP addresses that made unwanted connection attempts to the computer.  You stated that you could not think of any reason why that should not be in place on every computer that accepts incoming connections."



So he says:  "One downside is that active blocking can be used to create a denial of service condition.  If the attacker notices the active blocking, they can spoof the source addresses for connection attempts to make the server start blocking all inbound connections.  Since the connection does not need to be 'real' [he has in quotes], the TCP connection handshake is not required.  There can be a balance where the new firewall rules can have a time limit before they're removed.  But if left to 'block forever,' a server can effectively be disconnected from the Internet.  Signed, Kris."



Okay.  So generically, I completely agree with Kris.  He's absolutely correct that blocking incoming connections by IP opens up the possibility of creating deliberate denial of service attacks by deliberately filling up the block list of IP addresses.  But I'm a little unsure what he meant when he wrote "Since the connection does not need to be 'real,' the TCP connection handshake is not required," because I believe it is.  TCP-based services will not consider a client to be connected until the three-way handshake has been acknowledged.  It's true that the client can provide data along with the final ACK in its reply to the server's SYN/ACK, but those roundtrips definitely do need to occur before the server's TCP/IP stack decides that it has a valid connection.



And of course you can't get roundtrips back and forth if you're spoofing your source IP because the acknowledgment packet will go off to the IP that you're spoofing, not back to you, so you're unable to complete the handshake.  So while UDP services, which do not have a TCP three-way handshake, could definitely be spoofed to create such an attack, TCP-based services such as SSH, which is what we were talking about, the OpenSSH flaw fortunately cannot be spoofed.  So blocking based upon authentication failures would be spoof proof for them.



And Leo, this has brought us to the one-hour point, and we are now ready to talk about CrowdStruck.



LEO:  Oh, wow.



STEVE:  Let's do our third break, and then we're going to plow into...



LEO:  Get into it, yes.



STEVE:  ...exactly what happened. 



LEO:  I know everybody's really interested in your take on this, so I can't wait.  All right.  Back to CrowdStruck.  I'm very curious what you have to say.



STEVE:  The Legend of Channel File 291.  So I start this in the show notes with a picture from a listener.  This shows the blue screens of death at the deserted Delta terminal of the Seattle Tacoma Airport, which was taken Friday morning by a listener who has been listening since Episode 1.  And we see three screens.  It looks like maybe there's someone back in the distance there sort of behind one of the screens facing away from us.  But otherwise, there's nobody in line.



LEO:  It's deserted.



STEVE:  There's an empty wheelchair.  It's...



LEO:  4,000 flights canceled.  4,000 flights canceled.  For Delta alone.  By the way, Paul Thurrott, being a little bit of a pedant, says that's not the Blue Screen of Death, that's a recovery screen.  But you know what?  It's a Blue Screen of Death.  It's a Blue Screen of Death.



STEVE:  Okay, Paul.  Point taken.  So it's fortunate that GRC's incoming email system was already in place and ready for this CrowdStrike event.



LEO:  I bet you got the mail.  Whew.



STEVE:  Oh, did I.  And I'm going to share some, right from firsthand accounts from the field, because it enabled a number of our listeners to immediately write last week to send some interesting and insightful feedback.



LEO:  A lot of our listeners I'm sure have very sore feet going from machine to machine all weekend.



STEVE:  Oh.



LEO:  Holy cow.



STEVE:  In one case, 20,000 workstations.



LEO:  Oh, oh.



STEVE:  And these people don't hang out on Twitter, so email was the right medium for them.  Brian Tillman wrote:  "I can't wait to hear your comments next week about the current cloud outages happening today.  My wife went to a medical lab this morning for a blood test and was turned away because the facility cannot access its data storage."



LEO:  Wow.



STEVE:  Another listener wrote:  "Good morning.  I'm new to this group, only been" - I love this.  "I'm new to this group, only been listening for the last eight years."



LEO:  Oh, a newbie.



STEVE:  That's right.  You're going to have to go back and catch up, my friend.  He says:  "I'm sure CrowdStrike will be part of next week's topics."  Uh-huh.  He said:  "I would love to hear your take on what and how this happened.  I'm still up from yesterday, fixing our servers and end-users' computers.  I work for a large hospital in central California, and this has just devastated us.  We have fixed hundreds of our critical servers by removing the latest file pushed by CrowdStrike, and are slowly restoring services back to our end-users and community.  Thank you for all you do keeping us informed and educated of issues like this.  Looking forward to 999 and beyond."



LEO:  Oh, I like - that could be our new slogan, "999 and Beyond."  I like it.



STEVE:  Doot da doo.



LEO:  That's it.



STEVE:  Tom Jenkins posted to GRC's newsgroup:  "CrowdStrike is a zero-day defense software, so delaying updates puts the network at risk.  I don't know how they managed to release this update with no one testing it.  Seems obvious at this point even casual testing should have shown issues."  And of course Tom raises the billion-dollar, and I'm not probably exaggerating, the billion-dollar question:  How could this have happened?  And we'll be spending some time on that in a few minutes.  But I want to first paint some pictures of what our listeners experienced firsthand.



Tom's posting finished with:  "We had over 100 servers and about 500 workstations offline in this event, and recovery was painful.  Their fix required the stations to be up.  Unfortunately, the bad ones were in a boot loop that, for recovery, required manual entry of individual machine BitLocker keys to apply the fix."  And of course that was often a problem because machines that were protected by BitLocker needed to have the recovery keys present in order for their maintainers to be able to get to the file system, even after being booted into Safe Mode, because Safe Mode's not a workaround for BitLocker.



Seamus Marrinan, who works for a major corporation which he asked me to keep anonymous, although I asked for permission and he told me who it is, but asked for anonymity there.  He said:  "For us, the issue started at about 12:45 a.m. Eastern time.  We were responding to the issue" - and get a load of this response and the way his team operated.  So the issued started for him at 12:45 a.m. Eastern time.  He said:  "We were responding to the issue by 12:55" - 10 minutes later - "and had confirmed by 1:05 a.m. that it was a global level event, and communicated that we thought it was related to CrowdStrike.  We mobilized our team and had extra resources on site by 1:30 a.m.," so 25 minutes later.



"The order of recovery we followed were the servers, production systems, our virtual environment, and finally the individual PCs.  In all, there were about 500 individually affected systems across a 1,500-acre campus.  We were able to get to 95% recovery before our normal office hours started, and we were back to normal by 10 am."  Okay, now, I am quite impressed by the performance of Seamus's team.  To be back up and running by 10:00 a.m. the day of...



LEO:  That's amazing.



STEVE:  ...after 500 machines were taken down across a 1,500-acre campus, taken down in the middle of the night, is truly impressive.  And I would imagine that whomever his team reports to is likely aware that they had a world-class response to a global-scale event since, for example, another of our listeners in Arizona was walking his dog in a mall because it's too hot to walk pets outside during the day in Arizona.  He took and sent photos of the sign on Dick's Sporting Goods the following day, on Saturday, stating that they were closed due to a data outage.  So it took many other companies much longer to recover.



A listener named Mark Hull shared this.  He said:  "Steve, thanks for all you do for the security community.  I'm a proud SpinRite owner and have been an IT consultant since the days of DOS 3 and Netware 2.x."



LEO:  Wow.



STEVE:  Uh-huh.  He said:  "I do a lot of work in enterprise security, have managed CrowdStrike, write code, and do lots of work with SCCM," he says, "(MS endpoint management), as well as custom automation.  So I feel I have a good viewpoint on the CrowdStrike disaster."  He says:  "CrowdStrike is designed to prevent malware, and by doing so provide high availability to all our servers and endpoints.  The fact that their software may be responsible for one of the largest global outages is completely unacceptable.  As you have said many times, mistakes happen.  But this kind of issue represents a global company's complete lack of procedures, policies, and design that could easily prevent such a thing from happening."  Now, of course, this is, what do they call it, something quarterbacking.  Monday night?



LEO:  Yeah, yeah, Monday night quarterback, yeah, yeah.



STEVE:  Okay.



LEO:  20/20 hindsight, yeah.



STEVE:  Okay.  And I do have some explanations for this which we'll get to.  Anyway, he said:  "Given that CrowdStrike is continually updated to help defend against" - and I should just say no one's disagreeing with him, and Congress will be finding out before long, you know, what happened here.  But he said:  "Given that CrowdStrike is continually updated to help defend against an ever-changing list of attacks, the concept of protecting their customers from exactly this type of issue should be core to their design.  Working in automation, the rule is that you always have a pilot group to send out software before you send it to everyone."



He says:  "I work with organizations with easily over 100,000 users.  If you don't follow these rules, you eventually live with the impact.  In the old days, companies would have a testing lab of all kinds of different hardware and OS builds where they could test before sending anything out to production.  This would have easily caught the issue," he says.  "Now it seems that corporations have eliminated this idea since this is not a revenue-generating entity.  They should research opportunity cost."  He says:  "With the onset of virtualization, I would argue the cost of this approach continues to decrease."  And again, at this point this is speculation because we don't understand how this happened.



But, he says:  "Since it appears this was not being done, another software design approach would be to trickle out the update, then have the code report back metrics from the machines that received the update at some set interval.  For instance, every five, 10, 30 minutes the endpoints could send a few packets with some minor reporting details, such as CPU utilization, disk utilization, memory utilization.  Then, if CrowdStrike pushed an update, and the first thousand machines never reported back after five minutes, there would be some automated process to suspend that update and send emails out to the testing team.  In the case of endpoints that check back every 10 minutes, you could set a counter," and blah blah blah.



Anyway, he goes on to explain, you know, the sorts of things that make sense for means of preventing this from happening.  And, yes, I agree with him completely.  From a theoretical standpoint, there are all kinds of ways to prevent this.  And again, as I said, we'll wrap up by looking at some of that in detail.



Samuel Gordon-Stewart in Canberra, Australia, wrote.  He said:  "Here in Australia it was mid-afternoon on a Friday.  Most broadcast media suffered major outages, limiting their ability to broadcast news, or anything else for that matter.  Sky News Australia had to resort to taking a feed of Fox News as they couldn't even operate the studio lights.  They eventually got back on air in a limited capacity from a small control room in Parliament House.  The national government-funded broadcaster ABC had to run national news instead of their usual state-based news services and couldn't play any pre-recorded content, so reporters had to read their reports live to camera.  A lot of radio stations were still unable to broadcast even Friday night.



"Supermarkets had their registers go down.  One of the big supermarkets near me had half their registers offline.  A department store nearby had only one register working.  Train services were halted as the radio systems were all computerized.  Airports ground to a halt.  Half a dozen banks went offline.  Telecommunication companies had outages.  Many hospitals reverted to paper forms.  A lot of state government systems seemed to be affected, but the federal government seemed less impacted.  And who knows how long it will take for IT departments to be able to physically access PCs which won't boot so they can implement the fix.  As you would say, Steve, about allowing a third party unilaterally updating kernel drivers worldwide whenever they want, 'What could possibly go wrong?'"



After I thanked Samuel for his note, he replied with a bit more, writing:  "In my own workplace, we're offline until Monday.  I think we got lucky because our network gateway was the first to take the update and failed before anything else..."



LEO:  How clever.



STEVE:  "...had a chance to receive the update."  I love that.



LEO:  So it took them offline, but fortunately it stopped the update for everybody else.  That's good.  I like it.



STEVE:  Yup.  He said:  "Nothing will get fixed until the Head Office looks at it, but I think they'll be pleasantly surprised that only a couple of devices need fixing, and not dozens or more.  Not my problem or role these days, although I did foolishly volunteer to help."



LEO:  Good man.



STEVE:  And I saw the following on my Amazon app on my iPhone.  I got a little popup that said:  "A small number of deliveries may arrive a day later than anticipated due to a third-party technology outage."



LEO:  I saw that too, yeah.



STEVE:  Yup.  Meanwhile in the U.S., almost all airlines were grounded with all their flights cancelled.  One, however, was apparently flying the friendly skies all by itself for the day.



LEO:  Before you repeat this story, it's been debunked.



STEVE:  I'm not surprised.



LEO:  Yeah.  It didn't seem possible.



STEVE:  Yes.  Digital Trends reported under the headline "A Windows version from 1992 is saving Southwest's butt right now."  Anyway, yeah.



LEO:  Southwest got saved because they didn't use CrowdStrike is how they got saved, not because they were using Windows 3.1.



STEVE:  Yes, exactly.  There are companies all over the world who are not CrowdStrike users.



LEO:  Exactly.



STEVE:  Exactly.  And so it was only those who had this csagent.sys device driver loading at boot time in their kernel that had this problem.



LEO:  Yup, yup.



STEVE:  So, and I think that this was made more fun of because in the past, remember that Southwest Airlines has come under fire for having outdated systems.



LEO:  Right, yes.  But not that outdated.



STEVE:  Yeah, they had scheduling systems they hadn't updated for a long time.



LEO:  Yeah.  It's an interesting story, and somebody on Mastodon kind of went through it.  And really this happens a lot in journalism nowadays.  Somebody tweeted that the Southwest scheduling software looked like it was Windows 95.  It wasn't, but looked that way.  It got picked up and, like Telephone, it got elaborated to the point where DigiTrends and a number of other outlets, including I might add myself, reported this story.  And then we found out it was, you know, Southwest that confirmed it.



STEVE:  Yeah.  And really, I mean, even I'm having problems today on Windows 7 because, you know...



LEO:  Yeah, you can't run Windows 3.1.



STEVE:  ...increasingly things are saying, what are you thinking, Gibson?  Like, what is wrong with you?  So, yeah.



LEO:  You'd have to really, really work hard to keep 3.1 up and running, I think.



STEVE:  Yeah.  So I was initially going to share a bunch of TechCrunch's coverage, but then yesterday Catalin Cimpanu, the editor of the Risky Business newsletter, produced such a perfect summary of this event that only one important point that TechCrunch raised made it - later - into today's podcast, which I'll get to in a minute.  But first, here's Catalin's summary, which was just - it's perfect.



So he writes:  "Around 8.5 million Windows systems went down on Friday in one of the worst IT outages in history.  The incident was caused by a faulty configuration update to the CrowdStrike Falcon security software that caused Windows computers to crash with a Blue Screen of Death."  Paul, we realize that's not what it is, thank you.  "Since CrowdStrike Falcon is an enterprise-centric EDR, the incident caused crucial IT systems to go down in all the places you don't usually want them to go down.  Outages were reported in places like airports, hospitals, banks, energy grids, news organizations, and loads of official government agencies.



"Planes were grounded across several countries, 911 emergency systems went down, hospitals canceled medical procedures, ATMs went offline, stock trading stopped, buses and trains were delayed, ships got stuck in ports, border and customs checks stopped, Windows-based online services went down."  He says,  for example, ICANN.  "And there's even an unconfirmed report that one nuclear facility was affected.  The Mercedes F1 team, where CrowdStrike is a main sponsor, had to deal with the aftermath, hindering engineers from preparing the cars for the upcoming Hungarian GP.  Heck," he wrote, "even Russia had to deal with some outages."  Whoops.  I guess they're not quite Windows-free yet over there.



He says:  "It was a cluster you-know-what on so many levels that it is hard to put into words how much of the world was upended on Friday, with some outages extending into the weekend.  Reddit is full of horror stories where admins lost their jobs, faced legal threats, or were forced to sleep at their workplace to help restore networks.  There are reports of companies having tens of thousands of systems affected by the update.



"The recovery steps aren't a walk in the park, either.  It's not like CrowdStrike or Microsoft could have shipped a new update and fixed things in the span of a few minutes.  Instead, users had to boot Windows into Safe Mode and search and delete a very specific file.  The recovery cannot be fully or remotely automated, and an operator must go through the process on each affected system.  Microsoft has also released a recovery tool which creates a bootable USB drive that IT admins can use to more quickly recover impacted machines, but an operator still needs to be in front of an affected device.



"For some super lucky users, the BSOD error corrected itself just by constantly rebooting affected systems.  Apparently, some systems were able to gain short enough access to networking capabilities to download the fixed CrowdStrike update file and overwrite the old buggy one.  However, this is not the universal recommended fix.  There are people reporting that they managed to fix their systems after three reboots, while others needed tens of reboots.  It took hours for the debug information to make its way downstream, meaning some of the world's largest companies had to bring their businesses to a halt, losing probably billions in the process."  And he said:  "Extremely rough estimation, but probably in the correct range."



He writes:  "Unfortunately, the Internet is also full of idiots willing to share their dumb opinions.  In the Year of the Lord 2024, we had people argue that it's time to ditch security products since they can cause this type of outage.  Oh, yes, that's the solution (eye-roll).  But CrowdStrike's blunder is not unique, or new for that matter.  Something similar impacted loads of other vendors before, from Panda Security to Kaspersky and McAfee.  Ironically, CrowdStrike's founder and CEO George Kurtz was McAfee's CTO at the time, but don't go spinning conspiracy theories about it.  It doesn't actually mean that much."



He writes:  "Stuff like this tends to happen, and quite a lot.  As an infosec reporter, I stopped covering these antivirus update blunders after my first or second year because there were so many, and the articles were just repetitive.  Most impact only a small subset of users, typically on a particular platform or hardware specification.  They usually have the same devastating impact, causing BSOD errors and crashing systems because of the nature of security software itself, which needs to run inside the operating system kernel so it can tap into everything that happens on a PC.



"CrowdStrike released an initial post-mortem report of the faulty update on Saturday.  It blamed the issue on what the company calls a 'channel file' update, which are special files that update the Falcon endpoint detection and response" - that's EDR, that's what EDR stands for - "client with new techniques abused by threat actors.  In this case, it was Channel File 291."  And then he gives us the full file name C-00000291*.sys that causes the crashes.



"CrowdStrike says this file is supposed to update the Falcon EDR to detect malware that abuses Windows named pipes to communicate with its command and control server.  Such techniques were recently added to several C2 frameworks - tools used by threat actors and penetration testing teams - and CrowdStrike wanted to be on top of the new technique.  The company says the Falcon update file, unfortunately" - yeah, unfortunately - "triggered a logic error.  Since Falcon ran in the Windows kernel, the error brought down the house and caused Windows to crash with a BSOD.  After that, it was just a house of cards.  As the update was delivered to more and more CrowdStrike customers, the dominos started falling all over the world.



"Kurtz [the CEO] was adamant on Friday that this was just an error on the company's part and made it explicitly clear that there was no cyberattack against its systems.  U.S. government officials also echoed the same thing.  For now, CrowdStrike seems to be focused on bringing its customers back online.  The incident is likely to have some major repercussions going beyond the actual technical details and the global outages.  What they will be, I cannot be sure," he writes, "but I smell some politicians waiting to pounce on it with some 'ideas,'" he has in quotes.  Oh, great.



"This might also be the perfect opportunity/excuse for Microsoft to go with Apple's route and kick most security vendors and drivers out of the kernel.  But before that, Microsoft might need to convince the EU to dismiss a 2009 agreement first.  Per this agreement, Microsoft cannot wall off its OS from third-party security tools.  The EU and Microsoft reached this arrangement following an anti-competitive complaint filed by security software vendors after Microsoft entered the cybersecurity and AV market with Defender, with vendors fearing Microsoft would use its control over Windows to put everyone out of business by neutering their products."  Doesn't this sound like, no, sorry, we can't cancel third-party cookies because some people are making money with them.  Right.



He writes:  "After the recent Chinese and Russian hacks of Microsoft cloud infrastructure, we now know very well what happens when Microsoft has a dominant market position, and it's never a good thing, so the existence of this agreement isn't such a bad idea.  If Microsoft wants to kick security software out of the kernel, Defender needs to lose it, too.  Unfortunately, blinding security tools to the kernel now puts everyone in the iOS quandary, where everyone loses visibility into what happens on a system.  That's not such a good idea, either.  So we're back with this argument where we started.



"In closing, just be aware that threat actors are registering hundreds of CrowdStrike-related domains that will most likely be used in spear-phishing and malware delivery campaigns."



LEO:  They're so evil.  God.



STEVE:  "It's honestly one of the best and easiest phishing opportunities we've had in a while."



So as suggested by this week's Picture of the Week, which shows the Windows kernel crash dump resulting from CrowdStrike's detection file update, I will be getting down to the nitty-gritty details that underlie exactly what happened.  But I want to first finish laying out the entire story.  The part of TechCrunch's coverage that I wanted to include was their writing this:  "CrowdStrike, founded in 2011, has quickly grown into a cybersecurity giant.  Today, the company provides software and services to 29,000 corporate customers, including around half of Fortune 500 companies, 43 out of 50 U.S. states, and eight out of the top 10 tech firms, according to its website.  The company's cybersecurity software, Falcon, is used by enterprises to manage security on millions of computers around the world."  And now we know exactly how many millions.  "These businesses include large corporations, hospitals, transportation hubs, and government departments.  Most consumer devices do not run Falcon and are unaffected by this outage."



And with that lead-up, this was the point:  "One of the company's biggest recent claims to fame was when it caught a group of Russian government hackers breaking into the Democratic National Committee ahead of the 2016 U.S. presidential election. CrowdStrike is also known for using memorable animal-themed names for the hacking groups it tracks based on their nationality, such as Fancy Bear..."



LEO:  Oh, that's where that came from.  Ah.



STEVE:  "...believed to be part of Russia's General Staff Main Intelligence Directorate, or GRU; Cozy Bear, believed to be part of Russia's Foreign Intelligence Service, or SVR; Gothic Panda, believed to be a Chinese government group; and Charming Kitten, believed to be an Iranian state-backed group.  The company even makes action figures to represent these groups, which it sells as swag."



LEO:  I want them.  Oh, cool.



STEVE:  "CrowdStrike is so big it's one of the sponsors of the Mercedes F1 team, and this year even aired a Super Bowl ad  a first for a cybersecurity company."



LEO:  And they were also one of the first cybersecurity companies to advertise on this show, Steve.



STEVE:  Right.



LEO:  In fact, we interviewed the CTO some time ago.  It's an impressive company.  You know, I'm very curious to hear what happened here.



STEVE:  So as I have here written, I have not counted the number of times this podcast has mentioned CrowdStrike.  It's certainly been so many times that their name will be quite familiar to everyone who's been listening for more than a short while.  And not one of those previous mentions was due to some horrible catastrophe they caused.  No.  As the writer for TechCrunch reminds us, CrowdStrike has been quite instrumental in detecting, tracking, and uncovering some of today's most recent and pernicious malware campaigns and the threat actor groups behind them.



How are they able to do this?  It's entirely due to exactly this same Falcon Sensor instrumentation system that's been spread far and wide around the world.  It's this sensor network that gives them the visibility into what those 8.5 million machines that have been running it are encountering day to day in the field.



LEO:  Exactly.  Exactly.



STEVE:  And we need that visibility.



LEO:  By the way, here is the Aquatic Panda figurine, currently $28 on the CrowdStrike swag shop.



STEVE:  Wow, you've got to be really deep into whatever.  Wow.



LEO:  I want Fancy Bear.  Okay.  Sorry.



STEVE:  Okay.  So this is not to in any way excuse the inexcusable.  Make no mistake that what they just caused to happen was inexcusable.  But any coherent answer to the question "What is CrowdStrike and why in the world are we putting up with them?" should in fairness acknowledge that the same network that just crippled much of the world's cyber operations has also been responsible for discovering malicious activities and protecting not only their own customers, but all of the rest of us, as well.



Catalin referred to George Kurtz, late of McAfee and now CrowdStrike's Founder and CEO.  George's note about this was addressed to "Valued Customers and Partners."  He wrote:  "I want to sincerely apologize directly to all of you for today's outage.  All of CrowdStrike understands the gravity and impact of the situation.  We quickly identified the issue and deployed a fix, allowing us to focus diligently on restoring customer systems as our highest priority.  The outage was caused by a defect found in a Falcon content update for Windows hosts.  Mac and Linux hosts are not impacted.  This was not a cyberattack.  We're working closely with impacted customers and partners to ensure that all systems are restored, so you can deliver the services your customers rely on.



"CrowdStrike is operating normally, and this issue does not affect our Falcon platform systems.  There's no impact to any protection if the Falcon sensor is installed.  Falcon Complete and Falcon OverWatch services are not disrupted.  We will provide continuous updates through our Support Portal via the CrowdStrike blog.  Please continue to visit these sites for the latest updates.  We've mobilized all of CrowdStrike to help you and your teams.  If you have questions or need additional support, please reach out to your CrowdStrike representative or Technical Support.



"We know that adversaries and bad actors will try to exploit events like this.  I encourage everyone to remain vigilant and ensure that you're engaging with official CrowdStrike representatives.  Our blog and technical support will continue to be the official channels for the latest updates.  Nothing is more important to me than the trust and confidence that our customers and partners have put into CrowdStrike.  As we resolve this incident, you have my commitment to provide full transparency on how this occurred and steps we're taking to prevent anything like this from happening again.  George Kurtz, CrowdStrike Founder and CEO."



And with that, this podcast can now dig into some of the interesting and more technical nitty-gritty that will answer all of the remaining questions.  Leo, let's take our final break, and then we're going to do that.



LEO:  All right.  Fascinating stuff.  I wonder if CrowdStrike ever will really tell us what happened in greater detail than they have.



STEVE:  The industry has reverse engineered, and I'll be talking about that.



LEO:  Oh, good.  Okay, well, that's one way to find out.  Good.  All right.  We'll get to that in a moment.  All right.  So a little reverse engineering, and maybe we can figure out what happened here.  You know, the email you read a few emails ago is the question everybody's asking.  How could a company do this?  How could they not have tested?  How could they not know?  But to understand that, we need to understand what happened.



STEVE:  Right.



LEO:  And that's your job.



STEVE:  Okay.  So let's begin with CrowdStrike's predictably not-very-technical update which they titled:  "Falcon update for Windows hosts technical details."  And I'm doing this because I need to establish the context for what we're going to talk about next.  So they ask themselves the question, what happened?  And they answer:  "On July 19th, 2024 at 04:09 UTC, as part of ongoing operations, CrowdStrike released a sensor configuration update to Windows systems.  Sensor configuration updates are an ongoing part of the protection mechanisms of the Falcon platform.  This configuration update triggered a logic error resulting in a system crash and blue screen (BSOD)" - they actually said that - "on impacted systems.



"The sensor configuration update that caused the system crash was remediated on Friday, July 19th, 2024 at 05:27 UTC."  So 4:09 it happened; 5:27 they fixed it.  Of course the damage was done; right?  You're stuck in a boot loop, you can't update yourself.  They said:  "This issue is not the result of or related to a cyberattack.  Customers running Falcon sensor for Windows version 7.11 and above, that were online between Friday at 04:09 UTC and up to 05:27 when they fixed it, may be impacted.  Systems running Falcon sensor for Windows version 7.11 and above that downloaded the updated configuration from, again, those same two times, were susceptible to a system crash."  Yeah, no kidding.  Find somebody who doesn't know that.



Okay.  Then Configuration File Primer, however you want to pronounce it:  "The configuration files mentioned above are referred to as Channel Files and are part of the behavioral protection mechanisms used by the Falcon sensor.  Updates to Channel Files are a normal part of the sensor's operation and occur several times a day in response to novel tactics, techniques, and procedures" - you know, TTPs - "discovered by CrowdStrike.  This is not a new process; the architecture has been in place since Falcon's inception."



Then Technical Details:  "On Windows systems, Channel Files reside in the following directory:  C:\Windows\System32\drivers\CrowdStrike.  So they have their own subdirectory under drivers, and have a filename that starts with C-."  Then they said:  "Each channel file is assigned a number as a unique identifier.  The impacted Channel File in this event is 291 and will have a filename that starts with C-00000291- and ends with a .sys extension.  Although Channel Files end with the .sys extension, they are not kernel drivers.  Channel File 291 controls how Falcon evaluates named pipe execution" - and I'll talk about that in a second - "on Windows systems.



"Named pipes are used for normal, interprocess, or intersystem communication in Windows.  The update that occurred at 04:09 UTC was designed to target newly observed, malicious named pipes being used by common C2 framework" - you know, command-and-control frameworks - "in cyberattacks.  The configuration update triggered a logic error that resulted in an operating system crash.  CrowdStrike has corrected the logic error by updating the content in Channel File 291.  No additional changes to Channel File 291 beyond the updated logic will be deployed."  Meaning they're not changing their driver.



"Falcon is still evaluating and protecting against the abuse of named pipes.  This is not related to null bytes contained within Channel File 291 or any other Channel File."  That was another, you know, specious rumor that took hold of the Internet that the file was all nulls.



"The most up-to-date remediation recommendations and information can be found on our blog or in the Support Portal.  We understand that some customers may have specific support needs, and we ask them to contact us directly.  Systems that are not currently impacted will continue to operate as expected, continue to provide protection, and have no risk of experiencing this event in the future.  Systems running Linux or macOS do not use Channel File 291 and were not impacted."



And they finish with this "Root Cause Analysis."  They said:  "We understand how this issue occurred, and we are doing a thorough root cause analysis to determine how this logic flaw occurred.  This effort will be ongoing.  We are committed to identifying any foundational or workflow improvements that we can make to strengthen our process.  We will update our findings in the root cause analysis as the investigation progresses."



Okay.  So now we have a comprehensive understanding of what happens to today's truly IT-dependent global operational existence if around 1%, or about 8.5 million, extra-specially secured and running Windows OS-based machines are all caused to spontaneously crash and refuse to return to operation.  It's not good.  I was unable to use my mobile app to remotely order up my morning five shots of espresso from Starbucks.  This is unacceptable.  Something must be done.



Before we address the most burning question of all, which is how CrowdStrike and their systems could have possibly allowed something so devastating to occur, let's first answer the question of why Windows' own recovery systems were also unable to be effective in recovery from this.  Many years ago this podcast looked at a perfect example of a so-called "rootkit" in the form of Sony Entertainment's deliberately installed DRM, digital rights management software.



We saw how a normal directory listing of files could be "edited" on the fly to remove all visibility of specific files, turning them into ghosts.  The files were there, but we could not see them.  Sony's programmers did that to hide the presence of their own DRM.  That Sony rootkit demonstrated that we could not believe our own eyes, and it perfectly drove home just how much we depend upon the integrity of the underlying operating system to tell us the truth.  We take for granted that when we ask for a file listing, it's going to show all of them to us.  Why would it lie?  Why indeed?



If malware of that sort is able to infect the core operation of an operating system, then it's able to hide itself and strongly protect itself from both discovery and removal.  That was Sony's goal with their rootkit.  This fact creates a competition between the good guys and the bad guys to get in and establish the first foothold in an operating system because the entity that arrives on the scene first is able to use its placement to defend itself from anything that might happen afterwards.



For that reason, the actual CrowdStrike pseudo-device driver - not the mistaken channel file, but the actual driver that later reads those channel files - which Microsoft themselves previously examined, approved, and digitally signed, was given the highest honor of being flagged as a "boot-start" device.  Microsoft's own documentation says:  "A boot-start driver is a driver for a device that must be installed to start the Microsoft Windows operating system."



In other words, once device driver code is present containing that "boot-start" flag, Windows is being told and believes that it must load that driver in order to successfully start itself running.  For all it knows, that driver is for the mass storage RAID array that contains Windows itself; and if that driver is not installed and running by the time the motherboard firmware has finished loading the various pieces, Windows will be unable to access its own mass storage.  So Windows may not know why, but it does know that a device driver that carries a valid Microsoft digital signature that has the "boot-start" designation must be loaded.  So it will be loaded and initialized in order for Windows to successfully boot.  And this brings us to CrowdStrike's boot-start driver.



CrowdStrike's engineers designed a powerful driver that's designed to significantly augment Windows' own antimalware defenses.  When it's loaded into Windows, it hooks many of Windows' Application Programming Interface (API) functions.  What this means is that it places itself in front of Windows, so that any code that would normally ask Windows to do something on its behalf will instead be asking CrowdStrike's code.  And only if CrowdStrike sees nothing wrong with the request will CrowdStrike, in turn, pass that application's request on to Windows.  It creates a defensive wrapper shell around the Windows kernel.  And if this sounds exactly like what a rootkit does, you would be right.  CrowdStrike's driver kit is in the root.  It needs to, to get its job done.



What little we've learned about the specifics of this CrowdStrike failure is that it involved named pipes.  And that again makes sense since named pipes are used extensively within Windows for interprocess communications. It's pretty much "the" way clients obtain services.



The on-the-fly code-signing service which I created before the release of SpinRite 6.1 is started by Windows after it finishes booting.  When that service starts up, the one I created, it uses the Windows API to create a "named pipe" with a unique name.  Then later, when a user purchases a copy of SpinRite, GRC's web server opens a dynamic connection to the code-signing service by opening a named pipe of the same name.  This deliberate name collision connects the two separate events to establish a highly efficient interaction which allows the service and its client to then negotiate the details of what needs to be done.



CrowdStrike told us that malware had been seen using the named pipes API for communication with its command and control servers, so they needed to "hook" Windows named-pipes API in order to monitor the system for, and to possibly block, that activity.  And clearly something went terribly wrong.



For reasons that CrowdStrike is still being silent about, the presence of this channel file caused a bad parameter to be passed to a function.  This week's Picture of the Week is a snapshot of the crash event that brought down all of those 8.5 million Windows machines.  We see a procedure being called with two parameters.  The first parameter is zero, and the second parameter has the value of hexadecimal 9C, nine Charlie, which is decimal 156.  That value of 156 is loaded into the CPU's r8 register, where it's then used as a pointer to point to the value in question.



So then the CPU is asked to load the 32-bit value from that location into the CPU's r9 register.  The only problem is, the way the system's memory is mapped, there's nothing, no memory, located at location 156.  The processor, realizing that it has just been asked to load 32 bits of memory that does not exist, figures that something must have gone very, very wrong somewhere, so it panics.



LEO:  And Steve doesn't get his venti latte.  It's just that simple.



STEVE:  I can't get my five shots.  I have to actually show up in person.  It's like the Pilgrims, Leo.  Suddenly we're reduced to fighting Indians.



LEO:  It's not really funny for the, I mean, some people got stranded for two or three days in the airport.



STEVE:  Well, and IT admins lost their jobs.  There were people fired because they were blamed for this.



LEO:  Oh, that's sad.  That's really sad.



STEVE:  Yes.



LEO:  No one should be blamed for this.



STEVE:  Yes.



LEO:  Except CrowdStrike, I guess.  So...



STEVE:  Okay.  So...



LEO:  It's interesting.  So it jumped to a kernel panic because it jumped to an area of memory that doesn't exist.  That's what a machine is supposed to do.  That's what a blue screen does.  That's the whole point.



STEVE:  Right.  Yes.  The kernel panics, and it declares an emergency.



LEO:  It's over, yeah, right.



STEVE:  Yes.  Now, what I just described is a very, exactly as you said, Leo, a very typical series of events which precipitates what the entire world has come to know as the Blue Screen of Death.



LEO:  Yeah.



STEVE:  It's something that should never happen.  But hardware is not perfect, and neither are people.



LEO:  And I've seen it, it's a green screen on the Mac.  It's a black screen...



STEVE:  A black screen on Linux.



LEO:  On Linux, yeah.  So every operating system does this.  There's no reasonable way to recover from a failure like this.



STEVE:  No.  Things that are not supposed to ever happen, happen anyway, is the point.



LEO:  Right.



STEVE:  And just to be clear, if an application running on top of the Windows operating system as one of its client applications, were to ever ask the processor to do something impossible, like read from non-existent memory or divide by zero, Windows could and would just shrug it off and allow the app to crash.  Windows might display some mumbo-jumbo on the screen that would be entirely meaningless to the app's user, but life would go on without any other app in the system caring or being the wiser.  The crucial difference is where this unrecoverable error occurs.  When it occurs deep within the operating system kernel itself, there's no one to call.  It's game over.  And the thing that gets terminated is the operating system itself.



Now, we do not know exactly where that aberrant value of 156 came from.  There's been speculation that it looks like an offset into a structure, and that would make sense.  So that if the structure's pointer itself were null, an offset like this might result, and then be fed to a function that was expecting to be pointing to a member of a valid object, instead of out into hyperspace.  Some have observed that CrowdStrike's channel file contains nulls.



But the unsatisfying reality is that all we have today is conjecture.  There is no question that CrowdStrike definitely and absolutely already knows exactly what happened, but they're not saying.  I have absolutely zero doubt that their corporate attorneys clamped down the cone of silence over CrowdStrike so rapidly and forcibly that no one inside CrowdStrike even dares to mumble about this in their sleep.  I mean, this is, you know, to say that there will be repercussions has got to be the, you know, just doesn't even begin.



Okay.  There is very likely good news on that front, however, that is, what actually happened.  CrowdStrike's driver and its associated Channel File 291, now infamous, which together triggered this catastrophe, still exist in the world.  And the world contains a great many curious engineers who are gifted at reverse engineering exactly such disasters.  So I doubt we'll be waiting long before we receive a beautifully detailed explanation of exactly what happened, at least at the client end.  But it will not be forthcoming from CrowdStrike, and it won't need to be because we're going to figure this out as an industry.



But now we face the final biggest question of all, which is how could CrowdStrike, or any other company for that matter, having as much at stake and to lose from being the proximate cause of this incredibly expensive earthshaking catastrophe, possibly have allowed this to happen?  The thing that everyone wants to know is how could CrowdStrike have possibly allowed this defective Channel 291 File to ever be deployed at all?  And when I say that everyone wants to know, that now of course includes members of the United States Congress who are demanding that George Kurtz present himself before them on his knees with his head bowed and his neck exposed.



I have a theory.  Not long ago, just a few months ago, Microsoft asked us to believe that an incredibly unlikely, credulity-stretching chain of events - as I recall it was five unlikely failures, each of which were required for a private key to leak - enabled a malicious Chinese actor to obtain Microsoft's private key, which led to that large and embarrassing Outlook 365 email compromise.  Assuming that all of this was true, it's clear that so-called "black swan" events can occur, no matter how unlikely they may be.  So one view of this is that CrowdStrike does indeed have multiple redundant systems in place to guard against exactly such an occurrence.  After all, how could they not?  Yet, as we're told happened to Microsoft, despite all of that, something was still able to go horribly wrong.



But I said "one view" above because there's another possibility.  It's not outside the box to imagine that this particular failure path, whatever it was, was actually never expected or believed to be possible by the system's designers, so there may not actually have been any pre-release sandboxed sanity testing being done to those multiple times per day malware pattern updates that CrowdStrike was publishing to the world.  And remember that CrowdStrike had been doing this successfully and without any massive incident such as this for many, many years.



Now, I understand that to someone who does not code, like those in the U.S. Congress, this is going to sound quite nutty.  But developers typically only test for things that they know or imagine might possibly go wrong.  If a coder adds two positive numbers, they don't check to make sure that the result is greater than zero because it must be.  So I would not be surprised to learn that what happened slipped past CrowdStrike's own sanity-checking verifiers because it was something that was not believed to be possible.  And while now, yes, a final last-stage sanity check against live systems prior to deployment seems like the obviously important and necessary addition, either it must not have seemed necessary and hasn't ever been in the past, or it IS present, and nevertheless it somehow failed, much as those five sequential and separate failures within Microsoft allowed their closely held secret to escape.



Also, we don't know exactly what sort of CDN (content delivery network) they're using.  If 8.5 million instances of their device driver are reaching out for more or less continual updates 24/7, they're likely using some distribution network to spread those updates.  What if everything WAS good at CrowdStrike's end, but the upload of that file to the cloud somehow "glitched" during transit, allowing that glitched file to then be distributed to the world's Windows machines?  Of course, we have means for protecting against this, too, digital signing by the authorized sender and signature verification by the recipient.  We would certainly hope that these updates are signed, and any failure of signature verification would prevent the file's use.  But we've seen that not happening, too, in the past.  So distribution failure should, you know, we would hope it does not explain the trouble, but it might.



I'll finish with the acknowledgement that what has happened is so bad that I doubt we're going to soon learn the truth from CrowdStrike, who wrote:  "We understand how this issue occurred, and we are doing a thorough root cause analysis to determine how this logic flaw occurred.  This effort will be ongoing."  Okay.  That's just gibberish.  You say you understand how this issue occurred because you don't want to seem totally incompetent, but you're not sharing that understanding because you're too busy determining how it occurred.  Right.



Again, I expect that our industry's beloved reverse engineers will have a great deal to share long before CrowdStrike's attorneys finish proofreading and massaging the carefully worded testimony which George Kurtz will soon be delivering in front of Congress.



LEO:  So really any number of things could have happened.  It's hard to ascribe blame if you don't know, you know, like they may have done testing, and it could have been a flaw in the CDN.  So, yeah.  We just need to hear more from them.  Probably won't.  Probably won't.



STEVE:  Yeah.  I've heard reports, again unconfirmed, that the files are not signed.  If the files are not signed, that's a crime.



LEO:  Yeah.



STEVE:  Because if that's true, it would mean that it would be possible for a glitch, like a transmission glitch in the content delivery system to cause this.



LEO:  Yeah.  Or a miscreant in the middle.  Just stick another one in.



STEVE:  A miscreant in the middle. 



LEO:  Yeah.



STEVE:  But, you know, really we can't rule that out.



LEO:  And I do blame Microsoft a little bit.  I guess they blame the EU.  But to allow an interpreter to run in ring zero, and then to allow these unsigned files, if they're unsigned, to run in the interpreter is a very risky proposition.  Right?  You've talked all along about how interpreters are the number one cause for these problems.



STEVE:  Yes.  And the other thing, too, is again, if I didn't have a day job, I would be looking around for a copy of this file.  The burning question to me is whether there's executable code in this.  It may not itself be an executable file.  But it could have...



LEO:  But Falcon is an interpreter, I think.



STEVE:  Okay.  So then it contains p-code.



LEO:  Right.



STEVE:  And the p-code could have been bad, and that caused a problem in the interpreter.



LEO:  Right, right.



STEVE:  So, yeah.



LEO:  So, you know, these systems are based - it's not quite an antivirus signature, but it's basically the same idea, which is they are...



STEVE:  Well, it has to do with behavioral analysis.



LEO:  Exactly.



STEVE:  Behavioral analysis requires more of an algorithmic approach.



LEO:  It's heuristic, yes.  So it has to be more of a program than let's look for a string.



STEVE:  Right.



LEO:  And so I think that's the thesis is that these little sensors are p-code, and then this way basically you have a security system running in ring zero that can be modified on the fly.



STEVE:  Yes.



LEO:  Problem.  Right there.



STEVE:  Dangerous.  Dangerous.



LEO:  Problem.  Dangerous.  And you might, you know, the Falcon can be signed.  Probably, I'm sure the Falcon would sign it, wouldn't run in ring zero.



STEVE:  Microsoft signed the Falcon driver.



LEO:  Yeah, of course.



STEVE:  You can't run...



LEO:  It can't run in ring zero.



STEVE:  You can't run in the kernel unless it's blessed by Microsoft.



LEO:  And one of the things we talked about on MacBreak Weekly is that Apple has very carefully, but fairly aggressively, removed the ability to put kernel extensions into macOS for this very reason.  Right?  They're not required to by the EU.  You know, nobody's saying Apple's not big enough for anybody to say, well, you've got to allow these security guys to put stuff in the kernel.  There are ways around it.  You can run kernel extensions.  But it takes - you have to jump through a lot of hoops, and Apple basically disallows it.  And they've been slowly moving that direction.



STEVE:  It might be that the upshot of this is another level of disaster recovery, which Microsoft has not yet implemented.



LEO:  Maybe that's it, yeah.



STEVE:  Because, you know, it did - the fact that it turned the screen blue and put up some text means that something [crosstalk].



LEO:  Something's running, yeah.



STEVE:  Yes.  So, yeah, I mean, and what happens is an exception is created when you try to read from nonexistent memory.  That exception goes to code that is an exception handler.  And what it could do is set something that then causes a reboot into a different copy of the OS, for example.  I mean, they could, you know, or as we know, you could reboot under a manually forced Safe Mode in order to keep that Falcon driver from running.



LEO:  Precisely.  And I think a number of people recommended that, that this would be an, not easy, but this would be an implementable solution, which is if something caused a crash, you don't load it on the next boot and see what happens then.  I can see problems with that also.  But I think that something will have to be done.



STEVE:  Well, yes.  For example, if it was the system's RAID array driver...



LEO:  Right, that's not going to work, obviously.



STEVE:  Then, yes.



LEO:  Yeah.  But if it's a third-party signed driver, I mean, any driver can do this, by the way.  Drivers generally do run in ring zero because that's how you talk to the hardware.  



STEVE:  Right.



LEO:  But remember Microsoft created HAL, the Hardware Abstraction Layer, for that very reason, so that you could talk to a variety of hardware, and you didn't have to operate at such a low level.  Their own interpreter, in effect.  You know, it's a very - seems solvable, but obviously I don't have enough information to know what really happened.



STEVE:  Well, no one does.



LEO:  No one does.



STEVE:  They're not saying, and they're not going to say.



LEO:  Yeah.



STEVE:  I mean, their entire existence is hanging by a thread at the moment.



LEO:  Oh, yeah, yeah.



STEVE:  I mean, this was a big deal.  And they're going to have to explain, not only to Congress, but to the world, exactly what this root cause nonsense is.



LEO:  Well, and that's one of the reasons you and I have both for a long time said end users should not be running antivirus software because, as a necessity, it runs at a low level and has permissions that normal software doesn't.  And we've seen it again and again that that can become a vector for malware.



STEVE:  Yup.



LEO:  There's no evidence that this was intentional or malware, though, at this point.



STEVE:  None.



LEO:  None.



STEVE:  There's no evidence either way.  You know, they of course don't want it to be a cyberattack.  They don't want it to be deliberate.  But they haven't yet explained how a bad file got out to 8.5 million machines.



LEO:  They're in a bad position because it's either stupidity, incompetence, or a bad guy.  I mean, it's hard.  That's why they haven't said anything; right?  Because if they tell us what really happened, it's very possible people will say, well, that was dumb.



STEVE:  Yeah.



LEO:  I think if there were a good excuse, they would have said what it was already.



STEVE:  Right.



LEO:  Right?



STEVE:  Right.



LEO:  Oh, it got corrupted in the CDN.



STEVE:  This will not be the last time we are talking about CrowdStrike.



LEO:  Yeah.  You know, and as you pointed out early on, they were the good guys.  They are the good guys.  They do good work.



STEVE:  Oh, my god, they have - the power of that sensor network.



LEO:  It's incredible.



STEVE:  The way we are finding bad guys on a global scale.  They are absolutely, you know, we need them there.  They just tripped up.



LEO:  Right.  I talked to the CTO, and that's exactly what we talked about is their sensor network and how many billions of signals they got.  You know, some huge number every minute.  



STEVE:  It's astonishing.



LEO:  Yeah.  It was so valuable because they could see threats in real time, as they happened, catch zero-days right then.  Ah.  This is fascinating.  Thank you.  As always, Steve does a great job explaining this.  And I was waiting all weekend to get here and find out what Steve had to say.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#985

DATE:		July 30, 2024

TITLE:		Platform Key Disclosure

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-985.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  The obligatory follow-up on the massive CrowdStrike event:  How do CrowdStrike's users feel?  Are they switching or staying?  How does CrowdStrike explain what happened, and does it make sense?  How much blame should they receive?  An update on how Entrust will be attempting to keep its customers from changing certificate authorities.  Firefox appears not to be blocking third-party tracking cookies when it claims to be.  How hiring remote workers can come back to bite you in the you-know-what.  Did Google really want to kill off third-party cookies or are they actually happy?  And is there any hope of ending abusive tracking?  Auto-updating anything is fraught with danger.  Why do we do it, and is there no better solution?  And what serious mistake did a security firm discover that compromises the security of nearly 850 PC makes and models?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  The post mortem on the CrowdStrike flaw.  Actually, CrowdStrike explained how it happened.  I think you'll enjoy Steve's reaction to that.  Firefox is apparently not doing what it says it's doing when it comes to tracking cookies.  We'll talk about that.  And then a flaw that makes nearly 850 different PC makes and models insecure.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 985, recorded Tuesday, July 30th, 2024:  Platform Key Disclosure.



It's time for Security Now!, the show where we cover the latest news in the security front.  And, boy, there's always a lot of news in the security front with this guy right here.  He is our security weatherman, Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  And the outlook is cloudy with precipitation.



LEO:  Chance of disaster, yes.



STEVE:  Yes.  Remember duck and cover?  Well, anyway.  We did not, well, no, we did not have a new disaster since 10 days ago when, you know, we had one that pretty much made the record books.  But we do have a really interesting discovery.  And what's really even more worrisome is that it's a rediscovery.  Today's podcast is titled "Platform Key Disclosure" for Security Now! #985, this glorious last podcast of July and the second-to-the-last podcast, the penultimate podcast where you are in the studio, Leo.



LEO:  Yes, it is.



STEVE:  Rather than in your new attic bunker.



LEO:  Can an attic be a bunker, though, really?



STEVE:  Oh, that's a good point, yeah.



LEO:  I think I'm actually more exposed.



STEVE:  [Crosstalk] light tower, the lighthouse, yeah, you'll be the first to go.  



LEO:  Yes.



STEVE:  But really, sometimes you think maybe that's the best; right?  End it now.



LEO:  Oh, I always think that.  I don't - the worst thing is a prolonged, slow, agonizing, suffering death.



STEVE:  What we know as life.



LEO:  Or life, as it's known.  That's the worst thing.  Oh, we're being...



STEVE:  When you're a CIO, and you're - anyway.  So we've got a bunch of stuff to talk about.



LEO:  Yes.



STEVE:  We've got of course the obligatory follow-up on the massive CrowdStrike event.  How do CrowdStrike users feel?  Are they switching or staying?  How does CrowdStrike explain now what happened?  And does that make any sense?  How much blame should they receive?  We've also got an update on how Entrust will be attempting to retain its customers and keeping them from wandering off to other certificate authorities.  Firefox just - no one understands what's going on, exactly, but it appears not to be blocking third-party tracking cookies when it claims to be.  Also we're going to look at how hiring remote workers can come back to bite you in the you-know-what.  



LEO:  Oh, don't tell me that.  That's all remote workers now.  Ai ai ai ai ai.



STEVE:  Yeah, a security company got a rude awakening, and they learned something about just how determined North Korea is to get into our britches.  Also, did Google really want to kill off third-party cookies, or are they maybe actually happy about what happened?  And is there any hope of ending abusive tracking?  Auto-updating anything is obviously fraught with danger.  We just survived some.  Why do we do it, and is there no better solution?



And what serious mistake did a security firm discover that compromises the security of nearly 850 PC makes and models?  This is another wakeup call.  And I'll be further driving home the point of why, despite Microsoft's best intentions - assuming that we agree they have only the best of intentions, I know that opinions differ widely there - they can't, they can't keep Recall data safe.  It's not necessarily their fault, it's just not a safe ecosystem that they're trying to do this in.  So anyway, we have a fun Picture of the Week and a great podcast ahead.



LEO:  Actually, that's a good topic.  We could talk about could any computing system make Recall a safe thing?  Probably not; right?  It's just the nature of computing systems.



STEVE:  Yup.  We have not come up with it yet.



LEO:  Yeah.  There's no such thing as a secure, perfectly secure operating system.  Do we have a - I didn't even look.  Do we have a Picture of the Week this week?



STEVE:  We do indeed.  This one was just sort of irresistible.  I know what they meant.  But it's just sort of fun what actually transpired.



LEO:  I just saw it.



STEVE:  Yeah.  Now, I think, and you'll probably recognize this, too, the signage, I think it's a Barnes & Noble.  It's sadly  been quite a while since I've walked into an actual bookstore and filled my lungs with that beautiful scent of paper pulp.  Used to love it.  I grew up in the San Mateo Public Library.



LEO:  Yeah, me, too.  I love that, yeah.



STEVE:  They just would sort of nod to me was I would walk in, "Hello, Steve.  Enjoy your time in the stacks."  Anyway, so what we have here is a sign over a rack of books.  The sign reads:  "Large Print Audio Books."  And of course I gave that the caption:  "That's right, because large print is what one looks for in an audio book."  Now, many of our clever listeners who received this already via email a couple of hours ago, they actually responded with what my first caption was, which was variations of, "Do you think they actually meant loud audio books?"



LEO:  Okay.  That could be.  Wow.



STEVE:  And somebody took it more seriously and said, well, you know, Steve, somebody who's visually impaired might need large print on the instructions for how to play the audio book.  And that's, you know...



LEO:  That makes sense, yeah.



STEVE:  That's a point.  Actually, what we know is the case is that large print books are on the upper few shelves.



LEO:  That's right, and the audio's down there.



STEVE:  And the audio books are down below.  And they put them both on the same sign, creating this little bit of fun for us and our audience.  So thank you for whomever sent that to me.  It's much appreciated.  And I do have many more goodies to come.



Okay.  So I want to share the note today that I mentioned last week.  This was somebody who was forced to DM me because - I don't remember why.  But he wrote the whole thing, then I think he created a Twitter account in order to send it to me.  Now, I'll take this moment to mention I just was forced to turn off DMs, incoming DMs from unknown senders, or Twitter people who I don't follow.  And of course I famously follow no one.  So, and the reason is, when I went there today, I had a hard time finding any actual new direct messages to me from among the crap.  It was so many people wanting to date me, and I don't think any of them actually do.  You know, and I am wearing a ring, and I'm proud of that.  That, and foreign language DMs that I can't read.  And I finally thought, well, what am I, you know, why?  What?  No.  So I just turned it off.



So I'm sorry, but I will still post the show notes from @SGgrc on Twitter.  I've had a lot of people who thanked me for, even though I now have email up and running - and this system's working beautifully.  7,500 of our listeners received the show notes and a summary of the podcast and a thumbnail of the Picture of the Week and the ability to get the full sizes and everything several hours ago.  But I just - it no longer works as an - I can't do open DMs.  And I don't know why because it's been great for so long, Leo.  I mean, I haven't had any problem.  But maybe, I mean, the only thing I can figure is that the spam level on Twitter is just going way up.



LEO:  Yeah.  You've just been lucky, really.



STEVE:  I think I've just been lucky.  Maybe I've just been sort of off the radar because I don't do much on Twitter except send out the little weekly tweet.  So anyway, this is what someone wrote, really good piece that I wanted to share.



He said:  "Hi, Steve.  I'm writing to you from New South Wales, Australia.  I don't really use Twitter, but here I am sending you a DM.  Without a doubt you'll be mentioning the CrowdStrike outage in your Security Now! this week.  I thought to give you an industry perspective here."  And just to explain, I didn't get this from him until, as I do every Tuesday, I went over to Twitter to collect DMs and found this.  So it didn't make it into last week's podcast, though I wish it had, so I'm sharing it today.



He said:  "I work in cybersecurity at a large enterprise organization with the CrowdStrike Falcon agent deployed across the environment.  Think approximately 20,000 endpoints."  He said:  "Around 2:00 to 3:00 p.m. Sydney, the BSOD wave hit Australia.  The impact cannot be overstated.  All Windows systems in our network started BSOD'ing at once.  This is beyond a horrible outage.  CrowdStrike will need to be held accountable and show they can improve their software stability, rather than sprucing AI mumbo jumbo," he said, "if they want to remain a preferred provider.



"Okay," he says.  "In defense of CrowdStrike, their Falcon EDR tool is nothing short of amazing.  The monitoring features of Falcon are at the top.  It monitors file creation, network processes, registry edits, DNS requests, process executions, scripts, https requests, logons, logoffs, failed logons and so much more.  The agent also allows for containing infected hosts, blocking indicators of compromise, opening a command shell on the host, collecting forensic data, submitting files to a malware sandbox, automation of workflows, an API, Powershell/ Python/Go libraries.  It integrates with threat intel feeds and more and more.



"Most importantly," he says, "CrowdStrike has excellent customer support.  Their team is helpful, knowledgeable, and responsive to questions or feature requests.  Despite this disastrous  outage, we are a more secure company for using CrowdStrike.  They have saved our butts numerous times.  I'm sure other enterprises feel the same.  Full disclosure:  I do not work for CrowdStrike, but I do have one of their T-shirts."



He says:  "Why am I telling you this?  Because the second-in- line competitor to CrowdStrike Falcon is Microsoft Defender for Endpoint (MDE)."  He says:  "MDE is not even close to offering CrowdStrike Falcon's level of protection.  Even worse, Microsoft's customer support is atrocious to the point of being absurd.  I've interacted with Microsoft's security support numerous times.  They were unable to answer even the most basic questions about how their own product operated, and often placed me in an endless loop of escalating my problem to another support staff, forcing me to re-explain my problem to the point where I gave up asking for their help.  I even caught one of their support users using ChatGPT to respond to my emails.  And this is with an enterprise-level support plan."



He says:  "As the dust starts to settle after this event, I imagine Microsoft will begin a campaign of aggressively selling Defender for Endpoint.  Falcon is often more expensive than MDE, since Microsoft provides significant discounts depending on the other Microsoft services a customer consumes.  Sadly, I imagine many executive leadership teams will be dumping CrowdStrike after this outage and signing on with MDE.  Relying on Microsoft for endpoint security will further inflate the single point of failure balloon that is Microsoft, leaving us all less secure in the long run."  And then he signs off:  "Finally, I'm a big fan of the show.  I started listening around 2015.  As a result of listening to your show, I switched to a career in cybersecurity.  Thank you, Leo and Steve."



So I wanted to share that because that's some of the best feedback I've had from somebody who really has some perspective here.  And it all rings 100% true to me.  This is the correct way for a company like CrowdStrike to survive and to thrive.  That is, by really offering value.  They're offering dramatically more value and functionality than Microsoft, so the income they're earning is actually deserved.



One key bit of information we're missing is whether all of these Windows systems, servers, and networks that are constantly being overrun with viruses and ransomware and costing their enterprises tens of millions of dollars to restore and recover - and remember, you know, those are the things that we're normally talking about here every week - are they protected with CrowdStrike, or is CrowdStrike saving those systems that would otherwise fall?  You know, if CrowdStrike is actually successfully blocking enterprise-wide catastrophe, hour by hour and day by day, then that significantly factors into the risk/reward calculation.  Our CrowdStrike user wrote:  "They have saved our butts numerous times."  And he said:  "And I'm sure other enterprises feel the same."



Well, that is a crucially important fact that is easily missed.  It may well be that right now corporate CIOs are meeting with their other C-suite executives and boards and reminding them that while, yes, what happened 10 days ago was bad, but even so it's worth it because this same system had previously prevented, say, I don't know, for example, 14 separate known instances of internal and external network breach, any of which may have resulted in all servers and workstations being irreversibly encrypted, public humiliation for the company, and demands for illegal ransom payments to Russia.



So if that hasn't happened because, as our listener wrote, "CrowdStrike saved our butts numerous times," then the very rational decision might well be to stick with this proven solution in the knowledge that CrowdStrike will have definitely learned a very important and valuable lesson and will be arranging to never allow anything like this to happen again.



Now, if it never happens again, then remaining with this superior solution is the obvious win.  But if it ever should happen again, then in retrospect remaining with them will have been difficult to justify, and I, you know, you could imagine not being surprised if people were fired over their decision not to leave CrowdStrike after this first major incident.  But even so, if CrowdStrike's customers are able to point to the very real benefits they have been receiving on an ongoing basis for years, from their use of this somewhat, okay, can be dangerous system, then even so it might be worth remaining with it.



Before we talk about CrowdStrike's response, I want to share another interesting and important piece of feedback from a listener and also something that I found on Reddit.  So our listener Dan Moutal sent, he wrote:  "I work at a company that uses CrowdStrike, and I thought you would appreciate some insight.  Thankfully, we were only minimally affected as many of our Windows users were at a team-building event with their laptops powered down and not working, and our servers primarily run Linux.  So only a handful of workstations were affected.  However, the recovery was hampered by the common knowledge, which turned out to be false, that the BitLocker recovery key would be needed to boot into Safe Mode.



"When you try to boot into Safe Mode in Windows, you are asked for the BitLocker recovery key.  Most knowledgeable articles, even from Microsoft, state that you need to enter the BitLocker key at this point."  He says:  "But this is not required.  It's just not obvious and not well known how to bypass this."



He says:  "Here's what we discovered over the weekend:  Cycle through the blue-screen error, when the host continues to crash, until you get to the recovery screen.  Perform the following steps.  First, navigate to Troubleshoot > Advanced Options > Startup Settings.  Press Restart.  Skip the first BitLocker recovery key prompt by pressing Escape.  Skip the second BitLocker recovery key prompt by selecting 'Skip This Drive' at the bottom right.  Navigate to Troubleshoot > Advanced Options > Command Prompt.  Then enter bcdedit /set and then {default) safe boot minimal, then press Enter.  Close the command prompt window by clicking the X in the top right.  This will return you back to the blue screen, which is the Windows RE main menu.  Select Continue.  Your PC will now reboot.  It may cycle two to three times.  But then your PC should boot into Safe Mode."



He said:  "I confirmed this worked and allowed us to recover a few systems where we did not have the BitLocker keys available."  He says:  "I think Microsoft deserves a lot of blame for the poor recovery process when Safe Mode is needed.  They should not be asking for BitLocker keys if they're not needed.  At the bare minimum, they need to make this knowledge much more well known so system admins who don't have BitLocker keys handy can still boot into Safe Mode when disaster strikes.



"I also want to send a shout-out to CrowdStrike's technical support team.  I'm sure they were swamped by support requests on Friday, but despite that we were talking to them after waiting on hold for only 10 minutes, and they were very helpful.  Most vendors are not quick or useful on a good day, let alone on a day when they are the cause of a massive outage.  CrowdStrike is an expensive product, but it is clear that a large chunk of that expense pays for a large and well-trained support staff."  So that was from a listener of ours.



LEO:  Those are really excellent points.  I mean...



STEVE:  Aren't they?  Yes.



LEO:  Yeah, yeah.  I mean, so much worse to get bit by ransomware than a temporary boot flaw.



STEVE:  Yes, exactly.  So for our listeners to say, yes, you know, this was not good, that first listener had 20,000 endpoints.  But he said, "Still, it has saved our butts."  Well, "saved our butts" must mean that he has evidence that something bad would have happened to them, had CrowdStrike not blocked it.  So what's that worth?  You know?  It's worth a lot.



Okay.  So on Reddit I found a posting.  Oh, and by the way, Leo, I just should mention, I don't normally have our monitor screen up so that I know if you're there or not.  So just a note to your control room.



LEO:  I'm here, by the way.



STEVE:  Nice to hear your voice.  So this post on Reddit said:  "Just exited a meeting with CrowdStrike.  You can remediate all of your endpoints from the cloud."  There's news.  He said:  "If you're thinking, 'That's impossible, how?'" he says, "this was also the first question I asked, and they gave a reasonable answer.



"To be effective, CrowdStrike services are loaded very early in the boot process" - which of course is what we talked about last week - "and they communicate directly with CrowdStrike.  This communication is used to tell CrowdStrike to quarantine windows\system32\drivers\crowdstrike\ and then the infamous c-00000291*.sys file." 



So he said:  "To do this, you must first opt-in" - that is, for this cloud-based recovery - "by submitting a request via the support portal, providing your customer IDs and request to be included in cloud remediation.  At the time of the meeting," he says, you know, when he was in this meeting - "the average wait time for inclusion was less than one hour.  Once you receive email indicating that you have been included, you simply have your users reboot their computers."



LEO:  Oh.



STEVE:  In other words, it's a self-repair of this problem.



LEO:  That seems sensible, yeah.



STEVE:  Yeah.  He said:  "CrowdStrike noted that sometimes the boot process completes too quickly for the client to get the update, and a second or third try is needed; but it is working for nearly all of their affected users.  At the time of the meeting, they had remediated more than 500,000 endpoints this way."  He says:  "It was suggested to use a wired connection when possible since WiFi-connected users have the most frequent trouble with this approach, probably because WiFi connectivity becomes available later in the boot process, after a crash will have occurred."  He says:  "This also works with home and remote users since all they need is an Internet connection, any Internet connection.  The point is, they do not need to be and should not be VPN'd into the corporate network."



So anyway, I thought that was interesting since essentially we have another of those race conditions.  We've talked about those recently; right?  In this case it's one where we're hoping that the CrowdStrike network-based update succeeds before the crash can occur.



Okay.  So with all of that, what more do we know today than we did a week ago at the time of last week's podcast?  The questions that were on everyone's mind were variations of "How could this possibly have been allowed to happen in the first place?"  "How could CrowdStrike not have had measures in place to prevent this?"  And even, you know, "Staggering the release of the buggy file would have limited the scale of the damage.  Why wasn't that, at least, part of their standard operating procedure?"



For last week's podcast we had no answers to any of those questions.  Among the several possibilities I suggested were that they did have some pre-release testing system in place; and, if so, then it must have somehow failed.  And that's the explanation that the industry has received from them since that time.  I have no doubt about its truth, since CrowdStrike executives will be repeating that under oath shortly.



Last week we shared what little was known, which CrowdStrike had published by that point under the title "What Happened?"  But they weren't yet saying how.  And we also had that statement that I shared from George Kurtz, CrowdStrike's founder and CEO.  This week we do have their "What Happened?" which is followed by "What Went Wrong and Why?"  And, okay.  Despite the fact that it contains a bunch of eye-crossing jargon which sounds like gobbledygook, I think it's important for everyone to hear what CrowdStrike said.  So here's what they have said to explain this.  And in order to create the context that's necessary, we learn a lot more about the innards of what's going on.



They said:  "CrowdStrike delivers security content configuration updates to our sensors."  And when they say "sensors," they're talking about basically a kernel driver.  So some of it is built into the kernel driver, or delivered with driver updates, and the other is real-time.  So whenever you hear me say "sensors," you know, it's not anything physical, though it sounds like it is.  It is software running in the kernel.  They said:  "In two ways:  Sensor Content that is shipped with our sensor directly, and Rapid Response Content that is designed to respond to the changing threat landscape at operational speed."  They wrote:  "The issue on Friday involved a Rapid Response Content update with an undetected error.



"Sensor Content provides a wide range of capabilities to assist in adversary response.  It is always part of a sensor release and not dynamically updated from the cloud.  Sensor Content includes on-sensor AI and machine-learning models, and comprises code written expressly to deliver long-term reusable capabilities for CrowdStrike's threat detection engineers.  These capabilities include Template Types," which this figures strongly in the response, "which have pre-defined fields for threat detection engineers to leverage in Rapid Response Content.  Template Types are expressed in code.  All Sensor Content, including Template Types, go through an extensive QA process, which includes automated testing, manual testing, validation, and rollout.



"The sensor release process begins with automated testing, both prior to and after merging into our code base.  This includes unit testing, integration testing, performance testing, and stress testing.  This culminates in a staged sensor rollout process that starts with dogfooding internally at CrowdStrike, followed by early adopters.  It's then made generally available to customers.  Customers then have the option of selecting which parts of their fleet should install the latest sensor release ('N'), or one version older ('N-1') or two versions older ('N-2') through Sensor Update Policies."



Now, okay.  To be clear, all of that refers to essentially the driver, the so-called "sensor."  So for that stage they are doing incremental rollout, early adopter testing and so forth.  Unfortunately, not for the rapid response stuff.



So they said:  "The event of Friday, July 19th, was not triggered by Sensor Content, which is only delivered with the release of an updated Falcon sensor."  Meaning, you know, updated static drivers.  They said:  "Customers have complete control over the deployment of the sensor, which includes Sensor Content and Template Types.  Rapid Response Content is used to perform a variety of behavioral pattern-matching operations on the sensor using a highly optimized engine."  Right, because you don't want to slow the whole Windows operating system down as it takes time out to analyze everything that's going on.



So they said:  "Rapid Response Content is a representation of fields and values, with associated filtering.  This Rapid Response Content is stored in a proprietary binary format that contains configuration data.  It is not code or a kernel driver."  But I'll just note, unfortunately it is interpreted. And how much time have we spent about interpreters going wrong on this podcast?



They said:  "Rapid Response Content is delivered as 'Template Instances,' which are instantiations of a given Template Type.  Each Template Instance maps to specific behaviors for the sensor to observe, detect, or prevent.  Template Instances have a set of fields that can be configured to match the desired behavior.  In other words, Template Types represent a sensor capability that enables new telemetry and detection, and their runtime behavior is configured dynamically by the Template Instance - in other words, specific Rapid Response Content.



"Rapid Response Content provides visibility and detections on the sensor without requiring sensor code changes.  This capability is used by threat detection engineers to gather telemetry, identify indicators of adversarial behavior, and perform detections and preventions.  Rapid Response Content is behavioral heuristics, separate and distinct from CrowdStrike's on-sensor AI prevention and detection capabilities.  Rapid Response Content is delivered as content configuration updates to the Falcon sensor.  There are three primary systems:  the Content Configuration System, the Content Interpreter, and the Sensor Detection Engine.



"The Content Configuration System is part of the Falcon platform in the cloud, while the Content Interpreter and Sensor Detection Engine are components of the Falcon sensor."  In other words, running in the kernel.  So we've got a content interpreter running in the kernel.  What could possibly go wrong?  Well, we found out.  They said:  "The Content Configuration System is used to create Template Instances, which are validated and deployed to the sensor through a mechanism called Channel Files.  The sensor stores and updates its content configuration data through Channel Files, which are written to disk on the host.



"The Content Interpreter on the sensor reads the Channel Files and interprets the Rapid Response Content, enabling the Sensor Detection Engine to observe, detect, or prevent malicious activity, depending on the customer's policy configuration.  The Content Interpreter is designed to gracefully handle exceptions from potentially problematic content."  Let me read that sentence again because that's what failed.  "The content interpreter is designed to gracefully handle exceptions from potentially problematic content."  Except in this instance, as we know, it did not.



And they finish with:  "Newly released Template Types are stress tested across many aspects, such as resource utilization, system performance impact, and event volume.  For each Template Type, a specific Template Instance is used to stress test the Template Type by matching against any possible value of the associated data fields to identify adverse system interactions."  In other words, that's the nice way of saying "crashes."  And "Template Instances are created and configured through the use of the Content Configuration System, which includes the Content Validator that performs validation checks on the content before it is published."



Okay, now, I've read this a total of I think maybe five times, and I now finally feel like I understand it.  So, you know, don't be put off if that just all like, what did he just say?  I get it. 



Then they lay out a timeline of events which I'll make a bit easier to understand by interpreting what they wrote.  So recall that there was mention of named pipes being involved with this trouble.  And I explained that named pipes were a very common means for different independent processes to communicate with each other within Windows.  So way back on February 28th of this year, CrowdStrike sensor v7.11 was made generally available to customers.  It introduced a new Inter Process Communication (IPC) Template Type which was designed to detect novel attack techniques that abused Named Pipes.  This release followed all Sensor Content testing procedures outlined above in that Sensor Content section.  So again, sensor content is - this was an update essentially to the driver and all of its AI and heuristic stuff.  And that happened on February 28th.



A week later, on March 5th, a stress test of the IPC, this newly created IPC Template Type was executed, they said, in our staging environment, which consists of a variety of operating system workloads.  The IPC Template Type passed the stress test and was thereby validated for use.  Later that same day, following the successful stress testing, the first of the IPC Template Instances were released to production as part of a content configuration update.  So like what happened on this fateful Friday, that happened back on March 1st for the first time for this new IPC Template Type.  They said, after that, three additional IPC Template Instances were deployed between April 8th and April 24th. These Template Instances performed as expected in production.



Then, on that fateful day of Friday, July 19th, two additional IPC Template Instances were deployed, much as multiples of those had in February, March, and April.  One of these two, one of the two deployed on July 19th was malformed and should never have been released.  But due to a bug in the Content Validator, and also in the interpreter, that malformed IPC Template Instance erroneously passed validation, despite containing problematic content data.



They said, based on the testing performed before the initial deployment of the new IPC Template Type back on March 5th, and in the trust in the checks performed by the Content Validator, and the several previous successful IPC Template Instance deployments, on Friday the 19th these two instances, one of them malformed, were released and deployed into production.  When received by the sensor and loaded into the Content Interpreter, which is in the kernel, problematic content in Channel File 291 resulted in an out-of-bounds memory read triggering an exception.  This unexpected exception could not be gracefully handled, resulting in a Windows operating system crash, the infamous Blue Screen of Death, which then followed its attempt to recover.



And Leo, we're going to talk about how they prevent it from happening again, but let's take a break so I can catch my breath and sip a little caffeine.



LEO:  Bessie in our YouTube chat says:  "How can all those PCs fail, but no PCs at CrowdStrike itself failed?  Don't they use CrowdStrike at CrowdStrike?"  I'm curious, and I sure you'll cover this, how quickly they figured out that this update was causing a problem.  Surely - maybe CrowdStrike was closed for the day.  I don't know.  It happened in Australia first; right?



STEVE:  Well, it did happen in the wee hours of the morning in the United States.



LEO:  It's wild.  It's just wild.  Anyway, we will get to that.  The post-mortem.  Steve, I'm trying to decide, I'm bringing, you know, I'm packing up stuff, as you probably noticed, stuff's starting to disappear from the studio.  I'm trying to decide, should I take this needlepoint that says "Look Humble"?  Can't decide.  I'll have to think about it.



STEVE:  Where did it come from?  Does it have a special meaning for you?



LEO:  It wasn't my Grandma, it was probably a listener sent it to me.  I just always thought it was very funny.  I've always had it.  I am definitely taking the Nixie clock.  You cannot have a studio without a Nixie clock.



STEVE:  I guess, no, I think you should keep the Nixie clock.  It's definitely a...



LEO:  Everything that blinks I'm taking.



STEVE:  Good.



LEO:  I am not taking this darn clock, which has been the bane of my existence since the Brick House.  People get mad when that clock is not visible on the show, that digital clock.  But I have other clocks.  I'm not going to bring that one.  We'll see.



STEVE:  Well, I guess the question is also how much room do you have.



LEO:  Well, that's the point is, you know, pretty much everything you see behind me I'm going to leave here.  We've got a company that does...



STEVE:  Leave behind.



LEO:  Yeah.  There's a company that's a liquidator.



STEVE:  Ah.



LEO:  They come in, and they sell what they can, they donate what they can, recycle what they can, and toss the rest.  And I guess that's all we can do.  This is a crazy amount of stuff we've accumulated over the years.



STEVE:  As one does.



LEO:  As one does.  We are opening the studio on the 8th to any Club TWiT members who want to come and get something.  Come on down.  We're blowing it out to the bare walls.  You know I'm giving away, and I hate to do it, but I have that giant demonstration slide rule.  I don't think you can see it.  You can see the bottom of it.  It's one of those yellow...



STEVE:  Oh, I do see the bottom of it, with the plastic slider.



LEO:  But where am I going to put that?  Hanging off the roof, I don't know.



STEVE:  Yeah.



LEO:  I only have, you know, a tiny little attic studio.  So a lot of stuff getting left behind, I'm sad to say.



STEVE:  Well, I've got some friends who - some of my high school buddies are actively lightening their load.



LEO:  Yeah.



STEVE:  Like one guy deliberately converted his huge CD collection over to audio files and threw away all the CDs.  I mean, it hurt to do it, right, because those are, like...



LEO:  Lifetime.  That's a lifetime of collecting.



STEVE:  Yeah.  But it's like, you know, I want to travel more.  I don't want to...



LEO:  And, like, am I going to dump this on my kids?  I don't want to do that.  So, yeah, they call it - there's a book about it called "Swedish Death Cleaning," where you prepare for your death and do your heirs a favor.  You get rid of the stuff that you don't think they'd be interested in.  It's hard, though.



STEVE:  Yeah, I don't know what those PDP-8s that I've got are going to...



LEO:  I'm taking my IMSI, you know, the two Raspberry Pi devices, and the PiDP-8.  I'm absolutely taking those.  They're too cool.  Blinking lights have got to stay.



STEVE:  Well, and I have been remiss in not telling our listeners, it's just there's so much that's been happening on the podcast, but the guy that did the fantastic PiDP-8...



LEO:  PiDP-8, yeah.



STEVE:  ...has done a PDP-10.  And it is astonishing.  I mean, it is.  I'll have to make time to - I don't know when.  The podcast has just been so crazy lately.  But the emulated PDP-10, he gathered all the software from MIT...



LEO:  It was Oscar Vermeulen; right?  Was it...



STEVE:  Oscar, yes, Oscar has done a PDP-10, also with a little Raspberry Pi running behind it, a gorgeous injection-molded PDP-10 console recreation.  So there's the 8.



LEO:  Yes, this is the one we have.



STEVE:  And the 10.  Is there a link to it there?



LEO:  Let's see if he has - his vintage computer collection, I think this is...



STEVE:  Yeah, he gave it its own website.



LEO:  Oh, okay.  I'll find that.



STEVE:  I'm surprised he's not linked to it.  But you might put in PDP-10 recreation or something.



LEO:  Yeah, see what I can find.



STEVE:  To find it because, oh, my goodness.



LEO:  Here we go.  There we go.  Obsolescence Guaranteed.



STEVE:  Look at that.



LEO:  Oh.  Okay.  Oscar, I want it.



STEVE:  It is astonishing.



LEO:  Oh, it's beautiful.  It's very Star Trek.  That is...



STEVE:  It is gorgeous.  That's an injection molded, you know, full working system.  All of the software is there.  You're able to connect a normal PC to it so you can work with a console and keyboard.  



LEO:  Oh, it's got Adventure.  Uh-oh.  I might have to buy this.  So it's running a Pi, Raspberry Pi, but that's the same performance as a PDP-10?  I guess so.



STEVE:  Oh, it blows the PDP-10 away.  He had to slow it down in order in order to make it...



LEO:  Oh.  This is so cool.



STEVE:  I mean, the original software running.  And so there they were, comparing the operation of their console to a real one to the - in fact, I was thinking about this because Paul Allen was selling some of their original machines; right?



LEO:  Wow.  Oscar Vermeulen.  Obsolescence.wixsite.com is Obsolescence Guaranteed.  And you can build the kit.  You can buy it or build the kit.



STEVE:  Yup, it is embarrassingly inexpensive, again.



LEO:  Nice.



STEVE:  And, oh, boy.  But, I mean, he and his buddy, they were out demonstrating it to the Boston Computer Museum.  And he said, "Steve, could you make time for us to show you?"  So he came and set this up, plugged its HDMI output into our screen in our family room and gave Lorrie and me a full demonstration of the operation of this.



LEO:  Poor Lorrie.  Did Lorrie know what she was getting into when she married you?  And later we're going to have somebody demonstrate a replica PDP-10.  Won't that be fun?  I love it.  Well, Oscar, well done.  I'm glad he did it again.  Yeah, so I got the email, too.  And I guess I'll have to build - because I have the PiDP-8 is already on the set in the attic.  And I can't upgrade.



STEVE:  Oh, this 10, oh, just look at that console.  It is just gorgeous.



LEO:  Oh, it's beautiful.



STEVE:  It was funny, too, because her 28-year-old son Robert happened to also be there.  And he made - he was, like, watching this.  And he'd never seen a console before, right, with like lights and switches.  And he said, "Are those bits?"



LEO:  Good question.  Good question.



STEVE:  It was like, it would have never occurred to me to ask that question.  But it's like, yes, those are bits.  Those are what bits are, are those individual lights turning on and off, and the switches are bits.



LEO:  Wow.  Wow.  Very, very cool.



STEVE:  And what's cool is that, whereas the PDP-8 is a pain to program because you've only got a three-bit op code, so you've got seven instructions, the PDP-10, oh, it's a 36-bit system, and it's a gorgeous instruction set.  I mean, really just a joy.  So, I mean, so this is a complete recreation.  You can be using it with its editors and its compilers and the works.



LEO:  Wow.  Oh, that's what you want; right, Steve?  There, by the way, are Oscar and Otto, showing off the entire line,  Oscar's on the left, at the Vintage Computer Festival.  Very cool.  Very cool.  So they've got a 1, 8, 11, and a 10.



STEVE:  Yes.  Oh, and it says down there, and the 1 are at - I guess 1 and 10 at that time were in prototype.  10 is finished, and they're now working on a PDP-1.  And I have to tell you, he credits this podcast as changing this from a hobby to a business because there was so much interest shown in the PiDP-8, and then in the 11, that they turned it into a business.  And so anyway, I'm glad that this came up because I've been - I felt badly that I have not found time because, I mean, our podcasts have been running more than two hours recently.



LEO:  I know, I know.  But I'm so glad because I wanted to get this in, too.  So I'm glad we could mention it, as I pack up my PiDP and bring it home, my PiDP-8.



STEVE:  And gentlemen listening, if you saw the actual size, it's not a huge thing.  So it does store in the closet.  You were talking about how patient Lorrie is.



LEO:  Don't put it on the dining room table, whatever you do.  I know you're tempted, gentlemen.  But don't.  Yeah, it's the white one right here.  This is it.



STEVE:  Yup, exactly.



LEO:  So it's what, it's about a couple of feet wide maybe.



STEVE:  It is a scale, yes, it's a scale size replica of the console of the original PDP-10.



LEO:  Yeah.  And I have that one.



STEVE:  But loaded with all of the original software.  They even have one guy who specializes in recreating recovering data from unreadable nine-track magnetic tapes.  And so they were getting mag tapes.  You can see one right there behind my head.  That is a nine-track magnetic tape.  That actually came from SAIL, from Stanford's Artificial Intelligence Lab.



LEO:  Oh, that's cool.



STEVE:  And that's got my code on it.



LEO:  Oh, that's really cool.  Wow.



STEVE:  So they've literally - they went back and recreated the original files and in some cases hand-editing typos out in order to get everything to recompile again, in order to - because there was no, like, preservation project until now.  So they've really - they did a beautiful job.



LEO:  Mm-hmm.  Mm-hmm.



STEVE:  And, you know, congratulations.



LEO:  Yeah.



STEVE:  Okay.  So following on what happened...



LEO:  Oh, back to the bad news.



STEVE:  Naturally, yeah, naturally CrowdStrike wants to explain how this will never happen again.



LEO:  Yeah.  Let's hear it.



STEVE:  Yeah.  So under the subhead of Software Resiliency and Testing, they've got bullet points.  And I have to say that this first batch sounds like the result of a brainstorming session rather than an action plan.  They have Improve Rapid Response Content testing.  That was the problematic download; right?  By using testing types such as local developer testing; content update and rollback testing; stress testing, fuzzing and fault injection; stability testing; and content interface testing.  And of course many people would respond to that, why weren't you doing all that before?  Unfortunately, that's the generic response, right, to anything that they say that they're now going to do is, well, why weren't you doing that before?



Anyway, so they also have:  Add additional validation checks to the Content Validator for Rapid Response Content.  A new check is in progress to guard against this type of problematic content from being deployed in the future.  Good.  Enhance existing error handling in the Content Interpreter, right, because it was ultimately the interpreter that crashed the entire system when it was interpreting some bad content.  So that should not have been able to happen.



And then under the subhead of Rapid Response Content Deployment, they've got four items:  Implement a staggered deployment strategy for Rapid Response Content in which updates are gradually deployed to larger portions of the sensor base, starting with a canary deployment; improve monitoring for both sensor and system performance, collecting feedback during Rapid Response Content deployment to guide a phased rollout; provide customers with greater control over the delivery of Rapid Response Content updates by allowing granular selection of when and where these updates are deployed; provide content update details via release notes which customers can subscribe to.



And I have to say, kind of reading between the lines again, you know, programmers have egos; right?  We write code, and we think it's right.  And then it's tested, and the testing agrees that it's right.  And it's difficult without evidence of it being wrong to, like, to go overboard.  They did have systems in place to catch this stuff.  It turns out in retrospect something got past that system, or those systems.  Now they know that what they had was not good enough, and they're making it better.



So, you know, I get it that could they have done more?  Yes.  But can't you always do more?  Yes.  And then one could argue in that case, if it's possible, if it's in any way possible for something to go wrong, then shouldn't you prevent that?  Well, they thought they had.  They thought that the content interpreter was bulletproof, that it was an interpreter.  It would find any problems and refuse to interpret them if they were going to cause a problem.  But there was a bug in that.  So this happened.



Okay.  So the bottom line to all of this, I think, is that CrowdStrike now promises to do what it should have been doing all along, like the staggered deployment.  Again, I mean, that's indefensible; right?  Why were you guys not incrementally releasing this?  Well, it's because they really and truly believed that nothing could cause this to happen.  They really thought that.  They were wrong.  But they, you know, it wasn't negligence.  I mean, in the same sense that, you know, most programmers don't release buggy code; right?  They fix the bugs.  Microsoft is an exception.  They've got a list of 10,000 known bugs when they release Windows.  But they're small, and they figure they won't actually hurt anybody, and they're not showstoppers.  They actually use that term.  So it's like, okay, fine, it works.



So is this another of those small earthquake tremors I've been recently talking about?  You know, I guess it would depend upon whom you ask.  The source of the problem was centralized, but the remediation of the problem was widely distributed.  Across 8.5 million machines, several hundred thousand individual technicians got to work figuring out what had happened to their own networks and workstations, and each was repairing the machines over which they had responsibility.  Because initially, you know, CrowdStrike, as we saw, they ended up coming up with a cool cloud-based solution.  But initially that didn't exist.  Presumably it will now be deployed in some sort of a permanent fashion.



And as we know in the aftermath, some users of Windows appear to have been more seriously damaged than others.  In some cases machines that were rebooted repaired themselves by retrieving the updated and repaired Template File.  Whereas in other situations, such as, wow, Delta Airlines, the effects from having Windows system crashing lasted days.



I have no direct experience with CrowdStrike; but not a single one of our listeners from whom we have heard, even after enduring this pain, sounded like they would prefer to operate without CrowdStrike-level protection and monitoring in the future.  And I think that's a rational position.  No one was happy that this occurred, and it really is foreseeable that CrowdStrike has learned a valuable lesson about using belts, suspenders, Velcro, and probably some epoxy.  They may have grown a bit too comfortable over time, but I'll bet that's been shaken out of them today.  And I have no doubt that it will be, I mean, like that they really raise the bar on having this happen to them again.



Another little bit of feedback.  Since it's relevant to the CrowdStrike discussion, I wanted to share what another listener of ours, Vernon Young, shared with me.  He wrote:  "Dear Steve.  I am the IT Director for a high school and manage 700 computers and 50 virtual servers."



LEO:  Oh.  Oh.



STEVE:  Get this, Leo.  "A few weeks before the Kaspersky ban was announced, I placed a $12,000 renewal order with Kaspersky, $12,000 which will now be lost..."



LEO:  Forever.



STEVE:  "...since the software won't work after September," he wrote.  He said:  "After the ban was announced, I started looking for alternatives.  I decided on Thursday, July 18th to go with CrowdStrike."



LEO:  Oh, my god, the day before.  Oh, my god.



STEVE:  He said:  "The day before the world collapsed."  He said:  "Thankfully, I didn't feel like walking to the copier to scan the purchase order to send to the sales rep before I left for the day.  Needless to say, I changed my mind Friday morning."



LEO:  Bullet dodged.



STEVE:  Now, I cannot imagine being Vernon, our listener, and needing to corral a high school campus full of mischievous and precocious high-schoolers...



LEO:  People like you.



STEVE:  ...who imagine, you know, as high-schoolers will, that they're more clever than the rest of the world and whose juvenile brains' sense of right and wrong hasn't yet had the chance to fully develop.  But think about the world Vernon is facing.  He invests $12,000 to renew the school's Kaspersky AV system license, only to have that lost.  Then decides that CrowdStrike looks like the best alternative, only to have it collapse the world.  For what it's worth, I stand by the way I ended that CrowdStrike discussion.  I would go with them today.



LEO:  Yes.  Yeah.



STEVE:  All of the feedback we've received suggests that they are topnotch, that they're very clearly raising the bar to prevent another mistake like this from ever slipping past.  There's just no way that they haven't really learned a lesson from this debacle, and I think that's all anyone can ask at this point.  And the fact that our listeners are telling us they are the best there is...



LEO:  That there's no other choices, no good choice, yeah.



STEVE:  Right.  Microsoft is number two, and they don't hold a candle to...



LEO:  Right.



STEVE:  I mean, you know, certainly all of the systems that we talked about succumbing to ransomware are at least running Microsoft Defender, and that's not helping them.



Okay.  So who is to blame?  Our wonderful hacker friend Marcus Hutchins posted a wonderfully comprehensive 18-minute YouTube video which thoroughly examines, explores, and explains the history of the still-raging three-way battle among Microsoft, third-party AV vendors, and malware creators.  That video, it's on YouTube, it's this week's GRC shortcut of the week, so your browser can be redirected to it if you go to grc.sc/985, today's episode number, grc.sc as in shortcut, grc.sc/985.  When you go there, be prepared for nearly 18 minutes of high-speed nonstop perfectly articulated techie detail because that's what you're going to get.



I'll summarize what Marcus said.  Since the beginning of Windows, the appearance of viruses and other malware, and the emergence of a market for third-party antivirus vendors, there's been an uncomfortable relationship between Microsoft and third-party AV.  To truly get the job done correctly, third-party antivirus has always needed deeper access into Windows than Microsoft has been willing or comfortable to give.  And the CrowdStrike incident shows what can happen when a third party makes a mistake in the Windows kernel.  But it is not true, Marcus says, that third-party antivirus vendors can do the same thing as Microsoft can without being in the kernel.  This is why Microsoft themselves do not use the APIs they have made available to other antivirus vendors.  Those APIs do not get the job done.



And the EU did not say that Microsoft had to make the kernel available to other third parties.  The EU merely said that Microsoft needed to create a level playing field where the same features would be available to third parties as they were using themselves.  Since Microsoft was unwilling to use only their own watered-down antivirus APIs and needed access to their own OS kernel, that same EU-mandated access has remained available to third-party vendors, as well.



Any comprehensive retelling of the saga of Windows kernel access must recognize that this area of Windows has been constantly evolving.  Marcus notes that Windows Vista was a real mess, that many changes have been made along the way since then, and that the latest Windows 10 1703 has made some changes that might offer some hope of a more stable world in the future.  The problem, of course, is that third parties still need to be offering their solutions on older Windows platforms which are still running just fine, refuse to die, and may not be upgradeable to later versions.



So Marcus holds Microsoft responsible.  That's his position.  And I commend our listeners to go to grc.sc/985 to get all the details.  Anyone who has a techie bent will certainly enjoy him explaining pretty much what I just have in his own words.  And Leo, we're at an hour.



LEO:  Yes.



STEVE:  Let's take another break.  And then we're going to look at what happened during Entrust's recent webinar, where they explain how they're going to hold onto their customers.



LEO:  Some of these never-ending stories are really quite amusing, I must say.  I must say.  See, you know, you talk about, well, are there options to Bitwarden?  There are.  And a lot of them are seeing this as an opportunity; right?  This is the time.  By the way, did my clock disappear?  I think it did.  Stuff's leaving the building.



STEVE:  Yeah, the Nixie clock just is gone.  Wow.



LEO:  Yeah.  I had an interesting giant sword that seems to have disappeared, as well.  So if you see somebody going down the street with a sword about yea-long, you might call the authorities.



STEVE:  Looks like the ukulele is still there, though.



LEO:  The uke, no one's taken the uke.  I don't know why.



STEVE:  Okay.



LEO:  All right.  Continue on, my friend.



STEVE:  So Entrust held a 10:00 a.m. webinar last week which included the description of their solution with the partnership we mentioned last week with SSL.com.  It was largely what I presumed from what they had said earlier, which was that behind the scenes the Certificate Authority SSL.com would be creating and signing the certificates that Entrust would be purchasing from SSL.com and effectively reselling.  There were, however, two additional details that were interesting.



Before any certificate authority can issue domain validation certificates, the applicant's control over the domain name in question must be demonstrated.  So, for example, if I want to get a certificate for GRC.com, I need to somehow prove to the certificate authority that GRC.com is under my control.  I can do that by putting a file that they give me on the root of the server, the web server that answers at GRC.com, showing that, yeah, that's my server, because they asked me to put this file there and I did.  I can put a text record into the DNS for GRC.com, again, proving that I'm the guy who's in charge of GRC.com's DNS.  And there's a weak method, the weakest is to have email sent from the GRC.com domain; but when all else fails, you can do that.



So anyway, you need to somehow prove you own the domain, you have control over it.  So it turns out SSL.com is unwilling to take Entrust's word for that.  So the additional wrinkle that will exist for any Entrust customers who wish to purchase web server certificates from Entrust after this coming October 31st is that they will need to prove their domain ownership, not to Entrust, as they have in the past, but to SSL.com.  Not surprising; but still, oops, not quite what Entrust was hoping for.



The second wrinkle is that Entrust does not want SSL.com's name to appear in the web browser when a user inspects the security of their connection to see who issued a site's certificate.  No, it's got to be Entrust.  So although SSL.com will be creating each entire certificate on behalf of Entrust, they've agreed to have SSL.com embed an Entrust intermediate certificate into the certificate chain, since web browsers only show the signer of the web server's final certificate in the chain.  By placing Entrust in the middle, SSL.com will be signing the Entrust intermediary, and Entrust's intermediary will be signing the server's domain certificate.  In this way, it will be Entrust's name that will be seen in the web browser by anyone who is checking.



So, you know.  The webinar was full of a lot of, you know, all of this how they're going to get back in the good graces of the CA/Browser Forum, and all the steps they're taking, and blah blah blah.  We'll see how that goes with the passage of time.  For now, that's what they're doing in order to, in every way they can, hold onto the customers that they've got who've been purchasing certificates.



I should mention that the webinar also explained that all of the existing mechanism of using Entrust is in place.  Everything is Entrust-centric with, whoops, the exception of needing to prove domain ownership to somebody else.  No way around that one.



And Leo, this actually came as a result of our talking about the GRC cookie forensics stuff last week.  Something is going on with Firefox that is not clear and is not good.  After last week's discussion of third-party cookies, and you playing with GRC's Cookie Forensics pages, several people commented that Firefox did not appear to be doing the right thing when it came to blocking third-party cookies in what it calls "Strict" mode.  Strict mode is what I want; but, sure enough, Strict mode behavior does not appear to be what I'm getting.



Under Firefox's "Enhanced Tracking Protection" we have three settings, three overall settings:  Standard, Strict, and Custom.  Standard is described as "Balanced for protection and performance.  Pages will load normally."  In other words, third-party cookies, we love you.  Anybody who wants one can munch on one.  Strict is described as "Stronger protection, but may cause some sites or content to break."  And then it details this further by claiming, it says:  "Firefox blocks the following:  social media trackers, cross-site cookies in all windows, tracking content in all windows, cryptominers, and fingerprinters."



Well, that all sounds great.  The problem is, it does not appear to be working at all under Firefox.  This issue arose, initially came to my attention in our old-school newsgroups, where I hang out with a great group of people.  So it grabbed a lot of attention.  And many others have confirmed, as have I, Firefox's Strict mode is apparently not doing what it says, what we want and expect.  It says cross-site cookies in all windows.  That's not working.  Chrome and Bing work perfectly.



In order to get Firefox to actually block third-party cookies, cross-site cookies, it's necessary to switch to Custom mode, tell it that you want to block cookies, then under which types of cookies to block you cannot choose "Cross-site tracking cookies."  I mean, you can, but it doesn't work.  You need to turn the strength up higher.  So "Cross-site tracking cookies, and isolate other cross-site cookies"?  Nope, that doesn't work either.  Neither does setting "Cookies from unvisited websites."  Nope.  Still doesn't work.  It's necessary to choose the Custom mode, and then the cookie-blocking selection of "All cross-site cookies," with then it says in parens "(may cause websites to break)."



Once that's done, GRC's Cookie Forensics page shows that NO third-party session or persistent cookies are being returned from Firefox, just as happens with Chrome and Bing when you tell them to block third-party cookies.  They actually do.  Firefox actually does not.  Back when I first wrote this, when third-party cookies were disabled, some of the broken browsers, they had really weird behavior.  It's why I'm testing eight different ways of setting cookies in a browser because they used to all be jumbled up.  And some worked; some didn't.  Some were broken; some weren't.  In some cases, when you told it to disable third-party cookies, it would stop accepting settings for new cookies.  But if you still had any old, you'd call them "stale" cookies, then those would still be getting sent back.  All of that behavior's been fixed.  But it's broken under Firefox.



Looking at the wording, which specifically refers to cross-site tracking cookies, it appears that Firefox may be making some sort of value judgment about which third-party cross-site cookies are being used for tracking, and which are not.  That seems like a bad idea.  I don't want any third-party cookies.  Chrome and Bing and Safari, well, Safari's had that all shut down for years.  Chrome and Bing will now do it if you tell them to.  So, what, do they imagine that they can and have somehow  maintained a comprehensive list of tracking domains and won't allow third-party cookies to be set by any of those?  The only thing that comes to mind is like some sort of heuristic thing, and all of that seems dumb.  Just turn them off, like everybody else does.



You know, there may be more to what's going on here, though.  One person in GRC's newsgroup said that they set up a new virtual machine, installed Firefox, and it is working correctly.  If that's true, and it's not been confirmed, that would suggest that what we may have is another of those situations we have encountered in the past where less secure behavior is allowed to endure in the interest of not breaking anything in an existing installation; whereas anything new is run under the new and improved security settings.  But if so, that's intolerable, too, because it appears to be completely transparent, that is, no sign of that is shown in the user interface.  And if that's really what's going on, Firefox's UI is not telling the truth.  Which, anyway, that's a problem.



I wanted to bring all this up because, you know, it should be on everyone's radar in case others like me were trusting and believing Firefox's meaning of the term "Strict."  I would imagine that some of our listeners will be interested enough to dig into this and see whether they can determine what's going on.  As everyone knows, you know, I now have an effective incoming channel for effortless sending and receiving of email with our listening community.  So I'm really glad for that.  And, you know, I'm getting lots of good feedback about that from our listeners.



Okay, Leo, get a load of this one.  PC Magazine brings us the story of a security training firm who inadvertently hired a remote software engineer, only to later discover that he was an imposter based in North Korea.  They wrote:  "A U.S. security training company discovered it mistakenly hired a North Korean hacker to be a software engineer after the employee's newly issued computer became infected with malware."  The incident occurred at KnowBe4, and apparently they didn't - K-N-O-W-B-E and then numeral 4 - which develops security awareness programs to teach employees about phishing attacks and cyber threats.



So, yeah, you know, they're certainly a security forward, security aware company.  The company recently hired a remote software engineer who cleared the interview and background check process.  But last week, KnowBe4 uncovered something odd after sending the employee a company-issued Mac.  KnowBe4 wrote in a post last Tuesday:  'The moment it was received, it immediately started to load malware.'



"The company detected the malware thanks to Mac's onboard security software.  An investigation, with the help of the FBI and Google's security arm Mandiant, then concluded that the hired software engineer was actually a North Korean posing as a domestic IT worker.  Fortunately, the company remotely contained the Mac before the hacker could use the computer to compromise KnowBe4's internal systems."  Right?  So it was going to VPN into their network and get up to some serious mischief.



"When the malware was first detected, the company's IT team initially reached out to the employee, who claimed 'that he was following steps on his router guide to troubleshoot a speed issue.'  But in reality, KnowBe4 caught the hired worker manipulating session files and executing unauthorized software, including using a Raspberry Pi to load the malware.  In response, KnowBe4's security team tried to call the hired software engineer, but he 'stated he was unavailable for a call and later became unresponsive.'"  Yeah, I'll bet.  Oh, I should also say a stock photo of a Caucasian male was modified by AI to appear to have Asian descent.  And that's the photo that this employee submitted as part of his hiring process.  



"KnowBe4 says it shipped the work computer" - and get this - "to an address that is basically an 'IT mule laptop farm,' which the North Korean then accessed via VPN."



LEO:  Oh, interesting.



STEVE:  "Although KnowBe4 managed to thwart the breach, the incident underscores how North Korean hackers are exploiting remote IT jobs to infiltrate U.S. companies.  In May, the U.S. warned that one group of North Koreans had been using identities from over 60, six zero, real U.S. citizens to help them snag remote jobs.  The remote jobs can help North Korea generate revenue for their illegal programs and provide a way for the country's hackers to steal confidential information and pave the way for other attacks.  In the case of KnowBe4, the fake software engineer resorted to using an AI-edited photo of a stock image to help them clear the company's interview process."



So this should bring a chill to anyone who might ever hire someone sight-unseen based upon information that's available online - as, you know, opposed to the old-fashioned way of actually taking a face-to-face meeting to interview the person and discuss how and whether there might be a good fit.  One of the things we know is going on more and more is domestic firms, we've talked about it recently, are dropping their in-house teams of talent in favor of off-shoring their needs as a means of increasing their so-called agility and reducing their fixed costs.  This is one of those things that accountants think is a great idea, and I suppose there may be some places where this could work.  But remote software development?  I'd sure be wary about that one.



The new tidbit that really caught my attention, though, was the idea of something that was described as "an IT mule laptop farm."  Whoa.  So this is at a benign location, where the fake worker says they're located.  Being able to receive a physical company laptop at that location further solidifies the online legend that this phony worker has erected for themselves.  So this laptop is received by confederates who set it up in the IT mule farm and install VPN software, or perhaps attach the laptop to a remote KVM-over-IP system to keep the laptop completely clean.  Either way, this allows the fake worker to appear to be using the laptop from the expected location, when instead they're half a world away in a room filled with North Korean hackers all working diligently to attack the West.  I wish this was just a "B" grade sci-fi movie, but it's all for real, and it's happening as we speak.  Wow.  The world we're in today.



Okay.  Some feedback from our listeners.  Robert said:  "Hello, Steve.  I'll try to not take too much of your time, but I'd like to mention one thing that irked me about the entire 'Google trying to eradicate third-party cookies is a good thing' business."  He said:  "TL;DR:  Google tries to get rid of third-party cookies to gain a monopoly in the ad market, not to protect users."  He says that.  I don't see it that way, but okay.



He said:  "First and foremost, eradicating third-party cookies is a good thing, as a vehicle to stop tracking of website visitors.  The only reason why Google would be able to actually force website owners to move from their cookie-based ad strategies to something else (FLoC, Topics, labels, whatever they call it) is that they have a near-monopoly in the browser market."  Of course I 100% agree with that.  The only way the world would ever be able to drop third-party cookies would be if it was forced to do so.  And at this time in history, only Google is in the position to have the market power to force such a change.



Anyway, he goes on:  "It's important to keep in mind that Google is still a company that makes most of their money selling ads."  Right.  Okay, agreed.  "Every move they had made so far smelled like they wanted to upgrade their browser monopoly into an ad tech monopoly."  Okay, I would argue they already have that.  He said:  "My suspicion is that it wasn't necessarily the ad companies directly that threatened Google about its plan to eradicate third-party cookies, but rather some pending monopoly concern about the ad market.  But maybe I'm just too optimistic about that.  So, well, just a thought I felt was a bit underrepresented.  Thanks again for the work.  Robert."



Okay.  So the problem I have with Robert's analysis is that I cannot see how Google is giving itself any special privileges through the adoption of their Privacy Sandbox.  While it is absolutely true that they were dramatically changing the rules, everything that they are doing was a 100% open process, with open design and open discussion and open source.  And they themselves were also being forced, you know, forcing themselves to play by those same rules that they were asking everyone else to play by.



You know, very much like Microsoft.  We were just talking about them versus AV.  Microsoft is unwilling to accept the same limitations that they're asking the AV vendors to accept by using the API that they provide.  Google, not doing that.  They're going to use the same Privacy Sandbox that they're saying everyone else should.  So there was no advantage that they had over any other advertiser just because it was their Chrome web browser.  And had they been successful in bringing about this change, the other browsers would have eventually adopted the same open Privacy Sandbox technologies.  But as we know, that hasn't happened.



I did not have time to address this fully last week due to the CrowdStrike event.  So anyway, I'm glad for Robert's note.  The EU bureaucrats' apparent capitulation to the slimy tracking and secretive user-profiling underworld, which in turn forced Google's browser to retain its historically abused third-party cookie support, represents a massive privacy loss to the world.  This was the wrong outcome, and I sincerely hope that it's only a setback that will not stand for long.



Lisa in Worcester, Massachusetts wrote:  "Steve, another intriguing podcast.  Many thanks.  I find it interesting the influence Google has and doesn't have.  It seems more powerful over one company like Entrust than a whole market like third-party cookies.  Is it influence, or is it calculated cost benefit analysis that helps Google/Alphabet decide where to flex its muscles?  Thoughts from Worcester, Massachusetts.  Lisa."



Okay.  As an observer of human politics I often observe the simple exercise of power.  In U.S. politics we see this all the time.  Both major political parties scream at each other crying foul and unfair, but they each do what they do simply because they want to, and they can when they have the power to do so.  And I suspect the same is true with Google.  Google has more power than Entrust, but less power than the European Union.  So Google was able to do what it wished with Entrust, whereas the EU had the power to force Google to do what it wished.



And who knows what's really going on inside Google?  I very much wanted to see, as we know, the end of third-party cookie abuse.  But we don't really know that Google, or at least that all of Google did.  Others are suspicious of Google's motives, and maybe they're right to be.  The EU's pushback against Google's Privacy Sandbox might not be such a bad thing for Google.  I would imagine that an entity the size of Google has plenty of its own internal politics, and that not everyone may have identical motivations.  So I'm sure that some of Google was pleased and relieved by this turn of events.



But I mostly wanted to share Robert's and Lisa's notes as a segue to observing that this entire issue is more of a symptom than a cause, and that there's an underlying problem.  The cause of the actual problem is that, unfortunately, the online collection and sale of personal information has become a large, thriving, highly profitable, and powerful industry all unto itself, and that it may now be too big to stop.  This is what keeps the EFF awake at night.  We know enough from our previous examination of the EU's extended decision process here to have seen that their decision was directly influenced by commercial interests that wanted the status quo to remain unchanged.  And those interests were powerful enough to have their way. The question then becomes, how will this ever change?



The only thing more powerful than a minority of strong commercial interests is a majority of even stronger voting citizens.  But people cannot dislike what they're unaware of, and the personal data collection industry has always been careful to remain in the shadows since they know how vulnerable they would be if we, the larger public, were ever to learn a lot more about what was really going on.  We've often observed on this podcast that conduct that goes unseen is allowed to occur in darkness, and we know that users sitting in front of browsers have nearly zero visibility into what's taking place right before their eyes on the other side of the screen.  Those who perform this spying and data collection claim that no one really cares.  But the only reason people don't appear to care is that they don't really know what's going on.



When iOS began requiring apps to ask for explicit permission to track people outside of their own apps, the response was overwhelmingly negative.  People who were asked said no.  Similarly, it likely never occurs to the typical consumer that their own ISP who provides them with Internet bandwidth and who knows their real-world identity because they're being paid every month, is in the perfect position to catch and aggregate our unencrypted DNS queries, and the IP connections our account makes to remote sites.  This data represents a profit center, so it is routinely collected and sold.  It's allowed to happen only because it goes undetected and unseen.



Nearly three years ago, in October of 2021, the U.S. Federal Trade Commission, our FTC, published a news release with the headline:  "FTC Staff Report Finds Many Internet Service Providers Collect Troves of Personal Data; Users Have Few Options to Restrict Use."  And the subhead reads:  "Report finds many ISPs use web browsing data and group consumers using sensitive characteristics such as race and sexual orientation."  Why would they be doing this if it wasn't of some commercial use to them?



It seems obvious that if any consumer ever gave their permission, it was not truthfully made clear to them what was going to transpire.  I certainly never gave my cable provider permission to do that.  But I have no doubt that the consumer agreement I originally signed but, you know, never read, or any of the updated amendments which may have been sent to me about it, I'm sure it contained the sort of language we've talked about before, where information about our online use may be shared with business partners and so forth.



Anyway, sometimes enterprise can be a little too free.  This is why the protection of consumers from this sort of pervasive privacy violation for profit is the role of government.  Unfortunately, government is just people, too, and people can be purchased.  In the U.S. at least, lobbyists for commercial interests hold a great deal of sway over how the government spends its time and our tax dollars.  The U.S. has a couple of senators who see and understand the problem.  But, you know, they're investing a great deal of their time in doing what they can.  But most legislators appear to feel they have bigger fish to fry.



I think what all this means is that it's up to those of us who care to do what we can.  I'm disappointed that Google appears to have lost this round, but I understand that it probably had no choice.  I'm sure we'll be talking about this again, once we see what Google says that they'll be coming up with, you know, as a compromise of some sort.



Okay.  Lee Mossner shared a Mastodon posting by Brian Krebs about, speaking of the devil, the collection and reselling of automotive data being done by automakers without their drivers' clear knowledge or permission.  Brian posted - and this, you know, I mentioned a couple of the senators who are doing what they can?  "Senator Ron Wyden has released details," Brian Krebs posted, "about an investigation into automakers' disclosure of driving data, such as sudden braking and acceleration, to data brokers for subsequent resale to insurance companies.  General Motors also confirmed to Wyden's office that it shared consumers' location data with two other companies, which GM refused to identify.



"The senators' letter to the FTC included new details about GM, Honda, and Hyundai's sharing of drivers' data with data brokers, including details about the payments the data broker Verisk made to automakers.  Based on information Wyden obtained from automakers, the senators revealed Hyundai shared data from 1.7 million cars with Verisk, which paid Hyundai a little over a million dollars, $1.043 million.  Honda shared data from 97,000 cars with Verisk.  And automakers used deceptive design tactics, known as 'dark patterns,' to manipulate consumers into signing up for programs in which driver data was shared with data brokers, for subsequent resale to insurance companies."  So yes.  Technically we're giving permission, but not intending to, or not understanding what it is that will be done as a result.



And of course I have no answer to this other than for us to be aware of what's going on and take whatever measures make sense.  I presume that it's no longer possible to purchase any modern vehicle that isn't connected to the Internet and dutifully feeding back everything that goes on within its perimeter to some hidden agency in the cloud.  So for the time being that's part of what it means to be an owner and operator.



And finally, Alex Neihaus, one of our earliest supporters, or maybe he was THE earliest supporter, he was with Astaro, and they were advertising the Astaro Security Gateway in the early days.  He sent an interesting note asking an interesting question.  He said:  "Hi, Steve.  I tried the DNS Benchmark today running on Windows 11 ARM64 in a VM hosted on a MacBook Pro using Apple Silicon.  See the image below."  He said:  "It appeared to run flawlessly and at full speed."  And he says in parens - and you'll like this, Leo.  "Windows 11 ARM" - this is Alex saying - "runs faster, in my humble opinion, on Apple Silicon than on any real PC I've tried.  It's astonishing," he says.



So he says:  "I'm wondering if you have an opinion about accuracy of the app's results in this scenario, emulation of x86 instructions in a VM."  He said:  "I think I remember you saying the DNS Benchmark is highly timing dependent for accuracy.  I wonder if sheer brute computing capability, as provided by an Apple Silicon processor, can overcome the costs of double emulation.  Really enjoying Security Now! these days.  Thanks.  Alex."  And Alex attached a screenshot of GRC's DNS Benchmark running on a Windows 11 desktop hosted on a MacBook Pro.



I included this because with the rise of ARM and Windows for ARM finally becoming real after so many false starts, I've been thinking about the fact that I write x86 code, and that all of my utilities are written in Intel x86 assembly language.  I've always felt that this meant that my code had unnecessary performance overkill, since it uses so very little of the processor's available resources to run.  But this becomes something of an advantage when an ARM-based machine is used to emulate Intel's x86 instruction set.  My utilities are objectively small because there are so many fewer instructions in them.  And that means significantly less emulation overhead.



So I think that my approach is going to stand the test of time because there's no way any version of Windows, whether hosted on Intel or ARM, will not be able to run x86 instructions one way or another.  And the fewer of those there are, the faster the code will go.



And to answer Alex's question, yes to the DNS Benchmark's proper operation under ARM.  The only thing the benchmark requires for accuracy is reliable timing snapshots from the system's processor clock counter, and that would be one of the first things the designers of the x86 emulation and its VM would have provided.  So accurate, like what is this instant right now, that's what we need.  And I'm sure that would be available within a VM emulating an Intel x86 environment.  And Leo?



LEO:  Yes, sir.



STEVE:  Our last break, and we're going to talk about something of potentially significant interest to our listeners.  And there is a way to find out, as we will see, if your systems are vulnerable.



LEO:  Okay.



STEVE:  So we've got some takeaway user action, too.  Okay.  So Platform Key Disclosure.  Today's topic will reveal the details behind a widespread - and by that I mean industry-wide, shockingly industry-wide - security failure that was discovered in the supply chain of hundreds of today's PCs from some of our largest manufacturers.  The upshot of this is that these machines are unable to boot securely, despite the fact that that's what they say they're doing.  And while that'll be our primary focus, the larger point I hope to drive home is that this is additional evidence to substantiate my belief that Microsoft's Recall is an inherently doomed proposition, at least if we require absolute privacy for the accumulated device history that Recall proposes to aggregate.



The reason for this is that despite all of Microsoft's assertions about how tightly and deeply they'll be protecting the accumulated history of their users, doing so with sufficient security is simply not possible due to the pervasive lack of security that pervades the entire PC ecosystem.  It's like Leo asked earlier in this podcast, you know, there's really no security anywhere, is there.  It's like, well, you know, there's barriers.  But I've always referred to security as "porous," and this is why.  Much lip service is given to how securable everything is, yet it keeps getting broken over and over and over.



Okay.  So get a load of what's happened now.  The security firm Binarly (B-I-N-A-R-L-Y) has coined the term "PKfail" for their discovery of serious problems with the Platform Keys - this is abbreviated PK - being used across our industry to provide the root of trust for our system's Secure Boot technology.  Here's what they explain.  They said:  "Today we disclose PKfail, a firmware supply-chain issue affecting hundreds" - and I should say just shy of 850 hundreds - "of device models in the UEFI ecosystem.  The root cause of this issue lies with the Secure Boot 'master key,' called the Platform Key in UEFI terminology.



"The Platform Key used in affected devices is completely untrusted because it's generated by Independent BIOS vendors and widely shared among different hardware vendors.  This key is used to manage the Secure Boot databases that determine what is trusted and what instead should not be executed, effectively maintaining the chain of trust from the firmware to the operating system.  Given its importance, the creation and the management of this master key should be done by the device vendors following best practices for cryptographic key management, for example, by using Hardware Security Modules.  Specifically, it must never be disclosed or known publicly."  Right?  Now, the hardware that supports the firmware is a hardware security module.  The hardware's designed to keep secrets.  But if what you store in there is not secret, then it doesn't matter if you keep it because it's already known.



So they said:  "However, the Binarly Research Team found that these keys are generated and embedded in a BIOS vendor's reference implementation as sample keys under the expectation that any upstream entity in the supply chain" - I guess I would call that a downstream entity, but anyway, you know, any subsequent entity in the supply chain - "such as OEMs or device vendors would replace them.  When this does not happen, devices are shipped with the original 'sample' untrusted keys in place.  Binarly researchers identified the private part of one of these Platform Keys in a recent data dump following a leak.  This key is currently being used by hundreds of devices in the market, putting every one of them at immediate risk.  A peculiarity of PKfail is that it represents yet another example of cross-silicon issue, as it affects both x86 and ARM devices."



They wrote:  "We've developed proofs of concept to demonstrate that attackers with access to a device vulnerable to PKfail can easily bypass Secure Boot by signing their malicious code and thus enabling them to deliver any sort of UEFI rootkit, like the recently discovered BlackLotus.  Modern computing relies on establishing and maintaining trust, starting with trusted foundations and extending through operating systems and applications in a chain-like manner.  This allows end users to confidently rely on the integrity of the underlying hardware, firmware, and software.  In modern systems, trust is typically rooted in hardware-based implementations such as Intel Boot Guard or AMD's Platform Security Processor.



"The root trust is then propagated to the operating system via Secure Boot, which ensures that only digitally signed and verified bootloaders and OS kernels are executed by the boot manager.  Secure Boot technology uses public-key cryptography and relies on four keys and databases for authentication.  The four keys are, one, the Platform Key (PK)."  And they say:  "The root-of-trust key embedded in the system firmware establishes trust between the platform owner and platform firmware.  Two, the Key Exchange Key (KEK).  This key establishes trust between the operating system and the platform firmware.  Third is the Signature Database (db), this database containing trusted signatures and certificates for third-party UEFI components and boot loaders, which are thus granted execution.  And then fourth, the dbx, which is the Forbidden Signature Database, a database containing signatures and certificates used to sign known malicious software, which are thus denied execution."



In other words, these are specific signatures and certificates that would otherwise be valid because they somehow got themselves signed by keys that are valid, but these are specifically known to be not valid.  And I should mention that it was Security Now! Podcast 500.  We're at 985.  We're approaching the famous 999 boundary and going to a thousand.  So this was exactly half of that ago.  Episode 500 was the one where the entire podcast talked about this trusted platform technology and the UEFI with platform keys and all that, if anyone wants to go back and get more information.



Anyway, they said:  "Each database is stored in its corresponding" - each of those four things - "corresponding nonvolatile RAM variable (PK, KEK, db, and dbx).  These variables are authenticated, meaning that when Secure Boot is enabled, updates are only allowed if the update data is signed by a higher level key, the highest level being PK, the Platform Key.  The Platform Key can be used to add or remove keys from the Key Exchange Key database, while the Key Exchange database key can be used to update the Signature Database and the Forbidden Signature Database.



"Being at the root of the trust hierarchy, that master Platform Key (PK) plays a critical role in the security of Secure Boot.  Access to the private part of the Platform Key allows an attacker to easily bypass Secure Boot.  The attacker can update the Key Exchange Key database with a malicious KEK which can be subsequently used to tamper with the Signature Database and the Forbidden Signature Database.  Since these databases are used during the verification process, an attacker exploiting PKfail can cause untrusted code to be run during the boot process, even when Secure Boot is enabled.



"The Binarly Research Team discovered that hundreds of products use a sample test Platform Key that was generated by American Megatrends International (AMI).  This key was likely included in their reference implementation with the expectation that it would be replaced with another safely generated key.  That never happened.  Since these test keys are shared with commercial partners and vendors, they must be treated as completely untrusted.



"Several facts give us confidence in this assessment," they wrote.  Okay.  So here they are, three of them.  "One, by scanning an internal dataset of firmware images, we confirm that devices from unrelated vendors contain the same Platform Key, meaning that these keys must have been generated at the root of the firmware supply chain.



"Number two."  Get this.  "These test keys have strong indications of being untrusted.  The certificate issuer contains the clear strings 'DO NOT TRUST' and 'DO NOT SHIP' in all capital letters."  Like the common name, the CN, the Common Name in the certificate is "DO NOT TRUST," and the issuer is "DO NOT SHIP."  It couldn't be made any more clear.  Yet they are trusted, and they did ship.



"Number three," they said.  "More importantly, Binarly Research discovered the private component of one Platform Key in a data leak, where an alleged OEM employee published the source code containing the private Platform Key to a public GitHub repository.  The private key was stored in an encrypted file, which was 'protected' [they have in quotes] by a weak four-character password and thus easily cracked with any password-cracking tool.  Thus the untrustworthiness of this key is clear.



"Shortly after discovering PKfail, it became apparent that this was not a novel vulnerability.  In fact, it's quite the opposite.  A quick search on the Internet returned numerous posts from users finding keys marked as 'DO NOT TRUST' in their systems, worried about the security implications of that.  But even more concerning, we discovered that the same vulnerability was known as far back as 2016, and it was even assigned CVE-2016-5247.  Why are so many devices still vulnerable to this issue almost 10 years after its public disclosure?



"The harsh truth is that the complex nature of the firmware supply chain - where multiple companies contribute to the production of a single firmware image, and the security of each component relies on others' security measures - demands inspection capabilities that are far from the current industry's standards or simply unavailable from a technological point of view.



"In 2019, the Linux Vendor Firmware Service project (LVFS) introduced a check based on YARA rules to detect non-production keys.  This rule matches on the strings 'DO NOT TRUST' or 'DO NOT SHIP' with the intent of identifying and reporting firmware vulnerable to PKfail.  This rule works well when the Platform Key is stored in an uncompressed form, but fails when the key is compressed and stored in a raw section or within the data section of UEFI modules, as is often the case.



"To address this and other software supply-chain security vulnerabilities, Binarly Transparency Platform analyzes firmware images and autonomously unpacks all nested components, creating a detailed blueprint of the input, allowing for the detection of PKfail and other known and unknown security vulnerabilities.



"To understand the actual scope of PKfail and its historical patterns, we scanned an internal dataset of UEFI images using our Binarly Transparency Platform.  This dataset is representative of the UEFI ecosystem as it contains tens of thousands of firmware images released in the last decade by every major device vendor, including Lenovo, Dell, HPE, HP, Supermicro, Intel, MSI, and Gigabyte.



"The macro results of this scan are quite alarming.  More than 10% of firmware images in our dataset use an untrusted Platform Key" - meaning a key that was specifically branded, labeled "DO NOT TRUST," "DO NOT SHIP," and has been cracked and broken and is known - "and are thus vulnerable to PKfail."  More than one in 10.  "When reducing the dataset to only more recent firmware released in only the past four years, the percentage drops to 8%, though remaining at concerning levels.



"The first firmware vulnerable to PKfail was released back in May of 2012, while the latest was released last month, in June of 2024.  Overall," they write, "this makes this supply-chain issue one of the longest lasting of its kind, spanning more than 12 years.  The list of affected devices, which at this moment contains nearly 850 devices, can be found in our BRLY-2024-005 advisory.



"A closer look at the scan results revealed that our platform extracted and identified 22 unique untrusted keys.  The table below reports the five most frequently used keys, along with a breakdown of affected products and vendors."  Thank you, Leo, for putting that on the screen.



So what we see in this table, we have the certificate serial number, five different certificates.  The certificate subject, that is the CN, says "DO NOT TRUST AMI Test PK," Platform Key.  All five of them clearly labeled "DO NOT TRUST AMI Test Platform Key."  The issuer is "DO NOT TRUST AMI Test Platform Key."



LEO:  Who issued these keys?  Some guy named DO NOT TRUST.



STEVE:  So where are they in use?  Acer, Dell, Fujitsu, Gigabyte, Intel - Intel themselves - Lenovo, and Supermicro.  First seen in April of 2018; most recently seen last month, in June, in firmware released on a new machine from some one of these guys in June of 2024.  And Leo, what this means is Secure Boot is subverted on that platform.  Malware can install itself, even with Secure Boot enabled.



LEO:  Do you have to have access to the machine?



STEVE:  Physical access definitely allows it to happen.  But we've seen many instances where, once malware got into the system, they were able to modify the UEFI boot just using their software access to the machine.



LEO:  And then the machine can't detect it because...



STEVE:  Exactly.  And then you've got a bootkit installed in your firmware permanently, where even reinstalling the OS, reformatting the drive, taking the drive out, blah blah blah, nothing gets rid of it.



Okay.  So they explain and finish:  "From the certificate subject and issuer strings" - which all say DO NOT TRUST - "we conclude that these keys were generated by AMI."  Because it says AMI Test PK.  "This conclusion is further supported by how these test keys ended up in devices sold by unrelated vendors, as shown in the last column of the table."  Okay.  So who?  Acer, Dell, Fujitsu, Gigabyte, Intel, Lenovo, Supermicro.  A repeat on the second certificate, looks like same people in the third certificate, same people in the fourth certificate, and same people in the fifth.  So Acer, Dell, Fujitsu, Gigabyte, HP, Lenovo.  Oh, Samsung appeared in the fifth most recently.  Oh, earlier.  First appeared in 2012, in May of 2012; and last seen in March of 2021.  So for that fifth one.



They said:  "By looking at the actual product names and models, we found another concerning issue:  the very same key is used to protect a highly heterogeneous set of products.  For example, the key with serial starting with '55:FB:EF' was found both in gaming laptops and in server motherboards."  So probably, what, an Acer gaming machine and a Supermicro server, since Acer and Supermicro were both found to be using that first key.  "Moreover, as we can see in the Last Seen and First Seen columns, these keys survive in the ecosystem for years, up to almost 10 years in the case of the key with a serial number beginning '1B:ED:93.'



"When looking at the historical trends of PKfail, several worrisome observations can be made.  First, even after CVE-2016-5247 was made public, the release rate of images vulnerable to PKfail retained its previous increasing trend, suggesting that the firmware industry was not responsive to that 2016 vulnerability discovery.  This behavior is consistent with the findings from our retrospective analysis of LogoFAIL patches, where we found that the industry needed several months after the disclosure before security patches were ready and propagated to end-users.



"The reaction to CVE-2016-5247 can finally be seen in the period from 2017 to 2020, where the number of vulnerable images steadily decreased.  This trend, however, changed again after 2020 and has persisted until current day, with a constant increase of vulnerable devices," which means people forgot about this problem and then started replicating the bad keys once again across their own devices.



"Another observation related to the private Platform Key leaked on GitHub is that this leak did not result in any visible impact on the number of vulnerable devices.  This is once again unsurprising when put into the historical context of the firmware industry just not caring.



"To be clear, this leak went almost unnoticed.  In fact, they write, "we were the first to report that it contained the private part of a Platform Key.  Second, the slow reaction to this security incident.  By mining the data provided by the GitHub Archive and available on the Wayback Machine, we confirm that the repository remained publicly available for at least four months before getting noticed and removed by its original author, while it took five months to delete all the forks of the offending repository.  Quite concerningly, the leaked key is still in use today in many devices and has been used for quite some time.  The first occurrence of this key in our dataset dates back to April of 2018."



Okay.  So this means that, while much as been made of Secure Boot, because Secure Boot is utterly reliant upon the secrecy of the private key at its root, around one out of every 10 machines in use today, and even shipped as recently as last month, is only pretending to have Secure Boot because its private key is one of the handful that AMI originally provided - and clearly marked, they believed well enough - as DO NOT TRUST and DO NOT SHIP.  Yet shipped they were, and trusted they are still being.



One very cool bit of tech that Binarly shared is the way any Linux or Windows user can check their own PCs for the presence of these insecure keys.  They wrote:  "Devices affected by PKfail will have the strings 'DO NOT TRUST' or 'DO NOT SHIP' in the subject and issuer fields of the Platform Key certificate.  On Linux, PKfail can be easily detected by displaying the content of the PK variable."  And they show the command:  efi-readvar -v PK.  And that causes Linux to dump out Variable PK, length 862.  And then basically you're looking at the PK variable certificate, where you can clearly see subject is CN=DO NOT TRUST - AMI Test PK, and issuer is CN=DO NOT TRUST - AMI Test PK.



Then they write:  "On Windows, running the following command in a privileged PowerShell console will return True on affected devices."  And this command is a little long for me to read out on the podcast.  It's at the bottom of the show notes, page 23.  Basically, it is a powershell command Get-SecureBootUEFI and then space PK, then .bytes.  That returns a bunch of ASCII, which you then run a match on, matching on DO NOT TRUST or DO NOT SHIP.



LEO:  Now, will this work on all machines?  Because didn't you say some of them were obfuscated?



STEVE:  Yeah.  No, no.  This will work because it's...



LEO:  It's already loaded.



STEVE:  ...looking at the certificate.



LEO:  Yeah.  Okay.



STEVE:  Well, it's looking at the certificate of the Platform Key.  So anybody who wants to know whether they're one of the one in 10 whose machine has this, is able to run it.  And I'm sure, I'd love to hear some anonymous feedback from our listeners who, either under Linux or Windows, run these commands and do or don't find they've got this insecure, well-known key.



The Binarly Research Team recommends that affected users update their firmware when device vendors release an updated version with fixes for this PKfail event.  Expert users, they said, can re-key the Platform Key (PK) with a trusted key.  The other Secure Boot databases - the KEK, the db, and the dbx - must also be assumed to be compromised, thus expert users should check and replace them with new databases of trusted signatures and certificates.  And I'm sure that will be part of the pack that the vendors release.



So just so everyone is clear here, there is no obvious remote vulnerability.  Just because you have, like, a bad Secure Boot doesn't in any way make your system actively vulnerable.  The primary danger is from local boot tampering with a machine that was believed to be secured against exactly such manipulation.  And as I said when Leo asked, we have seen plenty of instances where remotely injected instances of malware were then able to establish persistent bootkit rootkits by messing with the user's motherboard firmware from that machine.  That would, you know, and this compromise means that could work again.



So this is not the end of the world by any means.  And since this time, unlike previous times, Binarly's research, which is breathtaking in its scope, has generated far more interest than the issue did back in 2016.  And since there really does appear to be a great deal more interest in security in general today than eight years ago, I would expect that the manufacturers of all still supported systems will arrange to respond to the egg that they now all have on their faces because this is all their fault for not ever generating their own Platform Key and just using the one that came with the firmware which said "DO NOT TRUST" and "DO NOT SHIP."  There is no excuse for being so negligent as to ship systems with those keys in place.



And again, I'm not going to tell anybody that they should not use Recall once it becomes available.  That's not my place.  I understand 100% that the reward from its use may well justify the very slim chance that the data it gathers might be put to some use that its owner would find objectionable.  After all, that's certainly happening no matter where we go today on the web, or where or even how we drive our cars in the real world.  You know, why should this be any different?



But that said, what does appear to be imperative, you know, Microsoft's repeated and well-meaning assertions about their ability to secure this information notwithstanding is for every individual to be given the well-informed choice about whether or not they want this for themselves, this Recall storage, and for that choice to be honored without exception or excuse.  Here's another example today that, you know, Microsoft says, oh, yeah, we've got Secure Boot.  Turn it on, you know, and let Windows Hello logon.  Well, that was violated two weeks ago, but I didn't have time to talk about that because of CrowdStrike crashed the world.  So, yeah.  Good luck.



LEO:  I have to say, you know, in order to install Linux, I'm in the habit of turning off Secure Boot anyway.  Nowadays you can keep it on in many Linux distros and so forth.  But it's not, I mean...



STEVE:  No.  It's not.  Yes, I agree. 



LEO:  Not a huge loss, I guess.



STEVE:  And as somebody who would love people to be able to boot DOS on their systems, Secure Boot...



LEO:  Same thing; right?  Just gets in the way, yeah.



STEVE:  ...is a thorn in my side.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  I understand it was created at the time when we were really worried about BIOS-resident malware, which you can't ever get rid of.  So I understand that.  Rootkits and that kind of thing.  But I don't know.  I guess I've lived dangerously.



STEVE:  Yeah, Leo, just use CrowdStrike.  What could go wrong?



LEO:  Chromebooks do the same thing.  They have a Secure Boot.  Macs now do that, as well.  They verify their boot code.  It does make sense to do that.  I think it really does, yeah.  And most Linux systems now support UEFI and Secure Boot.



STEVE:  Yeah.  I would say that the vulnerability is the targeted, you know, state-level actor.



LEO:  Yeah, exactly.



STEVE:  You know, the kind of vulnerability that Stuxnet was wanting to take advantage of, where you've got somebody who is able to get brief, you know, the kind of Mission Impossible thing where they switch out someone's laptop and then take it in the backroom and install the rootkit and then switch them back before they know.  Now they've got a rootkit that they didn't have before.



LEO:  Yeah.  Didn't have to take it in the backroom anymore.  It saves a lot of time.



STEVE:  We don't think you're worried about, you know, Ethan Hawke coming down from a guy wire...



LEO:  On a cable.



STEVE:  On a cable, making the switch from hanging from the side of a cliff.  Then I think you're probably okay.



LEO:  All right.  This has been another Mission Impossible to figure out what the hell is going on and to keep you safe in the face of extraordinary threats.  That's this guy.



STEVE:  That's this ever-changing world.



LEO:  An ever-changing world.  Mr. Steve Gibson, he's at GRC.com.  There are many things, many things at GRC.com.  I'll mention a few.  SpinRite, the world's best mass storage performance, maintenance, and recovery utility.  Performance is important.  We talked on Ask the Tech Guy about a guy had an SSD, was worried that, you know, what do you need to do.  And he had this very elaborate thing of copying everything off the SSD and then formatting it and copying everything back on.  And I thought, I don't think that's really what you should be doing.  But maybe check out SpinRite.  It'll help your SSD if you're having performance problems.



STEVE:  Bye.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#986

DATE:		August 6, 2024

TITLE:		How Revoking!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-986.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What's been learned over the past week about the PKfail Platform Key misuse issue?  What is "IsBootSecure?" and why does that sound suspiciously like a new piece of GRC freeware?  There's plenty of news on the third-party cookie front.  What's going on with Firefox and what position has the World Wide Web Consortium (W3C) taken on this important issue?  Now that we're a few weeks downstream of the CrowdStrike disaster, the attorneys have come out to play.  What are we learning about the legal side of this massive outage?  What's been going on with GRC's incoming "SecurityNow" email system?  And we finish by looking at DigiCert's recent mass certificate revocation event:  Why it happened?  What happened?  Did it matter?  Was it necessary?  And how does it compare to Entrust's past behavior?



SHOW TEASE:  It's time for Security Now!.  This episode with Steve Gibson blows the lid off of a number of my deeply held convictions.  For instance, third-party cookies.  Does Firefox block them?  No.  No.  Even though they say they do.  Does certificate revocation work?  No.  It turns it never has.  Steve explains, shows you how to test your browser.  And then finally, yes, Steve has another brand new freebie program, he wrote it over the weekend, that you're going to find very useful.  There's a lot of great stuff coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 986, recorded Tuesday, August 6th, 2024:  How Revoking!



It's time for Security Now!, the show where we cover the latest security news, the show you wait all week long for.  And here he is, the man of the hour, Steve Gibson, our host.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  It's great to be with you once again for, well, the last episode we're recording in the TWiT Eastside Studios.



LEO:  I think a lot of people probably, especially on this show, listen.  And they won't notice any difference at all.



STEVE:  No.  There will be no effect.  No.



LEO:  Yeah.  Even if you watch, I mean...



STEVE:  It will be just as good as it's always been.



LEO:  It's pretty much the same.  Really, it's mostly me that's suffering from this.  You're going to have exactly the same experience.  The listeners probably have 99% the same experience.  I'm the one who's going to be all alone.  Who's going to bring me lunch?



STEVE:  I know, Leo.  Leo, it's just tough.  I don't know.



LEO:  I told Lisa, "You're going to bring me a sandwich every day at 1:00; right?"  She said, "What, are you crazy?"



STEVE:  We're going to have to watch cats crawling around on you.



LEO:  And there will be cats.  



STEVE:  On the video.



LEO:  There will be cats.  So what's up in the news this week?



STEVE:  Okay.  So lots of good stuff.  We've got a bunch of things that have been learned over the past week about the big topic from last week, which was this PK file exploit, the Platform Key misuse issue.  And we're going to answer the question, what is "IsBootSecure?"  That's the name of it, IsBootSecure?  And why does that sound suspiciously like a new piece of GRC freeware? 



LEO:  Uh-oh.



STEVE:  Ah.  Then there's plenty of news on the third-party cookie front.  What's going on with Firefox, and what position has the World Wide Web Consortium, the W3C standard-setting body, finally taken on the third-party cookie issue after, as we know, Chrome basically capitulated to, okay, fine, I guess we're not going to be able to get rid of those.  Also, now that we're a few weeks downstream of the CrowdStrike disaster, the attorneys have come out to play.  What are we learning about the legal side of this massive outage, things like limitations of liability and so forth?  Also I'm going to talk briefly about what's been going on or what is going on with GRC's SecurityNow! email system.  And then we're going to finish - thus the title "How Revoking!" - by looking at DigiCert's recent mass certificate revocation event, why it happened, what happened, did it matter, was it necessary, and how does it compare to Entrust's past behavior?  So I think lots of interesting information and news for our listeners this week.



LEO:  Can't wait.



STEVE:  And of course we have an apropos Picture of the Week.



LEO:  Yes.  I did peek this time, I'm sorry.  It's very funny.  Now, time for the Picture of the Week.  Steve?



STEVE:  So this was just a great picture one of our listeners sent me.  I gave it the caption, "Is it too late to change my  mind?"  This shows a very unhappy person wearing a CrowdStrike Intern T-shirt, obviously labeling him as a CrowdStrike intern.  He's holding up a little laptop PC with the screen open and the frowny face "your machine has just crashed" blue screen.



LEO:  Oops.



STEVE:  And we can't really identify where he is.  There's no obvious airport-ness about the background except that there's a bunch of people who have kind of got carry-on-ish things, and they're standing on industrial-strength indestructo carpet, the kind that you see in the airport where...



LEO:  Oh, yeah.



STEVE:  You know.  So it suggests that this, you know...



LEO:  I've slept on that carpet a few times.  I know that carpet, yeah, yeah.  Wow.



STEVE:  So it does, it does look like this maybe had been an opportunistic shot that was taken when his flight was canceled, as well as everybody else's.  And, you know, he turned his computer on, it's like, oops, me, too.



LEO:  Yeah, yeah, wow.



STEVE:  Yeah.  So I don't know that I would recommend he changes his mind, but...



LEO:  Wear that T-shirt with pride.



STEVE:  Well, that's a good point.  You might want to turn it inside-out for a while.



LEO:  Actually, I would love that T-shirt.  Wouldn't you like to wear that T-shirt once in a while?  That would be fun.  Wow.



STEVE:  Yeah, it could be "I survived."  Yeah, yeah.



LEO:  Yeah, yeah.



STEVE:  Okay.  So last week's discussion of Binarly's discovery that by their estimate the manufacturers of around one out of every 10 PCs, even those sold recently, have never bothered to replace the "sample" and inherently insecure Platform Key which AMI clearly marked as "DO NOT USE" and "DO NOT TRUST," has continued to bear fruit.  Over the weekend one of the people in GRC's newsgroup discovered that one of his recently purchased Dell PCs - I think it was last month he got this thing - did indeed contain one of the insecure "DO NOT USE" Platform Keys.



Now, since last week's podcast I received a great deal of email from listeners - and we'll be talking about a great deal of email and what that means a little bit later - who were unable to get the command-line that Binarly had supplied, and which I quoted in the show notes and mentioned last week, to work.  And I'm not surprised.  When I tried it myself, it was the definition of a mess.  For one thing, in order for any of the UEFI commands to work, which is one of the commands you must use, an extra not-normally-present UEFI module must first be installed.  So you have to do that.



And then, since the module apparently needs to run a script, the normally protective PowerShell script blocker must be disabled by disabling some of PowerShell's security.  And then it turns out that, if Secure Boot is not enabled, the command doesn't work at all.  So if the machine is booted from BIOS rather than UEFI, you know, it's not even working.  And then, to make matters worse, the Binarly command-line silently checks the output text from the command and, like, pipes it into a string finder, well, the output might well be complaints about any of the above being a problem.



But even so, then a match is searched for the DO NOT USE/DO NOT SHIP strings, which will not be present in an error output.  So as a result, this will tend to return massively false negative results, since any output which is not the Platform Key's actual certificate will not contain those strings.  So even though the bogus AMI sample Platform Key might be sitting right there on the motherboard, you would never know.  So longtime listeners of this podcast probably know where this is going.  By Friday morning around 9:00 a.m. I had collected all of this feedback and tried using the command-line myself.  So I thought, okay, this needs another one of GRC's little Windows apps to...



LEO:  Oh, my god.  This is why we love you, Steve.  This is amazing.  Unbelievable.  I'm just going to write - I'll write it myself.



STEVE:  That's right, to let anyone quickly and easily get to the truth of what's going on.  So I was all set up to continue working on SpinRite 6.1's documentation, which is my current focus since I need to get that behind me.  I opened a new code project and began writing code to extract and analyze the motherboard's Platform Key.  And everybody can try it today.



While I was doing that, and publishing incrementally more capable pre-releases for the gang in the newsgroups to test over the weekend, we were also playing the name game, as we call it.  That collective name brainstorming had previously resulted - actually it was from someone you know, Leo, Paul Holder - it had previously resulted, it was his idea to give the fake USB drive testing app ValiDrive, which I love.



LEO:  Yeah.



STEVE:  And oh, my god, years ago somebody else came up, I was working on that DCOM piece of freeware, and the guy said, "Well, you have to call it the DCOMbobulator."  I'm like, "Oh, yes I do."  Anyway...



LEO:  I thought you made up all these names yourself.  Aww.



STEVE:  I think maybe all the other ones.  Those are the only two, I think, that I got from...



LEO:  Trouble, what was it, Trouble in Paradise?



STEVE:  Oh, yeah, that was mine.



LEO:  That was the first one we talked about.



STEVE:  Well, you and I and Kate on the old Screensavers show.



LEO:  The Screensavers, yeah, yeah.



STEVE:  In South San Francisco, yup.



LEO:  Click of Death, TIP.



STEVE:  Yeah, Trouble in Paradise.  And OptOut and all the other things, those are my names.



LEO:  ShieldsUP!, yeah, yeah.



STEVE:  But in this case I was coming up dry.  So there was a lot of conversation with ChatGPT among those in the newsgroups.  Lots of brainstorming.  Finally, pretty much exhausted at the end of a long weekend of, like, endless names, we just settled on IsBootSecure?  Which, you know, it's kind of a good name.



LEO:  I like it.



STEVE:  Yeah.  So if you go to GRC.com right now, there is an interim fully working version of this.  You can find it under the Main Menu > Freeware Security > IsBootSecure?, or just GRC.com/isbootsecure.  It's currently a very small, dare I say 21K?  And actually about half of that is the signature that I had to give it.  So, you know, because it's got to be digitally signed.  It's a little 21K Windows executable that can simply be run on any Windows machine.  And I'm not sure about Wine yet.  Again, this just happened.  The paint is still wet on this.  And it will tell you exactly what's going on.  And Leo, I see it there.  You just ran it, apparently.  Or you're looking at the show notes.  That might be...



LEO:  That's in the show notes, yeah.



STEVE:  Oh, okay, yeah.  Because I...



LEO:  I don't actually have a Windows UEFI machine to run it on.



STEVE:  Anyway, it detects which firmware boot mode the system is in, BIOS or UEFI.  If UEFI, it determines whether the UEFI firmware is in Secure Boot mode or not, which is not obvious, you know, unless you reboot.  So it shows you that.  And if Secure Boot mode is enabled, and on some machines even if it's not enabled, it will then extract, which is to say if it's available, the Platform Key from the motherboard and display the certificate's most useful information - the serial number, its issuer name and subject name.  That's where the DO NOT TRUST or DO NOT SHIP strings will be found, if they're present.  And it displays the certificate's not valid before and not valid after dates, just because they're kind of interesting.



LEO:  Did you put in the not valid after date yourself?  Or is that the actual not valid after on one of your certificates?



STEVE:  Yeah, the...



LEO:  Why I ask, that's four days before the end of the Unix Epoch.



STEVE:  That's interesting.  That is interesting.



LEO:  Yea.  It's not the Epoch, which is the 19th.  It's four days earlier.  



STEVE:  Yeah.  I did not put that in, and I haven't looked at the certificate format.  But it might be Unix time in the cert, in which case...



LEO:  I think it is, yup.



STEVE:  That would make a lot of sense, wouldn't it.



LEO:  Yeah.



STEVE:  Cool.  Good catch, Leo.



LEO:  No, when I see 2038, my ears perk up.



STEVE:  Yeah, and you know, let's see, how old will we be, and what podcast will be on?



LEO:  That I don't want to think about.  Oh, my goodness.  For people who don't know, Unix time started January 1, 1970, but it's a 32-bit integer; right?  So after 32 bits it will roll over, just like the Y2K problem.  And that turns out to be 3:14 a.m. UTC, on 19 January 2038.



STEVE:  Well, that's only 14 years from now, Leo.



LEO:  So we'll not be that old.  In our 80s.



STEVE:  At this point, we're only a little over halfway there.  So...



LEO:  Jerry Pournelle used to come on the show in his late 80s.  I think we could do it.



STEVE:  Yeah.  And you just have to somehow get up the stairs to the attic.



LEO:  Yeah.  I need one of those little elevators, it's seven steps, and it says mmmmmmmm, and I'll be ready to go.



STEVE:  Okay.  Anyway.  So right now this is just a Windows text screen which pops up.  Oh, also, as an extra bonus, it displays the standard Windows certificate dialog box containing your Platform Key certificate so that any of the other details of the certificate can be examined.  For a while over the weekend it was also exporting the certificate into a file.  But since it didn't make sense to have it always doing that, I've turned that off for the moment.  That will be coming back.  So what's available right now does all that.  But as I said, it does it as a simple text window since what I have was mostly meant as a tech development proof of concept.



What I'm going to do now, starting this evening after today's podcast, is turn this into one of GRC's standard little Windows GUI apps.  So I'll interpret what's going on to very clearly display the important bits and show the conclusion of what's happening to its user.  And then if you wish to view the full certificate, there'll be a button for that.  And if you want to save it to a file, there'll be a button for that.



So anyway, since this Platform Key mess is going to be a persistent and enduring concern, since there's no telling how long it's going to take to address these bad keys, if they are ever addressed, I felt as though this was exactly the sort of freeware app that GRC should offer.  It can be downloaded, as I said, right now.  It's just in its pre-release text window incarnation, but it works.  And I'm sure I'll be announcing the final app on next week's podcast, you know, in a week.  So okay.  So much for PKfail.  We'll briefly touch on it again next week.



Let's talk about Firefox's third-party cookie mess and GRC's Cookie Forensics test.  Two things are going on with Firefox and with what GRC's Cookie Forensics page shows.  Part of the problem is that Firefox is being a bit, dare I say, "foxy," and appears to not have updated its user interface after it integrated its so-called "Total Cookie Protection" into Firefox.  There are no UI controls for it in evidence anywhere.  There's a note about it in the UI in a link, but you don't turn it on or off or do anything.



So as many people have noted, currently the only way to make GRC's Cookie Forensics test happy, meaning to say, oh, yup, no third-party cookies at all, is to use Firefox's custom setting and really forcibly turn off third-party cookies, like we said last week.  All that's correct.  At the same time, just turning off third-party cookies on other browsers works.  Whereas it sort of doesn't seem to with Firefox.  So that's what brought this conversation up, the whole dialogue last week.



There are a couple of interacting things going on.  Most important is that I never designed GRC's testing with an awareness of the inter-site third-party cookie isolation that Mozilla has now built into Firefox under the banner of "Total Cookie Protection."  As I commented when the idea first surfaced, I think this is a really terrific idea.  It potentially changes the game by "stovepiping" cookie collections and aggregating them into their individual sites where they're being set.



As Mozilla described it, instead of browsers having one big cookie jar that's shared among all websites, which is the way it's already been and has always been, and of course that's the reason tracking works, is that the advertiser's cookie is in this big common cookie jar, which it's able to access no matter where the user goes.  Instead, Firefox creates individual isolated mini cookie jars, one for every first-party domain that a browser is visiting.



So an Ad-Tech company is welcome to place their third-party cookie at site "A."  But that cookie will be tagged with a site "A" context, and it will only be returned, again, when visiting site "A."  So when a user visits another site where the same Ad-Tech company is also using cookies, that Ad-Tech company will not receive the cookie it set over on site "A" so it will not know that this visitor is the same one who was over there viewing ads.  So this is a huge change, and it's a big win.



But I registered the independent domain "grctech.com" on New Year's Day of 2002 because I needed to have an unaffiliated third-party domain of my own available, you know, not a subdomain of GRC because cookies know about subdomains.  I needed an unaffiliated domain which I could build around a Cookie Forensics system.  And back then, there was no notion of the cookie site isolation that we have today with Firefox.  As a consequence, Firefox may well be isolating cookies by domain, but my 22-year-old test doesn't know about that.  It's just seeing that Firefox appears to be very reluctant to fully disable third-party cookies.  It can be forced to, but it doesn't really want to.  And the reason Firefox can do this is because it has the site isolation trick up its sleeve.



What we're going to need in order to verify that site isolation is working and real is an updated Cookie Forensics test that's aware of the possibility that while third-party cookies themselves might be enabled, they might not be shared with other first-party sites.  And I don't recall why this was on my mind 12 days ago, back on July 25th, because that was before last week's podcast.  But the question of the need to confirm the presence of inter-site cookie isolation must have occurred to me since Hover shows that I registered another independent domain, "grctech.dev," on that Thursday.  I needed a second unaffiliated domain to create a second first-party site to see whether it would be able to see the cookies that had been set by "grctech.com" when visitors were at "grc.com."



So, at some point in the not-too-distant future, I'll make some time, I'll dust off the cookie forensics site, figure out how it works because I knew 22 years ago and I haven't looked at it for a long time, and then create an updated facility for the specific characterization of browser cookie handling which incorporates an understanding of inter-site isolation since this seems like a good thing.  And clearly it's based on what just happened with Chrome and the EU or the UK, you know, third-party cookie handling is going to be as important today as it was back in 2002 when I first got a bug about this and decided to fix it.



Okay.  But there's more.  It turns out that simply blocking all cross-domain third-party cookies can break things.  A more nuanced approach is going to be needed, apparently, to create a real-world solution that does not cause more harm than good.  I want to share Mozilla's discussion of this since it helps to demonstrate how complex the web has become, and also how dependent upon the details of its operation other services have become.  And Leo, your timing is perfect because this would be a great time to take a break.  And then I want to share what Mozilla is explaining about the complexity of third-party tracking.



LEO:  Absolutely.  I was taking the old server out to the car.  I realized that I'm running...



STEVE:  As one does.



LEO:  As one does.  I was running the web server and three, four, five Minecraft instances here in the other.  And I realized, oh, I guess I'd better take that home.  I hope I have enough bandwidth for your Minecraft enjoyment, but Club TWiT members get to play in our little Minecraft sandboxes.  Now back to Steve and Security Now!.



STEVE:  Okay.  So if anyone wants a good example of the definition of a kludge, unfortunately we're about to encounter one.



LEO:  Uh-oh.



STEVE:  As I said, it turns out that simply blocking all cross-domain third-party cookies, like Mozilla's Total Cookie Protection does, it can break things.  So you need more nuance.  Unfortunately, there's no clean solution.  So get a load of this.  This is Mozilla's explanation, which is titled "Third-Party Trackers," and they're going to explain to us what they've had to do.



They wrote:  "Cookies are invisible pieces of data that a website can ask your browser to store on your device.  The next time you visit the same website, it can ask the browser to read that cookie. That's how a website can "remember" things, such as your preferences for that website and that you're logged on.



"Another use for cookies is to transfer information from one website to another.  For example, a sales website can store information about your purchase in cookies and redirect you to a payment or a review website.  From the website's point of view, the cookies created by the sales website are called third-party cookies.  So the point is they're demonstrating there's a non-tracking use case for third-party cookies."



They said:  "There are also several web libraries that developers use to add functionality to their websites.  These libraries can set cookies on your device, too.  If cookies are set by a library that's on a different domain from the website's domain, they are also third-party cookies.  Again, another use case.



They said:  "Popular libraries are used by numerous websites.  When you visit a website that uses a particular library, that library can set a cookie on your device.  If you later visit another website that uses the same library, that library can read the cookie that was set when you visited the previous website.  These third-party cookies, set and read by libraries from multiple websites, are called cross-site cookies."



So they say:  "There are two main reasons websites and libraries use cross-site cookies.  Cross-site tracking is by far the most common use of cross-site cookies. Trackers use cross-site cookies to collect information about the websites you visit and send them to other companies, often for advertising purposes.  When you feel like an advertisement is following you while you browse, this is the result of cross-site tracking.  If the same tracker is present on multiple sites, it can build a more complete profile about you over time.



"The other type of cross-site cookie is a functional cookie.  Some websites rely on these cookies in order to function properly.  For example, some websites may need access to cross-site cookies to let you use their service to sign into another website, like Facebook Login does this; or to process a payment for that website, Amazon Pay does that.



"Firefox's Enhanced Tracking Protection," they write, "blocks cookies from cross-site trackers and isolates cookies from all other third parties.  This helps prevent your browsing activity on one website from being visible to other websites.  Total Cookie Protection is now enabled by default. It is an advancement built into Enhanced Tracking Protection that works by maintaining a separate cookie jar for each website you visit."



They said:  "While cross-site cookies from trackers are blocked in Firefox by default, a site may signal to the browser that it needs to use them for important functionality."  What?  Okay.  "In this case, Firefox will allow a third-party..."



LEO:  Please, can I have those cookies?



STEVE:  I know, Leo.



LEO:  There's no functionality that third-party cookies could possibly implement that I want.



STEVE:  I know.  This starts to get really messy.  So they said:  "In this case, if a website says, oh, but sir, may I please have one, Firefox will allow a third-party website to use cross-site cookies" - get this - "five times."  What?  I know, Leo.



LEO:  You've got to use uBlock Origin.  You know?  Just block it all.  Oh, my god.



STEVE:  This is so bad.



LEO:  Oh, my god.



STEVE:  So they said:  "In this case, if a site asks, Firefox will allow a third-party website to use cross-site cookies five times.  Or" - I'm not making this up - "or up to 1% of the number of unique sites you visit in a session, whichever of the two is larger."



LEO:  Where did they get that from?



STEVE:  I know.  It gets worse:  "...without prompting you.  After that, Firefox will prompt you to block these cookies."  What?  "Without your consent, Firefox blocks these cookies from that point because a site requesting access that many times may be a tracker."  Okay, now...



LEO:  Maybe?  You think?



STEVE:  To interrupt this horror for a minute...



LEO:  Holy cow.



STEVE:  ...the first time I read that, I had to go back to make sure I'd read it correctly.  I thought, what?  This is my browser.  My browser.  You can't do this to my browser.



LEO:  You know where some of this comes from, I think, is, at least in the EU and the UK, advertisers and sites are complaining.  This is why Google says we had to stop, you know, implement that change.



STEVE:  Back off, yeah.



LEO:  And I think that Firefox is concerned about getting attacked in the EU by the government because advertisers are complaining.  And it's very disappointing.



STEVE:  It is.  Okay.  So, but Leo, it's going to still get worse.  Okay.  So but first let me just make clear, this means that Firefox now blocks cross-site cookies by default.  They are also providing websites with a mechanism by which to ask to please have cross-site cookies turned back on - but only just a little."



LEO:  I'm only a little pregnant.  Just a little.  A tiny, tiny bit.



STEVE:  It's just a flesh wound.



LEO:  Yeah.



STEVE:  Presumably only enough to accomplish some specific task that needs to communicate with cookies across sites.  So apparently they understand that if this was just a blanket request, then websites would eventually all claim to need it all the time; right?



LEO:  Sure, all the time, yeah.



STEVE:  They just add, you know, it's like the Do Not Track.  It'd be, oh...



LEO:  Who is asking for it, though?  Is it the third party?



STEVE:  Yes.  It's a third party.



LEO:  It's a third party.



STEVE:  Yes.



LEO:  So if you're going on a site, and you go to a Starbucks site and have a Facebook thumbs-up like button, Facebook, which owns that button, could say, oh, please leave these tracking cookies on because - why?



STEVE:  Leo, it gets even worse because Firefox makes exceptions based on the domain that's asking.



LEO:  Ugh.  Ugh.



STEVE:  Oh, I know.  It's so bad.  It's so bad.  Okay.  So...



LEO:  And really they're doing this so that they can still have that checkbox that says block third-party cookies.  Because they know people look at that, and they won't have read this whitepaper.  So they won't be - they'll think they're blocking third-party cookies when they're not.  And that bothers me even more.



STEVE:  Right.



LEO:  It's deceptive.



STEVE:  Right.  Well, that's what we discovered last week was that GRC's forensics was saying, what?  I've got a bunch of red bubbles here saying that it's not being blocked.



LEO:  It's deceptive.



STEVE:  Even though it says it is.  So presumably they, Mozilla, have done some testing of this, and they've decided that no website has a legitimate need for more than one or two, so they added some margin and set it at five before they surface a dialog to the user.  Now, what?  What is the user going to do about this?  Like, wait, what?  Do I say yes or no?  You know?  So, and then they also look at the number of unique sites visited during a session of a browser use, then take the larger of either five or 1% of the unique sites count during that session.  So that means that the breakeven point would be after visiting 500 different sites.  Right?  Then once you've visited another 100, then 1% of 600 sites total would be six requests that are allowed.



LEO:  Can't allow that.



STEVE:  So I suppose this means that if anyone is using their browser like crazy, the site they're visiting might need a bit more leeway.  But a bit more is all they're going to get.  You know, if all of this sounds like a horrendous mess to you, our listeners, then I would say you've been paying attention because this is a horrendous mess.  This sort of heuristic is not the way the Internet and the web were designed.



The Internet is as robust as it is because it doesn't have any sort of this nonsense.  You know, it was designed with clear, clean, and simple rules, which is, I would argue, why it's done as well as it has.  You know, but this allow sites to request an exception, but not too many times, you know, it's the very definition of a kludge.  I'm surprised they're not just tossing a coin.  Heads, you can use a cross-site cookie; tails, uh, sorry, you know, better luck next time.



Okay.  Anyway, they continue by writing - this is Mozilla.  "Third parties will only be able to prompt you if you interact with the website you are on."  Okay.  "For example, if you visit Dogs.com and select the payment field, Amazon Pay cross-site cookies may be allowed to facilitate that transaction.  After that, Firefox will ask if you want to keep allowing them."  Oh, and they say:  "You can view the Permissions panel to see if a website has been allowed or denied permission to use cookies, by clicking the permissions icon in the address bar."



So now there's going to be a new permissions icon in the address bar.  If you deny the request, the third party will not be able to use cross-site cookies during that session.  But if you refresh or reload the page, the third party may prompt you again.  From the Permissions panel for a site, you can click the X or revoke previously allowed access to cookies."



Wow.  So website visitors, who have no idea what's going on behind the scenes, are being expected to make on-the-fly allow/deny decisions about whether or not the third party appearing at any given site should be allowed to use cross-site cookies.  This is going to be interesting.



They said:  "Firefox automatically allows third-party websites to use cross-site cookies on the first five or so - or so - websites you visit.  For example, Amazon Pay would be able to use cookies on Old Navy, Blick, Dog.com, and a handful of other sites without asking you for permission."  But, oh, I guess that means if you encounter Amazon.pay a sixth time, during something, a session, then, whoa, wait a minute, do you want to allow this because you've been using it a lot.  Wow.



"If a third-party continues," they write, "if a third party continues to use cross-site cookies across multiple sites, this becomes a signal to Firefox that the third party might be a tracker.  At that point, the third party would have to prompt you to ask for permission to use cross-site cookies."



They said, and they actually used the word "heuristics":  "There are other rules," they said, "('heuristics') that will make Firefox temporarily grant access to cross-site cookies to certain websites.  These rules are designed to enable special use cases such as single sign-on services and usually require some special interaction such as a top-level redirect or a user interaction, making it difficult for trackers to exploit them."



Okay.  So, boy.  It is a tangled mess.  Presumably these "heuristics" that Mozilla is talking about for Firefox are things like Firefox knowing in advance the domain names of large single sign-on providers so they automatically get special permission and can use cross-domain cookies without limit because that's their legitimate business purpose.  So now that puts Mozilla in the position of deciding whose business model will and will not receive more permissive treatment under Firefox.  I mean, we're losing the browser that we have always loved.



LEO:  Thank god for Gorhill.  I don't know what we will do.  He'd better not die or anything.



STEVE:  Well, he did post recently that he's about to be in trouble with uBlock Origin, but over on Chrome.



LEO:  Yeah.  It won't work with Manifest v3.



STEVE:  That's correct.  So, you know, at least we do still have Gorhill here on Firefox.  And I'll be talking about that next week.  So what's happening is that the industry is now tying itself in knots because cross-site information sharing we want and cross-site information sharing we don't want are using the same mechanisms.



Mozilla's Third-Party Tracker explanation begs the question:  In light of this, how could Chrome's Privacy Sandbox designers have possibly imagined the elimination of all cross-site third-party cookies?  Mozilla's just finished telling us all the reasons that you can't eliminate them because they're having to poke a whole bunch of holes in this in order to keep things working.  So annoying though it may be to purists, Mozilla highlights some compelling use-cases which have arisen for needing cross-site information sharing in some form, sometimes.



Okay.  So the W3C finally weighs in.  Amid all this, the World Wide Web Consortium, the W3C, has finally weighed-in on third-party Cookies.  So just to remind everyone and update everyone on the W3C, Wikipedia's first sentence or two says:  "The World Wide Web Consortium (W3C) is the main international standards organization for the World Wide Web.  Founded in 1994 and led by Tim Berners-Lee" - and this is after he left CERN - "the consortium is made up of member organizations that maintain full-time staff working together on the development of standards for the World Wide Web.  As of March 5th, 2023, the W3C had 462 members.  The W3C also engages in education and outreach, develops software, and serves as an open forum for discussion about the web."  So these are the guys that are defining the APIs that our browsers support, and the standards that make the World Wide Web go.



"About a week and a half ago, on July 26th, the W3C's official statement was titled:  '"Third-Party Cookies Must Be Removed.'"  Okay, that's never been said before.  Third-party cookies must be removed.  The article's Abstract says:  "Third-party, aka cross-site cookies, are harmful to the web, and must be removed from the web platform.  This finding explains why they must be removed, and examines the challenges in removing them."  So a finding is what they're calling this thing that they published on July 26th.  It examines the challenges in removing them.



"We highlight some use cases that depend on third-party cookies and offer some examples of designed-for-purpose technologies that can replace them."  In other words, instead of you, like, overloading in an object-oriented sense, in like using cookies for a purpose they were never intended, the web is going to come up with designed for purpose technologies, replacement technologies, that don't track, don't allow tracking, cannot be abused, and will allow third-party cookies to be terminated."



So, they write:  "Specification authors are expected to ensure they do not undermine the benefits of removing third-party cookies when proposing new web platform technologies."  And their introduction is interesting because they call out Chrome. They write:  "We consider privacy to be a core design principle and differentiator for the web platform.  Many browsers have restricted third-party cookies."  They said:  "(See WebKit and Mozilla."  And they said:  "Unfortunately, not all browsers have followed suit."  And at that point they had a link to the article that the Privacy Sandbox Group had published four days before this which acknowledged that third-party cookies would not be removed from Chrome.



Okay.  So TAG is their abbreviation for their Technical Architecture Group, TAG.  So the piece continues:  "TAG calls for all browsers to drop support for third-party cookies, as this provides an opportunity to further improve the privacy preserving features of the web platform.  Removing third-party cookies from the web platform is not without complications.  There are use cases for third-party cookies that need to be preserved, and pitfalls we need to be careful to avoid in doing so.  This document sets out some aspects that specification editors and implementers should be aware of in order to make sure we ultimately leave the web better than we found it after third-party cookies are removed."



Okay.  So this all sounds wonderful.  This might be significant.  If nothing else, it likely provides some cover for any browsers that wish to move in this direction.  There's now an official position from the Internet's major web standards body unequivocally calling for the end of third-party cookies.  Their "Leaving the web better than we found it" section says:  "We support removing third-party cookies from the web platform, and we embrace the opportunity to improve the privacy features of the web.  When we review new technologies to replace third-party cookies, we need to ensure that the replacements do not recreate the same pitfalls to privacy.



"The TAG considers each new technology proposal both individually and as they fit together with the web platform as a whole.  The web must be cross-platform, so multi-implementer/multi-browser support and developer support for privacy-related specifications is essential if they're going to achieve the goal of increasing privacy on the web.  When we consider whether something makes the web platform better, we should be explicit about what that baseline for comparison is.  Is a proposal better for privacy when compared to usage of third-party cookies?  Or when compared with a web free from third-party cookies altogether?  What about when some user agents restrict third-party cookies, but others do not?



"We want to emphasize that as any replacement proposals progress, implementations should have a strong commitment toward, and reasonable time frame for, removing third-party cookies.  We are also wary of new mechanisms being introduced that could be abused together with cookies, fingerprinting surface, or other tools, for greater privacy invasion.  Given this context, we see an urgency to have a strict timeline for the removal of third-party cookies.  We are strongly in favor of innovations to build sustainable business models on the web platform, but an in-depth discussion of the various possibilities are outside the scope of this document.  From an architectural standpoint, web standards should avoid encoding particular business models" - which unfortunately Mozilla has just done - that are available to authors, publishers, and web content creators.



"In conclusion, when accommodating changes caused by the removal of third-party cookies, we should avoid introducing new technologies that, when deployed either individually or in combination, effectively preserve the status quo of harmful tracking and surveillance on the web."



Okay.  So I could certainly be reading too much into this.  I'm not nearly enough of an insider to be able to venture a guess about what this might actually mean for changes in the future.  But, boy, does it sound good.  They're using all the right words.  And I think what this is saying is that those organizations, those entities like Amazon Pay, that are aware they are dependent upon third-party cookies, need to get serious about working with the emerging standards.  And the W3C did mention a handful of them.  Among them was Chrome's Privacy Sandbox.  But it turns out there are others.  There is other work in progress, not from a single company, but from multiple standards bodies that will provide a solution.



So what we could see happening would be that, you know, Amazon Pay and Facebook Logon and so forth would switch to using a non-third-party cookie standard, thus protecting themselves from the end of third-party cookies, which the W3C is now calling for.  So this lays out a template for the shape of the future approaches that would have a chance to succeed.  You know, they said:  "We are strongly in favor of innovations to build sustainable business models on the web platform."  And of course we know what that means; right?  That means that sustainable business models, on one hand Amazon Pay and using the web and third-party authentication for logon, probably also means that, like Google, they understand that advertising powers much of the Internet, and that targeting ads makes those ads enough more effective that it can, as we've seen, double a site's revenue from advertising.  At the same time, I lost count of the number of times they used the word "privacy."  So it's clearly not their intent for anything like the status quo to remain.  But will anything change?



And it's definitely worth repeating what I noted last week, which is that there are significant enterprises employing many people whose entire purpose is to deliberately and surreptitiously violate the privacy of everyone who ventures out onto the Internet with their computer.  You know?



LEO:  It's true, yes.



STEVE:  That's exactly what it is; right?  And these days, who doesn't venture out onto the Internet with their computer?  And equally unfortunate is the fact that these enterprises have paying customers who have some use for the aggregated private data of individuals - which is to say, all of us.  So this W3C action of very clearly working toward putting an end to third-party cookies can be expected to see more opposition, much as it appears happened with Google in Europe.



LEO:  What is the - they said in there there are uses of third-party cookies that are legit.  I'm wracking my brain.



STEVE:  So apparently, and I didn't dig into it, but apparently Amazon Pay is using third-party cookies in order to link itself.



LEO:  Ah.  So this is the case, and you do this, too, it's not unusual for a site to have images from another server, to have data from multiple servers.  So those servers would look like third parties when you're on a first-party page.



STEVE:  By definition, they are third parties, yes.



LEO:  They are literally, yeah.



STEVE:  Yes.



LEO:  So that makes sense.  So that's probably some sort of related thing where, well, I need to set a cookie from the image server so that I don't send you that image again or whatever.  But it's not going to come from this page because I'm a first party.



STEVE:  Right.



LEO:  So I can see that, yeah.



STEVE:  Right.  There are non-tracking, currently legitimate applications.  Now, and so their point is that cookies have been used legitimately because they're there.  Right?  I mean, again, third-party cookies are being used to create these inter-domain connections, just because they're convenient.  They're there.  But it would be possible for a site like Amazon Pay to use a designed-for-purpose, you know, a non-third-party cookie solution.



LEO:  Yeah, that makes sense.



STEVE:  And so that's what the W3C is calling for is an alternative technology that everybody who currently is using third-party cookies for good will be able to switch to.  So then third-party cookies can be turned off.  And the problem is, lots of companies are using them for privacy-invading purposes, and they're going to fight back.



LEO:  Right.  And the EU's listening because they say, well...



STEVE:  Yes.



LEO:  These are journalistic enterprise support.



STEVE:  Legitimate business.  They say they're legitimate businesses, yeah.



LEO:  For advertising, yeah.



STEVE:  They have, you know, little babies they have to feed.  Okay.



LEO:  So really it's a conundrum.  It really is.  But I can see now, yeah, there are lots of servers, many servers you go to now who have multiple endpoints.  And those are all third-party endpoints except for that one main server.  So that makes sense.  I can see that.  That's those JavaScript things that everybody loads, things like that.



STEVE:  Right.



LEO:  Yeah.



STEVE:  Time to take a break, and then we're going to update on what has been - what has happened to CrowdStrike?



LEO:  What the heck?



STEVE:  As a consequence of that little, that little problem a couple weeks ago.



LEO:  I'm curious.  



STEVE:  Think anybody's upset, Leo?  Do you think anybody called their attorneys?



LEO:  We should preface that, as you did a couple of weeks ago when we talked about this, with the fact that up till then CrowdStrike was a very well-known, very well-respected security firm; that they did a lot of important research on the Internet.



STEVE:  Leo, believe it or not, one group of I don't know what it is going on, they're actually suing because their reputation was so good.  They're saying, "You misled us."



LEO:  Yes.



STEVE:  You know, you made all that money for us because you were just pulling the wool over our eyes.



LEO:  Yeah.  You were so good until you weren't.



STEVE:  Wow.



LEO:  That does happen.  All right, Steve.  Let's talk about the sad tale of CrowdStrike.



STEVE:  Well, predictably, yeah, predictably the lawsuits and class actions have started up in the wake of CrowdStrike's little bitty problem.  Did anybody notice that?  I think that some details of them would be of interest to our listeners.  The first is a shareholder lawsuit blaming CrowdStrike for the entirely predictable significant drop in the company's stock price.  Apparently, the Plymouth County Retirement Association of Plymouth, Massachusetts, now regrets having invested so heavily in CrowdStrike.



The coverage of this, after removing the redundant stuff that we already know, it says:  "Austin-based cybersecurity firm CrowdStrike is facing a class action lawsuit from shareholders who claim the company defrauded them by concealing how its inadequate software testing could cause a global computer outage, resulting in a big hit to the share price and overall market value.  According to the July 30th complaint filed in the United States District Court in the Western District of Texas, CrowdStrike's Chief Executive George Kurtz characterized CrowdStrike's Falcon software as 'validated, tested, and certified during a conference call on March 5th.'



"The plaintiffs say these statements were 'false and misleading' because CrowdStrike allegedly failed to properly test and update" - actually, they did update, and that was the problem - "allegedly failed to properly test and update its Falcon software before rolling it out to customers.  The complaint alleges CrowdStrike 'instituted deficient controls in its procedure for updating Falcon, and was not properly testing updates to Falcon before rolling them out to customers.'"  Okay, I think everybody would agree with that.  "CrowdStrike did not disclose that 'this inadequate software testing created a substantial risk that an update to Falcon could cause major outages for a significant number of the Company's customers.'



"The complaint alleges:  'Such outages could pose, and in fact ultimately created, substantial reputational harm and legal risk to CrowdStrike.  As a result of these materially false and misleading statements and omissions, CrowdStrike stock traded at artificially high prices during the Class Period."



Okay.  So they're upset over the terrific management, operation, and reputation of CrowdStrike before the event, that had presumably made them so much money.  And now they're suing because what was once overly inflated is apparently no longer.  The article notes that:  "Following the outage, the legal complaint says CrowdStrike's share price fell 32% percent over the next 12 days, wiping out $25 billion of market value.  As of July 31st, CrowdStrike shares are worth $231.96. They closed at $343.05 on the day before the outage.



"The lawsuit, led by the Plymouth County Retirement Association of Plymouth, Massachusetts, seeks unspecified damages for holders of CrowdStrike Class A shares between November 29th of  2023 and July 29, 2024.  The class action also alleges that Delta Air Lines' hiring of an attorney to represent them in seeking damages from the company, along with Kurtz's being called to testify before the U.S. Congress over the incident, caused CrowdStrike's share price to fall."  So they're also unhappy that other things that allegedly hurt CrowdStrike's reputation also hurt the stock price.  Okay.



The article says:  "CrowdStrike said, in a statement provided to media outlets:  'We believe this case lacks merit, and we'll vigorously defend the company.'  Speaking at the time of the outage, CrowdStrike Chief Executive George Kurtz said:  'We identified this very quickly and remediated the issue.'"  Period.  He added that its systems were constantly being updated to ward off "adversaries that are out there."



Okay.  So that's the first of the entirely predictable legal actions that are underway.  The second one surrounds the excessive degree of damage caused to Delta Airlines.  As we know, whatever it was that happened to Delta kept many of their flights on the ground far longer than any of their other competitors who, like most of the rest of the planet, removed the bad file, rebooted, and resumed operations.  But not Delta.  Something happened at Delta.



Just yesterday, on Monday, the publication CIO Dive's headline read "CrowdStrike rebukes Delta's negligence claims in a fiery letter."  And the subhead of their piece was "After the airline said it was considering legal action, CrowdStrike said Delta's contract capped the cybersecurity provider's liability to 'single-digit millions.'"  They weren't any more specific.  But sorry, folks, the $500 million you're claiming in damages, we're not going to get there with our contract.



Okay.  So the article says:  "CrowdStrike struck back forcibly against Delta Air Lines' claims of negligence and misconduct in a letter sent Sunday" - just last Sunday - "to the firm representing Delta Air Lines, signed by attorney Michael Carlinsky."  And I should mention, Leo, that he addressed the letter to David Boies.  So, you know, some high-powered legal talent there.  "So this is the latest in what has become a public dispute" - that is, between Delta and CrowdStrike - "following recovery from the global CrowdStrike outage, which was caused by a faulty software update," as we know, "pushed to Windows servers on the 19th of July.



"Delta was the hardest hit major airline carrier.  Its disruption lasted longer and reached further than what United Airlines, American Airlines, and others experienced.  As the airline grappled with the scale and length of the outage, it moved to shift some of the blame publicly against the cybersecurity provider.  Delta's CEO Ed Bastian told CNBC last week the airline was considering legal action, seeking compensation for the $500 million in costs that the airline had endured."  Yes, we're grounded, and it's hurting.  "He said:  'We're looking to make certain that we get compensated, however they decide to, for what they cost us,' Bastian said."



Okay.  So CrowdStrike pushed the recovery responsibility back on Delta, interestingly.  The airline declined CrowdStrike's help with systems recovery, according to the letter, which was shared with CIO Dive.



Okay, now, no one disputes CrowdStrike's ultimate culpability here.  Even they don't.  So what appears to be at issue is matters of degree.  I tracked down the text of CrowdStrike's Sunday letter to Delta.  So this is CrowdStrike saying:  "Dear David," as in David Boies, you know, superpower well-known attorney.  "I am writing on behalf of my client CrowdStrike, Inc. in response to your letter dated July 29th, 2024, in which Delta Air Lines, Inc. raises issues and threatens CrowdStrike with legal claims related to the July 19th, 2024 content configuration update impacting the Falcon sensor and the Windows Operating System (the 'Channel File 291 incident')."



He writes:  "CrowdStrike reiterates its apology to Delta, its employees, and its customers, and is empathetic to the circumstances they faced.  However, CrowdStrike is highly disappointed by Delta's suggestion that CrowdStrike acted inappropriately, and strongly rejects any allegation that it was grossly negligent or committed willful misconduct with respect to the Channel File 291 incident.  Your suggestion that CrowdStrike failed to do testing and validation is contradicted by the very information on which you rely from CrowdStrike's Preliminary Post Incident Review.



"CrowdStrike worked tirelessly to help its customers restore impacted systems and resume services to their customers.  Within hours of the incident, CrowdStrike reached out to Delta to offer assistance and ensure Delta was aware of an available remediation.  Additionally, CrowdStrike's CEO personally reached out to Delta's CEO to offer onsite assistance, but received no response.  CrowdStrike followed up with Delta on the offer for onsite support and was told that the onsite resources were not needed.  To this day, CrowdStrike continues to work closely and professionally with the Delta information security team.



"Delta's public threat of litigation distracts from this work and has contributed to a misleading narrative that CrowdStrike is responsible for Delta's IT decisions and response to the outage.  Should Delta pursue this path, Delta will have to explain to the public, its shareholders, and ultimately a jury why CrowdStrike took responsibility for its actions swiftly, transparently, and constructively, while Delta did not.



"Among other things, Delta will need to explain why Delta's competitors, facing similar challenges, all restored operations much faster; why Delta turned down free onsite help from CrowdStrike professionals who assisted many other customers to restore operations much more quickly than Delta; that any liability by CrowdStrike is contractually capped at an amount in the single-digit millions; every action, or failure to act, by Delta or its third-party service providers, related to the Channel File 291 incident; and the design and operational resiliency capabilities of Delta's IT infrastructure, including decisions by Delta with respect to system upgrades, and all other contributory factors that relate in any way to the damage Delta allegedly suffered."  Oh, and yes.



"In light of Delta's July 29 letter, CrowdStrike must also demand that Delta preserve all documents, records, and communications of any kind - including emails, text messages, and other communications - in the possession, custody, or control of Delta, its officers and directors and employees concerning, but not limited to, the items listed below.  As I am sure you can appreciate, while litigation would be unfortunate, CrowdStrike will respond aggressively, if forced to do so, in order to protect its shareholders, employees, and other stakeholders.  CrowdStrike's focus remains on its customers, including Delta.  CrowdStrike hopes Delta reconsiders its approach and agrees to work cooperatively with CrowdStrike going forward, as the two sides historically have done."



So, okay.  You want to blame us?  Let's figure out, Delta, why you stood alone as an outlier in the amount of damage that you are alleging that you took relative to all of your peer airlines.  Industry estimates are that the CrowdStrike outage to the various Fortune 500 airlines will be around a total of $860 million or so, and apparently that's on average about $143 million per airline.  But it appears that there is in fact a contractual upper limit that does cap CrowdStrike's liability.  And that's not in dispute.



The article then offers some interesting background about this, quoting a guy named Scott Bickley, the advisory practice lead at Info-Tech Research.  Scott explained, he said:  "The standard limitation of liability (which is interestingly known as the LOL clause, limitation of liability) for most SaaS (software as a service) agreements caps liability at the actual funds spent on the subscription over a set period of time, usually the previous twelve months."  That's what I said before, that often these things just say that you're entitled to getting your money back, essentially, is what they're saying.



"Many enterprises will negotiate a multiple of this amount," he says, "or a set capped amount."  Bickley said that CrowdStrike's liability is likely to match annual spend or a multiple of annual spend if the clause was negotiated.  He said:  "Many large enterprises surprisingly do not negotiate these terms and default to using the language in the vendors' agreements, which of course benefits the vendor."  He said:  "Delta is likely going to pursue damages outside of the LOL cap and may rely on other legal arguments to bring the claim to a third-party dispute mediation or litigation."



So this is exactly what's expected.  One last note that is, sadly, happening, individual travelers who were inconvenienced by this are also suing, even though travel delays and flight cancellations and rerouting is quite common.  Yesterday Reuters reported:  "CrowdStrike's legal troubles from last month's massive global computer outage deepened Monday" - that's just yesterday - "as the cybersecurity company was sued by air travelers themselves whose flights were delayed or canceled.



"In a proposed class action filed in the Austin, Texas federal court, three fliers blamed CrowdStrike's negligence in testing and deploying its software for the outage, which also disrupted banks, hospitals, and emergency lines around the world.  The plaintiffs said that as fliers scrambled to get to their destinations, many spent hundreds of dollars on lodging, meals and alternative travel, while others missed work or suffered health problems from having to sleep on the airport floor."  And  yes, we did see the carpet on the airport floor, and that's not where you want to sleep.



"They said CrowdStrike should pay compensatory and punitive damages to anyone whose flight was disrupted, after technology-related flight groundings for Southwest Airlines and other carriers back in 2023 made the outage 'entirely foreseeable.'"  So they're saying, okay, these things happen, so we want to be compensated.  And, you know, we know what the effect will be; right?  After last years' troubles, I would be surprised to learn that the fine print in all passenger agreements doesn't already include a blanket liability waiver for any failure to fly caused by any foreseeable or unforeseeable events.  And if it already doesn't, it certainly will in the future.



You know, this is just the way the world is now.  When you sign forms for a relatively minor procedure in a hospital, the fine print explains that "We're really going to do the best we can to keep you alive; but, you know, stuff happens.  So if something happens, we'll be really sorry.  Sign here, and good luck."  You know, it's always a bit unnerving, but what are you going to do?



So anyway, just as I was finishing up this podcast I received email from a listener, Rob Woodruff - and actually since then a bunch more - informing me that as a CrowdStrike customer he had just received email from CrowdStrike's CEO, and that their much anticipated "Root Cause Analysis" had finally been completed.  I had no time to dig into it for today, and I do not plan to spend much time on it next week since we've already given this lots of time and coverage.  But if we learn anything new, I will briefly share what we learned from it.



And Leo, let's share a break, our last break.



LEO:  Yes.



STEVE:  Then I'm going to talk a little bit about GRC's email and how that's going, and we're going to take a look at how DigiCert handled a certificate revocation.  I guess it's an emergency, but...



LEO:  We've talked about this before and how hard it is to do.  You know, it's not...



STEVE:  Well, Entrust doesn't do it.



LEO:  Yeah.



STEVE:  You know?  It's so hard that Entrust just says, uh, no.



LEO:  Nah, can't do it.  We thought about it.  We decided not to.  Now back to Mr. Steve "Tiberius" Gibson and "How Revoking!"  Hmm.



STEVE:  Well, first I just wanted to mention that today's podcast filled up before I could share any of the terrific feedback that I've been receiving, in great volume, from our listeners.  The incoming listener feedback system has been an utter success, and I've been drowning in thoughts, notes, pointers to news, and other great content.  I just checked my "securitynow" inbox, and we're currently at 1,512 pieces of email received.  I guess it's, like, been about four weeks now.



I'm mentioning this because I'm torn by my inability to reply to this much incoming email.  Initially, I was trying to.  But then I looked at how much time was going into creating replies and saying even not much more than "Thanks so much for sending that."  So I need to explain that everyone should know that even in the absence of any reply from me, if the email did not bounce back to you, I have it.  I've read it, and I'm sure I appreciated it, really.  When I feel that I have to reply, I tend not to read them since my seeing the feedback creates the feeling of an obligation to reply that I'm just not able to service.  That's the way I am.



So I want to sincerely, really sincerely thank everyone here collectively who has written and who will write.  I really value this feedback, and I am seeing what you have sent. You'll probably often sense it in the effect that you have on the podcast afterwards.  Bits of obscure news that I may have missed if you hadn't told me about it.  So please no one ever imagine that because you don't receive a reply, that means I never saw what you took the time to send.  If you took the time to send it, I will have taken, at least taken the time to read it, even if I was unable to say so explicitly.  So really, thank you.



One last point is that I've received a number of complaints from listeners who were forced to write to Sue or Greg at our Sales or Support email because they were unable to find the special "securitynow" email address anywhere on the site.  The reason for that is it's not meant to be public, and it's not meant for GRC's non-podcast visitors.  It's just for us, and I want to keep it between us.  And the email address should not be too difficult to remember.  It's "securitynow@grc.com."



And to make it even easier, I've set the "reply-to" address of every one of our weekly podcast mailings to that "securitynow@grc.com" email address.  So anyone who receives the weekly email can simply reply to any email they receive, and many of our listeners just do that.  So anyway, again, I've been feeling like the mail's piling up.  I just wanted to tell everybody how much I appreciate it.  The system's working.  I am seeing everything.  I've been feeling guilty that I can't thank everybody.  So here's a collective thank you.



Okay.  Now, revocation.  Just as last week's podcast was happening, my favorite certificate authority, DigiCert, announced their discovery of a mistake their systems had made during the process of Domain Control Validation, or DCV.  I talked about this last week with regard to Entrust, noting that SSL.com, the Certificate Authority from whom Entrust plans to purchase interim certificates on behalf of their customers, will still need to have their own customers, Entrust's customers, prove their control over their pending certificate's domain name directly to SSL.com.  And I noted at the time that there are various ways to get that done.  The weakest of these is to use email received by and sent to the domain in question.  Better ways involve adding DNS records to the domain, which the Certificate Authority can then remotely pull and verify.



As we know, on the one hand we have Entrust, who has for years effectively refused to revoke certificates which had been shown to be mistakenly issued, out-of-compliance certificates mistakenly issued to their customers.  And as we'll see today, on the other hand, we have DigiCert, who takes the CA/Browser Forum requirements to heart, and immediately jumped into action when a truly insignificant mistake was discovered.  But as this shows, a rule is a rule.  Many of our listeners understood this difference and sent notes to me with variations on the subject, "This is the way it's supposed to be done."  You know, congratulating DigiCert, essentially.  Consequently we have, you know, "How Revoking!" for today's podcast title.



The Hacker News explained the whole event, complete with updates as of Sunday, and there was one.  Their headline was "DigiCert to Revoke 83,000+ SSL Certificates Due to Domain Validation Oversight."  But just wait till you hear what it was.  I've mixed their reporting with my own clarifications and some small additions.



So here's what we have:  "Certificate authority DigiCert has warned that it will be revoking a subset of SSL/TLS certificates within 24 hours, due to an oversight in how it verified if a digital certificate is issued to the rightful owner of a domain."  So they wrote:  "The company said it will be taking the step of revoking certificates that do not have proper Domain Control Validation (DCV)."



DigiCert said:  "Before issuing a certificate to a customer, DigiCert validates the customer's control or ownership over the domain name for which they are requesting a certificate using one of several methods approved by the CA/Browser Forum.  One of the ways this is done hinges on the customer setting up a DNS CNAME record containing a random value provided to them by DigiCert."  And this is a 32-character, right, it's a massive gibberish random number, random value.  "DigiCert then performs a DNS lookup for the domain in question to make sure that the random values are the same."  So they tell the customer, put this in.  Here.  Here's some gibberish as a DNS CNAME record.  Add this to your DNS.  We'll query your DNS once you say you have.  Only you who control that domain could have done that.  So we're good to go.



"The provided CNAME record is for a subdomain of the user's domain, and the random value from DigiCert is prefixed with an underscore character" - so it begins with underscore - "so as to prevent a possible collision with an actual randomly named subdomain that might use the same random value.  Note that domain names are not allowed to begin with an underscore, thus the collision prevention."  In other words, DigiCert's, or the CA/Browser Forum specs say thou shalt begin a name with an underscore because it is an illegal domain name, but it's good for this purpose.



So what DigiCert found was that it had failed to include the underscore prefix before the random value used in some CNAME-based validation cases.  Okay.  So whether or not that matters is really an edge case.  There is an instance where it can matter, which I'll explain in a second.  But it is a minor problem.  But this demonstrates that in the certificate authority space a rule is a rule, and we certainly beat up on Entrust over their flagrant violation of the rules.  This wasn't flagrant by any similar means, but okay.



Okay.  So the Hacker News continues:  "DigiCert's mistake has its roots in a series of changes DigiCert enacted starting in 2019 to revamp their underlying architecture, as part of which the code to add an underscore prefix was removed and subsequently 'added to some of the paths in the update system,' but not to one path that neither added it nor automatically checked if the random value had a pre-appended underscore."  Apparently there was some instance where the user was expected to do that, but they didn't verify that the user had.



"So DigiCert said:  'The omission of an automatic underscore prefix was not caught during the cross-functional team reviews that occurred before deployment of the updated system.  While we had regression testing in place, those tests failed to alert us" - boy, this sounds a little familiar - "to the change in functionality because the regression tests were scoped to workflows and functionality instead of the content/structure of the random value.  Unfortunately, no reviews were done to compare the legacy random value implementations with the random value implementations in the new system in every scenario.  Had we conducted those evaluations, we would have learned earlier that the system was not automatically adding the underscore prefix to the random value where needed.'"



Okay.  So the Hacker News said:  "Subsequently, on June 11th of this year, DigiCert said it revamped the random value generation process, eliminating the manual addition of the underscore prefix within the confines of a user-experience enhancement project, but acknowledged it again failed to 'compare this UX change'" - you know, the user experience change - "'against the underscore flow in the legacy system.'



"The company said it didn't discover the non-compliance issue until 'several weeks ago,' when an unnamed customer reached out regarding the random values" - in other words, somebody noticed, you know, one of their customers out in the field - "used in validation, prompting a deeper review."  Yeah, I'll bet.  "DigiCert noted that the incident impacts approximately 0.4% of the applicable domain validations which, according to an update on the related Bugzilla report, affects 83,267 certificates issued across 6,807 customers.



"Notified customers are recommended to replace their certificates as soon as possible by signing into their DigiCert accounts, generating a Certificate Signing Request, and reissuing them after passing Domain Control Validation.  The development prompted CISA to publish an alert - interestingly, not over any danger that this incredibly minor and inconsequential mistake may have had, but stating that 'revocation of these certificates themselves may cause temporary disruptions to websites, services, and applications relying on these certificates for secure communication.'



"In a later update, DigiCert said:  'DigiCert continues to actively engage with customers impacted by this incident, and many of them have been able to replace their certificates.  Some customers have applied for a delayed revocation due to exceptional circumstances'" - like they're on the International Space Station or something, I don't know - "'and we are working with them on their individual situations.  We're no longer accepting any applications for delayed revocation.'"



And so Hacker News finishes:  "These include customers who are operating critical infrastructure, who it said 'are not in a position to have all their certificates reissued and deployed in time without critical service interruptions.'  And DigiCert further noted that all impacted certificates'" - yes, all 83,267 of them - "'regardless of circumstances, had been revoked as of August 3rd, 2024, at 7:30 p.m. UTC.'"



So here we have an example of a company that's doing it right.  Even in the face of a quite significant pain being caused to their own customers by the sudden and essentially emergency requirement to revoke and then replace, well, hopefully replace before they're revoked, their certificates.  And what's more bracing is that there was nothing whatsoever wrong with those certificates.  You know, they weren't formed incorrectly.  They didn't have bad fields or anything.  It was the fact that the domain validation missed something which was meant to prevent a collision with real certificate names.



So this lack of a leading underscore in no way allowed anyone to nefariously obtain a certificate fraudulently.  Now, I should say I wrote that, and since I wrote it, one of our listeners who is on the emailing list got the show notes and read this, said, "Uh, Steve, actually there is an instance where there is a collision with a non-underscore name."  So for there to be a collision, you would need to have a situation where somebody was creating a CNAME record under a domain they don't control.  And it turns out that DynDNS works this way.  Right?  Dynamic DNS allows people - you normally have, like, you're able to choose from a number of top-level domain names, and then you choose your own subdomain under that domain name.



Well, if that subdomain was deliberately set up to collide with a CNAME record for the same domain name - well, okay.  So what this means is that a user of DynDNS could deliberately create a Dynamic DNS subdomain matching what DigiCert had given them, lacking an underscore, that would then allow them to get a certificate for that DynDNS domain.  Now, that's not good news.  And in fact it turns out that the DynDNS people are deliberately preventing the use of an underscore, whereas the certificate authorities are going to be deliberately requiring the use.  In other words, this is all, once again, kind of a big mess.



Anyway, the point is, DigiCert jumped on this, fixed the problem, sent the painful email to all, what did I say it was, 68,000, no, 6,807 customers, immediately actually revoked the certificates, got it done in some cases within a day, and where customers needed longer within five, which is the requirement in the CA/Browser Forums, got it done, you know, took responsibility, took the hit, informed their customers, and did it immediately.  So, yes, they did it right.



I should note that, since this issue with the DynDNS possibility occurring, history has been checked.  This never happened.  So there was never any abuse of it.  And now the DynDNS people will be disallowing the use of a leading underscore, whereas all of the use of CNAME records will require one, and this will keep those two uses from colliding.



Finally, I'm out of time today.  But I want to note that, as many of this podcast's longtime listeners know, and as Leo will remember well, the topic of web server certificate revocation has been another one of those long-term hobby horses of mine, much like third-party cookies, because the entire web browser revocation system is a total and utter joke.  It is completely broken, it has always been broken, and it has never worked.  Despite some half-hearted attempts from time to time to fix it, it is broken, and it never got fixed.  Web browsers do not know when the certificates they're receiving from web servers have been revoked.



Now, if we have some time next week we're going to update ourselves on this space, on this issue, because the truth is there may have been some use cases where certificates were being used where something actually would have known if they'd been revoked, and that would have caused a problem.  But mostly no.  Not a single browser.



In the meantime, try aiming your browser, whatever browser you have, at revoked.grc.com.  I brought that interesting and revealing test back to life last week in anticipation of this topic.  So revoked.grc.com.  Go there with your browser.  If it tells you, sorry, this site is untrusted, well, that's amazing.  That used to happen in some cases with Firefox if you forced it to do online certificate revocation checking and told it not to fail open and a bunch of other things.  The revoked.grc.com site  provides the browser with a certificate I revoked on purpose last week.  No browser that I have tested works right.  Even Safari, that used to, no longer does.



LEO:  [Indiscernible] doesn't either.



STEVE:  So if you see that page, sorry, folks, you shouldn't.



LEO:  What should I be seeing?  Like you can't go here?



STEVE:  A big warning.  A big, scary...



LEO:  Should I see that?  Is that what I should see?  There you go.



STEVE:  No.  Whoops.



LEO:  That's - I shouldn't be seeing that.



STEVE:  No.  That means that your browser is showing you a certificate that has been revoked.



LEO:  It's based on Chromium.  Let me try Safari.



STEVE:  Safari used to work.  On iOS, that was the one that I remember, like, saying wow, that's pretty good.  I tried it last week after I brought the site back to life and issued a new - this certificate is only a week old.  But I revoked it immediately after DigiCert made it for me.



LEO:  Oh, there you go, Safari.  Nice job.  It says, if you can see this, and apparently you can - show this one.  That's Safari.  Let me see Firefox.  This is terrible.  This is terrible, Steve.



STEVE:  I know, Leo.



LEO:  This is terrible.



STEVE:  It doesn't actually - certificate revocation does not work.



LEO:  Apparently, because it crashed Safari, as well.  Let's see here.  Not now.  Revoked.grc.com.  Oops, I said erevoked.  That failed.  Oh, no.  You've got to show these.  Oh, no.  They're all the same.  Everybody.  That's amazing. 



STEVE:  Yup.



LEO:  Everybody.



STEVE:  And if you go to ssllabs.com and have it check the server, revoked.grc.com, because SSL Labs, that's Ivan Ristic's really terrific site.  Ssllabs.com.  And then tell it to check revoked.grc.com.  You will see what it thinks because it actually checks to see what's going on.



LEO:  You're supposed to see an error; right?



STEVE:  No.  It's actually doing a deep test of, like...



LEO:  Testing all the different TLS systems?



STEVE:  It tests, yeah, different protocols.  It checks the certificate chain.  It checks which protocols you accept.  And I think if you scroll down you'll see some red there.



LEO:  Oh, yeah.  Insecure.  Revoked.  Do not trust.  Do not use this in your code.



STEVE:  And every one of our browsers ignores that.  They do not know that certificates have been revoked.



LEO:  It's broken.



STEVE:  It's always been broken.



LEO:  Broken.



STEVE:  You remember when I went off on a binge about this years ago.



LEO:  Yeah.  But also remember that Google said, yeah, we're not even going to try to support this because it doesn't work.



STEVE:  Actually, it embarrassed them, and so they added my certificate as a special case.  



LEO:  Oh.  Oh.  That's not the way to do it.



STEVE:  No.  I just changed the certificate, and it came back, and then they didn't bother doing it again because they knew.



LEO:  That's hysterical.  So they wrote an exception just for you.



STEVE:  Yes, they did.



LEO:  Ugh.  Shame, shame, shame.  So what makes this revoked?  Just that you - it's not that it's invalid, it's just that you specifically said to DigiCert "I revoke this."



STEVE:  Well, so like for example, if the certificate got loose and malware was using it, then you would want to revoke it so that malware wasn't able to use it to impersonate your site.



LEO:  Can't, apparently.



STEVE:  Or DigiCert just revoked, you know, for what good it did, they just revoked 83 thousand, what was it...



LEO:  8,300.



STEVE:  No, it's 6,800 customers 83,267 certificates.  And it didn't matter.



LEO:  Didn't matter.



STEVE:  They didn't have to revoke them because...



LEO:  Because the browser doesn't know.



STEVE:  Browser doesn't care.  It doesn't know.



LEO:  Unbelievable.



STEVE:  Yup, doesn't work.



LEO:  Right now it's testing Bleichenbacher.  So we're still testing.  What Bleichenbacher is, I'm glad you tested for Bleichenbacher.  I don't - this is SL - you're right.  This SSL Labs is pretty thorough.



STEVE:  Yeah, it's really terrific.



LEO:  Yeah, yeah.  I mean, it's taking a long time to go through this.  Wow.



STEVE:  Yeah, because it turns out it's going back and forth handshakes with the server.



LEO:  Right.



STEVE:  Those are all exploits against TLS that it's testing.



LEO:  Right.  Very nice.  But the main test is, is this certificate revoked?  Yes.  It is.



STEVE:  It is revoked.  And the browser doesn't know.



LEO:  Unbelievable.  Wow.  That is really - so how long has this page been up?  I know this is a new cert, but you've had this page before; right?



STEVE:  Oh, yeah, yeah.  We did multiple podcasts about it back in the day.



LEO:  I remember, yeah.



STEVE:  And so that page will show that I just edited it because I did in order to bring it back to life.



LEO:  Right.  If you're reading this, good luck.



STEVE:  You may not be where you think you are.



LEO:  Good luck.



STEVE:  What that means is you may not be where you think you are.



LEO:  Yeah, yeah.  Because the certificate's meaningless.



STEVE:  Yup.



LEO:  Wow.  What a world.  See, what people count on is that you don't exist, that people don't listen to this show, that they don't know this stuff, and they're just kind of blithely going along, well, yeah, we have certificate revocation.  Of course we do.  It's in the spec.  We must have it.  Wow.  No, we don't.



STEVE:  No.



LEO:  This is why you listen to the show.  This is why Steve is a national treasure.  This is why you should join Club TWiT to support Steve and his work.  I don't know what else to say.  You could buy ads on the show, too.  GRC.com, that's his website, the Gibson Research Corporation.  If you want to be on the mailing list or send an email, same page, GRC.com/email.  He'll validate your address.  Then you can or cannot subscribe to the mailing list, which you probably want to do.  But you can also now email him because your address is good, verified.



STEVE:  Yeah, we have a really neat one-click unsubscribe.  What happens is I'm also getting subscribers, just random GRC visitors.  And they go, oh, yeah, I want to get email from GRC.  And so they subscribe to all three.  Then on a Tuesday morning, when they actually get the email, they look at it and go, I don't want this.  And so there is a single-click, an instant unsubscribe.  And so I think I got, like, six unsubscribes after doing the emailing because these are people who had joined the list during the past week and thought, no, this is dumb.  I just want to get, you know, I want to find out, well, actually I'll do my first mailing as soon as the IsBootSecure? app is ready.



LEO:  We should, by the way, there are a number of people saying, oh, my god, this means the whole certificate thing is bogus.  No.  Certificates still are valuable for encrypting your traffic between a site, to verify the site is who they say they are.  It's just it's that this revocation thing does not work.  So you can't revoke a site that has - or a cert that has leaked out or is somehow malicious.  You can't take it back.  But I guess it does mean you shouldn't necessarily trust the certificate on any site.



STEVE:  That's the whole point.  That's why we have revocation.  Where we're headed is, and Google has mentioned this before because they're the brokest of all.  The Chrome has never even, I mean it was the pretense of what Chrome was saying that upset me so much back when we were talking about this because they said, oh, we have CRL set and, you know, we're managing this ourselves.  I said, no, you aren't.  And I created this to demonstrate it.  So where they're headed is 90-day certs like Let's Encrypt uses.



LEO:  So they just expire.



STEVE:  Exactly.  You can't revoke them, but you don't have to wait very long for them to die a natural death.



LEO:  But that gives a cert as much as 90 days to be wrong.



STEVE:  Bogus, yes.  But it's better than nothing, which is what we have now.  And unfortunately it does mean that all certificate management has to be automated.  You know, right now they...



LEO:  Oh, because of the 90 days, yeah, yeah, yeah.



STEVE:  It used to be five years, then three years, then - now we're one year and one month, you know, 13 months.  And so eventually we will probably get down to 90 days.  And no one's going to want to do that by hand.  They're just going to want to use the ACME protocol to do that.



LEO:  Right.  If you want to share that last little bit about certs with other people, you can go to YouTube.com/securitynow and just clip it.  That's a great way to do that.  Steve has copies of the audio of the show, 64Kb audio as well as 16Kb audio, which is of course much smaller, for the bandwidth impaired.  He also has transcripts, if you like to read along.  And he has the full show notes, which is what I show from time to time on the screen.  All of that's at GRC.com.  While you're there, pick up a copy of SpinRite, the world's finest mass storage performance enhancer, recovery utility, and maintenance utility.  Did I get it all?  



STEVE:  That's good, yeah.



LEO:  6.1's the current version.  You can get it right now at  GRC.com.  And it's his bread and butter, Steve's bread and butter.  So if you don't have a copy, everybody who has storage of any kind should have it.  Steve's got a lot of free stuff there, like ValiDrive and DCOMbobulator.  I don't know anybody who really needs DCOMbobulator anymore, but...



STEVE:  No.



LEO:  No.  You don't get a lot of downloads for that anymore.  ShieldsUP!, though, every time I get a new router I test it with ShieldsUP!.  That's a must.  And on and on and on.  He's really the king of freebies, including this new one.  What do you call it again?



STEVE:  IsBootSecure?



LEO:  IsBootSecure?  IsBootSecure?  It sounds like Boris Badenov version of utility.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#987

DATE:		August 13, 2024

TITLE:		Rethinking Revocation

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-987.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  A million domains are vulnerable to the "Sitting Duck" attack.  What is it?  Is it new?  Why does it happen?  And who needs to worry about it?  A CVSS 9.8 (serious) remote code execution vulnerability has been discovered in Windows' RDL (Remote Desktop Licensing) service.  Patch it before the bad guys use it!  All of AMD's chips have a critical (but patchable) microcode bug that allows boot-time security to be compromised.  Now what?  Microsoft apparently decides not to fix a simple Windows bug that allows anyone to easily crash Windows with a Blue Screen of Death anytime they wish.  You sure don't want that in your Windows startup folder!  GRC's IsBootSecure freeware is updated and very nearly finished.  Believe it or not, the entire certificate revocation system that the industry has just spent the past 10 years getting working is about to be scrapped in favor of what never worked before.  Go figure.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  I am here in my new attic studio.  But we have lots to talk about, including a new attack called Sitting Duck.  Are you a sitting duck?  Why all AMD, almost all AMD chips have a critical microcode bug that is about as bad as you can get.  Microsoft decides not to fix a simple Windows bug that can cause a Blue Screen of Death or worse.  And then we'll talk about certificate revocation.  Turns out it's a hard computer science problem.  And for some reason Let's Encrypt has decided to do it a different way.  Stay tuned.  Security Now! is coming up next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 987, recorded August 13th, 2024:  Rethinking Revocation.



It's time for Security Now!, the show where we cover your security, your privacy, your safety, your online habitation at all times with this man right here, Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.  Great to be with you again for, now, wait for it, the last episode of our 19th year.



LEO:  Holy camoly.  So you're saying as of next week...



STEVE:  Which will be the 20th, we will be here.



LEO:  ...we'll be in our 20th year.



STEVE:  Our birthday will have been the previous day.  Monday, August 19th of 2005 was...



LEO:  A day that will live in infamy.



STEVE:  Episode number one, Honey Monkeys.  Or was that number two?



LEO:  Wow.  



STEVE:  And, oh, I think that was all of 15-minute podcast.  Oh, yes.  Fortunately we don't have...



LEO:  They've gotten longer ever since, unfortunately.



STEVE:  We don't have four ads in a 15-minute podcast, or I'd be like, uh, Leo, wait, wait, let me get a word in.  No.



LEO:  If you're interested, this is the episode, it's still online, it's 18 minutes and 10 seconds, and it's Security Now! Episode 1, August 18th, it says.



STEVE:  Oh, wait a minute.  Okay, two days, yes, two days ahead we will have been...



LEO:  Look at that.  Do you want to hear how young we sounded back in the day?  First of all, we had the worst music.  Can you hear that?  I can hear it, but I don't - oh, it's coming out of the wrong hole.  Sorry about that.



STEVE:  That's what happens when you get older, Leo.  That's right.



LEO:  How many times have I heard that?  Well, anyway, we'll figure that out.  Actually, we'll leave it as an exercise for the viewer.



STEVE:  Perfect.  It is there, if anyone's really dying to know what it was like 20 years ago, or 19.  Wow.



LEO:  It was us kind of staring into the headlights, actually.



STEVE:  Yeah, what are we going to do now?  What is this, a what cast?



LEO:  What the hell is this? 



STEVE:  When you first said this, "How would you like to do a weekly podcast?"  I said, "A what cast?"



LEO:  Well, if you want to go directly there, TWiT.tv/sn1 is for now and forever the first Security Now!.



STEVE:  Yes.  We'll see how long you leave this online, Leo.  That remains to be seen.



LEO:  Oh, you mean like after we retire and all that?



STEVE:  Yeah, exactly, yeah.



LEO:  It is an interesting question that people talk about all the time is like, what happens to your digital life when you pass away?  I don't know.



STEVE:  Wikipedia pages, they stay around.  They say, "Yeah, we remember him."



LEO:  Yeah.  And, you know, a lot of what we do is on Archive.org.  So maybe those live on in Archive.org.



STEVE:  Well, today, Episode, and I like the digits, 987.  We're never going to get to 9876.  I think that's asking too much.  But 987, that we got.  And no one is worried about 999 because we know that we're going to - I will have to spend a little time fixing my software to deal with four digits.  That was always true.  But, you know, I've got time.  I'll make time.



Okay.  So a bunch of stuff, fun stuff to talk about.  It turns out that a million domains, it's estimated, are vulnerable to something known as the "Sitting Duck" attack.  What is it?



LEO:  That doesn't sound good.



STEVE:  It is not good.  You do not want to be a sitting duck.



LEO:  Oh, boy.



STEVE:  Is it new?  Why does it happen?  And who needs to worry about it?  Also where the name came from is kind of fun.  We'll get to that.  Also we've got, believe it or not, another 9.8 CVSS remote code execution vulnerability discovered in Windows Remote Desktop Licensing service.  Unfortunately, some 170,000 of those are exposed publicly.  The good news is it was patched last month, that is, last month's Patch Tuesday.  Today is August's Patch Tuesday.  So if you happen to be a corporation, an enterprise of any kind who's been delaying updates, and you have Remote Desktop Licensing service publicly exposed, at least patch that.  You know, you can go get the incremental patches from Microsoft if you want.  That one, either turn off the service or patch it.  Anyway, we'll talk more about that.  I realize I forgot that I was doing a summary here.



LEO:  Yes, this is just the short part, yeah.



STEVE:  Yes.  Also, all of AMD's chips have a critical, but probably patchable, if they choose to do it, and apparently they're saying, eh, we're not really sure we're going to bother, microcode bug that allows boot-time security to be compromised.  Okay.  Also, Microsoft apparently decides NOT to fix a simple Windows bug which will allow anyone to easily crash Windows with a Blue Screen of Death anytime they want.  Anyway, you don't want that in your startup folder.  GRC's IsBootSecure freeware is updated.  It's now a GUI, a regular Windows app, almost finished.  I'll mention that briefly.  And believe it or not, today's podcast 987 is titled "Rethinking Revocation"...



LEO:  Oh, no.



STEVE:  ...because the entire certificate revocation system that the industry has just spent the past 10 years getting to work, is about to be scrapped in favor of what never worked before.  Go figure.



LEO:  Well, at least it never worked before.



STEVE:  Yeah, I think maybe we're just never happy with what we have, and it's like, well, okay, even though that didn't work before, maybe we can make it work now because what we have today has problems that we don't seem able to fix.  Anyway, we're going to talk about that because everything's about to change.  And of course - once again.  And we've got a great Picture of the Week which kind of makes you wonder.  So another great podcast for our listeners.



LEO:  It's going to be a good one, kids.  Stay tuned.  Security Now!.  I don't think there'll be much difference to this show because I'm in the attic, but Steve's in the same exact place.  The only difference is we both have blinking lights now over our shoulders.  Now let's get back to Steve and our Picture of the Week.  I'm looking at it, and I don't - oh.  Oh.



STEVE:  So, you know, like, the welcome mat that you have in front of your front door?



LEO:  Yeah, yeah.



STEVE:  It says "Welcome" on it; right?



LEO:  Yeah.



STEVE:  But to do that, you've got to be able to print the word.  Now, what we have here is a - looks like a front door mat.  And I've seen these before.  You might have them in like an area that gets a lot of snow, where you'd like to be able to scrape your feet off.  So this mat is a grid of holes which, you know, could be useful for scraping stuff off the bottom of your shoes.



LEO:  Sure.



STEVE:  Now, the problem, however, for this particular instance is that someone thought that, you know, they'd hide a key under the mat.



LEO:  No.



STEVE:  But, you know, the operative word in that phrase "hiding the key under the mat" is "hiding."  And this mat, which is all holes, is a really bad hiding place for a key.



LEO:  Yes.  It's pretty obvious there's a key there, yeah.



STEVE:  It's really obvious, yeah.



LEO:  That's great.



STEVE:  So anyway.



LEO:  I hid the key under the mat.  Well, I didn't hide it, but it is under the mat, yeah.



STEVE:  Another one of those weird "What were they thinking?" pictures that we entertain ourselves with every week here on this crazy podcast.



Okay.  Every few years a security researcher rediscovers that many sites' DNS services are not properly configured and authenticated.  To no one's surprise, until it happens to them, this leads to those misconfigured domains being taken over, meaning that the IP addresses being resolved from a domain name are pointed to malicious servers elsewhere.  People going to those sites are often none the wiser.  The domain name is right, and it looks like it's the right site.  Little do they know that they're talking actually to a server in Russia.



So this generally doesn't happen to major sites because that problem would be seen and resolved almost immediately.  But many large organizations register every typo variation of their primary domain name that they can think of in order to keep the so-called "typo squatters" from registering those almost-typed-correctly domains and thus catching people who mistype the actual domain name.  But that's just an example of where you would have, like, lots of potential domains where, if it was being hacked, if something had been broken in the DNS, it wouldn't immediately come to any attention, but you could still get up to some mischief with it.



So the most recent round - as I said, every few years someone rediscovers this.  The most recent round of this DNS hijacking surfaced recently when two security firms, Eclypsium and Infoblox, co-published their discovery.  Now, as I said, though, it really should be considered a rediscovery - which is not to take anything away from them, we know that independent invention happens - since this issue does deserve as much attention as it can get, if for no other reason than as an industry we're still obviously having trouble getting this right.  So, great, let's run the headlines again.



Eclypsium's write-up said:  "Multiple threat actors have been exploiting this attack vector which we are calling Sitting Ducks since at least 2019" - and actually 2016, as we'll see in a second.  But they wrote:  "...2019 to perform malware delivery, phishing, brand impersonation, and data exfiltration.  As of the time of writing" - which I think was yesterday - they wrote, "numerous DNS providers enable this through weak or nonexistent verification of domain ownership for a given account."  Well, that can't be good.  "There are an estimated one million exploitable domains, and we have confirmed more than 30,000" - and I did see 35,000 elsewhere - "hijacked domains since 2019.  Researchers at Infoblox and Eclypsium, who discovered this issue" - okay, rediscovered - "have been coordinating with law enforcement and national CERTs since discovery in June 2024."



Okay, now, as I said, I'm glad this resurfaced.  But we talked about this same issue eight years ago when on December 5th of 2016 security researcher Matthew Bryant posted his discovery under the title "The Orphaned Internet - Taking Over 120,000 Domains via a DNS Vulnerability in AWS, Google Cloud, Rackspace, and Digital Ocean."  Now, I should just clarify, those weren't actually taken over.  They were susceptible to being taken over.  And that posting was Matthew's second of two postings about this problem.



So in this second of two postings, on December 5th, 2016, Matthew said:  "Recently I found that Digital Ocean suffered" - you know, a major cloud hosting provider early on - "suffered from a security vulnerability in their domain import system which allowed for the takeover of 20,000 domain names.  If you haven't given that post a read" - meaning his first one - he said, "I recommend doing so before going through this write up.  Originally I had assumed that this issue was specific to Digital Ocean; but as I've now learned, this could not be further from the truth.  It turns out this vulnerability affects just about every popular managed DNS provider on the web.  If you run a managed DNS service, it likely affects you, too.



"This vulnerability is present when a managed DNS provider allows someone to add a domain to their account without any" - like a bad guy adding a domain to their bad guy account - "without any verification of ownership of the domain name itself."  Which, again, why would anybody do that, but okay.  "This is actually an incredibly common flow and is used in cloud services such as AWS, Google Cloud, Rackspace, and of course Digital Ocean.  The issue occurs when a domain name is used with one of these cloud services, and the zone is later deleted without also changing the domain's nameservers.  This means that the domain is still fully set up for use in the cloud service, but has no account with a zone file" - a set of DNS records is called a "zone file" - "to control it.  In many cloud providers, this means that anyone can create a DNS zone" - meaning a set of DNS records - "for that domain and take full control over the domain."



Now, I'll just note that this sounds like that recent CNAME vulnerability we talked about where Microsoft lost control of a CNAME record because someone, you know, they deleted their  service from Azure, but left DNS pointing to it.  Yes, it's very  much the same.  And so what we're seeing is we're seeing this rush to everything being done in the cloud having some downstream consequences because the system really wasn't built for that.  But why would that stop us?



So Matthew says:  "This allows an attacker to take full control over the domain to set up a website, issue SSL and TLS certificates, host email, et cetera.  Worst yet, after combining the results from the various providers affected by this problem, over 120,000 domains were vulnerable."  And he says:  "(Likely many more.)"



Okay.  So in the recent reporting, it turns out that Russian hackers have been doing this for years, that is, actually doing this, sort of quietly behind the scenes, just getting up to what they get up to.  This time around, Eclypsium and Infoblox have brought this to everyone's attention again, and giving it a catchy name, you know, Sitting Duck.  Okay.  Now, I particularly like this name since it nicely and accurately captures the essence, which is that a surprising number of domains are, in fact, as we say, sitting ducks.  The naming was a bit of a stretch, but it's fun.  The initials DNS, okay, Domain Name System, are repurposed to Ducks Now Sitting.  Right.



LEO:  Okay.



STEVE:  Thus Sitting Ducks.  Okay.



LEO:  Okay.



STEVE:  Ducks Now Sitting; that's right.  That's what DNS stands for, kids.  The trouble lies in the fact that responsibility is being irresponsibly delegated for something that's potentially as mission critical as one's DNS.  Eight years ago, as I said, in Matthew Bryant's original posting, he explained the entire problem and provided means for its detection.  He also responsibly disclosed his discovery to a bunch of cloud providers, those he enumerated, where, due to the nature of delegating DNS to them, the trouble is or was rampant.



Google Cloud DNS at the time had around 2,500 domains affected.  They did what they could.  Amazon Web Services, their Route 53 DNS - you know, they named it Route 53 because that's port 53 that DNS runs on - had around 54,000 domains affected so they performed multiple mitigations for the problem after this was brought to their attention.  Digital Ocean had around, you know, those 20,000 domains we talked about earlier at risk, and they worked to fix what they could.  And, interestingly, Rackspace felt differently than the others about the problem.  They had around 44,000 domains affected, and they said this was not their problem.



Matthew explains how simple it is to take over an abandoned Rackspace domain.  He wrote:  "Rackspace offers a Cloud DNS service which is included free with every Rackspace account."  Oh, well, then let's use the free one.  Right.  "Unlike Google Cloud and AWS Route 53, there are only two nameservers (dns1.stabletransit.com and dns2.stabletransit.com) for their cloud DNS offering so no complicated zone creation or deletion process is needed," he writes.  All that needs to be done is to enumerate the vulnerable domains and add them to your account.



"The steps for this are the following:  First, under the Cloud DNS panel, click the Create Domain button and specify the vulnerable domain and a contact email and Time To Live for the domain records.  Step two, simply create whatever DNS records you'd like for the taken-over domain."  And you're done.



LEO:  What.



STEVE:  Yeah, it's like, ouch.



LEO:  Oh, it's easy.



STEVE:  Yeah.



LEO:  Press the Easy button on that one.



STEVE:  That's right.  "This can be done," he writes, "for any of the 44,000 domain names to take them over.  Rackspace does not appear to be interested in patching this (see below)," he writes, "so if you are a Rackspace customer, please ensure you're properly removing Rackspace's nameservers from your domain" - meaning your domain registration, wherever that be - "after you move elsewhere."  I'll explain all that in a second.



So Matthew's blog posting provides the entire timeline of his interactions with the various providers.  In the case of Rackspace, he initially notified them of this problem on September 9th, 2016.  They responded on the 12th.  So that was good.  There was some back and forth for a few months until finally, on December 5th, after notifying Rackspace that he would be disclosing in 30 days, Rackspace replied:  "Thank you again for your work to raise public awareness of this DNS issue.  We've been aware of the issue for quite a while.  It can affect customers who don't take standard security precautions when they migrate their domains, whether those customers are hosted at Rackspace or at other cloud providers.



"If a customer intends to migrate a domain to Rackspace or any other cloud provider, he or she needs to accomplish that migration before pointing a domain registrar at that provider's name servers.  Otherwise, any other customer of that provider who has malicious intentions could conceivably add that domain to his or her account.  That could allow the bad actor to effectively impersonate the domain owner, with power to set up a website and receive email for the hijacked domain.  We appreciate your raising public awareness of this issue and are always glad to work with you."  Have a nice day.



LEO:  So we're not going to do anything about it.



STEVE:  Yeah, exactly.  But in the meantime, you know, good luck to you.  So in other words, yes, it's entirely possible for someone to set up their DNS incorrectly.  And if they do so, others who share the same DNS nameservers, such as generally happens within cloud providers, could register and take over their domain.  Have a nice day.  So, now, as I said, if this seems vaguely familiar, it's probably because we recently talked about that somewhat similar DNS record problem, you know, with Microsoft and Azure and that CNAME record pointing somewhere.  During today's round of rediscovery of this existing problem, which has been exacerbated by the move to the cloud, Brian Krebs interviewed a guy named Steve Job.  You know, he's still alive, and there's no "s" on his name, so just Steve Job.  He is the founder and senior vice president of the firm DNSMadeEasy, and the interview was conducted by email.



Steve Job said, interviewed by Brian Krebs, that the problem isn't really his company's to solve - and of course, you know, he understands about it, too - noting that DNS providers who are also not the domain's registrars have no real way of validating whether a given customer legitimately owns the domain being claimed.  He wrote:  "We do shut down abusive accounts when we find them, but it's my belief that the onus needs to be on the domain registrants themselves.  If you're going to buy something and point it somewhere you have no control over, we can't prevent that."



So, okay, here's the bottom line takeaway from all of this.  Despite all the press it has received, largely because they gave it a great name this time, you know, Matthew didn't name it.  But now we've got Sitting Ducks.  So, okay.  We've got something we can hang onto.  The problem is not new.  The registration of a domain name inherently contains pointers to the domain's DNS nameservers that are providing the records for that domain.  My registrar, Hover, manages the domain records for GRC.com and points to GRC's nameservers since I run my own DNS.  Anyone who does that is safe from any of these problems.  The other way to be safe is to have one's domain registrar, like Hover, also provide the domain's DNS servers and services.  That's the optimal solution for most people.  That has the advantage of keeping both the registration pointers and the servers to which they are pointing all in the family.



The danger that Matthew discovered and first raised back in 2016, and which Eclypsium and Infoblox have highlighted again, occurs when a domain's registrars are pointing to DNS servers that are being shared among tens of thousands of others.  If an account in that shared cloud environment is discontinued while the external domain's registration is still pointing to the cloud's nameservers, anyone else can register that domain in that cloud and start receiving all of its traffic.



So as I noted before, this is unlikely to be happening with anyone's primary accounts since those accounts would tend to be well watched.  But DNS for low-priority domains such as hundreds of excess typo-squatting prevention accounts which might have a server in the cloud which is just being used to bounce incoming traffic over to the correct domain, they could be, dare I say, sitting ducks.  So just something to be aware of.  If you're registered somewhere with your DNS, yet your servers are elsewhere, you want to be very sure with how you handle repointing your registration, the idea being the only safe way to do it is to set up DNS where you're moving to first, and then change your registration records to point to the new DNS.  At no time do you want your registration name servers pointed somewhere where you do not have DNS, pointed to a cloud service.



LEO:  So I'm confused.  Because I do this all the time.  So when they say "migration," I don't understand what that means.  So I have my DNS, my domain name company is Hover, right, like yours.



STEVE:  Yup.



LEO:  But I often want the DNS to be hosted by Fastmail.  So what I do is I go in the DNS record at Hover, and I make sure that it says in the DNS servers that it's Fastmail servers.



STEVE:  So you have a MX record in Hover for...



LEO:  It's everything.  So Hover, you can go into the full DNS, or you can just say, well, who's your DNS provider, and you say my DNS provider is Fastmail.  At which point none of the other DNS settings apply.  Fastmail has CNAME, has MX, it has A, everything.  Right?



STEVE:  Right.



LEO:  So where's the risk, is what I'm worried about.  Because I do that all the time.



STEVE:  So what that would mean is that your Hover - so a domain registration has, like, the root of it is two name servers.



LEO:  Right.



STEVE:  Which are something.  And presumably it would be something dot Fastmail dot com.



LEO:  Yeah, ns1.fastmail.com, ns2.fastmail.com.



STEVE:  Right.  Okay.  So the danger would be that your account at Fastmail would ever disappear.



LEO:  Oh, oh, I see.



STEVE:  Or you would move it somewhere else.



LEO:  Right.



STEVE:  And you would do that...



LEO:  But without repointing it.



STEVE:  Yes, exactly.



LEO:  I see.



STEVE:  So your registration would still be pointing to Fastmail, even though you no longer had DNS services from Fastmail.



LEO:  Right.



STEVE:  And that would allow someone else to swoop in and register an account, saying, you know, this is my domain.  



LEO:  Ah.  Because there's no authentication...



STEVE:  Right.



LEO:  ...when I'm pointing over to Fastmail.



STEVE:  Right.



LEO:  Ah.



STEVE:  Right.  Fastmail relies on you to point to them.



LEO:  I see.



STEVE:  They're not responsible for verifying that you, you know, the ownership of the domain that they're hosting.  They assume that...



LEO:  You must own it.  You pointed it.



STEVE:  They assume your ownership because you were able to point it to them.



LEO:  Right.



STEVE:  Exactly.



LEO:  So if you point it and forget, then you're in trouble.  And move your account out.  Okay.



STEVE:  Then you would be a sitting duck.



LEO:  Well, and I do this with Cloudflare, as well, because, again, Hover's the registrar, but sometimes I want things hosted on Cloudflare.



STEVE:  Yup.



LEO:  So I will point the servers - now, the nice thing about Cloudflare, I'm sure they do this right.  Fastmail really only has two servers, ns1 and ns2.  Cloudflare it's a different server every time that you point to.  They have many, many DNS servers.



STEVE:  Right.



LEO:  So does that help?  It helps.



STEVE:  It makes taking it over, well, it's a good question because Matthew specifically said that Rackspace only had two specific nameservers.  So that means...



LEO:  Right.  That's what raised my interest because that's the same as Fastmail.



STEVE:  Yeah.  But however Cloudflare has things set up, normally your domain registration is just a pair of nameservers.  I don't know if I...



LEO:  It is, but it's a different - but it's like a serialized - it's like a GUID.



STEVE:  Ah.  Then that would probably be good because that suggests that...



LEO:  It's unguessable.



STEVE:  ...that they're not all clustered under one destination server somewhere.



LEO:  That makes sense.



STEVE:  So it sounds like it has your account name, you know, built into the nameserver name.



LEO:  I would expect Cloudflare to do it right.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  Speaking of doing it right, Leo, we're 30 minutes in.  Let's take a pause.  And then, believe it or not, we have another 9.8 remote code execution vulnerability courtesy of Microsoft.



LEO:  Oh, no.



STEVE:  Oh, yes.



LEO:  Oh, no.  All right.  Thank you, Steve.



STEVE:  They did patch it, but you've got to update.



LEO:  Yeah.



STEVE:  Did I mention that this zero-day exploit has a proof of concept released publicly?  I may have missed that.



LEO:  That makes it really dangerous; right?  Because any script kiddie now...



STEVE:  Yeah.  Microsoft, yeah, when you're a Microsoft shop, headlines which read "Exploitable proof of concept released for a zero-click remote code execution..."



LEO:  Oh, it's zero-click.  You didn't mention that, either.  Oh, boy.



STEVE:  Zero-click.  Yes, because we wouldn't want to put anyone out by asking them to click their mouse to be taken over, no.



LEO:  Holy cow.



STEVE:  Yeah, "...zero-click RCE that threatens all Windows servers."  Now, okay, my heart skipped a beat because I am a Microsoft shop.



LEO:  Yeah.



STEVE:  And it's like from Windows, like, 2000 all the way through 2025, all of them.  So if your organization is publicly exposing the Windows Remote Desktop Licensing Service (RDL) - and the good news is I'm not exposing anything, so then I went <sigh>, okay, I'm okay - then it's hopefully not too late for you to do something about it.  Three security researchers have detailed and published proof-of-concept exploit code for a critical CVSS 9.8 vulnerability being tracked as CVE-2024-38077.  It's referred to as "MadLicense."



This 9.8er impacts all iterations of Windows Server from 2000 through 2025.  And it's not even 2025 yet, it's 2024.  In other words, all of them.  And if your service is publicly exposed, that's as bad as it gets.  It's a pre-authentication remote code execution vulnerability that allows attackers to seize complete control of a targeted server without any form of user interaction.  So it's remote and zero-click and affects all Windows services which are exposing the Windows Remote Desktop Licensing Service.



This RDL service is responsible for managing licenses for Remote Desktop Services, as its name sounds, and it's consequently deployed across many organizations.  Security researchers have identified a minimum of 170,000 RDL services directly exposed to the Internet, which renders them all readily susceptible to exploitation.  And, you know, at some point I'll get tired of saying that it just is not safe to expose any Microsoft service except web, you know, because there you have no choice.  But put it behind some sort of protection, a VPN, some extra filtering of some sort.  You know, port knocking, anything.



But don't just have a Microsoft service on the public Internet.  One after the other they are found to be vulnerable, and people are being hurt.  Anyway, the good news is, well, actually it's not the good news.  But in terms of limiting the total number, 170,000, not good, but the nature of this is that it's going to be larger enterprises which are exposed.  And unfortunately, those are the tasty targets.



Okay.  So this so-called "MadLicense" vulnerability arises from a simple heap overflow.  By manipulating user-controlled input, attackers can trigger a buffer overflow which leads to arbitrary code execution within the context of the RDL service.  Researchers have successfully demonstrated a proof-of-concept exploit on Windows Server 2025, achieving a near 100% takeover success rate.  The exploit effectively circumvents all contemporary mitigations.  There were some details that I didn't want to drag everyone through, where the latest Server 2025 has, you know, so-called advanced protection mitigations.  This just ignores them.



So while the proof of concept demonstrated the vulnerabilities exploitation on Windows Server 2025, the researchers emphasized that the bug could be exploited more quickly and efficiently on older versions of Windows Server.  And of course there's lots of those, and those have fewer mitigations in place.  The proof of concept was designed to load a remote DLL.  Wow.  But the researchers noted that, with slight modifications, it could execute arbitrary shellcode provided by the attacker within the RDL process, making the attack even stealthier, meaning that the infected system would not be reaching out onto the Internet to obtain and download that malicious DLL, which might give it away.  So just provide your own shellcode to do what you need.



The researchers did responsibly disclose the vulnerability to Microsoft a month before going public.  And while Microsoft patched the flaw in last month's security Patch Tuesday, meaning July's Patch Tuesday, the fact that it was initially marked, lord knows why, as "exploitation less likely" highlights the potential for underestimating such threats.  You know, Microsoft would like to say, oh, don't worry about that.  But, you know, wow.  They should be protecting their customers.  Doesn't seem like they are in this instance.



So anyone who's not actively using and depending upon this Remote Desktop Licensing Service, who does have it publicly exposed, should definitely shut it down immediately.  And if for any reason you're a month behind on updates, since as I mentioned at the top of the show today is August's Patch Tuesday, so now there's two months' worth of patches, both which fix this, you should seriously consider at least applying the patch for this one.  There is a proof of concept posted on GitHub, so we know it's not going to be long before the bad guys are attacking any unpatched network.  And we know there will be some, for reasons which defy belief or understanding.



Okay.  This next bit of news would have been today's main topic if I didn't think it was also important and very interesting to catch up on what's been happening in the important and still troubled world of certificate revocation.  We opened this door last week, that is, the revocation door last week after DigiCert's mass revocation event and my own "revoked.grc.com" server event, which we'll catch up on.  It turns out that in researching this further I've found that everything we've been doing for certificate revocation over the past 10 years is about to change again.  Okay.  So we'll get to that.



For now, let's talk about the big runner-up this week, which was dubbed "Sinkclose."  It was dubbed Sinkclose by the two security researchers who discovered it.  We've just been talking about Secure Boot and how the AMI BIOS's never-meant-to-be-shipped sample Platform Key was indeed shipped in as many as 850 different makes and models of PCs.



LEO:  You mean the one that said "Don't trust this key"?



STEVE:  Yes.  DO NOT SHIP, DO NOT TRUST.  And if you look at the certificate for it, it says that right there on the front page.  It's like, okay, well, I guess no one looked at that.  Another thing that's not good is when two researchers discover a fundamental vulnerability in AMD's processors - and by that I mean nearly all of AMD's processors - which allows for the subversion of this same Secure Boot process, regardless of what key its platform is using.



LEO:  Well, that makes it easier.



STEVE:  And AMD is a popular processor, as we know.



LEO:  Oh, yeah.  I have one right over here.



STEVE:  Uh-huh.  Okay.  So WIRED covered this discovery and gave it the headline "'Sinkclose' Flaw in Hundreds of Millions of AMD Chips Allows Deep, Virtually Unfixable Infections."  Now, that's a bit of an exaggeration.  But, you know, in fairness, we seem to be encountering similar "sky is falling" headlines these days almost weekly.  So what's going on?  Their subhead was not any more encouraging.  It read "Researchers warn that a bug in AMD's chips would allow attackers to root into some of the most privileged portions of a computer, and that it has persisted in the company's processors for decades."



Okay.  So I'm going to first share a somewhat trimmed-down version of what WIRED wrote because we have a more sophisticated audience.  But then we'll see where we are.  So WIRED explained:  "Security flaws in firmware have long been a target for hackers looking for a stealthy foothold.  But only rarely does that kind of vulnerability appear not in the firmware of any particular computer maker, but in the chips found across hundreds of millions of PCs and servers.  Now, security researchers have found one such flaw that has persisted in AMD processors for decades, that would allow malware to burrow deep enough into a computer's memory that, in many cases, it may be easier to discard a machine than to disinfect it."



LEO:  Oh, I'm not discarding my game machine.  Hold on there, buddy.



STEVE:  "At the DEFCON hacker conference" - which is underway right now - "researchers from the security firm IOActive" - that we've spoken of many times in the past - "plan to present a vulnerability in AMD chips they're calling Sinkclose.  The flaw allows hackers to run their own code in one of the most privileged modes of an AMD processor, known as System Management Mode."  That's even below ring 0.  That's, like, the code that has the lights on the motherboard even when nothing else is running.  WIRED wrote:  "...designed to be reserved only for a specific, protected portion of its firmware.  IOActive's researchers warn that it affects virtually all AMD chips dating back to 2006, or possibly earlier.  The researchers, Nissim and..."  Uh.  I've practiced pronouncing this before.  But it's like, it's O-K-U-P-S-K-I.



LEO:  Oh, sure, that's Okupski.



STEVE:  Okupski.



LEO:  Okupski.  Okupski.



STEVE:  Reminds me of Shabubi.  But no.



LEO:  The key in all of these is, if you say it in the Boris Badenov accent, it works.



STEVE:  You can get away with anything.



LEO:  You can get away, too.  Okupski.  That's Okupski, yeah.



STEVE:  So the researchers, thank you, Nissim - or, no, wait, yeah, Nissim and Okupski, "note that exploiting the bug would require hackers to have already obtained relatively deep access to an AMD-based PC or server, but that the Sinkclose flaw would then allow them to plant their malicious code far deeper still.  In fact, for any machine with one of the vulnerable AMD chips" - and again, everybody who has AMD chips are vulnerable, essentially - "the IOActive researchers warn that an attacker could infect the computer with malware known as a 'bootkit' that evades antivirus tools and is potentially invisible even to the operating system, while offering a hacker full access to tamper with the machine and surveil its activity.



"For systems with certain faulty configurations in how a computer maker implemented Platform Secure Boot, which the researchers warn encompasses a large majority of the systems they tested, a malware infection installed via Sinkclose could be harder yet to detect or remediate, they say, surviving even a reinstallation of the operating system."  Right.  We've talked about this; right?  It's down in the firmware that boots the system.  It is a bootkit.



"Okupski said:  'Imagine nation-state hackers or whoever wants to persist in your system.  Even if you wipe your drive clean, it's still going to be there.  It's going to be nearly undetectable and nearly unpatchable.'  Okupski said:  'Only opening a computer's case, physically connecting directly to a certain portion of its memory chips with a hardware-based programming tool known as an SPI Flash programmer, and then meticulously scouring the memory, would allow the malware to be removed.'"  And of course you'd have to know what was expected there because normally no motherboard provider is going to let you have what should be in that flash chip.  "Nissim sums up that worst-case scenario in more practical terms:  'You basically have to throw your computer away.'



"In a statement shared with WIRED, AMD acknowledged IOActive's findings, thanked the researchers for their work" - while silently cursing them - "and noted" - no, I added that - "and noted that it has 'released mitigation options for its AMD EPYC datacenter products and AMD Ryzen PC products, with mitigations for AMD embedded products coming soon."  And WIRED said:  "(The term 'embedded' in this case refers to AMD chips found in systems such as industrial devices and autos.)  For its EPYC processors designed for use in data-center servers specifically, the company noted that it released patches earlier this year," because the researchers gave them plenty of time.  "AMD declined to answer questions in advance about how it intends to fix the Sinkclose vulnerability, or for exactly which devices and when, but it pointed to a full list of affected products that can be found on its website's security bulletin page."  And I do have a link to that below this in the show notes.



"In a background statement to WIRED, AMD emphasized the difficulty of exploiting Sinkclose."  That is, you know, they're trying to keep everyone from hyperventilating.  They said:  "To take advantage of the vulnerability, a hacker has to already possess access to a computer's kernel, the core of its operating system.  AMD compares the Sinkhole technique to a method for accessing a bank's safe-deposit boxes after already bypassing its alarms, the guards, and the vault door."  On the other hand, "Ocean's Eleven."  "Nissim and Okupski respond that while exploiting Sinkclose does require kernel-level access" - that's true - "to a machine, such vulnerabilities are exposed in Windows and Linux practically every month."



LEO:  Yeah.



STEVE:  And we know that's true.  "They argue that sophisticated state-sponsored hackers of the kind who might take advantage of Sinkclose likely already possess techniques for exploiting those vulnerabilities, known or unknown.  Nissim said:  'People have kernel exploits right now for all these systems.  They exist, and they're available for attackers.  This is the next step.'"  In other words, this is what they've been waiting for.



"Nissim and Okupski's Sinkclose technique works by exploiting an obscure feature of AMD chips known as TClose.  The Sinkclose name comes from combining the TClose term with Sinkhole, the name of an earlier System Management Mode exploit found in Intel chips back in 2015.  In AMD-based machines, a safeguard known as TSeg prevents the computer's operating systems from writing to a protected part of memory meant to be reserved for System Management Mode known as System Management Random Access Memory, or SMRAM.  AMD's TClose feature, however, is designed to allow computers to remain compatible with older devices that use the same memory addresses as SMRAM, remapping other memory to those SMRAM addresses when it's enabled."  In other words, and when you do that, you're no longer able to access the underlying memory because you've mapped other memory on top of it, and so that's what you now access.  But there's a problem with that.



"Nissim and Okupski found that, with only the operating system's level of privileges, they could use that TClose remapping feature to trick the System Management Mode code into fetching data they've tampered with, in a way that allows them to redirect the processor and cause it to execute their own code at the same highly privileged System Management Mode level.  Okupski said:  'I think it's the most complex bug I've ever exploited.'"  And speaking of complex, I've avoided his first name because it doesn't have any vowels, so I can't even begin to pronounce it.  



"Nissim and Okupski, both of whom specialize in the security of low-level code like processor firmware, said they first decided to investigate AMD's architecture two years ago, simply because they felt it had not gotten enough scrutiny compared to Intel" - which of course we've over-scrutinized, if anything - "even as its market share has risen.  They found the critical TClose edge case that enabled Sinkclose, they said" - get this - "just by reading and rereading AMD's documentation.  Nissim said:  'I think I read the page where the vulnerability was about a thousand times.  And then on the 1,001 I noticed it.'  They alerted AMD to the flaw back in October of last year, but have waited nearly 10 months to give AMD ample time to prepare a fix" because they need to change the microcode in all of the affected AMD chips in order to fix this flaw in TClose.



"For users seeking to protect themselves, Nissim and Okupski say that for Windows machines, likely the vast majority of affected systems, they expect patches for Sinkclose to be integrated into updates shared by computer makers with Microsoft, who will roll them into future operating system updates.  Patches for servers, embedded systems, and Linux machines may be more piecemeal and manual; for Linux machines, it will depend in part on the distribution of Linux a computer has installed.



"Nissim and Okupski say they agreed with AMD not to publish any proof-of-concept code for their Sinkclose exploit for several additional months to come" - now that this has everyone's attention - "in order to provide more time for the problem to be fixed.  But they argue that, despite any attempt by AMD or others to downplay Sinkclose as too difficult to exploit" - well, everyone listening to this podcast knows better than that - "it should not prevent users from patching as soon as possible.  Sophisticated hackers may already have discovered their technique."  Right?  I mean, we don't know that somebody else didn't study those pages and go, uh, wow, there's a problem here.  "Or they could figure out how, now that they know there's a problem there, exactly how Nissim and Okupski present their findings once this has been presented at DEFCON."  Again, it's now public.



"Even if Sinkclose requires relatively deep access, the IOActive researchers warn, the far deeper level of control it offers means that potential targets should not wait to implement any fix available.  Nissim finished:  'If the foundation is broken, then the security for the whole system is broken.'"  So as I said, I've got a link in the show notes.  It's, like, all the chips that AMD makes.  As we know, when we've covered this relative to the need for Intel to patch their microcode, Windows is able to, and fortunately does, patch the underlying processor microcode at boot time on the fly.  It's somewhat annoying that there isn't anything more definitive from AMD about what's patched and what isn't, and when what isn't will be, since this really is a rather big and important vulnerability.  It allows anything that's able to get into the kernel to drill itself right down into the firmware and live there permanently, period.



But, you know, we may need to wait a few months until the IOActive guys are able to disclose more.  And hopefully they'll produce a definitive test for the presence of the patch, or someone will create a benign test for the vulnerability, which would be very useful.  So, wow, another bad...



LEO:  That's something.  Jeez.



STEVE:  Bad biggie.  And Leo?  We're at one hour.  Let's take a break.  And then we're going to look at, okay, the most bizarre thing imaginable, a user - a trivial to reproduce Blue Screen of Death allowing anyone who wants to, to crash Windows.  Which Microsoft said, well, it didn't happen to us, so what?  What are you talking about?  And they're not fixing it.



LEO:  Okay.  Your mileage may vary, I guess.  I'm glad you're here.  We're glad you're watching Security Now! with Steve Gibson.  We do it every Tuesday.  It's kind of the day of the week that most security professionals wait for all week long to find out what's going on in the world of security.  And this show is no exception.  Wow.  Holy cow.  This one's wild.



STEVE:  This is.  Okay.  So we have yet another example, we've unfortunately had a few, of Microsoft apparently choosing, deliberately choosing, not to fix what appears to be a somewhat significant problem.  This allows anyone to maliciously crash any recent and fully patched Windows system, and that would seem to be significant.  Yet Microsoft doesn't appear to think so.  Or they can't be bothered.



This all came to light just yesterday, and the Dark Reading site has a good take on it.  They wrote:  "A simple bug in the Common Log File System driver" - that's CLFS, so it's clfs.sys, the Common Log File System driver - "can instantly trigger the infamous Blue Screen of Death across any recent versions of Windows.  CLFS," they write, "is a user- and kernel-mode logging service" - which is why any user can do it - "that helps applications record and manage logs.  It's also a popular target for hacking."  And if you google clfs.sys, you'll see it's had a troubled history.  So you'd think Microsoft would go, oops, again?  Apparently not.



They write:  "While experimenting with its driver last year, a Fortra researcher discovered an improper validation of specified quantities of input data which allowed him to trigger system crashes at will.  His proof-of-concept exploit worked across all versions of Windows tested  including 10, 11, and Windows Server 2022  even in the most up-to-date systems.  The associate director of security R&D at Fortra said:  'It's very simple:  Run a binary, call a function, and that function causes the system to crash.'  He added:  'I probably shouldn't admit to this, but in dragging and dropping it from system to system today, I accidentally double-clicked it and crashed my server.'"



LEO:  Oh, that sounds like something I'd do.



STEVE:  "The underlying issue" - which has been given a CVE-2024-6768  "concerns Base Log Files (BLFs), a type of CLFS file that contains metadata used for managing logs."



LEO:  Ah, it's an interpreter.



STEVE:  The clfs.sys driver, it seems, does not adequately validate the size of data within a particular field in the BLF.  Okay, so, right?  So this is a metadata file which is used to control the CLFS.  And it's assuming the metadata is correct because why wouldn't it be?



LEO:  Why wouldn't it be.



STEVE:  Who would ever tamper with that?



LEO:  Ever.



STEVE:  Right.  Any attacker with access to a Windows system can craft a file with incorrect metadata size information which will confuse the driver.  Then, unable to resolve the confusion, it triggers KeBugCheckEx, the function that triggers the Blue Screen of Death.



LEO:  Closure.



STEVE:  This CVE has a, you know, it's a medium badness; right?  It's a 6.8 out of 10 on the CVSS scale.  It doesn't affect the integrity or confidentiality of data, nor cause any kind of unauthorized system control, although it probably could ground Delta Air Lines, when you think about it.  It does, however, allow for wanton crashes that can disrupt business operations or potentially cause data loss.



"Or, as Fortra's associate director of security R&D explains:  'It could be paired with other exploits for greater effect.  It's a good way for an attacker to maybe cover their tracks, or take down a service where they otherwise shouldn't be able to, and I think that's where the real risk comes in,' he says.  'These systems reboot unexpectedly.  You ignore the crash because it came right back up and, oh, look, it's fine now.  But that might have been somebody hiding their activity, hiding the fact that they wanted the system to reboot so that it could load some new settings or have them take effect.'



"Fortra first reported" - get this - "its findings last December 20th," five days before Christmas.  Here's a present for you, Microsoft.  "After months of back and forth," they say, Fortra said, "Microsoft closed their investigation without acknowledging it as a vulnerability or applying a fix.  Thus, as of this writing yesterday, it persists in Windows systems no matter how updated they are.



"In recent weeks, Windows Defender has been identifying Fortra's proof of concept, which apparently they provided to Microsoft, as malware."  Oh, so don't do that.  Which does not fix the problem.  "But besides running Windows Defender and trying to avoid running any binary that exploits it, there's nothing organizations can do with CVE-2024-6768 until Microsoft releases a patch.  Dark Reading has reached out to Microsoft for its input, but has heard nothing back."



Okay.  So get a load of this timeline, which Fortra published on their vulnerability disclosure report:  December 20th, 2023:  Reported to Microsoft with a proof-of-concept exploit.  So, hey, Microsoft, I know it's close to Christmas, but here's some code that crashes everything.  Um, just thought you should know.  January 8th:  Microsoft responded that their engineers could not reproduce the vulnerability.  Wonder what version of Windows they're using?  January 12th, four days later:  Fortra provided a screenshot showing a version of Windows running the January Patch Tuesday updates - meaning, okay, we patched in January  Here's the memory dump after it crashes.



February 21st:  Microsoft replied that they still could not reproduce the issue, and they were closing the case.  A week later, February 28th:  Fortra reproduced the issue again with the February Patch Tuesday updates installed and provided additional evidence, including a video of the crash condition.  Maybe they should just put it on YouTube.  Okay.  So that's February 28th.



Now we zoom, fast-forward to June 19th, 2024:  Fortra followed up to say that, they said, "We intended to pursue a CVE and publish our research."  A month, almost a month goes by.  Now we're on July 16th:  Fortra shared that it had reserved CVE-2024-6768 and would be publishing soon.  August 8th:  Reproduced on latest updates, that's the July Patch Tuesday, last month's Patch Tuesday, of Windows 11 and Server 2022 to produce screenshots to share with the media.  August 12th, yesterday, is the CVE publication date.



So this is the same company, meaning Microsoft, that wants to continuously record everything every Windows PC user does while using Windows while assuring us that our entire recorded history will be perfectly safe.  No, thank you.  Oh, and I did see on GitHub that what was a Windows crash has now evolved with this flaw into an elevation of privilege.



LEO:  See?  Isn't that what happens?  That's why you never can say, oh, it's just a crash.



STEVE:  Yup.



LEO:  Because the next step after the crash is, well, now we've got - what do we do to control a system now.



STEVE:  Now we have an unprivileged user, any, apparently, unprivileged user is able to elevate themselves to full root.



LEO:  And that's how it always progresses.



STEVE:  Yup.



LEO:  That's why you take that stuff seriously.



STEVE:  Yup.  Crazy.  Okay.  So as we know, two weeks ago we covered the news from Binarly that an estimated 10% of PCs had shipped with, and were still currently using and relying upon AMI's sample demonstration UEFI Platform Keys which had never been intended to be shipped into production, blah blah blah.  Secure Boot isn't.  Binarly named it "PKfail."  The following Friday I saw that it was not easy for anyone to determine whether they had these keys.  By last Tuesday I had a Windows console app finished and published, and I said I would have the graphical version this week.  And I do.  I'm going to, after the podcast today, there's a problem with foreign language installs of Windows.  I know how to fix it.  I just didn't have time before the podcast to do so.



So the first release is there.  Works for everybody running English version of Windows.  And it kind of works for everybody else in the world, but not the way I want it to.  So there's a cosmetic problem that I'm going to fix.  So IsBootSecure is available for download from GRC.  You're able to run it.  It tells you immediately what your situation is.  Are you BIOS or UEFI?  If you're UEFI, is Secure Boot enabled or not?  And if it's enabled, it'll show you your cert.  Many BIOSes will show you your cert even if Secure Boot is not enabled.  Not that it matters, but still it's nice to know.  Some BIOSes don't.  So it tells you that either way.



And if it is able to get the cert, it'll show it to you and allow you to browse around and look at the information in it.  And if for some reason you want to write it to a file, you're able to export the cert from your firmware using IsBootSecure, as well.  So anyway, a cute little freebie, and case closed.  We'll see how this evolves over time.



LEO:  So is it like a real graphical interface?  Or is it just a TUI?  Is it just text...



STEVE:  No, no, it's a dialog.  Yeah, in fact if you go to IsBootSecure.htm, you'll see two sample dialogs, and you can bring it up on our screen while we're talking about it.  GRC.com/isbootsecure.  And it ought to...



LEO:  Do I have to have the htm, or...



STEVE:  No, you don't, because I put that on for you.



LEO:  How does he do that?  And here it is.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  And so it shows you that it is or is not enabled.



LEO:  Nice.



STEVE:  And whether or not you've got good keys or keys that say DO NOT TRUST/DO NOT SHIP.  Yeah.



LEO:  And it's a freebie.



STEVE:  Yeah.  33, is it 3,300 downloads so far?



LEO:  Wow.



STEVE:  So, yeah, it's taking off nicely.



LEO:  And it's only just begun.



STEVE:  Yeah.  And what's really cool is that I did the mailing to Security Now! listeners this morning.  8,145 of our listeners, 8,145 received the email hours ago, and I've already had feedback from them.  So even before the podcast is happening we're getting feedback on the podcast.



LEO:  You've got a good group, I have to say.



STEVE:  Yeah, we really do.



LEO:  It's really impressive.  If you're not on the email list, go to GRC.com/email.  And it's actually got two benefits.  You can sign up for emails, but you can also validate your own address so that you can from then on email Steve, so if you have feedback or...



STEVE:  Yes.  Then, exactly.  And, boy, did I get feedback when what I'm about tell you happened, happened.



LEO:  Oh.  All right.  Let's hear about this Revocation Revoked.



STEVE:  Oh.  Last week we covered DigiCert's immediate revocation of more than 83,000 certificates after it was discovered that their automation software had been found to not be prefixing an underscore onto the front of randomly-generated 32-character subdomain names which were being used in DNS CNAME records to create proof of control over those domains.  So, you know, you would say to DigiCert, hey, I want a certificate for Jimmy's Tire Works.  And they'd say, okay, here's a blob, a random string.  Add this to Jimmy's Tire Works' DNS.  When you have done that, let us know.  We will query that record with that random string.  And if we can, then that means you actually have control of Jimmy's Tire Works' DNS, and we're going to give you a certificate.  So that's proof of control over a domain.



At the end of that discussion last week we talked about the historical mess of certificate revocation, and I reminded everybody about GRC's "revoked.grc.com" test website which I originally created back in 2014, 10 years ago, when this podcast covered this topic in depth.  And to bring, in order to talk about it, to bring the "revoked.grc.com" site back to life for last week's podcast, the previous week, on Thursday, August 1st, I had created and then immediately revoked a brand new certificate for the "revoked.grc.com" web server.



And as we all know, five days later on August 6th, one week ago today, during last week's podcast, Leo and I and all of our online listeners at the time were verifying that, sure enough, that site, which was serving a certificate that had been revoked five days earlier, which Ivan Ristic's SSL Labs site verified, Leo, you brought that up while we were on the air, verified was revoked.  Despite that, that site was still being happily displayed by everyone's browsers of every ilk on every platform, even those that were set to use OCSP, the Online Certificate Status Protocol.



As luck would have it, the next day I began receiving email from our listeners informing that none of their various web browsers would any longer display the revoked.grc.com site.



LEO:  Yeah, I just tried it on Arc on Mac.



STEVE:  What do you know.



LEO:  And this is what you'd want to see; right?  That's a bad certificate, buddy.



STEVE:  It is revoked.  And it even says there net::err_cert_revoked.



LEO:  Yeah.



STEVE:  So for everybody it was showing as an untrusted site with a revoked certificate.  Okay.  What happened?



LEO:  Yeah, one day, overnight.



STEVE:  What happened is that, five days before, that is, five days before the podcast, when I was bringing this up, with DigiCert's help I created a valid certificate for the domain "revoked.grc.com" and installed it on its server.  Then I surfed over to that site with my browser to verify that everything was good.  Like, you know, the server was up; that, you know, the page came up and everything was fine.



In the process of displaying that page, behind the scenes the revoked.grc.com web server  which is OCSP-stapling capable  examined its own certificate and found DigiCert's Online Certificate Status Protocol server's URL which is at http://ocsp.digicert.com.  And note that the URL actually is HTTP because the service that's behind this URL has no need for any protection from spoofing or privacy.  It's http://ocsp.digicert.com.



Upon finding that URL, five days before the podcast, GRC's revoked server queried DigiCert's OCSP server for the current status of this brand new certificate.  And because I was testing the server to make sure the certificate was installed and working properly, I had not yet revoked it.  So DigiCert's OCSP response was to return a short-lived, seven-day, timestamped certificate attesting to the fact that this bright and shiny new certificate was valid as the day it was born.



Upon receiving that certificate, GRC's revoked server stored that brand new OCSP certificate in a local cache.  And from then on, every time anyone's web browser attempted to connect to revoked.grc.com, GRC's server would not only send out its own revoked.grc.com certificate, but "stapled to" that certificate was DigiCert's OCSP certificate having a valid expiration date of August 8th, one week from the date it was first asked to produce an OCSP certificate for the "revoked.grc.com" site.



So having then seen that everything looked good and was working as I expected, I returned to DigiCert and immediately manually revoked the brand new certificate.  I checked DigiCert's own site's status page for the "revoked.grc.com" domain, and it confirmed revocation.



I went over to Ivan Ristic's excellent SSLLabs.com facility and verified that it was very unhappy with the revoked.grc.com site. So everything looked fine.  But because GRC's server, as are many web servers today, is OCSP stapling-capable, it was proudly stapling to its revoked certificate the original and not-yet-expired pre-revocation OCSP certificate it had received from DigiCert the first time it was asked.  Whoops!



And just as today's web servers are now stapling OCSP certificates to their own certificates, today's web browsers are now relying upon those stapled OCSP certificate assertions since it means that they do not need to look any further.  They don't need to bother making their own independent queries to the certificate's authority to check on the current status of the certificate because there's a freshly stapled short-life certificate from that Certificate Authority saying yes, the certificate is good.  Except when it's not.



As a consequence of that, last Tuesday, everyone's browser going to the revoked.grc.com website received not only the recently minted revoked.grc.com certificate that was valid for the next 13 months, but also a valid and unexpired OCSP assertion, "stapled" to that certificate, stating that it was, as of the date of the OCSP certificate, the revoked.grc.com certificate was valid and in good standing.  So no one's normally configured web browser would, or did, look any further.  Here was a recent, fresh, unexpired assertion, signed by DigiCert themselves, stating that all's good with the cert.



But on Wednesday, a day before its cached copy of that original OCSP certificate was due to expire, GRC's revoked server reached out to DigiCert for an update, to update its cached OCSP stapling.  Web servers that do OCSP stapling typically begin asking for an OCSP status update well before their existing OCSP certificate is due to expire so that they're never caught flat-footed without one, and are certain to obtain a replacement from their Certificate Authority well before the current certificate expires.  That happens a day before, typically.



And at that point, six days after the certificate's creation, last Wednesday, GRC's revoked server learned that the certificate it was serving had been revoked.  So it got out its stapler and replaced the soon-to-expire original OCSP certificate with the new one, carrying the news to everyone who was asking from that point on.



So what we've all just witnessed is a perfect example of a pretty good compromise.



LEO:  Yeah, it worked.



STEVE:  Well, it worked.  And it wasn't immediate; but it was, you know, six days.



LEO:  Within the week, yeah.



STEVE:  Yeah.  Okay.  So to see what I mean by that, let's turn the clock back 10 years to see how we got here.  We first looked at all this 10 years ago, in 2014, on this podcast.  At that time the concept of OCSP stapling existed, but it was still not widely deployed.  I recall talking about it wistfully, since it really did seem like an ideal solution.  But it still wasn't widely supported.



Our long-time listeners will recall that OCSP services themselves did exist 10 years ago, but their services were not used by default.  As we know, again, OCSP stands for Online Certificate Status Protocol.  And the idea behind it is elegantly simple:  Any Certificate Authority that's issuing certificates runs servers at a public URL that's listed in every certificate they sign, so anyone can know how to find it.  This allows anyone who receives a certificate that the CA signed to obtain the URL from within the certificate and query the Certificate Authority's OCSP service right then and there to obtain up-to-the-moment information about any specific domain's certificate.



And I'll just note that, if browsers were doing that today instead of relying on the stapled and potentially up to a week-old OCSP certificate, everyone's browser would have immediately complained, I mean, like immediately after I revoked the site.  But browsers aren't doing that now.  They're relying on stapling when it exists.  So what the OCSP service returns is a timestamped, short-lived, signed-by-the-Certificate Authority attestation of the current status of the queried certificate.  It created, as its name says, an Online Certificate Status Protocol.



At the time, 10 years ago, we played with Firefox's settings, which could cause Firefox to query Certificate Authorities' OCSP servers in real time.  But there were a couple of problems with this.  One problem was that OCSP services back then were still not very reliable and could be slow to respond.  So a browser visits a website, like the main website the visitor wants to go to; receives the site's TLS, back then SSL, certificate; examines that certificate to obtain its issuing Certificate Authority's OCSP server URL; and queries the URL for a real-time verification of the certificate's validity.



Even if the OCSP server responds quickly, a cautious web browser should wait before doing anything with the website's page since the presumption is that the certificate it was served might be bogus.  This is, after all, the reason for the OCSP double-check is to make sure.  So this could introduce a significant delay in the user's experience that no one wants.  And what if no one is home at the OCSP service?  What if the CA is having a brief outage, or they're rebooting their service, or updating it, or who knows what?  Does the web browser just refuse to connect to a site that it cannot positively verify?



The goal, of course, is to never trust an untrustworthy site.  But given the fact that nearly all checked certificates will be good, and that only the very rare certificate will have been revoked before it expires on its own, it seems wrong to refuse to connect to any site whose Certificate Authority's OCSP service doesn't respond or might be slow.  As a consequence, 10 years ago, even when a web browser had been configured to check OCSP services, unless it was forced to do otherwise, browsers would generally fail open, meaning they would default to trusting any site that they were unable to determine should definitely not be trusted.



So one clear problem was the real-time trouble that real-time certificate verification created.  Recall that back then Google was, like, on a tear about, like they were frantic about the performance of their web browser, Chrome, which was still kind of new.  The last thing they were going to do was anything that might slow it down in any way.  So they created these bogus CRL sets, and I called them out on it because they were saying, oh, yeah, this works.  And I said, "No, it doesn't."



LEO:  Yeah, we talked about this.



STEVE:  Oh, yeah.  Believe me, I...



LEO:  Yeah.



STEVE:  It was a number of podcasts, and it came back a few times.  Okay.  But another problem was the privacy implications which this brought to the web browser's user.  If every site someone visits causes their browser to reach out to remote Certificate Authority-operated OCSP servers, then anyone monitoring OCSP traffic - which as I mentioned has always been unencrypted HTTP, and even is today - that would allow anyone monitoring OCSP traffic to track the sites users were visiting...



LEO:  [Crosstalk]



STEVE:  ...since their browsers, yeah, their browsers would be continually querying for every site's current certificate status.  This was a huge concern for the privacy of the Internet's web users.



So the reason I was so excited about OCSP stapling 10 years ago was that it beautifully addressed and resolved all of these problems.  As you've probably figured out from what I've already said, OCSP stapling flips the original OCSP model on its head.  With stapling, instead of having the user's web browser going out to continually fetch every site's current OCSP status, the same site that's offering its certificate "staples" a recent and unexpired OCSP response from the Certificate Authority, just as a browser would retrieve it.  Instead, the site staples that to its own certificate.



Now the web browser doesn't need to perform another fetch and query.  So there's no delay, no extra fetching and waiting, and no privacy issue because, you know, the browser's already going to the site and might as well get confirmation that the certificate's good.  Essentially the server is saying "Here is my certificate, and the Certificate Authority that issued it no more than a week before has asserted that as of that time it's good."



What we learned over the past week is that the phrase "much more recent," that is, the CA's much more recent assertion might not always be as recent as we would hope.  That is to say, it's not immediate.  The OCSP response for every domain I have checked - mine, Google's, Let's Encrypt, Hover's, Facebook's (actually Facebook is also using DigiCert, so GRC uses the same OCSP services as Facebook) - they are all, all of those Certificate Authorities are issuing OCSP response certificates with a seven-day life.



And this is why I referred to this earlier as a pretty good compromise.  The reason we need some compromise is that for practical purposes we need to tolerate some local caching of previously received and unexpired OCSP responses.  Ten years ago, the entire world had not yet switched over to using HTTPS all the time for everything.  Now it has.  Today, virtually every of the many connections that our web browsers make are HTTPS.  Every one of those connections returns a certificate.  And if that site's web browser has not done the courtesy of stapling a valid OCSP certificate to its own certificate, and if the connecting client has been configured to use OCSP, then without allowing for some reasonable local caching, every one of those received HTTPS certificates not having a stapled OCSP response would need to be checked against their respective Certificate Authority's OCSP servers every single time.  So it's not just web servers that are retrieving and caching OCSP certificates, it's anyone who's relying upon a web server certificate that does not include a stapled response.



Put another way, if OCSP responses could not be cached for some reasonable compromise time, the entire world would be continuously DDoSing the world's Certificate Authorities' OCSP services due to the need to always look up the latest status of every unstapled certificate they had received.  And if you think about it, stapling doesn't solve the problem either.  It just moves it to the server-side because, if web servers were not similarly able to staple their most recently received cached and still valid responses, that is, if web servers could not cache the response, then every web server would be pounding away at their Certificate Authority's OCSP service for every new certificate they were sending for every new TLS connection they accepted.  So it's clear that caching of OCSP status is crucial for the survival of the system.  Which brings us to how long that caching should be.



I mentioned that the Certificate Authorities of every site I checked were issuing OCSP responses with a seven-day lifetime.  There's a terrific clean and simple online service for displaying OCSP responses.  It's at https://certificatetools.com/ocsp-checker, and I gave it an easier-to-use GRC shortcut of OCSP.  So you can just go to grc.sc/ocsp, which will bounce your browser over to the OCSP checker at the certificatetools.com site.  If you put whatever domain you like in there, you'll receive the OCSP response from that site's Certificate Authority.  Just as I was assembling this, I thought to put in "TWiT.tv," and I was rewarded with the shortest OCSP lifetime I had encountered so far.  TWiT.tv's Certificate Authority is GoDaddy, and GoDaddy's OCSP expiration is only four days.  I haven't seen anything that short anywhere else.  But the standard everywhere else is seven days, Google and everyone.



What we just saw with my "revoked.grc.com" site is a perfect model for malicious exploitation.  If a bad guy manages to get hold of an important website certificate and sets up a malicious clone of the site using that certificate, they will definitely want their web server to be stapling valid OCSP responses to every connection since doing so prevents the client from looking any further.  But the moment that certificate's OCSP response changes to show that its certificate has been revoked, the malicious server will want to continue using the last good OCSP response it had received for as long as it lasts, which might give it another few days of malicious use before its site spoofing would be brought to a close.



Thus the compromise.  It's not a solution that immediately detects certificate revocation, but it's sure better than it was 10 years ago when we looked at this back in 2014, and GRC's revoked site with its revoked certificate was just sitting there for years without any regularly configured browser even noticing.  That is not so today.  Now we know that within typically six days, seven at the most, if a certificate is revoked, the OCSP for that certificate will either not be signed, not be stapled by a malicious server, in which case maybe the user's browser will go out and ask for an OCSP verification on its own - we don't know what it would do.  And I'm kind of curious.  But I'm not pursuing it because, as they say, "Wait, there's more."



Believe it or not, three weeks ago today, on July 23rd, Let's Encrypt, which currently commands nearly 60% of the global web browser TLS certificate space, posted their news under the headline "Intent to End OCSP Service."  I kid you not.  Here's what Let's Encrypt posted.  They said:  "Today" - three weeks ago; right?  Three weeks ago today they said:  "Today we are announcing our intent to end Online Certificate Status Protocol support in favor of Certificate Revocation Lists (CRLs) as soon as possible."



Okay.  What?  We already had Certificate Revocation Lists.  They were much worse, a total disaster, in fact.  Which is why OCSP was created.  We're going back to that?  Why?  They wrote:  "OCSP and CRLs are both mechanisms by which CAs can communicate certificate revocation information, but CRLs have significant advantages over OCSP."  Okay.  "Let's Encrypt has been providing an OCSP responder since our launch nearly 10 years ago.  We added support for CRLs two years ago, in 2022.  Websites and people who visit them will not be affected by this change, but some non-browser software might be.  We plan to end support for OCSP primarily because it represents a considerable risk to privacy on the Internet."



Okay.  So they're talking about non-stapled OCSP, where everybody is making queries, their own queries, rather than relying upon the certificate being stapled to and accompanying the cert from the web server.  Privacy.  And they're right, that's a problem.  They said:  "When someone visits a website using a browser or other software that checks for certificate revocation via OCSP, the Certificate Authority operating the OCSP responder immediately becomes aware of which website is being visited from that visitor's particular IP address.  Even when a CA intentionally does not retain this information, as is the case with Let's Encrypt, CAs could be legally compelled to collect it.  CRLs do not have this issue."



Okay.  So again, I ask myself, has Let's Encrypt not heard of OCSP stapling, which solves this problem?  Since they must have, it must be the case that, since OCSP stapling is not mandatory, it's just polite, even today not all web servers are going to the trouble of stapling.  Okay.  Anyway, let's hear from Let's Encrypt.



They continue:  "We are also taking this step because keeping our CA infrastructure as simple as possible is critical for the continuity of compliance, reliability, and efficiency at Let's Encrypt.  For every year that we've existed, operating OCSP services has taken up considerable resources that can soon be better spent on other aspects of our operations.  Now that we support CRLs, our OCSP service has become unnecessary."  Wow.



"In August of 2023" - so exactly one year ago - "the CA/Browser Forum passed a ballot to make providing OCSP services optional for publicly trusted CAs like Let's Encrypt.  With one exception, Microsoft, the root programs themselves no longer require OCSP.  As soon as the Microsoft Root Program also makes OCSP optional, which we are optimistic will happen within the next six to 12 months" - okay, and that's from this posting, which was three weeks ago - "Let's Encrypt intends to announce a specific and rapid timeline for shutting down our OCSP services."  This is 60% of the certificate space.  "We hope to serve our last OCSP response between three to six months after that announcement.  The best way to stay apprised of updates on these plans is to subscribe to our API Announcements category on Discourse.



"We recommend that anyone relying on OCSP services today start the process of ending that reliance as soon as possible.  If you use Let's Encrypt certificates to secure non-browser communications such as a VPN, you should ensure that your software operates correctly if certificates contain no OCSP URL.  Fortunately, most OCSP implementations 'fail open,' which means that an inability to fetch an OCSP response will not break the system."  And so that answers the question I had about what browsers would do.  Do they look it up on their own if the OCSP response is not stapled, or do they just shrug and let the user use it anyway, which apparently is what they do.



Okay.  So in order to figure out what was going on, I referred to their statement, "We added support for CRLs in 2022," and I found their news about that.  Here's what Let's Encrypt posted nearly two years ago on September 7th, 2022 under their headline "A New Life for Certificate Revocation Lists (CRLs)."  They said:  "This month, Let's Encrypt is turning on new infrastructure to support revoking certificates via Certificate Revocation Lists.  Despite having been largely supplanted by the Online Certificate Status Protocol over a decade ago, CRLs are gaining new life with recent browser updates."  What?  "By collecting and summarizing CRLs for their users, browsers are making reliable revocation of certificates a reality, improving both security and privacy on the web.



"Let's talk about exactly what this new infrastructure does, and why it's important.  When a certificate becomes untrustworthy, for instance because its private key was compromised, that certificate must be revoked and that information publicized so that no one relies upon it in the future.  However, it's a well-worn adage in the world of the Web Public Key Infrastructure, the Web PKI, that revocation is broken.  Over the history of the Web PKI, there have been two primary mechanisms for declaring that a TLS/SSL certificate should no longer be trusted:  Certificate Revocation Lists and the Online Certificate Status Protocol (OCSP).  Unfortunately, both have major drawbacks.



"CRLs are basically just lists of all of the certificates that a given Certificate Authority has issued which have been revoked.  This means that they're often very large, easily the size of a whole movie.  It's inefficient for your browser to download a giant list of revoked certificates just to check if the single certificate for the site you're visiting right now is revoked.  These slow downloads and checks made web page loads slow, so OCSP was developed as an alternative."



They say:  "OCSP is sort of like 'what if there were a separate CRL for every single certificate?'  When you want to check whether a given certificate has been revoked, your browser can check the status for just that one certificate by contacting the CA's OCSP service.  But because OCSP infrastructure has to be running constantly and can suffer downtime just like any other web service, most browsers treat getting no response at all as equivalent to getting a 'not revoked' response.  This means that attackers can prevent you from discovering that a certificate has been revoked simply by blocking all your requests for OCSP information.



"To help reduce load on the CA's OCSP services, OCSP responses are valid and can be cached for about a week.  But this means that clients don't retrieve updates very frequently, and often continue to trust certificates for a week after they're revoked.  And perhaps worst of all, because your browser makes an OCSP request for every website you visit, a malicious or legally compelled CA could track your browser behavior by keeping track of what sites you request OCSP for.



"So both of the existing solutions don't really work.  CRLs are so inefficient that most browsers don't check them, and OCSP is so unreliable that most browsers don't check it.  We need something better.  One possible solution that's been making headway recently is the idea of proprietary, browser-specific CRLs.  Although different browsers are implementing this differently - for example, Mozilla calls theirs CRLite, and Chrome's are CRLSets - the basic idea is the same.  Rather than having each user's browser download large CRLs when they want to check revocation, the browser vendor downloads the CRLs centrally.  They process the CRLs into a smaller format such as a Bloom filter, then push the new compressed object to all of the installed browser instances using pre-existing rapid update mechanisms.  Firefox, for example, is pushing updates as quickly as every six hours.



"This means that browsers can download revocation lists ahead of time, keeping page loads fast and mitigating the worst problems of vanilla CRLs.  It keeps revocation checks local, and the pushed updates can take immediate effect without waiting for a potentially week-long OCSP cache to expire, preventing all the worst problems with OCSP.



"Thanks to the promise of these browser-summarized CRLs, both the Apple and Mozilla root programs are requiring that all CAs begin issuing CRLs before October 1st, 2022."  Okay, so that's a year and a half ago, almost two years ago.  "Specifically, they are requiring that CAs begin issuing one or more CRLs which together cover all certificates issued by the CA, and that the list of URLs pointing to those CRLs be disclosed in the Common CA Database.  This will allow Safari and Firefox to switch to using browser-summarized CRL checking for revocation."



And finally:  "When Let's Encrypt was founded, we made an explicit decision to only support OCSP and not produce CRLs at all.  This was because the root program requirements at the time only mandated OCSP, and maintaining both revocation mechanisms would have increased the number of places where a bug could lead to a compliance incident.



"When we set out to develop CRL infrastructure, we knew we needed to build for scale, and do so in a way that reflects our emphasis on efficiency and simplicity.  Over the last few months we have developed a few new pieces of infrastructure to enable us to publish CRLs in compliance with the upcoming requirements.  Each component is lightweight, dedicated to doing a single task and doing it well, and will be able to scale well past our current needs.



"Let's Encrypt currently has over 200 million active certificates on any given day.  If we had an incident where we needed to revoke every single one of those certificates at the same time, the resulting CRL would be over 8GB.  In order to make things less unwieldy, we will be dividing our CRLs into 128 shards, each topping out at a worst-case maximum of 70MB.  We use some carefully constructed math to ensure that, as long as the number of shards does not change, all certificates will remain within the same shards when the CRLs are re-issued, so that each shard can be treated as a mini-CRL with a consistent scope."



Okay.  So they weren't explicit about this, but I assume their reason for breaking their master CRL into multiple consistent mini-CRLs is that any addition of a certificate to a CRL would then affect only one of the 128 individual CRLs.  So that rather than replacing the entire list, only 1/128th of the entire list would require updating. 



And they said:  "As part of developing these new capabilities, we have also made several improvements to the Go standard library's implementation of CRL generation and parsing.  We look forward to contributing more improvements as we and the rest of the Go community work with CRLs more frequently in the future.  Although we will be producing CRLs which cover all certificates that we issue, we will not be including those URLs in the CRL Distribution Point extension of our certificates."  In other words, they're going to be creating CRLs, Certificate Revocation Lists, but they're not going to be making their access public.



They said:  "For now, as required by the Baseline Requirements, our certificates will continue to include an OCSP URL which can be used by anyone to obtain revocation information for each certificate.  Our new CRL URLs will be disclosed only in the CCADB" - that's the CA/Browser database - "so that the Apple and Mozilla root programs can consume them without exposing them to potentially large download traffic from the rest of the Internet at large."  In other words, this new model would be that Apple will obtain the CRL from Let's Encrypt.  Mozilla will obtain the CRL from Let's Encrypt.  They will then process them and periodically, when necessary, upload them to our browser instances so our browsers are always maintaining a relatively recent CRL, you know, master CRL.



And they finish:  "There's still a long way to go before revocation in the Web PKI is truly fixed."  In other words, it never has been.  They wrote:  "The privacy concerns around OCSP will only be mitigated once all clients have stopped relying upon it, and we still need to develop good ways for non-browser clients to reliably check revocation information.  We look forward to continuing to work with the rest of the Web PKI community to make revocation checking private, reliable, and efficient for everyone."



Now, digging around in the CA/Browser Forum history, I found ballot measure SC-063 from a year ago.  It was titled - and I'm not going to share the whole thing.  I'll just summarize.  It was titled "Make OCSP Optional, Require CRLs, and Incentivize Automation."  In other words, we're switching back.  We're giving up everything, we're shutting down OCSP, we're going back to CRLs, but this time we're going to do it differently.



Okay.  The result of voting on this measure - I checked the ballot results.  The result of voting on this measure to do that was 23 certificate issuers voting yes, and only one certificate issuer voting no.  And all three of the certificate consumers who were participating in the Forum, namely Google, Mozilla, and Apple, voted yes, with no certificate consumer voting no.  So it's very clear that the certificate revocation tides will be turning once again in an attempt to continue fixing what everyone agrees is a barely functioning certificate revocation system.



As I was reading through the Let's Encrypt plans, a couple of things occurred to me.  As we all know, certificates self-expire.  This is highly significant for the practicality of certificate revocation lists since these lists only need to suppress the trust of certificates that have not yet expired.  In other words, all certificates will eventually become untrusted by virtue of their own "not valid after" timestamps.  Ten years ago, web browser certificates were available with five-year lifetimes.  This meant that a certificate that escaped the control of its owner, or was subjected to a Heartbleed attack or whatever, or may have been misused, would need to remain on its CA's CRL for the balance of its life, which might be as long as five years.  Therefore, the gradual reduction in certificate lifetime that the industry has been facilitating over the past 10 years has significantly reduced the CRL burden.



With all TLS web certs now expiring after a maximum of 398 days, CRLs can be much shorter and smaller because the certificate will, you know, a bad certificate will take itself out after a maximum of 398 days.  Then it can be dropped from the CRL.  And despite Let's Encrypt having nearly 60% of the certificate market, their much shorter 90-day certificates which are being  issued through their ACME automation means that their CRLs can benefit from their even shorter lifetimes.



The other thing that has changed in the past 10 years is the general increase in bandwidth and local storage capacity, all while reducing cost.  Right?  This makes shipping web browser updates far more practical and effectively invisible to their users.  Today's web browsers have become large operating systems in their own right, yet most of today's users aren't even aware as these behemoths are being updated silently in the background.



This all suggests that the move away from individual certificate revocation checking to browser-side list checking can make sense.  It does impose a new burden upon all browser vendors since they now will need to be continuously collecting and merging the individual CRLs from every source of web browser certificates, and as we've seen in the past there are a great many of those.



So we may not have yet seen the end of GRC's revoked.grc.com site.  In another year or two, once the switch has been made from OCSP back to browser-based Certificate Revocation Lists, I think it's going to be fun and interesting to create a new freshly revoked certificate for that site and see what happens, see how it goes.



LEO:  This is a tough computer science problem.



STEVE:  Yes.



LEO:  Doesn't look like we have the answer to it.



STEVE:  Yes.  That is exactly right.  It is, you know, the idea is that certificates are meant to be freestanding representations of something.



LEO:  Right.



STEVE:  That have a lifetime.  Yet that representation may, you know, may not - or the representation may outlive the truth of what it is saying.  So you need some way of saying, uh, whoops, we did promise that, and we signed it.  And we were right when we said it, but something happened.  Whoops.  Yeah, Heartbleed or bad guys or whatever.



LEO:  Well, even if you divide it up into 128 70MB files, that's still a lot of data.



STEVE:  Well, that was worst-case.



LEO:  Okay.



STEVE:  They were saying, if they had to replace - I think that was if they had to replace every single certificate all at once.  And that's Let's Encrypt with 60% of the certificate market.  Yeah, so they were taking an 8GB file, dividing it into 128 pieces.  So I think what's going to happen is we're all going to have local CRLs of all of the various CAs' revoked and not-yet-expired certificates.  And boy, Leo, are we going to have fun when that happens.



LEO:  That seems like a lot of data still.



STEVE:  Yeah.  What I'm going to do, I was thinking about this actually, somehow I was able to think while I was reading at the same time, which sort of surprised me.  I didn't think I was able to do that.



LEO:  I can't even think while I'm switching cameras, so you're way ahead of me.



STEVE:  I was thinking I'm going to put DigiCert's OCSP site into my hosts file so that I can blackhole it.  That way my server will not be able to staple.  And then I'm going to get a certificate again and revoke it, which it won't be able to staple, and we're going to find out what our browsers do once stapling is suppressed.



LEO:  Well, they're probably going to fail insecure.  Isn't that what they've been doing?



STEVE:  I think they're going to fail open, yup.



LEO:  Yeah, fail open.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#988

DATE:		August 20, 2024

TITLE:		National Public Data

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-988.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  As we embark on our 20th year of this weekly Internet security and privacy-oriented technical news podcast, we're going to look at some more interesting certificate revocation news, and we have an experiment for our listeners.  What six zero-days were patched during Microsoft's Patch Tuesday last week?  Fifty-three episodes of the 1980s "Famous Computer Caf" radio show were recently discovered and are now online; hear Bill Gates before his voice changed.  We have Release #3 of IsBootSecure, and a GRC email update, and some interesting listener feedback.  Then, to no one's surprise, we're going to take a deep dive into the background, meaning, and impact of the largest personal data breach in history:  How to look up your own breached records online, what to do, and what this means for the future.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a very big show for you.  Ninety, count 'em, 90 fixes on Patch Tuesday last week.  Steve will count them all.  No, he won't.  Ninety-nine fixes on the wall, no.  He's going to talk about a few of them.  We've got a great Picture of the Week that explains how RAID works, sort of.  An update on the certificate revocation issues.  And then, finally, a look at the biggest data breach we think of all time.  How to find out if you're part of the NPD breach, and what to do about it, coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 988, recorded Tuesday, August 20th, 2024:  National Public Data.



It's time for Security Now!, the show we cover the latest security news.  Oh, there's a few little tiny stories to talk about with this guy.



STEVE GIBSON:  Happy birthday to us.  Happy birthday to us.	



LEO:  It's our birthday.



STEVE:  Happy birthday, happy birthday, happy 19th birthday to us.



LEO:  That's Singing Steve Gibson, everybody, the security cowboy.  



STEVE:  Yes, sir, it was August 19th, 2005.  We were all security virgins, and we stepped into this not having any idea what we had to scrape off our shoes. 



LEO:  You weren't, yeah, you weren't a security virgin.  But we were all podcast virgins because it was the very earliest days of podcasting.



STEVE:  Well, let me just say I'm way less virginized than I was back then.



LEO:  You know a lot more.  Whew.



STEVE:  Oh, goodness.  Yup.



LEO:  Yes, yes.  And now, of course...



STEVE:  Beginning to believe my own PR on this because, wow.  After today, oh, goodness.  When our listeners go and check for their own personal data...



LEO:  Oh, I already did this.  It's depressing.



STEVE:  Oh.  You're there, Leo, as I'm sure you know.  I'm there.  My wife is there.  Everybody I've looked up is like, whoa, doggie.



LEO:  Yeah, this is a bad one.



STEVE:  So, yeah.



LEO:  So what are we talking about, Steve?  This is the subject; right?



STEVE:  We're talking about today's podcast of course had to be titled National Public Data, which the press, you know, in their typical hyperventilation, thought that it was three billion people.  And I thought, wait a minute.  What is the current carrying capacity of the Earth?  Because it is...  



LEO:  Well, it's more than three billion.  But still, that seems a lot.



STEVE:  Yeah.  That says that people who've never held a phone or gone to the Internet somehow have their Social Security Numbers up - they don't even have Social Security Numbers - up on the Internet.  No.  It's three billion, actually just shy of three billion, 2.9 billion records.



LEO:  Wow.



STEVE:  Which pretty much tells an individual's history over the last several decades.  Anyway, we're going to get to that and have fun with that.  We've got a bunch of stuff.  We've got, of course, because yesterday was our birthday, our 19th birthday, we are now, with Podcast 988, and fortunately no end in site...



LEO:  Thank you.



STEVE:  In another 11 we'll make 999.



LEO:  Yay.



STEVE:  And we're just going to seamlessly cross over into four digits.  I will have to do a little tweaking of my software, as I mentioned several years ago.  But we're not going to stop.  So Year 20, here we come.  We're going to - I have some interesting update on the topic that's sort of been a running theme for a while, and it's probably going to be one next week, but in a different sense, which is this challenge of certificate revocation, which is important, which is why it's gotten so much of our time and so much of the industry's time and attention.  Something happened, Leo, at 2:00 a.m. this morning which we're going to be talking about and explaining, and also what it means.  And also there's one last piece that I haven't talked about which we're going to nail down.  So there's that.



Also we've got the six zero-days which were patched last week, on Tuesday, during Microsoft's Patch Tuesday.  I also ran across, thanks to a listener, and I saw you mentioning it at the end of MacBreak Weekly, the 53 discovered episodes of the 1980s Famous Computer Caf radio show.



LEO:  Were you ever on that show?



STEVE:  I wasn't.  But we get to hear Bill Gates before his voice changed.  So that's going to be fun.  We also have the third release, third and final I should say, of IsBootSecure; a quick note on GRC's email, how that's going, which is to say really well; and some listener feedback.  Then, again, to no one's surprise, because I imagine our audience has already heard about this, but we're going to take a deep dive into the background, meaning, and impact of the largest personal data breach in history, and how everybody how they can find themselves and all their personal information online.  We're going to talk about what to do about that and what it means for the future.  And as if that wasn't enough, oh, Leo, we have a Picture of the Week.  This has got to be, on the geek rating scale, it's off the scale probably.  Most people would look at this and go, what are these...



LEO:  What are they smoking?



STEVE:  ...nerds talking about?  Why are they - have they fallen off their chair in laughter?  Because it's very much on the inside.  But I had someone already respond, say this is definitely among the top five you've ever done, so...



LEO:  Wow.  Oh, I haven't seen it yet.



STEVE:  It's great.



LEO:  We'll see it together for the first time.



STEVE:  It's oh, so clever.  I just - whoever this genius was who did this, it's like, hat's off.



LEO:  Very nice.  All of this still to come.  In just a minute we'll get the Picture of the Week on Security Now! 988, as we gradually head toward 999 and beyond.



STEVE:  We are drifting under power.



LEO:  I love it.  I love it.  Okay, Steve.  Let's get the - I'm pulling up the Picture of the Week even as we speak.



STEVE:  Just so good.



LEO:  Do you want to describe it a little bit?



STEVE:  Oh, yes.



LEO:  Go ahead, and I'll pull it up.



STEVE:  Okay.



LEO:  I don't want to break the, you know, the surprise.  Should I look at it first?



STEVE:  Yeah, you should look at it because it's just so wonderful.



LEO:  I will look at it, and then you can describe it.  Okay.  So this is me.  I haven't looked at it yet.  This is how we're going to look at it together.  I'm going to scroll it up now.  "If water coolers were RAID arrays."  Okay.  Now I'm ready to show it.



STEVE:  It's so good.



LEO:  That's fantastic.  That is really great.



STEVE:  It is.  So, okay, as you said, I gave this one the caption "If water coolers were RAID arrays."



LEO:  I apologize.  I'm mirroring it.  Let me fix that.  Go ahead.  Keep talking.



STEVE:  Okay.  So anyway, looks fine for me.



LEO:  Yeah, okay.  Well, it's just me then, okay, good.



STEVE:  Yeah.  We have your standard water cooler, which, you know, is a water cooler with a big jug of water on top.  And that's labeled "Standalone."  There are seven frames here to represent all the various configurations of mass storage redundancy and management.  The second one is labeled a "Cluster" because that's two water coolers, each with its water jug on top.  The third one is labeled "Hot Swap," where of course we have one water cooler with its jug on top and, as you often see next to a water cooler, another jug standing by its side, waiting to be swapped in to replace the first jug when it's emptied.  So that's the Hot Swap configuration.  Now, for RAID 1 we've got - somehow this guy has managed to have...



LEO:  They were having a lot of fun in the office.



STEVE:  Oh, my god, yes.  Two jugs side by side, feeding into one water cooler.  So they're next to each other.  Kind of off, you know, pushed apart a little bit because they won't actually fit completely vertically.  But that's the RAID 1 configuration.  RAID 5 is three full-size water jugs similarly arranged.  Somehow he's managed to get all three feeding into the top of one water cooler.  Thus RAID 5.



Now, of course we have two other RAID configurations.  RAID 0 is the one shown on the bottom.  That, of course, RAID 0 is concatenating two drives; whereas, as we saw RAID 1 is also known as mirroring, so that's why RAID 1 had the two jugs side by side; while RAID 0 has one jug feeding into the next jug, so they're stacked on top of each other, two jugs feeding down into  a single water cooler.  And then finally, of course, the RAID 0+1. 



LEO:  Striped, it's the striped array.



STEVE:  Yes, it is, it's the combination of 1 and 0.  So we've got two jugs side by side with two jugs above them feeding down into them.  Anyway, it's just - one of our listeners sent this to me.  I glanced at it, and I thought, oh, this is so good.  And actually I had this around the middle of last week so I shared - because I love this picture so much that I shared it with the gang over in the newsgroup.  And one of our active followers/contributors over there ran it through some sort of AI generative thing, which significantly cleaned it up.  It made the text much more legible than it was and kind of cleaned up the pictures a lot.  So I was very appreciative of that.



LEO:  I love Gumby's suggestion that in this case RAID stands for a Redundant Array of Inexpensive Dasanis.  Okay.  Thank you very much.



STEVE:  Yeah, very good.  Okay.  So a contributor over in GRC's newsgroups posted some terrific observations following from last week's discussion of revocation.  Andrew wrote:  "I have a sneaky suspicion that Steve's long-term plan for revoked.grc.com is going to ferret out what software is swimming the waters in this space, naked, shall we say."  Now, he was addressing my plan, which I had shared there, which was to back GRC's servers away from the use of OCSP, Online Certificate Status Protocol, to see what happens.  And at 2:00 a.m. this morning the last received and stapled OCSP status which my revoked.grc.com site had received from DigiCert a week ago, so it had its one-week life, it expired at 2:00 a.m. this morning.



I will share what happened after that in a minute.  But let's first examine one last feature of OCSP that we have not talked about yet during this go-round of talking about revocation, and work out OCSP's Achilles heel, which is what made it impractical to require at scale.



Okay.  So as we all witnessed in the last two weeks, when GRC's revoked.grc.com site began stapling an OCSP status that loudly stated its certificate had been revoked, and, you know, no one's browser, as we saw, would show the page, which suggests that OCSP appears to be working better than anything else ever has so far.  But as we also learned, the CA/Browser Forum now plans to make OCSP support optional and to switch back to requiring the use or requiring Certificate Authorities to publish Certificate Revocation Lists, thus making them mandatory.



So at the time, last week, I replied to Andrew.  I wrote:  "If the industry has inexplicably chosen to abandon the system that all browsers are currently using with 100% success - as was just demonstrated with GRC's OCSP stapling working perfectly everywhere - then, okay, we'll start testing the replacement system, that is, Certificate Revocation Lists, to see how well it does."



Andrew added:  "With browser providers like Google, Mozilla, and Apple providing the CRL important bits as a centralized service with rapid-update to their browsers in the field," he said, "It sounds like insanity."



Okay.  To which I replied:  "I 100% agree.  If Let's Encrypt and the rest of the CA/Browser Forum are suddenly so worried about web privacy due to individual browsers reaching out to query Certificate Authority OCSP services, then we'll start a countdown on the mandatory support for CRLs."



Okay.  So the final piece that I've not discussed at all - I may have mentioned it, but I'm not sure, though we did cover it in depth back at the time 10 years ago when we first talked about all this, is known as "OCSP Must Staple."  So here's the problem that solves.  A web server obtains a web server TLS certificate signed by its Certificate Authority.  Right?  That's what they all do.  And the whole point of signing is that not a single byte of that certificate can be changed, or its signature will become invalid.



This means that when that web server wishes to include that Certificate Authority's recently received OCSP assurance about the certificate's validity, or its lack thereof, whatever the case may be, the Certificate Authority's signed OCSP status can only be appended to the certificate.  It cannot in any way modify or be incorporated into the certificate, right, because that would break the certificate's signature.  So that's why the term "stapling" has been adopted, since stapling is such a good analogy.



So the problem is, stapling an up-to-date OCSP status to a web server's certificate is optional.  If a bad guy gets hold of a valid web server certificate, they'll gladly staple any OCSP good news to that certificate every time they send it out to a web browser; right?  Since, as we know, that prevents browsers from looking any further.  If they've got a valid stapled good news OCSP certificate, or even as we saw last week with GRC sending out a bad news OCSP certificate, they don't look any further.



But once the certificate has been revoked by its Certificate Authority, and its Certificate Authority's OCSP now contains bad news, as I said, as GRC's revoked site certificate did last week), no bad guy would continue stapling that bad OCSP revocation news to their fraudulently obtained TLS web server certificate.  Instead, they'll remove any stapling and hope that no one's browser checks the current validity of the certificate by querying the Certificate Authority's OCSP service or their CRL, their Certificate Revocation List, directly themselves.  And so far as we know, browsers no longer make their own queries to OCSP on their own.  Actually, there's one exception standing out, but we'll get to that.



Okay.  So to prevent a stolen certificate from not having an OCSP response stapled to it, it's possible for a website that wants the best security for itself, and which always intends to staple, and can commit to stapling, to ask its Certificate Authority to include a flag built into its TLS certificate, which is known as "OCSP Must Staple."  Since that "Must Staple" flag becomes an integral part of the signed certificate itself, it is immutable and, once issued, can never be changed.  And the presence of that flag in any certificate that's received by a browser is an assertion that this certificate must only be honored and trusted if and when it's accompanied by a current and still valid OCSP assertion to that effect, talking about an updated status for the certificate.  An OCSP statement must be stapled to the certificate, or it must not be trusted.



So that solves the problem of the bad guys choosing not to staple any bad news to their stolen certificate, and just assuming, as they can, certainly for Chrome and most other browsers, that the browser won't go out of its way to check on its own.  With OCSP Must Staple, the browsers do not need to make a second query because they're being told if there isn't a stapled assertion, a recent, you know, currently unexpired assertion, that they cannot trust that certificate, specifically to deal with that case.



In my reply to GRC's newsgroup I wrote:  "Let's Encrypt, and all other CAs following the CA/Browser Forum, can mandate that they will be setting the 'OCSP Must Staple' requirement in every certificate they issue after some date certain.  And that will force all web servers to support stapling and to staple.  So what's the problem with that?"



With enforceable stapling - which we haven't had until we bring the OCSP Must Staple into it.  With enforceable stapling, after the stapling mandate were to take effect - if it was going to, but we already know this is not the direction the world has gone, but why not - every certificate will have OCSP stapled to it within 397 days once all pre-mandated certificates will have expired.  So the entire industry would move there within the maximum lifetime of a web browser certificate, which is now 397 days.  And the big win for privacy, which Let's Encrypt is saying they're all worried about all of a sudden, the big win for privacy is that with a fresh OCSP response stapled to every certificate, browsers can and will be inhibited from making their own queries for OCSP status because it's right there on the certificate.  So they'll be sped up, and they have no reason to ask any further.



Thus we have fast revocation notification with zero privacy risk, since browsers will get their updates from the server's cert; zero performance overhead for the same reason, no need to ask anyone else or look any further; and reduced load on CAs' OCSP services since only the servers they've issued certificates to will be querying them, not everyone's browser all over the place that are relying on those certificates.  And that querying interval can also be readily changed by the CA simply by changing the OCSP's response lifetime.  Could be made longer, could be made shorter, whatever, I mean, on an ongoing basis.  It's sort of a beautiful system.  So why isn't that what's being done?



After not coming up with any answer that I liked, I did some digging in the CA/Browser Forum's documents.  In the discussion surrounding that CA/Browser ballot measure SC-63 that we discussed briefly last week, the adoption of which was nearly unanimous, everybody's for it, I found this couple lines of text:  "Independent of usage statistics, relying parties cannot consistently depend on OCSP stapling for security unless responses are stapled on all connections."  Okay.  "Further, even if the web server ecosystem had improved support for OCSP-stapling, and we could require the use of the Must Staple extension, we'd remain dependent upon robust and highly reliable OCSP services, which have been an ongoing ecosystem challenge."



So what they're saying is that they recognize that the use of the Must Staple extension, which does solve this problem, and the privacy problem, and the load problem, also it creates a dependency, I mean, a serious dependency upon OCSP services being available.  And they're saying so far they've not been highly reliable.  I actually found a piece of intelligence from a project that Mozilla did showing that their browser gets 7% failures on OCSP queries.  So that's not good.



Okay.  Anyway, so that set of lines in the ballot measure gave me a clue.  So as I said, on the one hand, what they appear to be saying is, all experience to the contrary, unfortunately, when we looked at this 10 years ago, things sort of were like this.  And it doesn't seem that that much has changed in the intervening 10 years since we last looked at this.  They're suggesting that OCSP Must Staple cannot be used because the robustness of OCSP services has never been sufficient, and still isn't today.



So thinking about this, one thing that the plain vanilla certificate system offers, the one, you know, just in the old days, Certificate Authority, web server, browser, that original plain vanilla certificate system is no requirement for real-time communication with anyone other than between the client and server, which after all is connecting to the server, wants to make a connection.  The connection works.  Part of the handshake is the certificate exchange which the browser verifies.  So there's, like, zero overhead in that, and no other requirement for any other real-time communication.  The server has a signed certificate.  The client locates the server by its domain name using hopefully secure enough DNS.  And during the client's connection to the server, the server provides its certificate to prove its identity at that domain name.  It's an elegant system, and it is minimal.



Of course, the one place this beautifully minimal system falls down completely is when the certificate it's sending, which was signed, and it's got lots of life left in it, can no longer be trusted.  There's no way for the browser to know that within this minimal system that we originally had.  In the absence of any other facility, that certificate will be trusted until it expires, which in the case of Let's Encrypt will be a maximum of 90 days, or a maximum of 397 days for traditional now annual web browser TLS certificates.  If our goal is for this otherwise simple and elegantly minimal system to deal with the need to revoke certificate trust before the certificate's natural end of life, we're going to need to add something else.  And what we see is that the industry has been struggling from the beginning to come up with a solution that works well for everyone.



None of GRC's servers have had any problem with OCSP stapling.  And I frankly doubt that anyone's would.  And remember that the server starts looking a day in advance to update its stapled assertion.  So a brief outage would not be a problem.  Whereas Mozilla is probably referring to brief outages in their browser trying to do OCSP lookup on the fly, which, you know, it might be off now, but on again in five minutes.  Who knows?  If the CA was restarting their OCSP service at the moment, who knows?



Okay.  So if all of the sites which are using stapling now had Must Staple in their certs, but if they were, for some reason, if all of those servers were for some reason unable to obtain an update from their Certificate Authority during that last day of the OCSP assertions lifetime, they would effectively go offline because with OCSP Must Staple in their certificate, no web browser would trust their expired stapling.  Every affected website would look like GRC's deliberately revoked site looked after our podcast two weeks ago.  That's not good.



Browsers have given up their own checking of OCSP for performance reasons, and they've been absolved of any guilt by stating their concern over the privacy of their users, although that doesn't seem like such a big problem, but okay.  So they rely upon stapling to do the work for them when stapling is present, and they do nothing when it's not present.  But that means that any certificate that does not use Must Staple will always be vulnerable to long-term abuse if it's stolen, as I said, since no illegitimate server would include a negative OCSP response, which would cause all current web browsers to default to, if a negative response was there, it wouldn't work.  But not including a stapling would cause all current web browsers to default to trusting the malicious website.



And the industry cannot improve certificate revocation and user privacy by moving to Must Staple because a DDoS of the apparently still not very robust OCSP service, which would have then been a requirement for all a Certificate Authority's customers, would result in a widespread web server outage.  All if would take, say for example that some Certificate Authority, well, that the CA/Browser Forum said we're moving the entire industry to Must Staple, and everybody has that in their certificates, all the old certificates that, you know, a year goes by.  All the old certificates that didn't have the Must Staple bit got renewed, and now they have the Must Staple bit.  Stapling is now the thing we do.  Revocation is wonderful.



All it would take, and it doesn't take much imagination at this point to imagine that, you know, it would happen, baby, is a one-day DDoS on some CA's OCSP service.  All of the staplings that were trying to refresh themselves would not be able to get updated.  Their attestations of the certificates still being trusted would expire.  But because Must Staple was in the certificate, no web browsers would trust them.  And there would be growing, as the days went by, but it would start after a day, growing mass outage of all the servers that were trusting, that were using that given under-attack CA.



So it's clear that, by adding an online facet to this, no matter what we do, we have a problem.  I believe that this explains why we're not seeing OCSP going any further.  You know, unless stapling is mandatory, it can be bypassed simply by not including a stapled certificate.  And the danger of making stapling mandatory is that an OCSP outage, for whatever reason, malicious or accidental, lasting more than a day, would have devastating consequences as all of that Certificate Authority's certificates would over time become untrusted.



So where are we left?  Certificate Revocation Lists, imperfect as they are, are less "online" than OCSP which, after all, OCSP stands for Online Certificate Status Protocol.  The bottom line is that in the reality of today's Internet, "online" is not something that can be made to work.  Its strength is also its failure.



Okay.  So as I mentioned last week, I think I just sort of mentioned it in passing that I was thinking about maybe what would happen if I deliberately blocked all GRC access to DigiCert's always online for me OCSP Service.  I did that.  Before I did that, I double-checked that my certs did not for some reason have "OCSP Must Staple" enabled, or I would have, you know, put myself, I would have created the same outage I was just talking about.  



The last-received OCSP status from DigiCert for the revoked.grc.com site was set to expire around 2:00 a.m. this morning.  I did it a week ago.  And sure enough, when I checked this morning, GRC's revoked.grc.com server was no longer stapling that expired OCSP status to its thoroughly revoked - uh-huh, there it is, Leo - to its thoroughly revoked TLS certificate.  And what do you think happened?  Yep.



LEO:  Oh, boy.



STEVE:  Every web browser other than Firefox has resumed showing the revoked.grc.com website.  Chrome loves it, Safari loves it, Edge proudly shows its page.  Everyone's happy with the site despite the fact that its certificate was revoked 21 days ago.  Every web browser except Firefox is once again completely happy with the site.



LEO:  Well, good on Firefox.  Why did Firefox not...



STEVE:  Because, if you go, Leo, in Firefox to - you can do about:preferences#privacy, or open Firefox and go to Settings, and then Privacy and Security on the left, under the main topics, then scroll about two thirds of the way down.  And you will find a checkbox which to their endless credit is enabled by default.  It says:  "Query OCSP responder servers to confirm the current validity of certificates."



LEO:  And there it is.  It's revoked.



STEVE:  Yup.  Firefox is revoked.  And if you go into Settings, Privacy and Security, scroll two thirds of the way down, you'll see OCSP querying is enabled.  To verify, since it was on mine, I installed Firefox in a virgin Win10 VM this morning.  The first time it was installed.  Sure enough, it was enabled by default.  And there it is.  Query OCSP response servers.  If you turn that off and refresh the revoked page, comes right up.



LEO:  You're right.  So let's keep that on.



STEVE:  Yes.



LEO:  And what's funny is, despite all the concerns, I didn't notice anything slow with Firefox.  It works fine; right?



STEVE:  Yeah.  Yeah.  And we've always had it on.  Now, I don't know what percentage of servers were stapling.  Mine always was.  Mine isn't now.  Neither GRC.com nor www nor revoked are stapling because I'm preventing those servers from obtaining, unfortunately, in my version of IIS you're unable to disable that through configuration.  So I had to - I actually used my hosts file.  I just blackholed ocsp.digicert.com, and my servers were no longer able to obtain that status.  I just set it to 127.0.0.1, you know, the localhost IP.



So the reason for all this, we've now proven, is that the revoked.grc.com server is not stapling its negative OCSP response, you know, which said, "Oh, by the way, this site's certificate, the one I've just been stapling, you know, has been revoked.  It's not saying anything now."  And so in the absence of either a positive or negative OCSP status, all browsers other than Firefox trust the revoked, but otherwise valid, certificate.



In other words, here we are, 21 days after that certificate's revocation, presumably revoked - it may have been revoked for administrative reasons or because somebody stole it.  We know that's not the case here, but I'm simulating that.  Somebody, a bad actor, could have stolen any other site's certificate, and the site could have known about it, immediately revoked it with its Certificate Authority.  Chrome could care less.  Chrome doesn't know.  And we've had three weeks of this right now.



LEO:  That's not an accident.  That's Google assertion as they have always asserted that the whole system's broken.  So they're just going to ignore it.



STEVE:  Well, we know that for EV certs that they do some special looking.  And even Apple with iOS and presumably macOS, EV certs they care about.  So they maintain some sort of a certificate revocation list.  GRC's certs 10 years ago were EV, so I was seeing that treatment.  But like a lot of the rest of the world, when all the browsers stopped giving you any special treatment, you're just throwing your money away to have an EV cert.  So I switched back to domain validation certs which, you know, is where the web is going.



LEO:  Right.



STEVE:  So anyway, Firefox stands alone in correctly refusing to show the pages being served under the guise of this very well-revoked TLS certificate.  We'll see what happens.  I don't think this is going to change because it's not an EV cert.  Neither Chrome nor Apple are giving this any special attention.  And the entire revocation system, we are now seeing it function the way it really does, which is only if you ask the authority's OCSP server directly do you get an answer.  And Firefox is the only browser that does that.



LEO:  Wow.  Truly amazing.  You want to take a break?



STEVE:  Yup, thanks.



LEO:  Because Patch Tuesday there's a lot to talk about.



STEVE:  Ninety security vulnerabilities.



LEO:  Oh, man.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  But Leo, these are the last 90. 



LEO:  Oh.



STEVE:  They found them all.



LEO:  Oh, good.  That's it.  This is it.



STEVE:  Yes.



LEO:  No more Patch Tuesday.



STEVE:  Canceling September.



LEO:  It's all fixed.



STEVE:  Nothing to fix.



LEO:  Nice.



STEVE:  It's all working perfectly.



LEO:  It's about time.



STEVE:  Working perfectly.  It won't boot, but it's working perfectly.



LEO:  You know, that's the secret.  Just don't boot it.  I'm still waiting for my super-duper Snapdragon Windows Copilot developers machine.  I thought it was going to come last week.  But I'll ask Richard about it tomorrow.  But I guess I'll be running Windows 11 in here, too, so that'll be fun.  That'll be interesting.  I'll be able to Patch Tuesday along with the rest of you.  We now stream live to everywhere, including Steve's favorite platform, X.com.  Kidding.  Kidding.



But we do have hundreds of viewers every time we do this on X, so we welcome you.  YouTube.com/twit/live.  Twitch.tv/twit.  Facebook, LinkedIn, X.com, of course our Club TWiT Discord, and Kick.  Seven different ways to watch us live, every Tuesday, right after MacBreak Weekly, about 2:00 to 5:00 p.m., Pacific 5:00 to 8:00 Eastern time, 21:00 UTC.  Make sure you watch live.  But even if you do, subscribe, because you're going to want that library - right, Steve? - of great Security Now! episodes, 19 years' worth.  Download them all.  Collect all 988. 



STEVE:  Yeah, and, you know, we don't mention often enough, and our newer listeners probably don't know, that back then we were sort of still in the early knowledge dump phase of the podcast.  I did a series, several series.  One of like five or six was how the Internet works.  And then there was the other one was how CPUs, like, you know, how computing technology works.  And we've had many people who've, like, created like box sets of those, I mean, like...



LEO:  Steve's premise was you're going to need the foundational knowledge before you can understand what we're going to be talking about.  So, yeah, I mean, that was, I don't know, in the first hundred episodes, I think.



STEVE:  Yeah.



LEO:  But it's all there at TWiT.tv/sn.  You can go back, back, back, back, all the way.



STEVE:  Got to dig.  Got to dig back a little ways.



LEO:  People have written scripts to scrape it and all that stuff.  We don't make it that easy, I understand.  But I don't know, if I can find those scripts, I'll dig them up.  Otherwise you can just download.



STEVE:  And then of course we have the ever-favorite Portable Dog Killer episode.



LEO:  Yes.  Which was our, weirdly enough, our Christmas episode for many years.



STEVE:  No animals were hurt during the production.



LEO:  There's a lot of history in this show.  I told you that Burke brought in the modern-day version of this.  It looks like a bird house.  You're supposed to hang it on your fence.  If you've got a neighbor's dog that barks, it senses the barks and sends out a tone that only dogs can hear, just like your Portable Dog Killer.  But it's disguised so your neighbors don't know why your dog suddenly goes agh.  What was that?  All right.  On we go.  Let's get this...



STEVE:  It's like putting it like a secret bark collar on your neighbor's dog.  Yeah.



LEO:  It's kind of like that.  That's exactly what it is.  When they bark, they go, ooh, I don't like that.  All right.



STEVE:  Okay.  So Patch Tuesday.  Last Tuesday was August's Patch Tuesday for Microsoft.  And just so that everyone knows, I was not serious about all the bugs being found because that doesn't seem to be a problem that Microsoft has of worrying, like, are we going to hold Patch Tuesday this week or not?



LEO:  This will go on for years.



STEVE:  Or this month.  No.  They're having it.  Okay.  So it's become sort of standard for the second Tuesday of the month for Microsoft and for many other publishers who also appear unable to ever get the important bugs out of their code.  In this month's installment of trying some more, Microsoft released updates to fix at least 90, nine zero, security vulnerabilities in Windows and their other software, which included a startling six zero-day flaws that were, or maybe are still, being actively exploited by attackers.  Flaws were found and fixed in Office, .NET, Visual Studio, Azure, Copilot, Microsoft Dynamics, Teams, Secure Boot, and of course Windows.



Among the six zero-days fixed this month, half of them, thus three, are local privilege escalation vulnerabilities which, you know, while when we talk about them they are severe, inasmuch as they really enable an existing attack to be made much worse by giving somebody who's already managed to get inside a machine the system-level root privileges that they need in order to do, you know, to get up to much more mischief.  So we know little about these, although you can bet that would-be attackers are hard at work reverse engineering the changes that Microsoft shipped in order to figure out what was going on before and put them to nefarious use for systems that haven't yet been patched, you know, when they're already able to get in.



There are, however, some worse problems.  A remote code execution vulnerability when Microsoft's Edge browser is operating in Internet Explorer Mode.  Although IE mode is not enabled by default in Edge, thank goodness, the fact that this is or was being actively exploited, like today, suggests that there are occasions where an attacker can arrange to either enable it somehow or has identified a user or an organization who has enabled this, probably because they've got some very backward compatibility need, back to the last version of IE.



So, you know, it makes - turning that on and using it makes Edge look like IE.  And I'm trying to think why I did that the other day.  There was something I was doing forensically.  It might have been back when I was messing with the cookie system again.  But I had some need to do something with IE mode in Edge.  And, you know, they don't make it easy.  But there are people who do need it.  And so the problem is that if you're using it, there's a remote code execution vulnerability, believe it or not.  So it's good that it's not on by default.



Another zero-day is a bypass in their "Mark of the Web," which we've talked about extensively before, security feature, which causes Windows to be far more mistrustful of any files obtained from the Internet.  As we've seen in the past, however, this MOTW bypass is always used as a part of a larger exploit chain, but its bypass does enable something to be done that was supposed to be impossible.  And it's not in the user's best interest, whatever that was.



This month's third and final zero-day is a remote code execution flaw in Microsoft Project.  Microsoft and several security firms have pointed out correctly that this vulnerability is only useful against users who had previously disabled notifications about security risks of running VBA Macros in Microsoft Project.  So, you know, Project has been a source of lots of problems.  So Microsoft just puts up a note and says, are you sure you want to run a macro?  By the way, did you know one is running, or wants to?  And many people go, what?  Who's doing that?



So those are the six zero-day flaws out of the total kettle of 90.  And they were under active use a week ago when Microsoft patched them.  Hopefully, you know, again, we know that patching in a timely manner is something that has bitten enterprises in the past.  So some enterprises are reluctant to do so because they don't want to have, like, apps that they depend upon stop working.  This is an instance where, depending upon your profile, you may want to get caught up with updates.  Sounds like it would be a good idea.



And Leo, I mentioned that you caught my attention at the end of MacBreak Weekly, which I was listening to as we cross over into this show.  Thanks to a listener of ours, Larry Deniston, who brought this to my attention, I have some news that might be of interest to both our old-timer listeners and our younger audience, who may have heard the names of these people who've in many cases grown to become somewhat legendary within the PC industry that we all love.  Audiotapes of a mid-'80s, 1980s technology radio show, which was known as "The Famous Computer Caf," were found earlier this year by an archivist who restored and digitized them.



Yesterday, Monday, the Internet Archive posted.  They wrote:  "A previously lost cache of celebrity and historical interviews from a long-dormant radio show have been discovered, digitized, and made available for all.  The Internet Archive is now home to 53 episodes of The Famous Computer Caf, a 1980s radio show about the new world" - new at the time - "of home computers.  The program included computer industry news, product reviews, and interviews, and aired from 1983 through 1986 on radio stations in Southern and Central California.



"The creators of The Famous Computer Caf saved every episode on reel-to-reel tapes, but over the years the tapes were forgotten, and ultimately lost.  Earlier this year, archivist Kay Savetz recovered several of the tapes in a property sale and, recognizing their value and worthiness of professional transfer, launched a GoFundMe to have them digitized and made them available at Internet Archive with the permission of the show's creators.



"While full of time-capsule descriptions of 1980s technology news, the most exciting aspect of the show has been the variety and uniqueness of the interviews," they wrote.  "The list of people that the show interviewed is a who's-who of tech luminaries of the 1980s - computer people, musicians, publishers, philosophers, journalists.  Interviews in the recovered recordings include Timothy Leary, Douglas Adams" - of course "Hitchhiker's Guide to the Galaxy" fame - Bill Gates, Atari's Jack Tramiel, Apple's Bill Atkinson, and dozens of others.  The recovered shows span November 17, 1984 through July 12, 1985.



LEO:  Isn't that cool.  So cool.



STEVE:  So for ease of access, I've made this GRC's shortcut of the week for this episode, which this episode is 988.  So if you go to grc.sc/988, that will jump your browser to the Internet Archive's blog posting, which includes a link to a Google Docs spreadsheet listing all 53 recordings, who's on them, and then their direct links to their Archive page at the Internet Archive.  So very cool.



LEO:  Really neat stuff.



STEVE:  Yeah.



LEO:  You know, I wish I had archives of all of the shows we did, Dvorak and I and so forth.  It's great that they saved so many of them.  And it's great that they were saved.  The problem with those reel-to-reels is they're going to die in a few years.



STEVE:  Yeah, you get cross-wrap imprinting and, you know, heat and...



LEO:  They flake, yeah.



STEVE:  ...humidity and, yeah.  And in many cases the magnetic coating flakes off of the plastic backing.  Or Mylar, rather.



LEO:  So kudos to Kay Savetz and to the Internet Archive for preserving and distributing these.  That's really great, yeah.



STEVE:  A quick follow-up on GRC's newest IsBootSecure freeware.  Last Wednesday I posted Release 2, which added keyboard accelerators and tooltips to the buttons, and I fixed a noncritical misreporting edge case.  Then Release 3 later the same day, last Wednesday.  It added a "silent" option which causes IsBootSecure to fully suppress its user interface.  So it runs silently on any Windows machine.  It examines the machine and immediately exits with an exit code that Batch or PowerShell scripting is able to capture and check.  This allows the app to be deployed by scripts within an organization, organization-wide, to check and inventory an entire enterprise's inventory of PCs to ascertain their boot-time status.  And not just do they have a mistrusted platform key or not, but you can quickly see whether those machines are booting with Secure Boot enabled or not.  So it returns a status zero through seven, which encompasses all the various things that it might find.



And with that finished, I got back to work on SpinRite 6.1's documentation.  I've finished all of the static content, and I'm now down to producing the video walkthroughs which I'm looking forward to creating so that non-owners will have a sense for actually, like, watching SpinRite run while it works.  So that's going to be fun.



And I'm also very pleased to report that GRC's email system is working very well.  I sent out 8,443 announcements about this podcast a couple hours ago.  Only five were bounced as undeliverable for some reason.  So, you know, five out of 8,443...



LEO:  Pretty darn good.  Yeah.



STEVE:  ...I'm delighted with.  I did also want to mention that, and I'm sure some people have found out for themselves, that I finally started last week bouncing any email, any incoming email which had not been registered with GRC beforehand.  Anything that is sent to securitynow@grc.com, rather than being redirected into a separate folder where I'm able to monitor it as I had been, is now bouncing back.  For the first couple months I just wanted to help people who were sending things through.  So I would collect a bunch and then just send them back a notice explaining that, well, this would have gone into our, you know, like bounced into our spam and never be seen or sent back.  So please figure out, like, why.



Mostly people registered one account and then sent from another.  Like I could see their account name registered with Proton Mail, but they were sending from iCloud.com.  So it's like, okay, you need to send me from the one that you registered, or that's not going to work.



And I also did get some complaints from people who look around for the email address to send to, and they are pissed off when they can't find it anywhere.  I understand the annoyance, but I want to keep this more or less just between us.  You know, I'm not advertising the securitynow@grc.com account anywhere on GRC.com's site.  It's just for, you know, us insiders.  So securitynow@grc.com, and you're not going to find it written down anywhere.



Michael French said:  "Hi, Steve.  I'm stumped on how to get information through WiFi firewalls that block everything except HTTP on port 80 and HTTPS on port 443.  I run an OpenVPN server at home in Alabama on TCP port 443."  So, right, just looks like a web server.  He said:  "But some firewalls outside of my home still block my communicating with it after only a few seconds of operation, presumably from their implementing deep packet inspection."  He said:  "I just returned from Europe and found that all the countries I visited (UK, the Netherlands, Belgium, and France) have rigged their WiFi firewalls to block everything except HTTP and HTTPS.  I would sure appreciate your suggestions on how to get communications through these over-restrictive WiFi firewalls.  Thanks, Mike."



Okay, so this was an interesting puzzle.  It cannot be deep packet inspection unless Michael had been installing and trusting a certificate from the various access points he was using, which I'm sure was not the case.  Any TLS connection being made to port 443 will be 100% opaque to anyone monitoring the packets.  They'll see what looks like a normal connection to a remote web server with a back-and-forth handshake, but only from the outside.  They will have no way of knowing what's going on inside.



My best guess about what might be going on comes from Michael's comment that he's getting disconnected after only a few seconds of operation.  So first of all, I should say I'm wondering why it's consistent, why his experience is so consistent in the UK, the Netherlands, Belgium, and France.  I mean, that's a little suspicious to me that he's seeing, like, the same thing from so many different places.  It makes me think it's something more about what's happening at his end.



But although persistent HTTP connections between a web browser and web server can be made and sustained, even then they're usually dropped after all pending queries and replies have been exchanged.  It's unusual for a passive and unused connection to be maintained.  So it might very well be that these WiFi access points are watching the flow of traffic and are deliberately dropping any connections that go idle.  Doing that would not interfere with normal web browser use, while it would be an effective way of blocking other "web-like" traffic such as an HTTPS VPN.  So that's my thought, Mike, about what might be going on.



The problem is the VPN doesn't, even from the outside, the flow of packet traffic does not look like an actual browser web server interchange, which is generally a burst of stuff and then the connection goes away.  You know, you're having probably a brief burst, and then you basically have a remote network connection, and something might be looking at the packets going, even without seeing into them, just the size and, you know, the timing of them, and decide this doesn't look like a web server and a web browser.  We're going to bail.



Doug White said:  "Aloha, Steve.  Listened to the Tuesday podcast and thought I'd mention something when it came to transferring DNS.  I switched over to Hover from GoDaddy, something I've put off for years, and wanted to mention that the DNS server entries after the transfer to Hover still pointed at the GoDaddy DNS servers.  I had to look to find the Hover DNS server names and replace the GoDaddy entries in the Hover settings.  I'm guessing it's so that everything still works after the switchover, but I wasn't alerted to the fact, that I'm aware of, that I needed to make that change.  Cheers."



And I'm sure that Doug's correct that in switching his domain registrar from GoDaddy to Hover, Hover would have examined his previous registrar's domain nameserver entries and would have deliberately left them, copying them into his relocated registration at Hover.  Eventually, presumably, GoDaddy could be expected to suspend their support for his DNS once his registration had been relocated, so this would have probably eventually come to light, like stuff would have stopped working for him.  So after moving to a new domain registrar, the point he's making, and it's a good one, which is why I wanted to share it, you'll want to make sure that its registered nameserver records are pointing where you intend to have them be pointing, which is probably to the DNS server offered by that registrar.



Scott asked:  "Is there an advantage of a seven-day stapled OCSP attestation over a TLS certificate with a seven-day expiration?  With certificate automation, there's no reason an expiration needs to be 30 days or a year or a week.  If seven days is enough time to catch a revocation, just expire the cert that quickly.  Revocation," he says, "only really seems like it makes sense if it's instant."



So what I believe we've clearly seen now is just how much all of this revocation and certificate life business involves a tradeoff.  Let's Encrypt has been automated from day one, but they chose 90 days for their certificates, when they could have chosen seven days.  Why?  One advantage to 90 days, especially when they were starting out in the beginning, was that it would significantly reduce the load on their certificate issuance and delivery infrastructure by a factor of nearly 13, from between nine and 70 days.  But it's true that so long as a network outage or attack would not hold Let's Encrypt off the air while their certificates were expiring and were unable then to renew, then their certificate recycle time could be as short as they like.



But I know that if my servers were using Let's Encrypt certs, instead of DigiCert's 397-day life certificates, I would want as much life per certificate as I could get, you know, just for the safety margin that that provides.  Because one thing that has happened to our industry is that you're no longer very effective on the web if you don't have a valid TLS certificate.  You know, browsers actually, depending upon which one, you can normally force them past an expired cert.



You know, I'm sure that those of us who surf to less-often-visited sites will sometime encounter a site that says, oh, this can't be trusted, the certificate expired.  If you look at it, it expired yesterday.  So it's like, okay, you know, the guy who's in charge of that is on vacation; or, you know, it's an unimportant server that hasn't come to the person's attention yet.  So it's like, okay, fine.  And then you push past all these warnings and bewares and cautions, and then you get to the site anyway.  But still, as I said, you're going to see your traffic fall off for sure if your site starts serving an expired certificate.



And in a final different twist on expiration, Brandon Foust sent a very short note.  His email just said:  "Will it boot on 6/2/2031?"  And he sent me a picture of his Dell something computer's UEFI platform key where it shows it's not one of the bad ones, it's issued to Dell Inc. Platform Key, issued by Dell Inc. Platform Key, and it says "Valid from 6/1/2016 to 6/1/2031."  So he says, what about the next day, 6/2/2031?  So he is wondering what happens on June 2nd seven years from now.  And the answer is that in this instance the date does not matter.  It would be up to someone deciding whether or not to trust the root certificate based upon its date, and it's the UEFI firmware that's using this platform certificate to check the signatures of the other signatures.  So it's the root, and it has signed other certificates in its database.



So distrusting the platform key root certificate would require a deliberate act within the UEFI firmware, which is not something that it cares about or would do.  So the only reason we have a "not valid before" and "not valid after" date is because certificates have to have them in order to be valid certificates.  So they just put something in there.  As it is, it's a 15-year certificate, so they weren't in any hurry to expire it.



Okay.  And Leo, let's take a break, and we're going to talk about, oh, boy, the clearly most worrisome data breach that we have had yet.



LEO:  All right, Steve.  We're going to get to the big, big story that everybody's been talking about, the National Public Data Breach.



STEVE:  For good reason.  Wow.



LEO:  I'm really curious what you have to say about this because there's quite a bit of kind of back-and-forth controversy over exactly what was revealed.  There seems to be a lot of errors in the data.  Troy Hunt had a long post that made me more confused, frankly.



STEVE:  That's what we're going to share, actually.  I've edited Troy's post in order to bring his information to us.



LEO:  And obviously it's on the Pentester site, which is a great way to find your data and my data and everybody else's data.  Yeah, soon as I entered my name, and it didn't need much.  You know my birthday.  That's all you needed.  You found my father's information, my addresses, it was - this is everything that these creeps at National Public Data were selling.



STEVE:  Yup.



LEO:  Now it's free.  Congratulations.



STEVE:  Grc.sc/npd, for those who are listening live, grc.sc/npd, National Public Data.



LEO:  And we will get to that in just a bit.  But first, let's talk about our sponsor for this segment on Security Now!.  And actually our next sponsor is very appropriate, and you'll want to stay tuned for that one, too.



All right, Steve.  We are really ready for this one.  I am, I'm all ears.  And as you told us earlier before the show, you emailed me, you said, if you've got DeleteMe on this show today, this would be a good day to have them.  We will.  We'll follow up with a special offer from DeleteMe.



STEVE:  There was something that surprised me, I think it'll surprise our listeners, too, which it was an observation that Troy Hunt made.  Troy, of course, is famous for his Have I Been Pwned, HIBP site.



LEO:  Right.



STEVE:  And so we'll talk about that.  I was very impressed.  But before we wrap up today's podcast, and I mentioned this already just before you told us about ThreatLocker, Leo, I'm going to provide everyone listening with a URL for a searchable online database containing the records from this breach, or at least a subset of them, but the ones that make sense.  You enter your first and last full legal name, your state of residence - for example, a nickname doesn't work - your state of residence and the year of your birth.  And you'll be, and I mean everyone of our, you know, U.S., Canadian, and UK apparently, as we'll see, that seems to be where this list is most focused, will be immediately presented with a list of all the people who share your name, state, and date of birth.



You'll find that the list is sorted by middle name or initial, if you have one, and you'll almost certainly find yourself listed there, often redundantly, many times at different physical addresses, all which will be familiar to you because they will be correct, often with your telephone number, and always with the correct last two digits of your otherwise redacted full Social Security Number.



LEO:  That's what scared me.



STEVE:  It should.



LEO:  And they were correct.  They were correct.



STEVE:  This is as real as it gets.  And for that reason I'm going to start out this week with the main takeaway from what we're going to be describing, which, you know, is probably accurately by those who would know, they're describing it as the largest data breach in history.  I would argue that it is clearly the most critical to people data breach we've seen so far.  And the takeaway is, we've spoken previously, a number of times, about the need to freeze credit reporting at the three primary credit reporting agencies.  If that previous coverage was still insufficient to motivate you, your friends, or your family to take the steps to do so, then if this one doesn't do the trick, I'm pretty sure nothing ever will because this is the pay dirt, unfortunately, for the bad guys.



So naturally this has been a big headline grabber.  The Verge's coverage was headlined "The weirdest '3 billion people' data breach ever."



LEO:  Yeah.



STEVE:  BleepingComputer covered this under their headline "Hackers leak 2.7 billion data records with Social Security Numbers."  Brian Krebs' piece was titled:  "NationalPublicData.com Hack Exposes a Nation's Data."  And if it wasn't too long to be the title of today's podcast, Leo, I so much would have loved to use the start of National Public Data's own admission which read:  "There appears to have been a security incident."



LEO:  Yes.



STEVE:  Gee.  Ya think?



LEO:  Wow.  Talk about understatement.  Holy cow.



STEVE:  Wow.  Yeah.  So from across all the coverage, the person who's probably better suited than anyone else to put all of the pieces together, while providing some perspective from his years of involvement with exactly these sorts of incidents, is, as we mentioned, the known to us as "Have I Been Pwned?" Troy Hunt.



And indeed, Troy's coverage is the best I've seen anywhere.  He titled his write-up "Inside the 3 Billion People National Public Data Breach."  And he wrote, which I've edited for the podcast, he said:  "I decided to write this post because there's no concise way to explain the nuances of what's being described as one of the largest data breaches ever.  Usually it's easy to articulate a data breach, a service people provide their information to have someone snag it through an act of unauthorized access and publish a discrete corpus of information that can be attributed back to that source."  And I should mention, and you'll hear this throughout Troy's posting, he's really focused on attribution.  He wants to understand where the data came from.



He writes:  "But in the case of National Public Data, we're talking about a data aggregator most people had never heard of, where a 'threat actor' has published various partial sets of data with no clear way to attribute it back to its source.  And National Public Data is already the subject of a class action lawsuit, to add yet another variable to the mix."  And I'll just interrupt to note that Bloomberg Law reported that this first case is Hofmann v. Jerico Pictures, Inc., which was filed - a suit that was filed in the Southern District of Florida.  And they're reporting a couple interesting little tidbits that I wanted to share.



They wrote:  "Jerico Pictures Inc., a background-check company doing business as National Public Data, exposed the personal information of nearly three billion individuals in an April data breach, a proposed class action alleges.  On April 8th, a cybercriminal group by the name of USDoD posted a database entitled 'National Public Data' on a dark web forum, claiming to have the personal data of 2.9 billion people, according to the complaint filed Thursday in the U.S. District Court for the Southern District of Florida, which said the group put the database up for sale for $3.5 million.  If confirmed, the breach could be among the biggest ever, in terms of the number of individuals affected.  It's unclear exactly when or how the breach occurred, according to the complaint, and the provider still hasn't provided notice or warning to affected individuals as of the filing.



"The complaint said, to conduct its business, National Public Data scrapes the personally identifying information of billions of individuals from non-public sources, meaning plaintiffs didn't knowingly provide the data to the company.  Some of the information exposed includes Social Security Numbers, current and past addresses spanning decades, full names, information about relatives - including some deceased for nearly two decades - and more, according to the complaint.  National Public Data did not immediately respond to a request for comment."  So, you know, that is, other than their post, which says "There appears to have been a security incident."



LEO:  We're not sure what the hell happened.



STEVE:  Yeah, you know, we're looking into it.  We'll let you know if we think we have something more to share.  Anyway, they said, Bloomberg said:  "Named plaintiff Christopher Hofmann, a California resident, said he received a notification from his identity-theft protection service provider on July 24th, notifying him that his data was exposed in a breach and leaked on the dark web.  He accused National Public Data of negligence, unjust enrichment, breaches of fiduciary duty, and third-party beneficiary contract.



"Hofmann asked the court to require National Public Data to purge the personal information of all the individuals affected" - whoops, well, that's not going to happen, that's their entire life.



LEO:  It's a little late for that anyway.  It got out; right?



STEVE:  Yeah, yeah.  "And to encrypt all data collected going forward.  In addition to monetary relief, he also asked for a series of requirements, including that National Public Data segment data, conduct database scanning, implement a threat-management program, and appoint a third-party assessor to conduct an evaluation of its cybersecurity frameworks annually for the next 10 years."  Okay.  So that's the first of likely many similar...



LEO:  I think there's eight at last count.  So more to come, of course.  [Crosstalk] on this one.



STEVE:  Okay, yeah.  Wow.  Of course, I mean, this is obviously going to happen.  Everyone's going to jump in.  Okay.  So let's see what more Troy Hunt has for us.  He continues.  He said:  "I've been collating information related to this incident over the past couple of months, so let me talk about what's known about the incident, what data is circulating, and what remains a bit of a mystery.



"Let's start with the easy bit.  Who is National Public Data (NPD)?  They're what we refer to as a 'data aggregator,' that is, they provide services based on the large volumes of personal information they hold.  The front page of their website says:  'Criminal Records, Background Checks, and more.  Our services are currently used by investigators, background check websites, data resellers, mobile apps, applications, and more.'"



He says:  "There are many legally operating data aggregators out there, and there are many that end up with their data in Have I Been Pwned, for example, Master Deeds, Exactis, and Adapt, to name a few.  In April we," he says, "started seeing news of National Public Data and billions of breached records, with one of the first references coming from the Dark Web Intelligence account, which is @DailyDarkWeb."  So he quotes a tweet from April 8th in his posting.  The tweet is headlined "USDoD Allegedly Breached National Public Data Database, Selling 2.9 Billion Records."



In the show notes I have a picture of this, and basically which is the standard, we've got something for sale.  And in this case, you know, they describe it loosely.  They give a sample Social Security Number and a person and a way to verify, a means of looking it up.  And they say 200GB compressed, 4TB uncompressed.  And it says "includes USA, UK, and CA," you know, Canada.  And for the price, this is where they're asking $3.5 million for this treasure trove of data.



Okay.  So Troy says:  "Back then, the breach was attributed to USDoD, a name to remember as you'll see that throughout this post.  And," he says, "this is the first reference to the 2.9 billion number we've subsequently seen flashed all over the press, and it's right there alongside the request for $3.5 million for the data.  Clearly," he says, "there is a financial motive involved here, so keep that in mind as we dig further into the story.  The image also refers to 200GB of compressed data that expands out to 4TB when uncompressed, but that's not what initially caught my eye," he says.



"Instead, something quite obvious in the embedded image doesn't add up.  If this data is 'the entire population of USA, CA and UK,'" he says, "which is around 450 million people in total, what's the 2.9 billion number we keep seeing?  Because that doesn't reconcile with reports about 'nearly three billion people' with the Social Security Numbers exposed.  Further, Social Security Numbers," he notes, "are a rather American construct, with Canada having SINs (Social Insurance Numbers), and the UK having, well," he says, "NI (National Insurance) numbers are probably the closest equivalent."  He says:  "This is the constant theme you'll read about in this post, stuff just being a bit off.  But hyperbole is often the theme with incidents like this, so let's take the headlines with a grain of salt and see what the data tells us."



He said:  "I was first sent data allegedly sourced from NPD in early June.  The corpus I received reconciled with what vx-underground reported on around the same time."  And he said:  "Note their reference to the 8th of April, which also lines up with the previous tweet."  He said:  "On June 1st, vx-underground tweeted:  'April 8th, 2024, a Threat Actor operating under the moniker USDoD placed a large dataset up for sale on Breached titled "National Public Data.'  They claimed it contained 2.9 billion records on United States citizens.  They put the data up for sale for $3,500,000.'"



So Troy says:  "In their message" - that is, vx-underground's, "they refer to having received data totaling 277.1GB uncompressed, which aligns with the sum total of the two files I'd received," he wrote.  "These also mentioned the data contains first and last names, addresses, and Social Security Numbers, all of which appear in the first file among other fields.  These first rows also line up precisely with the post Dark Web Intelligence included in the earlier tweet.  And in case you're looking at the data and think 'that's the same SSN repeated across multiple rows with different names,' those records are all the same people, just with the names represented in different orders and with different physical addresses, typically all in the same city.



"In other words, multiple rows, in one case six rows all represent one person," he said, "which got me thinking about the ratio of rows to distinct numbers.  Being curious, I took 100 million samples and found that only 31% of the rows had unique Social Security Numbers.  So extrapolating that out, 2.9 billion would be more like 899 million."  He said:  "This is something to always be conscious of when you read headline numbers like 2.9 billion.  Doesn't necessarily mean 2.9 billion people, it often means rows of data.  Speaking of which, those two files contain 1,698,302,004 and 997,379,506 rows respectively..."



LEO:  Math is hard.



STEVE:  "...for a combined total," yes, "for a combined total of 2.696 billion rows."  So that's where the headline numbers come from; right?  It's, you know, it's very close.  So that's the number of total records in this total dataset.  He said:  "And in this story there's no question that there is legitimate data in there.  From the aforementioned BleepingComputer story:  'Numerous people have confirmed to us,' meaning to the Bleeping Computer guys, 'that it included them and their family members' legitimate information, including those who are deceased.  And in vx-underground's tweet, they mentioned:  'It also allowed us to find their parents and nearest siblings.'"



LEO:  Yeah, my dad's in here.  But I have to say all the data is pretty old.



STEVE:  Right.



LEO:  Yeah.



STEVE:  Right, right.



LEO:  You can tell it's from an earlier collection.



STEVE:  Unfortunately, the data that matters, like Social Security Numbers, never change.



LEO:  Yeah, never change.  Yeah.



STEVE:  Yeah.  So he said, anyway, "We were able to identify someone's parents, deceased relatives, uncles, aunts, and cousins.  Additionally, we can confirm this database also contains information on individuals who are deceased.  Some individuals have been deceased for nearly two decades."



And then, in his posting, Troy wrote, he said:  "A quick tangential observation in the same tweet:  The database DOES NOT [all caps] DOES NOT" - that's not my emphasis.  That was in Troy's posting.  "The database DOES NOT contain information from individuals who use data opt-out services."



LEO:  Oh.



STEVE:  "Every person," he wrote, "who used some sort of data opt-out service was not present."



LEO:  Oh ho ho.



STEVE:  Now, Leo, this is such an obvious lay-up for one of this network's sponsors that now would be a good time to pause.



LEO:  Should I talk a little bit about our sponsor?  Wow.  That is...



STEVE:  Isn't that something?  I had no idea that it would be that effective.



LEO:  Let me just go back to npd.pentester.com and look for somebody I know who uses DeleteMe, and just see - oh.  Oh.  Very interesting.  But, see, you never lived in San Diego or Ventura, did you?  It's got somebody by your name, but it's not you.  Wow.



STEVE:  And if it doesn't have about 10 records for her - because any adult of our age will have left a history behind.



LEO:  Yeah.



STEVE:  You have a bunch.  I have a bunch.



LEO:  I had a bunch.  But Lisa, who's been using DeleteMe for some time, is not in this database.  Holy camoly.  There is a Lisa Laporte who lives in Ventura and San Diego, I'm sorry, dear, but not my Lisa Laporte.  Holy - I don't want to show this other Lisa Laporte's information on the air.



STEVE:  Everybody can see it.  They just go to...



LEO:  They just go look for it, yeah.



STEVE:  Yeah.  I mean, that's what's unnerving.



LEO:  And she is not in there.  That is really - I'm glad you pointed this out, Steve.  That's kind of amazing.



STEVE:  It is.  It's...



LEO:  It really - so who said that?  Oh, I'm showing it now, wait a minute.  Let me - I don't know.  Turn that off.  Thank you.  Anyway, that's a remarkable fact, that if you were using - and it doesn't mean just DeleteMe.  Presumably any reputable data removal agency you wouldn't be in this breach.  Oh, that's something.



STEVE:  Yeah.  It's a big deal.  That's really...



LEO:  It's huge.



STEVE:  That's really huge.



LEO:  Yeah.



STEVE:  Okay.  So, but the point Troy was also making about all data being absent from those using data opt-out services was interesting.  He followed up by writing:  "This is what you'd expect from a legally operating data aggregator service."



LEO:  Yeah.  This is what they do, yeah.



STEVE:  Well, but a legally operating data aggregator service does honor deletion requests.  So he says:  "It's a minor point, but it does support my claim that the data came from NPD."  In other words, you know, if this was all dark web data aggregation that didn't actually come from NPD, then this sort of DeleteMe-style data opt-out wouldn't have had an effect.  It did, which means that it came from some legally operating data aggregator that, you know, that was breached.



LEO:  Is that the irony of this?  We know it's real because they're legal.  So awful.



STEVE:  I know.  So he says:  "None of the data discussed so far contains email addresses."  Now, we should note Troy has a strong email bias because that's what Have I Been Pwned keys on.  So he talks about email a lot, and it matters to him.  He says:  "None of the data discussed so far contains email addresses.  That doesn't necessarily make it any less impactful for those involved" - certainly doesn't for me - "but it's an important point I'll come back to later as it relates to Have I Been Pwned."



He says:  "So this data appeared in limited circulation as early as three months ago.  It contains a huge amount of personal information," he says, "even if it isn't actually 2.9 billion different people."  And we know it's not because you and I, Leo, are represented there 10 times.



LEO:  Oh, yeah, I'm in 20 rows, yeah.  Yeah, yeah.



STEVE:  Yeah.  He says:  "And then, to make matters worse, it was posted publicly last week."  Then he quotes a tweet:  "On August 6th, the Wolf Technology Group tweeted:  'National Public Data, a service by Jerico Pictures Inc., suffered a massive breach.  Hacker 'Fenice' [F-E-N-I-C-E] leaked 2.9 billion records" - now, notice here.  Here, like, when the techies talk, you get, like, records.  Well, as soon as, you know, like the non-technical press gets it, it turns into people.  It's like, no - "...leaked 2.9 billion records with personal details, including full names, addresses, and Social Security Numbers in plain text."  It's worth reminding people, Leo, that even though the NPD Pentester site that I linked to through grc.sc/npd, even though it politely blanks out, it redacts all but the last two digits of Social Security Numbers, they're all there in the data.  This is just for the web display that those are asterisked out.



LEO:  I can vouch that those two numbers that they do reveal are correct, for me anyway.



STEVE:  Exactly, and for me as well.  So, he said:  "The breach poses significant risks for identity theft and financial fraud.  Jerico Pictures Inc. faces potential lawsuits and legal challenges due to the incident."



So Troy writes:  "Who knows who 'Fenice' is, and what role they play; but clearly multiple parties had access to this data well in advance of last week."  He said:  "I've reviewed what they posted, and it aligns with what I was sent two months ago, which is bad.  But on the flipside, at least it allowed services designed to protect data breach victims to get notices out to them.  Inevitably, breaches of this nature result in legal action, which, as I mentioned in the opening, began immediately a couple of weeks ago.  It looks like a tip-off from a data protection service was enough for someone to bring a case against NPD.



"Up to this point, pretty much everything lines up, but for one thing:  Where is the 4TB of data?  And this is where it gets messy as we're now into the territory of 'partial' data.  For example..."



LEO:  See, this is one thing I wanted to ask.  Does this mean this NPD Pentester site bought the - where did they get the data?  Did they buy it?



STEVE:  It's a good question, although, well, what we do know is that I think later down here we're going to see it was then posted publicly.



LEO:  Oh, the whole thing.  Okay.



STEVE:  So it's now - that's the worst news is no one has to buy, you don't have to be on the underground.



LEO:  Everybody's got it.



STEVE:  It's free, baby.  Yeah.  Yeah.  So he says, and this is where it gets messy, is we're now into the territory of partial data.  And this drives Troy nuts.  "For example, an 80GB corpus was recently posted to a popular hacking forum.  While it's not clear whether that's the size of the compressed or extracted archive, either way it's still a long way short of the full alleged 4TB.  Earlier this month, a 27-part corpus of data alleged to have come from NPD was posted to Telegram.  The compressed archive files totaled 104GB and contained what feels like a fairly random collection of data.  Many of these files are archives themselves, with many of those containing yet more archives."



He says:  "I went through and recursively extracted everything which resulted in a total corpus of 642GB of uncompressed data across more than 1,000 files.  If this is 'partial,' what was the story with the 80GB 'partial' from last month?  Who knows.  But in those files were 134 million unique email addresses."  Now, that's what Troy likes because he wants to put that into HIBP.



He said:  "Just to take stock of where we are, we've got the first set of Social Security Number data, which is legitimate and contains no email addresses, yet is allegedly only a small part of the total NPD corpus.  Then we've got this second set of data which is larger and has tens of millions of email addresses" - right, 134 million unique email addresses - "yet is pretty random in appearance.  The burning question I was trying to answer is, is it legit?



"The problem with verifying breaches sourced from data aggregators is that nobody willingly - knowingly - provides their data to them, so I can't do my usual trick of just asking impacted Have I Been Pwned subscribers if they'd used NPD before.  Usually, I also can't just look at a data aggregator breach and find pointers that tie back to the company in question due to references in the data mentioning their service.  In part, that's because this data is just so damn generic.  We have first and last name, address, Social Security Number.  Attributing a source when there's only generic data to go by is extremely difficult."



He said:  "The kludge of different file types and naming conventions worries me.  Is this actually all from NPD?  Usually you'd see some sort of continuity, for example, a heap of .JSON files with similar names, or a swath of .SQL files with each one representing a dumped table.  The presence of a uniquely named CSV file ties this corpus together with the one from the earlier tweet, but then there's stuff like 'Accuitty_10_1_2022.zip.'"  But I should note it's A-C-C-U-I-T-T-Y.  Huh?  And so he says:  "Could that refer to Acuity (single 'c' and single 't') which I wrote about in November?"  He says:  "HIBP isn't returning hits for email addresses in that folder against the Acuity I loaded last year.  So, no, it's a different corpus.  But that archive alone ended up having over 250GB of data with almost 100 million unique email addresses, so it forms a substantial part of the overall corpus of data."



Then he says:  "The 3.6 billion gigabyte, it's  'criminal_export.csv.zip' file caught my eye, in part because criminal record checks are a key component of NPD's services," he says, "but also because it was only a few months ago we saw another breach containing 70 million rows from a U.S. criminal database.  And see who that breach was attributed to?  USDoD, the same party whose name is all over the NPD breach.  I did actually receive that data, but filed it away and didn't load it into HIBP as there were no email addresses in it.  I wonder if the data from that story lines up with this file?  Let's check the archives."



He says:  "Different file name, but hey, it's a" - okay, so I should have said before, that criminal_export.csv.zip file is 3,608,086 kilobytes.  This other one that he filed away before, he says, "But hey, it's a 3,608,086 kilobyte file, so exactly the same size.  Given the NPD breach initially occurred in April, and the criminal data hit the news in May, it's entirely possible the latter was obtained from the former, but I couldn't find any mention of this correlation anywhere."  And he says:  Side note:  This is a perfect example of why I retain breaches in offline storage after processing because they're so often helpful when assessing the origin and legitimacy of new breaches."  Right?  Because the bad guys are aggregating stuff, too.  And, like, the more billions of records they have, the more money they can ask for, even if it's, you know, if it's really a Mutt data breach.



Okay.  So, he says:  "Continuing the search for oddities, I decided to see if I myself was in there.  On many occasions now, I've loaded a breach, started the notification process running, walked away from the PC, then received an email from myself about being in the breach."  He says:  "I'm continually surprised by the places I find myself, including this one."



So he says:  "Yep, it's an email address of mine.  Yet, oddly, none of the other data is mine.  Not my name, not my address, and the numbers shown definitely are not familiar to me.  I suspect one of those numbers is a serialized date of birth.  But of the total 28 rows with my email address on them, the two unique DoBs put me as being born in either 1936 or 1967.  Both are a long way from the truth."



LEO:  He's a lot younger than that, yeah.



STEVE:  So he finishes:  "A cursory review of the other data in this corpus revealed a wide array of different personal attributes.  One file contained information such as height, weight, eye color, and ethnicity.  The 'uk.txt' file merely contained a business directory with public information.  I could have dug deeper, but by now there was no point.  There's clearly some degree of invalid data in there.  There's definitely data we've seen appear separately as a discrete breach.  And there are several different versions of 'partial' NPD data," he says, "although the 27-part archive discussed here is the largest I saw, and the one I was most consistently directed to by other people."  That was the one that was posted in Telegram.



He says:  "The more I searched, the more bits and pieces attributed back to NPD I found.  If I were to take a guess, there are two likely explanations for what we're seeing.  This incident got a lot of press due to the legitimacy of the initial dump of Social Security Numbers, and the subsequent partial dumps are riding on the coattails of breach hysteria.  It appears that NPD may have siphoned up a heap of publicly circulating data to enrich their offering, and it got snagged along with the initially released Social Security Number data."  In other words, he's suggesting that NPD themselves were just sucking up a whole bunch of random crap of much lower quality than their main offering so that they could brag about, you know, themselves.



LEO:  Sure, why wouldn't they?



STEVE:  How many billions of people's data they have.



LEO:  Makes perfect sense.



STEVE:  And he says:  "These conclusions are purely speculative, though; and the only parties that know the truth are the anonymous threat actors passing the data around, and the data aggregator that's now being sued in a class action.  So, yeah, we're not going to see any credible, reliable clarification any time soon."



Okay.  So Troy's focus is understandably on his Have I Been Pwned web service and the email addresses his site uses to allow users to look up their own records.  And he uses them to inform people with those email addresses when their data has appeared in a new breach.  And I thought that Troy's obsessive pursuit of, and verification of, the exact source of this breached data was interesting.



As we launched into this topic I said:  "Before we wrap up today's podcast, I'm going to provide everyone listening with a URL for a searchable online database containing the records from this breach."  Now, it's been so exciting that Leo and I have not been able to keep that a secret.



LEO:  We keep talking about it.



STEVE:  We keep talking about it.  To aid everyone's memory, I made a shortcut for it, which begins with our standard grc.sc, you know, for shortcut, then just slash and "npd."  So grc.sc/npd, of course, for National Public Data.  Using that shortcut will bounce your browser over to npd.pentester.com.  Pentester.com is a 100% legitimate penetration testing subscription service that can be used by websites to check their sites' security.  Pentester describes itself as:  "We are a cybersecurity technology platform that has sourced the tools, methods, and techniques attackers use.  Our system allows owners and operators to find potential risks and exposures before the attackers do."



So they're just using it, they're using this, they acquired the NPD data.  They immediately stuck it in the cloud and have indexed it and are making it available just to draw traffic to their pentester.com site in order to get some new business.  So, makes sense.  Okay.  So that's all that is.  It's just a quick way for anybody to go, you know, GRC.sc/npd.  Tell your friends and family.  When you go there, I say put your full legal first and last name in because a nickname doesn't do it.  These records are from various legal documents, you know, of the sort that will include one's Social Security Number.  I did that, and as I've said, and as Leo has said, the results took our breath away.  The search for me revealed multiple records for every address that had been associated with me through the years, in every case with my correctly associated Social Security Number.



And again, I just want to reiterate, and make this point to your friends, it's important for everyone to understand that what's shown here is just meant to be a proof of presence of everyone's highly personal data within this massive database.  The fact that this search only displays the last two digits of everyone's Social Security Number does not mean that that's all there is.  No.  Nor that the search fields are all there is.  Again, no.  Everything about us, our full physical addresses, our various past and present phone numbers, our complete Social Security Number, and a massive amount of other probably very personal data is all there for the taking.  It's just not all being displayed publicly.  They redacted it just to be polite.



It's unclear what the exact geographic spread of the population of this database is.  From the reporting, it appears to be at least the U.S., Canada, and perhaps the UK.  And since this podcast has a global listening audience, we may have many listeners who will not find themselves listed within this.  But given what I've seen from poking around at people whose year of birth I know, I would be surprised if all of our domestic U.S. and Canadian listeners were not quite chagrined by their lookup of their own data, and that of others whom they know and care about.  Anybody who had, you know, subscribed to DeleteMe previously, like Lisa did, will probably be very pleased that their money is being well spent because they're apparently not going to find themselves there.



So it is my sincere hope that the ridiculously massive scope and scale of this breach, I mean, when our government's representatives put themselves in here and find themselves there, that's going to be a bit of a wakeup call.  So I think this is going to shine a very bright light upon this otherwise dark personal data aggregating corner of the Internet, and that we might well see some changes made in the laws that have allowed this practice to evolve and thrive. 



The data that is out there has escaped already, and it will now forever be public.  There's nothing that can be done about that now.  And it's clearly wrong that it's up to individuals to pay to out-out of this personal data collection process.  But Troy's finding and our confirmation that we've had so far that the data of those who had previously done so was conspicuously absent from this breach, serves as a true testament to the effectiveness of today's opt-out services.  It actually works.



So if I were a young person today, for whom it is not too late to prevent one's entire personal life and history from being aggregated, sold, and eventually leaked out onto the Internet, I would seriously consider adopting paid-for measures today to require all data aggregators to remove any records they already have, and then I'd bide my time until the slow-moving legal and governmental system catches up with what's going on.  This event clearly demonstrates that the way things are now needs to be changed.



As for the rest of us, whose last several decades of personal data is now so obviously out in the Internet flapping in the breeze, all we can really do is tightly lock down all access to our credit histories so that no one can apply for credit in our names.  We last talked about this need on April 15th, and at that time I created the GRC shortcut of "credit" - so grc.sc/credit - which will bounce your web browser over to the Investopedia page which talks about freezing credit and provides updated links to each of the three main credit reporting bureaus where you need to freeze your credit.



It may also be that the policies of the credit bureaus will need to change, and can be made to change, so that rather than by default they are passively allowing anyone to access our credit histories unless we take proactive action, it will be incumbent upon them to first obtain our clear real-time permission to allow a specific individual or entity to have any access.  But that's not the way it is today.  Regardless, we know that any change will be slow and incremental, and it will take time to even get started.



So as this podcast enters its 20th year, it appears that one of our favorite phrases, "What could possibly go wrong," will continue to keep us on our toes for quite some time to come.



LEO:  And to answer DJLookup's question in our Twitch chat, no, the data is out there.  DeleteMe can't remove it because it's just out there.



STEVE:  Yes.



LEO:  It's not with the broker anymore.



STEVE:  The point of the legal aggregator, the legal aggregators will respond, but the bad guys are laughing.  They've already got it.



LEO:  Right, they got it.  Right.  So as Eric Dutton says, "The horse has already left the barn."  By the way, I don't know if you saw this yesterday, Krebs on Security had a piece about how this might have happened, which I thought was quite interesting.  His contention, or his story is that there is a site that was almost identical to the NPD site called RecordsCheck.net.  On that site was a file, members.zip, which had in the clear names and passwords for the site's administrator.  So it may really be that they just, you know, the second site was created, as apparently was the first, by an Indian website, or Pakistani, sorry, website creation company called CreationNext.com.



And so it's very possible that they created both the NPD site and the RecordsCheck.net site for Sal Verini, who's the principal who founded NPD.  And this left passwords on there because that second site wasn't used.  There's also, as long as we're talking stories, this just came out.  I'm a Flightaware subscriber.  Flightaware warned some customers' info has been exposed, including Social Security Numbers.  You just can't win this game, can you.  It's bad news.  It really is.  All around.



STEVE:  That's why I think that, I mean, the only thing we can do is keep our credit frozen [crosstalk].



LEO:  Yeah.  And the good news is, thanks to federal law, it is free to freeze and unfreeze your credit report at all the major reporting bureaus.  They don't want you to do that.  That's how they make their money is selling your data.  You want that.  So mine are all frozen, and I know yours are, and we've recommended that for a long time.  So you've got that great URL, grc.com/freeze, if people want to learn more.



STEVE:  Slash credit.



LEO:  I'm sorry.  Say again?



STEVE:  It's grc.sc/credit.



LEO:  Sorry, grc.sc/credit.  And, yeah, it'll take you a few minutes.  It's not hard to do.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#989

DATE:		August 27, 2024

TITLE:		Cascading Bloom Filters

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-989.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  CrowdStrike's president appears in person to accept the "Most Epic Fail" award.  A secret backdoor discovered in Chinese-made RFID access key cards.  Counterfeit and poorly functioning Cisco brand networking gear in use by major institutions, government, and military.  A startling SSD performance improvement thanks to SpinRite.  When is "Bing" actually "Edge," and other errata.  Another useful National Public Data breach check service.  And what are "Cascading Bloom Filters," and why do they offer the promise of 100% browser local and instantaneous certificate revocation detection?



SHOW TEASE:  It's time for Security Now!.  Guess who won the "Most Epic Fail" award.  We'll talk about that.  A backdoor found in RFID access key cards that are used almost everywhere.  And then this is a propeller hat episode.  We're going to have a lot of fun.  Steve is going to explain a really clever technique that may solve this certificate revocation problem.  We're going to study Cascading Bloom Filters.  And it's fascinating.  But that's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 989, recorded Tuesday, August 27th, 2024:  Cascading Bloom Filters.



It's time for Security Now!, your favorite show every week, every Tuesday.  We wait all week for this moment to talk and listen to Mr. Steven "Tiberius" Gibson.  There he is.  Hello, Steve.



STEVE GIBSON:  Oh, Leo.



LEO:  Uh-oh.	



STEVE:  I've taken a deep breath because, boy, do we have a brain-melting podcast today.  



LEO:  Oh.  Oh, no.



STEVE:  But in a good way.  We've also referred to them previously as our propeller beanie podcasts, you know, the deep dive podcasts.  And actually it's one of the things that our listeners keep saying that they really like.



LEO:  Oh, yeah.	



STEVE:  As much or more than anything else.



LEO:  Absolutely.



STEVE:  The problem is there just isn't always something like that to talk about.  But we have something today for our Security Now! Episode 989 for this last podcast of August - where did that month go? - titled "Cascading Bloom Filters."  And although this is going to be - this is like one of those podcasts where, if you are operating heavy equipment, keep your focus on the heavy equipment because that's more important.  But then listen to the podcast some other time because no one will be able to split their attention and come away feeling like, wow, I understood that.



And I don't mean to overplay its complexity.  It's not.  But it's brand new.  It's pure, beautiful intellectual computer science.  And I know that a bunch of our listeners are going to get a big kick out of it.  But we have other stuff to talk about first.  Although I should say that our overall page count is down because I'm quite sure that our listeners' brains are going to saturate before, you know, by the time we get to page 16.  So I don't think anybody will find anything missing here.



CrowdStrike's president appeared in person to accept the "Most Epic Fail" award.  We'll talk about that.  Also, this is something that I've wanted to talk about for a couple weeks, and I just didn't have a chance to get it into the last couple podcasts.  But it just - it upsets my stomach because a Chinese backdoor was discovered in Chinese-made RFID access key cards.  And what this portends about our supply chain is really worrisome.  We've got counterfeit and poorly functioning Cisco brand networking gear that were being sold to pretty much the who's who.



I wanted to share a startling customer bit of feedback about his SSD performance improvement, and I've got screenshots which he took and sent.  Also a little bit of errata, when is "Bing" actually "Edge," and another couple things.  Also another National Public Data breach check service which is different than the one we talked about last week, and in this instance more is better.  And then, for the second entire half of the podcast, we're going to dig into - we're going to answer the question, what are "Cascading Bloom Filters," and why am I talking about them?  Well...



LEO:  Has nothing to do with gardening, I'm assuming.  Yes?



STEVE:  Nothing to do with gardening, no.



LEO:  Okay.



STEVE:  Why do they offer the promise of 100% browser local and instantaneous certificate revocation detection?



LEO:  Wow.



STEVE:  This is what Mozilla has been working on for four years.  And if people pay attention, everyone will be able to go "I understand about Cascading Bloom Filters."  And boy, will you be the hit at the next cocktail party.



LEO:  It's a great way, I think, to meet people, say, hey, have I told you about Cascading Bloom Filters?  And then they go, oh, no, but please do.



STEVE:  Yeah, well, or if it's really someone obnoxious who you do not want to talk to, you can just launch into a Cascading Bloom Filter dissertation.



LEO:  There you go, perfect.



STEVE:  And they'll say, oh, I'm sorry, I have to pee, or something.  I mean, they'll come up with some - oh, is that my doorbell?  I think it's my doorbell.  Hold on.  Yeah.



LEO:  We asked AI Leo in our Discord what Cascading Bloom Filters mean.  And I don't know if his answer is right because I don't know what it is.  But according to AI Leo there's a term that can make even the most seasoned programmer's eyes glaze over faster than a doughnut at a cop convention.  It's a nifty way to test whether an element is a member of a set.  Is that right?



STEVE:  Yeah.  



LEO:  And cascading them is like stacking pancakes.



STEVE:  Okay, well...



LEO:  Great for efficiency, but a bit messy if you don't manage  your syrup.  It was good so far.  Right up to that point.



STEVE:  That's right.  Now, is it on one of your shows that you were talking about printing pancakes?  It's a pancake printer?



LEO:  Yes, yes.  That was a - we thought that was something that Craig Newmark owned at home...



STEVE:  That's right.



LEO:  ...because he took so many pictures of it on his Instagram.  But it turns out he just is in a lot of airport lounges.



STEVE:  But he does, he pre-syrups the plate that the pancakes drop onto.



LEO:  His technique, yes.



STEVE:  So as Craig always does, he's an innovator.  He's leading the pack.  And I should mention that, oh, Leo, we have a really good Picture of the Week.  This one, this one is going to be fun.



LEO:  Well, I haven't seen it, but I do have it all queued up for you.



STEVE:  Everybody should be ready for a lot of fun for the next hour and a half or so.



LEO:  Steve, I have the Picture of the Week for you.



STEVE:  And we're ready for your candid first look, Leo.



LEO:  Okay, that's a candid first laugh because I wasn't on camera.  But absolutely this is a winner.  Let me pull it up for you.



STEVE:  Okay.  So we're aware of the, like, the bragging signs that factory floors sometimes have where they say, you know, no accidents for the last 263 days; right?  And they've got like little pegs where every day somebody changes the number, and they're really impressed with how they're doing.  And I couldn't remember what it was, but we had something similar once where it was a smoke detector stuck on the ceiling, and it said, like, "change by" or something, or "installed by," or something that you were supposed to put the date in there, and something put something completely non-sequitur in there instead.



Anyway, so here we have, thanks to one of our listeners, a wonderful - some signage.  It says:  "This work center has been accident-free since," and then there's a big white field where someone's meant to be, like, putting in the date; right?  Like it's been accident free since some date.  And instead it says "This work center has been accident free since Joe left."



LEO:  Poor Joe.  Poor Joe.



STEVE:  So, yeah.  And I did get a listener, I've already sent out 8,747 pieces of mail with the summary of the show, this Picture of the Week, the link to the show notes and so forth to those listeners of ours who have signed up to receive that every Tuesday morning.  Someone wrote back, and he said, "Is that on the White House?"  Anyway...



LEO:  Oh, that Joe.



STEVE:  Joe has not left the building yet, at least not in body.  So anyway, just I love that, "Since Joe Left."  Right.  Okay.  So PC Magazine's piece was published on August 12th, to set the time, which was two weeks ago yesterday.  But I felt that it was important enough to share, and we've had our hands so full with over-the-top, two-plus-hour podcasts the last few weeks.  And Elaine always tells me, she says, "Oh, this is going to take a while to transcribe."  It's like, "Okay, Elaine, no problem, take your time."  Anyway, I just didn't have a chance to get to this.  But I wanted to share it because I thought it was very cool.



PC Mag wrote:  "CrowdStrike's reputation has taken a beating after last month's massive IT outage.  But rather than duck the ridicule, the cybersecurity company decided to own the mistake this past weekend" - so that was two weeks ago - "by accepting a cybersecurity humiliation award for Most Epic Fail.  At the DEFCON hacking show, the annual Pwnie Awards recognize achievements and major blunders in the cybersecurity industry.  Past 'winners'" - and it has that in air quotes - "of the Most Epic Fail include Microsoft, the TSA, and Twitter.  This year, there was no question that CrowdStrike would receive the notorious title after the company accidentally distributed a faulty security update that bricked millions of Windows PCs and servers.



"Although CrowdStrike could have easily ignored the award, CrowdStrike's President Michael Sentonas accepted it in person, which elicited applause from an audience made up of other cybersecurity professionals.  Sentonas said it was important he accept the award so that CrowdStrike could own its mistakes.  He said:  'Definitely not the award to be proud of receiving.  I think the team was surprised when I said straight away that I would come and get it because we got this horribly wrong.  We've said that a number of different times.  And it's super important to own it.'  He said he will prominently display the trophy'" - wonder what the trophy looks like, you know, like a garbage can?  Or who knows?



Anyway, "he will prominently display the trophy he received at CrowdStrike's headquarters in Texas 'because I want every CrowdStriker who comes to work to see it.  Our goal is to protect people, and we got this wrong.  I want to make sure everyone understands these things cannot happen, and that's what this community is about.'"



Anyway, so the article finishes, saying:  "The gesture received praise from other cybersecurity workers since it's rare for a company to accept the Most Epic Fail award from the Pwnies.  Still, CrowdStrike faces a long road to repair its reputation.  A pair of class-action lawsuits have already been filed against the company, demanding it pay damages for causing last month's outage.  In addition, Delta Air Lines, which was forced to cancel thousands of flights due to the disruption, is also considering a lawsuit against CrowdStrike and Microsoft."  Which, you know, we talked about before.



Anyway, I just thought it was so cool that CrowdStrike's president went to the trouble and had the class to show up in person, and also that the attendees of DEFCON had the good sense to applaud his in-person appearance.  So, you know, yeah.  Lots of damage caused.  And, you know, him doing this in no way influences lawsuits that are pending and class actions and all the rigmarole and hubbub that will result.  But still, you know, as a gesture of, okay, you know, yeah, we did this, and we're really sorry, I thought that was nice.



Okay.  So talked about at the top of the show a piece of news that upset my stomach.  I hit it early last week.  But again, the podcast, last week's podcast was already overflowing, so I didn't have a chance to share it.  But I think everyone needs to hear this.  The news has had some coverage in the tech press, but Catalin Cimpanu's Risky Business newsletter had the most comprehensive coverage I've seen.



So here's what Catalin wrote.  He said:  "A security researcher has discovered secret hardware backdoors in RFID key cards manufactured by a major Chinese company.  The backdoors can allow threat actors to clone affected smart cards within minutes" - actually quicker than that - "and access secure areas.  They impact smart cards manufactured by Chinese company Shanghai Fudan Microelectronics that were built using MIFARE Classic chips from NXP."  And you know NXP is Philips' new name.  "The chips have been on the market since 1994 and have been widely used over the past decades to create smart key cards and access badges for hotels, banks, government buildings, factories, and many other places."



Catalin then has a snippet in his reporting of a tweet from Rob Joyce, who notes:  "For those who don't know, these are the RF keycards that are used in the electronic locks for hotels and other businesses.  This backdoor allows trivial reading and duplication of the keycard for those that had the backdoor key."  So Catalin continues:  "The chips have been on the market since 1994 and have been widely used over the past decades to create these smart key cards," you know, again, hotels, banks, government buildings, factories, and even the military has these.



"Because they've been on the market for so long, they've not escaped the prying eyes of the cybersecurity community, which has previously found several ways to break their encryption and clone MIRAGE-based cards with attacks named Darkside, Nested Authentication, Hardnested, and Static Nested attacks."  You get the sense that there's something about nesting in the weakness of these cards.



"Over the years, vendors have developed improved versions of their smart cards that shipped with various improvements designed to boost security and prevent some of the discovered attacks.  Two of the most popular" - and these are the upgraded card designs - "are the FM11RF08 and the FM11RF08S, where the 'S' stands for 'Security improved version.'  But in a paper published last week" - and so this would be a few weeks ago from now - "Quarkslab's Philippe Teuwen says that while researching FM11RF08S cards" - the improved ones - "he found what proved to be a secret backdoor baked inside Fudan cards.  He discovered the backdoor while fuzzing the card's command sets and noticed that the card was answering to undocumented instructions within a specific range."



"He said:  'Apparently, all FM11RF08S implement a backdoor authentication command with a unique key for the entire production.  And we broke it.'  For the Fudan FM11RF08S cards, that key was" - it's a hex code - "A396EFA4E24F."  That's the backdoor that opens up any of those cards.  "Then, taking his research further, he also found a similar universal backdoor authentication key for its predecessor, the FM11RF08 cards.  That key is A31667A8CEC1.  This one also impacted many other Fudan card models, and then we get a list of numbers.  It also impacted card models from Infineon and Philips, now NXP, suggesting that these companies had likely licensed their card technology from the Chinese company.



"According to Teuwen" - the researcher who found this - "the backdoor seems to go as far back as 2007, meaning that many access key cards distributed over the past 17 years can be cloned with brief physical access within seconds.  As it was pointed out above, these RF access cards are based on MIFARE Classic, which were already considered insecure, but that was due to design errors and not a backdoor.  Attackers would still have to spend minutes and physical resources to crack and dump a card's data in order to clone access keys configured to it.  But the backdoor makes that possible, and it adds a whole new threat matrix cell."



So what I find upsetting about this is that this is an example of the true danger we face when we're outsourcing and importing proprietary microelectronics that cannot be readily audited.  And this is an example, I believe, that's crucially important because this is not just a theoretical attack; right?  You know, a theoretical attack is bad enough, like the fact that it would be possible to install extra chips on a motherboard to establish a secret backdoor for a theoretical attacker.  But there's nothing theoretical about what was just found.  This backdoor capability was actually secretly installed into countless supposedly secure and supposedly even more secure, like, you know, their successor enhanced security RFID access cards.  So this wasn't some mistake.  This was deliberate.



And we're never going to know the deeper back story here.  We'll never know exactly why this was done, from how high up in Chinese industry or government the decision was made to subvert an extremely popular family of authentication chip technology, and why.  And it doesn't really matter.  What matters is that someone said "We'll make super secure chips for you super cheap," and they were trusted not to design-in backdoor access.  Which they did design in.



So we can see how Apple is being so - and why Apple is being so incredibly circumspect about the servers that they'll be installing into their data centers.  We were talking about this after their announcement.  At the time, that seemed like some really cool over-the-top security precautions.  Now it seems not only prescient, but quite necessary.  We really do have a problem with supply-chain security.  And the only way it's ever going to get better will be when some means is made to thoroughly audit the design and operation of the technologies we use.  You know, the problem is that the extreme benefit of the level of integration that we have today buries literally the circuit diagram, the logic diagram of this technology under layers of silicon and masks and etching and everything that goes into building chips.



Older chips, and we've seen this before, it's possible to pop the lid off the chip and take a photomicrograph of it, and semiconductor designers are able to understand from the horizontal and vertical stripes running back and forth and, like, oh, look, that's a transistor, and that transistor's connected to this one, and they can unravel that back into an actual circuit diagram.  We've seen it done with, like, the 6502 and the 8088 and early generation ICs.  But now that they've gotten to be multilayer and in some cases deliberately obfuscated, it's possible to hide this stuff, it's just not feasible anymore.



So I don't know what we do.  But, you know, what we've seen happening over the 19 years so far of this podcast is all of the use of the Internet and our technologies moving rather rapidly from, you know, cat videos on the Internet to being utterly dependent upon this networking technology for financial and military and governmental security.  And, you know, it's coming up wanting, unfortunately.



And on a related topic, we have BleepingComputer recently carrying a piece with the headline:  "CEO charged with sale of counterfeit Cisco devices to government and health orgs."  They wrote:  "Onur Aksoy, the CEO of a group of dozens of companies, was indicted for allegedly selling more than $100 million worth of counterfeit Cisco network equipment to customers worldwide, including health, military, and government organizations.



"According to the criminal complaint, the 38-year-old Florida man ran a massive operation between at least as early as 2013 through 2022, importing tens of thousands of modified low-quality networking devices for as much as 95 to 98% off of Cisco's MSRP for the same devices, the devices these were purported to be, from Hong Kong and Chinese counterfeiters through a network of at least 19 firms in New Jersey and Florida.



"The indictment alleges these devices were sold as new and genuine Cisco products through dozens of Amazon and eBay storefronts to customers across the United States and overseas, some ending up on the networks of hospitals, schools, government, and military organizations.  The fraudulent Cisco devices sold by Pro Network Entities came with performance, functionality, and safety issues that led to failures and malfunctions which, in turn, generated significant damages to customers and operations and networks.  This happened because the counterfeiters who sold the fraudulent Cisco equipment to Aksoy were modifying older, lower-model products, some previously owned, to make them look like genuine models of new and more expensive Cisco devices.



"A U.S. Department of Justice press release reads:  'As alleged, the Chinese counterfeiters often added pirated Cisco software and unauthorized, low-quality, or unreliable components - including components to circumvent technological measures added by Cisco to the software to check for software license compliance and to authenticate the hardware.  Finally, to make the devices appear new, genuine, high-quality, and factory-sealed by Cisco, the Chinese counterfeiters allegedly added counterfeited Cisco labels, stickers, boxes, documentation, packaging, and other materials.'"  In other words, total front-to-back, soup-to-nuts counterfeiting of the entire unboxing and receiving experience.



BleepingComputer said:  "Aksoy's companies, collectively known as Pro Network Entities, generated more than $100 million in revenue, with millions lining the defendant's pockets.  However, despite his efforts to fly under the radar by using fake delivery addresses, submitting forged paperwork, and breaking shipments into smaller parcels, between 2014 and 2022, Customs and Border Protection agents seized roughly 180 loads of counterfeit Cisco equipment shipped to the Pro Network Entities by co-conspirators in China and Hong Kong.



"In July of 2021, law enforcement agents seized 1,156 counterfeit Cisco devices worth over $7 million after executing a search warrant at Aksoy's warehouse.  To top it all off, DOJ says that 'between 2014 and 2019, Cisco sent seven letters to Aksoy asking him to please cease and desist his trafficking of counterfeit goods.'  Aksoy allegedly responded to at least two of Cisco's cease-and-desist letters 'by causing his attorney to provide Cisco with forged documents.'"  So, you know, he was trying to play the game, too.  And, you know, Cisco would have seen this equipment on sale on Amazon and eBay and probably bought some of it and opened it up.  And, you know, there was a squirrel in a cage inside instead of an actual multicore processor capable of doing what their product alleged it could do.  And of course this hurts Cisco reputation a lot in addition to just lost revenue.



So the defendant was arrested in Miami on June 29th two years ago, 2022, and was also charged in the District of New Jersey the same day with multiple counts of trafficking counterfeit goods and committing mail and wire fraud.  If the charges stand up in court, and Aksoy gets sentenced to prove the truth of the allegations, this serves to show how easy it could be to infiltrate critical networks if a threat actor uses a similar approach, selling genuine but backdoored networking equipment.  And again, this makes us think that Apple was not in any way over the top to be taking the lid off of their servers, doing high-resolution photography to identify the authenticity and provenance of every single component on their servers as the beginning of the process that they go through before they let one of these servers get plugged into their super-secure datacenter.



So it's very clear that the bad guys have figured out that we're implicitly trusting the supply chain every step of the way.  And unfortunately there are a great many steps along the way.  We know that security requires that every step be secure because it only takes the subversion of one step to break the security of the entire chain.  And unfortunately, the way we are doing things at the moment, we're vulnerable to that.



Okay.  So this is from a listener of ours who identified himself as Sandor.  And he said:  "Thank you for SpinRite."  He said:  "I have a 10-year-old Toshiba Satellite laptop.  I installed a Western Digital Blue SSD just over five years ago."  And actually he doesn't say it in here, but it's a 1TB, so it's a 1TB WD Blue SSD, just over five years ago.  He said:  "The warranty just expired.  This Windows laptop was acting sluggish.  I could not immediately identify the issue.  So I ran down the checklist of items.  Checked for updates and ran CHKDSK - no problems.  Checked disk optimization - barely any fragmentation.  Checked the firmware on the SSD - it was up to date.  Ran a quick SMART check - everything was okay.



"I pulled out SpinRite and ran level 1 and checked my system afterwards.  The sluggishness was still there."  And that's what we would expect because level 1 is just a read pass.  He said:  "Thinking about what to check next, I remembered Steve mentioning SpinRite's benchmarking utility.  Ran the benchmark, and the issue revealed itself - the front, middle, and end read speeds were way off."  Get this.  2.489, 17.224, 11.642, and those are all MB/s.  So, wow.  2.489 MB/s, I think floppies do better than that.  Anyway, then he said:  "See BenchmarkBefore.jpg."  And I've got that in the show notes.



Then he said:  "Ran SpinRite at level 3."  And that's what he wants.  He said:  "After level 3 completed, I re-ran the benchmarks, which showed how the SSD returned to its proper performance."  So now remember before we had 2.489, 17.224, and 11.642.  Now, 564.353, 563.285, 564.222.  In other words, back to, from down at 2.5, 17, and 11 MB/s, to 564 MB/s across the board.  And he said:  "See BenchmarkAfter.jpg."  He said:  "The sluggishness I noticed is gone; the Satellite laptop is performing as I would expect for a 10-year-old system."



And I've got for anyone who's interested in the show notes he took a picture with his smartphone of the benchmarking output results before and after.  The other thing interesting is that SpinRite looks at the speed in the middle of the drive as the best estimate of the drive's average performance.  And it uses that to guess at, you know, to give the user a ballpark, you know, estimate of how long a SpinRite scan will take.  Due to the low performance in the middle of the drive before, SpinRite was estimating that that 1TB drive would take it about 17.72 hours to scan.  Of course, that's what old SpinRite 6 users were accustomed to seeing from a 1TB drive.  Not so any longer.  After he ran the level 3, SpinRite now estimates that it would be able to scan that 1TB SSD in 32.6 minutes.  So from 17.72 hours to 32.6 minutes.  And of course the entire drive's performance is restored to its original factory throughput.



So, you know, we are seeing more and more examples of this.  Initially when we first discovered this we were shocked, but we know what's going on.  It's that, especially at the beginning of the drive where the OS files reside, they are almost exclusively red.  And this SSD problem known as "read disturb" actually causes the storage levels in the bit cells to be disturbed, meaning altered, when neighboring regions are red.  So the SSDs are extremely good about trying to get that difficult-to-read data back.  They do all kinds of re-thresholding in order to set different threshold points for what are basically analog storage cells.  They end up getting your data back, but at a huge cost in performance, simply by rewriting all the data, which SpinRite's level 3 does.  All the ones are really ones, and the zeroes are really zeroes, and the drive comes back up to its factory original speed.



LEO:  Just like me.



STEVE:  That's right.  Glad to have you back at your factory original speed, Leo.



LEO:  You know what happened?  It's really hot here.  It's almost 100 degrees.  And I think Comcast just died.  And fortunately yesterday we put in Starlink.  So I'm talking to you now via space.



STEVE:  Wait.  You look perfectly synchronized and low latency.



LEO:  It's amazing.  Starlink is fantastic.



STEVE:  It works?



LEO:  Yeah.  Latency is really low, in some cases lower than the terrestrial Internet with Comcast.  And it's enough bandwidth to do the show.



STEVE:  Wow.



LEO:  So if you want to take a break now - you already took a break without me, I think.



STEVE:  I did, yeah.



LEO:  As you should have.



STEVE:  I created both sides of a standard break, and we will do another one here at the beginning of our main content.



LEO:  Okay.  So continue on.  I apologize for - nobody saw that I had disappeared, but I had.  And it was always crazy.  What was happening is it was coming and coming back.  So we have failover.  The Ubiquiti fails over to Starlink.  So when Comcast dies, about 10 seconds later I come back, and the way this works, I don't have to be - I'm not originating the call, so you'll continue on without it.  What happened was Comcast would drop, Starlink would come up, and then it would come back, Starlink would drop.  It was a mess.  So I just manually unplugged Comcast.  And I might leave it that way.  Anyway, continue on, my friend.



STEVE:  You know, Elon is such a challenge.



LEO:  Yes.



STEVE:  But he does manage to induce people to create technology.



LEO:  It's amazing.



STEVE:  He's like, what do you mean we can't have Low Earth Orbit satellites?  I'm going to throw some up in the air myself.



LEO:  Thousands of them up there, yeah.



STEVE:  Wow.  Okay.  So I have two pieces of errata to share.  First, it's been brought to my attention that I've been referring to Microsoft's Edge web browser as "Bing."



LEO:  Understandably.  Completely understandably.



STEVE:  Which is obviously their illustrious search engine facility.  And I'll confess that the two have occupied approximately the same undesirable location in my head.  So, yeah, they were confused.  But now that I'm aware of this glitch I will work to be more conscientious of my tendency to lump Edge in with Bing because, you know, they're both four letters, and they're both MSFT.  Wow.



Okay.  The second issue was obvious in retrospect, but it just didn't occur to me last week, and that was that the National Public Data breach search facility at Pentester.com requires the choice of a U.S. state.  That's not something non-U.S. listeners who were likely affected by the breach - at least those in Canada and the UK - would have.  So everyone was saying, hey, Gibson, I'm in Canada.  What state is that in the U.S.?  Oh, yeah, that's a problem.



LEO:  I might mention, though, and you know him well, Paul Holder, who is a Canadian and a resident of both of our fine forums, did an interesting thing.  Let me pull this up.  He downloaded the data in the NPD breach.



STEVE:  Yup.  He got both of the monster files.



LEO:  Yeah.  And he wrote a little utility, because he's a programmer, that went through it.  And he says he can't confirm there's anything in the database other than U.S. addresses.  The bad guy said it was U.S., Canada, and the U.K.



STEVE:  Ah, that's true.



LEO:  So...



STEVE:  Okay.  So what Paul got, and we talked about this in detail when we went through what Troy Hunt had found, Troy did see evidence of pollution of the National Public Data breach data with other lists that had been lumped in.



LEO:  Look at this.  Calgary, Alberta, Canada in the State of Texas.  That's definitely corruption, yeah.



STEVE:  Yeah.  Yeah, so it may well be that what Paul got was the good data from National Public Data, and that there was additional junk that was added in just to sort of salt it and make it look bulkier and more worrisome.  Who knows?



LEO:  Anyway, you may not have missed anything if you're not in the U.S. 



STEVE:  Good point.



LEO:  Maybe you lucked out.



STEVE:  Very good point.  That may not be anything that they're doing.  So I got some feedback.  Listener Mike wrote.  He said:  "Hi, Steve.  Just got off the Pentest site after doing a name lookup.  Yup, I'm there, but nothing very new.  We froze our credit at four separate sites many years ago, and nothing is showing up since before that happened.  We moved to our current house 10 years ago, and that address does not show up.  Thought you might like to know."



And then yesterday listener Kevin White wrote.  He said:  "Hi, Steve.  Very longtime listener.  If you count TechTV, I think you and I and Leo go back before SecurityNow existed!"  And yes, that would be, because it was during a Call for Help episode...



LEO:  In Canada.



STEVE:  ...that you and I were filming in Toronto...



LEO:  Toronto, yeah.



STEVE:  ...that you said, "What would you think about doing a weekly podcast on security?"  And I said, "A what cast?"



LEO:  Smartest move I ever made, Steve.  I've just got to say.



STEVE:  No, me, too.



LEO:  It was a good move.



STEVE:  This has been really good, Leo.



LEO:  Yeah.



STEVE:  It still is.  So he said:  "Last week you gave us a link to the Pentester site's NPD breach check web page.  I just wanted to note that for those of us with a fairly common name" - oh, and he's Kevin White - "Pentester's check page doesn't always give out complete information."  Now, he masked his middle initial.  He said:  "My name is fairly common, and Pentester's check page gave up listing similar names after FirstName, then middle initial 'B,' and then LastName," he said, "for example, 'B' being the middle initial.  And my middle initial is located somewhere else.



"So initially I thought that I wasn't on the breach list.  Then I checked" - now, here's the key - "the NPDBreach.com site" - different site, NPDBreach.com - "against my name and current zip code, and I confirmed I'm on the breach list."  Then he said:  "Sad face.  The benefit to the Pentester site is that it appears to show you the full number of entries there are for you on the breach list, whereas the NPDBreach site appears to just give you a somewhat distilled version of the results," he said, "a single entry for all breach list items with the same SS number, I think."



He said:  "So it's best to check both sites, especially if you have a common name, or if you've lived in a bunch of different places."  He said, you know, "checks against SSN, not by state.  One colloquial data point," he said, "a friend was on the list, but it did not have any recent addresses for him, nothing in the past 15 years.  But his wife was on the list with their most current address."



He finishes:  "I guess what's somewhat nice about these circumstances is that the breach list appears to have some data reliability weaknesses that might make it a little more difficult for someone to hijack an updated or recent account.  Thanks, and see you tomorrow."  And he wrote this yesterday.  Signed Kevin. 



So I wanted to make sure that everyone knew about the https://npdbreach.com website.  It also appears to be safe and legitimate.  They allow the NPD breached data to be retrieved by name and some address info, or by providing a full Social Security number or a full phone number.  However, now that might seem dangerous, but the site explains that the user's provided Social Security number or phone number are "blinded" by hashing locally on the browser.  They similarly hashed that same data at their end, and then indexed their database with those hashes.  That allows the user to retrieve their records with those hashes without the data that they're submitting to the site being exposed.



LEO:  That's how Troy Hunt does password searches in Have I Been Pwned, same thing.



STEVE:  Yes.  Yeah.  Not that it makes that much difference anymore since, as we know...



LEO:  It's all been revealed.



STEVE:  ...everything is now out there flapping on the Internet.



LEO:  Wow.



STEVE:  So, wow, yeah.  Oh, and I should mention that I did get a piece of email from someone, I think it was titled "The Seven Circles of TransUnion Login Hell."



LEO:  Oh, boy.



STEVE:  Where, oh.  And I actually had the occasion to need to log into my TransUnion data last week.  Boy, is that site broken.  Oh.  I mean, it's just so broken.



LEO:  I totally think it's intentional.  Yeah.  No, it's my opinion that all of these sites, they don't, look, they don't want to give you credit freezes.  So they make it - but they're legally required to.  In fact, they're legally required to do it online.  So they just make it broken.



STEVE:  Oh.  Yeah, I mean, every trick in the book in order to get logged in.  And I, lord knows, you know, I know the tricks. But for someone who doesn't it's like, they're just going to say, oh, well.  Well, they'll probably just give them a phone call and hope that they can talk to somebody.



LEO:  Yeah.  In fact, that's what - I think it was Experian I had the same login issue.  I showed it on Windows Weekly a few months ago.  But then you call them.  And I don't know why they - you wouldn't think they'd want a call.



STEVE:  No.  You would think that would be - but I tell you, that's what they're - they're driving call traffic.



LEO:  Yeah, yeah.



STEVE:  Okay.  So this is from Darren in New South Wales, Australia.  This was just a really nice note.  And I wanted to share it for - not for the flattery that he gives to us, but for the perspective of a recent listener that I thought was valuable.  He said:  "Hi, Steve.  Long-time follower of GRC, short-time follower of Security Now!.  Early last year I was looking" - so early last year, right, 2023 - "I was looking for a new podcast when I stumbled across Security Now!.  Despite being a long time GRC / Steve Gibson follower from the late '90s and visiting your website a lot, particularly in the early days of firewalls for the obligatory Shields Up! test on reinstall of Windows, for some reason I never clicked on the Security Now! link.



"When I came across the podcast last year I saw your name and thought to myself 'this guy knows what he's talking about, I might have a listen.'"  He said:  "It was Episode 909, 'How ESXi fell,' and I was hooked," he wrote.  He said:  "I spend quite a bit of time in the car, and I listen to the podcast.  So I quickly caught up and was waiting for the podcast for next week when I remembered you and Leo speaking about the fact that GRC has all the previous podcasts available, and I thought, why not?  So I downloaded episodes one through 100 and pressed Play."



He says:  "What a different world it was back then when you could afford the time to spend a number of weeks telling us how the Internet works and how PKI works, as opposed to today when it just seems like there is a never-ending collage of security disasters, vendor mistakes, and just plain carelessness.  Additionally," he said, "I really liked the episodes on NETSTAT (that was #49) and the History of Virtualization (#50)."



He said:  "As Leo mentioned recently, compiling your tutorial sessions into a consolidated package would be very valuable for the beginner, which is why I wish I'd come across your podcast back in the day as I'm an RF Engineer who somehow found myself in the murky world of IT, and I am still wondering how that happened.  The way you can explain a complex and sometimes abstract concept in simple and understandable terms is an enviable ability.  At work, I have to frequently provide a translation service between Engineering and Operations, and I've used your examples and explanations a few times to great effect.



"So my routine is this:  I will listen to episodes one to 10.  Then I will listen to the current podcasts, usually two weeks' worth, given the time it's taken me to listen to 10 episodes.  Then I listen to episodes 11 through 20, then the current ones I've missed, et cetera.  I'm now up to Episode 102.  I'm wondering what my listening schedule will look like as your episodes get longer and longer over time.  I'm thinking I may have to drop back to only five episodes from the before times before tuning back in to the current podcasts.  Otherwise I'll be too out of date, and I don't want that to happen.



"I would recommend to all latecomers to Security Now! to go back to the start and revel in the glory that is Security Now!, wonder at how naive we were back then, and remember a time when a couple of gig of RAM was overkill and way too expensive.  Congratulations on a fantastic podcast, a great website, and hopefully we can all be here listening to you and Leo describe the hopefully soft landing after the end of Unix time."



LEO:  2038.



STEVE:  "Thanks and regards, Darren from New South Wales, Australia."



LEO:  That's great, Darren.  Thanks, that's awesome.



STEVE:  So I also wanted just to share this as an example of how different our feedback from email can be from the "here's a link" messaging through Twitter.  Now, don't get me wrong, I know there's a place for both.  But the nature of our feedback is now completely transformed from the way it was and in a way that I think works for the podcast.  And Darren's point about those older podcasts still being relevant really is true.  You know, probably after we finally retire the podcast, Leo, I'll go back and pull together some tutorial retrospectives that, you know, what do they call those, "evergreens"?



LEO:  Well, and until then, people can of course do it themselves at your site.



STEVE:  Yeah.



LEO:  We also have, you know, all the episodes.



STEVE:  Well, and you've got the high-quality ones, which is good.



LEO:  And video, for the ones that there were video for.  It's a little tricky because you have to kind of go back a page at a time.  But I'll give you a little tip.  The URLs, if you type TWiT.tv... 



STEVE:  Our uniform?



LEO:  Yes, /sn and then a number, not with a zero, but just a number like sn1, that's the very first Security Now! episode.  So you can - it'd probably be easy enough to write a scraper to take advantage of that fact.  There's Security Now! 2, and so you could also do it this way, one by one.  But it is, you are going back in time to the year 2005.  Well, look at this one, Security Now! 4, we just discovered passwords.  That's very exciting.  I think they'll be useful.



STEVE:  [Crosstalk] password, what do you know.



LEO:  Ah, here's your Personal Password Policy, Part 2.  You probably talk about Password Haystacks or something.



STEVE:  I do, I do like alliteration; don't I.



LEO:  Yes, you do.  Keyboard sniffing, remember that, Bluetooth snarfing?



STEVE:  Ah.



LEO:  And what were they using, ROT13 to encrypt the keyboards?



STEVE:  Ah.



LEO:  Those were the days, my friend.  We thought they'd never end.



STEVE:  Actually, it was a fixed XOR mask.



LEO:  That's what it was.  Excellent.  For XORing.  Crazy.  Crazy.  With a fixed mask.



STEVE:  And on that note, we should do our third and second-to-the-last sponsor because I'll take a break here in the middle.  And everybody, you know, maybe some ice water against your forehead because your brain is going to heat up.



LEO:  You want a propeller hat episode?  Guess what?  You're going to get one.



STEVE:  Yeah, your brain is going to heat up.  But I tell you, this is a new concept, it will be a new concept for many people.  It is so friggin' cool.  So...



LEO:  I'll make sure AI Leo listens.



STEVE:  And Leo, actually, I was thinking of you and your own coding of your problem solving annually.  There's some value in this.



LEO:  Oh, yeah.  This is what, you know, I do every December the Advent of Code Calendar.  I'm stuck on Day 22, I think, from last year, December 22nd.  But every one of them is really about algorithms and using algorithms.



STEVE:  Yeah.



LEO:  And so, yeah, I've always...



STEVE:  This is a new algorithm.



LEO:  Good.  I can't wait.



STEVE:  This is a very cool algorithm.



LEO:  Can't wait.  All right, Steve.



STEVE:  Okay.



LEO:  Part 1 of our subject of the hour, Cascading something or other.  Bloom Filters.



STEVE:  So as I said, yeah, at the top of the show, I am well aware that this podcast's enduring favored feature is pure and clean theoretical deep tech explaining.



LEO:  Woo-hoo.



STEVE:  And of course, Leo, you used to introduce me here as the "Explainer-in-Chief."



LEO:  Yes.  I'll start that up again.  I forgot about that.



STEVE:  Well, I don't know that we're going to be able to do that all the time.



LEO:  Right.



STEVE:  You know, we were able to do far more of that back in the beginning, which, you know, 19 years ago, before pretty much everything had been thoroughly explained at least once, and in some cases multiple times.  So, you know, but I know that this is what people want.  So I'm always on the lookout for something to come along that we've not yet explained.  And it's the nature of those things to become increasingly rare because by the end of like the next hour, we will have done Bloom Filters.  And, you know, if the topic ever comes up, we'll just point back to Episode 989.



So this week we have as pure and clean and deep and theoretical a topic of computer science as we've ever had.  Which, and I have to tell you, I'm excited about it, too, because it's just so cool.  You know, and I just want to also say that I understand we have listeners who will write to tell me that, while they do very much enjoy the podcast, and pick up useful tidbits every week, they only, by their own appraisal, feel that they understand around 5%, they often say, of what is sometimes discussed here.



LEO:  And by the way, me?



STEVE:  No.  Anyway...



LEO:  I'm right in there with you, kids.



STEVE:  I'm feeling somewhat self-conscious that today's topic is likely to further reduce that percentage.



LEO:  I'm going to try really hard, Steve.  



STEVE:  I know.  Everybody's going to get it.  But again, I saw an email from someone who says he listens to the podcast while swimming laps.  And I'm just afraid he's going to drown because, I don't know. 



LEO:  Sink or swim time, kids. 



STEVE:  For me, and I know for a bunch of our nerdy listeners, there's nothing more gratifying than the moment when a new concept is grasped and understood and becomes clear.  So I predict that many of our listeners are going to find a lot of new ideas in what follows.  So here it is.  Today's topic is a technique in computer science known as a Bloom Filter.  It was so named back in 1970 after its inventor, Burton Howard Bloom.  And just to get everyone warmed up here, I'll explain, and everyone's going to get it completely by the end.  But a Bloom Filter is an extremely clever and resource-efficient probabilistic data structure that's used to test whether an element is a member of a set.



LEO:  Oh.  And you do that a lot in computer science.



STEVE:  Yes.



LEO:  That's a very common issue.



STEVE:  And is a revoked certificate a member of all certificates?



LEO:  This would speed up OCSP.



STEVE:  It would probably be the answer to our dreams.



LEO:  Well, well.



STEVE:  Which is where we're headed.



LEO:  Nice.



STEVE:  Okay.  So a probabilistic data structure that's used to test whether an element is a member of a set.  But it's got some problems.  Although false positive matches are a well-known and well-understood likelihood, so this thing can false positive, false negatives are impossible.  They don't occur.  So in other words, a test using a Bloom filter can return either a datum may be a member of the set, or this datum is definitely not a member of the set.  Now, Bloom's invention is deliberately information lossy, which is where it obtains its efficiency.  But as we'll see, this prevents members from being removed from the set.  And as more members are added to a Bloom filter, the greater the probability of false positives becomes.  So all of the math is understood, and it's all very cool.  And so it's easy to optimize the Bloom filter for the things you want.



Okay.  So why in the world have I suddenly apparently out of nowhere decided to talk about Bloom filters?  Where in our recent discussions might it be useful to efficiently know whether something, some item, might be a member of a set of other items.  And I just gave that away earlier, whether a website certificate might be a member of the set of all currently revoked and non-expired certificates.  If we have a really large number of revoked certificates, and we now have web browsers reaching out to scores of other sites, you know, all of the certificate-authenticated TLS connections need to be checked.



So we need to have some really fast and very efficient means for instantly determining whether any of the many TLS certificates the browser receives, even just reading one page, because as we know pages now reach out to all kinds of different servers in order to construct themselves, whether any of those certificates have been revoked.  And it just so happens that Burton Howard Bloom's 54-year old invention from 1970 is the best way to do that today in 2024.



One additional point is that today's podcast is not just titled "Bloom Filters."  It's titled "Cascading Bloom Filters."  The reason for this, as we'll see, is that the use of a cascade of successive Bloom filters elegantly completely solves the false positive problem that's inherently created by the Bloom filter's efficiency.



And as it turns out, this Bloom filter technology comes in very handy in computer science.  And it's everywhere, without us really being aware of it.  The servers of the massive content delivery network Akamai use Bloom filters to prevent the so-called "one hit wonders" from wasting their cache space.  One hit wonders are web objects requested by users only once.  After Akamai discovered that these one hit wonders were needlessly tying up nearly three-quarters of their entire caching infrastructure, they placed Bloom filters in front of their cache to detect a second request for the same object.  Only if an object was requested a second time would they bother to save that in the cache.



Google's Bigtable, Apache's HBase & Cassandra, ScyllaDB and PostgreSQL all employ Bloom filters to bypass - get this - to bypass disk lookups for non-existent rows or columns, that is, the Bloom filter can definitely say something's not there.  It's not so good about saying for sure it is there, that is, just a simple Bloom filter.  We'll see how that's been fixed.  But this is useful because quickly knowing when something is not, when something is for sure not in a database, significantly increases the system's overall performance.



LEO:  Well, and you do that all the time.  I do that all the time in coding because I want to know should I add this to a list?  I don't want to duplicate it.  Just like the caching problem.  It's the same thing; right?



STEVE:  Right.



LEO:  I don't want to duplicate it.  So I'll just say is it here yet?  No, it's not.  Good, I'll put it in there.  Perfect.



STEVE:  Yup, I think it's going to come in handy.  And it might say it is when it's not.



LEO:  Right, understand.



STEVE:  But that's better, you know, getting a no most of the time...



LEO:  Right.



STEVE:  Well, actually, if you ever get a no, you know it's correct.



LEO:  No is reliable.  It's the yes that's not.



STEVE:  Correct, exactly.  And we're going to see exactly why that's the case here in a second.  Also Google's Chrome browser used a Bloom filter to identify potentially malicious URLs.  Every URL was first checked against a local Bloom filter.  And only if the filter returned a positive result was a full check then made of the URL.  That's how Google handled the false positive problem.  Microsoft Bing - and yes, I really do mean Bing and not Edge - uses multilevel hierarchical Bloom filters for its search index, which is known as BitFunnel.



Even Bitcoin once used Bloom filters to speed up wallet synchronization until some privacy vulnerabilities with the implementation were discovered, though it wasn't the filter's fault, it was the implementation.  And the website Medium uses Bloom filters to avoid recommending articles a user has previously read.  Ethereum uses Bloom filters to quickly locate logs on the Ethereum blockchain.  So, you know, my point is this is a well-understood, very powerful technology that everybody's going to understand by the time we're done.  



And we don't really care about Akamai, Bigtable, HBase, Cassandra, PostgreSQL, Bing, Bitcoin, Ethereum, or Medium.  But we do care about Mozilla and Firefox, and about the fact that Firefox has always appeared to be leading the pack when it comes to actually protecting their users from revoked certificates.  As we've all now seen, the other browsers, most notably Chrome, just pay empty lip-service to revocation.  I mean, they've said they do it, and they weren't, as I showed 10 years ago.  I should note, though, that Vivaldi is also a browser who in its default settings is also properly doing revocation, I'm sure using OCSP at the moment, the way Firefox does.  And as we know, there's a bit of a problem with that.  And actually I'm going to get to that in one second here.



So what do we know about Mozilla's plans for next-generation CRL (Certificate Revocation List) based revocation?  Four and a half years ago, in January of 2020, Mozilla's blog posting was titled:  "Introducing CRLite."  They said:  "All of the Web PKI's revocations, compressed."  And they wrote:  "CRLite is a technology proposed by a group of researchers at the IEEE Symposium on Security and Privacy 2017 that compresses revocation information so effectively that 300MB of revocation data can become 1MG.  It accomplishes this by combining Certificate Transparency data and Internet scan results with cascading Bloom filters..."



LEO:  Well, there you have it.



STEVE:  "...building a data structure that is reliable, easy to verify, and easy to update."  They said:  "Since December" - so that would have been 2019 December - "Firefox Nightly has been shipping with CRLite, collecting telemetry on its effectiveness and speed.  As can be imagined, replacing a network round-trip with local lookups makes for a substantial performance improvement."  Meaning Firefox is not needing to do any querying of OCSP or a Certificate Authority's Certificate Revocation List.  It's able to do it locally.  They said although not all updates are currently delivered to clients, they've been doing this for four and a half years.



Okay.  And as we know, since the CA/Browser Forum's members almost unanimously voted to drop use of OCSP in favor of requiring a return to the original Certificate Revocation List approach, we were wondering, how does that make any sense?  Revocation Lists were no better and arguably worse in some ways.



So before we dig into what Bloom filters are and how they work, let's take a moment to understand Mozilla's position, which is driving this, on OCSP and CRLs.  In that January of 2020 posting, they wrote:  "The design of the Web's Public Key Infrastructure (PKI) included the idea that website certificates" - oh, I should mention something new is mentioned here that never occurred to me before.  They said:  "...included the idea that website certificates would be revocable to indicate that they are no longer safe to trust, perhaps because the server they were using was being decommissioned, or there had been a security incident.  In practice, this has been more of an aspiration, as the imagined mechanisms showed their shortcomings."  In other words, we'd like to know if certificates have been revoked, but we just can't figure out how.



So they said:  "Certificate Revocation Lists quickly became large, and contained mostly irrelevant data, so web browsers stopped downloading them.  The Online Certificate Status Protocol (OCSP) was unreliable, so web browsers had to assume, if it didn't work, that the website was still valid."  As we talked about, you know, failing open.  They said:  "Since revocation is still critical for protecting users, browsers built administratively-managed, centralized revocation lists.  Firefox's OneCRL, combined with Safe Browsing URL-specific warnings, provide the tools needed to handle major security incidents, but opinions differ on what to do about finer-grained revocation needs and the role of OCSP."  And as we know, although they didn't mention it here, Chrome said, oh, yeah, we have CRLSets.  We're all covered.  It's like, yeah, but they don't work.  We proved it 10 years ago.



So they said:  "Much has been written on the subject of OCSP reliability; and while reliability has definitely improved in recent years," they said, "per Firefox telemetry for failure rate, it still suffers under less-than-perfect network conditions."  Here it is.  "Even among our beta population, which historically has above-average connectivity, over 7% of OCSP checks time out today."  So 7% of OCSP you just don't get a response from whomever you were asking.



They said:  "Because of this, it's impractical to require OCSP to succeed for a connection to be secure; and, in turn, an adversarial monster-in-the-middle can simply block OCSP to achieve their ends.  Mozilla has been making improvements in this realm for some time, implementing OCSP Must-Staple, which was designed as a solution to this problem, while continuing to use online status checks whenever a server fails to staple a response.  We've also made Firefox bypass revocation information for short-lived certificates; however, despite improvements in automation, such short-lived certificates still make up a very small portion of the Web PKI because the majority of certificates are long-lived."



And I should just mention here that when they talk about "short-lived," they're talking about hours or days, not months.  Thus they consider Let's Encrypt's 90-day certs to be long-lived, as they should because that's now the majority of the web.  And what Mozilla says next is quite interesting.  It's something we've never considered before.



They wrote:  "The ideal in question is whether a Certificate Authority's revocation should be directly relied upon by end-users."  What?  They said:  "There are legitimate concerns that respecting CA revocations could be a path to enabling CAs to censor websites.  This would be particularly troubling in the event of increased consolidation in the Certificate Authority market.  However, at present, if one Certificate Authority were to engage in censorship, the website operator could go to a different Certificate Authority."



LEO:  Yeah, they would just route around it.



STEVE:  Yeah.  And they said:  "If censorship concerns do bear out, then Mozilla has the option to use its root store policy to influence the situation," is the way they put it, "in accordance with our manifesto," they wrote.  So that's quite interesting. They must not be talking about typical Certificate Authorities in the U.S., but perhaps CAs located in repressive governments which might be forced to revoke the certificates of websites whose certificates had been issued by them which the government later no longer approves of.



And then I assume that since Mozilla would certainly disapprove of Certificate Authorities being used as censorship enforcement, that's where Mozilla's root store policy would come in, with them completely withdrawing all trust from any such CAs' signatures, much as the industry recently did with Entrust.  And then they finish up, saying:  "Legitimate revocations are either done by the issuing Certificate Authority because of a security incident or policy violation, or they're done on behalf of the certificate's owner, for their own purposes.  The intention becomes codified to render the certificate unusable, perhaps due to key compromise or service provider outage, or as was done in the wake of Heartbleed.



"Choosing specific revocations to honor while refusing others dismisses the intentions of all left-behind revocation attempts.  For Mozilla, it violates principle six of our manifesto, limiting participation in the Web PKI security model."  In other words, as I said, they would be upset with any Certificate Authority that attempted to play games with their customers' certificates revocation status.  So they kind of like being able to decide for themselves rather than always have it be the CA.  And that's one place where requiring stapling of OCSP status becomes a problem.  This allows the browsers to retain more control of that.



And then switching from issues of policy back to issues of technology, they said:  "There is a cost to supporting all revocations.  Checking OCSP slows down our first connection by around 130 milliseconds.  It fails open if an adversary is in control of the web connection.  And it periodically reveals to the Certificate Authority the HTTPS web host that the user is visiting."  Okay.  So they said:  "Luckily, CRLite gives us the ability" - I guess I should just say CRLite - "gives us the ability to deliver all the revocation knowledge needed to replace OCSP, and do so quickly, compactly, and accurately."



Can CRLite Replace OCSP?  Firefox Nightly users are currently only using CRLite for telemetry.  But by changing the preference in some of their settings it can be entered into "enforcing" mode.  And they said there's not yet a mode to disable OCSP; there'll be more on that in subsequent posts.  And that was four years ago.  There certainly is, as we discussed last week, in Firefox an OCSP setting.  It's enabled by default.  You can turn it off.  And we see the effect of it being turned off.  I've not tried, I don't think I've tried turning OCSP off and then turning on CRLite, although I'm not on Firefox Nightly.  And I think it's still not everybody who's got this.  It's only their beta testers who are able to play with this.



Okay.  So let's talk tech.  We can broadly classify major cryptographic algorithms into two classes, lossy and non-lossy.  All encryption is non-lossy, meaning that encrypting a source plaintext into a ciphertext does not lose any of the original's information because decryption, the reverse process, perfectly recovers the original text.



By contrast, cryptographic hashing is a deliberately lossy process.  And for what a hash provides, that's what we want.  As we know, a hash, also known as a digest, takes an original plaintext of any size and reduces it to a single fixed-length output consisting of some number of bits.  The number of bits is chosen to be high enough so that the probability of two different texts hashing to exactly the same combination of every bit is astronomically small.



Now, common sense tells us that it would not be possible to take the entire text of "War & Peace," hash it into a tiny 256-bit - comparatively tiny - 256-bit digest, then somehow reverse that process to get the original text back.  "War & Peace" is definitely not all in there in those 256 bits, even though every character of "War & Peace" has an effect upon the hash's final output.



Okay.  So now let's look at the design of Bloom filters, an entirely different system which, like a hash, is similarly deliberately lossy.  Any Bloom filter starts with a large array of binary bits all set to zero.  The optimal size of the large array is determined by many interacting factors, but for our purpose we're going to use an array containing 1,048,576 bits.  That's exactly 128 Kbytes.  It actually happens to be 2^20.  So that's a nice number.  So basically a megabit; right?  1,048,576 bits.



Now imagine that we want to "add" an awareness of a revoked certificate to this currently empty Bloom filter, this one million bits which are all initially set to zero.  We need to use some fixed characteristic of the revoked certificate.  All certificates contain a thumbprint.  It was traditionally generated by taking the SHA1 hash producing 160 bits of the entire certificate, and later that's been upgraded to 256.  So older certificates have 20 bytes or, as I said, 160-bit thumbprints; and newer certificates have 32 bytes, or 256-bit thumbprints.  But every certificate has a thumbprint.



Now, one of the cool things about a cryptographically strong hash function like SHA1 or SHA256, which we've talked about before, is that all of the bits in the hash are equal.  That is, created equal.  A smaller hash can be taken simply by using some subset of the whole hash's entire value, and it doesn't matter which smaller subset is chosen.  So for our example, we're going to take the lowest 20 bits of the revoked certificate's thumbprint hash.  Thanks to the thumbprint being a hash, these are effectively pseudorandomly set bits.  Because 2^20 is that 1,048,576, the lowest 20 bits of a thumbprint can be used as the index into our bit array.  In other words, it can be used to select exactly one bit from our one megabit array.  And we set that chosen bit to a one.



Okay.  Now let's say that we add more, many more revoked certificates to this blossoming Bloom filter.  Let's say that we add a total of 100,000 revoked certificates.  Since adding each certificate entails taking the lowest 20 bits of its pseudorandom but fixed thumbprint, and setting the corresponding bit in the bit array to a one, this would mean that up to 100,000 bits out of our total of one million would have been set after we added those 100,000 revoked certificates.



I say "up to 100,000," though, because there's a problem.  There would be a good chance that out of those 100,000 certificates, many of the certificates could share the same lower 20 bits.  No certificate is going to share its entire thumbprint, 160 bits or 256 bits.  The chances of that are astronomically low.  But we're deliberately taking only a piece of the whole hash, so collisions are possible.  And as we know, the surprising fact is that the number of these collisions turns out to be higher than we might intuitively suspect.



We've talked about the "birthday paradox," which explains why the probability in this case of any two certificates sharing the same lower 20 bits is higher than we would think.  So in practice, this means that fewer than 100,000 sets, bit settings, will have been made, changing these bits to one, due to collisions where the bit that was set by a new certificate being added had already been set to one by the addition of a previous certificate.



Okay, now, also note that after adding 100,000 certificates, less than 10% of our total bit space of one million, a little over one million, has been used.  Bloom, to his credit, realized that this was inefficient.  And he had a solution:  Instead of only setting one single bit per certificate, let's set several.  And we can do that easily by taking successive 20-bit pieces of the certificate's thumbprint and using each successive 20-bit chunk to set an additional bit.  So let's set five bits in the one megabit array for every certificate we add to it.



So we start again with an empty array with all of its 1,048,576 bits set to zero.  We add the first certificate by taking each of five chunks of 20 bits - remember it's got 160, so we're just going to use 100 of those.  We take each of five chunks of 20 bits from its thumbprint, using each chunk to determine which bit to set.  After adding that one certificate, we'll almost certainly have five bits set.  It could be that a certificate might collide with itself, though the chances are slight.  My point being that, since we're taking five 20-bit chunks, it's like there's a one in a million probability that two of them, actually it's lower than that, but, I mean, it's more probable than that, that some pair of them might have the same 20 bits.  So it's possible.



But probably five bits are set which are representative of 100 bits of that certificate's thumbprint.  And then as before, we're going to add a total of 100,000 certificates.  So each certificate will be setting its five very specific bits to a one.  Now we'll have somewhere less than half but close to, but less than half of the array's bits set due to collisions.  But we'll be very nearly at 50%, right, because we've got 100,000 certificates.  They're trying to set five bits each.  So that would be 500,000 potential, which is half of our one megabyte.  So about half the bits are set, a little less than half, due to collisions.  And Leo, I see that you've got the cold soda can on your forehead.  That's good.



LEO:  So far I'm following you, though.  This is good.  This is good.



STEVE:  Yeah.  I think everyone's...



LEO:  We've got basically a megabit picture with five bits set for each certificate that tells us, really the only thing it tells us is there's a collision, that that certificate, if those five bits are set, is colliding with another certificate.  Not that they're 100% matching.



STEVE:  Well, yes.  So far - but so all we're doing, basically we're OR'ing ones into the bit array.



LEO:  Yes, right.



STEVE:  And so what we have is, as you said, a picture, and but we may have collisions.  We haven't yet used it, but we're going to next.  Let's take our last break.



LEO:  I'm loving this.



STEVE:  And then I'm going to show people how we use this to determine something about a certificate.



LEO:  Okay.  I think, I kind of think I know where you're going on this.



STEVE:  I know.  It's cool stuff.



LEO:  It's not that - it's not really complicated.



STEVE:  No, it's not really complicated.



LEO:  Have we got to the cascading part yet?



STEVE:  No.  And that's where you're really going to need two cans.  You're going to need two cans of soda on your forehead.



LEO:  Okay.  All right.  Let us get back to Bloom filters.  And are you ready?  Because it's time to cascade.



STEVE:  Not yet.  Not yet.



LEO:  Okay.



STEVE:  First we have to see how they actually work.  So we've designed a filter where five bits get set based on five pieces of information from a certificate's thumbprint, which is the hash of the certificate.  And we've added 100,000 certificates, 100,000 revoked certificates to this filter.



LEO:  I understand how this works because, if those bits are set, you can pretty - if they're set you say that there's a likelihood, but not a guarantee that that certificate's in the megapixel.



STEVE:  Correct.



LEO:  But if it's not there, you know it's not there.  Right?  It couldn't possibly exist.



STEVE:  That's exactly it.



LEO:  Yes.  That makes sense.



STEVE:  So let's say that our web browser receives a certificate it wishes to check.  It wants to ask the question, has this certificate previously been added to this Bloom filter?  To answer that question, we take the same five sets of 20 bits from the candidate certificate's thumbprint.



LEO:  And that's a fast thing to do.  It's really quick, yes.



STEVE:  Oh, yes.  That's the other thing.  All of this is super fast.  We successively use each set of 20 bits to test each of those five bits in the Bloom filter.  Here's what we learn from that, and think about this.  As you said, Leo, exactly, if any of the five tested bits are zero...



LEO:  You're done.



STEVE:  ...then we absolutely positively know that the certificate in question could never have been previously added to the Bloom filter's bit array.



LEO:  You can see why this would be fast.  In assembly language, those compares are almost instant; right?



STEVE:  Yes.  In fact, Intel has a single instruction...



LEO:  Oh, of course.



STEVE:  for querying a bit in a large field.



LEO:  Are you set?  Yeah.



STEVE:  So,  yeah.  So if any of the five bits that are tested are zero, then we absolutely positively know that the certificate in question could never have been previously added to the Bloom filter's bit array because, if it had been, then we know that all of its bits would be ones.  So if any of the five bits are zero, it's not revoked.  The certificate is definitely not revoked.



But the problem we still have, if all of the certificate's tested bits are ones, is that we cannot be certain that this is not a good certificate whose five chosen bits just happen to collide with bits that were set by any of the other 100,000 revoked certificates.  And remember at the top I said earlier, "Although false positive matches are a well-known and well-understood likelihood, false negatives are not."  So in other words, a test using a Bloom filter can return either a "datum may be a member of the set" or "this datum is definitely not a member of the set."  The "definitely not a member of the set" is what we get when at least one of the five bits are zero, since that can only happen if this certificate was never added to the filter.



LEO:  This is efficient also because it cuts short the search.  The minute you find a zero you're done.  You don't have to keep going.



STEVE:  Yes, you are indeed a coder, my friend.



LEO:  And if you had fewer than five bits, collisions would be more likely.  If you had more than five bits, collisions would be less likely.  I guess that they figured out five is kind of a nice balance.



STEVE:  Oh, the math is hair curling.  It's got all kinds of calculus that you don't want to even think about.



LEO:  You can also do it by trial and error.



STEVE:  But there are, given the size of the various sets...



LEO:  Right.



STEVE:  ...it's possible to exactly zero in what you want.



LEO:  What's optimum, yeah.



STEVE:  Okay.  So what do we do about this false positive problem which is created by a Bloom filter?  First, it's worth noting that many applications of Bloom filters are not sensitive to false positives.  If Akamai occasionally caches a "one-hit-wonder" file because the file's Bloom bits happen to collide with another file, who cares?  An extra file gets cached, big deal.  Or if Medium is using individual per-user Bloom filters to avoid having users see the same article twice, but that occasionally misfires and shows them something they saw before, again, who cares?  Or if a database mistakenly believes that a record exists and goes to look for it instead of immediately rejecting the request thanks to a Bloom filter failure, that's okay.



So before we solve the problem of Bloom filter false positives, I want to clearly make the point that a simple Bloom filter that is able to almost instantly make a go/no-go determination which never generates a false negative but occasionally generates false positives, can still be an extremely useful tool in computer science.



LEO:  Sure.  You just have to use it in the appropriate place; right?



STEVE:  Right.



LEO:  Yeah, that makes sense.



STEVE:  Yes.  There are, now, there are several important points, and you already touched on one, which are worth pausing to highlight here.  The first is that, in return for the Bloom filter's discrimination "fuzziness," we get amazing efficiency.  The hashes of 100,000 certificate thumbprints can be sorted, but they cannot be compressed because thumbprints are, by definition, incompressible pseudorandom data.  Each modern SHA256 thumbprint is 32 bytes.  So any simple list of those 100,000 revoked certificate thumbprints would require 3.2 megabytes just to store the list.  Yet our trained Bloom filter requires only 128K.  So in return for some inaccuracy, we obtain a factor of just shy of 25 times compression.  So it is super efficient.



And then there's speed.  As you said, Leo, searching a 100,000-entry sorted list is still time-consuming.  And more sophisticated tree structures consume more space and still take time to traverse.  By comparison, consider our Bloom filter.  The certificate we want to test already offers its thumbprint.  We need to perform five tests, each one which directly tests a single bit of the Bloom array.  For those who are not coders, any coder will tell you that directly addressing and testing a bit in an array is one of the fastest things a computer can do.  And as I mentioned, Intel has an instruction that does exactly that.  That's all it does.  You issue the instruction; you get the result instantly.  So nothing could be faster than these tests.



Now consider that with the Bloom array having fewer than half of its bits set, each of these tests has a better than 50/50 chance of selecting a zero, and that the instant we hit any zero in the array when we're doing our five tests, we know that this certificate being tested cannot be represented within this array.  So with five tests, each super fast, and each having a better than even chance of giving us an immediate out and go-ahead, this has got to be the fastest, most efficient and economical way of classifying a certificate as being a member of a larger set.



But unlike Akamai, Medium, and the databases that can tolerate false positives, this application of Bloom filters for certificate revocation absolutely cannot tolerate any false positives.  We can never tell a user that a remote server has just served them a revoked certificate when it has not.  So now what do we do?  You get extra credit if you guessed this is where the "Cascade" enters the picture.  Yes.  To pull this off, it is necessary for the creator of the final CRLite Bloom filter cascade to have access to the entire real-time active certificate corpus.  In other words, a record of every single unexpired certificate, both valid and revoked.



LEO:  How big is that?



STEVE:  It's big.  But we don't need it, just the persons building the filters need it.



LEO:  Right.



STEVE:  Now, although this might appear to be a tall, if not impossible order, the PKI (Public Key Infrastructure) actually already has all of this in place.  All Certificate Authorities  are required to publish every certificate they issue into a Certificate Transparency log.  That log exists, and it's available.  So check this out:



A first and largest Bloom filter bit array, the one we've been talking about, is first populated by adding all known revoked certificates.  As we've seen, this will generate an array with lots of one bits set.  And we know that, once this has been done, any certificate that has any zero cannot be a member of the set of all revoked certificates.  We also know that each of the five tests has a better than even chance of returning a zero, so most good certificates will be identified almost instantly.  But for this application we must enforce a zero-tolerance policy.



Every non-revoked valid certificate in existence is then run through this first-stage Bloom filter to explicitly detect every possible false positive that the first-level Bloom filter produces.  And what do you think we do with those?  That's right.  Those false positives are used to train the second-level filter in the Bloom filter cascade.  In other words, the first filter is trained on revoked certificates.  Then all the non-revoked certificates are passed through it, and any that come out because they're valid, those represent false positives created by the first level.



So those certificates are added to the second-level Bloom filter to contain all of those.  So this second-level cascade, the second-level Bloom, can be thought of sort of as a whitelist to handle the false positives which are generated by the first-level Bloom filter.  Thus any certificate that is not in the first level must be valid.  We know that.  That's the immediate okay.



Then, any certificate that was identified by the first level, which we might call a candidate revoked, and is not also in the second level, must actually be revoked since the second level was trained on valid certificates that were thought to be bad by the first level.  We know that Bloom filters false positive, but they never false negative.  So any certificate that the first level finds which the second level does not find must not have participated in the second level's valid certificate training.  So we absolutely know that it had to have been revoked.



However, there's one last bit of mind-melting logic to cover.  Since Bloom filters are known to produce false positive matches, there's a chance that the second-level Bloom filter might produce a false positive.  That would lead it to believe that a revoked certificate that was found by the first-level filter and had been found to be good by the second-level filter should be trusted.  So believe it or not, a third-level filter is used to catch and categorize those cases that may slip through the second-level filter.  But that third-level filter is the final stage of the Bloom filter cascade, and its decision is final.



So if the first-level filter does not produce a match, we immediately know that the certificate must be valid since that first level was trained on revoked certificates, and Bloom filters never false negative.  But if that first-level filter does produce a match, we then test the certificate against the second-level filter.  If it does not produce a match, we absolutely know that the certificate must have been revoked, since the second-level filter was trained on valid certificates.  And, again, Bloom filters never false negative.



And finally, since the third-level filter is again trained on revoked certificates, if it does not find a match, we absolutely know that the certificate is good.  And if it does find a match, thanks to the pre-weeding out of the earlier filters, we can finally know with certainty that the certificate is a member of the revoked set and must not be trusted.



LEO:  It's sufficient to three levels.



STEVE:  Yes, exactly.  And I should note that since the second-level Bloom filter only needs to be trained on the valid certificates that mistakenly false positive match at the first level, that total number of certificates is very small, so much so that a much smaller bit array and smaller filter will suffice.  And that's even more true for the third level, that's only dealing with really exceptional cases.  So we obtain significantly greater efficiency with successive levels of the cascade.



LEO:  You don't need a megabit on level two.



STEVE:  Right.  Nothing near that.



LEO:  Maybe half that, and then half again or something like that.



STEVE:  And so although this, I get it, is all a bit mind-bending...



LEO:  Oh, super cool, though.



STEVE:  It is super cool.  What we now have is a way of constructing an efficient means of rapidly categorizing certificates as valid or revoked.  In order to do this we need to train a trio of Bloom filters on every possible valid and revoked certificate that they might ever encounter in the wild, and that's actually not impossible.  We can do it.  Once this has been done, and Mozilla has been doing this now for several years, four years, we wind up with a small local certificate classifier that every user can efficiently run in their own browser.  And it can instantly identify any revoked certificate from among millions of candidates without ever making a single error.



This amazing local browser capability is supported and made possible by a massive, continuous, behind-the-scenes effort to produce and distill, four times per day, those three simple Bloom filter bitmaps, which are then made available to every browser over a common content delivery network.  Individual Firefox browsers reach out to obtain the latest set of bitmaps every six hours.  If or when all browsers have access to these filters, we'll finally have solved the certificate revocation problem with both low-latency and total privacy, since every browser will then be empowered to make these revocation determinations locally.



LEO:  So cool.



STEVE:  It is so cool.



LEO:  And now as an exercise for the listener, why is it that you're testing revoked, keeping track of revoked certs instead of keeping track of good certs?  Should be easy to figure out.  It's kind of fun.  I love this.



STEVE:  Isn't that cool?



LEO:  It'll be in Advent of Code this December.  He's very timely with his algorithms.



STEVE:  And it's such a cool technique to use in order to classify something.  And, you know, the fact that most things are going to immediately reject, the top-level Bloom will immediately reject most good valid certs.  Only where there are collisions do you then need to train the second one on the valid certs that are known to collide.  It's just - it's elegant.  And I'm impressed.  I mean, to me it feels like Mozilla is out there paving the way for all the other browsers to steal it, you know, like steal the cool tech that they come up with.  And then, I mean, because, you know, this is all just going to get copied by everybody else.



LEO:  Yeah.  As it should be.  This is a great replacement for OCSP.  This is it.



STEVE:  Yes.  I mean, it is it.  The problem is, as we know, Mozilla's endangered.  Firefox is endangered because...



LEO:  Yeah, but I can see Google adopting this pretty quickly.



STEVE:  Oh, yeah.



LEO:  I mean, this seems like a very clever - and it's nice because all the heavy calculation, which is calculating the hashes, is done on server-side.



STEVE:  Exactly.



LEO:  And just ports these small bitmaps.



STEVE:  Exactly.



LEO:  There are maybe a mega, let's see, a megabit is, what, 1K?  It's 128K.



STEVE:  Yeah.



LEO:  So maybe you throw in every six hours 156K or something.  It's nothing.



STEVE:  Right.  And, yeah...



LEO:  Brilliant.  And instantaneous results.



STEVE:  Yup.



LEO:  Now, I still have to understand why you only need three layers to guarantee.  You're doing 20...



STEVE:  The math is there.  And it turns out that you can show that the first filter will have resolved everything that the third one would.



LEO:  Got it.  So now it's a loop.



STEVE:  So all the mistakes are taken out, yeah.



LEO:  Very interesting.  I will read up on this and Professor Bloom, who thought this all out.



STEVE:  1970.



LEO:  Many years ago.



STEVE:  I was a freshman in high school, and he's like, you know, if you had a lot of bits, and you needed to get them straightened out, this is what you're going to do.



LEO:  I want you to, some day when we've got some time and there's no security issues to talk about, do the Dijkstra algorithm, Edsger Dijkstra's search algorithm.  I bet you could explain that pretty clearly.  That's fun.  Really good stuff.  This is why we love Steve; right?  Aren't you glad you listened to this episode?  And if it's all a mish-mosh in your mind, listen again.  It'll come clear.  And by doing so, you're building brain cells.  You're making yourself smarter.



STEVE:  And you're having much less room for trivia.



LEO:  I have no idea who Jason Momoa is married to, or not.  Doesn't - it's not in my head because I've got Bloom filters.



STEVE:  Yeah, like I said, you want to terminate an annoying conversation at a cocktail party, bring out the Bloom filter.



LEO:  But I have to say, if the person you're talking to says, "Oh, that's interesting," you've got a keeper.  Right?  Now you know.  Now you've got something.



STEVE:  That's true.



LEO:  Somebody you're going to want to talk to some more.



STEVE:  That might be a good first date question.



LEO:  No, it's not.



STEVE:  Maybe save that for later.



LEO:  Steve Gibson's at GRC.com, the Gibson Research Corporation.  His bread and butter is there, too, of course, SpinRite.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#990

DATE:		September 5, 2024

TITLE:		Is Telegram an Encrypted App?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-990.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Telegram's founder, owner, and CEO arrested in France.  What does that mean?  One year after Microsoft began offering free cloud security event logging, how's that going?  To no one's surprise, CrowdStrike is losing customers, but how many?  Microsoft to meet with CrowdStrike and other vendors to discuss new solutions.  Yelp is not happy with Google.  Did/does Google put their thumb on the scale?  Where do you go to purchase yourself some DDoS?  How about sending a Telegram?  Chrome exploits are becoming more rare and difficult to find so Google has upped the ante.  Believe it or not, Cox Media Group is still promoting their incredibly privacy-invading "Active Listening" capability.  How about secretly having foreigners doing all of your work for you?  What could possibly go wrong?  And Johns Hopkins Cryptographer Matthew Green has become increasingly annoyed by Telegram's claims of being an encrypted messaging platform.  So he finally asks the question:  Is Telegram an Encrypted App?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and of course as always there's lots to talk about.  How many customers did CrowdStrike lose, if any; and why Steve says "I'd still be a customer."  We'll also talk about Telegram's founder, owner, and CEO arrested in France with what many are saying is an attack on encrypted communications.  But Steve's going to do a deep dive on Telegram's, and I'm going to put this in air quotes, "encryption."  He says no, it's not really.  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 990, recorded Tuesday, September 3rd, 2024:  Is Telegram an Encrypted App?



It's time for Security Now!, the show where we talk about your security, your privacy, and what's going in the world around us with this cat right here, Mr. Steven "Tiberius" Gibson, the host at GRC.com.  Hi, Steve.



STEVE GIBSON:  You, Leo.	



LEO:  How are you?



STEVE:  I'm looking at these episode numbers, and it's getting pretty exciting.



LEO:  We're inching up.  We're inching up.



STEVE:  Thank goodness I let everyone know a long time ago that it wouldn't be over.  Otherwise it would be really sad, you know.



LEO:  Yeah, see, aren't you glad now? 		



STEVE:  Nine nine zero, nine nine one...



LEO:  The countdown?



STEVE:  Oh, not good.



LEO:  It'd be so sad.



STEVE:  Yeah.



LEO:  So now it's a countdown to nothing.



STEVE:  Yeah.  Well, no.  Actually, I was telling the truth a long time ago when I said, oh, I'm going to have to rejigger my technology to handle four digits because, you know, back when I wrote it we had one, or then maybe two.  But, oh, we're never going to need three, but what the heck, I'll just set it up for three.



LEO:  We could do hex.



STEVE:  No, no, no.  We're going to go 999, there'll be a celebration, that's sometime in November, and then we're going to go right on into four digits.  And however, I'm really sure we won't need five.  That's not going to happen.



LEO:  I think you're safe on that.



STEVE:  I think, yeah.



LEO:  Both of us will be just a memory then.



STEVE:  Okay.  So lots of interesting stuff to talk about.  As I know you've been talking about, but we haven't talked about it, touched on it here, and it had just happened for last week's podcast, and I didn't know what was going to happen.  But Telegram's founder, owner, and CEO has been arrested in France.  So we're going to look at what that means.  Also, one year after Microsoft began offering free cloud security event logging, how's that going?  Also, to no one's surprise, CrowdStrike is losing customers.  But how many?  Microsoft, on that topic, is going to meet with CrowdStrike and other vendors to discuss new solutions.  We'll talk about that.  Also that Yelp is not happy with Google.  You know, did or does Google put their thumb on the scale?  Yelp thinks so.  Where do you go to purchase yourself some DDoS, when that's what you want?



LEO:  I'd like a cup of DDoS.



STEVE:  Yes, how about sending a Telegram?  And Chrome exploits are becoming more rare and difficult to find, so Google has upped the ante.  And Leo, believe it or not, the Cox Media Group is still promoting their incredibly, I mean, just astonishingly privacy-invading so-called "Active Listening" capability.  We're going to revisit that.  Also, how about secretly having foreign agents doing all of your work for you?  What could possibly go wrong with that?  And the reason this podcast is titled "Is Telegram an Encrypted App?" is because that was the title given to the recent posting by our favorite Johns Hopkins cryptographer Matthew Green, who has become increasingly annoyed by Telegram's claims of being an encrypted messaging platform.  So he finally asks the question:  Is Telegram an Encrypted App?  We're going to look at that and answer the question.



LEO:  That was a great blog post, actually.  I really enjoyed that.



STEVE:  A little surprising, yes.  Yeah, he did a great job.



LEO:  Yeah, yeah.  And we've been talking, as you mentioned, we've been talking a lot about Pavel Durov's arrest since it happened a week ago.



STEVE:  Yup.



LEO:  And, yeah, it's quite a story.  But we will get to that in just a little bit.  I am ready with a Picture of the Week.



STEVE:  Ah.  This is a goodie.  So I gave this one the caption "When the Universe is suggesting that you should take the stairs, listen."



LEO:  Oh, dear.



STEVE:  Because we have what appears to be a not-that-well-maintained kind of grungy elevator interior, and it's got some instructions over the panel where you push the button for which floor you want to go to.  It says:  "If elevator does not move, do a small jump.  It should move after."  Now, again, if you get into an elevator, and you see that signage, the stairs really are looking better.



LEO:  Yes.  Good thinking, yes.



STEVE:  So, yes.  You know?  And I don't know, there's some signage off to the right.  There's something about a guy with a mask, it looks like delivery drivers must wear something or other.  Oh, and it says "have temperature," blah blah blah, so like have their temperature taken or something.  And then down below it says "You M," and then I see "CLOS," and then "ELE."  So, like, maybe you must, what, manually close the elevator doors or something?



LEO:  Yeah, you must close elevator doors before pressing a button.



STEVE:  That would be good. 



LEO:  Unless you want a good view as you...



STEVE:  Or just jump up and down, and that'll get the elevator going.



LEO:  You think that's a joke?



STEVE:  Oh.



LEO:  Do you think it's a joke?



STEVE:  No, I think it's, like...



LEO:  It's stuck a little bit?



STEVE:  Again, you get in, you see the sign, and you get out.  And you just - and so that's why the caption, "When the Universe is suggesting that you should take the stairs, listen."



LEO:  Yeah.



STEVE:  Because, yeah, anyway.  Thank you.  I will thank endlessly our listeners.  We've got some goodies in the queue.  So another great Picture of the Week.



Okay.  So I gave this week's lead story the title "Telegram Puts End-to-End Privacy in the Crosshairs" because I think that's probably what's ultimately being tested here.  At the time, as we said, at the time of last week's podcast, the news was that Pavel Durov, the founder, also owner and CEO of the Telegram instant messaging system, had been detained in France after he flew into and landed in French territory on a private jet.  Next, we learned that his status had changed from "detained" to "formally arrested."  And then last Wednesday he was released on five million euros bail and is banned from leaving France since he's now facing charges over his responsibility - this is what they're alleging - for the many illegal and in some cases abhorrent things that Telegram's users have been found doing in light of there being no content moderation, mediation, anything within Telegram of any kind.  And they're holding Pavel responsible for that.



And of course the reason this is intensely interesting is that,  especially to this audience, is that it brings us back to the big and still unanswered question of how the world is ultimately going to deal with end-to-end encrypted messaging, and whether governments are going to allow their citizens to hold truly private electronic conversations without any form of content moderating oversight.  And in the present case of Telegram, the charges which French authorities have levied against Pavel include being complicit in running an online platform that allows sharing of CSAM - which as we know is the abbreviation for Child Sexual Abuse Material - also drug trafficking, fraud and money laundering, as well as not cooperating with authorities when required to do so by law.



Now, the French news outlet Le Monde reported that France's police office that tackles violent crimes against children issued a warrant for his arrest.  And in a LinkedIn post that was later deleted, that office's Secretary-General said that "At the heart of this case is the lack of moderation and cooperation of the platform," which has nearly one billion users in total - though not all in the EU, much fewer than that - particularly in the fight against, they said, "pedocriminality."



And the EU arm of Politico reported that the specific incident that was cited in the arrest warrant was Telegram's refusal to identify a specific user after being served with a judicial request.  Politico wrote, after viewing a document relating to the warrant:  "The warrants for Pavel Durov and his brother Nikolai were issued after an undercover investigation into Telegram led by the cybercrime branch of the Paris prosecutor's office, during which a suspect discussed luring underaged girls into sending 'self-produced child pornography,' and then threatened to release it on social media."  So, you know, creeps are on Telegram.  Okay.



According to the document, the suspect also told the undercover investigators that he had raped a young child.  Telegram did not respond to the French authorities' request to identify the suspect.  As we've often observed, Telegram is the most combative of all the major social media platforms in their attitude and approach to content moderation and lawful assistance requests.  I mean, that's, like, one of the selling points.  And, you know, it paints this as a clear benefit in its own FAQ, which explains that its distributed architecture is used to confound court orders.  Telegram unabashedly boasts - so here's from their FAQ.



They said:  "Telegram uses a distributed infrastructure.  Cloud chat data is stored in multiple data centers around the globe that are controlled by different legal entities spread across different jurisdictions.  The relevant decryption keys are split into parts and are never kept in the same place as the data they protect.  As a result, several court orders from differing jurisdictions are required to force us to give up any data.  Thanks to this structure, we can ensure that no single government or block of like-minded countries can intrude on people's privacy and freedom of expression.  Telegram can be forced to give up data only if an issue is grave and universal enough to pass the scrutiny of several different legal systems around the world.  To this day, we have disclosed," they're saying, "zero bytes of user data to third parties, including governments."



Their terms of service do state that illegal pornographic content is not allowed on its publicly viewable areas.  But that doesn't stop people from doing that.  Its FAQ says it will only take action on illegal content in these areas, which comprise sticker sets, channels, and bots.  However, Telegram assures its users that:  "All Telegram chats and group chats are private amongst their participants.  We do not process any requests related to them."  In other words, within any private groups, which may include up to 200,000 people, anything goes, without any supervision and with an explicit guarantee of technically imposed privacy.  So it should be no surprise that many investigations have found child abuse material for sale on Telegram.



However, there are some interesting details here, as an example of an instance where the details matter and where encryption may not mean what its users imagine.  This is why today's podcast topic will address the interesting question of whether or not and to what degree Telegram is actually an encrypted app, and exactly what that term means.  Our long-time listeners may recall that I have never been impressed with Telegram's encryption from day one because it's a perfect example of what we all know should not be done:  Telegram uses a homegrown cipher that a couple of guys just made up.



LEO:  His brother.  Pavel's brother.



STEVE:  Right.  You know, and it's got some wacky name.  I'll get to it later.  But it's like the information garbling protocol or something.  It's like, what?  And literally Matthew says, "WTF?"  It's like, he's like, whoa.  Anyway, they did this well after the world had learned how to do encryption correctly.  So as we've said a long time ago, nobody needs another cipher.  Nobody needs another hash.  Those building blocks are in place.  They've been time and academically and in-the-wild tested.  They work.  They're solid.  So don't just go gluing some together in some weird arrangement and say, you know, we dare you to break it.  And of course the fact that they've offered a large cash prize to anyone who could break it does not change the fact that it's not based on any sound, formal design, or tested cryptographic system.  So anyway, we're going to take a far closer look at Telegram at the end of today's podcast, since as I said at the top of the show, Johns Hopkins Cryptographer Matthew Green just posted an intriguing piece titled:  "Is Telegram really an encrypted messaging app?"



Okay.  But be that as it may, Telegram does offer one important feature that makes it unique among all of the private messaging systems.  Whereas Telegram, as I noted earlier, can comfortably provide privacy for 200,000 members of a large group, Apple's iMessage groups are limited to 32 participants.  Signal groups are limited to 1,000, and WhatsApp's variant of Signal limits group size to 1024.  It turns out that implementing true end-to-end encryption across large groups with many participants is not trivial.  But what much of the media misses is that, as we'll see, Telegram doesn't actually do that.



So their unique value proposition is to provide large groups with unmoderated communication and certainly some degree of privacy.  Telegram describes itself as a "cloud-based messenger" that provides "seamless sync" across devices.  But to do that, it needs to have access to the content of those messages.  And we know that because Telegram themselves can access the content of conversations.  So it certainly could invest in moderation if it chose too.  It chooses not to.  NBC News reported that child safety groups in the U.S., UK, and Canada all get short shrift from Telegram when they report CSAM on the platform.



And, for example, this is in contrast to an app like Signal, which also espouses and has the technology to actually enforce privacy-first values.  Signal's built its app so that its technology implements those values as much as possible while still enforcing privacy.  So although Signal collects no content from its users and only minimal metadata about how they use the service, Signal is able to and will respond to law enforcement requests, but only to the extent of providing account creation dates and the date an account last accessed Signal.  This means that while Signal is not, in practice, a great deal more helpful than Telegram, at least Signal is not openly combative and can honestly say that it has wholeheartedly cooperated with court orders to the limit of its ability and technology.  When Telegram says that, it's not true.



Okay.  So what of Pavel Durov?  This may just be a shot across the bow.  And it might wind up being good for Telegram's business model to see their founder and CEO being detained and tried for his refusal to comply.  Since Telegram currently has only 41 million users in the European Union, this falls short of the 45-million user threshold that would subject it to the EU's Digital Services Act.  With Telegram not categorized as a very large online platform, it's not subject to the EU's stricter transparency and content moderation rules.  However, the Financial Times recently reported that the EU is now investigating Telegram for misrepresenting the total number of EU users in order to fall below that 45 million user threshold.  Yeah, right.  I'm shocked.  Last February's claim that they only have 41 million users within the EU is going to be carefully examined for its veracity.



Now, the one thing that, like, this gives me the occasion, and I think it's important to observe before we move on, and we'll be coming back to what Matthew Green said in a minute, is that both of today's major mobile platforms, iOS and Android, manage their client apps with an iron grip.  They do this to enforce both security and control over these client apps.  And we've spent a lot of time through the years talking about all of the ins and outs and mechanisms for this.  So, for example, you know, something close to home here, the reason SpinRite boots directly over its user's hardware and brings along its own minimal OS is because it cannot obtain the direct hardware access it requires from within any operating system environment.



But nothing like that exists for our mobile operating systems.  None of the various messaging platforms are able to obtain anything approaching direct access to the platform's underlying hardware.  So we should always be mindful of the fact that the OS runs the camera, runs the screen, and runs the virtual keyboard; and that access to those resources is granted to the applications that are running.  That's why we're able to seamlessly switch among applications without any application being able to prevent that.  Apps are powerless clients of their mobile platform OS.  So a messaging app such as Signal, WhatsApp, Telegram, or iMessage may be as clever as it wishes with how the content that it communicates is encrypted and decrypted.  But everything that is eventually communicated to and from its users passes through the control of the OS, and that OS is always able to "see" everything that's happening in the clear without any form of obfuscation or encryption.



I think we need to hold onto that because it's easy to get focused on the ins and outs and specifics of any given messenger.  But our mobile OSes have an iron grip over all of these messaging apps.  And what the user sees, the decrypted content coming out of the app and onto the device's UI surface and going in, the OS is the one that has unfettered, unencrypted access to that.  So there's a bit of a game of "hot potato" going on here, with no one wanting to take responsibility for the content that's passing through their control before it's encrypted or after it's decrypted.  But the truth is that the vendor of the underlying platform - Apple or the supplier of an Android OS - is in the unique position to monitor what's going on before it's turned over to any messaging app, and to similarly inspect what its client apps decrypt before it's presented to the user.



Now, we know, we've talked about this, too, I mean, this is a difficult subject.  We know how adamantly the platform vendors want to stay as far away as possible from taking any responsibility for what their users and their client apps do.  And I know that we all want to retain the total privacy that we're currently being assured we're receiving.  But Pavel Durov's arrest and indictment by French authorities shows us that we should probably regard the privacy we're enjoying today as fleeting, since no government wants to be completely blind to the conduct of its citizenry.



Okay.  So to set the stage for some news, recall that five months ago, last April, the U.S. Cyber Safety Review Board released a rather scathing report which squarely placed the blame on Microsoft for the nation state-linked intrusion into Microsoft Exchange Online which led to the theft of about 60,000 U.S. State Department emails that previous summer.  The CSRB report stated that the breach "was preventable and should never have occurred."



The report elaborated that a series of operational and strategic decisions by Microsoft pointed to a corporate culture that deprioritized investments in enterprise security and rigorous risk management, despite the central role the company plays in the larger technology ecosystem.  The CSRB urged Microsoft to publicly share its plans to make fundamental, security-focused reforms across the company and its suite of products.  The board also recommended that all cloud service providers and government partners enact security-focused changes.



Okay.  So among the criticism that was heaped upon Microsoft last year was that it was charging, you know, extra money for zero-cost features such as security logging that would have gone a long way, had more of the government entities been using them, to help detect the early states of the various intrusions its users and customers had been experiencing.  The tech savvy Senator Ron Wyden said at the time:  "Unfortunately, as Microsoft's $15 billion-plus cybersecurity business grows, Microsoft's incentives are not to deliver secure operating systems and cloud software to its customers, but to deliver insecure products and upsell them on cybersecurity add-ons.  It should not have taken multiple disastrous hacks of federal systems for Microsoft to make essential security features standard for government customers, but better late than never."



So we're talking about this today now because, one year later, evidence is emerging of the effect, the beneficial effect, of something as simple as free security logging.  Last Tuesday, the publication Cybersecurity Dive posted a report titled:  "CISA officials credit Microsoft security log expansion for improved threat visibility."  They wrote:  "Greater access to Microsoft event logs is paying off for U.S. government agencies and critical infrastructure providers, which have gained greater visibility into their network environments, the Cybersecurity and Infrastructure Security Agency said Saturday."  You know, CISA.



"Microsoft expanded free access to security logs in 2023 after a state-linked threat actor stole thousands of emails from the State Department after gaining access to Microsoft Exchange Online.  Jeff Greene, CISA's executive assistant director for cybersecurity, confirmed via email:  'Yes, Microsoft has expanded access to the logging elements that were used by the State Department to detect the 2023 compromise to a vastly larger set of customers, including all federal agencies and numerous critical infrastructure organizations.  These new logs are being used by organizations today to detect threats.'  Greene added:  'CISA will continue to work with Microsoft and other companies to ensure that their products are secure by design, and that Microsoft lives up to the commitments it has publicly announced around improving the security of its products following the 2023 compromise.'



"The win for the U.S. government comes as CISA, along with the FBI, National Security Agency, and a group of foreign cybersecurity authorities led by Australia, released a best practices guide for event logging last week.  The new guide is part of an effort to combat sophisticated threat activity from state-linked threat groups, such as Volt Typhoon.  The group uses living-off-the-land techniques to disguise its threat activities using normal security tools that won't trigger alerts when moved around computer networks.



"Security researchers at ReliaQuest have been tracking a ransomware actor known as Medusa, which is also using living-off-the-land techniques in multiple attacks.  Alex Capraro, cyber intelligence analyst at ReliaQuest, said via email:  'By implementing the best practices for event logging and threat detection outlined in this guide, organizations can enhance their ability to identify and mitigate malicious activities, thereby protecting their networks, devices, and data from compromise.'"



So yay, you know, it's unfortunate that Microsoft had to have so many problems and come under so much pressure before it made something that costs it virtually nothing free because it was making money from selling something that it's no longer making money from.  But really, I mean, anyone who's got any experience with IT security understands and has, I'm sure, used logs to find out what's going on.  I think the first instance where I saw logging being used to a level that at the time I thought was a little over the top was about 25 years ago, Mark Thompson, my friend whose site is AnalogX.com - Leo and I know Mark, he's been a friend of ours for decades.



LEO:  Is he still doing his AnalogX?



STEVE:  Yeah, he's still busy doing stuff, Leo.  You know, Mark made a comment about - I think it was we were talking about something, and he made a comment that he was logging something.  And I thought, I mean, it was like logging, like, how long his toothbrush takes to charge or something.  I mean, it was, like, what?  And but what do know; you know?  Turns out that that was useful somehow.  And I know that over time I've increased the amount of logging I'm doing.  And sure enough, I mean, I guess the point is you don't know what you don't know until you wish you knew.



"And if everything is being logged, then you may have to do some log processing.  You know, and I roll logs over monthly and zip them down because the logs tend to compress massively down to things that are much smaller.  But, you know, sure enough, I find myself going back and looking through logs to obtain information that I wasn't specifically thinking that I would need.  But, you know, if you log it, it's going to be there when you need it.  So anyway, the idea, I'm not at all surprised that this is a benefit.  And, you know, you could argue 30 years ago that hard drives were expensive, and you didn't want to log everything because, oh, think of all the space it would take.  Now, you know, hard drives, data storage is just - it's free.  So why not log?  And on that note, Leo...



LEO:  Log everything.  All the time.



STEVE:  Log everything.  Why not take a break to tell us about our sponsor?  And then we're going to talk about CrowdStrike and how they're doing with their customers.



LEO:  Gladly.  Gladly, Steve.  All right, Mr. G.



STEVE:  Okay.



LEO:  On we go with the show.



STEVE:  So far, CrowdStrike reports that it expects to lose around $60 million in net new annual recurring revenue and subscription revenue in the aftermath of its technical outage.  Now, I don't have a good sense for what that represents as a percentage of total revenue, but it does not sound like much.



LEO:  Yeah.



STEVE:  Because CrowdStrike is, you know, it's the big player in this EDR, the Endpoint Detection and Response market.  So nevertheless, CrowdStrike is endeavoring, as you would expect them to, to retain customers by offering various discounts.  Their CEO George Kurtz denied rumors that the company was losing customers to rivals, but of course that will happen to some degree after the so-called "CrowdStrike outage," which has now been named.  Although, you know, as I said, I'm sure I would be staying unless I was in some way otherwise unhappy because the changes they've made since have seemed solid.  We know, we talked about last week how George went to the Pwnie Award and accepted like the biggest mistake ever in history award, like in person; whereas other companies like Microsoft have just blown it off.  You're not going to get anyone from Microsoft there.  So that was impressive.  And, I mean...



LEO:  A broken bone is always stronger when it heals; right?



STEVE:  They've really, yeah, I mean, you can have an employee that screws up and, like, over and over and over and refuses to learn a lesson, in which case, okay, fine, we're going to have to let you go.  But, you know, it's possible also to learn a lesson.  And I'm sure they've, you know, this has really sunk in.  George said that he's putting that award in the lobby so that all the employees have to look at it.



LEO:  That's a good idea.  That's a great idea.



STEVE:  When they come into work every morning.



LEO:  We don't want another one of these; okay?



STEVE:  Yeah.  Yeah.  So for what it's worth, both SentinelOne and Palo Alto Networks have claimed that they've been fielding calls from soon-to-be-ex CrowdStrike customers over the past few weeks.  Again, I don't doubt that for a moment.  But to me, it doesn't seem like that many are leaving.  And we actually, I sent the email for this podcast out early today because I started working on it really early yesterday and so got it finished earlier this morning than I normally do, and notified about 8,900 of our listeners of the contents.  We have a listener who works at CrowdStrike, and he already sent me some feedback and said, "For what it's worth, we're doing fine."  And so, you know, I'm sure they are.



And on that note, interestingly, Microsoft will be meeting privately with a bunch of these guys.  Exactly one week from today, on September 10th, Microsoft will host a Windows Endpoint Security Ecosystem Summit, their announcement said, at their Redmond, Washington headquarters.  Their announcement said:  "Microsoft, CrowdStrike, and other key partners who deliver endpoint security technologies will come together for discussions about improving resiliency and protecting mutual customers' critical infrastructure."



They said:  "Our objective is to discuss concrete steps we will all take to improve security and resiliency for our joint customers.  The CrowdStrike outage" - this is Microsoft's phraseology - "in July 2024 presents important lessons for us to apply as an ecosystem.  Our discussions will focus on improving security and safe deployment practices, designing systems for resiliency and working together as a thriving community of partners" - we're all happy here - "to best serve customers now and in the future."



They finished, saying:  "In addition to ecosystem partners, Microsoft will invite government representatives to ensure the highest level of transparency to the community's collaboration to deliver more secure and reliable technology for all.  It's expected that the Windows Endpoint Security Ecosystem Summit will lead to next steps in both short- and long-term actions and initiatives to pursue, with improved security and resilience as our collective goal.  We will share further updates on these conversations following the event."



So I would imagine that the government representatives are invited as a means of showing that something is being done to keep anything like this from ever happening again.  And in other reporting about this I saw that Microsoft plans, not surprisingly, to discuss new ways of building these EDR products so that they can still get their job done while relying more on safer user-mode code and less on proprietary kernel drivers.



LEO:  And that's the key, isn't it.  Keep them out of ring zero.



STEVE:  Yeah.



LEO:  Give them an API.



STEVE:  Yeah.  And it's really, well, okay, so it's difficult to do.  That is, essentially Microsoft would have to provide hooks all over the place which the various EDR vendors now use.  That is, you know, they install a driver.  And when the system's booting up, they go and hook a whole bunch of Microsoft's APIs themselves.  And by "hook" I mean essentially the idea is that they revector the API service which the OS publishes so that any client running on Windows actually calls into this driver, the proprietary third-party driver, which examines the call, decides what it thinks about it, and then, if it looks okay, forwards it to Windows, the Windows kernel, where it would have normally gone directly to.



So basically it's a comprehensive filter, like, wrapping around the Windows OS.  So, you know, Microsoft doesn't want to offer that.  I mean, and this is why it's been so limited so far.  There are some things, yes, that you can do.  You know, AV vendors have some hooks they can use.  But nothing like the degree of low-level access that is really necessary to monitor the behavior of things that are trying to use Windows.



So it's going to, I mean, it's an interesting dance.  And of course Microsoft is marketing the crap out of their own solution, you know, Windows Defender for Enterprise and everything, because it's like, well, if you just used ours, you wouldn't have had a problem.  It's like, right.  Nor would we have had the functionality.  You know we heard from many users who are using CrowdStrike who said this thing saved our bacon a number of times.  So, yeah, we weren't happy that we all had to get up at 1:00 a.m. in the morning and work all day and lost a day of productivity.  But, you know, we're sticking with them.



Anyway, it is certainly the case, and we expected, right, that Microsoft would be holding a meeting with the vendors and say, okay, what do we do about this?  And of course Microsoft had response and responsibility, too.  Many people were saying, why didn't Windows, like, safe boot fix this?  Why wasn't it possible to identify the source of the trouble and then say, okay,  well, we're going to bring you back up, but you're not going to have your EDR solution enabled until you roll it back somehow.  So Microsoft's resilience, you know, the core Windows resilience could have been much higher than it actually turned out to be.  So, yeah, lots of things for everybody to fix.



I just wanted to note in passing that Yelp has filed an antitrust lawsuit against Google.  It seems, Leo, that Google has reached the size that Microsoft once did back in those days,  and their behavior is being viewed as a little aggressive by an increasing number of entities.  In this case, Yelp is alleging that Google has a monopoly over the search market, no surprise there, which it is abusing to promote its own review business.  Which of course Yelp is a famous...



LEO:  [Crosstalk] Yelp, yeah.



STEVE:  Yeah, famous reviewer.



LEO:  Yelp's been, by the way, whining about this for decades.  They went to the EU.  That's one of the reasons the EU investigated Google in the first place.



STEVE:  Right, right.



LEO:  So nothing new, and it's interesting they're taking a direct approach now.



STEVE:  Yes.  And it's certainly true that, as we know, controlling search is an incredibly powerful place to be.  You know, we view the Internet through what our chosen search engine reveals to us.  And I've spoken of her before, my wonderful luddite realtor friend thought that Google WAS the Internet.  She, you know, she didn't understand, like when she went to the  Google, that she wasn't, like, that it wasn't the Internet.



LEO:  It's the Internet.



STEVE:  Yes, like that's, you know - so, yeah.



LEO:  She was actually more insightful than we might realize.



STEVE:  Yeah.  I cried, "Oh, no, Judy, that's not the way - what?"  "Just go away, Steve.  I know what I'm doing."



LEO:  How do I find it if it's not on Google?



STEVE:  That's right, doesn't exist.  Good luck.  So anyway, of course there's a reason why SEO, you know, Search Engine Optimization is a booming business, that it matters if you're on the first page of Google's results or the second or where.



Okay.  So everyone seems to be piling on Telegram this week.  And it's, you know, not as if they probably don't deserve more attention.  And in today's Internet threat landscape, that's what's going to happen on any large unmoderated social network platform, which everyone assumes is somehow secure.  In this instance, the security firm Falconfeeds has taken a deep look into the flourishing business of DDoS for hire, or more specifically, DDoSaaS as it's formally called:  DDoS-as-a-Service.  So add the SaaS on the end, DDoSaaS.



In a posting last Thursday titled "DDoS-as-a-Service:  The Dominating Phenomenon on Telegram," Jacob Abraham wrote.  He said:  "In today's digital landscape, Distributed Denial of Service attacks have become one of the most powerful tools in a cybercriminal's arsenal.  These attacks, often facilitated by DDoS-as-a-Service platforms, DDoS-for-Hire services, and botnet-for-hire networks, can disrupt online services, extort businesses, and even advance political agendas.  At Falconfeeds.io, our latest research reveals a staggering 3,529 DDoS incidents occurred in Europe during just the first half of 2024, making up 60% of the total cyberattacks we analyzed.



"The rise of DDoS-as-a-Service on platforms like Telegram is a significant contributor to this alarming trend.  Telegram has emerged as a hotbed for cybercriminals looking to offer DDoS-as-a-Service.  On various Telegram channels and groups, vendors openly advertise a range of DDoS attack services at different price points, making it alarmingly easy for even those with minimal technical expertise to hire a DDoS attack.  Telegram's encryption and anonymity features create an ideal environment for these illegal activities to flourish unchecked.



"Our research," they wrote, "has identified over 140 Telegram channels and groups actively offering these services, with 80% of them being currently active and trading these services primarily through cryptocurrencies.  This trend underscores the growing accessibility and anonymity of DDoS attacks, posing a significant threat to businesses and individuals alike.  So-called 'Basic Attacks' are available for as little as $10 per month.  And the power and cost scales upward with more 'Sophisticated Attacks' being prolonged, with high-intensity costing as much as thousands of dollars.  Price lists are often displayed on Telegram channels, with discounts available for repeat customers or bulk orders."  Oh, my god.  "This availability and accessibility has turned DDoS into a commodity, available to anyone willing to pay."  And again, where are these services to be found?  On Telegram.



LEO:  Okay.  But wait, Steve.



STEVE:  You no longer even need the dark web.



LEO:  Read that whole news story again, replacing Telegram with the Internet and channels with web pages.  It's the same story.  So I don't understand what the point is.  Yeah.  You can also get all of that stuff on the Internet.  Getting rid of Telegram won't solve that problem.



STEVE:  Well, I'm not aware of any website that you just go to on the Internet.



LEO:  Oh, we've shown them.



STEVE:  Well, that's the dark web.



LEO:  Oh, the dark web.



STEVE:  Which is very, very difficult to get to.  You have to have Tor.  You've got to have Onion addresses.  I mean...



LEO:  So it's making this so easy that that's the problem.  Okay.



STEVE:  Yes.  And as we know, ease of access really changes the whole threat landscape.  



LEO:  Yeah.  We'd like to keep attacks away from the unwashed masses.  We only want people who know what they're doing to...



STEVE:  $10, Leo.  The bar of entry is really low.



LEO:  It's pretty easy, yeah.



STEVE:  Yeah.  Okay.  So last Wednesday Google announced that it would be increasing, in some cases by as much as a factor of five, the reward bounties it would be offering for the responsible disclosure, discovery and then disclosure, you know, reporting to them privately of Chrome exploits due to the increased difficulty, which is good news for everyone, of exploiting Chrome.  So that's all good news for the world's number one web browser.



Google said, they wrote:  "Time flies.  Believe it or not, Chrome Browser turns 16 this year."  Which, Leo, means you and I have been doing this podcast...



LEO:  We're really old.



STEVE:  ...longer than Chrome has been around.  We were three years into this before Chrome happened.



LEO:  I should go back and find that episode where you do the story.  "And now Google has announced it's going to release its own browser."  That would be interesting.



STEVE:  Yeah.  I mean, remember, we were talking about IE6 back at the beginning.



LEO:  True.  That's a good point, yeah.



STEVE:  You know, and like Firefox 4 or something.



LEO:  We're almost as old as Google itself, to be honest.  We've been around a while.



STEVE:  I do remember when a friend of mine said, hey - because we were all using AltaVista.



LEO:  Right.



STEVE:  That was the best search engine that there was then.



LEO:  Right, right.



STEVE:  And he said, "Hey, some Stanford guys came up with something.  Check this out, it's called - it's got a strange name.  It's Google."  It's like, what?



LEO:  I remember when Dvorak would use that as a litmus test to see if you were really a geek.  He'd say, "What search engine do you use?"  And if you said Excite or AltaVista, he's go, huh. 



STEVE:  Or Yahoo or something.



LEO:  Or Yahoo.  If you said Google, he'd go, hmm.  Doesn't work anymore.



STEVE:  No.



LEO:  No.



STEVE:  No.  My realtor is using Google, so...



LEO:  Yes, exactly.  It's the Internet.



STEVE:  Right.  Okay.  So 16 years old, and their VRP, which is their Vulnerability Rewards Program, to their credit is turning 14.  So it only took them two years, Google was two years old when they decided, you know, we should start rewarding people for finding vulnerabilities in Chrome.  So that's good.



Google posted:  "As Chrome has matured over these years, finding the most impactful and exploitable bugs has become more challenging.  At the same time, new features are frequently introduced into Chrome that may result in new issues which we also want to encourage being reported.  Therefore, it is time to evolve the Chrome VRP rewards and amounts to provide an improved structure and clearer expectations for security researchers reporting bugs to us and to incentivize high-quality reporting and deeper research for Chrome vulnerabilities, exploring them to their full impact and exploitability potential.



"In this blog post, we'll explain how we've moved away from a single table of reward amounts for non-mitigated bugs, and separated out memory corruption issues from other classes of vulnerabilities.  This will allow us to better incentivize more impactful research in each area, and also reward for higher quality and more impactful reporting."



Now, I should mention that reading between the lines what they're sort of saying is we're willing to pay if you're willing to do more work after you find a problem.  In other words, a lot of people have been saying, hey, look, I made Chrome crash.  Pay out.  And now Google is saying, well, okay.  If you just make it crash, this is how much you get.  But if you're willing to, like, go deeper and do more of our work for us post-crash, then we're willing to make it worth your time.  And that makes sense to me.  I mean, that's good because wait till you hear what you can earn if you go all the way here.



So they wrote:  "We've remodeled our reward structure for memory corruption vulnerabilities into the following categories."  They've got four:  "High-quality report with demonstration of remote code execution."  They said:  "Report clearly demonstrates remote code execution, such as through a functional exploit."  And that's the big money.  Or "High-quality report demonstrating controlled write, where a report clearly demonstrates attacker controlled writing of arbitrary locations in memory."  Third:  "High-quality report of memory corruption.  Report of demonstrated memory corruption in Chrome that consists of all the characteristics of a high-quality report."



And finally:  "Baseline" is their minimum.  They said:  "A report consisting of a stack trace and proof of concept displaying evidence that memory corruption is triggerable and reachable in Chrome."  So, right, different levels, different bar settings that they're asking you to jump over.



And they said:  "While the reward amounts for baseline reports of memory corruption will remain consistent, we have increased reward amounts in the other categories," meaning where you're willing to go do more work and give us more, "with the goal of incentivizing deeper research into the full consequences of a given issue.  The highest potential reward amount for a single issue is now $250,000."



LEO:  Oh, wow.



STEVE:  A quarter million dollars.



LEO:  That's enough to live on for a few months.



STEVE:  Yes, it is, for "a demonstrated remote code execution in a non-sandboxed process.  If the RCE in a non-sandboxed process can be achieved without a renderer compromise, it is eligible for an even higher reward, to include the renderer RCE reward."  So you can get them both.



So I've got a link in the show notes.  I'm not going to go into any finer detail here.  But anyone who's interested in more detail can follow the link.  It's to Google's "bug hunters" posting.  And I think it's a good move, and good news that since Chrome is becoming more difficult to exploit, the payouts are increasing commensurately.  This may also be the first time, and I really give them credit for this, Leo, the first time I've ever anywhere seen a software publisher actually say, they wrote this:  "At the same time, new features are frequently introduced into Chrome that may result in new issues which we also want to encourage being reported."



Anyway, anyone who's been following this podcast for more than a few months will think, "Yeah, of course."  You know, because we talk about this all the time, like Microsoft won't leave Windows alone, so they're never getting bugs fixed.  They're introducing as many every month as they're fixing.  So it's just rolling forward.



LEO:  Yeah.  Yeah.  But for them to admit it it's a pretty big deal.



STEVE:  Yes.  Who's actually ever heard any publisher say that?  So props to Google for that, yeah.  Okay.  Yikes.  Believe it or not, Leo, when I encountered this next bit of news I thought I was experiencing dj vu.  The summary was titled "CMG's Active Listening," and it read:  "After media companies and device vendors spent a decade telling customers that microphones baked into their devices are not secretly recording audio, a leaked pitch deck from the Cox Media Group (CMG) is advertising a new service that can show ads to users based on what they've said near microphones.  Google kicked CMG from its advertising platform after 404 Media acquired the slide deck and then asked Google to comment."



Okay, now, when I read that it was ringing some bells.  I went to GRC's Security Now! page and entered "Cox Media Group" into the search bar in the upper right of all of GRC's pages.  The first link and summary that appeared was from our podcast #953.  That was the last podcast of last year, dated December 21st of 2023; and that podcast was titled "Active Listening."  After the news of what CMG was reportedly doing and bragging about on their own web page, which had the URL ending in "active-listening-an-overview," they took the page down, but not before the Internet Archive's spiders found and archived the page.  And that was GRC's shortcut of the week, which is still pointing to the page in question.  So grc.sc/953, and it's still every bit as unnerving as it was nine months ago.



The page starts out saying:  "Imagine a world where you can read minds.  One where you know the second someone in your area is concerned about mold in their closet, where you have access to a list of leads who are unhappy with their current contractor, or know who's struggling to pick the perfect fine dining restaurant to propose to their discerning future fianc.  This is a world where no pre-purchase murmurs go unanalyzed, and the whispers of consumers become a tool for you to target, retarget, and conquer your local market.  It's not a far-off fantasy, it's Active Listening technology, and it enables you to unlock unmatched advertising efficiency today so you can boast a bigger bottom line tomorrow.  Do we need a bigger vehicle?  I feel like my lawyer is screwing me.  It's time for us to get serious about buying a house.  No matter what they're saying, now you can know and act."



And lower down under the "how we do it" they say:  "Whether you're a scrappy startup or a Fortune 500, Active Listening makes the unreachable in reach.  CMG can customize your campaign to listen for any keywords and targets relevant to your business.  Here's how we do it:  We flesh out comprehensive buyer personas by uploading past client data into the platform.  We identify top-performing keywords relative to the type of customer you're looking for.  We set up tracking via pixels placed on your site so we can track your ROI in real-time.  AI lets us know when and what to tune into.  Our technology detects relevant conversations via smartphones, smart TVs, and other devices.  As qualified consumers are detected, a 360 analysis via AI on past behaviors of each potential customer occurs.



"With the audience information gathered, an encrypted evergreen audience list is created.  We use the list to target your advertising via many different platforms and tactics, including:  Streaming TV, OTT, Streaming Audio, Display Ads, Paid Social Media, YouTube, Google/Bing Search (pay per click).  Our technology provides a process that makes it possible to know exactly when someone is in the market for your services in real time, giving you a significant advantage over your competitors.  Territories are available in 10- or 20-mile radiuses, but customizations can be made for regional, state, and national coverage."



And then, in their own FAQ, incredibly, they actually ask and answer:  "Q:  Is Active Listening Legal?  A:  We know what you're thinking.  Is this even legal?  The short answer is yes.  It is legal for phones and devices to listen to you.  And here they actually wrote the following:  When a new app download or update prompts consumers with a multi-page terms of use agreement, somewhere in the fine print Active Listening is often included.  Unbelievable.  Q:  How Does Active Listening Technology work?  A:  Our technology is on the cutting edge of voice data processing.  We can identify buyers based on casual conversations in real time.  It may seem like black magic, but it's not.  It's AI.  The growing ability to access microphone data on devices like smartphones and tablets enables our technology partner to aggregate and analyze voice data during pre-purchase conversations."



So what just happened to bring this back on our radar from nine months ago is that 404 Media, that same group that had previously reported on CMG's web page, which was quickly taken down, obtained the marketing pitch deck that is still, nine months later, being sent by CMG to prospective companies.  404 Media forwarded the deck to Google, who then reportedly kicked CMG off its Partner Program in response.  That of course was the right thing for Google to do.  But how is it that a massive media group such as CMG is able to, with a straight face, say that consumers are permitting this, making it legal for them, because "somewhere in the fine print" this permission is being given.  Unbelievable, Leo.



LEO:  Yeah, I feel like, you know, when this story first broke almost a year ago...



STEVE:  Yeah.



LEO:  ...and we talked about it - oops.  I don't know what that's doing there.  Turn that off.  That's our Discord, doing their thing.  We kind of thought, well, this is probably an overstatement on the part of Cox Media Group.  You know, these guys are salesmen and saying, well, we know what people are talking about probably.  I mean, do you think that Amazon is sending the contents of Echo texts to CMG?  I don't think so.  Or Apple?



STEVE:  Maybe it's, well, now, we know that Amazon responds to keywords.



LEO:  Yeah.



STEVE:  I mean, at least the enabled keyword.  Maybe it's responding to a broader range of specific phrases.  I don't, you know, I don't know.



LEO:  I don't know, either.  But I think it's completely possible to say that these guys are just salespeople overselling what they know because I can't see evidence that they actually, I mean, yeah, they probably can get stuff from Smart TVs.  I doubt there's much Samsung won't sell.  But I can't imagine that Amazon or Apple or Google [crosstalk].



STEVE:  How is Apple - and no, there's just no, I mean, maybe Android devices with some app that has like been installed and asked for permission to access your microphone?



LEO:  Right.  But you know when the microphone's accessed because a light lights up.



STEVE:  Well, yeah.



LEO:  I mean, we all know we're carrying microphones around, but they're absolutely, it's kind of an unwritten law that you don't record everything and then send it to marketers.  If they get caught doing that, you know that those companies are going to be history.



STEVE:  Well, and they're bragging about doing it.  So, like, how - I don't know, Leo.



LEO:  I honestly think this is Cox Media Group overhyping their capabilities in order to make sales.  That's what I think.  Because I don't...



STEVE:  And maybe they're not vulnerable to being held accountable because they're not actually doing it.



LEO:  That's exactly my point.



STEVE:  So if someone says, like, hey, what is this?  It's like, oh, well, we're not really doing that, we're just telling people we are.



LEO:  Well, and maybe there are, I mean, there are a few devices that they are doing that with.  But they're not doing it with the phone in your pocket.  They're not doing it with your voice assistant, I'm pretty sure.  I mean, if they are, that's a huge scandal.  But I think it's much more likely that Cox Media Group's lying, to be honest with you.  Not lying.  Overstating their capabilities, how about that?



STEVE:  Yes.  Embellishing.



LEO:  Embellishing.  I mean, what salesperson ever embellishes?



STEVE:  Yeah.  Who's ever heard of that?



LEO:  Nobody I know.  Would you like me to take a little break here, sir?



STEVE:  Yes, sir.  That'd be good.  I'm going to embellish my coffee.



LEO:  All right, Steve.  You're back.  You're on.



STEVE:  Okay.  So last week's serious propeller cap pure computer science nerd fest episode was every bit as much of a hit as I hoped it might be.  You know, it's fun thinking about new things, especially for this audience.  But I wanted to take a moment to acknowledge some of the feedback I received from a number of our more technical listeners who correctly observed that the three layers of Bloom filtering I described last week could not always be guaranteed to be sufficient.  Those observations were correct.  My goal was to clearly establish the concepts involved; to talk about Bloom filter applications where the filter's inherent tendency to produce false positives would and would not represent any actual trouble; and then, in cases where no false positives could be tolerated, to introduce the idea of successive layers of Bloom filters, where the next successive layer of the cascade would capture and be trained on the set of false positives which had been generated by the previous layer.



So those who noted that the third layer might also produce some false positives were 100% correct.  And a fully realized implementation of this system actually takes the form of a variable depth cascade where successively smaller layers continue to be added and trained until no misbehavior is observed when the entire corpus of unexpired certificates is fed down through the entire cascade.  Eventually, there will be nothing to train the next layer on, since not a single false positive will have managed to make its way all the way down through the cascade.  And I guess, you know, in retrospect, I could have explained that last week.  But as it was, I felt like it was already a lot for our listeners to take in.



And also for the record, I used one megabit as the number of bits in the first Bloom filter level, which would be addressed by 20 bits taken from any candidate certificate's hash, purely for the sake of illustration since that made it much easier to describe and visualize.  The actual size of the first filter and of each successive filter, as well as the number of Bloom layer bits that will be set by the addition of each certificate, are all well understood and are determined by a bunch of very fancy math.  But, you know, that was technically irrelevant to our understanding of the overall concept of probabilistic Bloom filtering, and getting that across was the goal of last week's.  So anyway, definitely big props to our listeners who said, "Uh, Steve, you do realize that three layers might not always do the job; right?"



And, you know, speaking of listeners and their feedback, I got an interesting piece of feedback.  We were talking a couple weeks ago about the security company who discovered that they had inadvertently hired an actively hostile employee based in North Korea who'd gone to extreme measures to spoof their identity and set up a fake domestic operating location.  What happened to one of our listeners is a little different, but I think it's just worth sharing it.



He wrote:  "Hi, Steve.  I was interested in the story from SN-985 about North Korean hackers posing as U.S. workers and getting hired by American tech companies.  I'm currently between jobs, and I got an email from someone claiming to be from Malaysia who found my profile on a job board.  This person is proposing a" - and he has in air quotes - "a 'collaboration' wherein I get hired for a remote American tech job, then he impersonates me and does all the work.  I send 85% of the paycheck to him, pocketing the other 15% for myself."  He said, "I don't think anyone's ever approached me to ask for my participation in something so blatantly illegal before.  Though if I'm being honest I was momentarily tempted, since it would be easy money for me and he'd still be making more this way than he could working in his own country.  Sounds like a win-win, apart from the whole fraud thing and serious criminal and reputational liability to me."



He said:  "Anyway, I never responded to the messages, so I can only speculate.  But I wonder if this is actually how the situation with KnowBe4 happened.  I have no reason to believe the sender of the email used his real name, or that he's based in Malaysia.  It might be more plausible that this message is part of the sort of large campaign that uses an 'IT mule laptop farm' as described in the story.  His Gmail address is generic and formulaic enough that I suspect there are many other identities being controlled by the same party.  The message itself is so carefully wordsmithed that it doesn't strike me as a personal note from a fellow dev.  I also received a follow-up message a week later, which felt more automated than not."



He said:  "Regardless, I thought you might be interested to see it since the public reads about the aftermath of these stories, but their onset usually happens behind closed doors.  Forwarding the full message here in case you'd like to read it on-air.  Thanks, signed Parker."



Okay.  So interesting and intriguing, indeed.  Here's the solicitation email that Parker received.  The subject was "Open to a collaboration?"  And it says:  "Hi, Parker.  I hope you're doing well and don't mind me reaching out.  I'm Lucas, a full-stack developer from Malaysia.  I found your profile on..." - and this was on usebraintrust.com/talent.  He said:  "...and wanted to propose a collaboration.  I don't currently have any projects that need your help, but our collaboration could be either technical or non-technical.



"For the non-technical aspect, I'd like your help with entrepreneurial factors for my development work.  If we end up getting jobs together and working on them, it would be a technical collaboration.  To keep it short, I'm looking to get well-paid jobs with companies or clients in the U.S.  While this is feasible from Malaysia, they tend to prefer hiring developers from similar time zones.  Unfortunately, I'm in GMT+8, while the United States is in PT to ET.  Especially for full-time jobs at companies, they typically don't hire developers outside of the U.S.  So I believe the best way to get U.S. jobs is to 'impersonate' someone who resides in the U.S.  It might sound risky, but it won't be risky as long as we keep this 100% confidential.  Besides, I don't mean that I want your identity information."



LEO:  No.



STEVE:  He says:  "Have you heard of Upwork.com or Toptal?  They're the largest freelancing job markets in the world, where most individual clients in the U.S. look for developers for their projects.  There's no easy way to get well-paid jobs, and Upwork or Toptal has a lot of competitive freelancers.  However, I'm very confident that I can get great jobs to make decent money.



"Here's how it would work:  First, you open an Upwork or Toptal account and log into it on your secondary laptop.  I connect to your secondary laptop via AnyDesk app, and I search for jobs.  You receive money into your bank account once I finish jobs with clients.  You take your commission and send me the remaining.  This would be a non-technical collaboration, and I would suggest a split of 15-20% for you and 80-85 for me.  For full-time jobs at U.S. companies, which obviously makes us way more money than freelancing jobs, I would apply for jobs on LinkedIn, and you would crack the interviews.  However, I'd say this is the advanced step of the collaboration, which should be based on a strong foundation of trust between us.



"Here's how that would work:  I apply for company jobs on LinkedIn using your LinkedIn account and get you scheduled with interviews.  You crack the interviews and get job offers.  I perform day-to-day work on those jobs while you attend the scrum meetings."  He says:  "(I can join the meetings if team members usually turn off their cameras.)"



LEO:  If you've ever done scrum, that's more work than doing the coding.



STEVE:  Exactly.



LEO:  I've got to say I would want more money for that.



STEVE:  Yeah, I had the same thought, Leo.  And finally he says:  "You get paid into your bank account bi-weekly or monthly, and you send me my portion after deducting your commission.  This would be a mixture of technical and non-technical collaboration, and I would suggest a split of 20-25% for you, 75-80 for me.  Please reply to this email if you want to schedule a call to discuss this further or if you have any other opinion for the collaboration.  Best, Lucas."



LEO:  It feels like it could be real.  I mean, I'm sure there is a group of people in other countries like Malaysia who can't get work.



STEVE:  I imagine it.  But I'll tell you, Leo, my own credibility filter snapped on when I read a sentence like:  "For full-time jobs at U.S. companies, which obviously makes us way more money than freelancing jobs, I would apply for jobs on LinkedIn, and you would crack the interviews."  That sentence, and the rest of the note for that matter, does not strike me as having been written by a non-native English speaker.  You know, maybe with AI generated, okay.  But in any case, Parker's sense of right and wrong kept him from responding, since this problem of North Korean infiltrators worming their way into Western jobs is clearly very real.  With a solicitation as slick and polished, it occurred to me that this might have been some sort of sting operation designed to catch Westerners who would be willing to expose their employers to potential hostile exploitation.



LEO:  Right.  Actually, in Malaysia many of them speak English, and they speak a British English.  So "crack" actually might have been exactly how he would have said it.



STEVE:  Yeah.



LEO:  But I don't - but, see, I don't - yeah.  I mean, I wouldn't, if I were Parker, I would not respond, of course.



STEVE:  Right.  Well, and this whole, you know, set up a secondary laptop and then he'll log into the laptop, which means he'll have a domestic IP address...



LEO:  Yeah, not good.



STEVE:  ...and is looping through the laptop from wherever.



LEO:  Yeah, yeah.  The one thing I would have liked him to do is take the call and just see who's on the other side.  Right?



STEVE:  Like how it goes, sort of explore it further.



LEO:  Yeah.  But if it were me,  I would not...



STEVE:  Well, no, no, no.



LEO:  Shine it on.



STEVE:  I mean, it just feels - it feels sketchy, you know, to say the least.



LEO:  Wow.  What an interesting email.



STEVE:  Okay.  So we're going to talk about what Matthew Green, his take on Telegram.  Maybe we ought to go a ways before we take our last break.



LEO:  Sure.



STEVE:  Or do you want to...



LEO:  Yeah, yeah, it's up to you.



STEVE:  So we'll get sort of halfway in, and then we'll take our last break.



LEO:  Okay.



STEVE:  Okay.  So Matthew wrote:  "This blog is reserved for more serious things."  Right?  And like he's normally talking about the details of subtle problems found in post-quantum hashing algorithms and things.  I mean, you know, Matthew isn't bothering to talk about abuse of commercial messaging.



LEO:  He's kind of the king of cryptographers from Johns Hopkins.  I mean, this guy is very, very - he's the guy, if he says it, I believe it, I guess is the bottom line.



STEVE:  He knows what he's talking about, yes.



LEO:  He knows what he's talking about.



STEVE:  So he says:  "This blog is reserved for more serious things, and ordinarily I wouldn't spend time on questions like the above."  Because his blog is titled "Is Telegram an Encrypted App."  He says:  "But much as I'd like to spend my time writing about exciting topics, sometimes the world requires a bit of what Brad Delong calls 'Intellectual Garbage Pickup,' namely correcting wrong, or mostly wrong, ideas that spread unchecked across the Internet.



"This post is inspired by the recent and concerning news that Telegram's CEO Pavel Durov has been arrested by French authorities for its failure to sufficiently moderate content.  While I don't know the details, the use of criminal charges to coerce social media companies is a pretty worrying escalation, and I hope there's more to the story.  But this arrest is not what I want to talk about today.  What I do want to talk about is one specific detail of the reporting.  Specifically, the fact that nearly every news report about the arrest refers to Telegram as an 'encrypted messaging app.'



"This phrase," Matthew writes, "drives me nuts because in a very limited technical sense it's not wrong.  Yet in every sense that matters, it fundamentally misrepresents what Telegram is and how it works in practice.  And this misrepresentation is bad for both journalists and particularly for Telegram's users, many of whom could be badly hurt as a result.



"So does Telegram have encryption or doesn't it?  Many systems use encryption," he writes, "in some way or another. However, when we talk about encryption in the context of modern private messaging services, the word typically has a very specific meaning.  It refers to the use of default end-to-end encryption to protect users' message content.  When used in an industry-standard way, this feature ensures that every message will be encrypted using encryption keys that are only known to the communicating parties, and not to the service provider.



"From your perspective as a user, an 'encrypted messenger' ensures that each time you start a conversation, your messages will only be readable by the folks you intend to speak with.  If the operator of a messaging service tries to review the content of your messages, all they'll see is useless encrypted junk.  That same guarantee holds for anyone who might hack into the provider's servers, and also, for better or for worse, to law enforcement agencies that serve providers with a subpoena.  Telegram clearly fails to meet this stronger definition for a simple reason:  it does not end-to-end encrypt conversations by default.



"If you want to use end-to-end encryption in Telegram, you must manually activate an optional end-to-end encryption feature called 'Secret Chats' for every single private conversation you want to have.  The feature is explicitly not turned on for the vast majority of conversations, and is only available for one-on-one conversations, and never for group chats with more than two people in them.  As a kind of a weird bonus," he says, "activating end-to-end encryption in Telegram is oddly difficult for non-expert users to actually do.



"For one thing, the button that activates Telegram's encryption feature is not visible from the main conversation pane, or from the home screen.  To find it in the iOS app," he says, "I had to click at least four times  once to access the user's profile, once to make a hidden menu pop up showing me the options, and a final time to 'confirm' that I wanted to use encryption.  And even after this I was not able to actually have an encrypted conversation, since Secret Chats only works if your conversation partner happens to be online when you do this.  Overall," he writes, "this is quite different from the experience of starting a new encrypted chat in an industry-standard modern messaging application, which simply requires you to open a new chat window."



Okay, now, I need to interrupt for a moment to clarify and explain something that's probably not clear.  There's a world of difference between a messaging app providing true end-to-end encryption, and merely having encrypted communications.  Matthew doesn't bother to draw attention to this distinction because he lives in the world of encryption where the phrase "end-to-end encryption" has a very specific meaning.  But it's easy to miss this important distinction.



The reason iMessage imposes a 32-member limit on group messaging, which I mentioned earlier, and Signal and WhatsApp both impose around 1K limits, is that these services, which Matthew describes as "industry-standard modern messaging applications," are all actually encrypting every party's message, end-to-end, individually to every other party.  Telegram is incapable of doing this ever.  It has no ability to do this under any circumstances.



So while it's true that Telegram's individual connections are always encrypted, it's only when two - and only two - parties are simultaneously online and Telegram's users opt to enable end-to-end encryption for that single, that two-party dialog, that any truly unobservable conversation ever takes place over Telegram.  All larger group chats are being decrypted by Telegram's servers for re-encryption and sending to other Telegram users.  Remember that Matt mentioned that industry-standard modern messaging applications never get the keys that are being used by end-users to exchange messages.  Telegram has all of the keys.  So obviously this is a crucial distinction.



Okay.  Returning to Matthew's explanation, he says:  "While it may seem like I'm being picky, the difference in adoption between default end-to-end encryption and this experience" - that is, having to do four clicks and digging down and hit menus and turning it on only when the other guy is online, he says - "is likely very significant.  The practical impact is that the vast majority of one-on-one Telegram conversations - and literally every single group chat - are visible on Telegram's servers, which can see and record the content of all messages sent between users.  That may or may not be a problem for every Telegram user, but it's certainly not something we'd advertise as particularly well encrypted."  He said:  "(If you're interested in the details, as well as a little bit of further criticism of Telegram's actual encryption protocols, I'll get into what we know about that further below.)"



He says:  "So does default encryption really matter?  Maybe yes, maybe no.  There are two different ways to think about this.  One is that Telegram's lack of default encryption is just fine for many people.  The reality is that many users don't choose Telegram for encrypted private messaging at all.  For plenty of people, Telegram is used more like a social media network than a private messenger."



LEO:  And by the way, when we talked about this ages ago, that was exactly the conclusion we came to.



STEVE:  Right.



LEO:  Was that Telegram is encrypted enough, or is not encrypted at all, but that's good enough.  I think that was actually the phrase you said, "good enough messaging."



STEVE:  Right.  Right.



LEO:  Yeah.  And so people, as long as you know that, and they don't advertise otherwise, that's fine.  But unfortunately they imply that it is encrypted.



STEVE:  Yes.  And even to the point where Pavel, I don't think I have it in the show notes, but Pavel has actively attacked Signal and WhatsApp...



LEO:  Oh, yeah.



STEVE:  ...deriding their encryption.



LEO:  He says, "Oh, the government has backdoors into those guys."  Well, the government doesn't need a backdoor.  It's Signal.  Geez Louise.



STEVE:  Yeah.  So he said, he was talking about how they use it as a social media network more than a private messenger.



LEO:  Right.



STEVE:  And he said:  "Getting more specific, Telegram has two popular features that makes it ideal for this use-case.  One of those is the ability to create and subscribe to 'channels,' each of which works like a broadcast network where one person, or a small number of people, can push content out to millions of readers.  When you're broadcasting messages to thousands of strangers in public, maintaining the secrecy of your chat content isn't important."



LEO:  No.



STEVE:  And he says:  "Telegram also supports large group chats that can include thousands of users.  These groups can be made open for the general public to join, or they can be set up as invite-only."  He said:  "While I've never personally wanted to share a group chat with thousands of people, I'm told that many people enjoy this feature.  In the large and public instantiation, it also doesn't really matter that Telegram group chats are unencrypted.  After all, who cares about confidentiality if you're talking in the public square?"



He says:  "But Telegram is not limited to just those features, and many users who join for them will also do other things.  Imagine you're in a 'public square' having a group conversation.  In that setting there may be no expectation of strong privacy, and so end-to-end encryption doesn't really matter to you.  But let's say that you and five friends step out of the square to have a side conversation.  Does that conversation deserve strong privacy?  It doesn't really matter what you want, because Telegram won't provide it, at least not with encryption that protects you from sharing your content with Telegram's servers.



"Similarly, imagine you use Telegram for its social media-like features, meaning that you mainly consume content rather than producing it.  But one day your friend, who also uses Telegram for similar reasons, notices you're on the platform and decides she wants to send you a private message.  Are you concerned about privacy now?  And are you each going to manually turn on the 'Secret Chat' feature, even though it requires four explicit clicks through hidden menus, and even though it will prevent you from communicating immediately if one of you is offline?



"My strong suspicion," he writes, "is that many people who join Telegram for its social media features also end up using it to communicate privately.  And I think Telegram knows this, and tends to advertise itself as a 'secure messenger,' and talk about the platform's encryption features precisely because they know it makes people feel more comfortable.  But in practice, I also suspect that very few of those users are actually using Telegram's encryption.  Many of those users may not even realize they have to turn encryption on manually, and think they're already using it.



"And this brings me to my next point:  Telegram knows its encryption is difficult to turn on, and they continue to promote their product as a secure messenger.  Telegram's encryption has been subject to heavy criticism since at least 2016, and possibly earlier, for many of the reasons I outlined in this post.  In fact, many of these criticisms were made by experts, including myself, in years-old conversations with Pavel Durov on Twitter."



And Leo, I'm going to inject something next, but let's take our final break.



LEO:  Okay.



STEVE:  And then we're going to get into what Matthew thinks about the actual technology that Telegram has deployed.



LEO:  Stay tuned for Steve's injection.  Okay, Steve.  Time for my injection.



STEVE:  Okay.  It was an interjection, but yes.



LEO:  Oh.  Not an injection, an interjection.  Much better.



STEVE:  I'm going to interject here.



LEO:  Okay.



STEVE:  To note that back in the morning of March 29th, 2015, after Matthew first sat down to take a serious long look at Telegram's encryption protocol and its system, his Tweet linked to Telegram's page.  I've got the link in the show notes for anyone who's interested.  And the Telegram page is titled "Creating an Authorization Key."  So he tweets the link, and then he says:  "Like seriously.  What the F is even going on here?"  Okay, so this is a top cryptographer who understands this stuff, who looks at Telegram's technical document on creating an authorization key and is scratching his head.



Okay.  So he writes:  "Although the interaction with Durov" - now he's speaking of the interactions that the security community, including himself, had sometime later, actually in 2016, the next year.  He said:  "Although the interaction with Durov could sometimes be harsh," he said, "I still mostly assumed good faith from Telegram back in those days.  I believed that Telegram was busy growing their network and that in time they would improve the quality and usability of the platform's end-to-end encryption."  And remember, when he says that, he means exactly that, end-to-end.  And he said, which is to say, the platform never has the keys, only the endpoints know the keys that are being used to encrypt and decrypt their conversation.  That's the key.  Telegram only offers that if you jump through hoops, and it's never on by default.  There is no on because of like the hoops you have to jump through.



So he said:  "I believed that Telegram was busy growing their network and that in time they would improve the quality and usability of the platform's end-to-end encryption.  For example, by activating it as a default or providing support for group chats, and making it possible to start encrypted chats with offline users."  You know, those are all things we take for granted, right, in all the other state-of-the-art platforms.  They all do all of that.  He said:  "I assumed that while Telegram might be a follower rather than a leader, it would eventually reach feature parity with the encryption protocols offered by Signal and WhatsApp.  Of course, a second possibility was that Telegram would abandon encryption entirely and just focus on being a social media platform.



"What's actually happened," he wrote, "is a lot more confusing to me."  And of course he's being generous.  He said:  "Instead of improving the usability of Telegram's end-to-end encryption, the owners of Telegram have more or less kept their encryption user experience unchanged since 2016.  While there have been a few upgrades to the underlying encryption algorithms used by the platform, the user-facing experience of Secret Chats in 2024 is almost identical to the one you'd have seen eight years ago.  This, despite the fact that the number of Telegram users has grown by seven to nine times during the same time period.



"At the same time, Telegram's CEO and sole owner Pavel Durov has continued to aggressively market Telegram as a 'secure messenger.'  Most recently he issued a scathing" - oh, I do have it in the show notes - "a scathing criticism of Signal and WhatsApp on his personal Telegram channel, implying that those systems were backdoored by the U.S. government, and only Telegram's independent encryption protocols were really trustworthy."  Well, you might argue the government couldn't understand them, so maybe.  Anyway, he says:  "While this might be a reasonable nerd-argument if it was taking place between two platforms that both supported default end-to-end encryption, Telegram really has no legs to stand on in this particular discussion.  Indeed, it no longer feels amusing to see the Telegram organization urging people away from default-encrypted messengers, while refusing to implement essential features that would widely encrypt their own users' messages.  In fact, it's starting to feel a bit malicious.



"So what about the boring encryption details?  Since this is a cryptography blog I'd be remiss if I didn't spend at least a little bit of time on the boring encryption protocols.  I'd also be missing a good opportunity to let my mouth gape open in amazement, which is pretty much what happens every time I look at the internals of Telegram's encryption.  I'm going to handle this in one paragraph to reduce the pain, and you can feel free to skip past it if you're not interested."



Okay, now, I am going to interrupt Matthew again to note that he has laced his description, which I'm about to share, with asterisks.  And later he explains that:  "Every place I put an '*' in the paragraph is a point where expert cryptographers would, in the context of something like a professional security audit, raise their hands and ask a lot of questions."  Okay.  So I'll just - I'll say the asterisks as I'm sharing this, and now you know that every time there's an asterisk, this is Matthew saying, uh, what?



Okay.  So he writes:  "According to what I think is the latest encryption spec, Telegram's Secret Chats feature is based on a custom protocol called MTProto 2.0.  This system uses 2048-bit* finite-field Diffie-Hellman key agreement, with group parameters (I think)," he says, "chosen by the server.*  Since the Diffie-Hellman protocol is only executed interactively, this is why Secret Chats cannot be set up when one user is offline.*  MITM protection is handled by the end-users, who must compare key fingerprints.  There are some weird random nonces provided by the server, which I don't fully understand the purpose of*  and that in the past used to actively make the key exchange totally insecure against a malicious server, but this has long since been fixed.*  The resulting keys are then used to power" - here it comes - "the most amazing, non-standard authenticated encryption mode ever invented, something called 'Infinite Garble Extension' (IGE), based on AES and with SHA2 handling authentication.*"



LEO:  You said "Infinite Garble"?



STEVE:  Infinite Garble Extension, IGE.



LEO:  Honestly, the more I've been thinking about this, the more I think this is actually malicious, that this is not ignorance.  They know exactly what they're doing is what I think.



STEVE:  Yeah, yeah.  And that is the point that Matthew's come to is that they know what's going on.  Pavel knows this is not actually encrypted.  And I'm sure he's telling governments, oh, we can't get in.  We can't moderate.  This is super secure.  No.  So anyway, he says, Matthew says:  "I'm not going to go further than this.  Suffice it to say that Telegram's encryption is unusual."  And I love that he said "the most amazing nonstandard authenticated encryption mode ever invented, something called Infinite Garble Extension."  Right.



Anyway, he said:  "If you ask me to guess whether the protocol and implementation of Telegram Secret Chats is secure, I would say quite possibly.  To be honest, though, it doesn't matter how secure something is if people are not actually using it.



"So," he says, "is there anything else to know?  Yes, unfortunately.  Even though end-to-end encryption is one of the best tools we've developed to prevent data compromise, it is hardly the end of the story.  One of the biggest privacy problems in messaging is the availability of loads of meta-data  essentially data about who uses the service, who they talk to, and when they do that talking.  That data is not typically protected by end-to-end encryption.  Even in applications that are broadcast-only, such as Telegram's channels, there is plenty of useful metadata available about who is listening to a broadcast.  That information alone is valuable to people, as evidenced by the enormous amounts of money that traditional broadcasters spend to collect it.



"Right now all of that information likely exists on Telegram's servers, where it's available to anyone who wants to collect it.  I'm not specifically calling out Telegram for this, since the same problem exists with virtually every other social media network and private messenger.  But it should be mentioned, just to avoid leaving you with the conclusion that encryption is all we need."



Okay.  So there are many useful wonderful bits among what Matthew wrote.  One is that while Telegram's crypto is bizarre, on its face it's not obviously insecure.  But neither has it ever been shown to be secure.  Mostly, it's just bizarre.  Or as Matthew put it, what the "F"?  The most important thing for Telegram's users to appreciate is that what Matthew referred to as today's industry-standard encrypted messaging apps provide always-on end-to-end encryption by default, while extending that true end-to-end encryption no matter how many individuals are participating in chat groups.  And you know, Leo, I didn't think of this when I was putting this down on paper yesterday.  But Telegram is actually riding on the coattails of the other messaging apps.



LEO:  Oh, yeah.  Oh, we do it, too.  We're end-to-end.  See?



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  Because Apple and Signal and WhatsApp have established the idea that everything is secure because they actually are.



LEO:  Right.



STEVE:  Telegram's just saying yeah, we are, too.



LEO:  Yeah, us, too.



STEVE:  Yeah, you know, we do that.  That's what messaging is, encrypted.  Yeah.  So also remember that last time we talked about iMessage and saw that not only had Apple implemented true multi-party end-to-end encrypted messaging, but that iMessage is also offering true forward secrecy by periodically and continuously rolling its conversation keys.  iMessage and Signal offer technology that Telegram has never had and, as Matthew noted, shows no sign of obtaining or even wanting.



LEO:  It's pretty clear they don't.  They don't want it, yeah.



STEVE:  Right.  Well, and look.  They've gone up by a factor of seven to nine, I mean, it's super popular.  Why complicate that with additional technology?  It's like they don't need more encryption.  They're able just to claim it.  And of course, Telegram's popularity may not really be about true security; right?  It's more about subscribing to its channels with a weaker assumption that, well, "Things are secure here," only because Telegram also has the unearned reputation of being a secure messaging system.



So anyway, you know, they're unable to offer what the other guys offer with much smaller groups.  And it's a benefit that Telegram is able to have these massive hundreds of thousands subscriber broadcasts.  They cannot make it end-to-end encrypted.  So they don't.  Yet they're getting the benefit of doing so.



Anyway, Matthew began, as we know, by posing the question:  "Is Telegram an Encrypted App?"  The most generous answer would be that, while it can be forced to provide state-of-the-art end-to-end encryption between two online parties, it certainly is not "as encrypted" as the general public, its users, and the press have all come to assume.  More than anything else its ability to broadcast to very large groups has turned it into a social media platform with an air of undeserved security and privacy.  So thank you, Matthew Green, for laying it out.



LEO:  There you have it.  Yeah.  I think that's - I read the piece, too.  I'm glad you brought it back because it was very interesting, and I thought a pretty big takedown of it.  Unfortunately, the people who most need to read it will not, never know.  



STEVE:  This is just for our listeners.



LEO:  Yeah.  And even our listeners already know this because we've covered this subject.



STEVE:  Yeah.



LEO:  Before.  I like Telegram.  Actually, I shouldn't maybe mention this, but right now we're streaming on seven streams, as you know - YouTube and Twitch, LinkedIn, Facebook, Twitter, Discord, and Kick.  And I think we're going to replace Kick with Telegram.



STEVE:  Telegram.



LEO:  Because I, you know, in fact I loved - maybe eight years ago when it really took off, I said, I want everybody to use this.  But we talked about this, and it was as you said, good enough.  It's not encrypted.  But most of the time you don't expect that.  The standards have changed now, thanks to Apple and Google, using RCS in Google's case, Apple uses RCS encryption.



STEVE:  And Leo, the podcast, your network podcasts don't need encryption.



LEO:  Right.  We don't want them to be encrypted.  We want everybody to see them.



STEVE:  Yeah.



LEO:  Yeah.  So I think Telegram, I don't know, you can't - I don't know.  We'll see.  You know what's cool, though?  We have 764 people watching on those seven platforms right now.  And I think that's a great way to introduce ourselves to a new audience.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#991

DATE:		September 10, 2024

TITLE:		RAMBO

HOSTS:	Steve Gibson & Mikah Sargent

SOURCE:	https://media.grc.com/sn/sn-991.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Microsoft's "Recall" uninstallability is a bug.  YubiKeys can be cloned.  How worried should you be?  When was that smoke detector installed?  We share and discuss lots of interesting listener feedback.  Is WhatsApp more secure than Telegram?  Does Telegram's lack of security really matter?  Elevators in Paris have problems, too.  There's a fourth credit bureau to be frozen.  Can high-pitched sound keep dogs from barking?  A reminder of a terrific Unix 2038 countdown clock.  A new Bobiverse sci-fi book and new Peter Hamilton novel.  Why does SpinRite show user data flashing past?  And TEMPEST is alive and well in the form of the latest RAMBO attack.



SHOW TEASE:  Coming up on Security Now!, I am Mikah Sargent subbing in for Leo Laporte as he is on vacation.  And we have a great show ahead.  Steve tells us all about how YubiKeys can be cloned and how maybe that's not something you have to worry about.  But, I mean, if you are, you know, a leader of a nation-state, maybe it is something you need to be concerned about.  Plus we talk about a lot of listener feedback, including whether  WhatsApp is more secure than Telegram, what is going on with elevators in Paris, and of course we cover the very interesting RAMBO, an attack that involves air-gapped systems, special encoding, and the ability to read RF signals coming from RAM.  Very interesting stuff coming up on this episode of Security Now!.



MIKAH SARGENT:  This is Security Now! with Steve Gibson and me, Mikah Sargent, Episode 991, recorded Tuesday, September 10th, 2024:  RAMBO.



It's time for Security Now!, the cybersecurity show that you tune into every week so you can panic and feel anxiety, but then feel better because Steve Gibson is here to help you feel better.  I am Mikah Sargent subbing in for Leo Laporte this week, who is enjoying a nice time away on vacation.  And I am pleased to be joined by the true star of this show, Steve "T" Gibson.  Hello, Steve.



STEVE GIBSON:  Hello, Mikah.  It's great to be working with you this week.  And on top of the normal level of anxiety that this show tends to induce, today's podcast is titled "RAMBO."  So if you were thinking, well, we're just going to have a calm little nothing podcast, no, that's not the case.  We've got all kinds of good stuff to talk about.



We're going to cover Microsoft's Recall, which briefly looked like it was going to be uninstallable.  Microsoft decided, no, that's a bug.  Also YubiKeys can be cloned.  How worried should we be?  When was that smoke detector installed?  Yeah, oh, this is probably one you missed, Mikah, so you're going to enjoy this one.  This actually relates to last week's Picture of the Week.



We're also going to share and discuss lots of interesting listener feedback.  Is WhatsApp more secure than Telegram?  Does Telegram's lack of true security really matter?  Turns out that elevators in Paris have problems, too.  The relevance of that will be made clear.  There's a fourth credit bureau that should probably be frozen.  Can high-pitched sound keep dogs from barking?  We'll be harkening back to a long-ago podcast.  We also have a reminder of a terrific Unix 2038, you know, "end of the world as we know it" countdown clock.



There's a new Bobiverse sci-fi book - that early series was very popular among our listeners - and a new Peter Hamilton novel, also another one of our sci-fi faves.  Why does SpinRite show user data flashing past?  And the main topic of the show, RAMBO, tells us that TEMPEST-style attacks are alive and well.  So a lot of good things to talk about.  And of course we've got a great picture, sort of a classic Picture of the Week.  And Benito pointed out that it also fits right in with today's recent Apple news theme that you just were talking about over on MacBreak Weekly.  So I think a great podcast for our listeners.



MIKAH:  Absolutely.  In fact, I thought this was specifically picked for the Apple event, so I was kind of pumped.



STEVE:  Okay.  So this week's Picture of the Week, again, I owe such a debt of gratitude to our listeners, who keep sending me these wonderful things that they encounter.  I gave this picture the headline "The Very Definition of Form Over Function."  So what we have, gates of various sorts seem to be a recurring theme.  We've often, like, found gates out in the middle of a field, like what is going on here?  In fact, we had one really famous one, a gate, and a bunch of sheep were standing behind it as if waiting for it to open, even though they could have walked around it.  So never really clear what was going on with those sheep.  But in this - oh, and another one of my favorite gates was a gate that blocked people from passing through with a series of horizontal bars, meaning that it was also a ladder that you could easily use to climb over the gate.  Like, okay, maybe these bars should have been vertical.



Well, in this case this "form over function" gate is blocking sort of a long corridor, clearly meant to prevent people from passing.  There's some sort of locking mechanism and handle and so forth over on the left.  So it makes it look like this gate will not open unless you're authorized.  Yet they wanted to celebrate the Apple.  And so more than a third, maybe not quite half of the gate is a large apple made out of the same bar material, except the center of the apple is open.  I mean, you can just go through the gate...



MIKAH:  Squeeze right through.



STEVE:  ...by ducking down and moving through.  And at first I thought, okay, maybe this was not really meant to keep people out.  Except that, if you look on the outside the gate,  like in the margins outside of that gate area, they have extra bars there, extending to the very edge, definitely intended to keep anybody from squeezing around the side.  But you don't have to squeeze around the side because the center of the gate is a wide open body of an apple.  Anyway, you know, there are so many of these pictures that we've shared where you want to find the person...



MIKAH:  You do; right?



STEVE:  ...who was in charge of this design and say, okay, now, like the apple.  Is this actually supposed to keep anybody out?  Like what's your thinking here?



MIKAH:  What were you thinking?  What were you thinking?  You have to tell me.



STEVE:  Okay.  So The Verge carried some news that really makes you wonder what's going on at Microsoft.  Their headline read: "Microsoft says its Recall uninstall option in Windows 11 is just a bug."  In other words, don't get your hopes up that we're going to allow our illustrious forthcoming "Recall" feature - which as we know most people don't want - to be removed from Windows.  That was a bug, not a feature.



So The Verge wrote:  "While the latest update to Windows 11 makes it look like the upcoming Recall feature can be easily removed by users, Microsoft," they wrote, "tells us it's just a bug, and a fix is coming."  Meaning that that option will be removed as like the bug fix.  They wrote:  "The Deskmodder spotted the change last week in the latest 24H2 version of Windows 11, with KB5041865 seemingly delivering the ability to uninstall Recall using the Windows Features section."  And I grabbed a snapshot of that for the show notes.  And you can see it very clearly.  Underneath the Print and Document Services option is Recall, which is checked, has the blue checkmark.  And that's just above the Remote Differential Compression API Support, which I guess everybody wants to have.  So, you know, there it is, suggested that if you were to uncheck that and click okay, Recall would be removed from your life, but only for apparently that release.



In a statement to The Verge, Microsoft's senior product manager, Brandon LeBlanc, said:  "We're aware of an issue where" - I love it, it's an issue.  "We're aware of an issue where Recall is incorrectly listed as an option under the 'Turn Windows features on or off' dialog in Control Panel."  Which we would all argue is exactly where it should be.  But Brandon said:  "This will be fixed in an upcoming update."



So the Verge goes on to tell us much of what we already know, which is to say why many of us wish that the checkbox would remain.  But The Verge also adds a bit of news.  So they wrote:  "The controversial Recall AI feature, which creates screenshots of mostly everything you see or do on a computer, was originally supposed to debut with Copilot Plus PCs last June.  Microsoft was forced to delay the feature after security researchers raised concerns."  Like multiple rounds of concerns; right?  "Microsoft says it remains on track to preview Recall with Windows Insiders on Copilot Plus PCs in October," so that's next month, "after the company has had more time to make major changes to Recall."  Which nobody would argue it needs.



They said:  "Security researchers initially found that the Recall database that stores the snapshots of your computer every few seconds was not encrypted, and malware could have potentially accessed the Recall feature.  Microsoft is now making the AI-powered feature an opt-in experience instead of on by default, encrypting the database, and authenticating through Windows Hello."  Now I'll just pause here to note that Windows Hello has been broken multiple times, and they're not going to be saying, well, you know, we have this Recall feature, but people are really uncomfortable with it.  No.  They're going to be saying, we have Recall.  It's jiffy quick spiffy wonderful, and you definitely want to...



MIKAH:  Use it.  You want it.  It's amazing.



STEVE:  Oh, my god, you're not going to - after a year, you're going to wonder how you ever got along without it.  So, yeah, it may be opt-on, but it's sort of like, if anybody's been using Windows recently and tried not to back up their computer using one of Microsoft's facilities, you know you have to say no, no thank you, no, I'm really seriously sure I don't want to use your backup because, oh, no, they want you to back up to OneDrive.



Anyway, The Verge said:  "We did ask Microsoft whether it will allow Windows users to fully uninstall Recall, as this appearance in the Windows features list suggests.  But the company only confirmed this was just 'incorrectly listed' for now."  They said:  "It's possible that Microsoft may need to add a Recall uninstall option to EU copies of Windows 11 to comply with the European Commission's Digital Markets Act," you know, the DMCA.  "Microsoft has already had to add an option to uninstall Edge in the European Economic Area countries, alongside the ability to remove the Bing-powered web search in the Start menu."  So, you know, maybe Europe is going to come to our aid.  Although the problem is, you know, Microsoft knows if these copies of Windows 11 are in the EU or not.  So that may not help us.



And, you know, when you really think about it, what does it mean that Windows has a feature that presents a clear and present privacy and security danger to all of its users, which Microsoft knows full well many of its users feel extremely uncomfortable about, and where it's obvious that the feature could be readily removed from Windows, yet Microsoft refuses to allow their users to do so.  One thing that means for certain is that GRC's forthcoming freeware, which will totally neuter and remove Recall, promises to be quite popular.



MIKAH:  Okay.  Let's talk about this.  Okay.  So first and foremost, I 100% agree, and right now there have to be at least 10 people inside of the company who are cringing at the fact that this was discovered in the first place because it's such a clear, easy way to say, if it can be removed, and we're not giving people the ability to do so, now that's there.  That's so bad that they've shown that it can be taken away, and now they're not giving people the opportunity to completely take it away.  Secondarily, though, do you have concerns that - because you've created other tools like this.  So I know that you have a better understanding of this.  Freeware that removes something that Microsoft says can't be removed or shouldn't be removed, does that introduce any issues in the system?  Or do you have concerns about that?



STEVE:  I guess I would say we'll find out.  So what this most reminds me of is what we went through with Internet Explorer back in the early days of Windows.  Microsoft was so committed to having their own web browser built into Windows that they told the world that it could not be removed.  You know, there was no way to remove IE from Windows.  And so, you know, that was the story that we heard, I mean, and this was where a lot of the antitrust problems came from back in the beginning.  So we kept hearing, oh, no, the browser is an integral component.  It's deeply integrated into Windows and cannot be removed, until the EU said, you know, we think that's wrong.



And so then Microsoft made it removable.  So, you know, it was only unremovable because Microsoft didn't want it to be removed, until they were forced to say, okay, well, I guess we'll let people turn it off.  And in fairness, there are some parts of Windows that have always been and almost still are dependent upon IE components.  So, you know, it was integrated into Windows.



What's interesting here is that the progression of this demonstrates that it is an add-on to Windows.  I mean, Windows 10 doesn't have it.  Windows 10 is apparently going to be getting it.  Windows 11 doesn't have it.  Windows 11 is definitely going to be getting it as a feature of Copilot.  So I think it's, I mean, Microsoft could certainly - Microsoft can do anything they want to with the OS; right?  So they could arrange to make Windows dependent upon it in some fashion.  But to your point, clearly the fact that there is a Remove It feature now, and they're removing the Remove, rather than removing Recall, suggests that Recall can be removed.



MIKAH:  Yup.



STEVE:  So I will - and if it turns out that it cannot be removed, that is, like it literally cannot be removed from the system, or that every Windows Update brings it back if it's taken out, you know, whatever, then the least I could do would be to have my thing set up a little background service in Windows whose job is to absolutely, you know, kill it when it appears, or turn it off when it gets turned on, I mean, just basically, you know, take responsibility...



MIKAH:  A Whac-A-Mole.



STEVE:  ...for keeping it shut down.  Now, and we know that that is possible because Microsoft has said, if you're doing something sensitive, you can stop Recall from snapshotting your system while you're doing something that you specifically don't want it to watch.  So I just...



MIKAH:  Which is everything for me.



STEVE:  Exactly.  And so it's certainly - and I do have a great name for it.  I'm just keeping it quiet for now.  But yeah.  Leo, I'll never forget him laughing when I told him that the freeware that was going to prevent Microsoft from upgrading your Windows 7, of course I called it Never10.  Which he really liked.  Anyway, I've got something good for this one.



MIKAH:  Oh, good.



STEVE:  Yeah.  As soon as it actually happens and becomes a problem, I'll spend, I mean, it's only going to take a couple days to create something that does the job.



MIKAH:  Nice.



STEVE:  Okay.  So many, many, many people sent me a link to, like, at least as many pointers to the recent YubiKey exploit stories as I received with news about this RAMBO attack because people wanted to hear what I thought about this RAMBO attack.  And also, hey, Gibson, look, your favorite key has a problem.  So of course this is due of course to the fact that Yubico themselves largely credits me, thanks to the listeners of this podcast, with discovering them at an RSA conference where I met Yubico's primary mover and shaker and co-founder, Stina Ehrensvard.  And then, you know, the podcast put them on the map and really helped them going.



Now, it's clear that this would certainly have happened for Yubico sooner or later, and knowing Stina, probably sooner.  So it was just fortune that I happened to be someone, you know, who had a microphone, who recognized the cleverness of what they had created back then.  And of course the YubiKeys have evolved dramatically since that first thing that was basically a keyboard, a USB keyboard emulator, which was very clever.  And so the world is changed.  But Yubico has remained the leader.



Ars Technica's headline about the recent discovery was:  "YubiKeys are vulnerable to cloning attacks thanks to newly discovered side channel."  And their subhead read:  "Sophisticated attack breaks security assurances of the most popular FIDO key."



The researchers at NinjaLab, who performed the research and previously informed Yubico of their findings, so much so that Yubico has already solved the problem for any new keys that they then sell, NinjaLab said:  "In the present work, NinjaLab unveils a new side-channel vulnerability in the ECDSA [Elliptic Curve Digital Signature Algorithm] implementation of Infineon 9."  That's the actual chip inside the YubiKey that does the crypto on any security microcontroller family of the manufacturer.  Meaning of Infineon.



They said:  "This vulnerability lies in the ECDSA ephemeral key (or the nonce) modular inversion and, more precisely, in the Infineon implementation of an Extended Euclidean Algorithm."  They said:  "To our knowledge, this is the first time an implementation of the EEA" - that's the Extended Euclidean Algorithm - "is shown to be vulnerable to side-channel analysis," they said, "contrarily to the EEA binary version.  The exploitation of this vulnerability is demonstrated through realistic experiments" - and we'll discuss how realistic they are in a minute because it takes something to make this happen - "and we show that an adversary only needs to have access to the device for a few minutes."  Although I'll put "access" in air quotes, as we'll see.



They said:  "The offline phase," that is, after the access for a few minutes, "took us 24 hours.  With more engineering work in the attack development, it would take less than an hour."  So yes, it's possible after we've demonstrated the problem to improve its performance.



They said:  "After a long phase of understanding Infineon implementation through side-channel analysis on a Feitian 10 open Java Card smartcard, the attack is tested on a" - so that was a smartcard using the same chip.  So they actually developed the attack on something no one had ever heard of.  Then they thought, okay, to get some press we're going to see if the YubiKey is vulnerable.  So they said:  "The attack is tested on a YubiKey 5Ci, a FIDO hardware token from Yubico.  All YubiKey 5 Series," they said, "before the firmware update 5.7.11 of May 6th, 2024 are affected by the attack.  In fact, all products relying on the ECDSA of Infineon cryptographic library running on an Infineon security microcontroller are affected by the attack."  In other words, all kinds of other things, too.



They said:  "We estimate that the vulnerability exists for more than 14 years in Infineon top secure chips.  These chips and the vulnerable part of the cryptographic library went through about 80 CC certification evaluations of level AVA VAN 4 (for TPMs) or AVA VAN 5 (for the others) from 2010 through 2024."  And they said:  "A bit less than 30 certificate maintenances."  Okay.  So in other words, this has had the crap tested out of it.



MIKAH:  That's what I thought it meant.  Okay, good.



STEVE:  Yeah, over and over and over.  So, like, no problems were ever found.  And everybody's using this in the industry because it is the industry standard secure microcontroller, super high volume, super low cost, and that's what's in the YubiKey 5 series, as well as many other secure tokens and HSMs, for example, of different kinds.



Okay.  So in his reporting of this for Ars Technica, Dan Goodin wrote:  "The attacks require about $11,000 worth of equipment and a sophisticated understanding of electrical and cryptographic engineering.  The difficulty of the attack means it would likely be carried out only by nation-states or other entities with comparable resources, and then only in highly targeted scenarios.  The likelihood of such an attack being used widely in the wild is extremely low," as in nil.  "Roche said that two-factor authentication and one-time password functionalities are not affected."



So this is a specific function among many functions in this cryptographic library.  So two-factor authentication, one-time password functionalities not affected.  So it's very likely, and I didn't dig into this enough to get a sense for, of all the things that are affected, what are.  But FIDO is.  So FIDO2 passkey stuff, that'll be a problem.



Okay.  So he said:  "Tuesday's report from NinjaLab outlines the full flow of the cloning attack as:  First the adversary steals the login and password of a victim's application account protected with FIDO."  Right?  So it's not like the key is all you need.  You still need the login and password first.



"The adversary gets physical access to the victim's device during a limited timeframe without the victim noticing.  Thanks to the stolen victim's login and password for a given application account, the adversary sends the authentication request to the device as many times as necessary while performing side-channel measurements."  In other words, you give your login and your password to authenticate, then the account says now you need to use your key in order for us to verify that you're in physical possession of the key.  So the bad guys have to make all that happen so that the device is actually doing successful passkey or FIDO authentications over and over and over.



Then they said:  "The adversary performs a side-channel attack over the measurements and succeeds in extracting the ECDSA private key linked to the victim's application account.  The adversary can sign in to the victim's application account without the FIDO device and without the victim noticing."  Now, notice that they already could because they had the key somehow; right?  So what this is doing is, it's allowing them future access, which they already got, you know, present access for because they had the guy's login and password and their device, like in a very intrusive way so they were able to do things with it, use it, successfully authenticate.  So it was during multiple successful authentications under the scrutiny of this $11,000 worth of equipment that they're getting the private key whose entire purpose is not to do something now, but to do something in the future.



And then of course they've got to get this back - it's like a "Mission Impossible" episode.  They've got to get this back to the user.  And wait till you hear that they had to crack the key open in order to do any of this anyway because that's part of it, too.  So they had to glue it back together after cracking it open, and getting it back to the guy before they know that it's ever been taken so that they won't go and change the key because anyone who knew this had been done would stop using it; right?  So it's like, okay, fine.  So this allows them to in the future sign into the victim's application account using the stolen login and password, and the then-stolen elliptic curve private key, which allows them to do this in the future.



And then Dan says:  "The list," which we just finished, "however, omits a step, which is tearing down the YubiKey and exposing the logic board housed inside."  He says:  "This likely would be done by using a hot air gun and a scalpel to remove the plastic key casing and expose the part of the logic board that acts as a secure element storing the cryptographic secrets.  From there, the attacker would connect the chip to the hardware and software that take measurements as the key is being used to authenticate an existing account.  Once the measurement-taking is finished, the attacker would seal the chip in a new casing and return it to the victim."  So...



MIKAH:  Okay.  This isn't easy.  This isn't realistic for the average - even the person who had the equipment, like there's still so much that could go - what could possibly go wrong?  So much.  It'd be hard to do this.



STEVE:  Yeah, it's not like you scan it in some guy's pocket...



MIKAH:  Right.



STEVE:  ...from 20 feet away or something.  So to put this into context, Dan adds, he says:  "The attack and underlying vulnerability that makes it possible are almost entirely the same as the one that allowed NinjaLab to clone Google Titan keys in 2021.  The attack required physical access to the token for almost 10 hours in the case of the Google Titan keys."  He says:  "The attacks violate a fundamental guarantee of FIDO-compliant keys, which is that the secret cryptographic material they store cannot be read or copied by any other device.  This assurance is crucial because FIDO keys are used in various security-critical environments, such as those in the military and corporate networks.



"That said," he writes, "FIDO-compliant authentication is among the most robust forms of authentication, one that's not susceptible to credential phishing or adversary-in-the-middle attacks.  As long as the key stays out of the hands of a highly skilled and well-equipped attacker" - with a hot air gun and a scalpel, I'll just interject - "add $11,000 worth of equipment and the ability to get it out of your possession for the time required to do it, and who already knows your login name and password.  It remains among the strongest forms of authentication."



He says:  "It's also worth noting that cloning the token is only one of two major steps required to gain unauthorized access to an account or device.  An attacker must also obtain the user password used for the first factor of authentication.  These requirements mean that physical keys remain among the most secure authentication methods.



"To uncover the side channel," he finishes, "the researchers reverse-engineered the Infineon cryptographic library, a heavily fortified collection of code that the manufacturer takes great pains to keep confidential.  The detailed description of the library is likely to be of intense interest to cryptographic researchers analyzing how it works in other security devices."



Okay.  So what we have here is Yubico in the spotlight only because it is by far the most successful and well-known user of high-security token hardware by Infineon that, despite years, 14  years of previous reviews and extensive analysis by the industry, was finally found to have an extremely subtle flaw that could be used to extract its secrets; and even and only then through the use of quite high-end expensive engineering equipment, including the need to physically compromise and crack open the key.  And even then, the attacker would still need knowledge that only the key's legitimate owner and user probably possesses.



So Infineon has fixed their problem with a firmware update.  But in the interest of security, Infineon's firmware is not field upgradeable.  So Yubico has obtained the improved hardware from Infineon and is now offering keys that have this fixed.  Whether or not anyone should or would bother to update is up to them.  But this attack seems so far-fetched, I mean, literally, Mission Impossible 9, and is so far out of the realm of ever happening to anyone - and, after all, we're just using the keys to contain additional factors of login credentials - that I can't imagine this is worth another thought.



MIKAH:  Can I ask you, you say in the interest of security, Infineon's firmware is not field-upgradable.



STEVE:  Right.



MIKAH:  How is it, and it's probably just not obvious to me, why is that a security thing, to say you can't upgrade the firmware and fix this?  Why does it have to be new hardware?



STEVE:  So what it actually is, the firmware is in ROM, in literally old-school, it's called Masked ROM, where the bits are actually little bits of metal mask which are present or not.  Literally like physical connections making ones and zeroes.  As opposed to it being in flash memory, where it is dynamically writeable.  And the reason is, if you are able to put the key in and change the firmware, then an attacker could put it in and change the firmware to be insecure. 



MIKAH:  Got it.



STEVE:  So you just, you absolutely, like the top-level security says we fixed this in the factory so that you can't ever change it.



MIKAH:  Got it.  So it is the, like, the fruitcake firmware.  It's so dense, so it's nearly concrete, as opposed to firmware that exists on the other side where it's a software update.  This is literally, like, actual physical firmware update.



STEVE:  I would say that we're using the term "firmware" because it's the code that drives the microcontroller inside.  But it's actually so firm that it's hard.



MIKAH:  Yeah.



STEVE:  It's actually hardware firmware.



MIKAH:  Got it, yeah.  Wow, that's cool.  That's really cool.  But you're saying, though, given that you basically have to have a significant other who is secretly a spy from some nation-state organization to, A, be able to guess that your username and password is this, or have access to that in the first place, and can get that away from you and can keep you entertained by something for 10 or more hours, I think it's more in this case, to be able to pull this off.  You and I probably don't need to go buy a new YubiKey.  



STEVE:  I'm not the least bit concerned.  You know, Yubico's been totally responsible.  They immediately, in concert with the announcement from NinjaLab, they put out their own explainer that said, yes, it turns out that our supplier and the supplier of everybody else on the planet has a problem.  This is the nature of it.  And we've responded the only way we can because these are not field-upgradeable, is if you really are concerned, we'll offer you a new key.  But, yeah, really, you know, we're really talking nation-state level exposure for this to be a problem.



MIKAH:  Got it.  So, yeah, I will keep my YubiKeys, thank you very much.  I have 5 Series, and so whenever you first said this I thought, oh, dear.



STEVE:  Yeah, not a problem.



MIKAH:  All right.  We are back from the break.  I am Mikah Sargent subbing in for Leo Laporte this week.  And it's time once again for Steve to take it away.



STEVE:  Okay.  So we've got - that was pretty much the big news of the week, along with RAMBO that we'll get to in a minute.  I have a bunch of interesting listener feedback that I wanted to share because the GRC mail system has been working overtime, I should say.  So our Picture of the Week podcast before last was that signage which was intended to have its blank field proudly filled in with the date since there had last been any sort of accident on the job.  But as we remember, instead of that, it cited that they'd had no accidents since a specific person, who everyone presumably knew, had left the job.  So, you know, this site has been accident free - yeah, there it is.  It said "Since Joe left."



MIKAH:  Aw.



STEVE:  Okay.  So I recalled at the time that we'd had a similar non-sequitur once before for our Picture of the Week in the form of a close-up photo of a smoke alarm that also had a blank space where its installer was expected to fill in a date.  But during the podcast two weeks ago I was unable to recall what had been written there when we showed that Picture of the Week before.  One of our listeners whose online moniker is "Mr Nobody 2" was kind enough to remind me.



Okay.  So the smoke alarm had, as I said, had a field where its installation date was meant to be filled in by its installer.  So it said:  "Installed On:"  and followed by a blank space.  In this case, the person filled the information in so that it read: "Installed On:  The Ceiling."



MIKAH:  Oh, that's great.



STEVE:  It was wonderful.  So anyway, I wanted to remind everybody of that similar repurposing of the original intent.  Like, you know, looking at it you wouldn't know that your smoke alarm was installed on the ceiling, so yeah.



MIKAH:  Mine, unfortunately, I can see here, it's just not - it doesn't have anything on it.



STEVE:  Well, and I'm not really sure because, you know, these things start to beep when the batteries get low.  So maybe technically you're supposed to replace them every 10 years or something, like maybe the battery isn't the only problem.  Maybe the smoke sensing sensor could go bad.  You know, so like, oh, well, replace it when you need to change the batteries.  But after a decade you should just really go get a new one.  Anyway, just in case anyone was wondering, the smoke sensor is on the ceiling, if you weren't secure what surface that was.



So anyway, as I said, there was not a huge amount of news.  And I got caught up in the terrific listener feedback that I'd been receiving.  As our longtime listeners will remember, many years ago we used to deliberately alternate episodes between security news and listener feedback, where we would do a pure feedback episode.  We dropped that approach over time in favor of always doing some of both, which is our normal routine, as we are this week.  But a little more feedback this week.



And now I should note how pleased I am with the way GRC's email system has worked out.  The nature of the feedback by email is completely different from Twitter; and having it, you know, coming into my own email client makes it significantly easier to manage.  So I'll just remind everyone that in order to send feedback directly to me at the email address securitynow@grc.com, and that's not listed anywhere at GRC because I'm intending this to be for podcast listeners.



So, you know, people say, hey, I looked around, I wanted to send you a note because I know you're talking about this all the time, but I couldn't find the address anywhere.  Right, because it's only for people who hear my voice.  And my voice says "securitynow@grc.com."  That's the email address.  So again, you need to register your sending email address with GRC.  You do not need to subscribe to any of the three mailing lists that you'll find there.  Just being registered allows my system to prevent all incoming spam from anyone sending to securitynow@grc.com who's not registered.  And that's a blessing because all I ever get there are actual listeners' email, and it's wonderful.



So anyway, I'm also hearing from many of our listeners who really appreciate receiving the weekly show title, summary, Picture of the Week, and the show notes link by mail every Tuesday morning before the podcast.  So subscribing to the Security Now! List will automatically make that happen for you.



Okay.  So Angus MacKinnon.  He said:  "Steve, I see on WhatsApp all the time that your messages are encrypted.  Is WhatsApp secure?  I thought WhatsApp had Signal embedded."



Now, of course, last week's podcast was all about the fact that Telegram, which claims security and boasts of its reputation for security, was not truly offering end-to-end messaging encryption, with the single exception that two - and exactly and only two - parties who were both online at the same time could deliberately enable point-to-point encryption for their conversation.  So I'm sure that Angus just wanted some assurance that WhatsApp's similar claim of encrypted messaging is actually legitimate.  And as he noted, since WhatsApp is based upon the open Signal protocol, all messaging is always fully encrypted, even in multi-party groups, you know, up to a thousand people in a group.  So and in fact, since it's based on Signal, there's no way to use it or Signal in an unencrypted mode.  That's all they offer.  So 100% yes for WhatsApp.



Now, at the same time, Andy Pastuszak shared some useful points which he feels favors Telegram.  He wrote:  "Steve, I'm a user of Telegram as well as Signal.  The definition of anything less than end-to-end encryption as not being true encryption would make a LOT [all caps] of services not encrypted, even outside the messaging space."  He says:  "There are almost no cloud providers that offer true end-to-end encryption.  Dropbox, OneDrive, and Google Drive don't.  Online calendar, to-do lists, and note-taking apps don't really either.  And the ones I find that do, charge A LOT for the privilege.  Some of the end-to-end encrypted note-taking apps I looked at charge well over $100 per year for their basic plan.



"Telegram," he says, "is obviously NOT [all caps] end-to-end encrypted.  But it is encrypted in transit and encrypted at rest.  For the things I use Telegram for, all I really need is encryption in transit.  If I really need end-to-end encryption, then I use Signal.



"The other nice thing about Telegram is how group chats work.  How many times have you been part of a group SMS text, and asked to be removed from it?  And that works great until someone responds to an old message that you're still included on, and then all of the sudden you're part of the conversation again.  With Telegram, you leave a group chat or channel, you're gone until you rejoin.



"And Telegram fully supports Siri and CarPlay.  I can easily say 'Hey, Siri, send a Telegram message to Joe' while driving, and it will happily do that.  Signal does not have Siri or CarPlay support yet.  So if you want something better than SMS, with Siri and CarPlay and Android Auto support, and you're aware of the encryption limitations, Telegram is an excellent choice."  Okay.  I agree...



MIKAH:  Lot of caveats.



STEVE:  Huh?



MIKAH:  I said a lot of caveats there.



STEVE:  Yeah.



MIKAH:  You like this, but you don't care about the - yeah, sorry, go ahead.



STEVE:  Yeah.  No, you're right.  I agree with Andy that true end-to-end encryption is rarely needed or necessary.  I use iMessage among my iPhone-using friends.  As we know, its encryption is what Matthew Green described as modern state-of-the-art, you know, true end-to-end encryption.  But the messages I'm sending are about what time we're meeting for dinner, or whether they saw some random piece of news.  Hardly anything that would ever be of interest to anyone else.



Andy is obviously a sophisticated user who understands exactly what's going on.  After all, he's listening to this podcast.  So there's nothing to disagree with him about.  One of the points of his sophistication is that he knows that when he truly needs end-to-end encryption, it's time to switch to Signal for that.  He said so.  But a big part of what Matthew Green wanted to convey - although unfortunately it was only being read by people like us, so it didn't come as a huge surprise - was that the typical Telegram user - not Andy - was extremely unlikely to have any such sophistication and thus appreciation of the distinction between Telegram and Signal or WhatsApp.



So Matthew told us that what he was growing increasingly annoyed about as the years rolled by with Telegram not making any significant improvements to the security of their messaging technology, was that they were essentially riding on the coattails of all of the other fully, truly end-to-end encrypted messaging platforms, while all the while claiming privacy parity with them while choosing to not actually offer it.



So Andy, I don't take issue with anything you said except you're the exception, not the rule, among Telegram users, of which there are, you know, a billion who don't understand all of this.



John Hickin said:  "Steve, we rented an apartment in Paris where a sign was present in the elevator, but in French, of course.  It was put up by owners who were annoyed when renters (AIRBNB) forgot to close the outer door after leaving the elevator, thus rendering it stuck in place so nobody on any other floor could recall and use it."  And he said, "Cheers, John."



Okay.  So I enjoyed John's note which related, of course, to last week's Picture of the Week, remember, which suggested that if the elevator didn't "go," its occupants should try jumping up and down a bit, which, you know, should give anyone the willies.  Presumably that would allow the elevator to know that they were present, although one would imagine that pressing a floor button would serve that purpose.



Anyway, apparently in Paris still to this day they're using those quaint elevators where its user first closes an outer door on the floor, and that door remains on the floor to close off the elevator shaft while the elevator is not there, and then the elevator itself is only responsible for closing the inner door of its own carriage.  So as John notes, if people leaving an elevator leave the outer door open, which is not under automation, then the elevator is unable to close that outer door, so the elevator is unable to move.  And anybody pushing for the elevator to go to their floor will end up having to take the stairs and be an extra-annoyed Parisian, which may go a little ways to explain how they feel about Americans visiting Paris.  So, you know, American tourists are, well...



MIKAH:  You can just leave that blank.  It fills itself in.



STEVE:  Craig Taylor said:  "Hi, Steve.  Longtime listener.  I wanted to provide you with some additional information on the article you cite for Freezing Credit after the NPD breach.  The article you reference for freezing credit only mentions three of the four major credit bureaus at which you need to freeze your credit.  Innovis is missing from that article."  He says:  "Our article at CyberHoot has a collection of many of the primary and secondary credit bureaus."  And I have a link in the show notes.  And he finished, saying:  "Great coverage, and thanks for doing what you do."



Okay.  So Craig is a co-founder of CyberHoot, and the page he linked to does indeed provide more comprehensive coverage of the various credit bureaus with links to each bureau's individual credit freeze resources page.  The GRC shortcut "npd" - that's what I thought it was, it's not NDP as he had.  He got those backwards.  Or maybe I did, I transliterated them.  Anyway, it's NPD, for checking the NPD breach database.  That shortcut, grc.sc/npd received the largest number of referring clicks ever, of all time, and that was just a couple weeks ago.  So it hasn't even had that long to age.  And GRC's "credit" link, grc.sc/credit, to the Investopedia page is next in line, just behind it, in second-place runner-up position.  The /npd was something like 10,000-plus clicks.  And the grc.sc/credit link was 9,000 some.



So I know this topic is, not surprisingly, of significant interest to this podcast's listeners.  Since I want to make Craig's more comprehensive listing of credit bureau credit freeze links readily available, I've created another new GRC shortcut.  This one is "freeze."  That points to Craig's excellent page about identity theft.  So the link is grc.sc/freeze.



Since I'd only previously frozen my own credit at the big three - TransUnion, Experian, and the infamous Equifax - I immediately used the new link to Craig's page to find the link to Innovis.  I went there and froze my credit.  To Innovis's credit (pardon the pun), it was the easiest of any of the freezing experiences.  No need to create an account.  You just fill out an online form - which, by the way, contains all the data that's already been made public in the breach, so it's not news to anyone - and your credit is immediately, from that moment, frozen against anyone's inquiry.  Innovis then sends, by postal mail, a credit freeze confirmation letter which contains a 10-digit PIN.  So you'll want to hold onto that.  That PIN can subsequently be used to then manage your freeze status at Innovis.  It was so quick and easy that I cannot imagine why anyone who cares about this would not do it.  So again, the GRC shortcut to get to Craig's page at CyberHoot is grc.sc/freeze.



And I should mention that Craig's quite comprehensive page mentions an additional five lesser-known bureaus which also offer credit freezing.  I didn't bother with them, but if you want to be fully covered you may wish to.  I mean, like, why not?  And Craig, thanks very much for bringing this additional major credit bureau and your page to our listeners' attention.  That's much appreciated.



And I now send email out every morning with a summary of the podcast and a link to the show notes.  So I've already received feedback from a listener who read the show notes and asked me, okay, what about these other five credit bureaus?  I mean, they're there.  Do we need to freeze them?  So I don't know how to answer that.  I remember Innovis.  And I remembered that when we talked about this years before, there was a fourth bureau.  I couldn't remember it when this came up again.  And so, you know, I didn't take the time to dig into it.  It was Innovis.  So they're at least in number four position, probably big enough to count.  The question about whether you should bother, like, how far down the list should you go, essentially, I don't know.  I know that when I needed to apply for an Amazon card because I wanted my Amazon purchases to be on their card because they gave you extra points...



MIKAH:  Really good deals, yeah.



STEVE:  Yeah, I found out which of the major three they used, and I did a temporary suspension of the freeze to allow Amazon to check my credit and then issue me a card, and then the freeze automatically snapped back on.  So I know that Amazon uses one of the big three.  Technically, anyone, you know, so the question is you're freezing your credit because you don't want someone to issue, some bad guy to obtain credit in your name.  Well, if they're querying a random - if the company granting the bad guy credit in your name is querying some random credit bureau that may not have the best information for you, then I guess anything's possible; right?  So, yeah, if this is a concern for you, lock them all down.



My feeling is that none of these bureaus received my permission at any point to collect this information about me; you know?  They're just doing it.  And I do think that probably the top four covers, you know, virtually all of the use.  Presumably not absolutely all, otherwise these other five wouldn't be around.  So I guess I don't really have a good answer to the question.  But, you know, if credit bureaus keep popping up, do we just run around freezing them all?  I don't know.  And besides, all the information is now public.



MIKAH:  Yeah, exactly.  It's already out there.  I wish that - so, you know, part of the requirements by the federal government involve providing the three major credit bureau reports every year, you get one free from each of the three.  I think that, A, if they can network to do that, they should network to let you do security freezes all together with just one button.  And I know this is just wishful thinking, but they've proven that they can work together in that capacity.  Let's let them work together in letting us freeze easily, and unfreeze, all at once.



STEVE:  Well, and, you know, when we talked about this a couple weeks ago, the way the system should actually work is that all of the bureaus should always be frozen by default.  And then if you are applying for credit, as I did for Amazon, or like someone's buying a home or buying a car or making any large purchase, anyone who wants credit should do something to specifically authorize that person to have access to their credit report.  So, you know, you receive a PIN from the credit supplier that that entity's going to use.  You give them this long access PIN, which they are then able to use to obtain access to your report directly from the granting agency.  I mean, there are ways we could make this work that don't even require, you know, fancy computers or being online or anything because that's still an issue.  Not everybody - you don't want to like require that you have Internet access in order to do all this because all this predates the Internet.



But anyway, the system's broken, and we're just sort of limping along as we go.  But I do think it was so easy to add a freeze to Innovis that I can't imagine why anybody who has taken the time to freeze the other three would not go ahead and do number four because it does round out the top four.  And I think that's probably worthwhile.



MIKAH:  Agreed.  That's very easy.



STEVE:  Yeah.



MIKAH:  I did it while we were talking.  Perfect.



STEVE:  Nice, nice.  Adam Tyler said:  "Hi, Steve.  I was curious if you or a listener have found a commercial version of the portable dog killer device?"  He says:  "I'm not really looking for a laser gun, but something that could sit on the fence line to deter a barking dog, ideally automatically activated, and a battery design that made sense.  Lithium ion with a little solar panel would be sweet."  He said:  "Anyway, love the podcast.  Glad you're going past 999.  I also only had an X/Twitter account to DM you and am very happy to see you've moved over to email.  Regards, Adam Tyler."



Okay.  So Adam is, of course, referring to one of this podcast's favorite past episodes which we've re-aired a number of times through the years because it tells a fun story which ends with a moral of the surprising benefits that can arise from being active rather than passive.  I first shared that youthful adventure of mine on the occasion of the 50th anniversary of the laser.  The device I designed and built when I was in high school was not a laser, though the beam of high-intensity directed sound energy it produced was likely coherent.



Now, 12 years ago, back in 2012, when this podcast was only seven years old, I recreated that device after so many of our listeners commented that their neighbors' barking dogs were ruining their lives.  Since I didn't have the web forum technology running that I have today, I created a Google group called "Portable Sound Blaster" for public discussion of this, and I published the final electronic design of the device which I had created on a page at GRC, naming the project "The Quiet Canine."  If you're curious, you can find it under GRC's website menu under "Other," and down at the bottom is "The Quiet Canine."  I think you can also just google "the quiet canine," and it comes right to my page.



Now, on that page I wrote:  "The good news is that we arrived at an extremely simple, inexpensive, and easy-to-build design for a small, lightweight, and painfully loud handheld sound emitter."  And then the page shows the design.  But then under the caption "The Bad News" I wrote:  "Many of these final TQC (The Quiet Canine) v2.2.2 devices were assembled and tested by those following and participating in the Portable Sound Blaster group at Google.  The devices were invariably incredibly loud and high pitched.  While their dads were assembling and testing the devices downstairs in the garage, their upstairs teenagers were complaining about the piercing sound penetrating their heads.  And of course dogs were at least as well able to hear it, and at much greater distances.



"But in no event was this able to function as any sort of barking deterrent.  Dogs heard it, and at great distance, but they didn't care.  We soon came to appreciate that my own original "point blank" blasting of the original "Portable Dog Killer," as I named my first device when I was in high school, was required for the device's effectiveness.  No dog next door, let alone down the block, will care about a high-pitched sound.  It needs to be blasted directly into the dog's face at a very short distance.



"Now, this means that while this device would not be useful for silencing dogs at a distance, it would likely be extremely useful and effective as a personal defense device for people walking, postal workers on foot delivering mail, and joggers who are harassed and threatened by overly aggressive canines on the loose.  Although we cannot and do not offer any specific guarantees, it's difficult to see how any attacking dog would not be stopped in its tracks by a close blast of incredibly loud and high-pitched sound."  That's what the website says.



So the bottom line is my particular use-case, which I described in that story, turned out to be unique.  I specifically designed and used that first device back in the early 1970s to train an incredibly aggressive, I mean, really rabid dog not to jump on the fence which bordered the sidewalk which was terrifying passersby, causing them to fall off the sidewalk into the street.  I saw it happen a number of times, and that's what motivated me to basically train the dog not to run at strangers by blasting it in the face several times point blank when it did that to me.  And after a couple days it just kind of peered around the side of the house to see who was there.  It completely changed its behavior.



But anyway, unfortunately what we learned was that a lot of people have a problem with dogs barking.  And I wish there was a solution for that, but this isn't it.  You know, I don't know that there is one except to try to talk to the dog's owner.  And unfortunately many dog owners who have loudly barking dogs are strangely unsympathetic to the complaints of neighbors.  So I don't know.  I don't have a solution for it.  I wish there was one.  For what it's worth, Adam, sorry.



But many of our listeners, due to this story, have occasionally sent me links to commercial devices that do this.  They do exist.  Unfortunately, based on all the experience of those who've built these devices, and these things really did, they were super loud, and they really did work, none of them stopped dogs from barking.  So I doubt that any of the commercial devices do that.  And they're probably just weak imitations of what we originally had.



MIKAH:  I want to say I think it depends on the dog because I have actual experience with - so when I first moved to California back in 2019, I have two small, small dogs.  One is a pure Chihuahua, the other is a Chihuahua fox terrier mix.  Really small dogs.  And I had in the past, when I lived in a home in Missouri, where there wasn't anyone attached directly next to me, I could leave them in the home during the day.  They would hang out on the sofa, they would eat, they would drink their water, I'd come home, everything would be fine.  And I was not cognizant of the fact that being attached to others in this town home meant that they would hear sounds through the walls that would frighten them, and they would bark at them.



So I would go off to work, and I didn't know this was happening, but they were barking while I was gone.  The way that I found out was - I can remember the day because it cranked my anxiety up because I got an email from the townhome people, and they said, hey, we've had complaints about your dogs barking.  I thought, oh, my god, I'm going to get kicked out.  This is awful.  What am I going to do?  I ordered this little, it looks like - and it's probably what you've seen.  It looks like a little tree house.  And it was intended to be used outdoors, and you'd, like, hang it.  And it hears the dogs bark, and it lets out a series of high-pitched [mimics sound].



But the sound that I'm making is not, whoops, the sound that they make.  Much different, much higher pitched.  And what it's intended to do on a sort of scientific level is to disrupt the dog's central nervous system to cause the dog to take its attention off of whatever is causing it to bark, and focus on that instead.



STEVE:  Yup.



MIKAH:  And if that happens enough times, it will break them of the pattern of choosing to bark at whatever they're barking at.  That's why some animals or some manufacturers make little devices to go around the neck, they're not shock collars, but they actually put out a spray of citronella, and the same thing happens.  That spray, whenever they're barking, they suddenly sniff that, it breaks their pattern of paying attention to whatever it was they were barking at.  Anyway, all of that's to say it actually did work for my small dogs.



STEVE:  Yay.



MIKAH:  And I actually, in the place we live now, we're back in a home that's not attached to other people.  But there are, on the other side of us, we have neighbors that because there's plants and stuff in the way, the dogs can't see, they can only hear.  And so it has been a little frightening for them at first.  And so I actually pulled those out of storage, hung them in the yards, and that has significantly reduced the barking as well.



STEVE:  Nice.  Okay, so your mileage may vary.



MIKAH:  Your mileage may vary, yeah, exactly.



STEVE:  Nice.



MIKAH:  I think it depends, yeah.  If you've got a big dog that's maybe prone to some aggressiveness and doesn't quite react as quickly to - because I think it depends also, you know, if you've got - if you're more of a prey animal than you are a predator, then those small sounds are going to draw your attention more than if you're a bigger dog, I think.



STEVE:  Yeah, it'll just eat the little tree house.



MIKAH:  Yeah.  I have tree houses like this for breakfast.



STEVE:  Actually I just got some feedback by email, which was written to securitynow@grc.com, while you were sharing the news about Melissa.  Apparently the lesser known security bureau links, many of them are broken.  Two are broken, one goes to a Wix page that doesn't go anywhere, and another one is a subsidiary of Experian, so you've already got that covered among the top four.  So it looks like Craig, who is the co-founder of CyberHoot, will want to click on those links himself and get them fixed up or remove them.  I mean, and if you've got something calling itself a credit bureau that's going to a Wix website...



MIKAH:  Yeah.



STEVE:  I don't think we need to worry about freezing anything there.  And you may not want to give all your personal and private information...



MIKAH:  Exactly.



STEVE:  ...to those people either.  And that was from Joey, I think it was Joey Albert who just sent me email, and I just saw it.  Yeah, Joey Albert.  So thank you, Joey, for that.  And we got everybody informed during the same podcast.



Okay.  So John wrote:  "Hey, Steve.  I stumbled across this very cool-looking hexadecimal clock face with ticking hands showing the time in the venerable Unix time and thought you, Leo, and the rest of the listeners would love to see it, too.  Check it out at," and then he's got a URL that I could read, but it's in the show notes, and I've got something better coming up.  He said:  "All the best to 999 and well beyond, John."



Okay.  So we've previously encountered this wonderful version of the Unix clock.  Thinking that I would probably have created a GRC shortcut for it previously, sure enough, I searched for and found it, created almost exactly two years ago on September 18th of 2022.  And the shortcut itself is, not surprisingly, "2038," so go to grc.sc/2038.



Unix time is represented by a 32-bit signed integer which has been incrementing once per second since midnight of January 1st, 1970.  In what's known as signed two's complement format, the most significant bit of a number's binary representation is reserved for the number's positive or negative sign, with the bit set to '1,' meaning that the number is negative.  Now, this works out naturally when doing two's complement binary math, which is the system used by all contemporary computers.  For example, subtracting 10 from 5 should produce negative 5, and that's what happens if negative values have their high bit set.  So the system works beautifully.



However, Unix time could and arguably should have been defined as an unsigned 32-bit integer since it was meant to be used for timekeeping into the future, not the past.  But as it is, the result of Unix time being a signed value means that negative values represent times before 1970, extending back to 1901, which is not highly useful for things like timestamping database entries and so forth, which is what we use this for.



The good news is that all modern Unix-like systems, and even some of the Unixes themselves, well, all of the Unixes themselves, have long ago switched to 64-bit time representations.  But as we always see, there are surprising corners of technology that are slow to update.  So it's entirely foreseeable that there will be some breakage somewhere when we finally get to 2023, 14 years from now.  I'm sorry, 2038, 14 years from now.



Okay, now, this specific clock site is very cool and very nerdy - and thus, you know, very appealing - since those 32 bits are broken into four 8-bit bytes, with each of the four bytes determining the position of each of the clock's four hands.  Since each 8-bit byte can have any one of 256 values, the clock has 256 "ticks" around its face.  And since trouble begins once the high byte, represented by the red hand, reaches its halfway point, because we're only able to use the positive half of all the values in a 32-bit signed integer, when that red hand is pointing straight down, something's going to break somewhere.  So this graphic makes it very clear that we're well on our way toward the Unix apocalypse.



Now, I have to say I would dearly love to still be doing this podcast 14 years from now and to be able to cover and discuss the events of the end of 32-bit Unix time.  I'm not sure I'll still be doing this in 14 years, but I would not be surprised if something didn't break.  So we'll see what happens.



Norbert shared:  "Bobiverse book number 5 came out on September 5th," so five days ago.  The book is titled "Not Till We Are Lost."  I just wanted to let everybody know.  The Bobiverse series has been a big hit among our listeners.  And so Norbert, who said "Thanks for the podcast," I will say thanks for the notification that there is now a fifth Bobiverse book.



MIKAH:  The Bobiverse is unique for me in that I very rarely enjoy, and I know this is kind of weird among nerds, I very rarely enjoy science fiction epics or anything like that, science fiction books in general.  I like science fiction shows.  But when it comes to books, typically if I'm going to read fiction, it's going to be high fantasy or some sort of fantasy.  Bobiverse really hooked me from the get-go.  And again, I was surprised myself and went into it expecting that I wouldn't keep listening to it.  And so it caught me off guard in really enjoying it.  So I was very pleased when September 5th rolled around, and I was able to get the next one.



STEVE:  Oh, cool.  So you already knew.



MIKAH:  Yeah.  Yeah, I did, because I had it in my wish list already to get it as soon as it was available.



STEVE:  Nice.  So I wrote here in the show notes, I said the Bobiverse books are pretty easy to breeze through.  But for anyone who's interested in really sinking their teeth into something that promises to be far more substantial, our listener Simon Zerafa sent me a note that one of this podcast's favorite sci-fi authors, none other than the great Peter F. Hamilton, is releasing his next novel next week.  Now, that's the good news.  What may be bad news, depending upon your need to achieve immediate closure, is that this is book number one of a two-part novel series.  The good news is there's only two of them.



In the past, as with for example "Pandora's Star," which left us hanging quite a while for the story's conclusion in "Judas Unchained," and later it was the same with Peter's "Dreaming Void" series, which had a number of books, you know, Peter's famous for laying down a lot - and I mean really a lot - of foundation in his novels, so that things are really finally just, you know, really get moving just as the first novel ends.  And it's like, oh.  So, you know, that may not bother everyone, I get it, but it bugs the crap out of me.  So I'm sure I am going to patiently wait for the publication of the series' second and concluding book because then I'll be able to purchase both books at once, and I'm sure I'll do that in order to read them back to back.



The first book's title is "Exodus: The Archimedes Engine."  And the synopsis, probably taken from the back cover of the hardback, just to give its reader a sense for what's to come while not being a spoiler, it reads:  "Forty thousand years ago, humanity fled a dying Earth.  Traveling in massive arkships, these brave pioneers spread out across the galaxy to find a new home.  After traveling thousands of light-years, one fleet of arkships arrived at Centauri, a dense cluster of stars with a vast array of potentially habitable planets.  The survivors of Earth signaled to the remaining arkships that humanity had finally found its new home among the stars.



"Thousands of years later, the Centauri Cluster has flourished.  The original settlers have evolved into advanced beings known as Celestials and divided themselves into powerful Dominions.  One of the most influential is the Crown Celestials, an alliance of five great houses that controls vast areas of Centauri.  As arkships continue to arrive" - right, remember they were all called by the announcement that we found a great place.  So "As the arkships continue to arrive, the remaining humans and their descendants must fight for survival" - I don't know why - "against overwhelming odds" - I don't know why - "or be forced into serving the Crown Dominion."



Okay.  So it sounds as though the Crown Dominion has become old and corrupt and, you know, bad.  So this thing says:  "Among those yearning for a better life is Finn" - who probably becomes the focus of this - "for whom Earth is not a memory but merely a footnote from humanity's ancient history.  Born on one of the Crown Dominion worlds, Finn has known nothing but the repressive rule of the Celestials, though he dreams of the possibility of boundless space beyond his home.  When another arkship from Earth, previously believed lost, unexpectedly arrives, Finn sees his chance to embrace a greater destiny and become a Traveler" - with a capital T - "one of a group of brave heroes dedicated to ensuring humanity's future by journeying into the vast unknown of distant space."



Okay.  So at this point this is not any sort of recommendation because I haven't read, you know, that first book.  I'm certain, as I said, I'll read both of them once they both become available.  So if anyone listening, I mean, it sounds like another fun Hamilton adventure.  And, you know, boy, when Hamilton gets going with a series, they can really be a lot of fun.  Lots of new, you know, brain-stretching tech in there.  So if anyone listening does decide to jump on the first book, knowing that they may be left with a classic Hamilton cliffhanger, please DO send your review to me at securitynow@grc.com, and I'll share what you think without any spoilers.



Hadrian said:  "Hi, Steve.  Longtime reader, then listener, then viewer.  I recently bought SpinRite.  Not yet needed for recovery, but I now have a burning question.  Am I the only one who looks at the raw data display and then suddenly says, 'Hey! I know which file that was!'?"  So I got a kick out of Hadrian's note because, though no one else has ever specifically mentioned it that I can recall, I, too, will often see something I recognize flash past on SpinRite's Real Time Activities display.  Of note is that SpinRite did not always show that.



Back before mass storage drives were able to manage their own defective sectors, SpinRite needed to and did handle all of that itself.  This meant that sectors which were embedded in clusters that had been found to be defective would need to be relocated, then replaced by good clusters.  So that region of SpinRite's Real Time Activities UI page once was used to track all of those changes and show totals by counts and by bytes of everything that SpinRite had done in moving things around within the file system in order to knit the file system back together after making those changes.



But at some point, once all drives became able to handle defect relocation autonomously, although SpinRite would still induce a drive to perform the needed relocation, now that would happen below the level of the file system.  That meant that I was able to remove all of that logic from SpinRite.  But that also meant that I needed to remove all of the tracking, totaling, and displaying of that work which SpinRite no longer needed to do.  And that left a big empty display region in SpinRite's user interface.  So I decided to fill that hole with an updating snapshot of the data that was passing by, so that SpinRite's user could literally see the data that SpinRite was working on.  It's ended up becoming one of SpinRite's more popular user-interface features.  So Hadrian, you're not alone in staring at the screen and saying, hey, I know what that was that just went by.



And I should tell you that I bought, during the work just last year on SpinRite 6.1, I realized from one of our other testers that it was possible to buy bad drives in bulk from eBay sellers.  I thought, what?  But I thought that was great, I need those.  So I bought several boxes of bad drives.  They're not very expensive because they're bad.  Mostly they're just heavy, so it costs a lot to ship them.  But I did that, and I'll just say drives you buy from eBay have not been wiped.  And so I was - since I needed these bad drives to test SpinRite, I was doing that.  And, you know, I would switch to the real-time activity screen, and I would see other people's data flashing by on the screen.



MIKAH:  Other people's "data."



STEVE:  Uh-huh.



MIKAH:  Oh, my.  I've seen their "data."



STEVE:  Yes.  So one of my announced and planned products which I will be doing, not immediately, but next to immediately, would that be, what's that next - no.  Anyway, it's a product that I've already got the trademark on it, I'm ready to go, it's called Beyond Recall.  And it will be a high-speed secure drive wiping utility which you can use for USB thumb drives or spinning drives that you connect to the computer.  Anyway, clearly a need to make secure data removal quick and painless, as I have a feeling it'll be very popular among this podcast's listeners.



MIKAH:  Beyond Recall, not to be confused with your upcoming freeware to get rid of Recall.



STEVE:  Yes.  



MIKAH:  Okay.  



STEVE:  And that's a very good point.  I've noted that there is a collision of the name Recall and Beyond Recall.  So I don't want to - I like calling something that is a secure drive wiper "Beyond Recall."



MIKAH:  Yeah, makes sense.



STEVE:  Yeah.  But so I think once you hear the name that I've got for the Recall product, everyone will agree we've got to stick with that one.



The final piece of feedback leads us nicely into this week's topic.  The feedback was sent by a UK listener named Laura, who wrote:  "Hi, Steve.  My name is Laura, from the UK.  As I have a Master's degree in Cybersecurity, I came across this article and hope you would be interested in talking about this both for me and everyone else."  She said:  "I love the show, and I'm so glad you're going past 999 as I have a standing appointment with you and Leo" - in this case me and Mikah - "every Tuesday evening..."



MIKAH:  Wow.



STEVE:  "...that no one is allowed to interrupt."  And then she says, in parens, "(my ex tried)."



MIKAH:  Oh, no.



STEVE:  So don't know if that was the deal breaker with the ex, Laura, but okay.  So she said:  "I've included the link below."  And this particular link is one to Cyber Security News, and in the URL I can see that it says "rambo-attack-air-gapped-systems."  So thank you again, Laura.  Oh, and she also said:  "P.S.:  Leo, love the new attic."  So she's apparently a video watcher of the podcast.



MIKAH:  Yeah, a watcher.



STEVE:  And I don't - what time zone in the UK, I'm not sure where that would put her.  But obviously Tuesday evenings she already has the video available.  So I don't know what that means relative to us making it Tuesday afternoon/evening.



MIKAH:  Oh, that's going to be - because I think it's like six hours plus.



STEVE:  Ahead?



MIKAH:  Yeah, yeah, they're definitely ahead.  Back when I was in Central time it was six hours ahead of me.  Now I'm not - so hold on.



STEVE:  So it's two or four.  Or would it be...



MIKAH:  So it's 11:41 p.m. right now in London.  So they're plus eight hours, 11:41 p.m.  So that's really late.



STEVE:  Yeah, yeah.  Maybe that's what happened to the ex.



MIKAH:  Yeah.  Maybe Laura watches live, and then that way at least it's only until, like, midnight that she's watching us on the podcast.



STEVE:  Oh, that would make much more sense, wouldn't it.  Yes, yes, yes, of course.  In that case, hi, Laura!



MIKAH:  Hi!



STEVE:  Happy to read your feedback.



MIKAH:  And it is time to talk about RAMBO with Steve Gibson.



STEVE:  So, thank you, Mikah.  Many of our listeners forwarded news to me of this latest side-channel attack brought to us by none other than another clever researcher at, you can probably guess, Israel's Ben-Gurion University of the Negev.  These guys are the ones who have brought us so many bizarre ways of exfiltrating data from computers through the years that no one would be surprised that we have another one.



It was easy to see how much attention this latest bit of research drew from the security press since the many links I received from our listeners - and thank you all, by the way, for sending them.  Basically you all voted for this week's topic by informing me of, like, hey, Steve, did you see this, did you see this, did you see this?  Yes.  They were from widespread security-related publications.  Before I dug into what it was all about, I was hoping that the reason for all the attention was not only because the new attack was named "RAMBO," and I was not disappointed.  So I decided that RAMBO should be this week's main discussion topic.



And also, everyone knows that I have a difficult time ignoring access to raw research.  The worst case is having to decipher something that some public relations person wrote that doesn't contain any of the really good nitty-gritty.  But in this case we have 18 pages of pure delicious research written by the researcher himself, Mordechai Guri, which explains his new attack in detail.



So the Abstract of Mordechai's research says:  "Air-gapped systems are physically separated from external networks, including the Internet.  This isolation is achieved by keeping the air-gap computers disconnected from wired or wireless networks, preventing direct or remote communication with other devices or networks.  Air-gap measures may be used in sensitive environments where security and isolation are critical to prevent private and confidential information leakage.



"In this paper, we present an attack allowing adversaries to leak information from air-gapped computers.  We show that malware on a compromised computer can generate radio signals from memory buses."  Thus RAM and RAMBO.  "Using software-generated radio signals, malware can encode sensitive information such as files, images, keylogging, biometric information, and encryption keys.  With software-defined radio hardware and a simple off-the-shelf antenna, an attacker can intercept transmitted radio signals from a distance.  The signals can then be decoded and translated back into binary information.  We discuss the design and implementation and present related work and evaluation results.  This paper presents fast modification methods to leak data from air-gapped computers at 1,000 bits per second.  Finally, we propose countermeasures to mitigate this out-of-band air-gap threat."



Okay, now, the first thing I'll note is that while 1,000 bits per second will not allow you to send Windows, or even a Windows update over the air, a modern state-of-the-art cryptographic private key is only several kilobits in length, so the keys to the kingdom could be broadcast from just such a compromised machine, over and over, every few seconds.  And since it would just appear as random RF noise, no one would ever be the wiser.



And unlike most malicious code whose purpose is readily revealed through inspection, any code that's being used to generate radio signals from memory buses will just be puzzling for any forensics researchers.  They'd stare at it and scratch their heads and never have any idea what the heck such code was doing.  They couldn't even be certain it was doing anything that was malicious.  It wouldn't appear to be doing anything at all since the designers of this code are using a far-fetched side channel of normal data processing to get their message out of the machine.



The second thing to note is that one of the consequences of today's heavy use of encryption is that we've grown to rely upon it completely.  What this means practically is that today we're far less worried about storing our sensitive encrypted data in far more accessible places, such as in the ubiquitous cloud. It's "Who cares if it's in the cloud.  It's encrypted; right?"  Sure thing.  That's true.  Right up until the time someone figures out how to exfiltrate the comparatively tiny secret key that's protecting the otherwise far less secure data.



So my point is, thanks to the application of cryptography virtually everywhere today, we now concentrate vastly more value into a handful of bits.  So whereas 1,000 bits per second cannot be used to transfer a massive database, those few thousand bits are the secret that's protecting a massive database in the cloud.  Then a few seconds worth of transmission is all that's needed to crack that database wide open.



Reminding us that air-gapping and air-gap exploits have a significant and deep history, Mordechai explains.  He writes:  



"Enforcing an air gap in a computing or networking environment involves physically and logically isolating a system, network, or device from external networks or communication channels.  This can be done by disconnecting network cables, disabling wireless interfaces, and disallowing USB connections.  In addition, it must be ensured that the isolated system has no direct link to any external communications infrastructure.



"Despite air-gapped networks being considered highly secure, there have been incidents demonstrating that air-gapped networks are not immune to breaches.  Stuxnet is one of the most famous air-gapped malware.  Discovered back in 2010, Stuxnet was a highly sophisticated worm that targeted industrial control systems, particularly those used in nuclear facilities.  It exploited zero-day vulnerabilities and used several methods, including infected USB drives, to jump the air gap and spread across isolated networks.



"The Agent.BTZ worm was another type of air-gap computer worm with advanced capabilities and targeted type.  It was specifically designed to spread through removable media, such as USB flash drives, and infiltrate computer networks, including those highly secure or air-gapped.  According to reports, that worm affected the U.S. Department of Defense classified networks."  Which is not easy to get into.  It did.  He said:  "Notably, more than 25 reported malware in the past targeted highly secured and air-gapped networks, including USBStealer, Agent.BTZ, Stuxnet, Fanny, MiniFlame, Flame, Gauss, ProjectSauron, EZCheese, Emotional Simian, USB Thief, USBferry, Retro, and Ramsay."  So plenty of them.



And then Mordechai discusses his new air-gapped attack.  He writes:  "In order to exfiltrate information from an infected air-gapped computer, attackers use special communication channels known as air-gap covert channels.  There are several types of covert channels studied in the past 20 years.  These attacks leak data through electromagnetic emission, optical signals" - like LEDs blinking - "acoustic noise" - like changing the noise of the fan of the computer - "thermal changes" - actually like ramping the CPU up, which generates more heat.  Now, those are extremely low data rate changes, but they are changes.  "And," he says, "even physical vibrations.  In this paper, we show how malware can manipulate RAM to generate radio signals at clock frequencies.  These signals are modified and encoded in a particular encoding allowing them to be received from a distance away."  And I'm actually going to focus on the encoding because that's the really cool part of this.



He says:  "The attacker can encode sensitive information - keylogging, documents, images, biometric information, et cetera, and specifically secret keys - and exfiltrate it via these radio signals.  An attacker with appropriate hardware can receive the electromagnetic signals, demodulate and decode the data, and retrieve the exfiltrated information.



Attacks on air-gapped networks involve multiphase strategies to breach isolated systems by delivering specialized malware through physical media or insider agents, initiating malware execution, propagating within the network, exfiltrating data using covert channels or compromised removable media, establishing remote command and control, evading detection, and covering tracks."



And finally:  "In the context of the RAMBO attack," he says, "the adversary must infect the air-gap network in the initial phase.  This can be done via a variety of attack vectors.  An attacker could plant malware on a USB drive and physically introduce it into an air-gapped network.  An unsuspecting insider or employee might connect the USB drive to a computer within the isolated network, unknowingly activating the malware and allowing it to propagate and exfiltrate data through the same USB drive or via covert channels.



An insider with access to the air-gapped network might intentionally introduce malware or provide unauthorized access to external parties.  This could involve transferring sensitive data to personal devices using covert communication methods like steganography to hide data within innocent-looking files.  An attacker could also compromise hardware components or software updates during the supply chain process."



I'll interrupt to note that the particular power of this attack is the degree to which its effects would be unsuspected and undetected.  I mean, like, the computer's working fine; right?  And it's not connected to anything.  Nothing is going out.  So an adversary, for example, might introduce their RAMBO-enabled malware into a device driver that's known to be used and needed by the targeted system.  Since no one would ever imagine that a device driver update could suddenly turn a PC into a covert short-range transmitter, the updated drivers might be delivered as part of a very careful and very clean offline CD or DVD carried update.  In other words, you can't infect a CD or DVD.  And that's all that would be required.



Mordechai continues:  "Once these components are installed within the air-gapped network, hidden malware might activate and communicate with external parties.  Note that APTs (Advanced Persistent Threats) in the past have targeted highly secured and air-gapped networks.  Recently, in August of 2023, researchers at Kaspersky discovered another new malware and attributed it to the cyber-espionage group APT31, which targets air-gapped and isolated networks via infected USB drives."  So that's still going on.



"In the second phase of the attack, the attacker collects information - keylogging files, other files, passwords, biometric data and so on - and exfiltrates it via the air-gap covert channel.  In our case," he writes, "the malware utilizes electromagnetic emissions from the RAM to modulate the information and transmit it outward.  A remote attacker with a radio receiver and antenna can receive the information, demodulate it, decode it into its original binary or textual representation."



Okay.  For the actual generation of the RAMBO RF signals, he explains:  "When data is transferred through a RAM bus, it involves rapid voltage and current changes, mainly in the data bus.  These voltage transitions create electromagnetic fields, which can radiate electromagnetic energy through electromagnetic interference (EMI) or radio frequency interference (RFI).  The radio frequency range of electromagnetic emanation from the RAM bus mainly depends on its specific clock speed, measured in megahertz or gigahertz.  This clock indicates how quickly data can be transferred between the CPU and memory.  The emanation levels are influenced by other bus characteristics, including its data width, clock speed, and overall architecture.  Faster RAM buses such as DDR4 and DDR5 having wider data paths can lead to quicker data transfers with increased emissions."



He said:  "When data is read from or written to memory, electrical currents flow through the RAM chips and the associated traces on the printed circuit board.  These electrical currents generate electromagnetic fields as a byproduct, which radiates electromagnetic energy.  To create an electromagnetic covert channel, the transmitter needs to modulate memory access patterns in a way that corresponds to binary data.  For instance, they could alter the timing or frequency of memory access operations to encode information.



"The sender and receiver must establish rules that define how memory access patterns translate to binary values.  For example, reading or writing an array to the physical memory at a specific timing interval might represent a zero, while another interval represents a one.  The receiver detects and decodes the EM emissions caused by the modulated memory activity.  This could involve sensitive radio frequency receivers or electromagnetic field sensors."



Okay.  So he says:  "One algorithm used OOK" - which is his abbreviation for On-Off Keying modulation, he says - "a basic form of digital modulation used in communication systems to transmit data over a carrier wave.  In our case, the OOK modulation involves turning the carrier wave on and off to represent binary data, where the presence of the carrier wave generated by memory activity corresponds to one binary state ('1').  The absence of the electromagnetic carrier wave corresponds to another binary state ('0').



"Note that to maintain the activity in the RAM buses, we used the MOVNTI instruction" - which is an Intel instruction - "which stands for Move Non-Temporal Integer.  It performs a non-temporal store of integer data from a source operand to a destination memory location.  This instruction is primarily associated with optimizing memory operations for certain types of data transfers, particularly in cases where the data is not to be reused immediately.  For the beginning of the transmission, we used the preamble sequence 01010101, allowing the receiver to be synchronized with the transmitter."



And I'll just interject to say that essentially what they discovered was that that particular instruction, MOVNTI, generates the most noise.  They found a specific Intel instruction that was a noisy instruction in terms of the amount of radiation it produced.



And he finishes, saying:  "For the fast transmission, we used Manchester encoding.  In this encoding, each bit of the binary data is represented by a transition or change in signal level within a fixed period.  Manchester encoding ensures a constant number of signal transitions, making it useful for clock synchronization and error detection."



Okay.  Now, I smiled when I saw that Mordechai had chosen to use Manchester encoded signaling since it's extremely simple and straightforward, and it's likely the best solution for his need.  Manchester encoding is still in wide use today due to its simplicity and robustness, though it dates back to 1948 where it was invented and first used to store and retrieve data on the magnetic storage drum for the University of Manchester's Mark 1 digital computer.  So this thing's been around for a while.



Because Manchester encoding provides such an elegant solution to a common problem, so common in fact, as I noted, it's still being used today - for example, it's the way our consumer IR remote controls send their data to our television sets and stereos.  And even RFID tags use Manchester encoding.  And since it provides another interesting dip into pure communications engineering and abstract computer science, I want to take some time to explain how it works.



The problem Manchester encoding beautifully solves is known as "clocking."  If you have a single-bit channel, as with RAMBO, where we have a radio signal that's either on or off, or a wire that's either carrying a current or not, you know, the current's being switched on and off, or a remote control's infrared LED that's either on or off, a significant problem arises when a long series of some number of ones or zeroes occurs in a sequence because the question quickly becomes, exactly how many zeroes or ones was that?



If some time passes without anything happening, for example, the radio is off or on for a while, how is the receiver to know precisely how many bits were just transmitted?  If the sender and the receiver both had perfectly and exactly equal knowledge of time passage, it would theoretically be possible to just count the elapsed time between a change from On to Off or Off to On, then divide that time by the time per bit to determine exactly how many "bit times" had elapsed.  The guys at Manchester may have initially tried that back in 1948.  But in their case, slight variations in the speed of their drum storage rotation rate would have quickly shown them that they needed some system that would be far more tolerant of slight timing variations.  And even today, two clocks are never precisely synchronized, nor are they ever running at precisely the same rate.



Communications designers have solved this problem by creating systems known as self-clocking encoding.  Self-clocking encoding systems ensure that something always happens often enough for the receiving end to stay synchronized with the sending end, even if their timing is not precise. And Manchester encoding, first used 76 years ago, does exactly that.  Here's how it works:  The key to understanding any encoding is to recognize that the signal is no longer the data.  Mordechai mentioned simple on-off keying which is an unencoded system.  With simple on-off keying, the signal IS the data. But that's where we run into trouble if many zeroes or ones are sent in an uninterrupted sequence.  So any encoding that we employ breaks this simple relationship between the signal and the data that the signal is intending to send.



Okay.  To talk about this, we'll refer to the signal as being "low" or "high," whereas the data bits are a zero or a one.  So "low" or "high" in the case of RAMBO would mean the RAM transmission is either on or off.  The transmitter is sending or it's not.  A one data bit is encoded as a low followed by a high, whereas a zero bit is encoded as a high followed by a low.  In other words, RAMBO transmits a one bit by having its RAM transmitter first not transmitting anything, then having it transmit.  And it sends a zero by having its RAM transmitter first transmitting a signal, then switching it off and not sending any signal.



The best way to think of this is that in Manchester encoding a one bit is encoded as a transition from low to high, whereas a zero bit is encoded as a transition from high to low.  This means that a so-called "bit cell" - which is the period of a single data bit - always contains two opposite states, both a low and a high, and the direction of the transition between those two states is the bit cell's data, a zero or a one.  If the bit cell contains a transition from low to high, radio off to radio on, that's RAMBO sending a one.  And if the bit cell contains a transition from high to low, that's RAMBO sending a zero.



Okay, now, if you think about this for a second, you'll see we have a problem.  In order to send a pair of ones, we need to have back-to-back low-to-high transitions, in other words, RAMBO radio off to RAMBO radio on transitions.  But at the end of that first bit the radio will already be on, and the next one we're sending requires the radio to start by being off.  We solve this problem by completely ignoring any inter-bit cell transitions.  In other words, only the transitions occurring in the middle of a bit cell carries any data.  The transitions occurring in between are ignored.



Okay.  So now, assuming that you've been following along, you're wondering how the receiver can tell the difference between the data transitions occurring in the middle of the bit cells and the transitions we're supposed to ignore which may or may not occur in between the bit cells in order to get ready for the next bit.  Manchester encoding provides the answer because every bit cell must always contain a transition, whereas there may or may not be a transition in between two bit cells.



Now, if you doodle with a piece of paper, with a pencil and paper for a bit, you'll quickly see that any receiver can perfectly "lock onto" the location of the bit cells the very first time a transition is missing, since that can only be the period in between bit cells.  That means that the next transition must be in the exact center of a bit cell as far as the transmitter is concerned.  So if the receiver knows only approximately the rate at which the transmitter is sending bits, that's now sufficient to allow it to judge whether an inter-cell transition opportunity has passed, and when the next guaranteed-to-always-be-present transition occurs.  And when that happens, the receiver updates its self-clocking "lock," which prepares it to judge whether the next transition occurs quickly, meaning that it's an inter-cell transition, or not until it's expecting the next data bit transition.



This simple system works so well that it was used in the earliest Ethernet physical layer standards.  And as I mentioned earlier, it's still used today by consumer home entertainment infrared remote controls, as well as RFID and near-field communications.



Mordechai had considered both simple on-off keying and Manchester encoding.  He wrote:  "Our analysis shows that the Manchester encoding is more relevant for the requirements of the RAMBO covert channel due to two main reasons:  One, the encoding aids in clock synchronization between the sender and receiver;  and, two, the frequent transitions make it easier to detect errors caused by signal loss, interference, or distortion.  However, it's important to note that Manchester encoding doubles the required bandwidth compared to direct on-off binary encoding, as each bit requires two signal states within the bit interval."



Okay, so how did all this turn out?  "Keylogging," he wrote,  "can be exfiltrated in real-time since UNICODE is only 16 bits per keystroke.  A 4096-bit RSA encryption key" - you know, the keys to the kingdom that you don't ever want to get loose - "can be exfiltrated in 4.096 seconds, and biometric information and small files such as .JPGs and small documents require a few seconds at the system's fast speeds."  They conclude:  "This indicates that the RAMBO covert channel can be used to leak relatively brief information over a short period."  And they were also able to receive this information - get this - at a distance of up to 700 centimeters.  For those of us who grew up using the Imperial system of measurement, as I did, 700 centimeters is 23 feet.  So this is useful and impressive.



And for those who remember the days of using the Pringle can to get increased WiFi range, I would imagine that if you got yourself a well-tuned and well-aimed Pringles can, you might be able to significantly improve on that performance distance.  For at least this first round of research, they seemed less focused upon distance than feasibility.  They've certainly shown that their RAMBO system is feasible.



It's been known for a long time that electronic devices generate and radiate electromagnetic interference while they're in use.  The somewhat strained acronym TEMPEST stands for "Telecommunications Electronics Materials Protected from Emanating Spurious Transmissions."  So TEMPEST-hardened devices are those which incorporate specific countermeasures designed to block or mask any useful information-carrying emanations from electronic equipment.



We can hope that any air-gapped machines which have been deliberately disconnected from any traditional form of data communications will have also been shielded so that none of the noise generated by the system's motherboard is able to find its way into the surrounding environment.  It would be necessary, of course, to first infect that machine with RAMBO technology malware.  But if that could be accomplished, any otherwise unprotected machine could be turned into a RAMBO transmitter.



MIKAH:  Wow.  This is what I have to say about all of this.  I was able to follow along with every bit of this, and that is what I really appreciate is that I actually got what was going on here and was able to piece it together.  So I always appreciate that about what you do.  Because as much as sometimes this security research can seem to go over one's head, you really did a great job with explaining what is going on here.  And I just - I think it's amazing.



We talked earlier about the person who made the fence, right, and wanting to get into the heads of the person or the people who made that fence with the hole in it shaped like an apple.  I want to know what was going on that they were able to - what was it, Mordechai? - yeah, Mordechai was able to even come up with this idea.  Was he sitting there, was Mordechai sitting there and was like, I wonder if I keep this antenna next to the device, and I'm looking at the RF signals coming by, and then you isolate those RF signals.  Or if it was like, I'm thinking about RAM one day, and then I think, maybe we could do something.  It's just so cool just to conceive of this stuff.  It's amazing.



STEVE:  Yeah, yeah, yeah.  It is really fun.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#992

DATE:		September 17, 2024

TITLE:		Password Manager Injection Attacks

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-992.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What happened during Microsoft's recent Windows Endpoint Security Ecosystem Summit?  And what, if anything, will probably result?  How reliable is ANY form of digital storage when used for long-term archiving?  What happened when an illegal Starlink Internet network was set up on a U.S. Navy ship?  What's the best solution for securing the Internet-facing "edge" of enterprise networks?  GRC has started notifying SpinRite 6 owners about 6.1.  What's been learned about the challenge of sending email in 2024?  Why might running SpinRite on an SSD cause the SSD to then appear to be running more slowly?  Why is true secrecy so difficult to achieve, and how were most password managers leaking some of their secrets?



SHOW TEASE:  Coming up on Security Now!, I, Mikah Sargent, am subbing in for Leo Laporte again this week.  Steve Gibson kicks things off by talking about what the heck happened at the recent Microsoft Summit, where they, you know, aimed to talk about what went wrong with CrowdStrike.  Plus we talk about storage and how, regardless of what you're using to store data, all of it falls to entropy in the end.  Then a conversation about a really fascinating investigation regarding Starlink and the U.S. Navy, and a very important conversation about password managers and how many of them are vulnerable to attacks.  All of that, plus so much more, coming up on Security Now!.



MIKAH SARGENT:  This is Security Now!, Episode 992, recorded Tuesday, September 17th, 2024:  Password Manager Injection Attacks.



It's time for Security Now!, the show where we talk to THE Steve Gibson about cybersecurity each week.  I am Mikah Sargent, subbing in once more for Leo Laporte.  Hello, Steve Gibson, reporting from another secret cave buried deep underground.



STEVE GIBSON:  An undisclosed location, yes.



MIKAH:  Good to see you.



STEVE:  Likewise, Mikah.  Great to be with you for our second of two, you standing in for Leo while he's out somewhere.



MIKAH:  Gallivanting.



STEVE:  Gallivanting around the territory.



MIKAH:  Yeah.



STEVE:  Yeah.  I'm looking forward to this.  I mentioned this last week, that I'd had some outreach from a security researcher who has previously been the source of many of our interesting and many times wacky sort of side-channel leakage issues.  So today's podcast, 992 - yes, closing in on 999, but no longer the end of the road for the podcast - is titled Password Manager Injection Attacks.  And of course the idea that password managers could be in any way subject to security trouble gets everybody's attention.



MIKAH:  Yeah.



STEVE:  They actually - they did two papers, one on password manager injection attacks and another on end-to-end encrypted messaging apps, specifically WhatsApp and Signal injection attacks.  I'll talk about that a little bit, just to kind of give some context.  But because everyone is using password managers, and the last thing you want is them to have a security problem, you know, that gets center stage for us.  But we're also going to talk about what happened during Microsoft's recent Windows Endpoint Security Ecosystem Summit, which, you know, was the thing that they did in reaction to the CrowdStrike global outage from a month and a half ago; and what, if anything, will result from that.



Also, and completely sort of off on the side, but of interest to our listeners, is how reliable is any form of digital storage when used for long-term archiving?  That's, you know, CDs, tape, hard drives, solid-state memory.  It turns out that we've got problems in terms of long-term archiving.  And, my god, talk about an explosion in the amount of storage and thus the amount of problem that we have.  Also, what happened when an illegal Starlink Internet network was set up on a U.S. Navy ship without permission, which is a big no-no, as we'll see.  What's the best solution for securing the Internet-facing edge of enterprise networks?  A security magazine reviewed all of the contenders, and I'm pleased that the one that had impressed me previously last Halloween turned out to win.  We'll come back to that briefly.



Also I finally started rolling out the notification of SpinRite 6.1 to all of 6.0 owners, and I've learned a lot, which I'll talk a little bit about.  Also why - this is actually a common question that I've had as a result of that - might running SpinRite on an SSD appear to make the SSD run more slowly, rather than more quickly.  And finally, the moral of the story underlying these password manager injection attacks is why is true secrecy so difficult to achieve, and how were most password managers leaking some of their secrets?  So I think a great and interesting podcast for our listeners.  And we've got another great Picture of the Week, as well.  So I think a lot of fun for everybody.  



MIKAH:  Absolutely.  I am very much, I don't know if looking forward to it is the right word, the stuff involving password managers.  But can't wait to hear about it, for sure.  All right.  Back from the break, and let's kick things off with a wonderful photo.



STEVE:  Okay.  So it didn't occur to me just until I was looking at this picture, as you were talking about our sponsor, that maybe I should have blurred this person's license plate.  I didn't take the picture myself, so it's out on the Internet, so it's already out there.  But still, you know.  Anyway, this is - and he probably doesn't care.  He's probably liking the...



MIKAH:  Attention, yeah.



STEVE:  The attention, yes, because clearly he's into attention.  Okay.  So this starts with a GMC truck which - and this is apparently kind of a techie owner because we notice from his back window in the lower left-hand corner he's got this - it's stenciled Transmission Lineman, as in one of those high-tension tower, like makes your hair stand on edge, you want to be very careful like with every single movement that you make sort of high-tension towers.  Anyway, apparently he noted that the dual exhaust ports at the bottom back of his truck are angled such that they actually really do resemble HDMI ports.  And so he went to the trouble of labeling them "HDMI 1" and "HDMI 2" on the back of his truck.



And so the truck was parked somewhere, and somebody thought that was kind of cool and took a picture of it.  I also noted that, and the caption I gave this picture was "Not only does this truck have dual HDMI outputs, but great signal strength and is fully charged."  Because in the upper right-hand corner of the back window we see that he's added four bars and full WiFi strength and 100% charge on his car.  So anyway, clearly a techie.



MIKAH:  I'm thinking about adding those.  Not the HDMI because it wouldn't work for my little Subaru, but I love the WiFi and [crosstalk].



STEVE:  That's just great, yeah.  That's just a kick.



MIKAH:  And yes, they do look like HDMI ports.



STEVE:  Oh, my god.



MIKAH:  You know, I'm just imaging the giant HDMI port that some clown would like, get out of the car and run behind and plug it in the back.  Oh, just funny.



STEVE:  Well, and if it were an eCar, then, you know, that would be great if it was the charging port.



MIKAH:  Oh, so good, so good.



STEVE:  Okay.  So we recently noted that Microsoft was responding, as we and they knew they must, to what's now being referred to as the Cloudflare Outage.  Wait.  I wrote Cloudflare.  I don't mean that.  CrowdStrike.  Wow.  Sorry, Cloudflare.  CrowdStrike Outage.  Duh.  It was late last night.  Although, you know, as we know, the proximate cause of this global meltdown was a bad update to CrowdStrike's kernel-level code, which forced a Windows kernel panic and thus shutdown, you know, the fact that third-party vendors are, first of all, allowed to install their code into the Windows kernel; and, secondly, that Windows lacked any graceful resiliency, as we all painfully saw, that would have allowed it to somehow arrange to get back on its feet using some sort of rollback to the pre-CrowdStrike update.  So, you know, this all meant that Microsoft also received some measure of blowback themselves.



So this summit was held last Tuesday.  They called it their Windows Endpoint Security Ecosystem Summit.  And then just this past Sunday, Microsoft posted about what happened, who attended, and some of what was said.  So this is what they shared with us.



They said:  "On Tuesday, September 10th, we hosted Windows Endpoint Security Ecosystem Summit.  This forum brought together a diverse group of endpoint security vendors and government officials from the U.S. and Europe to discuss strategies for improving resiliency and protecting our mutual customers' critical infrastructure.  Although this was not a decision-making meeting" - which, okay, be nice to have some decisions, but no, no.  They said:  "We believe in the importance of transparency and community engagement.  Therefore, we're sharing the key themes and consensus points discussed during the Summit, offering insights into our initial conversations."  There is a hand gesture I can make at this point, but this is a podcast for families, so I won't do that.



"We want to thank every one of our Summit attendees for dedicating their time to participating in these meaningful discussions.  The CrowdStrike incident in July underscored the responsibility security vendors have to drive both resiliency and agile, adaptive protection.  And it was inspiring to see the engagement throughout the event's agenda and activities.  Together with our Microsoft Virus Initiative" - they'll refer to that later as MVI - "partners - companies who develop endpoint protection and additional security products for Windows, covering client, server and IoT - we discussed the complexities of the modern security landscape, acknowledging there are no simple solutions."  Right?  Otherwise we would have done that.



"A key consensus point at the Summit was our endpoint security vendors and our mutual customers' benefit when there are options for" - there is some interesting reading between the lines here; right? - "when there are options for Windows and choices in security products.  It was apparent that, given the vast number of endpoint products on the market, we all share a responsibility to enhance resiliency by openly sharing information about how our products function, handle updates, and manage disruptions.



"In the short term, we discussed several opportunities to improve how we support the safety and resiliency of our mutual customers.  First, we spent time going into depth on how we employ Safe Deployment Practices at Microsoft" - in other words, you know, this is what we did, folks.  Why aren't you doing that?  Anyway, they said, "...and where we could create shared best practices as a community, including sharing data, tools, and documented processes.  We face a common set of challenges in safely rolling out updates to the large Windows ecosystem, from deciding how to do measured rollouts with a diverse set of endpoints to being able to pause or rollback if needed.



"A core SDP" - that's their Safe Deployment Practice.  "A core SDP principle is gradual and staged deployment of updates sent to customers.  Microsoft Defender for Endpoint publishes SDPs; and many of our ecosystem partners such as Broadcom, Sophos, and Trend Micro have shared how they approach SDPs, as well.  This rich discussion at the Summit will continue as a collaborative effort with our MVI partners" - that's the virus initiative partners - "to create a shared set of best practices that we will use as an ecosystem going forward."



So, right.  In other words, CrowdStrike, you didn't do that.  And I believe the proper term would be "bitch slapped," at this point, for not enforcing any sort of staggered rollout of their updates; right?  I mean, that's like the most obvious thing you could have done that would have caught this immediately.  Instead, the entire world was hit with this thing at once, and the entire world went down, you know, everyone using CrowdStrike.  So this, you know, there's no way to read this from CrowdStrike other than it's suggesting a level of "we can do no wrong" arrogance that did indeed come back to bite them.  And, you know, the lack of, like, it's just impossible to justify that in retrospect.  There's no good answer to the question, "Why the heck weren't you deploying any sort of staggered rollout?"  But, you know, they weren't.



Anyway, Microsoft continues, saying:  "Beyond the critical Safe Deployment Practices work, there are several ways we can enhance our support for customers in the near term.  Building on the Microsoft Virus Initiative program we have, we discussed how Microsoft and partners can increase testing of critical components, improve joint compatibility testing across diverse configurations, drive better information-sharing on in-development and in-market product health, and increase incident response effectiveness with tighter coordination and recovery procedures.  These are a sampling of the topics we plan to make rapid progress on, to improve our collective customers' security and resiliency.



"In addition, our Summit dialogue looked at longer term steps serving resilience and security goals.  Here, our conversation explored new platform capabilities Microsoft plans to make available in Windows, building on the security investments we have made in Windows 11.  Windows 11's improved security posture and security defaults enable the platform to provide more security capabilities to solution providers outside of kernel mode."  And of course that's a key aspect of this; right?  The problem is in order for endpoint security to do what it needs to do, because Microsoft has been relatively stingy about what they allow user-mode code to do, it's not able to get the deep access that it needs.  Microsoft is saying, well, Windows 11 is better.



Now, my eyebrows went up when I saw that because we're approaching end of life for Windows 10.  And this is going to be controversial because nobody wants 11.  And I mean, the enterprise has not moved.  And so there's a massive install base of 10 that won't be able to have these things because Microsoft is saying, well, it's in 11, so you'd better move.  The problem, as we know, is that they've also imposed arbitrary hardware constraints on the upgrade to 11 so that there's a huge install base of hardware that can't upgrade to 11 as it stands now.  So this is all - we're going to have some interesting times coming up this year.  And I'm glad we're going past 999.  



So they said:  "Both our customers and ecosystem partners have called on Microsoft to provide additional security capabilities outside of kernel mode which, along with SDP, can be used to create highly available security solutions."  In other words, everybody's saying, look, we don't want to be in the kernel either.  It terrifies us because of something like this happening.  But you're not letting us do what we need to do from user mode.



And I would argue that that's probably never going to happen.  Not because Microsoft wouldn't want to, but it's difficult to have user-mode hooks in the kernel that won't dramatically slow down Windows because in order to do what needs to be done, you need to not have user mode kernel mode ring transitions constantly.  And so to deeply get into the kernel, that's the only place you can watch everything going on without significantly slowing down Windows.



So, I mean, it really - it's truly a problem, which I understand that Microsoft gets, and it's why they're saying, well, the truth is they can't allow all this to be done from user mode or there'll be constant user-mode and kernel-mode transitions that will dramatically slow things down.  It's the reason that GDI, the Graphics Device Interface, was moved into the kernel.  We saw huge security implications and compromises as a consequence of that decision.  And on one hand Microsoft can be, you know, held responsible for that.  The flipside is, back when systems were a lot slower than they are today, they had no choice because the graphics device interface level had to make such deep and frequent access to the kernel.  That's why they moved it into the kernel.



Anyway, so they said:  "Some of the areas discussed during the Summit include performance needs and challenges" - oh, there it is - "outside of kernel mode; anti-tampering protection for security products" - right, because if you're in user mode you don't have the protections afforded by kernel mode - "security sensor requirements; development and collaboration principles between Microsoft and the ecosystem; and secure-by-design goals for future platforms."



They said:  "As a next step, Microsoft will continue to design and develop this new platform capability with input and collaboration from ecosystem partners to achieve the goal of enhanced reliability without sacrificing security."  And I'll just note that this is all happening as a consequence of that outage.  None of this would be happening if it weren't for that.  In other words...



MIKAH:  And isn't that - how often does it take something going wrong for people to start doing things correctly?



STEVE:  Right.



MIKAH:  Is that not the crux of so many issues?  It's really frustrating that - and it shouldn't have to be that way.  It shouldn't have to require things going this poorly to go, oh, right, now's the time.  But it's just so built into the way things work.



STEVE:  Well, and in this case, there had been sort of a - sort of a dtente had been achieved where Microsoft didn't like the fact that they had to open the kernel.  They did because the EU forced them to because Windows Defender has access to the kernel.  And they're saying, well, if your own endpoint protection technology, if you've given that technology kernel access, then you must open it up to the competition, in the interest of creating competition.  And arguably these third-party products like CrowdStrike do a much better job than Microsoft's endpoint protection system does.  So customers are getting a better result, at the consequence of this kind of event being possible.  And I will actually be surprised if much changes.



I think this is face-saving for Microsoft.  They had to do something to respond to the outage.  And so, oh, we're going to have a summit, and we're going to get everybody together, and we're going to figure out how to prevent this from happening.  The fact is CrowdStrike should have never let this happen because these things like incremental deployment are trivial to do.  They're doing it now.  I mean, they have said, you know, it's already in place.  They will never let this happen again.  Well, they shouldn't have let it happen the first time.  So there was some arrogance on their part.  They've learned their lesson.  But the fact is to do what these products need to do, the truth is they have to be in the kernel.  They have to be allowed that kind of access because it just - it can't be done up in userland.  



Anyway, so they had this first meeting.  They quoted a bunch of their partners coming out of this.  Adam Bromwich, who's Broadcom's CTO (Chief Technology Officer) and Head of R&D for their Enterprise Security Group, was quoted saying:  "Organizations today benefit from a diverse, layered security defense.  As a result, industry collaboration is vital to helping organizations stay ahead of persistent threats and remain resilient when unexpected business disruptions occur.  As a long-time Microsoft Virus Initiative" - so MVI - "Partner, Broadcom recognizes that working closely with Microsoft and other security vendors not only helps improve our customers' security posture, including endpoint protection, but also the greater global digital ecosystem."



Drew Bagley, the VP, Counsel for Privacy and Cyber Policy from CrowdStrike, the bad guys in this particular stumble that caused this summit to be created, said:  "We appreciated the opportunity to join these important discussions with Microsoft and industry peers on how to best collaborate in building a more resilient and open Windows endpoint security ecosystem that strengthens security for our mutual customers."  In other words, he said nothing.



MIKAH:  Thank you, because that's what it says to me.



STEVE:  Yeah, exactly.  It was like, okay, fine.  We're sorry that we brought the Earth to a standstill.  We realize the error of our ways, and we won't let it happen again.  And they quoted ESET and SentinelOne and Sophos and Trellix and Trend Micro.  You know, Trend Micro:  "I applaud Microsoft for opening its doors to continue collaborating with leading endpoint security leaders to make our mutual customers even more cyber resilient.  Looking forward to more collaboration."  And as I said, my feeling is that this was mostly for show.  In fact...



MIKAH:  Everybody probably - I'm saying this tongue in cheek.  Everybody probably walked away with a nice tote bag filled with lots of Microsoft goodies.  It's like, and here's your gift card for this quote, and here's yours for this.  It just feels very hollow.



STEVE:  Yes.  Well, and the photo that accompanied this blog posting was a perfect representation of this meeting.



MIKAH:  Love the signs.



STEVE:  Oh, my god.  A bunch of executives of various stripes, sitting around a conference table in a stunningly opulent office building conference room setting.  You know, there's the requisite whiteboard and a UI projected onto another screen.  Then we have the four world time digital clocks which are visible showing the time in London, Moscow, Beijing, and Sydney.



MIKAH:  For whatever reason.



STEVE:  Exactly.  For, like, what?  And the one laptop that we can see open is distinctly a Mac.



MIKAH:  Yup.



STEVE:  So okay.  I don't know what that means.  Maybe the guy's installed Windows 11 on his Mac because it runs better than it does on the Surface.  Anyway, I think my point is that nothing ever really gets accomplished at these kinds of meetings.  This was all just for show, for the government and for Microsoft shareholders.  You know, it's okay, you know, we realize we did bad.  We're responding to this problem.  What's actually going to happen now will be a long, multiyear series of slow, plodding, back-and-forth negotiations where Microsoft will present and may implement some next-generation set of userland hooks for use by their various third-party vendors.  The vendors will examine them and explain how what Microsoft is offering still doesn't give them the total freedom that they really want, and I think they can argue they need.  Which is only still available through true kernel-level operation.



And so it will go back and forth.  Perhaps something will eventually come of it.  But, you know, that's far from certain, I think, at this point.  So long as this meeting has been held and the parties are now "working on it together," face has been saved, lawsuits will trundle forward, and the vendors will all work harder not to make another similar horrible mistake.  As I said, if CrowdStrike had not made this mistake, this would have never happened.  It would never happen on its own because things were kind of okay.  You know, I mean, the third-party vendors had the deep access they needed.  They hadn't brought the world to a standstill.  And Microsoft had given only as much as they had to, you know, access and user mode.



My only hope was that since Mark Russinovich, who is truly a serious security kernel-level guy, we all know him from his founding of Sysinternals back in the day, which Microsoft then bought, Mark Russinovich tweeted about this, saying that this really did represent some future hope.  So I have some more hope that something might actually change, but nobody should be holding their breath.  We know that in retrospect CrowdStrike now realizes it needs to be able to catch anything like this before it is ever rolled out to the entire world.  And it's trivial to do.  That's what's so mindboggling is it's not like this is rocket science to do an incremental release.  Everybody else does that.



So anyway, we know that they're going to be doing it.  They've got that in place already.  It also means that everyone else must do the same and never fail at this trivial-to-implement requirement.  So, you know, no more cowboy developer jock behavior.  The stakes are now far too high.



MIKAH:  Amen.  Amen.  All-righty.  Back from the break, and let's talk about how - how it's a good idea, not a good idea, maybe it's safe, maybe it's not.  There's a problem with saving stuff.



STEVE:  Yeah.  A listener forwarded this piece from Ars Technica to me, which was titled "Music industry's 1990s hard drives, like all hard disk drives, are dying."  And the subhead is "The music industry traded tape for hard drives and got a hard-earned lesson."



MIKAH:  Yikes.



STEVE:  So I'll just share a bit of what they said.  They said:  "One of the things enterprise storage and destruction company Iron Mountain does is handle the archiving of the media industry's vaults.  What it has been seeing lately should be a wake-up call:  Roughly one-fifth" - so one out of five - "of the hard disk drives dating back to the '90s it was sent are entirely unreadable."



MIKAH:  Wow.



STEVE:  Yeah, you just think, okay, here's a drive.  You guys store it for us.  And we may need the data in the future.  We'll let you know.  And then we'd like it back.



"Music industry publication Mix spoke with the people in charge of backing up the entertainment industry.  The resulting tale is part explainer on how music is so complicated to archive now, part warning about everyone's data stored on spinning disks.  Robert Koszela, global director for studio growth and strategic initiatives at Iron Mountain, told Mix:  'In our line of work, if we discover an inherent problem with a format, it makes sense to let everyone know.  It may sound like a sales pitch, but it's not; it's a call for action.'



"Hard drives gained popularity over spooled magnetic tape as digital audio workstations, mixing and editing software, and the perceived downsides of tape, including deterioration from substrate separation and fire.  But hard drives present their own archival problems.  Standard hard drives were also not designed for long-term archival use.  You can almost never decouple the magnetic disks from the reading hardware inside, so that, if either fails, the whole drive dies.



"There are also general computer storage issues, including the separation of samples and finished tracks, or proprietary file formats requiring archival versions of software.  Still, Iron Mountain tells Mix that:  'If the disk platters spin and aren't damaged,' it can access the content.  But 'if it spins' is becoming a big question mark.  Musicians and studios now digging into their archives to remaster tracks often find that drives, even when stored at industry-standard temperature and humidity, have failed in some way, with no partial recovery option available."  So it's completely dead.



"Koszela says:  'It's so sad to see a project come into the studio, a hard drive in a brand-new case with the wrapper and the tags from wherever they bought it still in there.  Next to it is a case with the safety drive in it.  Everything's in order.  And both are bricks.'"



MIKAH:  No.



STEVE:  "Mix's passing along of Iron Mountain's warning hit Hacker News earlier this week, which spurred other tales of faith in the wrong formats.  The gist of it:  You cannot trust any medium, so you copy important things over and over, into fresh storage.  Optical media rots, magnetic media rots and loses magnetic charge, bearings seize, flash storage loses charge, et cetera.  Entropy wins, and sometimes much faster than you'd expect.



"There's a discussion of how SSDs are not archival at all; how floppy disk quality varied greatly between the '80s, '90s, and 2000s; how Linear Tape-Open, a format specifically designed for long-term tape storage, loses compatibility over successive generations; how the binder sleeves we put our CD-Rs and DVD-Rs in have allowed them to bend too much and stop being readable."



MIKAH:  Oh.



STEVE:  I know.  One thing after another, one format after another failing.  They said:  "Knowing that hard drives will eventually fail is nothing new.  Ars wrote about the five stages of hard drive death, including denial, back in 2005.  Last year, backup company Backblaze shared failure data on specific drives, showing that drives that fail tend to fail within three years, that no drive was totally exempt, and that time does, generally, wear down all drives.  Google's server drive data showed in 2007 that hard disk drive failure was mostly unpredictable, and that temperatures were not really the deciding factor.  So Iron Mountain's admonition to music companies is yet another warning about something we've already heard. But it's always good to get some new data about just how fragile a good archive really is."



So I can speak for myself.  I run a bunch of my own servers, you know, my own actual hardware.  I think I have four separate physical servers.  Each of the servers has four hard drives.  Actually, one of them may still have SSDs, but I'm beginning to swap them out because I have not found them to be more reliable then lower size, smaller size, and by that I mean like 2TB is sort of what I've settled on, spinning drives.  Every single one of those is running RAID 6, which is two drives of redundancy.  So any two of the four drives could fail, and I lose nothing.  And all of the RAIDs, all four of the RAID arrays are being monitored continuously.



And every so often I receive email telling me that one of the drives has failed.  So that's okay.  Another one could still fail in that group of four, and I'd still lose nothing.  So it's not an emergency, but within a day or two I go to Level 3, you know, go through all the security measures, get access to my hardware, pull the dead one out, put a new one in.  It spins up, and the RAID rebuilds itself, reestablishing its RAID 6 two-drive failure.  And it happens maybe every six months or so.  And in fact, right here, this is - I have it because I've been meaning to run SpinRite on it.  This is the most recent drive to die.  It's a 2TB Seagate Barracuda hard drive.  Something about it my system doesn't like.  And so I'll run SpinRite.



The time before that this happened, there was - sectors are not actually 512 bytes any longer.  They still look like they're 512 bytes because they always were in the past.  They're actually 4K.  It's much more efficient to have larger physical sectors because you get much more efficiency from error correction, and you just don't need all of the - all of the gaps between sectors take up space.  So what I found was there was a single block of what looked like eight logical sectors, but eight 512 byte, which is half a K, so eight half a K sectors is actually one 4K physical sector.  It was bad, and the RAID said, okay, I quit.  Bad drive.



Anyway, I ran SpinRite on it.  It said, oh, you've got a contiguous run of eight sectors which have a problem.  SpinRite did what it could to recover it.  It rewrote the sector, and it was fine.  And that was all that was wrong with the drive.  In other words, nothing actually wrong.  Just a tiny region, some set of bits that were uncorrectable, and so the RAID said, sorry, this drive's no good.  In fact, it was fine.  It just needed a little bit of fixing, and then it was good to go.



So there is good reason to believe that performing a periodic rewrite of either magnetic spinning or electrostatic solid-state mass media is an extremely useful thing to do.  And you don't need SpinRite to do that, although SpinRite does make that easy, and it provides a great deal of feedback about the state of the drive.  And if there is any sort of trouble, it'll fix it for you.  But we learned, for example, a few years ago, and it was a surprise at the time, but it's not anymore, that offline SSDs just sitting on a shelf tend to lose their data more rapidly when they're stored at high temperature.  Since SSD storage is just about charge leakage, it makes sense that higher temperature would tend to weaken the strength of the dielectric insulation which isolates the charge bits.



And so you do want to, if an SSD is offline, you want to store it in a cool place.  But the lesson here really is, and this is the point that the article made.  At one point it said don't assume that anything sitting on a shelf for years will be readable when you need it.  You really do need to periodically plug it in, make sure it's still readable, and I would rewrite it, just to strengthen the bits.



MIKAH:  I need to go back, though, because did I hear you right in that you said every six months you're having to replace a drive?  Is that...



STEVE:  Yeah.  So I have four times four, I have 16 spinning drives.  And, yeah, I would say every six months or so a drive says, okay, I'm hurting.  And so the RAID sends me email, and I go and swap it.  Now, I have not yet run SpinRite on this drive.  The last time that happened, there was actually nothing wrong with the drive.  It just had that one little burst of trouble, SpinRite rewrote the sector, then the drive was fine.  And I put it back in the RAID array, and it hasn't failed since.  So, you know, it's a very touchy failure which is not a big problem.  But I have had SSDs actually just completely die.  So that's why I'm no longer thinking, oh, solid-state, that's way more reliable.



It's like, I guess what I'm saying is I refuse to have any loss of data.  So refusing to have any loss of data means double redundancy and just keep swapping.  So drives are consumables is the way I guess I would think about it, is they're consumables, and I'm consuming a drive at the rate of about maybe one or two a year in order to be running four RAIDs with four drives per RAID.  And I've never lost a byte of data.



MIKAH:  There you go.  That's the brag that you get to at the end.  Like, yes, I may be doing it every six months, but never lost a byte of data.



STEVE:  Okay.  So this story is really fun.  The Navy Times recently blew the lid off an intriguing story of a U.S. Navy warship.  There were some officers who had installed a secret Starlink-based network on board so that a select few of the upper echelon would not be deprived of their precious Internet connectivity while they were deployed at sea.  In their piece headlined "How Navy chiefs conspired to get themselves illegal warship WiFi," the Navy Times wrote the following.  And I'm just going to share the beginning of it.  It's a long article.  I've got the link here for anyone who wants more.  But here's what they said.



They said - this will give you the gist:  "Today's Navy sailors are likely familiar with the jarring loss of Internet connectivity that can come with a ship's deployment.  For a variety of reasons, including operational security, a crew's Internet access is regularly restricted while underway, to preserve bandwidth for the mission and to keep their ship safe from nefarious online attacks.  But the senior enlisted leaders among the combat ship Manchester's gold crew knew no such privation last year, when they installed and secretly used their very own WiFi network during a deployment, according to a scathing internal investigation obtained by Navy Times.



"As the ship prepared for a West Pacific deployment in April 2023, the enlisted leader onboard conspired with the ship's chiefs to install the secret, unauthorized network aboard the ship, for use exclusively by them.  So while rank-and-file sailors lived without the level of Internet connectivity they enjoyed ashore, the chiefs installed a Starlink satellite Internet dish on the top of the ship and used a WiFi network they dubbed 'Stinky' to check sports scores, text home, and stream movies."



MIKAH:  Oh, come on.



STEVE:  "The enjoyment of those wireless creature comforts by enlisted leaders aboard the ship carried serious repercussions for the security of the ship and its crew.  The investigation noted:  'The danger such systems pose to the crew, the ship, and the Navy cannot be understated.'  Led by the senior enlisted leader of the ship's gold crew, then-Command Senior Chief Grisel Marrero, the effort roped in the entire chief's mess by the time it was uncovered a few months later.



"Marrero was relieved in late 2023" - and there was a court-martial, by the way - "after repeatedly misleading and lying to her ship's command about the WiFi network, and she was convicted at court-martial this spring in connection to the scheme.  She was sentenced to a reduction in rank to E-7 after the trial and did not respond to requests for comment for this report.  The Navy has yet to release the entirety of the Manchester investigation file to Navy Times, including supplemental enclosures.  Such records generally include statements or interview transcripts with the accused.



"But records released so far show the probe, which wrapped in November, found that the entire chief's mess knew about the secret system, and those who didn't buy into it" - literally buy, b-u-y, into it - "were nonetheless culpable for not reporting the misconduct.  Those chiefs and senior chiefs who used, paid for, helped hide, or knew about the system were given administrative nonjudicial punishment at commodore's mast, according to the investigation.  All told, more than 15 Manchester chiefs were in cahoots with Marrero to purchase, install, and use the Starlink system aboard the ship.



"The investigation said:  'This agreement was a criminal conspiracy, supported by the overt act of bringing the purchased Starlink onboard USS Manchester.  Any new member of the CPO Mess which then paid into the services joined that conspiracy following the system's operational status.'



"Records obtained by Navy Times via a Freedom of Information Act request reveal a months-long effort by Marrero to obtain, install, and then conceal the chiefs' WiFi network from superiors, including the covert installation of a Starlink satellite dish on the outside of the Manchester.  When superiors became suspicious about the existence of the network and confronted her about it, Marrero failed to come clean on multiple occasions and provided falsified documents to further mislead Manchester's commanding officer, the investigation states.  Unauthorized WiFi systems like the one Marrero set up are a massive no-no for deployed Navy ships, and Marrero's crime occurred as the ship was deployed to the West Pacific, where security concerns become even more paramount among heightened tensions with the Chinese.



"While Marrero claimed the WiFi system was secretly installed for morale purposes, the investigation notes that such a claim 'is undermined by the selective availability of the WiFi and strict control of its access to the CPO Mess only.'



"The Manchester's secret WiFi network was born in March of 2023, when Marrero and a co-conspirator got to work buying and installing the Starlink system before the ship's deployment began the following month.  The Starlink dish was installed on the Manchester's O-5 level weather deck during a 'blanket' aloft period, which requires a sailor to hang high above or over the side of the ship.  During a 'blanket' aloft, duties are not documented in the deck logs or the officer of the deck logs, according to the investigation.  It's unclear who harnessed up and actually installed the system for Marrero due to redactions in the publicly released copy of the probe, but records show Marrero powered up the system the night before the ship got underway to the West Pacific waters of U.S. 7th Fleet.



"Marrero and her cohorts paid $2,800 for a Starlink High Performance Kit with a personal credit card, and contacted Starlink to expedite shipping so the system would arrive in time for the deployment."



MIKAH:  I'm glad our tax dollars weren't used to purchase it.  I'm glad to hear that.



STEVE:  At least it was paid for with personal money; right.



MIKAH:  Yes.  Whether it was then sort of, you know, came out in the wash, I don't know.  But at least that [crosstalk] money.



STEVE:  "Starlink offers plans ranging from $90 to $5,000 a month, and allows users to control network settings via a cell phone app.  The Navy is installing such authorized capabilities aboard some ships in the fleet.  But that was not the case aboard the Manchester, where Marrero set up payment plans for the chief's mess to pay for the system  either $62.50 a month or a one-time fee of $375."



MIKAH:  It's a whole business.  It's [crosstalk] WiFi.



STEVE:  "So that the ship's Chief Petty Officer Association treasurer collected the money into a chief's mess checking account."



MIKAH:  What?  This is a - this is wild.



STEVE:  "Those involved also used the Chief Petty Officer Association's debit card to pay off the $11,000 monthly Starlink bill."  Ah, so they went for the $11,000 a month...



MIKAH:  A thousand, $1,000, not $11,000.



STEVE:  Oh, sorry, sorry.  Yeah, $1,000 a month Starlink bill.  "And Marrero warned the chiefs to only use the network in their rooms.  Marrero served as the gatekeeper of the system, records show, downloading and maintaining the Starlink app from her phone and naming it 'Stinky.'"



MIKAH:  [Crosstalk] Kingpin.



STEVE:  That's right.  "Only she could add others to the network  and would directly type the password into their devices" - so that they would know what the password was.  So it's like, don't worry, I'll type the password in, then you just use it, and it'll remember it.  "After Manchester got underway from San Diego, Marrero and the chiefs soon realized the WiFi signal" - oh, darn - "didn't cover all areas of the ship."



MIKAH:  Maybe because it's a lot of metal everywhere?



STEVE:  You think?  "So the senior chief purchased signal repeaters and cable at the Navy Exchange store in Pearl Harbor, Hawaii, during a port visit in late April or early May, according to the investigation."  That's right, we need to get us some repeaters here so that we'll be able to have access to Stinky no matter where we are.



MIKAH:  Honestly, I'm becoming impressed.  This is like multilevel - it's kind of getting impressive.



STEVE:  So they said:  "Little stays secret within the close quarters of a deployed ship.  And shortly after" - you can imagine; right?  "And shortly after getting underway..."



MIKAH:  Yeah.  How are you on your phone right now?  How are you watching that video?



STEVE:  Uh-huh.  How do you know what the sports scores were?



MIKAH:  Yeah, exactly.



STEVE:  "Scuttlebutt started swirling among some sailors about the unauthorized WiFi network.  The ship's former executive officer, Commander Matthew Yokeley, caught wind of the rumors in May and notified the commanding officer, Commander Colleen Moore.  Moore confronted Marrero about whether the chief's mess had an unauthorized WiFi network that same month.  Another unidentified crew member approached Marrero about a WiFi network aboard the ship after finding available networks on a device that started with the name 'Stinky.'  It's unclear who found the 'Stinky' network, due to redactions in the report.



"In both instances, Marrero denied that such a WiFi network existed.  But she soon changed the 'Stinky' WiFi network name to another moniker that looked like a wireless printer, even though no such general-use wireless printers were present on the ship, the investigation found."  Anyway, wow is right.  Wow.



MIKAH:  Wow.  That's just very impressive.  See, and here's the thing.  I was very pleased to hear that our tax dollars were not used to make the purchase of this.  But the money that went into the investigation of this and the subsequent court-martial and everything that was involved there, we did pay for that.  So in that way I'm a little bit perturbed.  Although, yeah, like, this is impressive.  And I would love to hear Marrero's sort of reasoning, you know, because you've got to - is it like we just really wanted to watch our shows?  Whatever it happened to be, you know, there's got to be more to it.



STEVE:  Right.  You know, with so many people today seemingly unable to separate themselves from the Internet, it's foreseeable that there would be significant pressure to maintain connectivity while at sea.  But at the same time we know how true it is that any form of Internet connection would need to be highly filtered.  And that's the problem.  You know, can you imagine how much any hostile foreign power would love to get into a smartphone or a laptop of someone onboard who clicked the wrong link?  You know, they're running TikTok, which is controlled by the Communist Chinese government indirectly.  And then once in there, pivot with that access and jump into the ship's internal networks.



MIKAH:  Yeah, well, and there's something to be said, too, for the individual who's behind the company that is responsible for Starlink in the first place also having that information of where that boat is at any given time.  That's also something to consider.  And it kind of makes you wonder if Marrero heard that these other ships were having this done and was like, well, I'm not waiting nine years to finally get it installed on this ship.  We'll just do it ourselves.



STEVE:  So, but you bring up a good point that hadn't occurred to me, like the underlying motivation.  Was it because she desperately needed to maintain access to friends and family or something?



MIKAH:  Yeah, was someone ill or...



STEVE:  Or was it actually a profit center?



MIKAH:  Right.



STEVE:  Was it like, hey, I can make some coin here by selling access to my fellow upper echelon.



MIKAH:  Right.  And I could also be simply, I mean, I can't speak to what all is involved, obviously, in any of these, what, patrolling bodies.  But maybe, you know, if you are trying to recruit certain people, it's like, do we want to go to the place that has the WiFi, so whenever we go out on these long voyages?  And it's like, look, we all have to put some money in because it's expensive.  But, hey, at least you've got access to WiFi, and you don't have those pesky Navy blockers on everything, so you actually get to look at your TikTok.  You know what I mean?  Like there could be people who...



STEVE:  We heard that this is the party ship.



MIKAH:  Yeah, this is the party ship.



STEVE:  So this is where we want to be.



MIKAH:  You've got to pay a little bit, but it's still the party ship.



STEVE:  And I think, you know, another interesting question here is that it really, I mean, I get how addicted people - and that's the right word to use; right? - how addicted some, especially some people, not everybody, but are to 24/7 Internet access.  And you're a sailor going out on a ship, and you're dark.  You know, I mean, it's over.  And the other thing is, how big and complex must the operation of these ships be that a Starlink antenna box sitting - it can be installed on the deck of the ship, pointing at the sky, and nobody walking by says "What is this box?"



MIKAH:  What is that?  I don't remember that being there.  Like it might be a spy - like, come on.  Yeah, those things have got a lot going on, apparently.  Wow.  And I've got to say, too, I sort of feel like Marrero could end up hosting one of our shows, given that Marrero knew - this is very clever, to change it to make it look like it was a wireless printer.  That's clever.  That takes knowledge of some level of networking, along with everything else that was involved, to think about being the one to do the WiFi pass, I mean, there's some intelligence there.  Maybe eventually Marrero will make her way into, I don't know, the Pentagon or something.



STEVE:  I have a friend who named his own WiFi "NORAD scanner" or something.  It was like, nobody's going to dare touch this.



MIKAH:  Yeah, I'll leave that one alone, thank you.  CIA watching you.  No, I'll skip that one.



STEVE:  Okay.  Next we're going to talk about the winner of a recent competition for the best secure access edge service.



MIKAH:  All right.  We are back from the break, and let us continue on.



STEVE:  Okay.  So it was, well, I thought it was a few months ago, turns out it was more like 10 months ago.  I shared how impressed I was after meeting with and learning about the technology that the guys at ADAMnetworks, which is Adamnet.works, had created to help secure the Internet-facing border of enterprises.  As I've noted several times, that is a daunting task, and it's not a job I would want.  You know, like how do you secure Sony, you know, Entertainment when anybody clicking on a bad link can infect the network.  So while I was delighted to see it, I was not surprised to discover that SC Magazine, a well-known and reputable security industry publisher, after running a head-to-head competitive comparison and evaluation of the industry's many various solutions, picked adam:ONE, which is the name of this system that they offer, as the winner.



SC Magazine wrote - and this is just a one-liner.  They said at the top of their coverage of this, they said:  "ADAMnetworks has claimed the prestigious Best SASE [S-A-S-E]" - I have what that acronym stands for here somewhere.  It's not coming to mind.  Oh, Secure Access Service Edge Solution.  So "ADAMnetworks has claimed the prestigious Best SASE Solution award at the 2024 SC Awards for its cutting-edge product, adam:ONE.  In a cybersecurity landscape where traditional reactive methods often fall short, adam:ONE stands out by providing a proactive, zero-trust security solution designed to eliminate threats before they infiltrate networks.  This recognition places ADAMnetworks among the leading innovators in the increasingly competitive Secure Access Service Edge market."



So anyway, I just wanted to follow up on what we initially talked about back then, since protecting the enterprise from all of the mischief that those inside the enterprise might get up to is no small task.  And since the adam:ONE folks appear to have the best handle on doing that job, I have a link to the SC Magazine's announcement with many more details in the show notes.  And it was Episode 946, which was Halloween, October 31st last year, 2023, when I shared the results of my meeting with the adam:ONE folks.  So anyway, for anyone who's interested, these guys, you know, won the best-in-class award for their solution for securing the enterprise perimeter.  And I'm not surprised because, as I shared back at the end of October last year, they figured out how to do it, and they do it right.



Many of our listeners may have received email from me about the availability of SpinRite 6.1.  Of course that won't come as news to anyone listening.  But for anyone who purchased SpinRite 6.0, which was released 20 years ago, back in 2004, it would likely come as a welcome surprise.  So over the weekend I received a note about this from a listener named Patrick.  He wrote:  "Good morning, sir.  Quick note to let you know I've received an email from SpinRite.news."  That was the domain that I used for sending the email.  He said:  "But it was flagged as spam by Exchange and dumped into my junk folder."  He says:  "Otherwise, thanks for the work on SpinRite 6.1.  I'll let you get back to work on 7.0 now.  Signed, Patrick."



So I replied to Patrick, writing:  "Thanks for your feedback, Patrick.  Since I'm mailing to all past SpinRite owners for the past 20 years, I'm sending those announcements through that domain you noted, SpinRite.news.  Since that domain has not earned a reputation as a valid email sender, Apple is bouncing all incoming email addressed to anyone at me.com, icloud.com, and mac.com.  And as you note, Exchange is routing incoming mail to spam," I said, "but at least it's not bouncing the mail back."



So I said:  "My primary goal for this is twofold:  I do want to inform any non-podcast listeners of the availability of a free upgrade to SpinRite 6.1, and I also want to remove all bad email addresses from the list for the future.  This is why I'm sending from the previously unused domain SpinRite.news, since I expect that the bounce rate will be high, especially for the oldest 20-year-old email addresses, and I want to keep GRC.com's email reputation as spotless as possible."  I said:  "Once I've managed to update GRC's creaky old SpinRite owner list, I'll be able to mail from GRC.com, using its clean email reputation."  I said:  "Mail should then get into people's inboxes."



So it's been an interesting - it's been an education.  I set up a relationship with the guys at Postmark that are an email forwarding service, and established all of the proper credentials and security and cryptographic signing and everything to authenticate email coming from SpinRite.news.  And then I began mailing from the most recent in the direction of the least recent.  So from 2024, and then 2023, 2022, 2021, and so forth.  And just as you'd expect, as I went back further and further in time, the bounce rate began to increase because people had, you know, left the companies where they were when they purchased SpinRite, so their inbox was terminated, and so forth.  So I got as far back as 2011.  I got all the way back through 2011, and the bounce rate had then reached one out of 10, which is the highest level that these guys are comfortable with me having, sending email through them.



So anyway, I'm going to come up with a different approach.  The interesting thing, I mentioned that all of the email into Apple bounced.  I saw the same thing occurring when I was first doing the regular podcast mailings like I did this morning.  I sent out 9,400-plus pieces of email for this podcast to the subscribers to the Security Now! list.  And for the first few weeks, Apple flatly blocked all of that.  But then it stopped.  And for the last handful of weeks, everything's been working perfectly, and email's been flowing without any problems.



So anyway, I thought it was just interesting.  We've talked many times about how, when software is digitally signed, reputation is everything.  Anybody can get a bad certificate, and bad guys do.  You really have to sign your software now.  But if it's not a certificate that has earned itself a reputation as signing non-malware, you know, Windows looks at it with a raised eyebrow and says, I don't know if I'm going to let this run and, you know, quarantines it.  Turns out reputation is just as important for email.  So my using - in retrospect, I'm not sure that I shouldn't just have sent from GRC because I'm able to see what the bounce rate is and throttle back or stop.  But I didn't know how it was going to go.  It actually kind of went better than I expected it to.  You know, I was able to get all the way back through 2011.  And now I need to trickle out emails the way I'm going to handle it, but from GRC, I think, and just make sure that our reputation stays good.  But anyway, sort of an interesting experience.



The one thing that there is a kind of a call to action that I would ask our listeners, all of those who are listening who own SpinRite and, well, I guess have purchased it ever, but certainly since 2011, check your spam folders and see whether an announcement of SpinRite is in your spam folder.  And, if so, mark it as not spam.  I would appreciate that because that's the way we train the ISPs who are filtering that this is not spam I'm sending.  It's, you know, I got your email address because you bought a copy of SpinRite once upon a time.  So anyway, I would appreciate it if people check their spam folders and let their ISPs know, nope, that's not spam.  I wish I had that in my inbox.  And of course we'll be using those email addresses in the future for other good useful announcements.



And one last point, speaking of SpinRite.  I have seen several people who have said, "Hey, you said that SpinRite 6.1 speeds up SSDs."  One guy sent me the three benchmarks that he had from before running SpinRite.  It was 131 MB/s.  Then the middle of his drive was 184, and the end was 185.  He ran SpinRite on it, and it came out at 120 MB for all three measures.  In other words, from 131 down to 120 at the beginning, and then the ends was 184 and 185, also all now at 120.  



So I replied to him to explain what happened.  I wrote:  "It appears that your SSD was mostly empty.  So what happened is that those 'pre' SpinRite benchmark readings were illusory and were not really returning results from reading from the drive's physical media.  SSDs and spinning SMR (Shingled Magnetic Recording) drives are aware of whether anything has ever been written to individual regions of their media.  If nothing has ever been written, then there's nothing to be read.  So they don't bother actually reading anything, since nothing is there other than blank space, you know, all zeroes or all ones, whatever they initialize to.  So they just return zeroes or ones at lightning speed, at the full speed of the interface that connects the drive to its computer.



"But when SpinRite re-wrote the SSD's entire surface, the drive now believes that all of the media is now 'in use,' even though it may still only be storing all zeroes or ones.  Now the drive believes that data is important to its owner.  So when SpinRite's benchmark is run afterwards, what will be shown is the true reading speed from the media, which was exactly 120 MB/s everywhere.



"Now, after running SpinRite and remounting the SSD in an operating system, the OS itself will 're-trim' the SSD.  It runs through the entire SSD's 'region in use' table, marking all of the regions that are not actually storing any file system data as 'empty' again.  All current operating systems do this periodically, so this will cure itself.  Under Windows this happens weekly for SSDs, although you can run the Windows Disk Optimizer on the drive to cause Windows to do this on demand.  And if you then re-run SpinRite's benchmark on the SSD, you'll see that the results have been returned to what they were before, so you can usefully then compare them with what SpinRite showed for its pre-benchmark results."  So anyway, yeah, isn't that cool?  So the drive's saying 184, 185, it wasn't actually reading anything.  The drive actually reads at 120MB from the media.  But when it knows that nothing was ever written in a region, why bother reading it?



MIKAH:  Meaning, when you say it knows that nothing has ever been written, does that mean it has knowledge of the fact that it is a factory-fresh drive?



STEVE:  Yes.



MIKAH:  Okay.  And so does that suggest, then, that if I got a drive and then plugged it in, and it did not read at those much, not much higher, but higher speeds, then maybe that one was quality control tested or something at the factory?  Or is there just kind of like a, right before it goes out into the world it gets a little blessing that says this is absolutely fresh, it's never been used?



STEVE:  It's exactly the second thing that you said.  So there's a thing known as "trim."  It's called "trimming the drive."  The drive, with a resolution of some level of blocks, some number of sectors, it maintains a bit flag that knows whether any of the sectors have ever been written to that drive.  And it turns out that it's really cool the way this works.  The reason it's done is that the way flash memory, which is the NAND memory, the solid-state memory used in SSDs and in flash drives, you know, thumb drives works, is when you erase the data, all of the bits are set into one state.  Like typically they're all set to ones.  So in a completely empty region, they're all set to ones.  But erasing could only be done in relatively large blocks, the same size as there is a bit for the trim of the block.  So the whole block is written to all ones.



Now, say that you write a sector of data into that block.  Writing the data sets a bunch of those ones to zeroes, but it's not possible to set the zeroes to ones.  That is, only the process of erasing the entire block is able to set all of the bits to ones.  So writing data is essentially like pushing the bits down.  All that can be done is like to push them down to zeroes.  And so if the system knows that regions have never been written to, then it knows there are no zeroes there, so it doesn't need to take the time to erase those.  So it turns out it's a performance-enhancing and wear-minimizing sort of like background management of the data stored on the drive.  And it's completely transparent to the user.  You never see that happening.  You know, you just store your data on the drive, and everything works.



But in the background, Windows is making sure that, like, so you delete a file from the file system.  And, you know, back in the day we know that you could undelete things.  That was what Peter Norton discovered that made him famous because he came out with Unerase or Undelete where you could just, you know, get your file back.  But today's file systems actually release the space.  So they release the space.  But the space, as we know, the file is still there until it's actually like physically rewritten.  Or until Windows says to the SSD on Sunday night, or whenever the so-called "optimization" occurs, Windows will say, oh, by the way, that chunk of region no longer contains any actual data.  It's no longer part of the file system.  And so then the drive takes advantage of that information to know whether or not it needs to preserve it when it's doing its normal work.  So a lot going on behind the scenes that people don't normally take a look at.



MIKAH:  Yeah, that is fascinating to me, honestly.  I didn't realize that it was kind of just, yeah, as you said, sort of you don't to worry about it, but I'm going to worry about it and will make sure that we're not doing more than needs to be done.  If it ain't broke, don't fix it sort of situation.



STEVE:  Right.  Yup.  Well, and my favorite reaction to the weekend's mailing was just a one-liner from someone named Jim.  He said:  "Wow."  He got the email.  He said:  "Talk about turning back time to the days of Madonna and a portable CD player that took 'D' cells."  And he said:  "Love it."  He says:  "I bet if I dig long enough, I might be able to find a copy on floppy of SpinRite from early 1990 or so."  And he said:  "Awesome.  Thanks.  You saved my tail many times in the past."



MIKAH:  Aw.



STEVE:  So anyway, very cool.  And finally, one piece of Closing the Loop feedback.  I mentioned this to you before the podcast, Mikah.  Chris Paetz wrote:  "Just listened to your podcast and heard you say that you won't be able to find your email address anywhere online.  But Perplexity.ai knows where to find it, apparently, FYI."  He said:  "I asked the question 'What is Steve Gibson's email address at Security Now!' and received this reply:  'Steve Gibson's email address for the Security Now! podcast is securitynow@grc.com.  This address is intended for podcast feedback and is mentioned regularly during the show, although it is not prominently displayed on the Gibson Research Corporation website.'"



MIKAH:  Whoa.



STEVE:  And I'll just say that I continue to be surprised by what we're creating here with this what can only be considered an AI revolution.  As we humans continue to push technology further and further, we create both new capabilities and new dilemmas.  You know, we're rapidly moving into a world that was only recently pure science fiction.  So it's going to be truly interesting to see what happens with AI.  But, Mikah, like, these sorts of summaries, where, like, it appears to actually get the context and, like, to understand what's going on, it's creepy.



MIKAH:  Yes.  It gets a little creepy.  And honestly, for me, I remember when we first started seeing the generative AI thing take off.  And I was still in a head space of, I know it's kind of silly, but I would feel bad about challenging the system too much.  I would feel as if I was, you know, making it - I didn't want to give it the opportunity to fail me because I don't like to - this is kind of - but I don't like it when I can't, you know, properly achieve a task.  And so I would be overly descriptive and overly mindful and try to help it get to the right answer that I was looking for because I didn't want to be disappointed, but I also didn't want it to, like, work hard.  I don't know what was going on psychologically, but that's just where I was.  And now I'm very lazy about how I'm asking the things because it does, you know, on the times whenever I'm asking it to...



STEVE:  It appears to understand.



MIKAH:  It appears to understand.  And I think that that's really going to make all the difference when we see more of this playing out in our virtual assistants.  The actor Bella Ramsey has just - Apple just debuted several advertisements featuring Bella Ramsey.  And they are an actor in "The Last of Us."  And they were showing off some of the aspects of a new version of Siri where you could say "Where do I know this person from," and it was able to say, oh, yeah, you met them because I can look at your calendar, I can look at this.  And up to this point, you know, we haven't - you have to do so much of the directing for that to happen.



STEVE:  Yes, yes.



MIKAH:  And so it is really wild seeing how this is just happening automatically in the background.  And then when you use something like Perplexity, because what Perplexity does is it is a search engine that has AI tacked onto it.  And so earlier when we were talking about it, Benito had pointed out that that was probably a big part of that is it has access to the transcripts for Security Now!, which that alone is something that needs to be, you know, if it has access to those transcripts because it does a search on finds the show on the TWiT website and can read through that, it has information that it wouldn't otherwise have.



And another quick thing that I'll say, and then I'll get off this topic, is I was just - you may have seen that Google had updated its NotebookLM system.  NotebookLM is the online tool where you can give it your Google docs or your other Google files.  And then you can say, for example, I would, for example, say "What were the articles that I wrote back in 2015 where I talk about this Smart Home product," because I was doing that at the time.



STEVE:  Wow.



MIKAH:  It looked back at my documents and helped me find those things.  And then I could say "Help me break down a summary of what my thoughts were at the time."  Right?  But here's where it gets wild, Steve, is just recently NotebookLM released a new sort of feature of this where you feed it your documents, and it makes a podcast conversation of the documents that you give it.  So it has two people that are voices that are completely AI-generated, having a conversation about specific documents.



And there was an app developer on, I want to say it was Mastodon.  And I'm sorry I don't have the links to this right off the top because I didn't know I was going to be talking about this.  But basically this developer wanted to see what would happen if they fed it some weird stuff and so just gave it, like, 35 documents, or maybe even was more than that.  They were all just named Patent One, Patent Two, Patent Three, Patent Four, and just had binary in it, all zeroes and ones.  And there was a somewhat compelling fake conversation taking place between these two AI voices about these Patents documents that weren't real.  Mind-blowing stuff.



STEVE:  So, like, the perfect example of an AI hallucination.



MIKAH:  Yes, yes.



STEVE:  Just completely made up out of whole cloth.



MIKAH:  Yes.



STEVE:  This, you know, stuff that sounded absolutely convincing.



MIKAH:  Because there were these little interruptions like we're doing here, where I'm kind of going "yes" and, you know, making those sounds.  It was doing that, too.  And so it was chilling.  It was honestly chilling.  But at the same time it was hilarious because it felt like an SNL parody almost.  And it was - I was feeling a lot of emotions all at once.  So I'm sure that you were feeling a little bit of that when this came up because it's like, what are we building here?



STEVE:  Well, and what you just described about Google LM and the idea that it can look at your document archive, your historical document archive, and be, like, bring search to, I mean, an entirely new meaning to the term "search," this is why Microsoft is so desperate to get Recall into everyone's Windows machine.  They want Windows to be taking a snapshot of everyone's screens every couple seconds, understanding and saving, archiving what's there, for exactly the same purpose.



So future Windows users who do not object to the concept, I mean like the privacy implications of having their machine and Microsoft to some degree having access to the entire, you know, like unredacted contents of all the screens that have been on their machine for years, they're going to be able to pose these sorts of questions.  I mean, so it would be not only Google Docs because those Google Docs were on your screen at some point, and Recall would have taken pictures of them, but everything else that you did with your computer.  And, you know, you want it to be highly confidential, to be just, I mean, just between you and your computer.  But one can imagine the power that it would have.  And it is, I think, terrifying.



MIKAH:  Can you imagine credit card companies getting some sort of proprietary score from your behavior that is generated by - and I know I'm going a little out there.  But at the same time, we've seen with health insurance companies and auto insurance companies who have those little dongles, or you get an Apple watch from them, and then you have to send it your stats, and then over time it can reduce your bill, imagine that same thing applied where your social credit is generated based on how your - think of it like, can you imagine a corporation that has cybersecurity insurance, and it uses a score generated by Recall to see how risky your employees' behavior is on all of your company-owned devices.



STEVE:  Yup.



MIKAH:  Right there, that can mean, you know, 10,000 fewer dollars paid every year because your employees are properly trained, and the behavior plays out.  And then, you know, the in-between, the company that's doing the Recall part can say, we're not going to give the company exact information.  We've generated a proprietary score that says there are 98 out of 100 mice icons, you know what I mean, whatever the score happens to be, that's interesting stuff.



STEVE:  Well, and we've seen and covered on this podcast instances where cars are now feeding back their drivers' driving habit data.



MIKAH:  That's right, yes.



STEVE:  And it's affecting their insurance rates.



MIKAH:  Yup, yup, I remember you talking about that.



STEVE:  So this is not farfetched.



MIKAH:  No.



STEVE:  We know that there will be tremendous pressure to obtain this information.  And Microsoft is all about generating revenue.



MIKAH:  It's a company, folks.



STEVE:  Uh-huh.



MIKAH:  It's all about the money now.



STEVE:  Yeah, we are the product.



MIKAH:  Yes.  Yes, indeed.  Yes, indeed.



STEVE:  Okay.  So today's exploration topic began with my receipt a week ago of a note which read:  "Hi, Steve.  My name is Ben.  I am a former Ben-Gurion University student and long-time listener of Security Now!.  You covered a few studies of mine in the past including:  Lamphone (speech recovery from light bulbs), Video-based Cryptanalysis (key recovery from a power LED), and Morris-II (the AI worm).



"In two recent studies," he says, "that I co-authored and were led by Andres from Cornell Tech, we revealed attacks against end-to-end encrypted applications, demonstrating the recovery of encrypted confidential data from backups of two messaging applications (WhatsApp and Signal), and 10 password managers (LastPass, Dashlane, Zoho Vault, 1Password, Enpass, Roboform, Keeper, NordPass, Proton Pass, and KeePassXC).  We named these attacks 'injection attacks,' and the papers were published on USENIX Sec '24 and Security and Privacy '24.  Attached are the links I believe that your audience will find interesting, as once again they prove that while end-to-end encryption is the best approach for applications, the devil is in the implementation."



Now, of course, I had seen from Ben's note envelope that his full name was Ben Nassi, which I recognized immediately since, as he noted, we've covered all of his work and exploitation discoveries through the years.  So needless to say, I was interested in what new mischief Ben and his fellow security researchers had gotten themselves up to now.



Five or six researchers, depending upon which paper, led by Cornell University's Andres Fabrega, from Cornell University and Cornell Tech, collaborated on two papers.  One was titled "Injection Attacks Against End-to-End Encrypted Applications," and the other was titled "Exploiting Leakage in Password Managers via Injection Attacks."  To get us started on our understanding of what they have done, I'm going to share the Abstract from the paper about Password Manager injection attacks.



It's pretty brief, and it says:  "This work explores injection attacks against password managers.  In this setting, the adversary only controls their own application client, which they use to 'inject' chosen payloads to a victim's client via, for example, sharing credentials with them.  The injections are interleaved with adversarial observations of some form of protected state, such as encrypted vault exports or the network traffic received by the application servers.  From this, the adversary is able to obtain confidential information.



"We uncover a series of general design patterns in popular password managers that lead to vulnerabilities allowing an adversary to efficiently recover passwords, URLs, usernames, and attachments.  We develop general attack templates to exploit these design patterns and experimentally showcase their practical efficacy via analysis of 10 distinct password manager applications.  We disclosed our findings to these vendors, many of which deployed mitigations."



Okay.  So that's interesting.  When they use the term "injection," they're referring to providing an unwitting target some information that the target will cause to be stored in their own instance of their password manager.  And then by examining the target's encrypted vault or their network traffic, presumably synchronizing their encrypted vault with the password manager's cloud backup, they're able to learn as much as the user's secret username, passwords, URLs, and attachments.  So we need to learn more about that.



But I'll first note that the second paper does something somewhat similar with end-to-end messaging, specifically WhatsApp and Signal.  In the case of messaging, an attacker sends messages to a targeted user.  Assuming that the attacker is somehow able to obtain and observe the targeted user's encrypted cloud backup, the researchers were able to demonstrate their ability to determine whether the target had received specific attachments; or, for example, which of two messages the target had previously received.



Now, clever as these researchers are, I came away feeling, I guess, better rather than worse about the safety of WhatsApp and Signal.  My feeling was like, okay, wow.  If that's the most intrusion that these guys were able to achieve given their obviously serious skills, then that says a lot about how good these apps are.  But that said, since the goal is true zero leakage of any kind, I would not be surprised to learn that the app vendors had added something like length fuzzing to the things being stored, specifically to thwart the leakage that these guys were able to induce.  So the research was definitely useful.



MIKAH:  So in other words, what you're saying is because they did this, even though it didn't give much information, it was still worth doing because it resulted in the vendors making changes to even make this small vulnerability even less likely to be effective.



STEVE:  Exactly.



MIKAH:  Okay.



STEVE:  Yeah, certainly the goal is zero leakage.  And we often quote Ben Schneier - not Ben.  I can't believe I've forgotten his name, his first name.  Bruce.  Bruce Schneier.



MIKAH:  Bruce Schneier, oh.



STEVE:  Yeah.  Bruce said, and I love this, attacks never get worse, they only ever get better.  Meaning, you know, I mean, it's obvious in retrospect how attackers are getting more clever, they're never getting dumber or less clever.  So the attacks only ever improve.  So we start with, like, okay, this doesn't seem very bad.  But still.



MIKAH:  The step one, yeah.



STEVE:  Exactly.  It's like the first step in a new vulnerability is that it crashes the system.  The next step is it no longer crashes it, you take it over.



MIKAH:  Yeah.



STEVE:  So, okay.  But the password manager leakages, from what we've seen so far, appear to be somewhat more dire.  So let's take a closer look at them.  These researchers set the stage in their introduction of that paper by writing:  "Password-based authentication suffers from well-known pitfalls" - that is, just password-based, you know, password authentication, using username and password on a website.



They said:  "...such as the fact that users tend to choose passwords that can be easily guessed by attackers.  Password managers are often cited as the default solution to this problem, as users can offload to them the complexities" - thank god - "of password generation, storage, and retrieval.  Indeed, password managers have enjoyed a notable rise in popularity, placing them among the most ubiquitous of security-oriented tools."  I can't believe, I can't imagine that anybody listening to this podcast is not an avid password manager user at this point.



MIKAH:  Right.



STEVE:  So they wrote:  "Password managers have benefited from academic attention, which has helped understand and improve their security along various dimensions.  The attacks uncovered by prior work broadly fall under two general threat models.  First are attacks that use a client-side resource controlled by the adversary, such as a malicious website visited by the client, a rogue application in the victim's device, or the client's own WiFi network.  Second are adversaries that somehow acquire a copy of a user's encrypted vault, and exploit leakage from unencrypted vault metadata or by offline cracking attacks of a user's master password.  State-of-the-art password managers are therefore designed to resist both kinds of threats, and notably use slow cryptographic hashing to prevent cracking attacks for well-chosen master passwords.



"In this work, we consider a new kind of threat model in which an adversary, one, controls their own application client, meaning their own password manager, through which they can send chosen payloads to the victim, for example, via the password sharing feature now found in most modern password managers; and, two, can observe some form of encrypted state and associated metadata, such as the user's encrypted vault backups or network requests received by the application servers.  Borrowing terminology from prior work in other domains, we refer to attacks in this threat model as 'injection attacks.'



"The core idea behind injection attacks," they wrote, "is that the adversary can use injections to trigger subtle interactions in the application logic between their data and target victim data, for example, other passwords used by the target, which are reflected in their observations of ciphertext, for example, inspecting their lengths, and metadata in a way that allows recovered sensitive information.  We argue that this threat model is increasingly important as password managers become more complex and feature-rich" - in other words, you know, more ways for things to go wrong because it's like, oh, let's add this.  What could possibly go wrong?  They said:  "...which provides new avenues for injection mechanisms and vulnerable cross-user interactions.



"To understand whether this threat model is of practical concern or not, we performed a security analysis of 10 popular password managers that support sharing:  LastPass, Dashlane, Zoho Vault, 1Password, Enpass, Roboform, Keeper, NordPass, Proton Pass, and KeePassXC.  Together, these reportedly account for over 30% of all password manager users."  And, okay, that's shocked me.  That seems low.



MIKAH:  I thought so, too, yeah.  Where are the rest of them?



STEVE:  Yeah, I mean, LastPass, though they had that trouble, I thought they were a huge percentage of the total password manager install base.



MIKAH:  Maybe they're Chrome?  Where everybody's got their passwords is Chrome?



STEVE:  Got me.  But that 30%...



MIKAH:  And Apple Keychain I guess would be the most commonplace.



STEVE:  Yeah.  So they said:  "We uncover a series of exploitable vulnerabilities that implicate all of the password managers investigated.  Our first class of attacks exploits the fact that a common feature of password managers is for clients to periodically log outside the device various metrics about the 'health' of a user's vault, such as the number of duplicate passwords.  We show..."



MIKAH:  Oh, good.



STEVE:  What?



MIKAH:  Those are outside of the vault?  That's great.  Love to hear that.



STEVE:  Uh-huh, yeah.  And so there's an example, right, of like what, you know, we're obviously done with the problem of storing usernames and passwords.  What new features can we add?  How can we further enhance this?  And that's where we start getting into trouble, as we'll see.  So they explain:  "We show how an adversary can leverage these benign-looking metrics to perform an efficient binary search-based dictionary attack that recovers the target's saved passwords.  Our attacks do not require the adversary to know additional information about the victim's saved credentials beforehand, for example, URLs nor usernames.



"Five out of the 10 applications are vulnerable to this attack.  In most cases, the adversary must be a passive eavesdropper that observes these metrics directly, for example, by having a persistent foothold in the application servers; while for one application the attack is feasible by a passive network adversary that simply observes the HTTPS channels under which the end-to-end encrypted data is transmitted.  We note that both eavesdropping and network adversaries are within scope for the threat models under which password managers are designed, meaning they weren't supposed to be leaking this information.  And the ubiquity of server-side breaches, combined with the difficulty of detecting such breaches, make it critical that password managers resist, that is, are resistant to such attacks."  So we'll come back to that in a minute.



They said:  "Our second class of attacks exploits another feature of password managers.  Clients often display a small identifying icon, such as a company logo, alongside each of a user's saved credentials.  Importantly, such icons are only fetched once per URL, and subsequent credentials reuse the icon stored in the client.  We show how this fact allows an adversary to perform an efficient dictionary attack on the URLs in a victim's vault.  The attack always succeeds in our experiments, and mounting it requires no additional assumptions about the victim's saved credentials.  Six of our 10 case study applications are vulnerable to this attack, and in all cases exploitation only requires observations by a network adversary," meaning someone able to watch the user's network traffic.



And then, finally:  "We turn our attention," they said, "to adversaries that have an encrypted copy of the entire vault, such as compromising a local password-protected database file or backup of it.  In this case, we analyze the security of KDBX, which is a file format used by many password managers, notably KeePass and its derivatives.  To optimize for storage, KDBX employs a variety of storage-saving techniques, such as file deduplication and compression.



"We show two attacks exploiting these features to recover URLs, usernames, and attachment contents.  Compression and deduplication have led to attacks against other systems before, but our work is the first to show that these types of vulnerabilities also arise in the context of password managers.  Our attacks target features of the underlying file format itself, and thus can potentially be leveraged against any application that uses KDBX.  We implement a proof-of-concept for our attacks in the case of KeePassXC, and experimentally show that its accuracy is sufficiently high to make it a practical threat.



"A summary of our attacks follows.  They exploit common design patterns found in password managers.  And as such, other applications that employ these can be vulnerable to our attacks.  Indeed, for each of our attacks, we describe a general template for it, which is agnostic to lower-level application details, and that can be used to target any application that follows the relevant design pattern.  More broadly, our findings uncover higher level issues in password manager design, and we discuss the future work that will be required to provide generally applicable mitigations for injection attacks."



Okay.  So I want to clarify the nature of these attacks so that the vulnerabilities they found will make more sense.  They identified three ways of attacking the security of today's password managers.  The first class of attacks, which the researchers refer to as "Vault-Health Logging," rely upon the newer features of application-wide metrics.  These arise from the behavior exhibited by many password managers which compute various metrics, like the example they gave, the number of passwords their user has duplicated in their vault, which includes both personal and shared vault entries.



MIKAH:  Would this also include, Steve, the Have I Been Pwned integration that a lot of these password apps have that tell you, oh, yes, your password is part of a vulnerability.  That's another health...



STEVE:  Yes, that's another perfect ability of like a new feature that can be, can inadvertently leak some metadata about what you've got in your vault.  When these metrics are logged outside the device, such as by the application's cloud servers, an adversary - and that means that logging is visible on your network, even if it's encrypted, it still, by the size of it, is visible, then an adversary can induce fluctuations in these metrics with so-called "injections," and observe how they are updated in the external location.  So to carry out these attacks, in addition to having the ability to inject credentials, this attack requires the adversary to have access to the location where the metrics are logged.  Okay.  So, you know, some sort of foothold of some kind is required.



So specifically, LastPass, Dashlane, Zoho Vault, Keeper, and NordPass are those five they mentioned before.  Those are all vulnerable to these vault-health logging attacks because those five offer these features.  All of the password managers except Zoho Vault require that server-side foothold be present.  So, and that's also effective against Zoho Vault, but it's additionally vulnerable, Zoho Vault is additionally vulnerable to simpler passive network attacks.  Okay.  So the vault-health attack can be used successfully against Zoho Vault only when passive networking, or passive network eavesdropping is present.



The second class of attacks is, I guess I would call it, and they refer to it as "URL icon fetching."  Actually, what's interesting, too, we've actually seen this in browser-based attacks where people's web browsers were caching the favicons of the sites they were visiting.  So this is sort of like that.  It arises from the fact that many password managers, as they said, display a graphical icon next to each credential, identifying the website associated with it.  This icon will only be fetched once from the application servers, and future entries for the same website reuse the image from a client-side cache.



An adversary could use this to determine whether or not a target's password manager has previously obtained and cached a credential for that particular URL.  If the attacker induces the target's password manager to get an icon, that means there was no previous entry for that website.  In addition to having a means of updating the target's credentials, probably without credential sharing, this attack requires the attackers to be able to observe the HTTPS request traffic that leaves the victim's client, or to have a foothold in the server from which the icons are fetched.  So the researchers found that Dashlane, 1Password, Enpass, Roboform, Proton Pass, and NordPass were all vulnerable to disclosing their protected URLs through this passive network eavesdropping.



And the third and final class of attacks only affects KeePassXC, that is, among the top 10 that were tested.  This arises from KeePassXC's storage file system.  KeePassXC uses this system to decrease the size of its encrypted vault.  Now, the problem is the presence of data compression - and this is something we've talked about before, in secure storage, which is used obviously to reduce the storage redundancy - can be used to reveal the data that's already been stored in the compressed store.  The size of the storage will increase when unique new information is stored. But if something is added that already exists in the storage, the compression that's present will keep the total storage from increasing.



MIKAH:  Oh.  You can use that, I mean...



STEVE:  And so you can infer, exactly.



MIKAH:  Wow.



STEVE:  You're able to infer what was there based on whether it now occupies as much space, additional space, as the new information would have required.



MIKAH:  Aha.



STEVE:  So isn't that a cool side-channel?



MIKAH:  That is so cool.  And so clever.  These clever, clever people.



STEVE:  Yeah.  So, you know, it's a side-channel that we've talked about in the past.  It's necessary to be very careful about compressing secret data before it's encrypted because this side-channel can be used to leak information about the content of the secret data by inference.  And we know that post-encryption compression is never used because it's not possible to compress encrypted information since anything that's been properly encrypted is indistinguishable from completely random noise, and completely random noise is, by definition, incompressible.



MIKAH:  You know, I needed that, Steve, because I always - the logic to me there never quite lined up in terms of how in the world can I take a thing that exists in this digital space, like why is it not already as small as it can possibly be?  Why does compression work at all?  So the way that you've just described it there helps me to get why compression even works in the first place because, if you do see those patterns, then you can use those patterns to help make it smaller.



STEVE:  Yes.



MIKAH:  I just thought, yeah, why isn't the JPEG even smaller than - why can I take some JPEGs, put them into a ZIP, and the ZIP is smaller?  And now I understand.



STEVE:  Right.  And the ZIP, we talked about this on the podcast, like, forever ago, two researchers who were at IBM, Lempel and Ziv, created something, and their initials are LZ, LZ compression, and that's the compression used in ZIP and a lot of the early compressors.  Basically, as you're feeding data in, the compressor is storing the history of the data that is seen.  And as new data comes in, it looks in the history buffer for any previous occurrence.  So like even, like the word "the," when it sees "the," like it looks in this buffer of past stuff that it has seen uncompressed, and it says, oh, look, that was here.  So instead of putting "the" in, it puts a pointer to where it is in the buffer because a pointer is much shorter than the word.



MIKAH:  Got it, yeah.



STEVE:  So normally when you're talking about something, the context of your conversation is reusing the same words over and over within a short period of time.



MIKAH:  Ditto, ditto, ditto, ditto.



STEVE:  Exactly, ditto, ditto, ditto, ditto.  And so it just aligns all that.  And all it does is put little pointers to where it recently occurred.



MIKAH:  That's cool.



STEVE:  Instead of having to restore it.  It is super clever.  They obtained a patent at the time, but patents only last 17 years.  So it's, you know, everyone's been able to use it afterwards.



Okay.  So finally, the last of the attacks, which is this compression attack, KeePassXC is vulnerable to this type of attack because it does exactly that.  By examining fluctuations in the size of KeePassXC's encrypted vault, after injecting known information, it's possible for an attacker to glean information about the vault's unknown contents.  To pull this off, in addition to injecting credentials, this attack requires the adversary to have persistent access to the victim's encrypted vault, either directly or by monitoring vault backups.



In their work, the researchers demonstrated attacks against two mechanisms, both the compression and attachment deduplication.  So if you gave it an attachment, you would see, if it expanded by the size of the attachment, you knew that it didn't already exist.  If it didn't expand because the vault deduplicates, it was like, aha, that already exists somewhere.  Again, doesn't seem like a big problem.  But it is.  You are leaking some information.



MIKAH:  Right.



STEVE:  So after putting these 10 password managers through the wringer, they contacted each of the password managers' vendors to share their results ahead of taking this work public.  Here's what the researchers reported from that effort.  They wrote:  "We reported our findings to the 10 vendors affected by our work, many of which proceeded to deploy mitigations.  LastPass adopted our suggested mitigation of separating vault-health metrics between personal and shared credentials, which disables the injection channel.  They released an initial implementation of this fix in version 4.129.0, removing shared folders from the vault-health logs, as these lead to the most severe variant of our attack.  Removing individually shared credentials from the logs is more technically challenging - and individual credentials lead to a less practical version of our attack - and thus has been deferred to later in their roadmap.  Their projection is to release this fix by the end of the year, which would complete a full mitigation of our attack.



"Zoho Vault plans to adopt a similar fix by implementing an option to separately compute vault-health metrics on personal passwords as of version 4.0.  Dashlane opted for a partial mitigation instead, namely, increasing their rate limits on the sharing endpoints 'as much as possible.'"  Okay, whatever that means.



MIKAH:  Yeah.



STEVE:  "Given the fact that their vault-health metrics are only logged once per day, their tight sharing limits significantly affect the practicality and runtime of the attack."  In other words, they're updating so seldom already that it would really slow things down.



MIKAH:  You'd have to really be sitting around, yeah, watching constantly.



STEVE:  Yeah, right.  "In addition, the resource limits on their web application and extensions prevent an adversary from sharing an unlimited number of credentials with a victim, which increases the runtime of the attack even more.  As part of their disclosure, they informed us that incorporating shared passwords is a core feature of their vault-health metrics, and thus removing shared passwords would represent a notable disruption to this feature."  So they don't want to stop doing that.  Whereas LastPass said, yeah, we just took it out.



"Then, to address our URL icon fetching attack, Dashlane implemented a new feature as of version 6.2415 that allows users to turn off fetching credential icons, which disables the side-channel for both an eavesdropping and network adversary, and thus provides a full mitigation to our attack.  To address our attack even when URL icons are turned on, they additionally migrated their icon fetching tool to a new endpoint, api.dashlane.com, which is used by multiple parts of their application logic.  As such, this would make it significantly more challenging for a network adversary to use traffic analysis techniques to identify whether an icon fetch request is included in the traffic sent to this top-level endpoint, due to the high amount of noise from other requests sent to the endpoint."



In other words, it used to be that the URLs were being pulled from their own domain.  And so they said, oh, no problem, we'll just add them to an existing domain which is under heavy use already.  So the actual URL icon request will be lost in the noise.  You won't, you know, an attacker who can just see the traffic happening without knowing what it is, because that is after all HTTPS encrypted, they just won't be able to determine.  They'll just give up and go away, even though it wasn't that bad an attack anyway.



They wrote:  "NordPass also adopted our suggested mitigation of separating vault-health metrics between personal and shared credentials, which disables the injection channel and is thus a full mitigation to our attack.  This fix is planned to be deployed by the end of August 2024."  And I guess that's already happened, then.  "Then, to address our URL icon fetching attack, NordPass added a feature to disable URL icons by default" - by default, thank you - "which provides a mechanism to disable the injection side-channel.  This fix is planned to be deployed before the end of 2024.  In addition, they're currently exploring more robust mitigations for this attack, to protect users even when URL icons are turned on.



"To address our attack on attachment deduplication, KeePassXC adopted our suggested mitigation of deduplicating files separately for every shared folder, which disables the injection side-channel.  Then, to address our compression-based attacks, they modified their file format by, every time the database is saved, picking a random length between 64 and 512 bytes, generating a random array of this length, and including this in a 'custom data' field for their file format.  We note that this is only a partial mitigation."  Basically what that is is that's fuzzing.  It's fuzzing the length so that there is no direct correlation between lengths any longer.



So they said:  "We note that this is only a partial mitigation, as an adversary can potentially use statistical techniques to bypass the noise."  Eh, okay, but not really.  "This, however, would require a significantly higher number of injections.  Both fixes were promptly implemented by the KeePassXC team, and have since rolled out as part of version 2.7.



"Enpass already provides support for turning off URL icons, which is off by default" - thank you - "and thus users who disable URL icons are not vulnerable to our attack.  As a first step towards more mitigations, Enpass added an option for organization admins to control this setting via an organization-level policy.  They've decided not to deploy mitigations at this time to address the attack even when URL icons are turned on.



"1Password" - a sponsor of the show as noted earlier - "already provides support for turning off URL icons."



MIKAH:  Which I'm doing right now.



STEVE:  Uh-huh.



MIKAH:  I did not know this.  I'm turning it off.



STEVE:  "And thus users who disable URL icons are not vulnerable to our attack.  However, 1Password decided not to deploy additional mitigations to address the attack at this time, even when URL icons are turned on.  Similarly, Proton Pass already provides support for turning off URL icons, and thus users who disable URL icons are not vulnerable to our attack.  We shared suggestions for how to address this attack even when URL icons are turned on, but do not have details on their plans to deploy these.



"Lastly, Keeper considers our attack on their system a very low severity issue" - and I can't really argue with that - "and opted not to deploy mitigations, and instead have added changing this feature as a consideration for an upcoming platform update.  As part of the communication, they shared that removing transferred credentials from the count of duplicate passwords displayed in the Admin Console would represent a notable disruption to a feature of the business product."



Okay.  So where does this leave us?  I agree with Ben Nassi that this is an interesting and potentially important work.  But it's clearly of more interest theoretically than practically.  As a true threat, it's really out there on the fringe.  It's not impossible that some extremely motivated attacker might somehow arrange to set themselves up in a position to pull off one of these attacks against a specific high-value target, but I would say it is safe to say that none of us listening to this have anything to worry about, even before these obscure holes were plugged.



So the reason I chose to share these attacks on this podcast is for what we learn from them about the true challenges that are associated with truly protecting secret information.  It's so easy for the salesman to boast, "Oh, don't worry, it's all military-grade encrypted.  And guess what!  We're using a bazillion-bit key!  So no one will ever possibly crack that."  Right.



But the lesson taught by these injection attacks is that no one ever needs to crack that bazillion-bit key.  The reason the password managers jumped to modify and improve their systems when they were informed of these subtle issues is that "subtle issues" may be all that's needed to infer the data that's being protected by those bazillion-bit keys.  Any mature and fully informed understanding needs to appreciate that encrypting something is far from being the end of the task.  The encryption is only the start.  Twenty years ago there was a general lack of understanding of the gulf that exists between theoretical and practical "field-ready" security technology.  But during these past 20 years this sort of research, exactly like we've just seen, has opened the eyes of people who are implementing these systems, and we have all benefitted.



So for anyone who may be interested in digging deeper, I've included the links to both of these research papers at the end of the show notes.  And that's it.



MIKAH:  One question I have for you because, if I don't ask it, the chat room will riot.  Do you feel there's an omission that Bitwarden, also a sponsor on the network in the past, was not included as part of these?  Or do you think that Bitwarden was not affected?  Or what do you think about Bitwarden being...



STEVE:  That's a great question.  I absolutely wonder that, why they didn't include Bitwarden in their 10.  I got the sense that, like from reading between the lines, that some of these are more enterprise-oriented solutions.



MIKAH:  Understood.



STEVE:  And so, not that Bitwarden can't be used and shouldn't be used in an enterprise, but that may have skewed their choice for some reason.  But it's a good question.  And what anyone could infer is that, if Bitwarden is being used to, like, pull icons from URLs, we don't know that that's a vulnerability, but it is a side-channel that all of these password managers have turned off or don't have turned on by default.  So it's really a good question.  I have no idea why Bitwarden was not there.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#993

DATE:		September 24, 2024

TITLE:		Kaspersky Exits the U.S.

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-993.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  The case of the exploding pagers and walkie-talkies.  Are Ford Motor Company autos planning to listen in to their occupants?  Highly personal data of 106,316,633 U.S. individuals was found unprotected online.  Passkeys takes a huge step forward with native support in Chrome.  Is there a serious 9.9-level unauthenticated remote code exploit in Linux?  More credit bureau freezing insanity, Drobo vs. Synology, GRC's email adventure, WiFi security with and without a VPN, obtaining CPE credits from listening to Security Now!, and in defense of Microsoft Defender XDR.  Then, what mess did Kaspersky make leaving the U.S. market last week, and what are the wider implications for the Internet's future?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, the man we trust with the information you need to stay safe these days.  We're going to talk about the weird thing that Kaspersky antivirus did on its last day in the United States.  Why you should worry if you're one of 106 million U.S. individuals whose information was, yes, once again leaked online.  And Google takes a big step forward with Passkeys.  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 993, recorded Tuesday, September 24th, 2024:  Kaspersky Exits the U.S.



It's time for Security Now!, the show where we cover the latest security news with this cat over here to my right, Mr. Steven "Tiberius" Gibson, your host at the Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  Hello, Leo.  



LEO:  Good to see you.  We're looking at each other like the Brady Bunch.



STEVE:  That's right.  There he is.  Welcome back.  Mikah held down the fort for you for a couple weeks.



LEO:  Thank you, Mikah.  Thank you, thank you.



STEVE:  We had a good time, learned some things.  I did not want to forget to tell you that one of our pieces of news is that Peter Hamilton is back.



LEO:  Oh, good.



STEVE:  With the first of a duology.  So we're not doing anything until he gets the second one done because...



LEO:  Yes, we've learned that lesson.



STEVE:  Oh, boy.



LEO:  You start the first one, and then it goes, well, it's not over, but you're going to have to wait a year.



STEVE:  Oh, and my god, you get all wound up, and you've got all this back story.  And then a year from now it's like, okay, who is that?  What did he do?  You know, because you just, you know...



LEO:  It's a lot of detail.



STEVE:  We're older.  We forget stuff.



LEO:  And I did hear you talking to Mikah about the Bobiverse and Book 5, which came out.  And I got it just in time for my trip.



STEVE:  Oh, good.  And that's - and he's, in fact, is not generally a sci-fi person, he explained.  But I guess the Bobiverse is so fun and sort of, you know, lightweight that it fits with his whatever he's doing.



LEO:  It's a starter drug for sci-fi.



STEVE:  [Crosstalk] and cooking and stuff, yeah, exactly.  Anyway, probably at the other end of the spectrum is Peter Hamilton, where we know far more than we ever wanted to know about the Commonwealth, which is the background for what happens in "Pandora's Star," and then later all that Dreaming Void business.



LEO:  Are we back in the Commonwealth?  I hope Al Capone's not.



STEVE:  No, this is a whole new deal now.  We're way, way in the future, so I'm sure people are going to be very, you know, altered and all kinds of cool stuff that Hamilton comes up with.  And this is the result of a mass migration exodus from Earth.  And colony ships go out, and one of them finds like the right place and sends a message out to all the other ones that says, hey, we've found the right place.  Everybody come on over here.  But apparently by the time those other ships get there, things are not good.



LEO:  Not right anymore.



STEVE:  Yeah.  And so we don't know what's wrong, but apparently there's like overlords, and people are not happy.



LEO:  Uh-oh.



STEVE:  Anyway, lord knows what he's cooked up.  But I'm excited about it, and I did say, if there are any of our listeners who are willing to, like, get all ramped up and then hit the cliffhanger at the end of Book 1, I'd love to hear what you think without any spoilers.  But I'm not going there until I can read both at once.  And, you know, I think the first book is 993 pages or something.



LEO:  Oh, my gosh.



STEVE:  I know.  Again, it will hold open a door with very strong springs.  So that's what you can count on Peter for.



LEO:  He's our favorite sci-fi author.  We do recommend that if you're going to start with Peter F. Hamilton, you start with the single volume "Falling Dragon"; right?



STEVE:  Yes.



LEO:  That's the easy one.  It's not - it's well, beautifully written.



STEVE:  Yeah.



LEO:  And he's really good at hard sci-fi, but excellent characterizations and stuff.



STEVE:  Oh, and it's got the best surprise ending, too.



LEO:  It's really good.



STEVE:  It's like, ooh.



LEO:  And then you can start getting into the much longer.  The Dreaming Void, not my crazy, my favorite.



STEVE:  No, no.  Especially the last one.  The last one was like, wow, did you promise somebody to do nine massive novels, and you just ran out of stuff after number eight, and so you just said, well, we're going to keep on going.



LEO:  This does happen.  This does happen.



STEVE:  Yeah.



LEO:  So what are we talking - I can guess, but I'm going to let you tell us what we're talking about this week.



STEVE:  Oh, that's right, we're here to do a podcast.  I almost forgot.  This is Security Now! #993.  And our long-time listeners are saying, well, thank god we're not ending at 999 because...



LEO:  Clock is ticking.



STEVE:  ...you know, we're not.  But in six episodes we're going be there.  That's going to be very cool.



LEO:  It's amazing; isn't it?  You know TWiT 1000 is next week.



STEVE:  Wow.



LEO:  Not this Sunday, but a week from Sunday, yeah.



STEVE:  Wow.  Very cool.  And so we're going to find out if your four-digit stuff works.  I think it will because you've always had a leading zero.



LEO:  You know, I must have been prescient.



STEVE:  I was very impressed when I saw that.  I thought, did he really think 20 years ago that he was going to need four digits?



LEO:  I mean, really?



STEVE:  I was thinking that two would be fine.



LEO:  Yeah, yeah.  Always over-ambitious.



STEVE:  Anyway, today's title is "Kaspersky Exits the U.S."  But by popular demand, and I didn't talk about this last week, actually I didn't have enough information to talk about it last week.  But by popular demand I want to talk about the technology and the supply chain aspects of the very powerful event that happened involving exploding pagers and walkie-talkies.



LEO:  Wow, what a story.



STEVE:  Also we've got the question, is Ford Motor Company planning to listen in on their occupants?  Some of their recent patent filings would suggest that.  But I want to help put that into perspective.  We've got the highly personal data of 106 million-plus individuals having been found unprotected online.  So again, another NPD-style breach.  Big news for Passkeys, making a huge step forward in the industry.  And is there a serious 9.9 level unauthenticated remote code exploit in Linux?  Probably is, and we're going to find out in two weeks.  But we'll share what's known today.



Also we've got more credit bureau freezing insanity, a little bit of question from our listeners about Drobo vs. Synology, an update on my email adventure, a question about WiFi security with and without a VPN, obtaining CPE credits from listening to Security Now!, as many of our listeners do, and a defense for Microsoft's own Defender XDR endpoint security.  We've been talking about CrowdStrike.  One of our listeners says, hey, XDR is worth looking at, too.  And then, what mess did Kaspersky make...



LEO:  Oh, what a mess.



STEVE:  ...leaving the U.S. market last week, and what are the wider implications for the Internet's future?  So I think a lot of interesting stuff for us to look at and talk about this week.  And of course we've got, as always, a great Picture of the Week.



LEO:  I have it ready.  I have it ready for you, Steve.  And of course among the most important things that you do, Steve, the Picture of the Week.



STEVE:  So I gave this one the caption, "What event could have prompted the addition of this sign?"



LEO:  "Parking available in empty spaces only."  Wow,



STEVE:  Because what you really want to do is to try to park in an occupied space.  I, uh, yeah.



LEO:  Ay ai ai is right.  I don't know.



STEVE:  Now, I have had some feedback from our listeners who saw this already.  My morning's mailing to just shy of 9,700 Security Now! listeners went out over the course of 30 minutes this morning, and a couple of them said, you know, it looks like this is kind of a park or something.  I mean, you can kind of see behind the sign? 



LEO:  Oh, you could park like on the grass.



STEVE:  There are some non-marked, like some areas which are not spaces.  And so maybe people were kind of parking on the lawn or whatever.  And I actually think that was probably likely.



LEO:  Never put it past them.



STEVE:  I thought this was funny, nonetheless.  And so, you know, we do try for a little humor for our Picture of the Week when we can get it.  And in fact we're going to need some humor.



LEO:  Uh-oh.



STEVE:  Talking about our first topic - which is not at all funny, I'm just saying to balance this topic - many of our listeners wrote last week to say, Steve, I hope you're not going to shy away from the topic of the exploding pagers, which happened in the possession of predominantly, largely, members of Hezbollah last week.  Actually it was last Tuesday.  As I said, shortly after the news of these exploding pagers broke, I started receiving notes from our listeners saying that they were looking forward to my talking about this today.



And as always, what most interests me, and what is also the proper focus of this podcast, is the technology and the facts, to the degree that either are known, behind what happened.  You know, this is not a podcast that will examine or comment upon the politics or the morality, even, of what occurred.  You know, that's not what we're here for.  And we've had hot topics occur in GRC's newsgroups and forums from time to time.  When that happens, I note that there are ample other places on the Internet for such discussion, if that is what one is seeking.  But that's not here, you know, this is about technology.



So I spent some time coming up to speed about what was known.  And I had the advantage of having been able to wait nearly a week for the dust to settle, almost literally, and for the various news reporting and investigative bodies to dig into the back stories and report their findings.  As it happens, I found exactly the sort of background and technical coverage that's appropriate for this podcast over on the CryptoMuseum.com site, of all places.  Here's what they wrote, and they dedicated a page to this event.



They said:  "On September 17th, 2024, thousands of pagers of the Lebanese terrorist organization Hezbollah exploded more or less simultaneously.  Around 5,000 pagers had been obtained by Hezbollah shortly before the incident, 4,000 of which exploded that day, after receiving a specially crafted message.  In the incident, at least 12 people were killed and around 2,750 were injured.  A day later, more than 400 handheld radios (walkie-talkies) used by Hezbollah also exploded.  Although there was no direct proof," they wrote, "it was widely speculated that Israeli services were behind the attack.



"The AR-924 pager from the Taiwanese manufacturer Gold Apollo is intended for use on local infrastructure in the 450-470 MHz UHF radio band, and does not depend upon the public switched telephone network.  Hezbollah used these pagers for security reasons, as they were apparently afraid that their communications via public networks could be intercepted or cut off.  It seemed," they wrote, "that the pagers had been manipulated somewhere in the supply chain between Taiwan and Lebanon, or that a special fake company had been set up by an intelligence service to supply manipulated devices to Hezbollah.



"Experts believe that Israeli intelligence services managed to manipulate the firmware and added a small plastic explosive device.  The Taiwanese manufacturer, Gold Apollo, denied that the devices were supplied by them, and suggested that they might have been supplied by a Hungarian company, BAC Consulting.  BAC had purchased the production rights and the use of the brand name for certain regions, and later produced their own pagers under the Apollo brand.



"A specially crafted message was sent to the newly purchased devices, which triggered a small plastic explosive device that was hidden inside its enclosure.  According to the German newspaper Welt, the explosive RDX had been integrated into the batteries.  In addition, the markers that are normally present in plastic explosives to reveal them in an X-ray scan were said to have been omitted.  Other sources report that the explosive known as PETN was used.  RDX and PETN are the main ingredients of the plastic explosive Semtex.



"According to the Taiwanese manufacturer of the pagers, their Hungarian license holder, BAC Consulting, could be involved in the rigging of the devices.  The company was founded on May 5th, 2022 and reported 2023 annual income of 549,000 euros.  It's possible that it was a shell company, created especially for the purpose of selling rigged equipment to certain parties.



"A day after the incident, a representative of the Hungarian President Viktor Orban, told the press that the pagers had never actually been in Hungary, and that BAC merely acted as an intermediary.  When reporters visited the company at its registered address, no BAC representative was available for comment.  At the address, a modest office building in the outskirts of Budapest, several other unrelated companies were housed there, and no one had seen the BAC director since the pager attacks of September 17th.  She had reportedly been placed under the protection of the Hungarian security services.



"In addition, the Bulgarian authorities started an investigation into a company that they thought might have facilitated the sale of the pagers to Hezbollah.  Although the name of that company was not disclosed, Bulgarian media revealed it was Norta Global Ltd. in Sofia, Bulgaria.



"A day after the incident with the Apollo pagers, on September 18th, a similar thing happened to the two-way handheld radios that were also used by Hezbollah.  In this case it involved the IC-V82 handheld radio, a 20-year-old model from the Japanese manufacturer ICOM.  The IC-V82 works in the VHF amateur radio band, which ranges from 144 to 146 MHz, optionally 136 through 174.  The ICOM IC-V82 is a straightforward two-way radio of the kind that are also used by Amateur Radio Operators.  It was discontinued in 2014 and should no longer be available on the market.  ICOM stressed that the devices had not been supplied by them.  It's known, however, that counterfeit IC-V82 radios, not manufactured by ICOM, are widely available.  Counterfeit radios are commonly produced in China and are difficult to distinguish from real ones.  They're available from electronics stores in Asia and come with 'original' packaging.



"In the case of the exploding IC-V82s, it was not the battery that exploded, but their front top.  Apparently an explosive device had been placed inside the radio, close to the microphone speaker, which is the part that's closest to the face when the radio is operated.  This suggests that it was the intention to cause maximum, potentially lethal, harm to the user.



"In Bulgaria, an investigation has been launched into a company in Sofia that might have been involved in the supply of the counterfeit IC-V82 radios from Asia to Hezbollah.  This is the same company, Norta Global Ltd., that might have been involved in supplying the AR-924 pagers.



"The number of radios, around 450, is smaller than the number of exploded pagers, which was around 4,000.  But since the radios are physically larger, they carried more explosives and were therefore more damaging.  It's currently unclear how and when the handheld radios were manipulated, and how they were triggered remotely.  But," they wrote, "we can make a few educated guesses.  The radio features CTCSS and DTCS, two techniques to selectively open the radio's noise-canceling 'squelch' system using analog or digital tones.  It's possible to fit an optional DTMF touchtone coder/decoder which can be used to activate the pager function of the device, by sending it a three-digit DTMF digit sequence user ID.  It's likely that a unique combination of the above techniques was used to trigger their synchronized detonation."



Okay.  So we have an example of a seriously well planned, well coordinated, and breathtakingly real-world physical supply chain attack.  The article said authoritatively, and I've seen it in several other places, that it was 5,000 pagers that had been recently ordered and received.  There's been no exact reporting of the elapsed time between the 5,000-pager order and their delivery.  But if Israel was somehow able - well, first of all, if Israel was behind this, and if whoever was behind it was somehow able to do this without extensive pre-order preparation, then it's even more impressive.



Given the evidence of this attack's sophistication, and what we know of the time that would be required to implement and test a fully functional pager incorporating specialized firmware with secret code recognition to trigger a custom detonator, probably along with an additional power transistor to supply the current for the detonation, which would require a custom circuit board, there's no way this could have been done overnight, or even close to overnight.  If it were, it would be quite astonishing.



The Gold Apollo SR-924 is a ruggedized device with a rechargeable 85-day battery life.  As such, it is particularly well-suited for rough use in the field.  I suspect, and this is entirely my conjecture, it's more likely that Hezbollah's choice of pager was known ahead of time, you know, from previous contact with them in the field, since this pager model has been available for years.  That would have given someone, presumably someone with ties to Israel, time to replace the original guts of thousands of these devices with their own.  At which point they would have been standing by and patiently waiting for Hezbollah to place their order.  And the same must have been true for the next day's handheld radio attack.  You know, this is another of those situations where we're ever unlikely to have all the facts, since those who have all the facts stand nothing to gain by leaking anything more.



In some other reporting by Vox, Charles Lister, a senior fellow at the Middle East Institute, was quoted, saying:  "What we've seen over the past two months shows that Israel and its intelligence apparatus have completely infiltrated the most sensitive echelons of the entire Axis of Resistance."  Charles' reference to the Axis of Resistance is the informal name for Iran's network of proxy militias throughout the Middle East.



He continued:  "It was only a year ago that the reputation of Israel's intelligence services took a major hit with their failure to anticipate the October 7th attacks, despite abundant signs that Hamas was preparing for a major operation.  It's worth noting that while the operations in Lebanon and Iran were likely carried out by Mossad, Israel's foreign intelligence service, Israeli-occupied Gaza is the responsibility of the Shin Bet, the domestic security service.  The Shin Bet official responsible for Southern Israel and Gaza resigned over that failure, as have two senior military intelligence officials.  While October 7th damaged the reputation of Israel's vaunted spy services, they have now restored that notion of deterrence based on fear and the notion that Israel has eyes everywhere."



LEO:  Yeah, no kidding.



STEVE:  I've left a link in the show notes to the page that I found at CryptoMuseum.com for anyone who's interested in learning more, though that's pretty much everything that they had to report.



LEO:  I think it's important to kind of emphasize that this must have been years in the making, and that they are, you know, this isn't the kind of thing that's going to happen all the time to us, to other people.  This must have been - this is a major effort that may have been as long as a decade in the making.  And so you're not going to see this, I hope we're not going to see this happening all the time because that would make everybody very nervous about their devices.



STEVE:  Yeah.  It was interesting that they managed to get a hold of plastic explosive that was specifically lacking the markers that would have made this obvious in any sort of an X-ray.  And, you know, this is, from a technology standpoint, custom firmware would have been necessary.



LEO:  Right.



STEVE:  Which would have meant that you had to have the source code for the original device's firmware, or maybe suck it out and reverse engineer it, and then edit it in order to incorporate secret code triggering.  You also, you know how software goes, you never want this to go off by mistake.



LEO:  Right.



STEVE:  That would be a disaster.  You would, for one thing, you would tip your hand.  If one blew up, then nobody would, you know...



LEO:  These people were carrying around...



STEVE:  They'd throw all the rest of them in the river.



LEO:  They were carrying explosives in their pocket for months; right?



STEVE:  Yes.  



LEO:  And unwittingly.  I mean, it's kind of a, I mean, I don't want to admire it too much, but with some grudging admiration for what an amazing operation this was.



STEVE:  And it is an example of a classic supply chain attack because, you know, these essentially off-the-shelf pagers were purchased by a specific organization.



LEO:  Middleman; right.



STEVE:  Yes.  And the pagers specifically sent to that specific organization were intercepted and swapped out.  And so, you know, people will say, yes, but there were innocent people who were also hurt.  And that is absolutely true.  And that's [crosstalk] consequence...



LEO:  Including children, at least several children, yeah.



STEVE:  Yes.  As the unavoidable consequence of the bluntness of this.



LEO:  Right. 



STEVE:  Because, you know, you're not in a position where you're looking at your enemy through a sniper scope.  You're, I mean, and I'm sure at some level Israel and Israeli intelligence services were holding their breath that they would get the outcome they were seeking, which was a highly targeted event.



LEO:  Well, remember that the CIA at one point was accused of, and I think it was true, of sending booby-trapped cigars to Fidel Castro, at least they thought about - thought about doing that.



STEVE:  It made a great story, even if it wasn't true.



LEO:  Yeah.  You know, this is the kind of spycraft you read about in novels or in movies.



STEVE:  Yes.



LEO:  The fact that this worked, and worked so effectively, is...



STEVE:  Yeah, nobody wants to call this a spectacular success.



LEO:  No, because it's horrible.  



STEVE:  Yes.



LEO:  But you have to have some grudging admiration for the ability to pull this off.  And the real point of it, as with all terrorist acts, is to terrorize.  And to make people say, hey, I wonder if my pager's okay.  I wonder am I safe.  Do they know where I am?



STEVE:  And there now is reporting that the upper echelon of Hezbollah is no longer using any technology, that they are having to meet face to face.  And so that significantly crippled their communications infrastructure.



LEO:  Right, slowed them down.  And that's why they were using pagers because they decided that phones were compromised, and they couldn't use those anymore.  So you get the feeling this had been planned for many years, over a long period of time.



STEVE:  Yeah.



LEO:  Very interesting story.



STEVE:  So the headline was "Ford seeks patent for tech that listens to driver conversations to serve ads."  And at that point you just turn the car off and get out.  But okay.  The Record, the publication The Record, published a piece two weeks ago that I did not have the chance to get to until now.  But it's too important for us to miss in this podcast.  And I've got some backpedaling to do after we lay the foundation for this.  The Record's headline makes it very clear where we're maybe headed, reading, as I said, "Ford seeks patent for tech that listens to driver conversations to serve ads."  Apparently they want to listen in on the conversations being held inside the car in order to present advertisements on the system's entertainment system.



So The Record says this.  They write:  "Ford Motor Company is seeking a patent for technology that would allow it to tailor in-car advertising by listening to conversations among vehicle occupants, as well as by analyzing a car's historical location and other data, according to a patent application published late last month.  The patent application says:  'In one example, the controller may monitor user dialogue to detect when individuals are in a conversation.  The conversations can be parsed for keywords or phrases that may indicate where the occupants are traveling to.'



"The tech, labeled as 'in-vehicle advertisement presentation,' will determine where a car is located, how fast it is traveling, what type of road it is driving on, and whether it is in traffic.  It will also predict routes, speeds, and destinations to customize ads to drivers, the application said.  The system could pull audio from 'audio signals within' - this is from their patent application - 'audio signals within the vehicle and/or historical user data, selecting a number of the advertisements to present to the user during the trip.'  By monitoring dialogue between vehicle occupants, the ad controller system can determine whether to deliver audio versus video ads, providing ads to drivers as they travel 'through a human-machine interface of the vehicle,' the patent application said."



Okay.  So hold on a second.  The ads can be audio.  So, what?  Your car interrupts during a pause in the conversation to helpfully comment with something like:  "Excuse me, but I heard you mentioning that you were hungry.  And we know from your past travels that you like burgers.  There happens to be a highly rated burger joint just around the corner.  If you're interested, take a left at the signal."  You know, wow.  If that's the case, I doubt that I'm ready for this brave new world.



The article continues:  "'Such systems and methods provide maximum opportunity for ad-based monetization,' said the patent application.  'These systems and methods may use knowledge of vehicle destination prediction to provide more relevant advertisements, for example, if a user is going grocery shopping, merchandise purchasing, et cetera.'



"The patent application does not describe how the collected data would be protected.  The technology would be primarily software-based and would require no new hardware, according to the application.  Ford filed the application in February, and it was published on August 29th.  Contents of the application were first reported by Motor1.com.  Ford has since defended the patent application with a Ford spokesperson saying:  'Submitting patent applications is a normal part of any strong business as the patent process protects new ideas and helps us build a robust portfolio of intellectual property.'"



Now, that is certainly true.  Many patents are defensive and are primarily meant to beef up a portfolio for mutual agreements among competing manufacturers within an industry.  Ford's statement continued, saying:  "The ideas described within a patent application should not be viewed as an indication of our business or product plans."  And in a follow-up statement, Ford said:  "It will always put the customer first in the decision-making behind the development and marketing of new products and services."  Okay, so there's hope.



"The system could cull data," the article says, "from third-party applications or set up screen input preferences to predict the number of ads a driver should be served.  The types of trips being made by drivers also will play a role, the application said, noting that whether a vehicle owner is making a 'long drive versus a trip to medical care facility' would be considered by the system."  That's right, because what we really want is our car reporting to our insurance companies that we've been spending a lot of time at medical facilities.



And speaking of tattletales, a Ford patent filed in July proposed technology that would enable vehicles to monitor the speed of nearby cars, photograph them, and send the information to police.  Not surprisingly, the idea sparked a backlash from privacy advocates, but that kind of thing is no concern to the U.S. Patent and Trademark Office.  That's not what they focus on.  The application pointed to how difficult it is for police to pinpoint speeding cars and said:  "It is desirable to provide systems and methods that assist traffic police and/or other law enforcement officers performing such tasks."  And let's not forget that Ford quietly backed away from another controversial patent application last October after a firestorm of criticism over its plans for a system that would commandeer vehicles whose owners were late to pay and allow the cars to repossess themselves.



That patent application said that the technology would allow self-driving cars to automatically head to repossession lots, while standard vehicle lenders would be able to permanently lock cars and cripple steering wheels, brakes, and air conditioning in order to pressure delinquent drivers into paying.  So you really have to wonder what they're thinking.  If nothing else, the communications available thanks to the Internet means that if this ever happened just once to someone anywhere, it would make headlines, and you'd have to imagine that Ford sales would crash overnight.



But I want to reiterate and reinforce the truth about what that Ford spokesperson said about patents not necessarily implying future product plans.  That is really true.  While patents certainly can indicate a company's future direction, they do not necessarily do so.  For a massive company like Ford, that has an entire division of in-house patent attorneys, their job is to emit patents more or less continuously.  So they'll have members of their patent squad regularly attending product planning and brainstorming meetings, taking notes, and turning random comments into patents.



Some random employee may have quipped at some point during a brainstorming meeting that, once they've got the self-driving technology figured out, it would be possible to eliminate the need for tow-truck repossession by having their cars start themselves in the middle of the night and drive themselves to the local repo lot.  While everyone was laughing at that idea, the weenie from the patent department was taking notes to get that idea captured and filed.  It's like, whoa, that's a great idea.  Let's patent it.  So again, it's a far cry from actually suggesting that that's what Ford's cars are ever going to actually do.



LEO:  Yeah.  I think it's really important to say that.  On MacBreak Weekly we are regularly challenged with the idea of talking about the patents that Apple has filed.  And often they're not, you know, with any intent to release a product, but just for a variety of reasons.  You could even, in fact Ford should probably say this, make the case that, well, we're doing this defensively so that if anybody tries to do it, we can stop them, saying no, we have the patent on that, and you'd better not.



STEVE:  And nobody should.



LEO:  And nobody should, yeah.  I mean, seriously.  Although I have to point out, you remember when Apple was in the car business briefly, $10 billion later, that a lot of what the consideration was was for autonomous driverless vehicles, where you're in a living room, and how they could turn that living room basically into a mobile ad platform.  And you can see that probably Ford's thinking along those lines, as are so many companies.



STEVE:  And while I was doing a little bit of research on this, Leo, it turns out that YouTube is planning to show ads when you pause the videos.



LEO:  Exactly.  Oh, yeah, they do now, yeah.



STEVE:  And so is Hulu.



LEO:  It's a popup.  It's just a popup that says, hey, you paused it.  Why don't you go get a Dr. Pepper?  Ads everywhere.  Remember the Philip K. Dick short story where ads were everywhere?  We thought at the time, oh, that'll never happen.



STEVE:  Oh, or any of the sort of dark sci-fi that you see.



LEO:  Sure, "Blade Runner."  Ads on every surface, yeah.



STEVE:  Yes.  Holograms are jumping out at you, soliciting, you know, everything.



LEO:  Exactly.



STEVE:  We are headed there, Leo.



LEO:  Speaking of breaches.  There's another one.



STEVE:  Just when you thought it might be safe to unfreeze your credit reporting.



LEO:  Never.  Never.



STEVE:  I'm joking.  I know you know that it will never again...



LEO:  Never.



STEVE:  ...be safe for anyone to unfreeze their credit reporting.



LEO:  That's true.



STEVE:  You know, that ship has unfortunately sailed and sunk. Following the National Public Data Breach, it's difficult to imagine that anything could be worse.  And I don't know that yesterday's news is worse because there's no evidence that the bad guys got their hands on this latest trove of treasure.  But it was publicly exposed, unencrypted and unprotected by any password, for some length of time.  So we don't know.  Yesterday, Cybernews' headline was:  "One-third of the U.S. population's background info is now public."



LEO:  Oh, geez.  Good news is they already knew all this stuff from the last breach; right?



STEVE:  That's right.



LEO:  So I'm not going to get too worried about it.



STEVE:  So they wrote:  "MC2" - that's the name of the company.  "MC2 Data and similar companies run public records and background check services.  These services gather, compile, and analyze data from a wide range of public sources, including criminal records, employment history, family data, and contact details."



LEO:  They're data brokers; right?



STEVE:  Yes, exactly, yes.  "They use this information to create comprehensive profiles that employers, landlords, and others rely on for decision-making and risk management.  Websites that MC2 Data operates include PrivateRecords.net, PrivateReports, PeopleSearcher, ThePeopleSearchers, and PeopleSearchUSA."



They wrote:  "Despite dealing with staggering amounts of sensitive data, it is not always kept secure.  Cybernews research reveals that the company left a database with 2.2TB of people's data passwordless and easily accessible to anyone on the Internet.  What was likely to be a human error exposed 106,316,633 individual records containing private information about U.S. citizens" - thus one third, since we have about 330 million people here - "which raises serious concerns about privacy and safety.  Estimates suggest that at least 100 million individuals were affected by this massive data leak.



"The people and the organizations using MC2's background check service were also exposed, as the data of 2,319,873 users who subscribed to MC2's data services was also leaked."  So we know who was pulling all this data, and all the data that was available to be pulled.  "The leaked data included names, email addresses, IP addresses, user agents, encrypted passwords, partial payment information, home addresses, dates of birth, phone numbers, property records, legal records, family, relatives, neighbors' data, and employment history."



Now, the good news is I looked over the data, and I didn't see any reference to social security numbers.  But on the other hand, that's already out there from NPD; right?  So, okay.  There is, however, plenty of personal data that goes far beyond what the NPD breach covered.  And this is the problem that we keep seeing with the so-called "Big Data breaches."  This latest MC2 leak data could be merged with the NPD breach data to create an ever bigger, more comprehensive database.  So basically, you know, lock your data, freeze your credit, and just stay home because it's bad out there.  I don't know.  Anyway, for what it's worth, there were no social security numbers.  But still, this could be merged into an even bigger breach.



Okay.  This next piece of news is significant.  Last Thursday Google Chrome's project manager blogged under the headline that I thought was under-hyped.  The headline just said "Sync Passkeys securely across your devices."  Okay.  That really doesn't say what I would think Google should be saying, because we're talking about having the world's leading web browser, you know, leading by a large margin, and presumably other Chromium-based browsers, too, now natively supporting Passkeys, built into the browser.  Here's what Google posted under that under-hyping headline.



They said:  "In addition to Android devices, you can now save Passkeys to Google Password Manager on desktop.  Signing into your favorite sites and apps on any device should be as quick and easy as unlocking your phone.  That's where Passkeys come in.  They're safer than passwords and easier to use, letting you use your fingerprint, face, or screen lock to securely sign into apps and websites, moving us one step closer to a passwordless future.



"Until now, you could only save Passkeys to Google Password Manager on Android.  You could use them on other devices, but you'd need to scan a QR code using your Android device, you know, using it as the authenticator.  Today, we're rolling out" - this was written last Thursday; right?  "Today we're rolling out updates that make it even easier to use Passkeys across your devices.  You can now save Passkeys to Google Password Manager from Windows, macOS, Linux, and Android, and under ChromeOS which is currently available for testing in Beta.  Once saved, they'll automatically sync across all your devices" - meaning all your instances of Chrome, right, where you're signed in - "making signing in to other websites as easy as scanning your fingerprint.



"To let you create Passkeys and access saved ones across your devices, we're introducing a new Google Password Manager PIN.  This PIN adds an additional layer of security" - and I'm glad for it - "to ensure your Passkeys are end-to-end encrypted and cannot be accessed by anyone, not even Google.  When you start using Passkeys on a new device, you'll need to know either your Google Password Manager PIN, or the screen lock for your Android device.  These recovery factors will allow you to securely access your saved Passkeys and sync new ones across your computers and Android devices.



"You can set up a six-digit PIN by default, or select 'PIN options' to create a longer alphanumeric PIN.  You can already create Passkeys for popular sites and apps, such as Google, Amazon, PayPal, and WhatsApp.  And since Google Password Manager is conveniently built into Chrome and Android devices, you can get started today, without having to download any additional apps."  So it is built into your browser.  



That "started today" was a link to passwords.google.  So anybody who's interested can open Chrome and head over to passwords.google to enable, configure, and start using Chrome's built-in Passkeys solution.  And I did note that, with the addition of a PIN, which can be a passphrase, that is the missing feature which SQRL always had which enabled you to securely use essentially the same kind of public key authentication, which I built, you know, which SQRL was built around, securely on your desktop.



LEO:  Oh, Steve.  Poor Steve.  There's just constant reminders of how it was done right at one time.



STEVE:  Yeah, yeah, well.



LEO:  The market didn't listen.



STEVE:  But Leo, to have Passkeys in Chrome, that's huge.



LEO:  Yeah.  I see.



STEVE:  So now wherever you're using Chrome, you'll have synchronized Passkeys protecting it.  Adding an additional layer of a PIN or a passphrase, you know, stronger than a PIN, makes sense.  It sounds like it's used probably in some time-consuming, I'm sure, in a time- and processor-intensive hashing to create a key which then decrypts your store of Passkeys so it's, you know, you need to provide that PIN to decrypt it in order to authenticate, so that's good.



LEO:  Of course it's a complete lock-in to Google Chrome.



STEVE:  It is.  So unfortunately that is the nature of Passkeys at this point.  It is not, you know, platform agnostic.



LEO:  You're going to be locked into something.



STEVE:  Yeah.  They don't want it to be.  So, okay.  Now, news of what appears to be a potentially serious (as in 9.9) Linux unauthenticated remote code execution vulnerability just broke.  It was sent to me this morning by a listener of ours, Alessandro Riccardi.  So thank you, Alessandro.



The researcher behind this is not someone we've encountered before, so I spent some time doing a bit of background checking, and he's clearly the real deal.  His name is Simone Margaritelli.  He's based in Rome, Italy and uses the handle "evilsocket."  But he's not evil.  He has a presence on LinkedIn with more than 500 connections with many projects under his evilsocket handle on GitHub.  His Twitter handle is, of course, @evilsocket; and since 2009 he's posted more than 15,000 - yes, look at that web page, Leo, the guy has got five pages of links to his stuff there.  It's really impressive.  He's posted more than 15,000 times under Twitter and has accumulated more than 42,000 followers.  His site is www.evilsocket.net, and he uses the glider from Conway's Game Of Life as his icon.



LEO:  Oh, good catch.  Wow.  That is impressive that you saw that.



STEVE:  You know that, yeah, this guy knows what's going on.  And I wrote in the show notes, "Looking over his five pages of projects indexed on his site, although he's been somewhat less prolific the past few years, as I said, this guy is clearly the real deal."



Now, I went to the trouble of doing this bit of vetting because of the potential significance of the claims he's making in his still-not-public responsible disclosure.  Here's what he just posted, and why it might matter.  He leads with six bullet points.  "Unauthenticated," meaning you don't have to provide a username and password, "unauthenticated RCE," so we know that's remote code execution, "versus all GNU/Linux systems, plus others" - and I should note that that also includes BSDs, so the Unixes - "disclosed three weeks ago," he wrote.  "Full disclosure happening in less than two weeks (as agreed with devs).  Still no CVE assigned."



He said:  "There should be at least three, possibly four, ideally six."  He said:  "Still no working fix."  Oh, and Leo, if you want to scroll down two pages, I've got a link that you could put up on the show of this.  "Still no working fix."  He said:  "Canonical, Red Hat, and others have confirmed the severity, a 9.9," and he said:  "Check screenshot."  He said:  "Devs are still arguing about whether or not some of the issues have a security impact."



Okay.  Then he wrote:  "I've spent the last three weeks of my sabbatical working full-time on this research, reporting, coordination and so on, with the sole purpose of helping, and pretty much only got patronized because the devs just can't accept that their code is crap."  He said:  "Responsible disclosure no more."



LEO:  Uh-oh.



STEVE:  And there was another link, but I'll explain why I don't have anything more there in a second.  He said:  "The write-up is going to be fun, not just for the technical details of it, not just because this RCE was there for more than a decade, but as a freaking example on how NOT to handle disclosures."



He said:  "Like, I write software.  I get it.  I get how someone can be defensive about the stuff they write.  I really do.  But holy sh*t, if your software has been running on everything for the last 20 years, you have a freaking responsibility to own and fix your bugs instead of using your energies to explain to the poor bastard that reported them how wrong he is, even though he's literally giving you proof of concept after proof of concept, and systematically proving your assumptions about your own software wrong at every turn.  This is just insane."



LEO:  This guy is Italian; isn't he.



STEVE:  He says - yeah.  He said:  "Just wanted to add for the sake of clarity that I have *so much respect* for the people at Canonical that have been trying to help and mediate from the beginning."



LEO:  The Ubuntu people.



STEVE:  "I really don't know how they manage to keep their cool like this.  This is going to be the write-up opening statement. It's an actual comment from the GitHub conversation.  I mean, it's not wrong...  And he said:  "And yes, I LOVE hyping the sh*t out of this stuff because apparently sensationalism is the only language that forces these people to fix."



Okay.  Now at this point we don't know more.  We do know that an unauthenticated RCE requires something to be listening on the Linux end and accepting packets.  It's impossible to say more than that without more information.  So we don't know, for instance, what percentage of Linux systems might be vulnerable, nor if not all, or why not.  The fact that there's some controversy about this with some distro devs apparently disagreeing should give us pause and should tamp down any panic.  Perhaps the exploitation of this requires, you know, the moon to be in a certain phase.  We just don't know.



Annoyingly, his Twitter feed is locked so I've been unable to view the various clues he's dropped.  However, I've been able to view the comments and reactions to his postings made by people whose feed is not locked, because they follow him.  I applied to follow him.  I don't know if I've been permitted yet.  I've been busy.  Anyway, so the comments in that thread are things such as:  "Probably and luckily the first to point it out publicly, but not the first that exploited it.  It's sad."  Or "Please don't disclose on a Friday.  Preferably on Tuesday.  I like my weekends."  Someone else said:  "Please don't rush this.  'All Linux systems' is a gigantic and diverse attack surface, and the vulnerability sounds trivial in hindsight, making it almost impossible to fix without telling the world about it."  Yikes.  But again, we don't know.



Another comment:  "Any observed active exploitation?  A vuln impacting all Linux distros with a low attack complexity going unnoticed for a decade is highly unlikely."  Also, "Seen both sides of this," wrote someone.  "Working on some unrelated disclosures at the moment, but it's taking a LONG time to keep all sides happy.  Thankfully other times, fixes, and CVEs have been confirmed in the blink of an eye.  Don't let one bad one put you off."  Someone else said:  "Also, all Linux systems and others?  Does that mean Android and BSD?"  And somebody else replied:  "Says elsewhere in the thread BSD is included."  And finally:  "Not to piss anyone off, but I have seen far too many high CVEs that just turn out to be a fringe," or "the devs don't agree with me so they are dumb."  



Anyway, as I said, I've put in my request to follow him.  If he accepts that request, I'll be able to see more of what he's shown his followers.  And in two weeks we'll apparently know more.  Either way, this will be interesting.  So stay tuned.  Maybe a big deal, maybe not.



LEO:  Yeah.  It sounds like he's credible.  But it is a lot to say.



STEVE:  It is, yeah.



LEO:  And that's a big claim.



STEVE:  I mean, you know, and some of these comments, like, you know, low attack complexity, trivial to execute, like, you know, your microwave is vulnerable, who knows.  Anyway.



LEO:  As long as my coffee machine's not vulnerable, I'm okay.



STEVE:  Don't mess with my coffee machine.



LEO:  Don't mess with that baby.  Let's close the loop, shall we?



STEVE:  Let's do it.



LEO:  All right.



STEVE:  So Stephane, he said:  "Hi, Steve.  I just wanted to post some feedback in regards to credit freeze.  I'm not sure why, but credit bureaus need to be forced by law by local government (provincial here in Canada) to allow us to freeze our credit.  I tried in Ontario, and there was no way for me at the moment."



LEO:  Wow.



STEVE:  I know.



LEO:  It's federal law in the U.S., but I guess not in Canada.



STEVE:  Yeah.  He said:  "Under the previous party they had started trying to implement it.  But since we changed from liberal to conservative, this law is now in limbo.  Could you share this feedback to have your listeners contact their local provincial MP to try and force the change?  Or if anyone knows where I could go to force the credit bureaus to change this without being forced by law, that would be great."  He says:  "It is my credit."



LEO:  Yeah.



STEVE:  "I should not be held hostage by credit bureaus.  Thanks, Stephane."



LEO:  Badrod in our Discord says Quebec is the only province in Canada now with credit freezes.



STEVE:  Wow.  So it is province by province.



LEO:  Yeah, apparently.



STEVE:  Wow.



LEO:  But that's why it's so good.  It was the same in the U.S. until they made that federal law.  And that's the way to do it.



STEVE:  Was it state by state?



LEO:  It was state by state.



STEVE:  Ah.



LEO:  And the reason that was problematic is that in some states, I think Maine it cost a lot of money to unfreeze your credit.  It was different amounts of money for every state.  And the federal law in the U.S. made it they have to offer a freeze and an unfreeze at no cost.  And all the credit bureaus do that by federal law.  You've got to do that in Canada, too.  That's shameful.



STEVE:  And as we know, it ought to be locked by default, and then you ought to...



LEO:  I agree.



STEVE:  ...selectively unlock it for specific lenders.



LEO:  We're not there yet, but that I agree.



STEVE:  When you want to permit access.  Yeah.  And something like this NPD breach is like, holy crap, I mean, it's no longer difficult.



LEO:  It tells you why you need this.  Yes.



STEVE:  Wow.  Okay.  And while we're on the topic of credit bureaus, a person wanting some anonymity said:  "Steve and Leo, I'm a software engineer in the fintech industry and have been an avid listener of Security Now! since I started my programming career in 2005.  Thanks to the podcast, I've had a frozen credit report ever since the topic was first introduced.  After the National Public Data breach, I persuaded my girlfriend to freeze her credit, as well; but we encountered a horrifying issue.



"When we created a new account for her on Experian and logged in, we discovered that her newly created account was linked to someone else's profile."



LEO:  Oh, no.



STEVE:  He said:  "That's right.  We had full access to another customer's credit history."



LEO:  Oh, my goodness.



STEVE:  "The signup" - get this, Leo.  "The signup process requires your first and last name, some address details, and part of your social security number."



LEO:  That's right.



STEVE:  "However, Experian seemed to match only the first three letters of my girlfriend's first name and the last four digits of her social security number."



LEO:  Oh, wow.



STEVE:  "This caused her account to be matched with another woman who had a similar first name (though spelled differently by four characters), the last same four SSN digits, and lived in the same state, Michigan.  But aside from these details, everything else was different - different last name, different previous addresses, and so on.  After hours of frustrating calls with Experian support, where several agents insisted this wasn't possible..."



LEO:  Of course not.  That couldn't possibly happen.



STEVE:  "...we could still view the other woman's entire credit profile."



LEO:  Oh, my god.



STEVE:  "Eventually, Experian reset my girlfriend's account, and on the second attempt the signup process completed correctly with the proper information."



LEO:  Wow.



STEVE:  "Thank you and Leo for all the incredibly valuable knowledge you have provided me over the years.  I was an early Astaro adopter, if that helps date how long I've been listening.  Thank you."



LEO:  Yeah, since the early days.



STEVE:  And Leo, wow, what a mess.  You know, thinking about this, I suppose the use of only a few characters of the person's first name - well, or last name, really - might make sense if matching against spelling variations was a problem.



LEO:  Right.



STEVE:  But what could be the possible reason for not matching against the individual's entire social security number?  The user needs to know it, and the credit bureau obviously knows it.  So why not require a complete match?  You know, what, are they afraid that the user cannot enter the entire thing correctly?  I'm at a loss to understand what twisted numbskull logic could suggest that only providing "the last four," as if like it has to be a secret from the credit bureau.  You know, that's why you do the last four and mask the others, right, is like you're wanting to prove that you know, but you're not wanting to share it.  But when you're creating an account with them to access your credit, you absolutely want to prove to them that you know your entire social security number.  It's insane.



LEO:  Yeah.  Yeah.  It doesn't surprise me in the least.



STEVE:  Wow.



LEO:  Unfortunately.



STEVE:  And these are the people who are happily giving away our credit data to anybody who asks.  Right.  Nile Davis said:  "Hello, Steve.  I've been a loyal listener since #1, and I love that you're going beyond 999.  I have a Drobo 5N that I got many years ago..."



LEO:  Oh, I'm sorry.



STEVE:  "...after hearing that you had one, too.  It seems to still work great.  I took my drives out and reran SpinRite 6.1 on each of them without a hitch.  I got a Synology DS1522+ a year ago, and I love it.  I know that with Drobo being out of business, the question that I have is this:  Should I still trust my Drobo, that's upgraded to the latest firmware, or should I just ditch it and get another Synology?  Thanks for all you and Leo do.  Take care, Nile."



Okay.  So I'm in a very similar situation, Nile.  My first experience with consumer-grade NAS was the Drobo 5N.  Since it was working well, when my wife and I set up a second nest for ourselves seven years ago, I purchased another identical Drobo 5N for that location.  Then my original Drobo 5N died.  It had given me many years of service, but something in it went south.  I tried another power supply, swapping drives, and doing everything I could think of, but it refused to behave.  I'd been hearing about Synology, and we knew that Drobo would be on the chopping block.  So even though I could still have obtained a replacement Drobo 5N from the supply chain, I decided it was time to switch to Synology.  And all I can say about that is that I have never been happier.



LEO:  I'd agree.  I had a little Drobo Mini which I loved.



STEVE:  Yup.



LEO:  But, you know, if you're just using it as a USB drive, that seems like that's probably harmless.



STEVE:  And if it's entirely behind your NAT, so it's not publicly exposed at all...



LEO:  Right, right.



STEVE:  ...then that would be a factor, too.



LEO:  But the "N" is a NAS, and it's intended to be sitting on your network.



STEVE:  And it's on mine, but not exposed.  And I've got all kinds of extra security stuff that your typical consumer doesn't have.



LEO:  Yes.  As long as it's not visible to the outside world, it's probably okay; right?



STEVE:  So, yeah.  So my feeling was the Drobo was fine for a non-power user who's happy with fewer options.  But that's not me.  That's not you, Leo.  And after I set up my four-drive Synology to replace the original Drobo 5N which had died, I purchased another identical Synology for my second location.  So now that location has the original second Drobo 5N, which is still going fine, and a Synology.



LEO:  Well, now you're fine.



STEVE:  The reason I have two is that my wife, who has a lot of letters after her name, some of which are PhD, has her doctorate in applied psychophysiology.  Seven years ago she asked me whether there was a way we could set up her clients with a laptop and a two-channel EEG amplifier to facilitate "at home" neurofeedback training.  As with all forms of real-time biofeedback, neurofeedback is the process of showing a client some aspect of their brain's function that would be better if it were changed.  And amazingly, it's possible to effect such change just by showing them what's wrong.  So I found a fantastic two-channel EEG amplifier from, of all places, Bulgaria, and we purchased a fleet of inexpensive recycled Dell laptops from Amazon.  I'm sharing this backstory because all of those widely distributed laptops are running instances of Syncthing which are synced to that second Drobo 5N...



LEO:  That is cool, Steve.  That's really cool.



STEVE:  It's still going strong, yup, and it is being used to keep all of the therapy that those laptops are doing synchronized with home base.



LEO:  And presumably backs up to the Synology or some other reliable drive.



STEVE:  Yes.  Yes.



LEO:  Right.  You wouldn't want it to be your only source.



STEVE:  So Lorrie is able to look at a local drive, a drive that's local to her, thanks to Syncthing, and look at all of the logs of the remote sessions that her clients are doing wherever they are.



LEO:  That's neat.



STEVE:  So it's very - she's even able to, like, to tweak the therapy that a given person is doing, and those settings get propagated thanks to Syncthing to their laptop the next time they boot it up, and they're automatically using updated therapy settings.  So it's really unique and cool.  The point is, you know, so Nile asked:  "Should I still trust my Drobo, or should I just ditch it and get another Synology?"  I'm still trusting that original Drobo 5N, and I'm hoping it continues purring away until my wife decides she no longer wants to offer this form of remote therapy.  If that second Drobo also throws in the towel while its laptop synchronization services are still required, then I'll move its sync functions over to Synology, which, again, I love.  I'm so happy with my Synologies.  But if I ever have to do that, it would be a pain.  So I'm just hoping that the Drobo continues to purr away.  I don't ask it to do anything new.  I just say, you just keep what you're doing and let's hope you last longer than my wife's therapy practice.



LEO:  Yeah.  People, I mean, if you use hard drives, you don't often say, oh, I hope this hard drive manufacturer doesn't go out of business.  I mean, if you think of it as a USB drive, that's fine then; right?



STEVE:  Yeah, exactly.



LEO:  As long as you back it up.



STEVE:  You back it up, and Syncthing's technology is very secure.



LEO:  It's so - it's encrypted in flight; yeah.



STEVE:  It is end-to-end encrypted.  And, boy, you know, it took me a while to, like, kind of really get how much work it was doing for me.  I was like, this has to be harder than this.  But it's actually not.  You just say, here, synchronize these things.  And it goes, okay, got it.



LEO:  And it's surprisingly fast.  I mean, it's, wow, I just changed that file, and it's already on the other drive.



STEVE:  Yes.  I abandoned Synology's dual Synology synchronization because, if I made a bunch of changes, it sent the entire drive across again.  You know, because I monitor my bandwidth.  And it was like, what the heck just happened?  You know, and it was like multi terabytes going from point to point. It's like, this is dumb.  So I shut that down.  Now I just use Syncthing, which is running natively on both of my Synologies.  So, yeah, it's a win.



LEO:  Yeah, there's a community distro of Syncthing.



STEVE:  Yes, exactly what I'm using, yup.  So Danny in sunny Scotland, he said:  "FYI, last week's Security Now! email was routed to my Junk folder."



LEO:  Uh-oh.



STEVE:  Uh-huh.  "I'm using Apple Mail,"  he says, "(and I have my own domain set up on iCloud).  It was fine until last week, so I guess something must have spooked their filters."  And I know what.  "In any case, I marked it as 'not spam.'  Hopefully their filters will get the message, so to speak.  All the best from sunny Scotland, signed Dan."



LEO:  It's sunny this year.  Wow.  Good for you.



STEVE:  He's making a point of that, yeah.  So last week, as our listeners may remember, I asked them, any who discovered their weekly Security Now! email going to spam to please mark it "not spam."  Many listeners like Dan noted that they had done so.  So I wanted to thank everyone for that.  What I've learned so far through this emailing adventure is that just as with code signing, the earned reputation of the signer is everything.  GRC, and I'm signing all my email cryptographically, the source as GRC is unspoofable.  And we've been using email for our business for decades, and we have enjoyed a spotless reputation since we never have and obviously never would actually send spam.



But GRC's reputation is now being challenged because for the past several weeks I've been slowly sending out email to GRC's past SpinRite 6.0 purchasers to notify them that they can have 6.1 at no cost.  And I'm careful not to use the word "free" because that raises...



LEO:  Or Viagra.  Those words.



STEVE:  I can't put any exclamation points in the email, either.



LEO:  No.



STEVE:  That's bad.  So, and in fact I actually saw one spam filter, I said "You are invited to download," and it's like, oh, you can't invite anybody, either.



LEO:  Unh-unh, unh-unh.



STEVE:  So it's like, wow.  That's what the world has come to.



LEO:  Isn't it sad?  Yeah.



STEVE:  But anyway, I've been slowly sending this out.  And I suppose that from the standpoint, you know, it would technically be classified as UCE, right, Unsolicited Commercial Email.  Except that I'm trying to give away an upgrade that I and many others spent three and a half years working to create.  So I'm not profiting from this.  But anyway, it's not email from a Nigerian prince, and every email address I'm using is what SpinRite's purchaser used at the time to receive a purchase receipt from us.



However, it is also true that I have not bothered any of those people until now, and those addresses date back as far as 2004.  And Leo, when I saw this I knew you'd get a kick out of it.  There are addresses with CompuServe account numbers in the list.  You know, they're like 76294.3276@compuserve.com.  So anyway, I've been mailing in reverse order, from the most recent toward the least recent, and I've now progressed as far back as 2008, all the way through 2008, so to the start of 2008.  At this point more than 10% of the addresses are bouncing.  Overall, I have to say it's going better than I had hoped.



But when a major ISP like Apple or Google or Microsoft sees GRC sending to many nonexistent addresses, they will quite reasonably decide to not bother their current users with email coming in from the same source that is going to a valid address, so they route it into the users' spam or junk folders.  There's nothing I can do about that.  It's not possible to check people's email addresses ahead of time.  I actually ran the list through something called Email Hippo in the U.K., which cut out about 20%, I think.  But I'm working through the rest.



Anyway, so the good news is this will be a one-time transient problem which we should be on the other side of in a few weeks.  Once that's happened, I will have a much smaller, but updated and cleaned list that I'll be able to use going forward with much less trouble.  Until then, I need to ask for everyone's patience.  It would help greatly if anyone who again or continues to discover their weekly Security Now! email in their spam or junk folder, if they'd mark it as "not spam," that would be great because that is the most effective way of training the ISP's spam filters that GRC is not and never has actually sent out spam, even if we are attempting to contact some of our very old purchasers.



LEO:  I will look in my spam inbox to see if I had anything.  By the way, before the show began I said, hey, I don't see your show notes, because normally they go into my "important" folder.  I realized why, and I don't know...



STEVE:  Oh.



LEO:  Because at the bottom of your email it says - there's an unsubscribe link because it's a newsletter.



STEVE:  Yeah.



LEO:  And I have a filter that whenever it sees an unsubscribe link, puts the newsletter into a mailing - it doesn't kill it, but it puts it into a mailing list folder.



STEVE:  Oh, oh, oh, doesn't - ah.



LEO:  Because I go, well, that must be a mailing list, and I don't want to have it fill up my inbox.  So I did find your email in my mailing list.  I can whitelist you so that that doesn't happen in the future.



STEVE:  Well, thank you.  And I am getting so much good feedback from our listeners who love the fact that they get this email from me every week.



LEO:  So good.



STEVE:  It's turned out to be a big win.



LEO:  Yeah.



STEVE:  And I can, you know, you mentioned the unsubscribe link.  It was very clear to me that many ISPs are clicking that link on behalf of themselves.



LEO:  Gmail does that, yeah.



STEVE:  When they see email coming into a nonexistent address.



LEO:  That's right.



STEVE:  They go, oh, and they unsubscribe, and that's wonderful.  There's nothing I would want more than for email to be unsubscribed when I send to a nonexistent address.



LEO:  That's a good point, yeah.



STEVE:  That is the perfect solution.



LEO:  Yeah.



STEVE:  And of course everything I send has a big, prominent "Unsubscribe."  And in fact I was just reading, too, Leo, get this.  The threshold for spam is astonishingly low.  Google's formal policy, and a service that I used briefly, Postmark, they want it to be 0.1%, which is to say one in a thousand spam complaints is the most you're able to have.



LEO:  Wow.



STEVE:  And if you go over one in a thousand using Postmark as your mailer - I'm not doing it.  I've gone back to using GRC.com.  But they will stop you.  They will freeze your mailing at that point and say what's the problem here.  Google says the same thing, but they don't have a policy exception until .3.  The problem is this policy was just implemented earlier this year, announced last October.  But most business-to-business mass mailings are around 3%, so 10 times the policy violation that Google has set, and 30 times the spam level that people are supposed to keep things under.  As I said, we're normally at zero, and we've always been at zero.  But, you know, we're getting people who click.  And in fact, I've been annoyed by this Gmail UI.  When I'm looking at my Gmail, and I click a bunch of things, I have to be very careful not to mismark things as spam.



LEO:  Yeah.  You'll never see it again.



STEVE:  Because Google makes it very simple to do that.



LEO:  Yeah, yeah.  It's gone forever now.



STEVE:  Yeah.  Exactly.  And I don't want to do that because some things I do want to keep receiving and not have them disappear into my spam folder.



LEO:  Right, right.



STEVE:  So anyway...



LEO:  But isn't it funny?  I would have thought, here we are in the year 2024, that spam would have been conquered by now.  And in fact it's worse than ever.  You don't ever see it because everybody's so aggressively blocking it.  But it's more traffic than ever before; isn't it.



STEVE:  And remember in the early days, Leo, of the podcast, Mark Thompson and I were working on - there was some, we had some heuristic filtering thing.



LEO:  I remember it, yeah.



STEVE:  Of course, well, and this came up because Dvorak's comment...



LEO:  "I get no spam."



STEVE:  "I get no spam."



LEO:  He gets spam now, I'm sure.  I don't know how anyone could not get spam.  But thanks to all of these efforts, we maybe don't see as much of it as we did in the past.  But we also, as a result, don't see a lot of email from Mom, either.  So I don't know.



STEVE:  Actually I had a conversation with John just the other day because someone had said to him, he had been using Mailchimp for his podcast stuff.



LEO:  Right, right.



STEVE:  And he was not happy with, you know, I guess the lack of absolute control over it.  And so someone had said, oh, you know, Gibson's using this great system, you know, and so he shot me a note, and we talked on the phone for about an hour.  And I just want to say again, this nuevoMailer that I found, I am - with all the stuff, all the rigmarole I've been going through, for example, I realized that .me, .mac, and .icloud were all being blocked because I was sending from SpinRite.news, which was a domain with zero reputation, and Apple just said, who the heck are you?  No.  And just, you know, so it's not that I misbehaved, it's just that they'd never seen me before.



And I guess of course that's what spammers are going to do; right?  They're going to create, if they damage the reputation of a given domain, they'll just create a new one and start spamming from there.  So Apple has a "deny first" policy.  Anyway, so I had to, like, filter out all of the .me, .icloud, and .mac domains and create a separate list which then I sent through GRC.com, which Apple loved.  Not so much now, but they did.



And anyway, all of the stuff I was doing, this thing is a workstation for working with email lists and sending.  It's nuevoMailer, and this Greek guy, I think he only got like 132 bucks from me one time.  It's in PHP.  So he and I have been sending code back and forth to each other because I'm sort of in a unique position of having this massive bouncing of the email that I'm sending out, so I'm able to look at and work with him on, like, improving his recognition of the reason for things bouncing.  Anyway, I cannot recommend this highly enough.  I said the same thing to Dvorak.  I said, if you've got anybody around who's a PHP developer, you need to be able to run PHP on your server, and you can use a third party to actually send your mail through.  It will connect beautifully so you don't need your own SMTP server.



Anyway, it's just - it is a fantastic facility.  So I just, you know, I like to tell people when I find a good sci-fi author or a piece of software like, you know, like the email archiver that I love also.  Anyway, for email, this is it.  So, you know, it won't do everything that you see from GRC because I wrote all of my own frontend.  But, boy, for backend sending of email - and you can do subscriber and double opt-in and all that stuff.  I just didn't want to use his because I like to do things, you know, my way.  But wow, it's really great.



Okay.  Steve P., and I must have given him some anonymity because I think he told me his whole name, but I thought he wouldn't want me to share it.  He said:  "Hi, Steve."  Oh, yeah.  He said:  "Hi, Steve.  I'd appreciate your latest thinking on the safety, or otherwise, of connecting to public WiFi.  I'm currently" - oh, I know why, it was because of medical things.  He said:  "I'm currently 'enjoying,'" he has in quotes, "an extended stay in hospital, and as with most public places here in the UK, they're offering 'free WiFi.'  However, unlike most, the network here does NOT require a password to connect.  You're briefly taken to a portal and then granted Internet access.



"I seem to remember long ago you touching on this subject on Security Now!, but I'm uncertain if this type of public WiFi network with no password is a 'risk too far.'  I'm using an iPhone and iPad here, typically via the Personal Hotspot on the iPhone, but this can be restrictive.  So I'd like to use the faster free WiFi offered here, but only if it's safe.  I've briefly connected to the free network, but I'm still uneasy about this.



"I'm also using a VPN (ExpressVPN) to connect.  If it's generally a bad idea to connect to public WiFi with no password, does a VPN mitigate the risks somewhat?  The ceiling-mounted WiFi access points appear to be branded Cisco, but of course I have no way of knowing how this is set up in the background, i.e., client isolation.  Anyway, thanks for all you do.  6.1 continues to work well, of course.  Regards, Steve P.," he said, "currently an in-patient at St. Thomas's Hospital, London, although hopefully not for much longer."



Okay.  So the short version is that the use of any high-quality VPN system, such as ExpressVPN...



LEO:  Our sponsor, which we should mention.



STEVE:  Oh, are they still a sponsor?  Cool.



LEO:  Oh, yeah.



STEVE:  ...completely encrypts all traffic inside of the VPN's tunnel.  It's only decrypted as it emerges onto the Internet at the VPN provider's servers.  There is no better or more complete protection available for shielding one's traffic as it passes through a WiFi hotspot, whether open or password protected.  The big upside to the use of a VPN provider is the convenience of being able to use their always-present servers.  The only possible downside to the use of any big provider is that once the tunneled traffic emerges onto the public Internet, it is visible to everyone.



And, you know, there's always been speculation, but I don't think really any evidence, that such places are where national intelligence services might be sniffing around since overall it could be expected that traffic emerging or going into a VPN service might be more interesting than just random packets on the Internet.  But who knows?  So one possible improvement would be to run one's own VPN server at home or office.  In that case, not being any big and well-known VPN service, there would be less chance of, you know, generic traffic capture.  On the other hand, if someone was interested in your traffic specifically, that's where they'd be looking for it.  So you kind of can't win.



And for the sake of completeness, what about the case of no VPN whatsoever in an open public WiFi hotspot?  Things are definitely 100% better these days than they were back in the earlier days of this podcast, podcast #272, recorded October 27th, 2010, titled "Firesheep."  Fourteen years ago, the simple unencrypted HTTP protocol was still dominant, and connections typically only switched to secure HTTPS when credentials were actively being exchanged.  But these days, 14 years later, all connections are always encrypted.  This makes it far safer to use any open public WiFi hotspot without a VPN than it once was.



Now, it's true that DNS queries are probably not being encrypted, so it would be possible for someone to eavesdrop on your DNS lookups.  But the IP addresses you're visiting could also not be hidden, even if what you do there at the location of the remote IP would be solidly and well encrypted.  So I suppose I'd say today, in a pinch, using open WiFi is not super high risk, though it's not ultra private.  And if you have access to a VPN or overlay network, there's no better time to use it.  So ExpressVPN, since our listener Steve P. has it, absolutely use it.



LEO:  Keep using it, yeah.



STEVE:  And you should use it without, you know, without any further concern.



LEO:  Point of order, Mr. Gibson.



STEVE:  Yeah.



LEO:  So what he described is a captive portal.  In other words, and you've seen, we've all seen this, you go, you join a WiFi network at an airport, and you have to go through a login page.



STEVE:  Right.



LEO:  Technically obviously different than password protect, WPA2 password protected.  But is it in fact less or more secure?  I don't think so.



STEVE:  I would say it's less secure because all it's doing is intercepting your initial attempt to get out, and requiring you to click OK to agree to the terms of service.



LEO:  Right.



STEVE:  Which are, you know, you're holding us harmless for anything that happens while you're using our WiFi.  So click YES if you want to; otherwise, sorry.



LEO:  If you're, though, in a hospital, and they have the password on the wall, and you log into a password-protected network, that's effectively exactly the same; right?



STEVE:  Correct.



LEO:  The issue is can someone, some bad guy get onto the network to see your traffic?  They could easily get through a captive portal.  A password, if they didn't know it, they couldn't get through.



STEVE:  Right.



LEO:  But on the other hand, if it's in a public place, the password is usually publicly posted.



STEVE:  And if a bad guy got a hold of the traffic on the other side of the access point, it's all of the traffic.



LEO:  It's unencrypted anyway.



STEVE:  I mean, it's regular Ethernet wired traffic.



LEO:  Right.



STEVE:  Which you can easily sniff.



LEO:  So a captive portal isn't necessarily better or worse than a password.  It's just a different way of doing it.



STEVE:  Right.  And what you really want is what Steve P. has access to, which is a VPN.



LEO:  A VPN; right.  And one of the reasons people often don't want to use a VPN at home is they're also trying to protect themselves from their ISP.



STEVE:  Yes.



LEO:  So if you use your home VPN, sure, you're private until you get home, and then everything is...



STEVE:  Exactly right.  That's exactly right.  Richard Anthony, CISSP, wrote:  "Steve, those of us who use Security Now! for CPE credits for certs like CISSP and CEH need to post proof when we submit credits.  Many of us grab a screenshot at the end of a video podcast for proof.  We use the end of the video to show that we watched the whole thing.  I've been doing this for many years now.  I do this on my iPad and show the date/time I watched, along with the episode number.  Would there be any way for Leo or his team to show the episode number on the screen during the last few minutes of the podcast?  It would be a terrific help to all of us who rely upon Security Now! to maintain our certifications.  Many thanks, Richard Anthony, CISSP."



LEO:  Huh.



STEVE:  And so I have no idea whether that could be done.  But Leo...



LEO:  Oh, it could easily be done.  I'm trying to think of how we would institutionalize that.



STEVE:  Just sort of bring it up toward the end while we're doing the wrap-up.



LEO:  Yeah.  I usually - yeah, like that.



STEVE:  If that's there, that's all we need.



LEO:  I have, in the past, when I was doing it in a studio, I would do that in the last segment of every show because it would have the title in there, and it would be the title topic.  These no longer have the topics on there anywhere.



STEVE:  No, but I think the number and the date probably...



LEO:  All they need is the date and number, yeah.



STEVE:  Absolutely sufficient.



LEO:  And don't forget, over my shoulder there's always the date and time, if you need a date and timestamp.  That's actually one of the reasons that clock is always there.



STEVE:  Nice.



LEO:  People have asked for that.  Yeah, we'll make sure to - I used to always do that.  I didn't know that's why I was doing it, but it turned out that was a benefit.



STEVE:  Cool.



LEO:  So Benito, just remember that, or whoever is in charge.



BENITO:  Copy that.



LEO:  How do I - how could I do that, Anthony?  Is there a way I could stick a - it's in the captions?  Let me go to the generic...



BENITO:  Yeah, [crosstalk] to the captions, and you can hide or unhide that lower third.



LEO:  Oh, okay.  Good, all right.  So if Benito's not around, which he always is...



STEVE:  Don't go away, Benito.



LEO:  If for some reason Benito's not around, I will endeavor to remember that at the end of the show.



STEVE:  Nice.  Nice.



LEO:  Which is not over.



STEVE:  And from another listener, Richard Cornell, who is an IT Security Manager in the UK.  He says:  "Hi, Steve.  A few times in recent SN episodes you've referred to Windows Defender when discussing CrowdStrike.  You are correct in saying the free Windows Defender product is nowhere near as feature-rich as the alternative enterprise products.  However, if you purchase a Microsoft 365 enterprise license such as M365 E5, you get Microsoft Defender XDR, which is every bit as capable as CrowdStrike and the alternatives.  Like all these products, it's not perfect.  But we've used it for a number of years, and it is just as good with the advantage that it's part of the integrated Microsoft stack.  Keep up the good work, and onwards to 999 and beyond."



So thank you, Richard.  I appreciate hearing from someone who has experience with Microsoft's high-end enterprise solutions.  And as a user of the simple free Windows Defender, I'll certainly admit to a bias towards native solutions as opposed to installing extra stuff outside of the box.



LEO:  Yeah.



STEVE:  So that's good to hear.  We hadn't heard anybody.  We were hearing people saying, yeah, I'm sticking with CrowdStrike because it saved our butts a bunch of times.  So even though they screwed up, we're not moving.  And here's somebody who says, yeah, you know, Microsoft Defender XDR works just great, too.



LEO:  I think one of the reasons CrowdStrike had such support was because they had so many sensors, distributed so widely globally.  But I would imagine XDR has to be the same number, or at least; right?



STEVE:  Yeah, yeah.



LEO:  Yeah.  Interesting.



STEVE:  Okay.  Our last break, and then we're going to talk about Kaspersky's somewhat ignoble exit from the U.S.



LEO:  Yes.



STEVE:  And a little bit about what I think that says about where we're headed in the future.



LEO:  Subtitled "How to Mishandle Your Transition Out of a Country."  Steve is not an AI.  I can vouch personally for that.  Unless there are AIs that particularly like Cabernet Sauvignon.



STEVE:  And there could be.



LEO:  There might be.  All right.  Let's get onto this Kaspersky thing here.



STEVE:  Yeah.  So I started off treating today's main topic as just another news item to which I'd given the title "How to mishandle an antivirus handoff."  And I'm going to still start with that because what transpired last Thursday is still news.  But this is also the perfect segue for addressing what I think is the much bigger and broader issue of what it means that Kaspersky has been kicked out of the U.S., and what this and similar moves mean for our future global technology landscape.



So let's start with BleepingComputer's headline, which read "Kaspersky deletes itself, installs UltraAV antivirus without warning."  So ask yourself what you would think if something completely new and totally unknown to you suddenly appeared in your computer.  And when you went to check on it using the AV system you had purchased and installed, that AV solution was nowhere to be found.  Talk about mishandling a transition.  So here's what BleepingComputer reported.



They said:  "Starting Thursday, Russian cybersecurity company Kaspersky deleted its antimalware software from customers' computers across the United States, automatically replacing it with UltraAV's antivirus solution.  This comes after Kaspersky decided to shut down its U.S. operations" - not like it had any choice - "shut down its U.S. operations and lay off U.S.-based employees in response to the U.S. government, in June, adding Kaspersky to the Entity List, a catalog of 'foreign individuals, companies, and organizations deemed a national security concern.'



"On June 20th, citing potential national security risks, the Biden administration announced a ban on sales and software updates for Kaspersky AV software in the United States beginning September 29th, 2024."  And of course we covered that event when it happened.



"In July, Kaspersky told BleepingComputer that it would begin closing its businesses and laying off the staff on July 20th because of the sales and distribution ban.  In early September, Kaspersky also emailed customers, assuring them they would continue receiving 'reliable cybersecurity protection' from UltraAV, owned by Pango Group, after Kaspersky stopped selling software and updates for U.S. customers.  However, those emails failed to inform users that Kaspersky's products would be abruptly deleted from their computers and replaced with UltraAV without warning.  According to many online customer reports, including BleepingComputer's forums, UltraAV's software was installed on their computers without any prior notification, but with many concerned that their devices had been infected with malware.



"One user wrote:  'I woke up and saw this new AV system on my desktop, and I tried opening Kaspersky, but it was gone.  So I had to look up what happened because I was literally having a mini heart attack that my desktop somehow had a virus which had somehow uninstalled Kaspersky.'



"To make matters worse, while some users could uninstall UltraAV using the software's uninstaller, those who tried removing it using uninstall apps saw it reinstalled after a reboot, causing further concerns about a potential malware infection.  Some also found UltraVPN installed, likely because they had a Kaspersky VPN subscription.



"Not much is known about UltraAV besides being part of Pango Group, which controls multiple VPN brands - Hotspot Shield, UltraVPN, and Betternet - and Comparitech, a VPN software review website."



And just to interrupt here, you've got to love that one.  This Pango Group controls multiple VPN brands and also runs their own VPN software review site because why wouldn't anyone go to a site that also publishes multiple VPNs to obtain an objective overview of all available solutions?  Apparently even they cannot decide which VPN is better so they publish three of them themselves.



Anyway, Bleeping Computer says:  "For its part, UltraAV says on its official website, on a page dedicated to this forced transition from Kaspersky's software:  'If you are a paying Kaspersky customer, when the transition is complete, UltraAV protection will be active on your device, and you will be able to leverage all of the additional premium features.  On September 30th, 2024, Kaspersky will no longer be able to support or provide product updates to your service.  This puts you at substantial risk for cybercrime.'



"A Kaspersky employee also shared an official statement on the company's official forums regarding the forced switch to UltraAV, saying that it 'partnered with antivirus provider UltraAV to ensure continued protection for U.S.-based customers that will no longer have access to Kaspersky's protections.  Kaspersky has additionally partnered with UltraAV to make the transition to their product as seamless as possible, which is why, on 9/19, U.S. Kaspersky antivirus customers received a software update facilitating the transition to UltraAV.  This update ensured that users would not experience a gap in protection upon Kaspersky's exit from the market.'"



Okay.  Now, anyone would take issue with the use of the term "facilitate."  This wasn't a facilitation, this was an abrupt and unsupervised "switcheroo."  I suppose they felt they were covered by sending that email notification in advance.  And I didn't see what the email said.  It may have said in the fine print that, if you did not want to have your AV and VPN services switched from Kaspersky to the Pango Group, you could terminate your subscriptions first.  Who knows?



What's clear is that for something as important as a system's antivirus/antimalware protection, users should have been in the loop.  A user interface should have popped up explaining that today was the day that Kaspersky was going to be uninstalled, and then giving the user the option of replacing it with UltraAV or uninstalling Kaspersky without replacement.  I would bet that did not happen because Kaspersky almost certainly made a bunch of money selling their entire paying AV and VPN subscriber base to this Pango Group.  So no one wanted to give anyone a button they could push to say "No, thanks" and opt-out of a continuing paying subscriber relationship now with UltraAV and UltraVPN.



Note that a continuing subscription relationship with these entities implies that Kaspersky also transferred their entire U.S. subscriber database, complete with all billing information, to these Pango Group-owned UltraAV and UltraVPN companies.  No one thinks this is ideal.  But Kaspersky's behavior was at least understandable under the circumstances.  And I have to say I can't recall a time that there wasn't a Kaspersky.  Consider the beginning of Wikipedia's article about them.



Wikipedia says:  "Kaspersky Lab is a Russian multinational cybersecurity and antivirus provider headquartered in Moscow, Russia, and operated by a holding company in the U.K.  It was founded in 1997 by Eugene Kaspersky, Natalya Kaspersky, and Alexey De-Monderik.  Kaspersky Lab develops and sells antivirus, Internet security, password management, endpoint security, and other cybersecurity products and services.



"Kaspersky expanded abroad from 2005 to 2010 and grew to $704 million in annual revenues by 2020, an 8% increase from 2016, though annual revenues were down 8% in North America due to U.S. government security concerns."



LEO:  You think?



STEVE:  "As of 2016" - yeah - "the software was about 400 million users and has the largest market share of cybersecurity software vendors in Europe."  So, you know, they're the real deal.  "Kaspersky Lab ranks fourth in the global ranking of antivirus vendors by revenue.  It was the first Russian company to be included into the rating of the world's leading software companies, called the Software Top 100.  Kaspersky Lab is ranked fourth in the Endpoint Security segment, according to IDC data.  According to Gartner, Kaspersky Lab is currently the third largest vendor of consumer IT security software worldwide, and the fifth largest vendor of Enterprise Endpoint Protection.  In 2012, Kaspersky Lab was named a 'Leader' in the Gartner Magic Quadrant for Endpoint Protection Platforms."



So, you know, this is not a fly-by-night outfit.  I would argue that they have far more cred than Pango, the Pango Group, who makes AV stuff no one's ever heard of before.  But that's what Kaspersky's users automatically got.  And I think this is unfortunate.  There's, remember, no evidence of any wrongdoing.  But despite the fact that Kaspersky's presence in the cybersecurity world has been nothing but a benefit, their business has been summarily ejected from the U.S. only because they share a country of origin with Putin.



In 2010, Kaspersky Lab worked with Microsoft to counteract the Stuxnet worm, which had infected 14 industrial locations in Iran using four zero-day vulnerabilities that were in Microsoft Windows.



In May 2012, Kaspersky Lab identified the malware Flame, which a researcher described as potentially "the most sophisticated cyberweapon yet unleashed."  According to the researchers in Kaspersky Lab, the malware had infected an estimated 1,000 to 5,000 machines worldwide.



The next year, in January of 2013, Kaspersky discovered the Red October malware, which had been used for widespread cyberespionage for five years.  It targeted political targets like embassies, nuclear sites, mostly in Europe, Switzerland, and North America.  The malware was likely written by Russian-speaking hackers, and the exploits by Chinese hackers.



Next year in February 2014, Kaspersky identified the malware Mask, which infected 380 organizations in 31 countries.  Many organizations that were affected were in Morocco.  Some of the files were in Spanish, and the group is believed to be a state conducting espionage, but Kaspersky did not speculate on which country may have developed it.



Later that year, in November of 2014, Symantec and Kaspersky authored papers that contained the first disclosure of malicious software named Regin.  According to Kaspersky, Regin is similar to QWERTY, a malware program discovered the next year.  Regin was used to take remote control of a computer, and is believed to have originated from the Five Eyes alliance.  In other words, you know, who we regard as the good guys.  The next year, in 2015...



LEO:  Well, us, in fact.



STEVE:  Huh?



LEO:  Us.



STEVE:  Yes, us.  In 2015, Kaspersky identified a highly sophisticated threat actor that it called "The Equation Group," speaking of us.  The group incorporated sophisticated spying software into the firmware of hard drives at banks, government agencies, nuclear researchers and military facilities, in countries that are frequent targets of U.S. intelligence efforts.  It's suspected to have been developed by the National Security Agency and included many unique technical achievements to better avoid detection.  Yet Kaspersky found it.



Later that year, in June of 2015, Kaspersky reported that its own network had been infiltrated by government-sponsored malware.  Evidence suggested the malware was created by the same developers as Duqu and Stuxnet, in order to get intelligence that would help them better avoid detection by Kaspersky in the future.  Kaspersky called it Duqu 2.0.



Also in June 2015, Kaspersky Lab and Citizen Lab both independently discovered software developed by Hacking Team and used by 60 - six zero - governments around the world to covertly record data from the mobile phones of their citizens.  The software gave police enforcement a "menu of features" to access emails, text messages, keystrokes, call history, and other data.



The next year, in 2016, Kaspersky discovered a zero-day vulnerability in Microsoft Silverlight.  Kaspersky identified a string of code often used by exploits created by the suspected author.  It then used YARA rules on its network of Kaspersky software users to find that string of code and uncover the rest of the exploit.  Afterwards, Microsoft issued a "critical" software patch to protect its software from the vulnerability.



Also that year, in 2016, Kaspersky uncovered the Poseidon Group, which would infiltrate corporations with malware using phishing emails, then get hired by the same company as a security firm to correct the problem.  Once hired, Poseidon would install additional malware and backdoors.  Later that year, in June, Kaspersky helped uncover a Russian hacking group, leading to 50 arrests.  And on and on and on.



So not exactly a blight on the cybersecurity landscape.  These guys have dramatically improved the state of cybersecurity through its entire history, starting from 1997.  Thus it's no surprise that so many people have rightfully trusted Kaspersky's antimalware solutions through the years, and actually through the decades.  And this was driven by high-quality independent reviews that found Kaspersky's solutions to consistently rank among the best.  Kaspersky earned and deserves the trust they've enjoyed.  And at no point have they done anything that would call that into question.  As I noted earlier, the world is better and more secure for having Kaspersky's beneficial and highly technical participation.



At the same time, what the U.S. Department of Commerce decided last April is also understandable.  Could Kaspersky Lab be forced to subvert all of the personal computers in the U.S. which are using its software?  We know the answer.  Yes, that's possible.  Could the KGB plant a rogue and trusted employee into Kaspersky's midst who might subvert their systems without anyone else knowing?  Sure, that could happen, too.  Just as Microsoft or a well-placed Microsoft employee could do the same for all of the machines in Russia and China that are still running Windows.  And as a result, as we've reported here previously, both of those countries are also moving away from their dependence upon closed software from the West - most notably Windows - as rapidly as they can.



Across the span of the last 50 years, computing has become personal to the point that we now all carry communicating computers in our pockets.  And those communicating pocket computers are all communicating through an incredibly well-working global network that grew up right alongside, and kept pace with, this incredible evolution in technology.  And across this span of time most of the world enjoyed relative peace and a great deal of relative prosperity while technology continued rushing ahead at breakneck speed.



Everyone was in a hurry to see what could be done, what new value could be created and what personal fortunes might be amassed.  National, geographic, and political boundaries were ignored in the rush to interconnect everything for maximum value and profit, and the entire world has been truly transformed.



But the world remains politically divided.  And now, after 50 years of astounding prosperity and technical advancement, we're beginning to witness rising tensions among some of the world's largest political powers.  Given how deeply intertwined the world's technologies have become, it's inevitable that these technologies, our software, and our networks would begin to fall victim to the rising tides of nationalism.  As we know, Russia has even been testing their big Internet cutoff switch which they can pull to isolate Russia from the rest of the global Internet, just basically running internally on RussiaNet.



And I've joked for years about my $5 automated AC outlet as a target, about the absurdity of the West being at odds with the Chinese manufacturers of most of our technology.  It's insane.  Our homes are filled with Internet-connected gizmos and gadgets that phone home to Beijing or other data centers outside of Western control.



And in recent news, the U.S. Commerce Department is expected to ban the use of Chinese and Russian hardware and software in American smart cars.  According to Bloomberg and Reuters, the upcoming ban is the result of an investigation of cybersecurity risks associated with smart cars.  The U.S. government fears foreign adversaries may use technology embedded in U.S. cars to hack vehicles, intercept communications, or track targets.  And so it goes.



The problem is that in today's current climate of increasing mistrust, the demonstration of "risk" is all that's necessary to drive policy, and policy drives behavior.  Having the Internet as a single connected global network is an inherent risk, but it's also been unbelievably valuable.  At the least it has allowed people the world over to have access to markets and opportunities that would have never been available without this incredible global communications network.  And yes, communication itself is risky, especially within countries that wish to exert tighter control over what can be communicated.  So is the solution to follow Russia and break this global network into separate pieces, with each piece only shared among those whose goals and motivations are aligned and trusted?  If that's what ends up happening, it would be a horrible shame and would represent incredible lost opportunity, especially for those parts of the world that are being so rapidly advanced and lifted up through access to this amazing Internet resource.



Kaspersky's ejection from the U.S. is worrisome as a tangible indicator of the changing politically-challenged technological environment that will affect us all.  I sincerely hope our various governments don't allow fear to blind them to the fact that communication is always better than isolation.



LEO:  It's interesting because this is the episode that you talked about the pager attacks, the supply chain attacks.  And in a way that's a similar risk with software from an unknown source or a potentially enemy source.



STEVE:  Yes.  You made the point, Leo, about how long ago those pagers may have been set up.



LEO:  Right.



STEVE:  We don't know what's in the chips.



LEO:  Right.



STEVE:  That we've been blithely purchasing.



LEO:  And this is only going to get worse.  We used to think, oh, you know, you can just trust everybody.  Now we're learning otherwise, and we're setting up perimeter defenses, saying, well, if they're outside the U.S., you can't use them.  But we know perimeter defenses haven't been a good solution for companies for some time.  They've moved to something called Zero Trust.  And I'm wondering if ultimately we're not going to have to move to a Zero Trust kind of attitude on everything that we use as consumers.



And this might be up to you, Steve.  I'm going to put this on you.  But we need people like you to come up with ways that we can use stuff, but minimize the risk from that stuff.  You know?  Because that's what we're going to have to do.  There's nothing that's known safe anymore.  And soon we're going to get in the position where we can presume that much of what we use is not safe.  So we're going to need a system, a Zero Trust system that allows us to somehow control that.  Yes?



STEVE:  I guess the question is, you know, and this reminds me of "Ghostbusters," who you gonna trust?



LEO:  I mean, we've gone from trusting everybody; right?  And I've often said this is part of - this is civilization.  Civilization doesn't exist without cooperation and trust.  When you drive down the street you trust the guy coming in the other direction at 30 miles an hour is not going to turn into your lane.



STEVE:  Yes.



LEO:  That's part of being in a civilization, a cooperative environment.  But it seems to me that maybe that's not going to work long term with technology.



STEVE:  So, I mean, it really means that we pull our skirts in, and we don't accept, I mean, for example, that every chip in every auto is designed and fabricated in the USA.



LEO:  Right.  But is that enough?



STEVE:  And this was my point about the economy.  I mean, yes, that's great for nationalism.  But we're able to use incredibly inexpensive design and fab in China because it's incredibly inexpensive.



LEO:  Yeah.



STEVE:  So we're talking about huge increase in cost.  Now, on the other hand, we saw a huge supply chain shortage during COVID when automobile prices jumped up because we couldn't get the chips that we needed from offshore.  And presumably, if they were being manufactured in the good old USA, then there wouldn't have been a supply chain problem.



LEO:  Or would there?  What about Ford?



STEVE:  Right.



LEO:  I mean, we live in a world where trust is required.  And yet we are rapidly experiencing the erosion of trust.



STEVE:  And it's so valuable, Leo.



LEO:  It is.  Without it...



STEVE:  It is so valuable.



LEO:  We can't do what we do without it.  Can't drive down the street safely without it.  You can't go to Starbucks and have a coffee without it.  So what do we do?  This is an interesting conundrum.  I don't think saying, okay, you can only use stuff from the U.S., clearly that's not the solution.  And that's, I mean, that's what we did with Kaspersky.  But is that the solution?  I don't think so.  All of our devices are made in China.



STEVE:  Well, and we just forced a high-quality source of cybersecurity out of the U.S.  Nobody gets to use that anymore.  



LEO:  Right.



STEVE:  And so, yes, there are alternatives.  There are, you know, there are non-Russia-based actors.  And, you know, and some specious claims were made about Kaspersky employees and - I was pronouncing the name wrong the whole time.



LEO:  I don't know.  Kaspersky, Kaspersky.



STEVE:  Kaspersky, Kaspersky.  Anyway, I mean, what it really is, is sad.



LEO:  We need, you know, a chain of trust.  And we have that system with the certificate systems.  We know how to do it.  And I think maybe that's ultimately what's going to happen here, and say...



STEVE:  Well, and look at what Apple is doing with the servers.  



LEO:  Apple's the first thing that comes up.



STEVE:  That are coming in from offshore.



LEO:  Yeah.  So do you trust Apple?  And then...



STEVE:  I mean, bend over and spread them.



LEO:  And then Apple's going to be - the burden's on Apple to make sure that those Shenzhen factories are not compromised, that those workers are not working for somebody besides Apple and us.  It's an interesting world we live in.  We are very interdependent.



STEVE:  Look at the extra cost that Apple's going through.



LEO:  Right.



STEVE:  In order to know that what they're plugging into their data centers has not had some supply chain compromise.



LEO:  Yeah, and I hope that it's not theater.  I hope that that's real.  You know what I'm saying?



STEVE:  Oh, yeah.



LEO:  That this isn't merely a marketing term.  Because more and more we're going to need to trust them.



STEVE:  Yeah.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




