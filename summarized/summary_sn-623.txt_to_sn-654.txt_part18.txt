GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#623

DATE:		August 8, 2017

TITLE:		Inching Forward

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-623.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss and look into DigiCert's acquisition of Symantec's certificate authority business unit, LogMeIn's LastPass Premium price hike, the troubling case of Marcus Hutchins's post-Defcon arrest, another instance of WannaCry-style SMBv1 propagation, this week's horrific IoT example, some hopeful IoT legislation, the consequences of rooting early Amazon Echoes, the drip drip drip of WikiLeaks Vault 7 drips again, Mozilla's very interesting easy-to-use secure large file encrypted store and forward service, the need to know what your VPN service is really up to, a bit of errata and miscellany, and some closing-the-loop feedback from our always-attentive terrific listeners.



SHOW TEASE:  It's time for  Security Now!.  Steve Gibson's here.  There's no big - nothing, nothing urgent.  Just relax.  We're just inching forward in the security world.  He will have his comments, though, on the WannaCry hacker who was arrested on his way home from DEF CON and a whole lot more.  It's coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 623, recorded Tuesday, August 8th, 2017:  Inching Forward.



It's time for Security Now!, the show where we cover the security and privacy of all of us online, in the real world and the unreal world.  And that guy right here, the guy looking over my - peering over my shoulder...



STEVE GIBSON:  "Cyber world," I think, is the official term.



LEO:  Cyber.  Cyber.  Cyber is always hard.



STEVE:  Yeah.  So...



LEO:  I didn't introduce you.  Steve Gibson.



STEVE:  Oh.  That's me.



LEO:  That's Barron Trump, ladies and gentlemen.  He is a genius with the cyber.	



STEVE:  Has suddenly grown up.



LEO:  All grown up.



STEVE:  And has lost his hair.  And what there is...



LEO:  Steve Gibson of GRC.com, genius fellow.



STEVE:  ...is gray.



LEO:  Gray, gray, gray.



STEVE:  So no real title for this week stood out.  So I just thought, okay, you know, how about we'll call this one "Inching Forward" because there are signs of progress amid many indications of need for progress.  So for our August 8th Episode 623, "Inching Forward."  We're going to discuss a bunch of fun things.  The top two were, like, clearly dominant in our listeners' minds because they know that they've both been a focus of this podcast in various ways.  The first is DigiCert's acquisition of Symantec's certificate authority business unit.  Then of course we have LogMeIn's LastPass Premium price doubling.  And I heard you talking about it on Sunday, we need to talk about it, too, the troubling case of Marcus Hutchins's post-DEF CON arrest.  Another instance of the WannaCry-style SMBv1 propagation, which was bound to happen.



This week's horrific IoT example gives me an opportunity to talk about the way it should be done, as opposed to the way they did it in one particular case.  We have some very good-looking, well-crafted IoT legislation on the horizon.  I will be surprised if - and this was introduced by the Senate.  I'll be surprised if it just doesn't become quickly signed into law.  I don't know why it wouldn't be unless there's some very strong anti-IoT security special interest lobbying.  I guess we'll find out.  The consequences of rooting early Amazon Echoes.  The drip drip drip of WikiLeaks Vault 7 drips again.



We've got a very interesting, easy to use, secure large file encrypted store-and-forward service being offered by Mozilla.  And I've taken a look at it, and we'll talk about that.  And, I mean, we'll be recommending it.  Also the need to know what your free VPN service is really up to.  Then a bit of errata, miscellany, and some closing-the-loop feedback from our always attentive terrific listeners.  So another great podcast...



LEO:  A jolly, jolly good day today. 



STEVE:  ...will two hours from now be in the can.



LEO:  Don't rush us.  It's going to be a fun ride.



STEVE:  So our Picture of the Week is actually from last year's DEF CON and Black Hat conferences, but it's just so perfect that I used the reoccurrence of this.  Someone sent it to me again.  I recognized it, as you did, from having been at a prior conference.  It shows the GSM cell providers, both before and then during Black Hat and DEF CON.  And it's a little troubling because you pretty much see all of the same ones that were there before, with maybe three times more have suddenly appeared, which is extremely suspicious.



LEO:  What do they call those, those phony cell sites?  There's a name for them.  Oh, Stingrays.  Those are Stingrays; right?  All the ones that aren't real.



STEVE:  Yup, exactly, yeah.



LEO:  Geez, and there are a lot of them.



STEVE:  Just wonderful.  So, yes, one wants to be very careful when you're doing anything, when you bring any electronics or WiFi or anything to Black Hat.



LEO:  Man.



STEVE:  Okay.  So our top of the news was the most tweeted item to me in the past week because everyone knows of my choice some years ago and my continuing happiness - and I should also mention, not only mine, but many people who have sent feedback, some of which I've shared on the show, about DigiCert as my certificate provider.  Both Symantec and DigiCert produced press releases over the last week saying that DigiCert and Symantec had some to an agreement about the sale of Symantec's certificate authority business unit to DigiCert.



So before I comment I want to be absolutely clear with everyone, I have no inside knowledge of any sort about this transaction.  I spoke to no one at DigiCert or Symantec about this.  So any and all conclusions are, you know, they're on a level playing field with everybody else's.  I don't know anything.  And I have to put that caveat out there because I'm going to assume a few things that are probable.



So all of our listeners know I could not be more bullish on and delighted with the quality of DigiCert's services and their support.  And as I mentioned before, our long-time listeners will recall that I was driven away from Symantec and VeriSign and into the arms of DigiCert because I was so unhappy with Symantec's performance.  They just, you know, frankly, they were too big.  They had a "no way to get their attention" sort of feeling.  And I was being upsold, and it just - and they were also very expensive, and it wasn't clear that I was getting my money's worth.



Also our listeners know that DigiCert has been uniquely able to meet some of my own weirder needs, such as minting a pair of specific year-end expirations with specific signature hash strengths in order to satisfy Chrome while Chrome was sunsetting SHA-1 certs, yet I wanted to keep GRC available until that actually happened at midnight on New Year's a couple years ago.  There's no possibility that I could have ever obtained that kind of service from Symantec.  Okay, so I'm not going to pretend to be an unbiased and objective observer, although my biases are public.  And I believe they're well-earned and deserved.  And I'll also note that, if it had been the other way around, if Symantec had purchased DigiCert, much as LogMeIn purchased LastPass...



LEO:  It would be time to panic.



STEVE:  Which we'll be discussing next, yes.  I would be heartsick.  I mean, I would just - I would be devastated right now since being able to work with and depend upon a high-quality certificate authority is way up there in my own hierarchy of needs, probably only second to Level 3 being GRC's bandwidth provider.  And so both of these guys just, I mean, you know, zero trouble service.  So anyway, thank goodness the acquisition was in the direction it was.



LEO:  Yeah, because DigiCert you use and I use.  That's what our TWiT cert comes from.  And they're fantastic.  We love them.



STEVE:  They are.  And, I mean, they have a boutique feel to them.  They don't feel big.  I mean, I have email exchanges with them, and I know it's not just me.  I hear from other listeners who they know less well, you know, yeah, we're like, wow, I can't believe the support.  And, I mean, when I don't interact with them, their facilities that they have established work right.



For example, I use EV certificates, and rather than waiting until it's necessary for me to renew an extended validation cert, which requires a lot more hoop-jumping, their system will in advance start the process of reverifying all of the extra stuff that EV requires.  And so my EV-ness is maintained, allowing me to issue my own certificates in the middle of the night.  In the dead of night I can get an EV cert in a couple minutes, which is just amazing.  And, I mean, but without any reduction in security.  They've just figured out we shouldn't wait till the last minute.  We should do this ahead of time.  And so they just take care of it.  I mean, it's just an entirely different experience than I had with Symantec.



LEO:  And, yeah, you moved there from VeriSign; right?



STEVE:  Yes.  Yes, I said, okay.  And we know there's a lot of switching inertia.  I mean, I'm also in the process of leaving Network Solutions, where I have always been.  GRC.com is finally getting close to renewal, and it'll be moving to Hover.  I'm moving everything over to Hover in the same way that I have already moved everything away from Symantec.  So, you know, it takes effort to do it; but it ends up, I think, being worthwhile.



So, okay.  So get this, though.  There's some money changing hands.  DigiCert purchased Symantec's entire certificate authority business for - okay, I won't ask you if you're centered over your ball anymore, Leo, because I know you're in a chair, so hold onto your armrests - $950 million in cash.



LEO:  Well, it's not quite a billion. 



STEVE:  Just shy of a billion dollars in cash, plus a 30% stake in DigiCert.



LEO:  Oh, wow.  Now, that I'm not so crazy about.



STEVE:  Well, I feel the same way.  But, okay.  So it must have been a win-win.  I mean, okay.  So in email which DigiCert's customers received - and this was last week when the deal got closed.  John Merrill, who's the CEO, wrote - and I've skipped all the opening boilerplate because the interest to us says, you know, so dot dot dot:  "Also, some of you may be wondering about any implications our announced acquisition will have on the ongoing debate between Symantec and the browser community about trust in their certificates."



LEO:  Right.  See, that's what I wonder, if this is why this happened.  It's like...



STEVE:  Exactly, Leo.  And that's why I put that caveat upfront about I don't have - I have no insider knowledge whatsoever.  But an observer seeing that - okay.  So anyway, I'll finish what John said.  John said:  "Earlier this year, the browsers proposed a plan to limit trust in Symantec certificates after discovering issues with how they were validating and issuing digital certificates.  Importantly," he writes, "we feel confident that this agreement will satisfy the needs of the browser community.  DigiCert is communicating this deal and its intentions to the browser community and will continue to work closely with them during the period leading up to our closing the transaction."



And, by the way, that's not until - this doesn't actually close until third fiscal quarter 2018.  But I don't know whose fiscal quarter, nor when that is.  So it's at least half a year, I would guess, depending upon when the fiscal calendar is, and whose.  Anyway, he says, "DigiCert appreciates and shares the browsers' commitment to engendering trust in digital certificates and protecting all users."



And separately, the principal security analyst at 451 Research, Garrett Bekker, wrote:  "Symantec's certificate business will immediately increase DigiCert's market share and make the company one of the biggest players in the PKI (Public Key Infrastructure) and SSL markets.  This will make DigiCert pretty much one of the leaders in terms of revenues in the digital certificate business."



Okay.  So in retrospect, this makes so much sense.  As we've talked about, as we've been discussing the various certificate authorities who have screwed up in the past and the reaction of the browsers, which is, I mean, it has to happen.  A certificate authority lives and dies by its attention to detail and the earned trust in the integrity of the assertions made by its signed certificates.  I mean, that's it.  As I've said before, I saw somebody quoting me in Twitter, we're paying them for digital bits.  They're minting bits which have value only because of what their signature represents.



And as we know, trust is, as it should be, hard won and easily lost.  And Symantec lost the industry's trust as a consequence of now very well documented misbehavior on third-party partner parts that Symantec had a responsibility to manage and didn't.  So having acquired their business from VeriSign, which is where they got into the CA business, Symantec was a huge going concern with a large customer base which, unfortunately, as a consequence of these problems which became public, they could no longer effectively leverage.  So this transfer of trust-requiring assets from a now untrusted owner who could no longer effectively leverage them for profit to a highly trusted owner who can makes total sense.



So anyway, it'll be interesting to watch how DigiCert handles this.  I want them, selfishly, I would love them just to stay exactly the way they are because they're perfect right now.  But on the other hand, the certificate minting business is inherently scalable.  And as I was saying before, DigiCert's system is already highly automated, and they've just got this nailed.  So in theory they should be able to acquire and transfer Symantec's certificate customer base to themselves without needing to massively expand.



So, I mean, I think it's perfect.  Symantec had destroyed their trust, and suddenly this asset that they had had crashed in value.  And only by moving it, only by transferring it to someone who could effectively leverage its value because they were so highly trusted, could Symantec get the value from it, like obtain its residual value.  And DigiCert gets to take advantage of the fact that they can purchase it for a lot less, probably, than they would have been able to before.  I mean, it probably wouldn't even have been on the market before.  So, you know, yay.  I think it makes a lot of sense.



LEO:  But there are other acquisitions maybe not so great.



STEVE:  Well, yes.  Speaking of which, all of our listeners were worried.  And you and I, Leo, were cautiously, well, optimistic about LogMeIn's purchase of LastPass.  We jumped onto the LastPass bandwagon when it was still wet behind the ears.  Joe Siegrist, a friend of the podcast, was always forthcoming, shared with me the details of the protocol, exactly how it operated, which is why I was able to explain it to our listeners and say these guys nailed it.  They did it exactly right.  This is what I'm using moving forward.



And that was even early in the growth of the password manager business.  I mean, LastPass was not the first.  But I would argue that they had the right set of features, cross-platform.  The security model was correct.  They did everything they could to secure our data for us in a 100% TNO-obeying fashion.  And, I mean, absolute TNO, as we know, comes at a price of convenience.  And so they've also cleverly implemented things like pre-issued, one-time, get-out-of-jail-free cards or tokens and things to take a little bit of the edge off of the absoluteness of TNO because they also want to produce a practical solution.



Anyway, and we've followed this now since the LogMeIn purchase.  And there's been no indication that we've seen of any failure in the technology.  Even when Tavis had his epiphany in the shower, you know, he had barely gotten himself dried off by the time that Joe and company had fixed the bug that he had come up with.  Which is all we could ask from anyone is a set of policies that are correct; and then, if mistakes are found, they're immediately fixed.  And even in this case of what Tavis most recently found, I mean, it was an obscure problem that it's good to have removed, but nobody got hurt from it.



So, okay.  So what happened which has caused a great deal of upset is that LogMeIn - okay.  So one way to put it is they've doubled the pricing of their premium subscription.  The other way to put it is they increased it by a dollar a month.  So it was $1 a month, $12 a year.  Or, well, yeah, or free, so there is a free version.  But the $12 per year is now $24 per year.  There have been a class of users who were willingly paying the dollar per month to support Joe back in the beginning.  And probably inertia and the fact that LastPass's value at $1 per month for the premium subscription with the additional features it offers, I think it's a bargain.



So I think it remains to be seen how those people will feel about this jump.  I've seen some people saying, oh, my god, you know, I've got to go find a different password manager.  Or saying, hey, I was supporting Joe at a dollar a month.  This rubs me the wrong way.  The free version, it's important to note, remains fully functional and useful.  And so I guess I would take a look at it objectively.  Rather than being upset at the change, I would say, okay, what are the alternatives?  How do they compare?  What are their track records?  For me, what I care most is about the security guarantees that LastPass offers and the quality of their service, that is, going forward.  As with the CA business, trust is hard-earned and easily lost.  And so far Joe and company, even after the LogMeIn acquisition, have never let us down.



So I'm not switching.  I still think it's the right solution.  But I certainly just wanted - because many people were wondering what I thought.  You know, I mean, I don't know what's going on behind the scenes.  It would be interesting to know, and we never will, what the effect is for LogMeIn.  This is the nature of a parent acquiring something like this is they want to monetize it.  And they now have built, thanks to Joe and their acquisition, a leading password manager that I think, independent of its cost or its pricing, is the one I still believe is the one to use.



So I guess I would separate the emotion from the service that's being delivered.  And for what it's worth, I'm staying where I am.  We know them.  We know their focus on the technology.  The second that changes, then that's a different story.  And if you're a person who was paying a dollar a month, or $12 a year, to support Joe, and $24 seems like more than you want to do, then look at the free version.  Look at backing away from premium and continuing to use it.  So anyway...



LEO:  I don't think 24 bucks a year is very expensive.  And it's, I think, still less than 1Password, which is, I would say, the number two competition.



STEVE:  Correct, correct.



LEO:  And they moved from a one-time-only fee to a monthly fee for the same reason, I think.  It just makes sense.  Why wouldn't you want to pay two bucks a month for a good password  manager?  That doesn't seem too much to me.



STEVE:  Yeah, well, I mean, I have a problem with...



LEO:  There's free ones.  They're just not as convenient.  So if you don't - if you want to go free, get KeePass or something like that.



STEVE:  Yes.



LEO:  And give up some convenience.  But you can save the money.



STEVE:  Well, and I have a problem paying a subscription for something that doesn't have an ongoing cost to the provider.  So, for example, BoxCryptor, we'll be talking about them later, they used to have a legacy version which they've discontinued.  The legacy version you could purchase, and then you owned it, and you could use it on all the cloud providers that you wanted to.  To me, that makes sense.  Now they only have a pay-as-you-go subscription business, and they lose me.  Sorry, I'm not doing that.  There are alternatives.



But, for example, in the case of LastPass, we're using their servers.  They're maintaining the apps that we use on our various platforms.  Joe is there when Tavis comes out of the shower on the weekend.  It was a Saturday afternoon, I think; you know?  And bang, within an hour this thing was fixed.  So that's what we're paying for.  So there it makes sense that it would be an ongoing support for what it is that we're getting in return.  In this case, I think it makes sense.  It's not a standalone utility.  It is networked.  It's cloud-based.  And we need it to be.  To be useful, it's got to be cross-platform.  And we know they're continuing to put a lot of work into it because we keep seeing improvements.



Okay.  Marcus Hutchins, Leo.



LEO:  I'm really interested in what you think.  I mean, I think we don't have enough information to know.  Is he a criminal?  If he's a criminal, he's a criminal.



STEVE:  No.



LEO:  Or is this overreach by federal prosecutors using hacking laws that are antiquated and not well defined?  Or is he completely innocent, and they're just being jerky jerks?



STEVE:  So let's catch our listeners up, and then we'll talk about this.  Of course Marcus came to everyone's attention in May, a couple months ago.  He was the guy who stopped the WannaCry worm by registering that obscure DNS domain that it was checking for.  And he discovered, quite to his surprise, that it suddenly stopped propagating.  So this crazy SMBv1 weaponized EternalBlue exploit that was WannaCry got stopped in its tracks because this white hat hacker reverse-engineered enough of the worm to spot its attempt to retrieve a web asset from that bizarrely named and, at the time, unregistered domain.



And, okay.  So last Wednesday, six days ago, as Wired wrote in their coverage, authorities detained - and they said 22-year-old Marcus Hutchins, although Ars Technica's coverage said he was 23, and they included a screenshot of his booking, and I had it - oh, at the City of Henderson near Las Vegas - that did show his age as 23.  So I think they're probably right.  Anyway, this was immediately following the DEF CON hacker conference last week in Las Vegas, as he was attempting to fly home to the U.K., where as we know he works as a researcher for the security firm Kryptos Logic, spelled K-R-Y-P-T-O-S.



So upon his arrest, the Department of Justice unsealed an indictment against Marcus, charging that he created the Kronos banking trojan, which is a widespread piece of malware used to steal banking credentials for fraudulent purposes.  He's accused of intentionally creating that banking malware for criminal use, as well as being part of a conspiracy to sell it for $3,000 between 2014 and 2015 on cybercrime market sites such as the now-defunct AlphaBay dark web market.  Now, it's worth noting that on Friday, two days later, when Marcus was arraigned, he denied any wrongdoing and pleaded not guilty to the charges against him.  He has a court date set for today, August 8th, in Milwaukee.  And until then, until probably just being transferred, he was being held in Las Vegas jail.



Now, it's also worth noting that his alias is unfortunately "MalwareTech," and he uses the Twitter handle "@MalwareTechBlog," which is probably not helpful, but that's who he's known as.



LEO:  And not a crime.



STEVE:  And not a crime, exactly.  So paraphrasing a bit from Wired's coverage, the short, eight-page indictment against Hutchins has already raised questions and skepticism in both legal and cybersecurity circles.  Orin Kerr, a law professor at George Washington University who has written extensively about cybersecurity and hacking cases, says that based on the indictment alone, the charges look like, quote, "a stretch."  Although the indictment claims Hutchins wrote the Kronos malware, nothing in the document illustrates that Hutchins possessed actual intent for the malware he allegedly created to be used in the criminal conspiracy he's accused of.  Kerr said, quote:  "It's not a crime to create malware.  It's not a crime to sell malware.  It's a crime..."



LEO:  Really?  Wait a minute.  It's not a crime to sell malware?



STEVE:  That's what he said.  That's what this guy says, yeah, which surprised me, too.



LEO:  It's a crime to use it, I guess.



STEVE:  Well, he says:  "It's a crime to sell malware with the intent to further someone else's crime."  Meaning that, like, if you sold it to your grandmother because she wanted to hang it on the wall, then that's not a crime.  But clearly most malware sales is with the intent to further someone else's crime, so...



LEO:  It's hard to imagine, I mean, you could create malware for research purposes.



STEVE:  Right.



LEO:  But selling it, you wouldn't sell it to another researcher.



STEVE:  Yeah, no.



LEO:  Now, that's, you know...



STEVE:  But so I guess the point is...



LEO:  He never sold it, though, I guess, and that's part of his defense is he made it, and somebody else he either knew or didn't know, it's not clear, sold it.



STEVE:  Yeah.  So anyway, Kerr says the story alone doesn't really fit.  There's got to be more to it, or it's going to run into legal problems.  Now, of course, there are a lot of people who are behind Marcus and are supporting him.  Some associates of his defended him Thursday on Twitter, that's last Thursday, even arguing that he has worked directly with U.S. law enforcement.  Kevin Beaumont, whom we've spoken of before, who is a U.K.-based security architect, wrote:  "I know Marcus.  He has a business which fights against exactly this, bot malware.  It's all he does."  Kevin said:  "He feeds that info to U.S. law enforcement."  And he wrote:  "The DoJ has this seriously F'ed up," and he didn't say "F'ed" in his tweet.



Jake Williams, another well-known researcher with the security firm Rendition Infosec, said he'd worked with Hutchins multiple times since 2013, met him in person at last year's DEF CON, and shared malware samples.  At one point in 2014, Williams says, Hutchins refused his offer of payment for help on an educational project.  And as we know and reported, even when Hutchins was awarded a $10,000 bug bounty from the security firm HackerOne for his work on stopping WannaCry, he donated it to charity.  Williams said:  "I have pretty good black hat radar.  It never went off when talking to Marcus or exchanging stuff with him."



So Wired finishes the coverage writing:  "For the moment, neither the FBI nor the Department of Justice is commenting further on Hutchins's case beyond the DoJ's statement and the facts of the indictment."  A spokesperson for the Electronic Frontier Foundation, our EFF, which often offers legal representation to embattled hackers, wrote in a statement to Wired that it's "deeply concerned" about Hutchins's arrest and are reaching out to him.  In their tweet, the EFF tweeted on August 3rd:  "EFF is deeply concerned about security researcher Marcus Hutchins's arrest.  We are looking into the matter and reaching out to Hutchins."



And in the show notes there is now a legal defense fund that has been set up at secure.lawpay.com.  I've got the link in the show notes, and the show notes are already posted online, so anyone can get them at GRC.com/sn.  And I scrutinized this page carefully.  It looks legitimate and looks like the real deal.  So I wanted to let people know, if anyone is interested in contributing, it looks like this is the place.  You might want to wait until the EFF puts something up on their site.  I would trust, clearly, a link from them.  I don't remember where I got this link.  It was part of the research that I was doing putting this together.  But it does look legit.



And it'll be interesting to see how this turns out.  And your coverage on TWiT on Sunday, Leo, I think, was spot-on.  Your panel agreed with what we were saying in fact just last week about the problem with this kind of reaction is the chilling effect it has on the benefit that white hat hackers produce for society at large in finding these.  I mean, clearly he really helped minimize the problem with WannaCry because he felt it was safe to do this.  And in fact you guys were talking about the fact that there are other security conferences, and how would out-of-country, non-U.S. researchers feel about coming to Black Hat and DEF CON if they thought there was some fear that they might get nabbed by border security, essentially, when they're trying to leave.



LEO:  Yeah.  And of course a lot of the opinions people have on this are preconditioned by how poorly the DoJ has acted in cases like Aaron Schwartz's case, where they overreached, and Marcus's reputation in the hacker community.  So while we don't know the facts, there's I think legitimate reason to be concerned and to want to know more.



STEVE:  Yeah.  And we've also, you and I have talked on this podcast about how it is also the case that some hackers sometimes do cross the line.  That is, you know, four years ago he may have been a different person from who he is today in 2013.



LEO:  Right.  We just found out, yeah. 



STEVE:  And so, you know, he was younger, less experienced.  And it's also the case that a lot of sort of more junior hackers start off a little more on the dark side and then realize, hey, I can do good with this rather than just bad.  So anyway, the good news is he will probably have access to the best defense that he deserves.  Let's hope he deserves a really good one, and then it will happen.



LEO:  Good, good.



STEVE:  So in a follow-on, and under the category of, well, this was entirely foreseeable, researchers at Flashpoint have discovered that the so-called "TrickBot" banking malware trojan has just been evolved to add WannaCry-style SMBv1 LAN scanning and propagation.  It doesn't yet have the capability of scanning out into the public Internet, which was WannaCry's big claim to fame.  But it does probe LAN-based servers via there's an API in Windows, NetServerEnum, that essentially allows software to get a list of servers that are known to be on the network.  And so it gets that list from Windows itself and then says, oh, thank you, and then does a WannaCry-style SMBv1 attack against them in order to insert itself and propagate through the Intranet within an environment that has servers.  It can also use LDAP, the Lightweight Directory Access Protocol, in order to find candidate servers to attack.



So if it gets inside an enterprise network which has not yet administratively and globally disabled SMBv1 - yeah, which I'm hoping all of the IT managers who listen to this podcast did back in May when Marcus Hutchins was shutting WannaCry down, and we found out that this was an SMBv1 vulnerability - and nobody needs SMBv1 - and noticed that it won't be until Windows 10 later this year that Microsoft disables SMBv1 proactively.



So it's up to IT people in enterprises to do that now.  You don't want this thing getting loose inside your network.  And it's also foreseeable that this won't be the last re-use of this SMBv1.  If history teaches us anything, we know that it's going to take a long time for SMBv1 to finally disappear, even though it's been deprecated for decades.  I mean, it's more than 20 years old.  It was the original file and printer sharing protocol in Windows 3.  So, yeah.



Oh, boy.  And in this week's IoT nightmare, we learn from the guys at BitDefender that they found at least 175,000 publicly available, publicly exposed, Chinese manufactured, Internet-connected security cameras located and indexed by Shodan which are completely hackable.  So the guys at Bitdefender, doing some poking around with Shodan - we've talked about it before.  Shodan is a queryable search engine for all the other ports on the Internet, rather than 80 and 443.  Eighty and 443 are of course HTTP and HTTPS, which Google indexes for us.  Shodan does the rest.  It doesn't deeply index the content the way Google does, but it scans the entire Internet and just says, oh, I found something over here on port 32726 - wait, 327, yeah, it's valid - and then allows you to make a query, if you have something that you want to find that you know lives on that port, and off you go.



And it also, if you have a server that responds to a connection, it indexes the response.  So, for example, if it's a web server in a camera, and that web server in the camera says hi, I'm from Shenzhen Neo Electronics, the NIP-22, which is actually one of the models which is vulnerable, then Shodan indexes that.  So if you determine that the NIP-22 has a vulnerability, Shodan says, oh, here's 175,000 of them that are currently on the Internet.  Have fun.



So, okay.  So these vulnerable cameras are manufactured, as I said, by Shenzhen Neo Electronics.  They produce security and surveillance solutions, including IP cameras, sensors, and alarms.  Bitdefender did some research, discovering buffer overflow vulnerabilities in two models of the cameras.  They have one called the iDoorbell.  Apparently it's being sold on Amazon, or there is at least an iDoorbell on Amazon which has one one-star review.



LEO:  Yikes.



STEVE:  I couldn't determine if it was the same one.  I couldn't find - I looked all over for the manufacturer.  I don't know.  But apparently it got like really hot when this one guy plugged it in.  And it's like, okay, this doesn't seem right.



LEO:  Do not buy that, yeah.



STEVE:  No.



LEO:  It's not the Ring, that's for sure.



STEVE:  No.  This is the anti-Ring.



LEO:  Yeah, the opposite end.



STEVE:  Oh, boy.  And the other one is the NIP-22 model.  But because many of Shenzhen Neo's devices share and reuse the same firmware, these researchers believe that other models also are certainly similarly vulnerable.  The security cameras - now, get this, Leo, here's the ultimate nightmare - use UPnP,  Universal Plug and Play, to automatically open ports through their users' firewall and router to allow unsolicited incoming access from the public Internet.  



LEO:  Geez Louise.



STEVE:  Yeah.  Querying the Shodan search engine for vulnerable devices, the researchers discovered, depending upon when they queried, between 100,000 and 140,000 vulnerable devices of various sorts, leading them to sort of conclude, okay, about 175,000 is what we're guessing, if you remove various overlaps.



Okay.  Now - oh, and there is both an HTTP and an RSTP.  That's the real - RTSP, sorry, Real-Time Streaming Protocol.  Both services and servers are exposed.  Okay.  So this is the least caring, least careful, and sloppiest way of doing this, by simply creating Internet-exposed, publicly available TCP servers.  So as a lesson, the proper way to do this for such devices, which are almost certainly going to be located behind NAT routers, is for those devices to occasionally send a single UDP packet outbound to a publicly accessible remote server.



The flow, the travel of that outbound UDP packet automatically creates, I was going to say "instantiates," same word, fancy word, creates return packet mapping through any and all NAT routers it encounters and passes through along the way, which denies incoming traffic from any but that single remote IP.  In other words, traffic coming back is not unsolicited, but any traffic coming from any other source IP can't get through.  It'll just be dropped, as it should be.



Then the remote server can use the implicit reverse mapping through the NAT routers, which is created by the camera very occasionally, just sending a little ping, essentially a UDP ping packet out, that allows the remote server to send instructions back on a per-camera basis when it wishes to enable, for example, audio and video streaming.  So that's the way you do this.  It's not difficult.  It just requires someone who cares.  Instead, they just plug the camera in, and it opens up TCP, HTTP, and RTSP services that anybody can access.



Okay.  So on top of this, Bitdefender found that both of these camera models are vulnerable to two different remote attacks, one that affects the web server service running on the cameras and the other that affects the RTSP, Real-Time Streaming Protocol server.  They showed that it was quite easy to exploit the flaws in the security cameras.  This doesn't even require high-end hacking.  Get a load of this.



First of all, they've got default credentials, believe it or not, user/user and guest/guest.  So, yes, the username is "user," and that password is "user"; and the guest account the username is "guest," and its password is "guest."  So then they also found several buffer overflow vulnerabilities that could be exploited to take control of the cameras remotely.  And again, those are trivial.



An example of one of the vulnerabilities is that, in the publicly exposed web service, it's triggered by an error, that is, the overflow is triggered by an error in the way the application processes the username and password information.  When the user attempts to authenticate, the credentials are passed in a GET request with name=value parameter pairs, so it's the <ip>/?, so then usr= and then the username, ampersand, pwd= and then the password.



But when the web authentication function parses these values - that's the libs_parsedata function - it copies the content of the two arguments onto the stack without checking their actual size.  You know, you just can't make this up.  Thus, of course, allowing for an out-of-bound data write onto the device's stack, which it's then easy to arrange to have that function execute when it returns, and the remotely provided code is then running in the camera to get up to any mischief it chooses.  Although, boy, with default usernames and passwords of user/user or guest/guest and a publicly exposed TCP server, it's not even clear that you need to bother with a buffer overrun.



So, boy.  I mean, it just can't get any worse.  This is pure irresponsible sloppiness.  But at the moment we have no policing of this.  Anybody can produce anything and sell it and say, oh, look, Internet cameras, they work.  Yeah.  But if you have this inside your network, not only can anyone see what's going on through your camera, but they can generate a query that ends up allowing their code to get into your camera, probably giving them access to the rest of your internal network.



So this is a perfect example of a way that an IoT device compromises your internal network because, if it's a WiFi device, it's on your WiFi, so it's got access to your Intranet.  And if remote hacker code is able to get in there and establish a beachhead, then they're in your network.  And there are 175,000 networks currently exposed to exactly this threat as a consequence.  Everybody who bought one of these things and just plugged it in and said, oh, look, I can see what the baby's doing when we're not in the baby's room.



LEO:  Me, too.



STEVE:  Wow.  Yeah, exactly.



LEO:  We all can.



STEVE:  And so can Shodan and everybody else.  Yikes.  Okay.  But on the heels of that, there is some very good IoT news.  As I mentioned at the top of the show, a new U.S. Senate bill has been proposed which will set security standards for Internet-connected smart devices and will use the U.S. government's buying power as its enforcement side.  A pair of well-known U.S. senators, one from each political party because this is not a partisan issue, Virginia's Democratic Senator Mark Warner and Colorado's Republican Senator Cory Gardner have introduced last week a new bill titled "The Internet of Things Cybersecurity Improvement Act of 2017."  And they did some homework.  I think they had help with - I want to say Harvard.  I don't think I wrote it down.  But they did get some knowledgeable input, it's very clear.



I read the legislation as a 20-page document.  For example, it forbids any interconnected device purchased by the U.S. government from having hardcoded, unchangeable usernames and passwords in those devices.  The legislation also requires vendors to ensure that their devices are patchable and, at the time they are purchased, are free from already known vulnerabilities.  Firmware updates must have an effective authentication mechanism such as a secure digital signature which is verified to prevent unauthorized updates.  The device must use only - this is what the legislation actually says.  It's wonderful.  The device must use only non-deprecated industry-standard protocols and technologies for communications encryption and interconnection with other devices or peripherals.



The device must be updatable.  Quoting from the legislation, it writes:  "Requires such Internet-connected device software or firmware component to be updated or replaced, consistent with other provisions in the contract governing the term of support, in a manner that allows for any future security vulnerability or defect in any part of the software or firmware to be patched in order to fix or remove a vulnerability or defect in the software or firmware component in a properly authenticated and secure manner."



And it must be repaired in a timely fashion.  The legislation says:  "Requires the contractor to provide a repair or replacement in a timely manner in respect to any new security vulnerability discovered through any of the NIST and other relevant security databases or from the coordinated disclosure program."



So, boy, you know, this is what we need.  It's not law yet.  But as I said, it's difficult to see how this hits an iceberg.  It sounds like something that the House would not have any problem adopting.  It's a nonpartisan deal, so it really feels like there's a chance that the government is, first of all, you know, legislators are waking up to the threat of IOT.  And by placing these requirements on IoT devices, the responsible IoT vendors can easily meet these requirements.  I mean, this is not onerous.  This is just the way it should be done.  It's only because there's been, like the case of the Shenzhen Neo cameras, absolute blatant irresponsibility, the abuse of technology, that this looks like in retrospect a huge change.  But it's the huge change we need.



So hats off to Warner and Gardner for putting this together.  And, I mean, it's everything in there is what we would like to see in an initial first pass at working to improve things.  And you've got to know that the U.S. government is a huge purchaser, and a lot of vendors would like to be able to sell to them.  Again, this will require vendors do what they should have always been doing.  And as a consequence, we'll see security improve.  And, boy, if there could be a certificate program or something so that we consumers could know that this meets the U.S. government purchasing guidelines, and that as a consequence it's compliant with this legislation once it gets pushed into law, that would be a good thing, too.  So anyway, as I said, "Inching Forward," the title of this podcast, because this represents some progress.



LEO:  [Crosstalk], one step at a time.



STEVE:  Okay.  So rooting an Amazon Echo is now, or at least was for a while, a thing.  Mark Barnes wrote last Tuesday, or posted while we were doing last week's podcast, a topic.  He used the "A" word.  I'll say, "Echo, are you listening?"  So this is interesting, but not overly alarming, because it requires physical access - so the so-called "Evil Maid" attack, meaning somebody that has access to your home could get up to some mischief with your Amazon device - and only applies to the earlier 2015 and 2016 model Amazon Echoes, although there are a lot of those that are out there because they were popular back then.



So it turns out - and Leo, on the next page of the show notes I have a picture of the bottom of the earlier Echoes.  Actually, it's the Echoes now.  They just changed the design a little bit.  Under the rubber base of those earlier Echoes, and actually of even the current ones, is an 18-connection, very powerful debugging and access pad which provides, among other things, a serial terminal interface where, if you hook a serial terminal to it and then plug the Echo in, you see all this Linux booting scrolling stuff, and with the names of everything.  So, I mean, its complete internal information disclosure just exported out of a ground and the UART transmit line.  So there's that, and support for a remote SD card booting interconnect.



Together those two things allow for operational discovery.  You're able also to talk to the Echo through the serial input, which allows the device to be monitored, probed, and then ultimately fully commandeered, without opening the Echo any further.  You take the rubber base off.  You press something up against the bottom, and you're off.  So Mark Barnes, who is with MWR Info Security, said that his team developed scripts that leveraged tools embedded on the Amazon Echo to continuously stream raw microphone audio over TCP/IP to a remote server without affecting the usual functionality of the device itself, thus as a proof of concept turning it into an always-on home surveillance device.



Thus, in the fully developed attack scenario, an Evil Maid would disconnect the Echo's power, remove the rubber footing, press a square connector pad against the exposed 18-connection debugging surface - a which would acquire power and access to the internals - then power up the Echo and wait while it boots.  The external SD memory would take over, modify and rewrite the Echo's internal firmware on the fly, turning the device into anything the attacker wished, like a 24/7 audio exporting streaming device.  Then the boot override would be removed, the rubber base reaffixed, and the Echo returned to its original location, essentially having been modified.



So users owning this year's 2017 models of the Echo are not affected by this latest hack because the new models introduced a mitigation that joins two of the crucial debugging pads in a way that prevents the device from that external booting.  So there's still a lot there but you can't do a boot override in order to bypass the internal firmware.



So our takeaway is that this is an interesting attack.  But we've seen these before.  We'll all remember the Samsung television attack where, if you stuck a USB key into the Samsung television, you could also compromise it.  But not remotely.  Not over the network.  Not from the public Internet.  You had to be physically present.  So it, too, was an Evil Maid style attack.  Arguably...



LEO:  Evil Maid with some skills, in this case.  She'd have to build that thing.



STEVE:  Well, yes, but...



LEO:  I mean, they didn't offer that for sale; did they?



STEVE:  No.  And, see, but that's the next step.  And so, I mean, for example, this is the kind of thing we could see the CIA doing, if they wanted to have access to somebody's audio and got a warrant in order to do so - get into the house, modify the Echo, and then leave.  And now it's a streaming device.  But again, not anything to get all worried about.  It requires physical access.  Cannot be done on the newer devices.  And, who knows?  This is very fresh.  It would not be unforeseeable that Amazon would push a firmware update a few days from now that would even disable the ability to override it by SSD, if that's possible to do through a firmware update.  I don't know one way or the other.  I mean, that little 18-pad connector, that is a very, I mean, that's an internal goldmine of stuff.



Now, again, you could also say, okay, all they did was, that is, all Amazon did by exporting those 18 connections is make it additionally easier.  I mean, for example, the Google Home.  It may not export all those pins.  But if you open it up, all that same stuff is there.  And so again, physical access, as we know, always lets you do pretty much anything you want.  So I don't consider this a weakness, which is why there's no reason to run around and get concerned.  It's just, you know, another example of us having a powerful Internet-connected computer in our homes that, if somebody has access to it, can be subverted.  But then all of our computers are that way.



Speaking of the CIA, the drip drip drip of WikiLeaks Vault 7 CIA leaks continue.  There was some coverage of the most recent, I guess we're at 19 now, or maybe it's 20, weeks of disclosures.  This one's called Dumbo.  They all get names.  And again, this is another one that's not a huge concern.  Dumbo is, if we're to believe that these are valid CIA documents disclosing internal technology, it requires system-level privileges on a Windows machine which is responsible for monitoring streaming audio and video devices in some environment, and physical access.  A USB drive must be plugged into the Windows system throughout the operation to maintain control over surveillance devices.



So what does it do?  It can mute all microphones, disable network adapters, suspend any processes using a camera recording device, and then selectively corrupt or delete recordings.  So as I read about that, okay, first of all, this seems like it's not doing anything fancy like we see in the movies, like recording a minute of video with nobody doing anything, and then looping that video inside so that everyone continues to think nothing is happening while they go tiptoeing around, modifying people's Amazon Echoes.  That doesn't happen.  It just basically creates a blackout.



So it has the feeling to me of something that was originally purpose-specific.  It was created to address some specific need somewhere.  Agents needed to shut down an audio/video surveillance facility where a blackout was preferable to being seen and recorded.  So the tool was - and I'm just hypothesizing, of course.  The tool was commissioned and created for that purpose, but then added to the arsenal of possible tools of some future use.  So again, this is not anything like EternalBlue.  But it's sort of an interesting bit of arsenal that it looks like, if we're to believe where all this came from, that our law enforcement and intelligence services have access to.



Okay.  I'm going to explain about Mozilla's very interesting Send service, and then we'll take our last break.  Okay.  So the URL is send.firefox.com.  And Leo, I'll be interested to know what browsers you have that it works on.  Naturally, it works on my Firefox.



LEO:  This is Chrome on Linux, and it seems to work.



STEVE:  Good.  What about Safari?  Because it did not like mobile Safari.  I tweeted that it was browser agnostic earlier today, and somebody sent me a page from it looked like their iPhone where it said, oops, sorry, your browser does not support the required HTTP levels of operation.  So it doesn't look like it runs everywhere.



LEO:  What does it do?



STEVE:  It just puts up a page saying, sorry, this browser won't work.



LEO:  No, but if it works, what does it do?



STEVE:  If it works, you should get a page inviting you to drop a file on it.



LEO:  Yeah.  And then what?



STEVE:  Well, and that's what I'm going to talk about now.



LEO:  It seems to work on Safari on the desktop.  I mean, I'm getting the page.  I haven't tried dropping a file on it.



STEVE:  Yeah, I'm not sure where the - my guess is that the moment it came up it said, oh, sorry, we can't work on your device.



LEO:  Yeah.  Private encrypted filesharing, yeah.  I use, you know, I use a command-line service that does the same thing called transfer.sh.  But it's not run by anybody as well known as Firefox.



STEVE:  Okay.  So here's what this does.  It's very cool.  I vetted the technology.  It's all open source, all on GitHub.  It's another one of the Firefox Test Pilot projects.  They do web experiments to sort of explore things.  It uses AWS as its  backend file storage.  So here's how it works.  Anybody who - and the way I called it, this is not for content distribution.  This is - I call it a "store and forward" because the file stored, it expires after 24 hours or one download.  So it's not meant for somebody putting something up for mass broadcast.  And I think that's a nice tradeoff because it's a free service.



Okay.  So a file, a big file, up to a gig, you're able to drop on the page.  It uses JavaScript running in the browser to generate a symmetric random encryption key and encrypts the file.  It then gets a unique file identifier token from the server and uploads the encrypted file to the server.  So all the server gets is, as we know, when crypto's done properly, is a blob up to a gig, a gigabyte blob of pseudorandom noise that it has no ability to decrypt.  The browser then gives you a URL which is send.firefox.com/download/ and then a one, two, three, four, five, six, seven, eight, nine, a 10-digit, it looks like it's hex, file ID.  Then a /# and the key, that is, the decryption key for that.



Now, so that's what you then arrange to send to someone.  And it's your responsibility to send that securely.  Maybe you're just sending it to yourself.  You could just write that down because it's not impossible to write it down.  The key's not that long, nor is the file ID.  So to transfer a big file between machines, or you want to send a blob to a friend of yours, so you drop the file on your browser.  It transfers it up.  It encrypts it locally, sends it up to the server where it will stay for 24 hours.  You then email your friend this key, maybe change a couple digits and then call him and say, okay, here's how you have to fix that, or leave off some or something.  So you could figure out ways to make it safer to send through email.



The point is this is not meant to be absolute crazy bulletproof security because of the need to send this URL.  But what's cool is, and what's clever, is that - and this is something we've never talked about before.  The pound sign truncates the URL in the browser, meaning that nothing after a pound sign is ever sent by a browser that is querying a URL.  The IETF calls that - formally it's known as the "fragment identifier."  And the IETF spec for the format of a URI says:  "When a URI reference is used to perform a retrieval action on the identified resource, the optional fragment identifier, separated from the URI by a crosshatch character" - the pound sign - "consists of additional reference information to be interpreted by the user agent after the retrieval action has been successfully completed.  As such, it is not part of a URI, but is often used in conjunction with a URI."



So the point is your buddy or yourself, within a day, on a different machine, essentially goes to that URL with any compatible browser.  And it's meant be cross-browser, so it's not Firefox only.  The browser will not send the decryption key to send.firefox.com.  It only sends the left-hand side, including the file identifier.  So that performs the retrieval.  The file is downloaded to your browser; and then, exactly reversing the process of it encrypting the file, it uses that tag after the pound sign to decrypt the file, and the recipient is then able to access it.  And having done that, the file is then deleted from the remote server.



I played with it a little bit.  The crypto looks solid.  And again, I'm not wanting to suggest that this is world-class, absolutely utterly bulletproof.  But it's all open source.  It is inspectable.  And it solves an interesting problem.  It makes it trivial for, especially, somebody not technically astute.  Leo, you were saying you have a command-line system that you use.  Here's something you could tell somebody who has a hard time figuring out that Google is not the Internet to say, "Okay, here.  Drop that file on this page.  You just go send.firefox.com."  That's all you would have to tell them.  "Drop the file on the page, and then send me the URL you get."  And then you'll be able to retrieve the file.



And the key is, it is encrypted in transit, while stored, and can be huge, up to a gigabyte.  And it's free.  So it gets this podcast's full endorsement.  It does everything right, with the understanding that you're responsible for handling that URL responsibly because the decryption key is in the URL.  And you can of course split it apart and send it different ways, or dictate it over the phone, or text message it, or do whatever.  So a very nice piece of work.



LEO:  Yeah, that's cool.  I hope they keep it going.



STEVE:  Yes, as do I.  I think, if it is successful, they probably will.  Oh, and it's worth noting it is open source.  And so anybody who wished to - it is dependent upon AWS.  But anybody who wished to could certainly spin up their own AWS instance.  And as I have been saying, AWS is surprisingly inexpensive.



LEO:  I wonder if they're using the DutchCoders code because that's what transfer.sh is.  And it allows you to run it locally as your own server or with S3.  And I'm wondering if it's related to that code.  Interesting.



STEVE:  Could be.  Although this is not difficult.  I mean, I could write this.



LEO:  No, it's not a hard thing to do, yeah, exactly.



STEVE:  Yeah, yeah.



LEO:  But that code's open source on GitHub and widely available, widely used.  So, yeah, it makes sense to do that.



STEVE:  Cool.



LEO:  Yeah.  And, yeah, if you upload the wrong one, just upload another one, upload the right one, and send a better URL.  Somebody's saying is there a way to delete that file.



STEVE:  Oh, yes.



LEO:  Is there?



STEVE:  And in fact I should say - yes.  First of all, it self-deletes after being downloaded.



LEO:  Right.



STEVE:  And after you have created one, if you go to send.firefox.com, there's a beautiful managed list of all the files you've created.



LEO:  Oh, that's nice.



STEVE:  And the remaining hours and minutes before their self-expiration.  There's a button you can click to manually delete one, if you choose to.  I saw online...



LEO:  That's much better than transfer.sh.  So that's very different.  Okay.



STEVE:  Online, someone said that the key was based on the hash of the unencrypted file.  And I thought, that's interesting.  And so I verified that by doing a transfer twice of the same file and seeing that the encryption key was different both times.  So while it may involve a hash of the unencrypted file, it's not entirely dependent upon it.  And in fact involving the hash would help to overcome any limitations or concerns about the integrity or the quality of the pseudorandom number generator that the browser has access to, although that also has been getting a lot better recently.  So, yeah, so you end up with a beautiful little file manager also where all the things you've created that have not yet expired are shown by name, size, and remaining time before self-expiration, and the ability to remove it if you so choose.  Anyway, very nice piece of work.



LEO:  One gigabyte size limit.



STEVE:  Actually, I think that's a soft limit.  The way they put it was "for reliability."  So I don't know if it actually has a problem, or if you try to send something bigger it just kind of says, oh, boy, ow.



LEO:  [Wordless utterance]



STEVE:  Inching forward.



LEO:  Inching forward.



STEVE:  So another example of why we need to allow research into the operation of things that their owners would rather we didn't look at.  Hotspot Shield VPN, which is a free app and service, has been accused of spying on its users and selling the data.  It was developed by a company called AnchorFree.  It's a free VPN service available on Windows, Windows Phone, Mac, iOS, Android, Chrome.  And unfortunately in this case you get more than you bargain for.  They boast that it's in use by more than 500 million users around the world.  That looks more like a download count, though, so you can't - it's not clear how many people are continuing to use it.  And with any luck, after the news that I'm about to share gets additional coverage, even fewer people will be using it.



The Center for Democracy and Technology, the CDT, is a U.S.-based nonprofit advocacy group who, working with Carnegie Mellon University, carefully examined the Hotspot Shield system and its clients.  Yesterday, that is, Monday, the 7th of August, they filed a 14-page complaint with the U.S. Federal Trade Commission alleging that it is willfully violating its own privacy policy of providing "complete anonymity."  The Hotspot Shield VPN app promises to "secure all online activities, hide the users' IP addresses and their identities, protect them from tracking, and keep no connection logs, while protecting its users' Internet traffic using an encrypted channel."  Which as we know a VPN does when it's done right.



However, according to research conducted by the CDT along with Carnegie Mellon, the Hotspot Shield app doesn't live up to these promises and instead logs all connections, monitors the users' browsing habits, redirects their traffic, and sells their customers' data to advertisers.  Hotspot Shield was even found to be injecting JavaScript code using iframes into web pages for advertising and tracking purposes.  Reverse engineering of the app's code revealed that the VPN uses more than five different third-party tracking libraries.  So it is actively tracking.  It's doing far more than providing an encrypted VPN tunnel.



The researchers also found that the VPN app discloses sensitive data including the names of wireless networks via the SSID, along with unique identifiers such as its users' unique Internet MAC addresses and their mobile device IMEI numbers, none of which are formally and properly part of IP traffic, which is what a VPN conveys.  So things like the Ethernet MAC and the IMEI are local network numbers, not IP-based numbers.  So there's no excuse for those things being captured and transmitted to the other end.



And as if that all wasn't enough, it sometimes redirects ecommerce traffic to partnering domains.  If users visit commercial websites, the app redirects that traffic to their partner sites, including ad companies, to generate revenue.  For example, the researchers found that when a user connects through the VPN to, in the cases they looked at, www.target.com and www.macys.com, the application intercepts and redirects HTTP requests to partner websites that include online advertising companies.



So, as we know, this is all tempered by the fact that not even a VPN can peer into HTTPS traffic.  So some of what they're doing is limited to the traffic that they can see into.  But since they're controlling both endpoints, all the other things like the Ethernet MAC address and the IMEI leakage, which had to be deliberate, can still be done even though you are doing HTTPS web browsing.



So my refrain on this is familiar to our listeners.  Hotspot Shield has the right to do whatever they choose, but they cannot misrepresent what they're doing.  And that's the CDT's only complaint.  If users are truly informed about why their apps and services are free, and how Hotspot Shield is monetizing their use of the free service, then I and the CDT and the FTC would have no problem with that.  But it's not okay to be offering a service which claims to provide absolute privacy enforcement while deliberately and by design violating that promise.



LEO:  How would they, just out of curiosity, how do they know that information's being leaked?



STEVE:  I have no idea.  This is just what I'm...



LEO:  How would you know that?  I guess you could test it by, I mean, there's no way they can monitor what Hotspot Shield is doing; can they?  No, because it's on the Hotspot Shield servers.



STEVE:  They could go to www.target.com both with and without Hotspot Shield VPN and discover that they're getting different content, and they're seeing inserted content.



LEO:  Well, you certainly could see that, yeah, yeah.



STEVE:  Yeah.



LEO:  I guess that would be the only output.



STEVE:  Well, and they did reverse-engineer the app to actually see what it was doing.



LEO:  Ah, okay, okay.



STEVE:  Yeah.  So they took it apart.  And again, this is why this has to be allowed.  Researchers have to be able to verify these sorts of claims and then hold them accountable.  And that's the only thing I want is that they say, okay, this is free, but here's what's going to happen.



LEO:  Right.



STEVE:  Now, of course the people who are using a VPN, there are a range of reasons.  And I could see that many people would be willing to trade a privacy disclosure that doesn't trouble them for the benefit of a VPN that does, for example, allow them to avoid local website filtering and so forth that they might be subjected to.  Whereas others do care about having a VPN service provider who really honors their commitment, and probably for which they pay a fee, in order to, you know, because you have to have a business model that has this make sense.



LEO:  Yeah.



STEVE:  And I did, I went to iOS to the App Store.  And sure enough, Hotspot Shield VPN, free download, and you get what you pay for.



LEO:  Yeah.



STEVE:  I got a kick out of - this is one little bit of errata from last week, Leo.  We were talking about "Colossus:  The Forbin Project."  And I was saying, yeah, I thought that the actor looked familiar to me.  I thought he was a Bond actor.  Anyway, Ed Moreau sent me a tweet saying:  "'Not famous' 'Bond actor' in 'Colossus' has starred in 'The Young and the Restless...'" 



LEO:  Yeah, he's a soap opera guy.



STEVE:  "...soap opera for decades."  So, yes.



LEO:  We now know Steve's secret viewing habits.



STEVE:  No, actually, I have dated women that have been into the daytime soaps.  You know, what was it, "All My Children."



LEO:  He looks familiar to me, too.  I feel like he's been in something.



STEVE:  He does look familiar.



LEO:  But I just - I can't place it.  By the way, Alex Gumpel gave me his - he had the DVD, so he gave me a rip of the DVD so I can...



STEVE:  Oh, good.



LEO:  Because you can't buy it.  You can't get it.



STEVE:  That's gone?



LEO:  Pardon me?



STEVE:  I mean, it's not available.



LEO:  The DVD might be, but it's not streaming anywhere.  It's not on Netflix or anywhere else.  And I suspect the DVD even would be hard to get.  This is a sad, actually, commentary on the state of the situation in the digital age.  There's no reason why every movie ever made shouldn't be available to stream or download.  But the movie industries are not bothering to digitize many great movies.  And so you can buy it on a DVD in many cases, but that's not going to do you any good unless you have a DVD player, and fewer and fewer people do.



STEVE:  Yeah.



LEO:  It's an opportunity missed.  Why isn't any - you don't have to give it away, just why isn't it free to buy?  Why can't you buy it on iTunes?



STEVE:  Right, right, because there's - with storage as inexpensive as it is now, it costs nothing to have it sitting there.  And if a few people are going to - and, I mean, imagine all the people, after listening to us talk about it last week, who would have wanted to watch it for a buck if it was available.



LEO:  Right.  What's going to happen is it's all going to end up in lower quality copies on YouTube, you know, pirated, essentially.  And that...



STEVE:  But that gets yanked down; right?



LEO:  Well, it turns out "Colossus" is up there.  It's not a very good quality rip.



STEVE:  Good [clearing throat].



LEO:  Yeah [clearing throat].



STEVE:  So one piece of miscellany.  Just yesterday I received email from Amazon telling me that the final book of the trilogy of the Rho Agenda was released.  The book is titled "The Meridian Ascent."  And so just to remind people, a lot of our listeners really enjoyed the original Rho Agenda trilogy, which was those teenagers running around New Mexico, finding an alien starship and getting up to some mischief.  Then there was a prequel trilogy that told the story of Jack and Janet and their adventures, which predate their involvement with the teenagers in the middle trilogy.  And now we have then the third sequel trilogy, which is the assimilation trilogy.



I've read, well, I've read them all.  And I've read the assimilation trilogy twice because, when the second one came out, I wanted to reread the first one to remember it all.  And I will read "The Meridian Ascent" when I finish my reread of the 19-book Frontiers Saga.  Now I've just started on book 14.  And it was 18 books, but the other one, the last one, an additional one came out while I was doing my reread.  So I'm closing in on having read it twice because it's so good.  And I get constant feedback from our listeners saying, oh, my god, this Frontiers Saga is the best thing I've read in years.  It's like, yeah, it is.



And speaking of the best thing in years, I have a piece of email from Darren in Brisbane, Australia, from the middle of last month, saying:  "SpinRite SMART data and relative drive performance," which was really interesting.  And there's an interesting takeaway from this that I don't think I've ever talked about before.  He said:  "Hey, Steve."  And I rewrote a little bit of this to paraphrase it for clarity.  He said:  "I've been using SpinRite on my Seagate NAS drives for years, each month or so picking the next drive" - I guess in rotation - "to check.  A few months ago I had a couple of drives give up completely, so I replaced them with Western Digital Red drives."



He said:  "I've been keeping track of the ECC corrected and seek error counts" - that is, those are what SpinRite reports while it's running on the drive.  He said:  "And I've noticed a correlation between SpinRite's reported ECC counts and the time required to perform a full Level 4 SpinRite pass.  The more errors, the longer SpinRite takes."  He said:  "The slowest remaining Seagate drive reports" - let's see, wow - "reports 1,350 million ECC errors."  So that's 1.35 billion ECC errors.



"The next faster drive, which is still slow, shows around 850 million.  But the new WD drives report zero ECC and seek errors, and they also run MUCH faster under SpinRite.  So," he asks, "does this difference in ECC counts indicate that the WD drives' overall performance will be much better than the Seagate drives?"  And he says:  "I will also be very curious how these drives perform with the new version of SpinRite, so I'll join your newsgroup when you get SQRL completed and are back to SpinRite development.  Love the podcast."  He says:  "Have been listening during my commute to work for years.  Keep up the great work."



So Darren, since you're listening, hi and thank you.  And, okay.  So, yes, drives have a nonzero temporal overhead when they encounter the need to correct.  The good news is they can correct.  The bad news is it kicks them out of the flow.  Essentially, drives are hoping for non-error reads, which allows them to maintain their 1:1 interleave, that is, allows them to just move through sectors at full speed.  But if they hit a sector that requires a lot of math, that blows a rotation.  So you will lose at least a rotation while the drive stops and needs to perform CPU-intensive math.  I mean, there's a lot of hardware assist, but again it knocks you - if you lose the ability to start reading that next sector, you've got to wait for the drive to spin all the way around again.



So there is definitely a performance hit from the need for ECC and/or reseeking.  A seek error is where the drive just goes to where it believes a sector is going to be, waits for the first sector header to come along which identifies it uniquely, and then goes, ooh, crap, I'm on the wrong track.  And then it has to move to a track on either side, if it just landed in the wrong place.  So all of those things slow it down.



Now, the problem is SMART data is not guaranteed to be provided.  The main parameters that SpinRite shows on those bar graphs, where we showed a picture a few weeks ago where one of the health parameters had been pushed down about halfway.  That's 100% assured to have meaning.  What SpinRite is doing is interpreting other data which the drive sort of leaks.  And that's where SpinRite is able to get something like the ECC corrected rate and total, which no other utility does.  But because it's not part of the formal SMART spec, we can't rely on it.  So if it's there, we interpret it and show it.  But it doesn't have to be there.



So the fact that the WD drive shows zero ECC and seek errors may well be that its design doesn't publish that leakage information which SpinRite picks up and reports, if it's available.  But what you can count on is speed.  So the fact that those WD drives are completing a SpinRite Level 4 pass in a fraction of the time relative to their size of the older Seagate drives, that strongly indicates that in fact the WD drives are not encountering nearly the number of error correction or seek errors that the older Seagates are.



So for what it's worth, if you're in a situation where you do have a bunch of drives, make note of SpinRite's given level total execution time because that's a very accurate measure of what's going on in the drive.  The lower that is, the better, even if the drive isn't exposing SMART data, which gives you another dimension of numeric value to lock onto.  So, neat question, and one that even now, after 12 years, we've never had before.



Okay.  So a bit of closing-the-loop feedback with our listeners.  Chris Beattie, whose handle is a contraction of "jabberwocky," says:  "Why are enterprises fussing" - he's referring to that TLS v1.3 perfect forward secrecy stuff we talked about a couple weeks ago.  "Why are enterprises fussing over TLS v1.3 perfect forward secrecy when they're surely doing man in the middle for internal traffic inspection anyway?  What am I missing?"



Well, Chris, I wanted to clarify this for you and others.  It's data centers who are concerned, not enterprises.  So I wanted to just make sure that I made that clear.  It's data centers whose internal networking monitoring topology they claim requires that they be able to see into the traffic transiting through their center.  And the counter argument is, well, but if it's going, if it's passing through you, then tough luck because there's nothing you can do.  But if it's terminating in a server in your data center, then it's decrypted at that server.  And so, again, seems like a gray area.  But anyway, it is data centers that are saying the nature of their networking needs are such that they have to be able to decrypt.  Wow.  But not enterprises.  As we know, enterprises, as Chris says correctly, are probably already doing that with their own middleboxes.



Craig Naples says:  "If we aren't mainly" - oh.  He says:  "If we are mainly relying on ISPs to block ports exploited in SMB attacks, would using VPNs to bypass ISPs leave us more exposed?"  And that's a great point.  So as I mentioned, most ISPs are blocking those ports - 137, 138, 139, and 445 - which are what SMB has historically used.  Some have been dropping their filtering and allowing that traffic through, so we can't count on it.  But as Craig notes, a VPN which bypasses all the things the ISP does would remove those blocks because our traffic is no longer subjected to them.



However, the good news is the VPN, because it looks like a network adapter, should be on the outside of our system's firewall.  That tunnel, the VPN tunnel would be also transiting the NAT router.  So it would lose the NAT router's protection, which would normally be our second line of defense after the ISP.  But because the VPN is typically implemented as a pseudo network adapter, it's probably on the outside of our system's firewall.  But it is worth checking.



One thing you can instantly do is fire up your VPN and go to ShieldsUP!.  You should be, let's see - oh, no.  That won't work, sorry, because we will be checking the VPN endpoint, the server that your traffic is emerging from, not the server at the end of the VPN tunnel.  That's actually a common source of confusion for people who do use ShieldsUP! and wonder why they're seeing all kinds of open ports.  It's because ShieldsUP! looks at the IP from which the traffic is emerging.



So anyway, I don't think, without - you'd have to have someone probe your port - I'll have to think about that.  I didn't think this through all the way, whether unsolicited public traffic could get back to you.  Depends upon what the VPN server's doing at its end.  You're probably safe.  And you also have the firewall, the local system firewall to protect you, as well.



And in a very nice piece of news Anthony Headley sent, he said:  "From the latest SN, you should probably switch to this command-line interface" - which I didn't know of - "aws.amazon.com/cli."  And it looks wonderful.  That page is titled "AWS Command Line Interface."  I didn't know there was one.  It describes itself:  "The AWS Command Line Interface is a unified tool to manage your AWS services.  With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.  The AWS CLI introduces a new set of simple file commands for efficient file transfers to and from Amazon S3 and runs under Windows, Mac, and Linux."  So aws.amazon.com/cli.  Thank you, Anthony, and for anyone who's interested.



Oh, and I also did, I was curious, I got my most recent S3 bill, and it was less than it had been.  Dropped down to $2.08.  And so I thought, okay, that's enough.  I've got to find out how much data I have stored there because maybe it's not that much.  So while I was putting the show together last night, I went to AWS, and they had something called the "AWS Cost Explorer."  And so I said, oh, show me.  And it said it'll take 24 hours for us to generate your - to, like, to populate your report from your request.  So it hasn't been 24 hours.  I just checked a few minutes ago, and it still says the same thing.  So next week I will be able to say, okay, I am paying two bucks a month, and this is how much data I have up there.  I have no idea how much, but I'm going to find out.  So I'll be able to give people some sense for that.



Seth Meister tweeted:  "Love SpinRite.  Is there an easy way to run it on an external hard disk drive while actually using the Windows computer it's plugged into?  Thanks."  Okay.  Now, the question, I guess, is easy.  It's definitely possible, and many people do.  If you fire up a VM, a virtual machine, and give it exclusive access to that external drive, then it looks like a physical drive, and SpinRite will run happily.



And this is actually far more practical than it might seem at first, since SpinRite and DOS is all you need, and they can run happily in a VM with 640K, yes, K, 640K, two thirds of a meg of RAM.  And it will use virtually zero of your hosting machine's CPU.  We've seen and shared photos on this podcast of people running even multiple VMs with multiple drives all running at once while they're using their computer.  So it's not a mode that we formally support at this time.  It's certainly something I'm going to look into for v7, once all of the 6.x technology is developed.  But in the meantime, SpinRite 6 can run in a very tiny VM and allow you to use your computer while it's working in the background.  It works perfectly.



Rene, looks like Feliu, he said:  "If ransomware attacks a system, does it upload your data?  If I get attacked, should I assume my files could be copied to the bad guys?"  Okay.  So Rene, the lesson here, and we've talked about this before, the rule of thumb is, if any malware gets into your system, you no longer know anything about it.  That is, we know from rootkits that they're able to hide.  We know that malware removers remove malware which then returns mysteriously.



So the problem is, I mean, the strict purist answer is, if a system is ever infected with something malicious, you can never absolutely trust it again.  No one can tell you how little you should trust it, except you can't absolutely trust it because, you know, the only thing you could do would be to restore from a pre-infection backup.  If you have a restore point and the malware didn't mess with that, you could restore from an earlier restore point.  But you just - you're never going to know again after that has happened.



So classic ransomware does not upload.  It merely encrypts.  So that's the deal.  It's just encrypting your data.  If everything goes well, and you decide you need it back because you didn't have a backup, you pay them the ransom, and then they give you a key that lets you decrypt it.  But still, somebody's been messing with your data.  So you have to make a judgment on how important your data is, how crucial the privacy is, and how much you can trust that the malware didn't do anything else.



And I should note we have discussed malware, ransomware, that does more, that is also a trojan, that also leaves stuff behind, that also opens a backdoor.  So ransomware is no longer just ransomware.  And so that further strengthens the theoretical argument that, once your system's been compromised, anything could be in it.



Gary S. Martin asked of my discussion of S3 storage:  "What encryption solution are you using?"  He said:  "You mentioned Duplicati in 2012 and Boxcryptor in 2014.  Either of these?"  And I should say, I'll take this opportunity to mention that I've switched to CloudBerry.  CloudBerry Lab are the guys.  I've discussed them before on this podcast, and in fact they quote my discussion of them on this podcast on their site.  They did the encryption right.  They offer a free version.  They offer, for $30, a one-time pay for the software that does 256-bit encryption and Trust No One operation.  It is massively cross-cloud platform.  I mean, the list is just huge.  They've got, like, the most popular, the also, and then everybody else.



I mean, and I should also mention they're what I use at GRC.  It is CloudBerry Lab.  I mean, they have ranges of plans from the individual to enterprise, and that's what I use for securely backing up GRC's servers at Level 3.  So they're the people.  As I was referring to earlier, I don't like this paying for no benefit for a software that I could purchase.  Like you, Leo, I immediately purchase lifetime subscriptions for my TiVos because I love my TiVos, and I want to use them forever.  That's just me.  But Boxcryptor used to have a legacy version which you could still purchase, while they had the subscription plan.  Now the legacy is no more.  So it's like, okay, fine.  CloudBerry is now my company.  So that's the one I'm using.



LEO:  Good to know.



STEVE:  Jeremy Malone asked:  "What we need for your barcoded voting paper is the 'turbo entabulator' so the results are even faster."  And I'll just note, I thought of that because somebody made a very good point, and unfortunately I didn't catch his Twitter handle go by.  So anyway, he noted that human-readable barcode, or I'm sorry, human-unreadable barcode could also be hacked.  So he asked the question:  "Wouldn't a better solution be a human-readable paper output?"



And that's a very good point.  I got a little carried away with my secret interchange language between the thing that runs the UI and generates paper, and then the turbo entabulator, which sucks that in.  Whoever said that is 100% right.  Better to produce a small paper readable summary in a format that an optical scanner can then read with high integrity, so that anybody can verify the paper output.  There then the machine is just a UI to produce this high reading integrity, but still human-readable piece of paper.  So great thought, Jeremy.  And the person who - or a great question, Jeremy, about the entabulator, and also the person who said, hey, it ought to be human readable.



And lastly, Norbert Boron said...



LEO:  What a great name, by the way.



STEVE:  Norbert Boron, yes.



LEO:  I want that name.



STEVE:  He said, well, and this is why our listeners are so great:  "If you rename Special Episode 85a to 85, and decrease all prior episodes..."



LEO:  Yeah, not going to happen.



STEVE:  "...by one, then you'll have Episode 0."  So very clever, Norbert.  Thank you for that.



LEO:  We had a couple places where we didn't - I forgot why that happened.  Probably because we did something wrong in the feed, and earlier there was no way to fix a thing without pushing a different...



STEVE:  Either that, or I think we once did a special episode.



LEO:  Maybe.



STEVE:  Of, like, something horrible happened, or wonderful, or something.



LEO:  Oh, maybe that's what happened.



STEVE:  I think it was like a - we were like, oh, my god.



LEO:  Like an extra episode.



STEVE:  Coming to you live because...



LEO:  Yeah, it was, it was a special episode of Security Now! to warn and inform listeners of a serious zero-day bug in XP and Vista.



STEVE:  Ah.  Now we do those every day.  That was...  



LEO:  The animated cursor vulnerability.  Dit dit dit, dit dit dit, dit dit dit.  This just in.  It was on April 2nd, which makes me a little nervous.



STEVE:  Oh, I'm sure I was skeptical even then.  It was like, uh, okay.



LEO:  It's only an 11-minute episode.  So 11 minutes.  Can you imagine?



STEVE:  No, I can't.



LEO:  Most of our ads are that long now.



STEVE:  Yeah, that's time for me to take a sip of coffee.



LEO:  Eleven minutes.  We couldn't even get started in 11 minutes.



STEVE:  Ah, the good old days.



LEO:  Ah, the good old days.



STEVE:  When a zero-day actually merited an emergency broadcast.  Now it's like, yeah, we'll get to that next Tuesday.  Yeah, we'll...



LEO:  Little did we know.  Little did we know where we were heading.



STEVE:  Yup.



LEO:  That's fun.  Well, I don't think we can easily renumber this without...



STEVE:  No, we're not.



LEO:  We'd have to do a whole big Apache mod rewrite to say, well, if you ask for this, get that.  And, oh, man, it would be...



STEVE:  No.  But I appreciated Norbert's note that there was a buckle in the flow, and that we could remove that little kink and push all the earlier ones down.  And then, with that, the Honey Monkeys would then be number one.



LEO:  Be zero, Episode 0.



STEVE:  Be number zero, just belong.



LEO:  Yeah, Honey Monkey Zero.  It's where it belongs.  Yeah, I think we'll keep it the way it is.  I don't want to break all those scripts and so forth.



STEVE:  Just appreciated his observation.



LEO:  Yeah.  Yeah, we did, so we did an Episode March 29th, and then we burst in on April 2nd to bring you this breaking news.



STEVE:  Zero day, oh, my.



LEO:  A flaw in Vista.  Imagine that.



STEVE:  It's like the sky is falling.



LEO:  We'll talk about "Game of Thrones" some other time, off the air, Steve Gibson.



STEVE:  Off the air.  We're not into spoiling it for anybody; but, boy, I'm [glitch] twice now.  They have been so good this year.



LEO:  Our show is available for you to download.  If you want to watch it live, you can.  We do it every Tuesday, right after MacBreak Weekly, which is around 1:30, if you want to watch the live stream.  It's really watching the live production of it.  It's not a TV channel.  You're not watching a live show, you're watching the production of it, 1:30 Pacific, 4:30 Eastern - I just don't want anybody's hopes to be all excited - 20:30 UTC.  The live stream is at TWiT.tv/live on our website.  You can also chat with us at irc.twit.tv.  A lot of chatters during this show, talking about this stuff.  It's great.  This is, by the way, what did you say, our 13th year?



STEVE:  We're closing in on the end of 12.



LEO:  Okay.  So this isn't beginning the 13th yet.



STEVE:  Correct.



LEO:  Soon.  Next week?



STEVE:  I think the 17th or - I looked a couple weeks ago because I thought, I think it's around this time of the year it keeps happening.  What was Honey Monkey's time?



LEO:  I will tell you the date of Honey Monkeys.  It was April, I'm sorry, August 18th, 2005.



STEVE:  Yup, 18th.



LEO:  So on the 18th we enter our 13th year.



STEVE:  So not the next one - oh, wait, yeah.  Next one will be the last episode of Year 12.



LEO:  So this is the penultimate episode.



STEVE:  Yes, the penultimate episode.



LEO:  Of Year 12.



STEVE:  Hang on for the ultimate.



LEO:  Volume 12 is almost over.  You can get the show in a variety of places.  Start with Steve's site, GRC.com, because not only do you get the audio, it's the one and only place you can get the transcripts.  They're nicely transcribed out.  It takes a few days.  But by the time they're up there, that's a very nice resource.  It's also searchable.



STEVE:  It's usually Thursday morning Elaine mails the transcript, and then I get it posted.



LEO:  And then he also has so much great stuff there, including his bread and butter, SpinRite, the world's best hard drive recovery and maintenance utility.  But he also has lots of free stuff there.  There's Steve's...



STEVE:  Everything else.



LEO:  What was it, a mind ranging - what was that quote about Newton?  I can't remember it exactly, but a free mind ranging widely.  And that's him, right there, at GRC.com.  That's the home...



STEVE:  [Crosstalk] that some of our listeners know. 



LEO:  Yes, they all know well.  You also will find audio and video, we have video for some reason, on our site, TWiT.tv/sn.



STEVE:  With you talking like that, it sounds like it won't be forever.



LEO:  Some reason, I don't know why.  Well, people like to see it.  And I think it gives them a little - not every episode.  Every once in a while they watch the video just to make sure we haven't changed too much.



STEVE:  It makes it seem more real.



LEO:  Yeah.  It's more real.



STEVE:  They can check on the state of our hair.



LEO:  Yeah, yeah.



STEVE:  Yours grew back.



LEO:  Yeah, you shaved your head, and it never grew back.  TWiT.tv/sn is where you'll find this show.  But you know the best thing probably is to get one of those podcast programs, there are so many of them out there, and subscribe so you get the complete set.  Wouldn't you like the complete set?  Sure you would.  Thanks for being here.  We'll see you next Tuesday, barring a zero-day on Windows XP, on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#624

DATE:		August 15, 2017

TITLE:		Twelve and Counting

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-624.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we have a Marcus Hutchins update and the back story on the NIST's rewrite of their 15-year-old password guidance.  Can DNA be used to hack a computer?  Can stop sign graffiti be used to misdirect autonomous vehicles?  We discuss the final nail in the WoSign/StartCom coffin, why we need global Internet policy treaties, this week in Researchers Need Protection, a VPN provider who is doing everything right, ElcomSoft's password manager cracker, a bit of errata and miscellany, and some closing-the-loop feedback from this podcast's terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to, among other things, of course all the security news, but debunk a couple of stories people saw in mainstream media about hacking a computer with DNA - I fell for that one - and putting stickers on stop signs to confuse autonomous vehicles.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 624, recorded August 15th, 2017:  Twelve and Counting.



It's time for Security Now!, the show where we cover your security and safety and privacy online.  And we couldn't do it if we didn't have the best guy in the biz on this subject, Mr. Steven "Tiberius" Gibson.  He's sitting right there in the Skype window.  Hello, Steve.



STEVE GIBSON:  Leo, great to be with you again.  We're starting a little late, but it ended up working out well because the leaf blowers made their pass, and the weekly garbage truck has already done its beep beep beep beep.



LEO:  Oh, we really are late, then.



STEVE:  So it's all worked out well.



LEO:  Good.



STEVE:  So here we are, Episode No. 624, which we're recording on August 15th.  And we recorded Episode 1 on August 18th of 2005.



LEO:  Oh, boy.  Wow. 	



STEVE:  Which means that between this show and our next show we are lapping ourselves for the 12th time.  So I titled today's podcast "Twelve and Counting."



LEO:  Nice.



STEVE:  Since we are finishing out our 12th year with this 624th podcast.  And then on to Lucky 13, which I think will be a busy one.  So we've got a lot of fun stuff to talk about and, I think, another great couple hours for our listeners.  We've got an update on the Marcus Hutchins versus the government situation.  The back story on the NIST's rewrite of their 15-year-old password guidance, which was making news last week; but I thought, you know, we talked about this from the NIST actual document back in June when this happening.  But it's generated so much popular attention because, of course, passwords are everything for most people, that I thought, okay, we've got to talk about this a little bit more.  And there is sort of a fun back story behind it.



Then the other week's top annoying headline leader was "Can DNA Be Used to Hack a Computer?"  So we'll discuss that.  And also, can stop sign graffiti be used to misdirect autonomous vehicles?  Oh, my lord.  We'll talk about that.  Then we've got the final nail in the WoSign - the woebegone WoSign/StartCom coffin.  Why we need global Internet policy treaties.  This week in Researchers Need Protection.  A VPN provider who is doing everything right, as opposed to our discussion last week of Hotspot Shield VPN doing everything wrong.



We've got something that worried a lot of our listeners, the ElcomSoft announcement of their password manager cracker that of course targets the top four password managers - 1Password, LastPass, and a couple others.  We'll talk about what that actually means.  And of course they're the guys that provide all of the rooting stuff and hacking for law enforcement into smartphones and so forth.  You know, we've talked about ElcomSoft before.  Then we have a bit of errata, some miscellany, and a bunch actually this week, time permitting, of closing-the-loop feedback from this podcast's terrific listeners.  So I think two hours from now, as we say goodbye to Year 12, we'll have another great podcast in the can.



LEO:  Nice.  Cannot wait.



STEVE:  So our Picture of the Week, our friend of the show Simon Zerafa sent this to me last week, and I had to preempt it for the cell tower diagram from pre- and post-DEF CON.  But this is just sort of - it's fun.  It's the security questions fill-out form, and the picture says:  "Select three security questions below.  These questions will help us verify your identity should you not have access to Google Authenticator app."



So the first question is:  "As a child, what did you want to be when you grew up?"  And because I'm a developer, and I care about these things, I immediately recognize what's been filled in as the format of a UUID, more easily pronounceable as a globally unique identifier, thus a GUID, because of the grouping of the hex.  It's actually 128-bit value which is generated by systems, and there's even an online GUID generator.  Some of the groups of digits are 32-bit time, so high resolution.  There's a unique system identifier and other stuff.  And, for example, things like your Ethernet MAC address, which we know is designed to be globally unique, it often forms part of that.



So the point is that the concept of the GUID or the UUID is that independent people with no communication are able to independently create a token which nobody in the past or the future will almost certainly have, so extremely low collision tokens, and 128 bits gives you a high probability of being able to pull that off.  And so anyway, these three questions were not filled out with the actual answers, but with three GUIDs, which are just basically pseudorandom nonsense.  And so nobody's going to be able to brute force them.  Now...



LEO:  Where did they get them?  You think this is like this guy's MAC address on his, I mean...



STEVE:  Yeah.



LEO:  There are GUID generators, I guess.



STEVE:  Yes.  For example, Microsoft has a GET GUID API, and it just gives you back one.  And every time you ask for one, it gives you another one.



LEO:  Right.  I would just use LastPass's password generator and fill it in.



STEVE:  Yeah, that's fine.  There are also online generators.  And you're right, Leo, just any blob of noise is going to be equally good.  You can also go to GRC.com/passwords, as a startling number of people still do.  Something took me, by the way, to the Haystacks page the other day, to 4,100 uses of the Password Haystacks page per day.  So that's a popular little puppy.



LEO:  Wow.  What does that thing do?



STEVE:  Anyway, so just sort of a fun picture, the idea being, yes, you know, because you and I have been talking about this whole - the problem with this kind of information which has, if you fill out the name of your favorite pet, the street you grew up on and so forth - you can see if you go all the way down to the bottom at the lower right corner, I show the running daily average over the past seven days.



LEO:  I realize I was just on this page, the Password Haystacks page, and I know why you're getting a lot of traffic.  It's because of this NIST thing.  People are going to test "horse stapler," whatever it is, versus - this lets you test how much entropy is in your password, how long it would take to do a brute-force crack of it.



STEVE:  Right.  And it sort of gently teaches people about the concepts of the size of your alphabet and so forth.  



LEO:  Yeah, yeah.  So it's for fun, but I think that's probably why you're getting traffic right now is because of the thing you're about to talk about.



STEVE:  Very good point.  Yeah, that's a very good point.  That makes sense.  Okay.  So Marcus Hutchins, our friend who created the WannaCry sinkhole, who with all good intentions went to Black Hat and DEF CON to just participate, screwed around a little bit in Las Vegas and, as he was at McLaren, which is the Las Vegas airport, got essentially picked up and arrested by agents of the federal government saying "We think you're a bad guy."  We've talked about him the last couple weeks.  So an update.  As we said before, he was going to plead guilty at his arraignment in Milwaukee, and he did.  Afterward, one of his attorneys...



LEO:  He pled guilty?



STEVE:  I'm sorry, not guilty.



LEO:  You scared me.



STEVE:  He pled not guilty.  Yeah, so pled not guilty.  Afterwards, one of his attorneys, Marcia Hofmann, called him a hero.  Of course she's on his side, so yes.  But we all agree.  Those people, as we said last week, as we were sharing tweets from people who were shocked and appalled that this had happened to him, who like really know him, that they feel this is ridiculous.  And of course the concern is, okay, we don't have all the information that the federal government at this point has.  And it's conceivable, I mean, if we want to try to explain, come up with a theory of their case for them, I don't think that's our job, but maybe things in the more distant past, he was different back then.  And, you know, I got up to some mischief myself back when I was a teenager.



LEO:  Well, who didn't, you know, I mean, come on.



STEVE:  That happens.  So Marcia called him a hero, said he would be fully vindicated.  And in a somewhat interesting change of tone in the government, like their change of tone suggested that something like that may well be the case; whereas at Hutchins's Las Vegas hearing the government used, for example, his appearance at a tourist-focused gun range, as if that had like some nefarious undermeaning, in their attempt to deny him bail.  But now the government was amenable to lifting many of the restrictions on his release conditions.  He can't go back to the U.K., so I'm sure they have his passport.  But he'll be able to live in Los Angeles where his other attorney, Brian Klein, is located.  He'll be able to continue working and can travel throughout the U.S.  But as I said, he cannot leave the country to return home.



The only other restriction, which is odd, aside from GPS monitoring, so I guess I don't know if it's an ankle bracelet or something in his pocket, whatever, is that he - and this is the thing that's so weird.  The government stipulated he cannot touch the WannaCry sinkhole, which seems oddly random because, like, what?  He's going to turn it off and release WannaCry to begin reproducing again?  I mean, no.  In fact, I mean, I don't even know if he still has control of it.  This sort of seems like something where somebody would have said, okay, Marcus, we need to transfer this domain to official government control because this thing is too dastardly to leave in an individual's hands.



Anyway, the government's attorney, whose name I cannot pronounce - Michael, that's easy.  Maybe it's Chmelar, looks like C-H-M-E-L-A-R.



LEO:  All on you today.  I'm not going to help you here.



STEVE:  Chmelar described...



LEO:  Chmelar.  Chmelar.



STEVE:  Thank you.



LEO:  No, I'm making that up.  I have no idea.



STEVE:  Every time I pause, you just interject that, Leo.



LEO:  Chmelar.



STEVE:  Described Hutchins's alleged crimes as "historic," okay, but as in historical, rather than as in historically sized.  So that sort of sounds like our theory that we started teasing last week, was that maybe this was something from, like, 12 years ago, or maybe - I don't know how old he is.  Maybe five years ago.  But like not who he seems to be today.  His trial is currently scheduled for this October, but there's some designation that trials can have known as "complex."  And the government has made some insinuations that they're going to move to obtain a complex designation for the case which would then likely cause the date to slide, like into next year probably.  So we'll sort of keep an eye on this and see where it goes.



LEO:  Meanwhile, he's in jail.  He can't, you know, this is appalling.  I'm sorry.  Now I'm getting upset.



STEVE:  I don't know when he's going to be released, but...



LEO:  Is he going to be released?



STEVE:  I think he was released yesterday.



LEO:  Oh, okay.



STEVE:  This all happened yesterday.  And so he can't go home, but he has free range within the U.S.  So I'm sure that...



LEO:  Well, that's what I mean, he's stuck.  He can't go back to work.



STEVE:  He can go eat where he wants and not have to have jail food and have scary...



LEO:  [Crosstalk] chills.  Nobody's going to ever come here.



STEVE:  No, no, no, no one is suggesting this is good.  And of course you're right.  So the first chilling effect was the arrest, and this doesn't make it any better because, I mean, now he's in the system, as they say, and he's got a GPS monitor on his ankle probably, and his life has been wrecked for the foreseeable future.  So the good news is this has gotten a lot of attention.  And as I said last week, once the facts are known, it's very clear that he will have access to every bit of quality of defense that he deserves.  And let's hope he deserves a lot.



LEO:  Yeah.



STEVE:  So, okay.  We talked about when the NIST, the National Institute of Standards and Technology, updated their password guidelines.  I jumped on it immediately a couple months ago because the one big issue there was one we've talked about often on this podcast because it was just like fingernails on a chalkboard for me, this ridiculous, apparently arbitrary, you must change your password every N days, or N where N is a small number of months, whatever.  I mean, it was just like, why?



I mean, and we've spent time over various points during the last 12 years trying to reverse-engineer this guidance.  It's like, what possible value could it have?  Yes, long passwords, we understand.  Yes, mixed types of passwords, upper/lowercase, special characters, numbers and so forth.  In other words, encourage people to try to make better passwords, higher entropy, less easily brute forced.  All of that makes sense.  But why force people to change their passwords?  And even years ago when we first talked about this, there are in Microsoft's code in the group policy, you can do things like you can set policies where passwords expire every X days, and N number of prior passwords are remembered, specifically - and N is like five - specifically to thwart people's ping-ponging between two different passwords every two months or month.  It's like, oh, no, we're not going to let you do that, either.  We're going to make you have five.



And so people are like, I mean, you know, this is water cooler talk.  Once the algorithm gets out in the coffee room, everyone knows, okay, whatever time of the month it is for your password policy.  It's like, I'll change it to add a one, then I'll add a two, then I'll add a three, then I'll add a four, then I'll add a five, and now I'm back to five, and basically I've subverted the entire policy because screw you, IT department.  So anyway, the good news is that disappeared after 15 years.



So the funny back story here is where those guidelines came from.  They came from a guy named Bill Burr.  And now he says, whoops, sorry about those bad password recommendations everyone's been living with for nearly 15 years.  Back in 2003, as a midlevel manager at the National Institute of Standards and  Technology, Bill was the author of "NIST Special Publication 800-63, Appendix A."  Okay, just say "bureaucracy."  The eight-page primer advised people to protect their accounts by inventing awkward new words rife with obscure characters, capital letters, and numbers, and to change them regularly.



Today Bill, who is now 72 years old and retired, said that:  "Much of what I did I now regret."  The Wall Street Journal wrote:  "The document became" - the original document from 2003, wrote The Wall Street Journal - "became a sort of Hammurabi Code of passwords, the go-to guide for federal agencies, universities, and large corporations looking for a set of password-setting rules to follow."  The problem is that the advice ended up being largely incorrect, Mr. Burr said.  Change your password every 90 days?  He laments what we have often noted on this podcast:  Most people being periodically forced to make an unwanted and unneeded change will make only minor changes that are easy to guess.



So a couple months ago in June, as we covered at the time, Special Publication 800-63 got a thorough rewrite, jettisoning the worst of these password commandments.  Someone who's there, Paul Grassi, an NIST advisor who led a two-year-long do-over, said the group thought at the outset, two years ago, when this project began, that the document would require only a light edit.  But two years later, Mr. Grassi says:  "We ended up starting from scratch."



The new guidelines, which are already filtering out, thank goodness, through to the wider world - and two months ago, at the time when this happened, I said, okay, yay.  Now if this is like, for some reason, the tablets from on high, then fine, at least they've been edited.  And now we have new tablets; and people can say, well, this is what the NIST guidelines are, so don't fire me.  Thank you.  So they're already filtering out through the wider world, writes the Wall Street Journal, and drop the password expiration advice and the requirement for special characters, interestingly.



Mr. Grassi said:  "Those rules did little for actual security and had a negative impact on usability."  Of course many people, they put in their normal password - unfortunately they have one, "monkey" - and then the system says, oh, you must have a digit and a special character.  So they add an exclamation point and pound sign on the end, just to satisfy the annoying incoming password filter.  So, okay, that's gone.



So a little on Mr. Burr's background.  He once programmed Army mainframe computers during the Vietnam War.  He had wanted to base his advice at the time on real-world password data, a sound approach.  We've looked, for example, at disclosed password lists, and from those breaches we've learned a lot about how horrible passwords are and the surprising dropping in the popularity of the word "monkey" as everyone - I think it was used, you know, it was way up there for a while.  Not quite up at where "password" is, but close.



But back in 2003, when Burr was tasked with this job, there wasn't much available real-world password data to work from.  So he was under pressure also to publish this guidance quickly because of course we have to have it now and then be stuck with it for 14 years, right or wrong.  So he asked the computer administrators at the NIST if they would let him have a look at the actual passwords on their network.  Well, they refused to share them.  And he said, citing privacy concerns:  "They were appalled that I even asked."



So with no empirical data on password security to be found, he leaned heavily on a whitepaper written in the mid-'80s which was pretty much the only source material that he could find.  A guy named Cormac Herley, who's a principal researcher at Microsoft who the Wall Street Journal pulled in for some comment, said that, collectively, humans spend the equivalent of more than 1,300 years each day typing passwords.  Well, we don't because we have wizards that do that for us.  "Microsoft once followed the Burr code for passwords," said Cormac, "but no more.  The NIST rules were supposed to give us randomness.  Instead, they spawned a generation of widely used and goofy-looking passwords such as Pa$$w0rd or Monkey1!."  Cormac said:  "It's not really random if you and 10,000 other people are all using it."



So anyway, I got a kick out of some of the coverage.  The idea that these rules were changing, of course, generated lots of interest in the wider public press.  And it was nice to see, okay, how did we get stuck with this bad advice for so long?  Well, now we know.



And speaking of bad, boy, I tell you, this was - the headlines were crazy for this one, and I have three of them here:  "You can hijack a gene sequencer by hiding malware in a DNA sample."  Or "Biohackers Encoded Malware in a Strand of DNA."  Or "Scientists Hack a Computer Using DNA."



And then that particular one starts out:  "In what appears to be the first successful hack of a software program using DNA, researchers say malware they incorporated into a genetic molecule allowed them to take control of a computer used to analyze it.  This biological malware" - oh, help me - "was created by scientists at the University of Washington in Seattle, who called it the first 'DNA-based exploit of a computer system.'"  Oh, wow.



Okay.  To carry out the hack, researchers led by a team "encoded malicious software in a short stretch of DNA they purchased online.  They then used it to gain 'full control,'" in quotes - and believe me, there's a lot of detail to be discussed here in a second - "over a computer that tried to process the genetic data after it was read by a DNA sequencing machine."



Okay.  So last Thursday all of this ridiculous, over-the-top nonsense was the popular press's interpretation of a 15-page paper which appeared at the USENIX security conference last Thursday.  The paper was titled, and I have it, and I read it, "Computer Security, Privacy, and DNA Sequencing:  Compromising Computers With Synthesized DNA, Privacy Leaks, and More."  Okay, so that's not a modest title.  Here's what these guys - okay, well, first I'll explain what they did.  But here's the abstract from the top of this 15-page research paper.



LEO:  And in my defense, because I did this story last week, and a lot of people did, all the coverage preceded Thursday's paper.



STEVE:  Right, right.



LEO:  So you have an advantage that we didn't have.  All we had was the abstract.  Go ahead.



STEVE:  Okay.



LEO:  And I fell for it.  I did.



STEVE:  I missed your coverage.  So the abstract says:  "The rapid improvement in DNA sequencing has sparked a big data revolution in genomic sciences, which has in turn led to a proliferation of bioinformatics tools.  To date, these tools have encountered little adversarial pressure."  Okay.  I'm completely - so I'm not saying this is not useful, but this is not what the press said.  "This paper," they say, "evaluates the robustness of such tools, if or when adversarial attacks manifest.  We demonstrate for the first time the synthesis of DNA which, when sequenced and processed, gives an attacker arbitrary remote code execution."  Oh, boy.  Yes, but with more caveats than even I could cover on the podcast.  But we'll get there in a second.



"To study the feasibility of creating and synthesizing a DNA-based exploit, we performed our attack on a modified" - that's an important word - "modified downstream sequencing utility with a deliberately introduced vulnerability."  Okay, so they created something that would be deliberately vulnerable which, when it encountered their purpose-built DNA snippet, would crash.  Oh, heavens.  Okay, well.



"After sequencing, we observed information leakage in our data due to sample bleeding."  Which is really not the term you want to use when you're sequencing DNA, but anyway.  "While this phenomenon is known to the sequencing community, we provided the first discussion of how this leakage channel could be used adversarially to inject data or reveal sensitive information."  Which is a little bit of a stretch, but okay.



"We then evaluate the general security hygiene of common DNA processing programs" - certainly that's useful - "and, unfortunately, find concrete evidence of poor security practices used throughout the field."  Bravo.  So if nothing else, this will drive the implementers of software to be a little more concerned the way the implementers of servers need to be for the robustness of their solutions.  And they conclude the abstract:  "Informed by our experiments and results, we develop a broad framework and guidelines to safeguard security and privacy in DNA synthesis, sequencing, and processing."  Noble goals.



Okay.  So what's going on here?  There's a utility on SourceForge that's open source called "fqzcomp," also known as the "FASTQ" compression utility.  It was designed to compress DNA sequences.  So these guys downloaded the source and deliberately broke the code by creating a short static buffer which was deliberately too small to contain their 177 base pair DNA so that, when that program attempted to compress the DNA they provided to it, that synthetically and purposefully shortened buffer would overflow.  And their modified fqzcomp version used a simple two-bit DNA encoding scheme.  As anyone who's looked into genetics knows, there are four nucleotides that have just been given the letters A, C, G, and T.  So four is a happy number.  So the four nucleotides were encoded A as 00, C as 01, G as 10, and T as 11.  Thus this allows packing pairs of bits into bytes so that you can get four nucleotides encoded, each nucleotide bringing two bits with it into an eight-bit byte.



Okay.  So, yes.  Their two-bit scheme, so to speak, coupled with their deliberately hacked reduced buffer size, did indeed allow them to place their own binary bits into non-buffered space, that is, into the overflow region past the buffer.  However, even given all of this cheating manipulation, their exploit only functioned 37.4% of the time since only an error-free gene sequencing would produce a running exploit.  Oh, darn, you know, your shell code has to not have bugs in it, literally, or bad genomes.  And only 76.2% of the DNA sequencing of this synthetically short little snippet was error free.  So the sequencing process isn't sufficiently robust today to even allow you to reliably transcribe code from DNA into bits, that is, just, what, a little over two thirds of the time, 76.2%.



But then there's the problem that DNA strands have two ends.  And the sequencing might begin from either end.  So of course this cut the 76.2% success in half again, down to 37.4 because, if you ran the wrong end in, you ran the back end in first, you'd get your shell code backwards.  Now, maybe if you were really a good hacker you could make the shell code work, regardless of which way it was encoded, which would be a cool hack.  But theirs didn't.



So what we really have here is a somewhat interesting, contrived, synthetic demonstration of how an inherently very low reliability attack could be created using DNA as the code-carrying agent, but almost certainly only if the DNA sequence processing pipeline had serious flaws that would cause it to crash quickly when processing non-contrived DNA.  In other words, it's difficult to see how any such flaws, such as what they put into the code to make this happen, could survive in the real world without immediately crashing upon encountering actual DNA, which of course would drive the developers of the code to fix it.



So anyway, that's the story behind, oh, my god, DNA can crash computers.  Yeah, except it can't actually.  And you know, Leo, it occurred to me that this whole merging of medicine, DNA, and computers is kind of a bit surreal.  And one of the things we've seen in the past is how various disciplines, when they intersect, kind of cross-pollinate with their terminology.  And one wonders in this case how much terminology from these disciplines might get combined because, for example, you could imagine that incontinence might someday be considered a buffer overflow.



LEO:  Oh, please.  I mean, I think the story stands as a cautionary tale that data files - and this is how I used it on the radio show - can, if there's an error in the rendering program, be used to infect...



STEVE:  In the interpreter.



LEO:  In the interpreter.  What they didn't, or I didn't see in the mainstream stories, or they didn't reveal until they gave their paper, was, A, that they hadn't modified DNA; and, B, more importantly, they modified the interpreter to make it flawed so it didn't - I mean, that's a silly proof of concept, but it is a proof of concept.  But it's not proving anything we don't already know, which is that, if you have a bad metafile interpreter, a JPEG can be used to infect a computer.  If you have a bad PDF reader, a PDF can be used.  We've seen these attacks happen.  And so it isn't that farfetched to say, well, I guess if you're running sequencing software on your PC, which they do, you could theoretically somehow malform a DNA strand to pwn the computer.



STEVE:  Correct.  And again...



LEO:  They didn't do it.



STEVE:  As I said at the top, I think it's cautionary and useful.  But again...



LEO:  They didn't actually do it, yeah.



STEVE:  Right, exactly.  So we have another - the week's top clickbait headlines, which were used to mislead and worry people.  And I put two headlines at the top of the story here:  "A self-driving car can be easily hacked by just putting stickers on road signs."  And the subhead of that one was "A team of experts showed that a simple sticker attached on a sign board can confuse any self-driving car and potentially lead to an accident."  Except that's not at all what the team of experts showed.  I mean, it's like there's not a shred of that is accurate.



And the other headline that I liked was "Researchers hack a self-driving car by putting stickers on street signs."  Except, no, that's not what they did, either.  Not even remotely close in this case.  The DNA story was more accurate than this one.



So in fairness, the audience for these articles is not our typical Security Now! listener.  But they do cause concern and put unwarranted doubt out into the ether, which is bad.  So in this case this team of expert researchers from four U.S. universities - the U. of Washington, U. of Michigan at Ann Arbor, Stony Brook University, and the University of California at Berkeley - got together and, okay, so now listen to the actual title of their paper, which was "Robust Physical-World Attacks on Machine Learning Models."  Okay?  Doesn't mention wheels or cars.  Doesn't say attacks on cars, or even one car, or even a shopping cart.  It says "attacks on machine learning models."



And yes, this has some relevance to the real world because we believe that autonomous self-driving automobiles also use machine learning models.  So cars have, to some degree, that in common with what these researchers did.  And they were interested in researching the question of spoofing cars, but that isn't what they actually did.



Okay.  So my point is at this point it's very different from what we've discussed, for example, in the past of, like, for example, commandeering that Jeep's management network and forcing its driver off the road and into a ditch.  I mean, that actually happened.  This hasn't.  This didn't.



Okay.  So the abstract of their paper reads:  "Deep neural network-based classifiers are known to be vulnerable to adversarial examples" - and I should just stop and say, notice now in both of these stories the notion of adversarial attack is presented.  And this is one of the best changes that we're seeing happening.  I mean, it's surprising that it takes as long as it does.  But now there's becoming this mature understanding that just getting the code to work is different from having it always do what you want it to, very different.  And so you have to challenge the code.



So anyway, so they say:  "Deep neural network-based classifiers are known to be vulnerable to adversarial examples that can fool them into misclassifying their input through the addition of small-magnitude perturbations."  Which is "I have a degree" speak for, like, tiny blemishes in the image can have outsize effects.  "However," they write, "recent studies have demonstrated that such adversarial examples are not very effective in the physical world.  They either completely fail to cause misclassification, or only work in restricted cases where a relatively complex image is perturbed and printed on paper."  In other words, real cars, you know, people have tried to spoof real cars.  And while we've talked about some of the limited successes they've had, there is some robustness there.



So they say:  "In this paper we propose a new attack algorithm" - which they named "Robust Physical Perturbations," and so it's RPP - "that generates perturbations by taking images under different conditions into account."  So they're going to try to strengthen the attack concept.  "Our algorithm," they said, "can create spatially-constrained perturbations" - meaning, again, small - "that mimic vandalism or art" - a.k.a. graffiti - "to reduce the likelihood of detection by a casual observer.



"We show that adversarial examples generated by RPP achieve high success rates under various conditions for real road sign recognition by using an evaluation methodology that captures physical world conditions.  We physically realized and evaluated two attacks, one that causes a stop sign to be misclassified as a speed limit sign in 100% of the testing conditions" - but hold on because we need to discuss what the testing conditions are because they weren't real - "and one that causes a right turn sign to be misclassified as either a stop or added lane sign in 100% of the testing conditions."



Okay.  So because this news generated these hysterical headlines, these guys had to do a FAQ, Frequently Asked Questions page in order to back people away from the hysteria that this was creating.  So in their FAQ they ask themselves, "Do you attack a real self-driving car?"  Answer:  "No."  "Okay, what did you attack?"  And they answered:  "We attacked a deep neural network-based classifier for U.S. road signs.  A classifier is a neural network, in the context of our work, that interprets road signs.  A car would typically use a camera to take pictures of road signs, and then feed them into a road sign classifier."  In other words, something that reads road signs.



They write:  "To the best of our knowledge, there is currently no publicly available classifier for U.S. road signs."  In other words, there are autonomous driving vehicles and experiments, but those are corporate enterprises that have proprietary algorithms that they value highly, and they're keeping to themselves.  So they couldn't use those.  "Therefore," they write, "we first built our own neural net consisting of three convolutional layers followed by a fully connected layer.  We then trained our network on the LISA dataset, a U.S. sign dataset comprised of different road signs like stop, speed limit, yield, right turn, left turn, et cetera. Our final trained road sign classifier accuracy was 91% on the test dataset."  Okay.  So it got it right a little, just slightly over nine out of every 10 times when re-shown signs that it had been trained on.  So sort of a synthetic result.  But again, it was correct for their purpose.



So then the next question:  "What are your findings?"  They said:  "We show that it is possible to construct physical modifications to road signs, in ways that cause that trained classifier to misinterpret the meaning of the signs.  For example, we were able to trick the classifier into interpreting a stop sign as a speed limit 45 sign, and a right turn sign as either a stop or added lane sign.  Our physical modifications for a real stop sign," they say, "are a set of black-and-white stickers."  And in the press coverage there were lots of pictures of this, basically stop signs with some black-and-white rectangles sort of stuck at seemingly arbitrary positions, but not arbitrary.



So then they finally say, just sort of for real-world-ness:  "What resources does an attacker need?"  Now, first of all, this would be an attacker attacking their neural network, not any known actual autonomous vehicles.  So they say:  "An attacker needs a color printer for sticker attacks; a poster printer for poster printing attacks.  The attacker would also need a camera to take an image of the sign he wishes to attack."



So then:  "Based on this work, are current self-driving cars at risk?"  "No.  We did not attack a real self-driving car.  However, our work does serve to highlight potential issues that future self-driving car algorithms might have to address."  No argument there.  Finally:  "Should I stop using the autonomous features - parking, freeway driving, et cetera - of my car?  Or is there any immediate concern?"  "We again stress," they say, "that our attack was crafted for the trained neural network discussed above.  As it stands today, this attack would most likely not work as-is on existing self-driving cars."



Okay.  So once again, they created a relatively simple neural network, that is, a four-layer network which they trained, and they were then able to understand and monitor.  So then they gave it a stop sign and moved black-and-white rectangles around the stop sign while this thing kept trying to classify the stop sign.  And as soon as the arbitrary shape and size and position rectangles caused a failure of this four-layer simple neural network, they said, "Ah-ha, we just fooled it."  Okay, yeah.



So anyway, this is another definitely useful research.  We definitely hope that all manufacturers of actual road sign-classifying AI will see this research.  I'm sure everybody who should see it, it has come to their attention, and they're probably busily explaining why their neural network AI classifiers would not fall to this kind of attack.  So, interesting and useful; but again, the headlines were crazed on this, saying, yes, you put a couple stickers on a stop sign and Teslas will drive right through and not stop at the stop sign and kill their owners.  No.  Sorry.



And the researchers realized, whoops, we weren't clear enough, so they added this FAQ page to say, okay, no, we didn't attack a car.  We didn't attack a real network.  We don't have any.  No one will give theirs to us.  So we made one that wasn't very good to start with, which we were then able to scrutinize while we did things to it until we made it guess wrong.  So, I mean, and nine out of 10 times it guessed wrong anyway, even when given a good sign that it had learned.  So anyway, yes.  Cars at this point are not in danger from graffiti on road signs.  But useful.



The final nail in the WoSign/StartCom coffin:  Microsoft has joined Mozilla, Google, and Apple in the abandonment of trust in WoSign and its StartCom subsidiary.  So it's finally game over for those clowns.  We've talked about, back years ago, when it first came to light the really surprising low quality of their automated frontend for their certificate minting.  So this and the Symantec debacle should serve as a useful and cautionary tale for all other presently widely trusted certificate authorities, which is what you do really matters.  That is, not what you say.  Not all the certificates and assurances you provide.  What you do is what matters because that's entirely why they're in the business of signing certificates, and the only way they're in that business is if they are trusted.  So if what they do breaks trust, they're out of business.



So in Microsoft's TechNet blog of August 8th, Microsoft writes:  "Microsoft has concluded that the" - I mean, and they're the last to join the party, so yay - "that the Chinese Certificate Authorities WoSign and StartCom" - which as we know is a subsidiary of WoSign - "have failed to maintain the standards required by our [Microsoft's] Trusted Root Program.  Observed unacceptable security practices include back-dating SHA-1 certificates, misissuances of certificates, accidental certificate revocation, duplicate certificate serial numbers, and multiple CAB Forum Baseline Requirements violations.



"Thus, Microsoft will begin the natural deprecation" - that word struck me as odd.  Natural deprecation?  Okay - "of WoSign and StartCom certificates by setting a 'NotBefore' date of 26 September 2017."  That happens to be Tuesday exactly six weeks from today.  "This means," they write, "all existing certificates will continue to function until they self-expire.  Windows 10" - and I'll come back to this in a second because the whole posting only addresses Windows 10 - they write, "will not 



trust any new certificates from these CAs after September 2017," so from October on.  "Microsoft," they write, "values the global Certificate Authority community and only makes these decisions after careful consideration as to what is best for the security of our users."  Okay.



But what was not addressed in that posting was the status of the earlier, still supported, and still in the majority despite Microsoft's every effort, Windows 7 and 8.1 operating systems.  Note that this sort of conditional certificate acceptance tweak, unless the facility was presciently already built in, requires custom modification of the system's certificate interpreter.  That is, by default, the certificate start and end dates are checked.  So you've got to do something other than that if you want to do something like conditionally allowing certificates that were only made before the end of September.



So users of earlier Windows operating systems cannot simply, if we're abandoned by Microsoft, cannot simply remove those root certs from their own trusted root stores in their Windows OSes, since StartCom's certificates were once quite popular and are still being sold today - today and for the next six weeks, doubtless - with two or three years in the future expirations.  So it's not the certificate owner's fault that the company from which they were obtained screwed up, although I guess I would hold people somewhat responsible for having purchased certificates from StartCom anywhere in the recent past after it was known that they were such poor managers of trust.



Okay.  So what this means, though, is that any StartCom certificate issued within the next six weeks will still be honored by Windows, even Windows 10, for the next two or three years.  Okay.  So to be responsible to its users, Microsoft really must similarly protect users of its earlier and still supported operating systems.  Failing that, we would be forced to defensively abandon use of IE in favor of Chrome or Firefox on those earlier platforms because those browsers are remaining proactive, even on earlier versions of Windows.



And out of curiosity I went over the StartCom this morning.  And it's business as usual over there.  You wouldn't know, looking around, that they're not long for the world.  But in fact the boom is rapidly being lowered on them because, unless they were to start backdating their newer certificates' start date - and remember this wouldn't be the first time they had done that.  But boy, this would end it.  That would immediately extinguish what little remaining shred of forbearance the industry has for them.  But from this date in six weeks they will be unable to ever again sell any certificates which would be honored by Windows 10 or these other browsers, starting six weeks from today.



So the problem is that, because StartCom certificates will still be valid for as many as two or three years, other versions of Windows need to be informed of this same thing.  I don't know that Microsoft isn't going to do this.  But Microsoft wants to pretend that nobody else is running anything other than Windows 10, so they just don't talk about 7 and 8.1.  Hopefully, we will get an update.  Maybe it's already there; it has been delivered.  Or it'll come next month or sometime.  But anyway, I hope they...



LEO:  It doesn't come in the browser.  It's a Windows Update file.



STEVE:  Correct.  Correct, because IE and Edge both use the underlying Windows crypto systems.  Chrome has historically.  I think they still do.  But the browser is able to look at the certificate, even though the OS processes it.  So Chrome, in looking at the certificate, gets to say, whoops, that's a StartCom cert.  And the NotBefore date, meaning the issuance date, we've decided we're not going to trust.  So the browser is able to conditionally trust a certificate that the underlying server still fully trusts.



And so that means that what Microsoft will do with Windows 10 is they're going to push that conditional trust down into the kernel, down into the underlying crypto system, which is why it's not just - they can't just change a certificate.  They have to arrange, you know, they have to change the code to educate it about the new policy for StartCom and WoSign certs.  Whereas Firefox carries their own completely internal certificate management system.  Chrome uses the underlying OS's, but does inspect the cert that the site has given it.  And so it's able to say, you know, to essentially do a blacklist of certs that would otherwise be trusted.



Okay.  So some news out of the U.K. sort of put me in mind of why we need, why we're going to need, why the only solution to the problems that are beginning to surface and metastasize are going to be some sort of global Internet treaties.  BleepingComputer reports last Monday that British lawmakers - or reports that:  "Last Monday, British lawmakers filed a statement of intent regarding" - and again, this is just a statement of intent, so not legislation yet, so it has an unclear future as anything not yet signed into law does - "regarding proposals for improvements to what they call the Data Protection Act, the DPA,  with a focus, interestingly, on criminalizing anonymous data reidentification, imposing large fines for cyber incidents and more user protections for British online netizens," writes BleepingComputer.  "The modifications are part of the U.K.'s effort to comply with the EU's General Data Protection Regulation, the GDPR, that's set to come into effect in May of 2018."



And that thing has some real teeth.  There's been some discussion of it, I think, in the Security Now! Forum in the GRC newsgroups.  And I mean, I have to do some things with GRC's ecommerce system in order to make sure I'm in compliance with the GDPR.  I don't remember now whether we've talked about it on this podcast.  But, I mean, it doesn't matter where you are.  If you have customers in the EU, the EU is asserting that they have teeth to go after anybody anywhere who is not complying.  So it's certainly a chill.



LEO:  We have a fan who has written a book on - I wish I had it with me, I think it's at home - on GDPR compliance.  It's a little book to get you started.  We'll send you a copy.



STEVE:  And believe me, you and I both need one because...



LEO:  Well, do I need one?  I don't have customers.  I don't collect information.



STEVE:  This thing is so horrifying when you read it, it's like, wait a minute.



LEO:  We have server logs, so I guess that counts.



STEVE:  Believe me, Leo, if you have a server online, I mean, unless you're Wikipedia, I think you're probably - you need to just make sure.  And I don't know if little fish will matter.  But, boy, there's no doubt that Google and Facebook and the big players are looking at this, thinking, okay, we've got to decide what we want to do about this.



Okay.  So the U.K.'s data protection act has this new statement of intent.  One of the high points is this notion of anonymous data reidentification.  Okay.  But the bill would add many provisions to the GDPRs.  For example, "Make it simple to withdraw consent for the use of personal data."  That is, require people who have personal data to make it simple for their users to withdraw consent for the use of their personal data.  "Make it easier and free for individuals to require an organization to disclose the personal data it holds on them."  So all good for visibility.



"Allow people to ask for their personal data held by companies to be erased.  Require explicit consent to be necessary for processing sensitive personal data.  Enable parents and guardians to give consent for their child's data to be used.  Expand the definition of 'personal data' to include IP addresses, Internet cookies, and DNA."  And, finally, "Make it easier for customers to move data between service providers."  So this would have a chilling effect on much of the way business is conducted today.



And unfortunately, as is too often the case when aggressive new legislation meets aggressive new technology, problems in practice arise.  For one thing, it's easy to drop the gavel on a law that's not possible or practical to implement.  We've been discussing this; we've been discussing sort of around this issue with a whole question of law enforcement somehow having access to encrypted information, especially if that information is encrypted with perfect forward secrecy so that the keys are constantly changing, and there's no history of them.  It's like, so the legislation can say whatever it wants, but the technology doesn't make that possible.



Another perfect example would be the requirement, which we've discussed, and there has been proposed legislation, and it's somewhere in the pipeline, that ISPs retain a log of everything each of their subscribers individually does and everything they do out on the Internet.  Okay.  Well, it's easy to write the law.  It's easy to say that.  But as we know, it's next to impossible for any ISP to actually pull that off, even if they wanted to, which they most assuredly don't.  They just, as we know, want to charge for the transitive subscriber traffic across their proprietary networks, thank you very much.  They don't want that responsibility.



But there are governments that say, well, sorry, but we need the information that you're in a unique position to get, not quite understanding that they're actually not.  That information is not gettable.  So similarly, in this instance, it's a noble ideal to imagine criminalizing the reversal of deliberately anonymized identity information, but how exactly is that reduced to practice?  It is, after all, the entire business model of several of the world's largest Internet entities.  It underlies the somewhat awkward Hobbesian bargain we have made with these entities in exchange for accepting their free offerings.  They track and explicitly deanonymize, even in the face of many of their users' explicit request that this not be done.  So good luck with telling Google and Facebook and now Microsoft as a Service that this is conduct unbecoming and could become criminal.



So paraphrasing from BleepingComputer's report, they say on top of the GDPR provisions the data protection bill includes an extra provision, the creation of a new criminal offense for when someone intentionally or recklessly reidentifies individuals from anonymized or pseudonymized data; and, in practical terms, answering the question who belongs to this cookie because all browser cookies are inherently pseudonymous, and you're not supposed to know who that is.  But that's what Google and Facebook are.  So the DPB reads:  "Offenders who knowingly handle or process such data will also be guilty of an offense.  The maximum penalty would be an unlimited fine."



In BleepingComputer's coverage, they quoted a Dr. Lukasz Olejnik, and he is an independent cybersecurity and privacy researcher who is an affiliatee of Princeton's Center for Information Technology Policy, who on one hand applauds the U.K.'s efforts, writing in his blog last Monday, quote:  "The U.K.'s GDPR implementation may have visionary traits in that it goes beyond merely implementing the GDPR as just a legislation.  The U.K. will introduce new criminal offenses, among them reidentification."  But then he adds, oh, by the way:  "There are several issues with the banning of reidentification.  First, it won't work.  Second, it will decrease security and privacy.'"



The biggest problem, in Olejnik's view, is that there's no effective way to enforce it in practice.  But wait.  Since this is Google's and Facebook's entire business model, enforcement seems pretty simple.  After the legislation is enacted, if it should ever actually see the light of day as law, the U.K. can simply give Zuck a call and say, you know that legislation that became law yesterday?  How are you guys still in business?  So secondly, though, adds Olejnik, the new legislation would stifle security and privacy research, which often and must often deliberately reidentify anonymized data in day-to-day research.  The DPB statement did mention protections for journalists and whistleblowers, but did not provide any details.



So anyway, this brings us all the way back around to the need for transnational treaties.  And this is the tension that we've been watching grow over the last couple years.  We have a global network with valuable and desired content being provided free of explicit charge to us, in exchange for a sacrifice of absolute, that is, of our absolute privacy and anonymity.  We can feel comfortable, and many people do, that Google is going to be responsible, and Facebook will be responsible.  But our privacy is not absolute in return for their services for which they don't charge us.



And of course these are massive global Internet-centric enterprises operating across national boundaries.  If regulation is to be imposed, that regulation cannot practically be disparate in every region of the globe.  You can't have a global network with entities like Google and Facebook spanning the globe which also spans governments, each with their own quirky laws that these entities operating within those countries all have to individually abide by.



So we're still in the early days.  But I think we're beginning to struggle with the big questions of encryption, privacy, and anonymity for all users of this incredible global network of ours.  It's clearly an important and necessary conversation for us to have.  But the very globalness of the Internet which creates so much of its value requires unified global regulation.  And I think a treaty mechanism is probably the way we do that.  I don't, you know, I'm not a legal scholar, so I don't know the details of that.  But presumably someone somewhere is thinking about this.  We can hope.



LEO:  So I just have a question on the NIST story.  I know we're going back in time.



STEVE:  Yeah, yeah.



LEO:  Our advice still stands.  And I guess that's what changed.  I was surprised that they didn't say use the special characters because I always use.



STEVE:  I always do.



LEO:  What I do is I tell LastPass, use every character you can - special characters, numbers, upper, lower, whatever.  The key is long, right, and totally random.  



STEVE:  Well, yes.  But the difference is these guidelines are still assuming user-remembered passwords.



LEO:  Oh, I get it.



STEVE:  And that's the real problem is that now we know you have to have a separate password for every site, or you risk cross-compromising non-compromised sites.  So all of our listeners, who are all password manager fanatics, none of this, you know, bears...



LEO:  None of it applies; right.



STEVE:  None of it applies because we're using...



LEO:  Because we don't have to remember it.



STEVE:  Exactly.  We're using high pseudorandom content gibberish every site we visit.



LEO:  Right, even for secret questions.



STEVE:  Yes.



LEO:  But there is one password you have to remember, and that is your password vault password.



STEVE:  Good point.  And actually we're going to come to the story in a minute here about the brute forcing of the password vaults.



So this week in Researchers Need Protection, Techdirt wrote up a very nice piece that features one of our frequently mentioned security people, Troy Hunt, titled:  "Company Storing Families' Personal Data Blocks Users/Researchers Informing It of a Security Flaw."  Techdirt wrote:  "It must be repeated over and over" - and everyone knows it's my refrain - "people who discover security flaws and report them are not the enemy.  And yet," writes Techdirt, "company after company after company treat security researchers and concerned users like criminals, threatening them with lawsuits and arrests rather than thanking them for bringing the issue to their attention."  And of course we've got the Hungarian ticketing, public transportation ticketing system from a couple weeks ago, too, as another example.



This company, Kids Pass, a U.K. company providing discounts for families attending restaurants, theaters, and amusement parks, had a problem.  And, boy, any user could access any other user's personal information just by altering numbers appearing in the userIDs in the URL.  Oh, I mean, I think it's like there's a top 10 worst practices.  This was number four, the idea of sensitive information in the URL.  No.  In other words, another glaring example of atrocious web application design which itself should be outlawed.



So a concerned user told security researcher Troy Hunt about the flaw.  And so Troy tweeted:  "Just this weekend I had a Twitter follower reach out via DM" - I'm sorry, Troy posted in his blog.  "Just this weekend I had a Twitter follower reach out via DM, looking for advice on how to proceed with a risk he'd discovered when signing up to Kids Pass in the U.K., a service designed to give families discounts in various locations across the country.  What he'd found was the simplest of issues, and one which is very well known:  insecure direct object references [is the technical term for this].  In fact, that link shows it's number four in the top 10 web application security risks, and it's so high because it's easy to detect and easy to exploit.



"How easy?"  Troy writes, "Well, you can count; right?  Good, you can hack.  Because that's all it amounts to, simply changing a short number in the URL," which of course points you to a different user.  And it's like, oh, now I can see their data.  "Troy told the user to stop doing anything, including accessing other users' information" - good advice, you don't want to actually be breaking the law just unintentionally - "and to immediately inform the company.  The user did as instructed, contacting the company via Twitter direct message."  Okay, so a private communication, Twitter direct message.



"Shortly thereafter, the user informed Troy that Kids Pass had blocked him on Twitter.  Troy then made an attempt to speak to someone at Kids Pass, only to find he'd been blocked, as well, most likely for having the gall to retweet the concerned user's message about the security flaw.  The responsible, ethical approach, notifying a company of a security flaw as soon as possible, was being treated like some kind of trollish attack on Kids Pass's Twitter account.  From all appearances, the company simply wanted everyone to shut up about the flaw, rather than address the concerns raised by a user.



"It was only after Troy asked his followers [he has many] to contact the company on his behalf [thus flooding them] that Kids Pass finally unblocked him and told everyone the 'IT department' was looking into it."  So yay for Troy.  "However, that belated reaction doesn't make up for the initial reaction, of course; and Kids Pass has shown it has little interest in addressing security flaws until the problem becomes too public for it to ignore.  Troy points to a blog post by another security researcher who informed Kids Pass last December about its insecure system, including the fact it sent forgotten passwords in plaintext via email to users."  So no technology on their backend.  This developer heard nothing back, finally publishing his discoveries in July, so last month, giving them ample time to fix it.  They didn't. 



"If you want people," writes Techdirt, "to be good web citizens and report breaches and flaws, you can't treat them like irritants or criminals when they do.  Securing users' personal info is extremely important, but some companies seem to feel they should be able to handle it however they want and mute/sue/arrest those who point out how badly flawed their systems are."  And our listeners will remember that, when one was pointed out at GRC, someone found a fault in my form processing that technically allowed cross-site scripting.  It turns out that there were other limitations.  But I thanked the person, I told everybody on the podcast, and I fixed it.  So that's the way you do it.



As I've always said, people are responsible for their policies.  They're only responsible for the way they deal with their mistakes.  But mistakes happen.  So a company like this, wow.  I mean, yeah, we have no regulations that make it illegal to conduct yourself this poorly.  And there's no robust reputation system in place.  If there were, it wouldn't matter.  People would still go, oh, hey, look, it's free, and dump all their information in with no idea that that URL is their account number, and that anybody can put that in and retrieve all their info.



So, I mean, again, very much like I was talking about last week with the Chinese webcam that mapped itself through UPnP and established open waiting service ports on the public Internet, I mean, like the worst possible way to do this.  And I gave an example of a perfect, simple, correct way to do it.  So there are already well-known good ways to solve all these problems.  But not only do companies not do it, but when someone says, hey, you know, this is bad over here, they get attacked by way of thanks so often.



So on the heels of last week's horrific Hotspot Shield VPN story, where our listeners will remember it turns out that the free-to-download clients are doing all kinds of noxious privacy-violating things as researchers had found, we have the flipside that I wanted to share, both because I've had a lot of our users asking me about this particular provider, but more so because their posting demonstrates all of the right stuff.  And our listeners will recognize and appreciate the honesty and integrity it displays.  So I'm going to share the entire thing - it's not overly long - because this is important, not only for current and prospective users of TunnelBear, but because there are also lessons here for any and all other high-integrity VPN providers.



I've got a link to TunnelBear's blog posting in the notes, as well as a link to the third-party audit which they have now done several times.  The headline on their posting is "TunnelBear Completes Industry-First Consumer VPN Public Security Audit."  And they write:  "Consumers and experts alike have good reason to question the security claims of the VPN industry.  Over the last few years, many less reputable VPN companies have abused users' trust by selling their bandwidth, their browsing data, offering poor security, or even embedding malware.



"Being within the industry, it's been hard to watch.  We knew TunnelBear was doing the right things.  We were diligent about security.  We deeply respected our users' privacy.  While we can't restore trust in the industry, we realized we could go further in demonstrating to our customers" - and, I would argue, to the rest of the industry - "why they can, and should, have trust in TunnelBear."  Or for the rest of the industry, what real proper conduct looks like.  "Today," they write, "we'd like to announce TunnelBear has completed the consumer VPN industry's first third-party public security audit.  Our auditor, Cure53, has published their findings on their website, and we're content with the results."  And they provided a link.



So a bit of history:  "In late 2016," they write, "we hired Cure53, a respected security company, to do a complete audit of our servers, apps, and infrastructure.  Using a white box approach" - as opposed to a black box approach, where what's inside is a mystery.  So "Using a white box approach, they were given full access to our systems and code.  Our original plan was to use their findings internally to confirm we were delivering on our promise to secure your browsing and proactively identify vulnerabilities.  However, the recent crisis of trust in the VPN industry showed us we needed to break the silence and share Cure53's findings publicly.  Today we're sharing a complete public audit which contains both the results from last year and the results from the current audit.



"As the auditor, Cure53's opinions and findings are their own, with the results being published on their website."  So clearly TunnelBear is trying to establish a bit of an arm's-length relationship here to assure us of the veracity of these findings.  "TunnelBear," they write, "was given the opportunity to provide feedback on the report before it was published, where we felt findings were inaccurate or irreproducible.  As is the case of most security audits, Cure53 was paid for their work.  We wouldn't expect any cybersecurity company to spend a few hundred hours auditing our code for free.



"What were the results?  If you've already looked at the results, you've seen that the 2016 audit found vulnerabilities in the Chrome extension that we weren't proud of.  It would have been nice to be stronger out of the gate, but this also reinforced our understanding of the value of having regular independent testing.  We want to proactively find vulnerabilities before they can be exploited.  We hadn't intended to publish the 2016 results.  However, we're hoping the security community has appreciation for our candid transparency in the 2016 report and for demonstrating our investment in security over time.  All findings discovered in the 2016 audit were promptly addressed by TunnelBear's engineering team and verified to be fixed by Cure53.  



"In the June 2017" - that is, the audit that they're writing about that just happened last month - "we were more content with the results.  All vulnerabilities represented low-risk findings.  As Cure53 put it:  'The results of the second audit clearly underline that TunnelBear deserves recognition for implementing a better level of security for both the servers and infrastructure as well as the clients and browser extensions for various platforms.'  All findings discovered in the 2017 audit have also been addressed by TunnelBear's engineering team, with only informational findings remaining."  They say you can read the full report on Cure53's website.



And, finally, "Our ongoing commitment to security," they write, "our plan is to earn trust and move the VPN industry in a new direction around transparency.  While many VPN companies will continue to live in obscurity with claims for protecting your security, it's our hope that by completing the industry's first third-party public security audit, experts and consumers alike can be sure that TunnelBear delivers on its security promises.



"If we've learned anything from this audit, it's that good security needs constant reevaluation."  Amen.  "Annual public audits will become routine to help us quickly identify vulnerabilities and demonstrate transparency in an industry where trust is sorely lacking.  In the coming months we'll share more announcements, industry insights, and how-tos to give you the information you need to make the right choices about your security."  And they sign off saying, "Grizzly Regards, The TunnelBear Team."



And again, props and bravo.  I mean, that's - I just like this on the heels of Hotspot Shield VPN as the way it's done right.  A company that already had the right stuff and contracted with an independent third party to check their stuff, and problems were found.  They addressed them immediately; and then they thought, you know, six months later, let's check again.  And so fewer problems, I mean, nothing significant was found.  And they said okay, let's explain who we are.



And I like their notion of the fact that most VPN providers, many are unclear about their commitment to user privacy.  They just say, oh, you know, "protects your privacy," but nothing to back it up.  Here we have somebody that was allowed in, with no axe to grind, cross to bear, axe to grind, saying okay, this looks good.  And so bravo.  As I said, a lot of users have asked me about TunnelBear.  I've not looked any further.  I don't know about their plans or their pricing or anything else.  But this, as an ethic for a VPN service provider, I don't know how you do any better than this.  I know proXPN was a longstanding VPN sponsor.



LEO:  So was TunnelBear, by the way.  TunnelBear's a sponsor, as well.



STEVE:  Oh, they are now?



LEO:  No, they have been.



STEVE:  Oh, okay.  I didn't know that.  So, nice.



LEO:  Yeah.  Once a sponsor, always a sponsor.



STEVE:  Okay, good.



LEO:  Disclaimer-wise, anyway.



STEVE:  Right.  So I've not looked at their plans, but I wouldn't have a second thought about either of those guys because we know that their hearts are in the right place.  So bravo to them.



Now, many users - I think this is the last one of our news topics.  Yes.  Many users worried about, once again, some other headline news.  This was ElcomSoft's blog posting, which began:  "One Password to Rule Them All:  Breaking into 1Password, KeePass, LastPass, and Dashlane," arguably the four leaders in the password manager territory.  I have a link to their full posting in the notes.  And I've just snipped a tiny few pieces out of it.



They wrote:  "We've just updated ElcomSoft Distributed Password Recovery" - which is a product of theirs - "with the ability to break master passwords protecting encrypted vaults of the four most popular password keepers:  1Password, KeePass, LastPass, and Dashlane.  In this article we'll talk about security of today's password managers and provide insight on what exactly we did and how to break into encrypted vaults."



Okay.  So skipping all of that, this amounts to an offline GPU-accelerated, brute-force password recovery.  So essentially they've done enough reverse engineering of those four password managers to figure out what the algorithms are which map the user-supplied password to the key, which is then used to decrypt the user's password vault.  And they then turned that into the highest performance code they could.  So, for example, they write:  "Different password managers employ different approaches to security.  As an example, LastPass generates the encryption key by hashing the username and master password with 5,000 rounds of PBKDF2 using SHA-256."



So we've talked about PBKDF2 in the past, and the SHA-256 hash.  So the idea being that it is an iterative process which turns the users' password into pseudorandom gibberish, with the requirement that you put the same password in every time, you get the same gibberish out every time.  And it also, because it's done 5,000 times, it slows it down way more than if it was just done once, if it was just hash the password through SHA-256.  Instead, it's ground on a lot longer.



And they say:  "...while 1Password employs even more rounds of hashing."  Although interestingly, 1Password was more attackable than LastPass.  We'll get to that in a second.  They write:  "This is designed to slow down..."



LEO:  Slow it down, buddy, slow it down.



STEVE:  I need some more tea.  Maybe I do need some caffeine.



LEO:  Slow down.



STEVE:  "...to slow down brute-force attacks, and it almost works," they write.  "Granted, these are still nearly an order of magnitude less secure, say, than Microsoft Office 2016 documents; but even this level of security is much better than nothing."  They write:  "Therefore, this is the benchmark.  We've added RAR5 and Office 2016 to the chart for comparison.  Higher numbers represent higher recovery speeds."  And Leo, I've got their chart in the show notes on the page that this leads into.  And it's a little interesting.



So, for example, this is using an NVIDIA GTX 1080 GPU to drive the hashing engine.  And with the default settings, LastPass passwords can be brute forced at 44,800 per second.  1Password can be brute forced at more than twice the speed, at 95,000.  So certainly we believe what they wrote about the higher number of iterations, but it still is faster.  So maybe 1Password is not using PBKDF2, it's just iterating the hash, which makes it extremely acceleratable by a GPU.



LEO:  So the bigger number is less secure.



STEVE:  Correct.  Correct.



LEO:  It's odd that Office 2016 is so secure.



STEVE:  I know.  Isn't that interesting?



LEO:  It surprises me, yeah.



STEVE:  Yeah.  Yeah, so that's guesses per second.  And so the least secure was Dashlane at 129,000 guesses per second.  But, okay, now, before anyone gets hyperventilated here, remember that, if you have a sufficiently long and random, really high-entropy password, well, then you can figure out how many bits it has.  For example, you could use the Password Haystacks to give you the size of the search space, and then divide that by your password manager's guesses-per-second figure, assuming this acceleration.  And that will tell you how long it would take to search the entire password space, cut that in half for the average search length.



And now, for a bit of context, so for example LastPass, 44,800 guesses per second.  Office 16 the strongest of everything, 7,300 guesses per second.  So for some context, because this sort of local brute-force attack is always a concern, the technology I designed for SQRL's PBKDF, that is, SQRL's password-based key derivation function, is deliberately highly acceleration resistant using the memory hard Scrypt function, and dynamically self-adjusting using what we call the "dynamically iterative EnScrypt function."  It yields a maximum brute-force rate of 0.00333 guesses per second.  Which is to say, one guess every five seconds.  And it is very difficult to make it go any faster.  So it is possible to do even better, if you really want to.  And for SQRL, of course, I did want to.



So I guess the takeaway is they didn't crack an of these products' code.  But they have added to their line a super, as much as they could, a turnkey GPU acceleration-enhanced brute-forcing capability.  So this is the kind of thing, if the FBI were to grab somebody's phone or laptop or something in a warranted search, and the user refused to divulge the password, they would, probably do have a contract with ElcomSoft, and they would update to the latest version and put it to work cracking that person's master password, password manager database.  And depending upon the quality of their password, and only the quality of their password, they would get those results.



Oh, and note that was 44,800 guesses per second at the LastPass setting of 5,000.  We've talked about that setting.  You can turn it up higher, if you want, and see if it noticeably slows things down.  SQRL, in return for making brute forcing a ridiculous challenge of futility, does take, the first time you use it, five seconds.  I mean, you sit there.  There's a little progress bar that runs across only once, the first time you use it, when your screen is unblanked, or you've switched users, or you haven't used it for a configurable length of time.  And you can turn those settings on and off.  So you pay the price once to prove to SQRL that you're you.  And then afterwards we allow you to use what we call the "quick pass," which is just a short prefix of that password and only takes a second, so just, you know, no time at all.



So it's a workable compromise that doesn't sacrifice security anywhere.  So it can be done.  And if you were worried about LastPass, or I don't know if the other password managers allow you to crank up their settings, but you might explore turning it up and seeing if, on a given platform and browser, it becomes burdensome.  Probably doesn't.  And so it might be worth going to a higher setting and waiting a few seconds for it to say, yeah, you guessed right.  Or you didn't guess, you knew your password.



I wanted to follow up a little bit with a little bit of errata from last week.  Many of our listeners confirmed that the send.firefox.com site that we talked about is working under Safari, including the listener who originally reported that it wasn't.  So either something weird happened at his end transiently, or maybe, because it is a web-based service, they may have fixed it between the time he tried it and the time I talked about it.  So anyway, I did want to follow up with everyone.  And in fact I think you, Leo, tried it while we were on the air last week, and it seemed to work.  So a lot of other people said, yeah, it works for me.



Last week I talked about the mystery, the ongoing mystery of my $2.04 Amazon S3 service billing.  And I said I was going to try to get some statistics.  Unfortunately, I only put the query in when I was beginning to assemble the podcast the previous evening, so it wasn't - and they kept saying, "You're going to need 24 hours."  And, boy, they sure didn't give it to me any earlier than that.  But as a consequence, I have it now.  So here's what I know.  I am currently storing 88GB at Amazon, for which I am paying $2.02 per month at 2.2 cents per GB per month, which is the price for the first 50TB of storage.  I'm not bothering with Glacier, which I understand is one fifth of the cost.  But that would almost be sad.



LEO:  They would charge you 2 cents.



STEVE:  There was no charge for data transfer because I'm just, as I said, mostly I'm just sending the podcasts up there and other - I have a bunch of encrypted and compressed directories that I also send there, and other stuff.  But, for example, in one month there were - I was charged 1 cent for 560 PUT requests and 1 cent, that is to say, one penny, which is worth now more than a penny, for 852 GET requests.  So I'm not a heavy user, 88GB, so I'm not storing images of multi-terabyte systems or anything.  But I'm storing the data that I care about for $2, and not even using Glacier.



So anyway, I did want to close the loop on that because I'm quite happy.  I mean, Amazon has the, what is it, it's a ridiculous number of nines, 99 point - it's much more than five nines.  It's, I don't know, like nine nines, 99.99999999% uptime.  And so I know there are alternatives.  There are cheaper solutions.  But you know me, I'm at Level 3 because I like their bandwidth.  And Amazon's got a good service.



So I got a nice tweet, I wanted to mention, from Richard Philips.  He is the author of the Rho Agenda trilogies.  And he just said:  "Hi, Steve.  Thanks for the Rho Agenda mention on last week's show.  I had a lot of fun with the final novel in that series, 'The Meridian Ascent.'"  And I wrote back and thanked him for his note and told him that, as soon as I get to it, I will read it and probably mention it again because I really do enjoy his books.



LEO:  Very cool.  Wow, that's nice.



STEVE:  Huh?



LEO:  That's nice.



STEVE:  Yeah.  And "Colossus:  The Forbin Project," a number of our listeners found it.  It exists over on Archive.org.



LEO:  Really.



STEVE:  Yes.



LEO:  Wow.



STEVE:  And there is a downloadable MP4, which is, boy, 409MB, I think.  And so, yup, you found it, Leo.  And I got the...



LEO:  Oh, it's high quality.  It looks good.



STEVE:  So what I wanted to tell our listeners is to please, it is browser-based.  So you can just fire it up in your browser, if you don't want to download the MP4, but you can.  Just watch the first five minutes.  It's just fun.  It's Dr. Forbin walking solo through the halls of this underground massive computer facility as he brings it to life.  And I don't want to say any more because it's just, I mean...



LEO:  This looks like an analog circuit.  I hate to say it.



STEVE:  Oh, I know.  Well, so its release date was April 7, 1970.



LEO:  Oh, there's paper tape. 



STEVE:  So I was a freshman in high school when this came out.  And so it's really all...



LEO:  It's fun.



STEVE:  It's all the fault of the movie, yeah.  Anyway, just the first five minutes.  I looked at the clock after Dr. Forbin had walked out of the facility, and other things had happened.  And I thought, okay, I just need to tell our listeners, if they're curious, you don't have to download it, just look at it in your browser, five minutes.



[Clip in background]



LEO:  I love the computer sounds, too.  Isn't that great?



STEVE:  I mean, this is classic old B-movie.



LEO:  That's so funny.  Wow.



STEVE:  So I did have a note, actually from somebody who's been working with me on SQRL over in the SQRL newsgroup.  He was having some weird behavior that he suspected might be a consequence of his system.  So he posted in the newsgroup.  First he said:  "There are a couple of unrelated things that have mysteriously stopped working over the last two or three days."  He said:  "I know my aging hard drive is slowing down.  So I'm going to shut down and SpinRite the hard drive."



And then he followed up in that thread to his own comment.  He wrote briefly:  "Ran SR Level 2 and then Level 4.  No sector or other issues flagged.  System boots and runs noticeably faster and smoother now.  Apps launch noticeably faster now.  Per past experience, this effect will last one to two weeks.  Then things will slow down again."  He said:  "I am looking for a replacement hard drive."



And so thank you, Dan.  I appreciate your sharing that.  And I should say that SQRL is now - the thing I've been working on for the last several months, the install, update, and remove facility, is essentially nailed.  We found one problem yesterday with a user, a tester who has the unbound local DNS resolver running in his machine.  And there is a problem with that, but we figured out what it is.



Anyway, essentially we've already moved on and are in the process now of cleaning up the few remaining issues.  I mean, it's done.  It's functional.  The spec is finished.  I mean, like the spec's definition is now finished.  So my plan is to spend X amount of days in the future, I have a to-do list that I've been making all along the way of things I wanted to remind myself to get back to and check.  I will do that.  And then we'll be talking about it on the podcast, and it will be released for everyone to play with.



And then while that's happening I will get back to work on the web side.  I don't want to delay allowing people to understand what it is that we've created here.  But I need to get it documented.  Apparently there is an Android client going that I wasn't aware of.  Jeff in the U.K. has got his iOS client.  I'll have one.  And it is running under Linux, also.  So we have those bases covered.  But I wanted to mention that, as is the case when odd things are happening, and this was what Dan was addressing, was if we can't trust brand new software, the question is, okay, is there something wrong with the software?  Or is there something wrong with the system on which it's running?  And he began to suspect that, well, other things were not working right, and his machine was seeming suspiciously slow.  So it might not be a problem with this new code that we were testing, but rather the platform it was being tested on.



I wanted to mention, though, I mean, the real takeaway here is this demonstrates what many SpinRite users have found through the years, which is it will keep a drive alive.  It will bring them back.  But ultimately this is hardware which is dying.  And again, it's like you give yourself an IV - oh, gosh.  What was that from "Silicon Valley," young blood, you know, my blood boy.  They were getting transfusions from young teenagers and feeling all energized.  Anyway, so SpinRite is your blood boy for your PC.  The point being, though, at some time in this battle between SpinRite refusing to allow your drive to die prematurely, it will ultimately lose.  Just as I'm currently in perfect health at 62, well, yes.  But this is a battle I'm ultimately going to lose at some point.  I hope not soon.



But the point is one of the things you need to be aware of is that SpinRite lets you get every last ounce of use out of your system.  But when the drive finally refuses to come back to life, well, it has the final say.  So take heed of these sorts of things.  Take heed of things seem funny.  Now they're okay after running SpinRite.  Oh, look, they got funny again.  Oh, I'll run SpinRite again.  Okay, yes, but your blood boy's getting tired.  So don't push it too far.



We've got a bunch of closing-the-loop stuff.  I'm going to save most of them for next week.  But there was one that I just, oh, my goodness.  Actually two.  A C. M. Au Yong tweeted:  "HTTP Error Code 418."  Now, for those who aren't as versed in HTTP error codes as some of us, everyone knows the 404.  That's the famous "this page doesn't exist" error code.  And there are other ones.  There's a 300 series for redirects like moved permanently, moved temporarily, and so forth.  But so in the HTTP spec there are all these defined error codes.  And from time to time, engineers being what we are, a little humor gets injected.  There actually is an Error Code 418.  And the format of these error codes is like HTTP - so it'll say HTTP 404 Not Found.  So the 404 gives you a numeric item to lock onto, and then Not Found is like the English version of that because you can imagine maybe some website or some browser would just print on the page the English tail at the end of that error.  Okay, well, Error Code 418's English part is "I'm a Teapot."  



LEO:  Short and stout.  Wow.



STEVE:  So if you make a query to a web server, and it gives you back HTTP Error 418 I'm a Teapot, well, you know that the web server has a sense of humor.  Anyway, so this tweet was the result of a Reddit thread because apparently this is about to be removed from Node.



LEO:  Awww.



STEVE:  And a person said, "We've got to do something."



LEO:  No, don't take that out.



STEVE:  We don't want to lose our teapot.



LEO:  There may actually be history.  I mean, remember there was the Cambridge Coffee Pot was one of the first spy cams, many, many years ago.  It may actually be that there was a teapot server.  And maybe that was the - don't ask me anything.  I'm a Teapot.  I can't - send me no GET commands.



STEVE:  That'd be perfect.  I'm a Teapot.  So Evron Baldwin retweeted something from I Am Devloper, and that's his handle.  And this is just so good.  This has got to be, well, certainly the Tweet of the Week.  So what I Am Devloper tweeted was a pro tip which is brilliant:  "Set your password to be the word 'incorrect,' and every password message becomes a password hint."  Your password is incorrect.  Oh, you're right.  Thank you for the reminder.



LEO:  That's like the Hobbit password.  What was it?  Say friend and pass or something like that?  I can't remember what it was.  It's one of those hide in plain sight passwords.



STEVE:  Yup.



LEO:  Yup.  Love it.  Steve, we've come to the end of another fabulous edition of Security Now!.



STEVE:  At the end of another fabulous year.



LEO:  Ah.  Year 13 begins next week.



STEVE:  Next week.



LEO:  Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to watch live.  Many of you do.  And if you do, you should probably be in the chatroom at irc.twit.tv so you can join the kids in the back of the class.  They knew about I'm a Teapot, I think.  Not required, of course, because we have many ways to watch the show on demand at your leisure, including, of course, Steve's site, GRC.com.  He has audio and transcriptions of each and every show at GRC.com.  While you're there, pick up a copy of SpinRite, world's best hard drive maintenance and recovery utility.  You never know when it will come in handy.



STEVE:  It'll keep your drive alive until it runs out.



LEO:  Keep your drive alive.  You'll also find information about SQRL, Password Haystacks, ShieldsUP!, all sorts of freebies.  Steve gives everything away except for that one bread-and-butter program, SpinRite.



We have our copies of the show at TWiT.tv/sn, audio and video, should you choose to watch Steve gesticulate.  And my suggestion, my strong suggestion is subscribe to the show because a lot of times we mention previous episodes.  You might want to go back in time.  And the best way to do it is to have a complete catalog of all 600, what is it?



STEVE:  624 and counting, baby.



LEO:  624 episodes.  All 12 years.  The complete set.  Steve, thank you so much for a great 12 years.  Here's to 12 more.  Well, I guess we'll get to 10 more, maybe.



STEVE:  Okay, my friend.  Thanks so much.



LEO:  Take care.



STEVE:  Talk to you next week in the beginning of Year 13 with Episode 625.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#625

DATE:		August 22, 2017

TITLE:		Security Politics

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-625.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the continuing Marcus Hutchins drama, the disclosure of a potentially important Apple secret, a super cool website and browser extension our listeners are going to appreciate, trouble with extension developers being targeted, a problem with the communication bus standard in every car, an important correction from ElcomSoft, two zero-days in Foxit's PDF products, lava lamps for entropy, the forthcoming iOS 11 Touch ID kill switch, very welcome Libsodium audit results, a mistake in AWS permissions, a refreshingly forthright security statement, a bit of errata, miscellany, and a few closing-the-loop bits from our terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here and, as always, whew, just in the nick of time.  He's going to explain what that Apple Secure Enclave Processor exploit means and whether it's time to worry.  He's also got more thoughts on Marcus's arrest and maybe why the feds are after him.  There's a whole lot more.  It's a jam-packed edition of Security Now!, coming up next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 625, recorded Tuesday, August 22nd, 2017:  Security Politics.



It's time for Security Now!, the show where we cover your privacy and security online with this cat right here, the man in charge of the operation, Mr. Steve Gibson of GRC.com.  We were talking reverse Polish notation before the show today.



STEVE GIBSON:  Ah, yes, our favorite calculator modes.



LEO:  Well, did you see that calculator?  It was Andy's pick for MacBreak Weekly.  It was a perfect reproduction on an iPad of the HP-65.  It was amazing.



STEVE:  Nice.  I did not see it.



LEO:  You didn't see it?  Oh, you would love it.  I bet you even have an HP around, probably four in the freezer; right?



STEVE:  Yeah, I actually just, you know, here's two within reach at the moment.  One is the 15C Scientific, and the other is the 16C.  So they're never far away.  And in fact just last week I changed one out because the "8" key was beginning to get a little funky, and I was noticing I was having some entry errors.  So I thought, okay, I've got 12 more of these.



LEO:  Wait a minute.  You use it?



STEVE:  Oh, daily, yes.  Remember, I'm an engineer, too, Leo.



LEO:  But we have computers for that now; don't we?



STEVE:  No, nothing is [crosstalk].



LEO:  So look at this.  You will love this.  This is a perfect reproduction of an HP-65.  And what's really cool, you know all the cards that they came with.



STEVE:  Yeah, yeah, yeah, the little magnetic...



LEO:  They're all available for free.



STEVE:  Oh, that's nice.



LEO:  So the next time you have to do your capacitance of parallel plates calculations, you just load that card into the HP-65.  Watch how they put the card in.



STEVE:  Oh, my lord.  Nice.



LEO:  And it reads it, and there's a little description of it.  You've got your paper tape here.  You can advance it.  Even the - what's really nice, Andy was pointing this out, even the segments on the display, they're not just using some font.  They're actually really drawing it.



STEVE:  Nice.



LEO:  Pretty sweet, huh?



STEVE:  It's very cool.



LEO:  I think it's one guy in Switzerland who makes this software.  It's the HP-65 or rather RPN-65 Pro is what he calls it.  [Crosstalk] HP.



STEVE:  Well, and we've talked - I have on every one of my iDevices a copy of PCalc.  And I think it's available on Android.



LEO:  Oh, I love PCalc, yeah.



STEVE:  I mean, it's - when I'm out and about, I don't carry a calculator with me.  I'm beyond that stage.  But that's actually absolutely my go-to calculator.  But when I'm here and I've got - I can physically hold this thing in my mind, the HP is just - it's just perfect.



LEO:  Yeah, that makes sense, yeah.



STEVE:  We started the podcast recording, didn't we, a while ago.



LEO:  Oh, yeah.  I'm recording.  Yeah, why?



STEVE:  So this is No. 625 for August 22nd.  And I was really struggling for a name for this because nothing really stood out.  But I want to talk about a number of things swirling around Marcus Hutchins, who as we've discussed for the last couple weeks was picked up at the Las Vegas airport upon trying to leave to return to his homeland, his home country, Britain, by U.S. law enforcement.  And there's an interesting technical side to this which caused me to call this podcast "Security Politics" because, as often happens, when we don't have enough facts, guesses fill the void.  And there is some interesting stuff that I wanted to address about what is going on in this realm, so we're going to talk about that.



We've got the disclosure of a potentially important Apple secret that I wasn't able to watch you guys discuss on MacBreak because I was busy pulling all this together.  So we'll have my take on that.  A super cool website that our listeners are going to love that includes a browser extension.  Trouble with Chrome extension developers being targeted by attackers.  A problem with the communication bus standard in all of our cars, which cannot be fixed.  An important correct from ElcomSoft regarding that benchmark we shared last week which was wrong in a very important and critical way.  Two zero-days in Foxit's PDF products.  Lava lamps actually being used by a company we know well for entropy.



LEO:  Ah.



STEVE:  The forthcoming iOS 11 Touch ID kill switch.  A very welcome Libsodium audit result from our friend Matthew Green.  A mistake in AWS permissions and what that means.  A refreshingly forthright security statement from a company, I don't even know what they're making, but I just loved what they said in their "What could possibly go wrong?"  It's so forthright and upfront.  A bit of errata, some miscellany, and then - time permitting, and I didn't expect we'd have much, so I only pulled two - interesting closing-the-loop bits from our terrific listeners.



LEO:  Nice.



STEVE:  So I think a great podcast.  So, wait.  They have their own top-level domain?



LEO:  Yeah, AWS.  You didn't notice that.  All of the ads about AWS...



STEVE:  Yow.  Well, there's...  



LEO:  Hey, it's only a couple hundred grand; right?



STEVE:  There's an expression of clout.



LEO:  Yeah.



STEVE:  Holy...



LEO:  No, you can buy it.  No, you can buy it.  When ICANN announced this - WordPress bought .blog.  And it was like, I don't remember the exact price, but it was like $150,000.



STEVE:  Oh, wait a minute, where's my wallet?  I need that.



LEO:  You need a GRC, steve.grc.



STEVE:  Finally, a use for those bitcoins that I've been holding onto.



LEO:  How many do you have?  You said you got 50 in one blow.



STEVE:  Yeah.  I woke up the next morning after the podcast, and there was 50.



LEO:  Now, you couldn't do that now.  In fact, all the big...



STEVE:  No.



LEO:  Does Mark still do his bitcoin mining thing in Arizona?



STEVE:  Yes.



LEO:  He does.  Because the key to it now, and this is - by the way, Satoshi Nakamoto, very smart, the whole thing was planned almost as if he knew exactly what he was doing, to cost more over time in terms of resources, particularly power.



STEVE:  We discussed it in our bitcoin episode [SN-287] years ago that there was a deliberate tracking, pre-planned, exponential curve where the difficulty of the coin would follow a trajectory.  And it automatically scaled independent of the rate at which hardware became faster because it used time.  And so if there was like a sudden breakthrough in the ability to solve these problems.  And the problems are coming up with an input to a hash that has some number of zero bits.  And the more zeroes in the output, the more difficult it is to come up with something - you don't care about the non-zeroes, but you just have to come up with something that generates, has some number of zeroes like at the right-hand side of the hash.



And so, for example, if it was just you needed one zero, well, half of the things you put in would end up with hashing to a zero in the least significant bit.  If you required two zeroes in the two least significant bits, one quarter of the guesses would immediately give you just random guesses of what to put into the hash function would give you two zeroes in the least significant place, and so on.  So by slowly increasing the number of zeroes that are required, this scales the difficulty of coming up with a value which hashes with that many zeroes and number of least significant bits.  And so the algorithm is independent of the rate of change of hashing ability.  I mean, it was immaculately conceived.



LEO:  It's brilliant, yeah.



STEVE:  And we covered all this back in our bitcoin podcast [SN-287].  So if anyone's interested, it's all laid out in a podcast years ago.



LEO:  But the upshot of it is that it's not merely better hardware, and there's custom ASICs and stuff now.  Really it's the cost of power. 



STEVE:  Yes.



LEO:  So the reason Mark can do it, and it makes economic sense, I guess, is his power must be really cheap.  But most...



STEVE:  Correct.  In California it is no longer economic...



LEO:  You can't do it.



STEVE:  No, you cannot mint bitcoin.



LEO:  No, power's too expensive.



STEVE:  Apparently there's a massive facility under Niagara Falls.



LEO:  Yeah, because they have hydro, yeah.



STEVE:  Exactly.



LEO:  But most miners are now in China, and I suspect subsidized by the Chinese government.  And they're mostly near hydroelectric plants.  And I think it only makes sense because they're paying virtually nothing for their power.  Anyway, you have what, 50?



STEVE:  Yes.  Back in the day my one i7 machine, left running overnight - and back then that's how many - so that's how many bitcoin you got when you solved one problem.



LEO:  Wow.



STEVE:  And so I woke up, and it said, oh, 50 bitcoin.  It's like, oh.  And, you know, that wasn't a big deal back then.  Now we're at $4,000 for a bitcoin.



LEO:  And I read in Forbes earlier today that one guy thinks it's going to go to something like $600,000 per coin.  So you, sir, have a retirement plan.



STEVE:  Yeah, I've got to find those.  I have them somewhere.



LEO:  Do not cash in those bitcoins.  In fact, it's worth enough to buy .steve, if you wanted it.



STEVE:  Oh, don't tempt me.



LEO:  You could have 50 bitcoin or .steve.



STEVE:  That's the ultimate vanity domain.



LEO:  OMG.



STEVE:  *.steve.



LEO:  Oh, man.  I'd like .leo.  I really would like .leo, man.



STEVE:  Oh.  Well, you know, maybe if you just brush your tongue some more, Leo...



LEO:  There's probably some other hoops to jump through.



STEVE:  ...you can convince your wife to give you permission to...



LEO:  Yeah.  "Honey, can I buy a TLD?"  "I don't know what that is, but if you need it...."  "Yeah, I do."



STEVE:  Ooh, well...



LEO:  Apparently from 2012 the price of an application for your own TLD is $185,000.



STEVE:  But it is not that much.



LEO:  It's not going up at the rate Bitcoin's going up, that's for sure.



STEVE:  That's certainly low, Leo.  That's, oh, boy.  Okay.  So the caption on this photo reads - our Photo of the Week.  Quote:  "Every employee has been implanted with biometric access RFID tags enabling secure access control to our extremely security-sensitive facility.  It is utterly state of the art and cannot be penetrated."



LEO:  And there's the picture.



STEVE:  The lesson, of course, being, ah, yes, the human factor can defeat any form of security that has been designed.



LEO:  So somebody's put a big rock keeping the door open.



STEVE:  Yes.  Meanwhile they're all limping around with, like, bandages on their thumbs because they've been implanted, and they're waiting for this thing to heal.  And someone walks over and says, oh, screw this, puts a rock out to hold the door open.  It's like, okay, yeah.



LEO:  Well, now, doesn't Level 3 do like one of those airlock things?  You've got double doors.  You can't have somebody butt-riding you.



STEVE:  Yeah.  I was told it's all computer controlled, camera monitored.  And you have automated access.  But the guy told me something I will never forget.  He says:  "When you leave, make absolutely sure this door is closed because our system monitors it, and all hell will break loose if this door remains open for longer than is reasonable" for someone to walk through carrying, you know, maybe two people at each end of a big server kind of a thing.  So it's not allowed to remain open for long.  And there are people on-premise.  There's a security guard who will come running with his hand on his hip in order to make sure that nothing nefarious is going on.  So, yeah.



Speaking of nefarious or not, Marcus Hutchins.  One distressing piece of information which came to light that we haven't covered here is that Marcus, who as we know pled not guilty, is facing six charges and up to 40 years in prison.  So I'm sure all of us feel as I do.  We need justice to be done, and I'm so glad that, as I have said each week, that one thing we can be assured of is that this has come to the attention of people who can provide him with a first-class defense so that there will be no miscarriage.  And for that I'm glad.



We also learned that the U.K. knew what was going to happen.  They were aware that Marcus - who, as we know, helped them. He stopped the WannaCry worm which was decimating the U.K.'s entire NHS, their National Health Service IT infrastructure.  GCHQ knew that he was under investigation by the FBI before he traveled to America and that he would be walking into a trap that was being set for him before he was then arrested by U.S. authorities for these alleged cyber offenses.  Multiple reporting indicates that the British government allowed him to wander into this trap because it saved them the headache of what would then have been a highly charged extradition proceeding with the U.S., who is of course an ally.  So they just said, oh, have a nice trip.



Okay.  So what's developing in this case?  Any time something important is happening, and this is why I titled this "Security Politics," people will form opinions.  And then those opinions evolve into positions.  Egos become engaged, and those positions start being defended, often before or beyond the point where they're supported by fact.  It's the politics of humanity.  And this is what's starting to develop in this interesting case with Marcus because there's a lot of background, but it doesn't have the context that is required for us to know what it means.



So during the past week there have been a number of new reports, some suggesting that Marcus was more deeply involved in the development of the Kronos banking malware, others arguing that the code was lifted from him by the Kronos malware authors, and some even alleging that he was more directly involved with the creating of the WannaCry worm.  So some of this confusion surrounds the origins and refinement of a commonly employed programming technique known as "function hooking" and the need for something called a "trampoline."  These are well-established, well-understood terms of art for antiviral software and also employed by malware.  Now, to create somewhat of a historical background, 2.5 years ago, on January 8th, 2015, Marcus at his MalwareTech.com blog posted a two-part tutorial, an explainer about these technologies, function hooking and trampoline.



So to give everyone a sort of a sense for who he is in this context, he wrote, 2.5 years ago, 2015:  "A lot of my articles have been aimed at giving a high-level insight into malware for beginners, or those unfamiliar with specific concepts.  Today I've decided to start a new series designed to familiarize people with malware internals on a programming level.  This will not be a tutorial aimed towards people creating sophisticated malware, but security enthusiasts looking to better understand it."



So then he has a topic, "Inline Hooking."  He says:  "What is it?"  Okay.  "Inline hooking is a method of intercepting calls to target functions, which is mainly used by antiviruses, sandboxes, and malware.  The general idea is to redirect a function" - that is, a function call, a call to a function - "to our own" - he meant code - "so that we can perform processing before and/or after the function does its work.  This could include checking parameters, shimming the function, logging calls to it, spoofing its returned data, and filtering calls.  Rootkits tend to use hooks to modify data returned from system calls in order to hide their presence" - and we discussed rootkits years ago using exactly this technology.



And again, we discussed rootkits and this technology years before Marcus posted this.  So this wasn't his invention, and he's not claiming that it was.  He says:  "...while security software uses them" - that is, hooks - "to prevent/monitor potentially malicious operations.  The hooks are placed by directly modifying code within the target function, called 'inline modification,' usually by overwriting the first few bites with a jump instruction.  This allows execution to be redirected before the function does any processing."  And he concludes, or I'm concluding my quote of the top of his post: "Most hooking engines use a 32-bit relative jump which is opcode hex E9, which takes up five bytes of space."  And then he goes into detail in his second part, the assembly language code, mixed C and assembly, of the actual implementation of this technology.



I looked at it this morning.  I've never seen that code before.  Yet I read it for the first time, and I was completely familiar with every aspect of it.  So I don't know, from what Marcus subsequently tweeted that has gotten him in some trouble, I don't know - he may have felt that he invented it.  It may be a refinement of something, I mean, of what had to be common practice 2.5 years ago.  I mean, rootkits are older than Marcus is.  So I don't quite understand why he then later said what he did.  But, for example, I have code from 1988 which does this because I had a...



LEO:  You might not want to say that out loud.



STEVE:  Well, I had a developer working for me named Michael Toutonghi, who was brilliant.  And we were developing a super high-performance disk cache back in the DOS days and the early Windows days that we called "Propel."  And Mike needed to hook the operating system and the DOS compression engine and all this stuff.  And this is the way you do that.  So Mike went on to Microsoft, where he became one of very few Distinguished Engineers, which is a formal designation of the top people they've ever had.  And Mike was the original architecture of the entire .NET system.  He did all that.



So my point is that, back in 1988, this is what we were doing.  And this reference to the trampoline, if you have a function, and you want to hook it, you want to intercept some other call to it, well, you need to replace the beginning of it with a jump instruction to you so that, when something else tries to invoke that subroutine, instead of seeing the beginning, instead of encountering the beginning of the subroutine, it encounters the jump instruction you have stuck there.



So, but notice that, in putting a jump instruction there, you have had to overwrite the first few instructions - the first few bytes at least, five bytes, which is why he referenced the size of the 32-bit jump instruction - you've had to overwrite the original beginning of the function.  So what is done is there's an instruction interpreter as part of the hooking system which reads the instructions, like the beginning of the function you're going to hook, in order to extract an even number, that is, the exact number of bytes and instructions that can then be relocated to your own code.



So you first look at what's there, figure out what the instructions are that you're going to smash by putting a jump there.  You copy those instructions to your hook.  Now the jump that you put there jumps to what's called the "trampoline," because you're going to jump off of it again.  So you jump onto the trampoline, which executes the beginning instructions that you had to overwrite to put your jump instruction there and then jumps back and continues execution.  So what you've done is you've hooked that function.  Anyone who calls that runs through you first before going in.



So you have two things you can do.  You can call, after your hook gets control, you can call the function yourself, that is, you call your own little trampoline, which then invokes the function.  And when it's done it comes back to you.  That allows you to filter the result.  Or you can inspect the parameters that are being used first and, for example, abort the function, just return to the caller, so don't do what it said.  For example, there's a function called "virtual protect" which turns on protection for virtual memory.  Well, you could just short-circuit that so software thinks it's calling the virtual protect function, but it's neutered.  It doesn't happen.



So the point is this is all super well understood.  Everybody, you know, for decades has been doing this.  But then, oddly, a month later, Marcus tweeted something.  And his tweet contains the "F" bomb, and you know me, I don't use it casually, and I avoid it on this podcast.  But I decided, and for a while I had concatenated it or hyphenated it, and I thought, it just doesn't convey his sense.  And it's important for us to understand who he is.



So I'm going to read his tweet.  I have a link to it in the show notes.  On February 7th - so remember, his posting was January 8th, 2015.  A month later, on February 7th, he posted, and it's still there on Twitter:  "Just found the hooking engine I made for my blog in a malware sample.  This is why we can't have nice things [bleep]."



So he's not happy.  And it's clear he's not happy.  Maybe it was an exact copy of what he did.  But, I mean, it's not like they couldn't have gotten it elsewhere.  It's not like this wasn't like a well-known, well-established technique.  It was.  And one of the things that's confused people is his use of a particular instruction.



Dan Goodin, writing for Ars Technica, said:  "Shortly after his arrest in Las Vegas two weeks ago, the tweet resurfaced" - that is, this tweet from 2.5 years ago - "and almost immediately it generated speculation that the malware Hutchins was referring to was Kronos.  An analysis of Kronos" - and, by the way, that's the well-known banking malware which is a known source of trouble.  "An analysis of Kronos soon showed that one portion used an instruction that was identical to the one included in the code Hutchins published in January 2015."



Okay.  Now, the instruction in question is what's called a "monotonic function."  It exists in the Intel instruction set.  It's a compare-and-exchange instruction.  There are separate compare instructions, and there are exchange instructions where you swap two values.  But performing those two functions separately, that is, doing a compare and then a conditional exchange, that allows the possibility that a thread context switch could occur, that is, an interrupt could occur.



Back in the old days, when we had a single core, you have a single processing core, but you've got a multiuser, multitasking, or more properly a multithreading environment.  What that means is that many things are going on at once, but you only have one actual CPU.  So typically there's a hardware timer that is ticking in the background.  And when that ticks, it yanks control away from wherever the processor was back to the OS, which then gives another thread some time to run.  So this gives the illusion that all the threads are running at once, where in fact we're just switching among them very quickly.



Well, there are so-called "race conditions" that you can get into where, for example, you have something that needs to remain coherent in a multithreaded environment.  Say, for example, you want to increment a 64-bit value, but you've only got a 32-bit chip.  Well, the math is not hard.  We know that you increment the low 32 bits.  And if the carry overflows, that is, if it wraps around to zero, then that means you need to increment the high 32 bits to create a 64-bit counter.



But you cannot do that.  It's hard for people to grasp.  But you cannot do that safely in a multithreaded environment because there is a chance that you could increment the low side of that counter.  And before your code has had the chance to immediately execute the conditional increment of the high 32 bits, it's interrupted, and another thread comes along and increments the low side.  Well, now, if the first thread caused a wrap, then it didn't have a chance to finish yet.  So now this other thread increments the low side, but it won't wrap.



Anyway, you can see what happens is this breaks a counter that was intended to reliably count the number of times it happened.  But because it's not monotonic, because there's no way to do a 64-bit increment at once, we run into a problem.  So in the old days, we would turn interrupts off.  Before doing that, you would disable the hardware interrupts so nothing could interrupt your code in just that tiny interval of sensitivity where you must not be interrupted to create, to sort of fake a monotonic operation.  Then you'd immediately reenable hardware interrupts.



But now we've got multiple cores.  And so we don't have virtual multithreading, where we're switching around between threads.  We have hardware multithreading, where actual physical cores are all working at once.  So what had to be designed was instructions that could solve this problem.  Thus the compare-and-exchange is one of the - there are several of these Intel instructions that are explicitly and deliberately thread safe, meaning that it does everything it needs to do at once in a single instruction, and you don't have to worry about it being, like, having to do a compare-and-test and a conditional exchange, which might get fouled up if thread-changing occurred.  My point of all this is that this is the way you solve that problem.  So Marcus's use of this instruction is like saying, gee, what instruction would I use if I had to add two numbers?  Uh, how about the add instruction?



LEO:  I'm sure a jury will understand that.  That's just...



STEVE:  Yeah, I mean, and this is the problem.  This is why I stopped being an expert witness is that these technical things are so clear to everyone who understands them.  But I told you the story, Leo, about the judge who was on oxygen.  I mean, I'm not kidding, he had a green oxygen tank next to him.  I was an expert witness for NEC because Princeton Graphic had sued them over an ad about the MultiSync monitor being able to have a long life because it would adapt to whatever you plugged it into, which was true.  But Princeton Graphic didn't have that technology, and they were annoyed.  And so I was trying to explain to this judge who literally was on oxygen about, okay...



LEO:  At least he was awake.



STEVE:  No, actually, he was nodding off, too.  So maybe he needed to turn it up a little higher.  But so this is the problem where, I mean, our listeners, you and I, I mean, we get this.  But again, and this is why we need a technically competent defense with lots of charts and graphs who can explain that this instruction is - there's nothing magic about it.  It has a purpose for which Marcus used it.  And arguably there is no other way to solve that particular problem than to use it.  And so the fact that some other code also used it absolutely doesn't mean that they got the idea from him or that he gave it to them.



I mean, first of all, remember, this was a public posting, a blog posting.  So a month later he tweeted he was annoyed that apparently - and my sense is maybe he was claiming a little more ownership of this than he should.  As I said, this is the way I would have solved the problem if someone said, okay, Steve, write some code, Windows code that does this.  I'd just sit down, and I would write what Marcus wrote.  That's what an engineer who understood the problem and what tools were available would use.



So anyway, again, exactly as you put your finger on it, Leo, the problem is the truth of this rests in some details which I'm just hoping a very good defense will be able to bring to light because that would be important.



LEO:  Well, is that specifically what he's being accused of is that?



STEVE:  No.



LEO:  We don't know.



STEVE:  Yeah.  We don't yet know what the allegations are against him.  Well, six counts of something with apparently up to 40 years of prison time.  So that just can't be allowed.  And, you know, as I was reading what he wrote, I'm thinking, no wonder he's popular.  He's so literate, too.  I mean, that was beautifully written, his description of the way the hooking and trampoline works.



Okay.  So Part 2 is - that's Kronos.  So what of WannaCry?  There's a head of a security firm Immunity named Dave Aitel.  He had a different immediate response to the reports of Marcus being, quote, and I'm just paraphrasing, "the hero of WannaCry," which of course we all understand because he created that domain, he registered that domain and shut down the propagation.



So in a posting, in Dave Aitel's posting at the Immunity blog earlier this month, he wrote:  "But let me float my and others' initial feeling when MalwareTech got arrested:  The kill switch story" - and here I will use an abbreviation because it's not important - "was clearly BS.  What I think happened is that MalwareTech had something to do with WannaCry, and he knew about the kill switch.  And when WannaCry started getting huge and causing massive amounts of damage, say to the NHS of his own country, he freaked out and, quote, 'found the kill switch,' unquote."



LEO:  Ah.  That's interesting.



STEVE:  Yeah, I know.  "This is why he was so upset to be outed by the media."  And then Dave says:  "Being afraid to take the limelight is not a typical white hat behavior, to say the least."  And then Dave continues, and backs off a little bit or, like, adds a little more depth.  He says:  "That said, we need to acknowledge the strategic impact law enforcement operations as a whole have on national security cyber capabilities, and how the lighter and friendlier approach of many European nations avoids the issues we have here in the states."



He writes:  "Pretty much every infosec professional knows people who have been indicted for computer crimes by now.  And in most cases the prosecution has operated in what is essentially an unfair, merciless way, even for very minor crimes.  This has massive strategic implications when you consider that the U.S. Secret Service and FBI often compete with Mandiant for the handling of computer intrusions, and the people making the decisions about which information to share with law enforcement have an extremely negative opinion of it.  In other words," he writes, "law enforcement needs to treat hacker cases as if" - and I like this - "they are the LAPD prosecuting a famous actor in Hollywood.  Or at least that's the smartest thing to do strategically, and something the U.S. does a lot worse than many of our allies."



So we're all aware, I mean, addressing David's point, we're all aware of the very real concern, to draw an analogy, over a hopefully fictional, highly virulent super virus being developed purely for study in a lab, somehow later escaping from the lab into the wild and wreaking havoc upon humanity.  Now, imagine a malware author who recently learned of the disclosed weaponized EternalBlue technology which we believe was weaponized and developed by the NSA, and who has a deep technical background in malware operation, being unable to resist the temptation of experimenting with that technology, and who creates an instance of a highly virulent worm which carries cryptomalware - why not - as its payload.  It makes it a little more exciting and a little more real.



And because this researcher is not insane, he or she builds in a kill switch, just in case.  Then somehow, some way, it finds its way out, like it's scanning.  And within the researcher's network there was an unknown or unappreciated vulnerability.  But, like, it gets loose, as viruses and worms will, and escapes its containment, exploding onto the public Internet.  In this scenario, this wasn't what the researcher ever intended, but it happened nevertheless.



So now what does he do?  He discovers his own responsibly installed kill switch and immediately registers the domain to shut down this creation of his which escaped his control.  I certainly hope that's not what happened because then we're faced with a moral dilemma while, as Dave noted above, U.S. and global law enforcement won't have any dilemma.  They will throw the book at Marcus.



So anyway, this is what's going on on the Internet in the technical forums and people combing through the evidence, looking at the code, and also just using, as Dave did, sort of his gut feel of how reasonable is the story that, wow, wasn't that great, you know, Marcus happened to find this wacky domain and thought, oh, I wonder what that does, and it shut down.  I mean, remember how skeptical we were about why any actual malware author who wanted to be wreaking havoc with cryptomalware would put a kill switch in.  It's like, why?  It seems antithetical to the intent of that worm, except if it was an experiment.  I mean, if someone responsible, fundamentally responsible did this in the lab and knew enough to give it a kill, like just in case, and the worst happened.  I mean, again, I think eventually we'll have answers to some of this really interesting case which I think is fascinating.



LEO:  Now, if he wrote it, whatever his intentions are, I think he's in trouble.



STEVE:  Precisely my point.  That's right.  U.S. law enforcement will have no moral dilemma.



LEO:  Yeah.  Well, it reminds me of the Morris worm, remember.  Robert Tappan Morris, when he wrote that - I think that was his name - when he wrote that worm, didn't write it to be a worm - a it was the very first computer worm - and was a little horrified that it was so effective.  Didn't stop anybody from prosecuting him.



STEVE:  Okay.  So I did not have a chance to hear you guys talk about the loss of or the escape of the Secure Enclave key.  So I'll explain my take on it, and then I'd like you to add what you guys discussed on MacBreak.



LEO:  I'll echo Rene's thoughts on it, yeah.



STEVE:  So, okay.  Apple had been keeping a secret.  It's unclear so far how important keeping that secret had been, but it is secret no more.  We do know that cryptographic security does require some secret keeping.  And as our listeners, our longtime listeners will know because we've discussed this in the past, the breakthrough, which was made quite some time ago in cryptographic maturity, occurred when we switched from keeping algorithms secret to developing keyed algorithms where the algorithms themselves could be made public, and then keeping specific instances of their usage keys were what was secret.



And when you think about it, just that, switching from secret algorithms to public algorithms and secret keys, that dramatically improves security because it distributed and spread the secret exposure risk.  Beforehand, if the secret algorithm were discovered, every usage of it would simultaneously be compromised.  But now, with keyed crypto, the disclosure of one key only compromises the secrets that are being kept under that key.  No one else's use of the same algorithm under different secret keys is compromised.



So the point is there still remains secrets we need to keep, and understanding that sets us up for understanding maybe what this means.  So Apple somehow lost control of their secret, that is, what they had been keeping a secret, the Secure Enclave firmware decryption key, and last Thursday it was published.  That key allows for the decryption and exploration of the Secure Enclave Processor's firmware in full detail.  And that was a closely guarded secret.



And so this is something that Apple has historically tried to prevent, not necessarily because it represents the end of mankind as we know it, but just because it's proprietary Apple code, and it's no one else's business.  And it could be, emphasis on "could," an unexpected treasure trove for curious security researchers and malicious hackers alike.  And the research community will doubtless be gaining much more information from it, like from the code that used to be secret that runs the Secure Enclave.



So I began this with a reminder about the nature of modern-day crypto.  As with all modern cryptographic systems, disclosure of the algorithm is not a huge concern.  So that's what we have had now is a disclosure of the Secure Enclave algorithm.  So, per se, it's not a huge concern.  But there are two dangers, the least bad of which is perhaps the more likely given the general history of complex unaudited code, would be that combing through Apple's code with an adversarial posture, which as we've often discussed is extremely difficult for Apple's own developers to ever really hold, thus the benefit of third-party audits, those with nefarious intent might discover some mistakes in the Secure Enclave Processor's code that could be leveraged into a powerful exploit that Apple could do nothing to prevent.  We don't know.  But now there's a chance that a mistake could be found.



But the biggest danger, and I haven't looked at it closely enough, and it'll take a while for what this means to mature more and to emerge, the biggest danger would be that the Secure Enclave Processor firmware itself contains embedded private keys whose security is crucial and whose disclosure, for some period of time until Apple can arrange to change them or do whatever needs to be done, could represent theoretically a catastrophic information leak.  At this point we don't know.



Hackaday had a nice write-up.  They said:  "The SEP [Secure Enclave Processor] is a security coprocessor introduced with the iPhone 5s, which is when Touch ID was introduced.  It's a black box that we're not supposed to know anything about."  But this hacker who goes by the handle "Xerub,"  "XErub," I guess, or "Zeerub," don't know how you want to pronounce it. 



LEO:  I said "Cherub."



STEVE:  Okay, yeah, that's probably good.  Xerub has now pulled back the curtain on the Secure Enclave Processor.  "The Secure Enclave handles the processing of fingerprint data from the Touch ID sensor and determines if it's a match or not" - that would be fun to look at.  Apple doesn't want us to, but still - "while it also enables access for purchases for the user," and many other functions, of course.  "The SEP is a gatekeeper which prevents the main processor from needing to access sensitive data.  The processor sends the data, which can only be read by the SEP, which is authenticated by a session key generated from the device's shared key," blah blah blah.  I mean, those details don't really matter.  There's been lots of write-ups about it.  Apple has given us a broad brush, and there was a "Demystifying the Secure Enclave Processor" talk at Black Hat not long ago.



And here's what was more worrisome.  It also runs its own OS called SEPOS, Secure Enclave Processor OS, which has a kernel, services, drivers, and apps.  And, yes, that frightens me because that means it's not just a simple crypto engine, it's a much more complex subsystem, the code for which is now public.  And as we know, security and complexity are always in constant conflict.  So the SEP performs secure services for the rest of the system on a chip that is the backbone of the iOS devices, and much more.



So this Xerub guy said, quote, after disclosing something that Apple desperately did not want to have disclosed and had deliberately kept secret:  "Hopefully Apple will work harder now that they can't hide SEP, resulting in improved security for users."  Uh-huh.  Yeah, that's his concern or his hope.



What remains unknown is how this Xerub guy obtained this presumably large key.  Now, I have a guess.  My guess is that it's the same old problem we've discussed here over and over and over.  Some secrets can be kept, like a remote web server's private key, because the work it does is done remotely, and the only access we have is over a protocol like TCP, and specifically TLS, where the key is used, but is never - we have no remote access to it.  And for it to do its job we don't need access to it.



But other secrets inherently cannot be kept, like, as we've often discussed, a DVD's secret key, which must be present and in use to play back an encrypted DVD.  Or in this case, similarly, the Secure Enclave's firmware decryption key must also be at least temporarily transiently present to decrypt the processor's stored firmware in order for it to be executed and used.  So the key is in there, and it had to briefly come out of hiding.  And this guy, I'll give him props for cleverness, apparently managed to capture it somehow.  So what were you guys talking about on MacBreak, and what were Rene's thoughts?



LEO:  Pretty much like as yours were.  One is that, of course, security through obscurity is never a great idea.  However, it doesn't hurt to have a number of layers.  And Apple's smart enough, he felt, not to have depended solely on that.  Obviously it doesn't want people to bang on the SEP if they can avoid that.  But now that people will be, my thought was Apple at this point needs to raise the value of its bug bounty because it's not really competitive.  It's offering hundreds of thousands of dollars when malicious actors and governments are offering millions of dollars.  And certainly, I mean, I don't know, but if there were an exploit in the Secure Enclave Processor, and it were to be sold to the NSA or a bad guy, that would be - probably a bad guy wouldn't have much use for it, I would think.  But certainly a government might.



STEVE:  You think Apple has the money, Leo?



LEO:  [Laughter] Nah, they're broke.  So it's not a good thing, but on the other hand it can't possibly be the only security they've got for this thing.  They just have other things going on.



STEVE:  Yeah.



LEO:  And so, yeah, I mean, what's going to happen now, and it's happening as we speak, I'm sure, is all sorts of people are banging on this thing, seeing if they can find flaws.  As we've learned in the show, all software has flaws.



STEVE:  And the more complex it is, the more likely those are.



LEO:  Right.  What wasn't clear to me is what, and I guess it really depends on the exploit, but what could be done with it.  I doubt you'd have a mass exploit that would exfiltrate everybody's password.  What good would that be anyway?  It's really going to be one of those targeted, I would expect, vulnerabilities that somebody like the NSA could use if they got a terrorist's phone.  Well, hey the Secure Enclave, the fingerprint, Touch ID is not protecting them anymore because we can get right to it.  Right?



STEVE:  Yeah.



LEO:  It could be used to unlock a phone, I would think.



STEVE:  The problem is, I mean, I was distressed to learn that it's so complex, that it's an operating system.



LEO:  Yeah, that's interesting, isn't it, yeah.



STEVE:  Processors and drivers and blah blah blah.  So, I mean, Apple has no need to publish it, so they chose not to.  I don't think this is the end of the world.  But we're not going to really know until the community looks at it.  I mean, again, it could be that nothing will be found.  It could be that there are some edge cases that can be leveraged.  We just don't know.  It's a functional module.  They didn't need to share it.  And they recognized better not to because it's complex.  And so you'd rather not let people poke at it.  Now, as you said, that's going to start happening.  And we'll know more a month from now.  And presumably this is all remotely, over-the-air updatable.  And so Apple will change the key, revise the...



LEO:  I wonder.



STEVE:  Yeah, I do, too.



LEO:  It could be in hardware.



STEVE:  Yeah.



LEO:  I wonder.  And if you do change the key - I don't know.  I wonder.  Yeah, that would be the best-case scenario; right?  Although people already have the SEP code, right, because the cracked one's out there.  



STEVE:  It's already escaped.



LEO:  I'm sure it's on GitHub.



STEVE:  It is, actually, on GitHub.  That's where it is.



LEO:  So [chatroom visitor] had an interesting thought.  It would be possible to craft a malicious application that would, because you'd have access to Touch ID, you'd be able to authenticate purchases, for instance, using Touch ID.  Maybe some faux authentication scheme so you could have the application buying stuff and you authenticating it, without actually authenticating it.  I don't know.  It's hard to think of what exploits are possible.  But no doubt some are.



STEVE:  Yeah.  Well, and from a props to hackers standpoint...



LEO:  Yeah, nice job, yeah.



STEVE:  Well, yeah, that.  But also, what comes next?  Apple has been a little, I don't want to say "snobby," but a little, you know, we know better than everyone.  We're all about security and so forth.  So a takedown of Apple is something I could see some hackers thinking, oh, I'm going to get them.



LEO:  They've made themselves a target, yeah.



STEVE:  Right, exactly.  Okay.  I've got the nice, the coolest site and browser extension that a chunk of our listeners are going to love.  We're all familiar with the common term TL;DR, you know, Too Long; Didn't Read.  So this is a great play on that, TOS;DR - Terms of Service; Didn't Read.  The site is TOSDR.org.  The page's subhead reads:  "'I have read and agree to the terms' is the biggest lie on the web.  We aim to fix that."  Isn't that great?



They said:  "We are a user rights initiative to rate and label website terms and privacy policies from very good Class A to very bad Class E.  Terms of service," they write, "are often too long to read."  Often?  Often?  Yeah.  "But it's important to understand what's in them."  Okay.  Maybe.  I'm guilty as everybody else is of lying, fine, do whatever you're going to do.  "Your rights online depend on them.  We hope that our ratings can help you get informed about your rights.  Do not hesitate to click on a service below to have more details."



And then they add:  "You can also get ratings directly in your browser by installing our web browser add-on.  This extension" - which, by the way, is available for Chrome, Firefox, Safari, Opera, and on its way for IE.  They didn't mention Edge.  "This extension will add a small icon to the right side of your address bar.  If you are browsing a website that is not in our database, nothing appears.  Click on the icon to get the summary of the terms provided by TOSDR.  When you land on a website with very bad terms, a small notification will pop up in the bottom right-hand corner to warn you."



So, okay.  So you can go without installing anything anywhere, just scroll down through TOSDR.org because right there on the home page is a bunch of samples, all of the sites that we all know very well - Google, YouTube, SoundCloud, GitHub, and so forth.  And as is always the case, a grade is kind of arbitrary.  That is, they're looking for specific features.  They've got thumbs up, thumbs down, plus and minus things, and then they have to somehow end up, you know, distilling that into a simple linear scale.  But they do break it all down so you can see if you care about the things that the site does or doesn't do and so forth.



But anyway, just a cool - this is nice to see, sort of a maturation of the industry.  And maybe this puts a little pressure back on the companies who are, because no one is reading their terms of service, able to have egregious things in there that they're never held to.  But if it's summarized like this, and some light is shined on it, maybe that makes them think, okay, maybe we shouldn't have listened to our attorneys when they gave us this boilerplate that no one reads.



LEO:  Yeah.  You know, I'm not sure about the grade.  I think that's maybe a little reductionist.  But I love it that you can kind of look at this summary of terms.



STEVE:  Yes.  You can scan through, oh, they do this, and they don't do that.  And they do do that, and they don't do that, yeah.



LEO:  That's great, yeah.



STEVE:  Yeah, that's neat.



LEO:  And you can decide, well, do you mind that Zing doesn't allow a pseudonym?  Is that an issue for you or not?  Right?



STEVE:  Precisely.  Precisely.  Or that LastPass is there, and it's like, they reserve the right to change the terms and conditions without notice.  Well, okay.



LEO:  [Crosstalk] every single site.  That's a thumbs-down almost everywhere.  It's rare that you don't see that.



STEVE:  Yes.  Well, and again, all of these big companies, they've got attorneys.  And the attorneys, you know, are the ones who write this impenetrable boilerplate.



LEO:  Yeah, that's boilerplate.  They're going to put that in, no matter what.



STEVE:  Yup.  Yeah, anyway, very cool, TOSDR.org.  So speaking of browser extensions, this was another one of these sort of foreseeable things.  As we know, Chrome is now the number one browser on the Internet; and, if you have enough RAM to keep it happy, deservedly so.  And except for the fact that it doesn't give me tabs that I want, someday I will be using it.  But I'm still happy with Firefox.  Developers of Chrome extensions may have weaker security than Google.  So attack the point of least resistance, which is less cautious developers.



"At the beginning of the month we had the news of the hijacking of an OCR add-on called Copyfish.  By 'hijacking' we mean that attackers managed to break into the systems of the Copyfish developers and compromise their code base so that the now 'quasi-legitimate' extension" - that is, it's carrying some additional baggage that the developers didn't intend, a legitimate extension downloaded by innocent Chrome users that could then attack them in various ways.



Now, according to an update more recent at Proofpoint, Copyfish has been joined by seven additional legitimate Chrome extensions which attackers have taken over and used to manipulate Internet traffic and web-based ads.  There's an extension called Web Developer, one called Chrometana, Infinity New Tab, Web Paint, and Social Fixer, those five.  And then they also believe that two, the TouchVPN and the Betternet VPN, were also compromised in the same way at the end of last month, at the end of June.  Oh, no, I'm sorry, month before last, end of June.



So Proofpoint said:  "At the end of July and beginning of August, several Chrome extensions were compromised after their authors' Google account credentials were stolen via a phishing scheme."  So what's happening is extension developers have become a target because, if their security can be compromised, then their publication of extensions to Chrome can be infiltrated with malware.



And browser extensions are powerful add-ons to browsers, whereas JavaScript code downloaded ad hoc from almost any website today, as we know, is inherently untrusted and is therefore executed with extreme caution and containment.  Browser add-ons, while not fully trusted, have significantly greater conditional trust and so can perform many actions, on behalf of their users and autonomously, which are explicitly denied to page-based, you know, web page-delivered and -based code.  So this represents a rational attack vector and a means of compromising high numbers of unwitting Chrome users, and we're seeing this.



So I don't know what the takeaway for our listeners is.  But, you know, it's very much like how it's not completely free to download every Android app you ever encounter.  I mean, there's some wisdom associated with, do I really need this?  Because there's, you know, the probability of anyone of them hurting you is very small.  But the more of those probabilities you have add up, then the total probability becomes a bit greater.  So I would say maybe just don't go crazy.



And who knows, maybe Google will be able to come up with some solution because they certainly are very proactive about their browser security, and they're not going to want well-intending add-on developers to be the delivery vector of problems for their own Chrome browser users.  And it's a problem because the reason you do extensions is to bind tightly across sites to the browser and to obtain those extra capabilities, the things you can't do on a page.  But with that capability comes responsibility.



Okay.  So a group of researchers has developed an attack on the CAN bus.  The CAN bus, C-A-N, stands for the Controller Area Network.  That is the glue that is what makes all cars for almost two decades run.  I mean, once upon a time there were wiring harnesses that ran through our cars.  And you had a harness going to all of the lights in the back of the car.  Now you don't.  Now you have power going to the lights in the back.  And in the lights is a processor on the CAN bus.  It's exactly analogous to - you can think of it as like the old-school coax original Ethernet where there was a network backbone, and nodes tapped into it.  Well, that's how our cars are now.  There's a bus that carries this CAN protocol, and everything hooks into it - your side view mirrors, your door locks, your power windows, the engine itself, the entertainment system, the backup lights, the antilock brakes, I mean, it is the glue.



So CAN messages, including errors, are called "frames."  And these guys write:  "Our attack focuses on how CAN handles errors.  Errors arise when a device reads values that do not correspond to the original expected value of a frame.  When a device detects such an event" - that is, when it receives an erroneous frame - "it writes an error message onto the bus in order to 'recall' the errant frame and notify the other devices to entirely ignore the recalled frame."



So it's very Ethernet-like.  Remember that something that's transmitting on the Ethernet is receiving its own transmission in order to detect a collision with another simultaneously sent packet.  So although the reason may not be collision, if something trying to emit a frame onto the CAN bus detects a failure to do that successfully, it then says, whoops, and sends this recall message saying ignore that, what I just said.  And everybody will accept that.



It turns out that this occurring is very common, and it's usually due, they write, to natural causes, a transient malfunction, or simply by too many systems and modules trying to send frames through the CAN at the same time.  So again, exactly analogous to Ethernet.  It was the packet collision problem.  And we've talked about how, as a consequence of that, Ethernet does fabulously well up to a point.  But you can't ever get, like, saturation of the Ethernet backbone because the probability of collision then rises fast, and everybody spends all their time retransmitting collided packets.



So, and I'll just note, too, that as an engineer, cars are one of the most horrifically noisy, electrically noisy environments you will ever find.  I mean, so the system needs to be extremely - any network communication needs to be extremely robust in the face of static surges and spikes and engine noise and just all kinds of events that can happen.



They write:  "If a device sends out too many errors" - okay, that is, so something is erroneously transmitting mistaken packets and recalling them - "as the CAN standard dictates, it goes into what's called a Bus Off state, where it's cut off from the CAN bus and prevented from reading and/or writing any data onto the CAN."  So it's sort of a self-healing feature so that if something, like if a taillight processor goes wonky and starts flooding the bus with nonsense, there's a system that is able to take it off the bus.  So this feature is helpful in clearly isolating malfunctioning devices and stopping them from triggering other modules and systems on the bus because this all, of course, is being used for communication, communication among different systems in the car.



This is a feature that these guys' attack abuses.  Their attack triggers this feature by deliberately inducing sufficient errors that a targeted device or subsystem on the CAN is forced off the bus into that Bus Off state and thus rendered inert and inoperable.  They have demonstrated that this in turn can drastically affect the car's performance to the point that it becomes dangerous and even fatal, especially when essential systems like the airbag system or the antilock braking system are deactivated.  All it takes is a specially crafted attacking device introduced to the car's CAN through local access.  And I'll note that we've also seen remote CAN bus access and the reuse of frames already circulating in the CAN, rather than injecting new ones.  So there were previous attacks of this sort, but they've further refined it and made it worse.



So how did we get here?  Well, this is another classic Security Now! lesson.  The original system is old.  And it was designed just to work in the face of the assumption of mutually cooperating interlinked systems.  And as we have seen so often of things like this, it was never designed to be secure against deliberate attack because, back when it was designed, there was no way for an attacker to gain remote access to the CAN bus.  But as we know, our newfangled autos, with all their fancy new connected-to-everything-else features, create that opportunity.  We've on this podcast talked about how things like inserting a  CD into the entertainment system or cell phone WiFi or OnStar radios can be used to access the car and its bus at a distance.



So the CAN bus dates back to 1983 at a company named Robert Bosch, where it was first created.  The protocol was officially released in '86 at the Society of Automotive Engineers, the SAE conference in Detroit, Michigan.  The first CAN controller chips were introduced by Intel and Philips, and they appeared on the market a year later, in '87.  And the 1988 BMW 8 series was the first production vehicle to feature a CAN-based multiplex wiring system.  So no more crazy wiring harness.  We're just going to give everything power and communication network.  And today virtually all cars are on - they are based on this CAN bus backbone.



So the problem is we can't fix this.  Maybe in some future, I mean, maybe there's a way to revise the spec in a backward-compatible fashion so that, over time, as new CAN-attached components are created, there can be some agreement about bumping this up.  But the problem is this system we have wasn't designed to be attack proof, just like the original Internet was not designed to be attack proof.  Thus it isn't.  And it'll be interesting to see if there's a way to evolve this spec eventually so that there is a way to make it robust, since now automotive security is a thing, that is, security of its network that glues all of this together is a thing.  And it never used to be.



I'm going to jump ahead, and then we will take a break.  But I wanted to share a nice piece of email from someone whose domain I really got a kick out of.  And I thought of you, Leo, because I know you.  Jesse Harris has the domain coolestfamilyever.com.  Which I just thought, that's just so neat.  Anyway, I saw his email, actually it was sent to support, and Greg forwarded it to me.  The subject was "SpinRite saves the day again."  He said - and this was a really nice piece of mail.  He said:  "I know I can't get enough of hearing how great something I've done is and figure you're probably the same."



LEO:  Awww.



STEVE:  "I purchased SpinRite around three months ago," he wrote, "when my primary desktop started experiencing lots of strange IO errors and related lags in the OS.  I ran a Level 2 scan on my SSD" - so it was solid-state-based - "which turned up no issues.  Despite this, it's been performing absolutely normal ever since."  So there's the first instance.  Again, and we've discussed this, just letting SpinRite run over your drive.  That was a Level 2, fastest mode available, and it just fixed whatever was wrong.  No errors were surfaced because it showed the SSD controller where its problems were, and so the controller said, ooh, shoot, and, like, fixed it.  And so, yeah, those are all done down in the firmware of the controller so it doesn't represent an era that surfaces, but SpinRite helps the SSD to find and fix its own problems.



But then he wrote - so that was, what, I think it's three months ago.  Now he said:  "This morning I had the worst-case scenario of two drives failing in a four-drive RAID array."  And he wrote this is a RAID-Z1 on FreeNAS.  So that sounds like a ZFS.  He's running my favorite file system, ZFS.  So he had one drive redundancy; right?  But, bang, two drives failed.  He was in trouble despite having RAID.  He says:  "I pulled one of the drives, ran a Level 2 scan, and dropped it back into the box.  One quick command later, the pool was up in a degraded but functional and accessible state as I scanned the other drive reporting errors."  He says:  "It's almost like the drive just needed to see that I did, indeed, mean business when it decided to whip itself back into shape."



And then he said:  "For a long time I balked at the $89 price tag, and yet now I wonder if you're even charging enough for what this software is capable of doing.  Looking forward to what's in store for the future of the 6.x series once SQRL is done.  Jesse Harris."  So Jesse, thanks for letting me share that with our listeners.



LEO:  Nice.



STEVE:  So, okay.  Big important correction.



LEO:  Uh-oh.



STEVE:  For users of 1Password.  Not our mistake, ElcomSoft's mistake.  Last week we talked about the NVIDIA GTX 1080 state-of-the-art GPU accelerated brute-force testing of those four password managers:  Dashlane, LastPass, Keepass, and 1Password.  And what was notable there was that LastPass appeared to allow very few, that is, the fewest number of brute-force guesses per time interval, whatever it was, minute, second, whatever.  1Password, in the graph presented by ElcomSoft last week, was more than twice as fast, meaning twice as many guesses per second in that data.  And then Dashlane and Keepass were also faster.  And we remarked that Office 2016, of all things, was the slowest to allow brute forcing - well, except SQRL, that is, like, way on the other side of the decimal point.  But still, these are in the multiple thousands of guesses per second.



Okay.  Turns out they screwed up.  Probably as a consequence of the 1Password guy saying, wait a minute, that's not right.  It turns out that they had snapshots of old data, and they mistakenly used the old database data for the new GPU-enhanced benchmarking, which failed to take into account years of improvement in the algorithms.  As a consequence, 1Password, rather than being twice as bad, if we say "bad" is enabling more guessing per second, it is now, when properly benchmarked, the best of all by default, even better than Office, which was previously the most laggardly, which is what you want in brute forcing, the most resistant to brute forcing.



Unfortunately, the two charts I have in the show notes have a different X axis scale.  I don't know why they didn't make the chart the same because that makes it confusing.  So I made a point in the show notes of "note also the change in X-axis scale."  None of the other numbers changed from last week to this week except 1Password, which went from the erroneous 95,000 guesses, and I think it's per second, down to the proper number.  And I didn't look into why they have two, but they show two.  One is 6,200; the other is 3,000.  But when you compare that with Office at 7,300, 1Password is more difficult to brute force than even Office 2016, that was by far harder than all the others.



So props to ElcomSoft for immediately correcting the record and fixing this, and I wanted to correct it for our listeners.  1Password, in its default settings, is way slower.  And so, as I said last week, normally these things are tweakable with configuration settings.  And people who want more security ought to consider slowing down their password-based key derivation function, if their password manager lets them change things, because in doing so they would make it more difficult to brute force.



At the same time, remember that that's only one class of attack.  If something evil gets in your machine and watches you type the password in, it doesn't have to guess because you just gave it to it.  So a class of attacks it's worth being brute-force resistant for.



Just a quick heads-up for users of Foxit, who produce and publish some PDF readers.  We've spoken of them in days past.  There were a pair of recently disclosed zero-day vulnerabilities that generated some headlines, but Foxit responded initially a little awkwardly.  They said:  "Our existing containment technology means these are not a problem.  We're not going to bother with them."  That apparently didn't go over very well anywhere.  So they, "Oops, sorry, that's not what we meant.  We meant that our containment will prevent anybody from being hurt until we get this fixed, and we're working on an improvement."



So for what it's worth, anybody who's using Foxit within the sound of this podcast, keep your eye out for updates and update yourself.  You're not in danger right now because they do have a good sandboxing technology.  But the vulnerability was maliciously embedded executables carried by PDFs could, if for some reason you had disabled the on-by-default protection, could then get loose in your machine.  So again, nobody's in danger.  But if you're using Foxit, it's worth fixing it.



And I ignored this story, this next one, the first time it came because I was like, okay, we've heard this before.  But it's a big, well-known company, and they actually are doing this, and it's fun.  There was sort of a question of whether this was just apocryphal back when we heard that Sun was using lava lamps as a source of entropy, the idea being that, when you look at the wax which is heating, and therefore being reduced in density compared to the oil that it's suspended in, and thus it rises up to the top where it cools off and then comes back down, the fluid dynamics are chaotic, inherently.  You have no idea if it's going to break into three pieces, or stay as one big one, or exactly how it's going to undulate around in the oil that contains it.



And so it is, in the same way that, as I've spoken of before, a reverse bias diode junction that you put under stress by essentially breaking down its resistance to reverse electron flow, which allows electrons to tunnel in a way which is absolutely unpredictable at the quantum level, produces absolutely high-quality entropic noise.  Similarly, looking at a lava lamp is entropy.  And so our friends at Cloudflare, who now handle 10% of the Internet - and they're a sponsor; right, Leo?  Cloudflare?



LEO:  Not exactly.



STEVE:  No?



LEO:  They were a sponsor.



STEVE:  Oh, okay.  Well, anyway...



LEO:  We like them.  I love them.



STEVE:  Yeah.  And they're handling 10% of the Internet's traffic.  They actually have, and I have a picture in the show notes, a wall in the lobby of their San Francisco facility of lava lamps - different, actually alternating colors.  There are some strange ones.  I think that there's one that's just turned off.  Sort of like, I don't know, red and green, kind of Christmas-y looking, lava lamps.  That wall is being captured, the image of these banks of lava lamps is being captured at the rate of 1,000 times per second; 1 millisecond frame rate is being snapped.



That data is then hashed to produce non-algorithmically generated, utterly unpredictable, true random numbers.  Remember that random numbers are only pseudo when an algorithm produces them because, if you know the state of the algorithm, then you are able to predict all of the future.  Anything that is truly random, like a hash of the photo of this crazy lava lamp wall, that's truly random because this is going to produce, at a rate of 1,000 hashes per second, utterly unpredictable results.  So it's just very cool.  I mean, it's simple, but it really works.



And so this is why I've been contending for some time that it is no longer the case generally for randomness to be hard to have.  Early in this podcast we talked about the problems of generating entropy.  And there are instances of devices, for example, if there was, like, no communication, so that you couldn't use the timing, the high-resolution timing of the arrival of wireless or wired packets, there's an example.  Or if you have a wristwatch that has an accelerometer, it's producing entropy because you're - especially if it's on my wrist because I'm waving my hands around all the time while I'm doing the podcast.  You know, it's spewing entropy.  No one is going to be able to predict what the future is.



And of course you need to "whiten" it, that's the term for removing bias, and also make sure that things aren't stuck and that it's reliable and so forth.  So there's post-processing you have to do.  But anything that's got, like, a connection to the real world, the real world is normally producing unknown, unpredictable stuff.  And if you just suck it all in and hash it, you get high-quality entropy, which is what I did in - I discussed it,  We did a podcast called "Harvesting Entropy" [SN-456] because the first thing I wrote, the very first thing for SQRL was an entropy-harvesting technology to produce really good true random numbers, not pseudo.



A nice feature that will be of interest to our listeners, we've often talked about this question of fingerprint biometrics versus password.  I guess there's news that maybe the iPhone 8 is going to use facial recognition in order to unlock?  I've not been tracking the recent MacBreak Weekly podcast, Leo.  Is that presumed to be happening on the iPhone 8, facial recognition?



LEO:  Yeah, presumably.  I mean, you know, these rumors.



STEVE:  Yeah, yeah.



LEO:  The rumors are very strong they're going to replace, on the high-end newest iPhone, fingerprint Touch ID with a very snappy facial recognition that won't even have to aim directly at you.



STEVE:  Good.  And I hope that it...



LEO:  Or maybe it'll be eye recognition.  I mean, that's not completely clear.  Might be iris or something.



STEVE:  Yeah.  I'm hoping that, if it's facial, and I'm sure they know this, that it needs to be stereo.



LEO:  Yes.



STEVE:  They need to have cameras at...



LEO:  There's many cameras on that thing.



STEVE:  Good.



LEO:  Again, rumors.  "Rumor has it."



STEVE:  Rumor has it.  So another piece of information.  This appeared in the most recent iOS 11 beta.  And again, our listeners will like it because there's been the problem, I know that there are people who are not using Touch ID because they're worried about being compelled to put their thumb on the phone.  And that worries them enough that they would rather enter a passcode than use the Touch ID to unlock.  And as we know, the law is still completely confused, with judges coming down on both sides of both questions, of whether passwords are testimonial, whether someone can be compelled to unlock their phone through various means and so forth.



So Apple has added to iOS a new feature.  If you click the power button, a.k.a. the sleep button, since as we know the phone always actually stays on, you click it five times in succession.  That primes the phone to make a 911 emergency call and disables the Touch ID function.  And it remains disabled, no matter what you do, until - very much like a cold power-up, where the only thing it will do is you must enter your device's passcode in order to enable Touch ID and unlock the phone.



So anyway, just a nice feature.  It's not something that I'm worried about, but something to keep in mind if you're going, I don't know, through TSA.  Or I saw somewhere called it the "cop escape" or something, where the cops are coming to get you, and so you quickly click your phone's power button five times, and now your fingerprint will no longer unlock.  So anyway, it's nice that Apple's thinking of these things.



LEO:  I think they were forced into it because in order to do a full screen without a bezel they had to eliminate the fingerprint reader.  They wanted to do a fingerprint reader that would work through the glass, and apparently - this is all rumor - apparently they couldn't get it working in time.  Why they didn't want to do a fingerprint reader on the back, I don't know, because that's what Google and everybody else does.  But apparently they didn't want to.  Probably the first [crosstalk].



STEVE:  Yeah, and for what it's worth, I can really see the trouble of, like, mixing it with the screen.



LEO:  Right, that's hard.



STEVE:  Those are fundamentally incompatible.  As we know, Touch ID is a high-resolution capacitive imaging sensor.  And, boy, it does not want to operate in the presence of all of the noise of an LCD screen, like sharing the same space.  I just - I don't know how you would do that.



LEO:  Well, and if they could what they say they're going to do with the - I'm sorry.  If they do what the rumor mill says they're going to do, this will be the best possible solution because, even when the phone's just - basically, all you have to do is have your face visible to the phone at any angle, and it knows it's you.



STEVE:  Wow.  Super wide, it's a wide angle.



LEO:  Yeah.  So it could be lying on this desk, and you just sit there, and it says, oh, yeah, Leo's here.  So it almost would be a - it sounds like, the way they're implementing, like presence detection.



STEVE:  I would call that "optical tethering."



LEO:  Yeah.  And that would be really cool because you basically would not have to do anything to authenticate.



STEVE:  Wow.



LEO:  Right?  Google's working on similar things, as we've talked about.  Based on how you walk, your gait, how you talk, how you hold.



STEVE:  That's cool, too, because Jeff Arthur, who is the guy who's doing the iOS version, the iOS client for SQRL, he's got it enabled now with Touch ID.  And this would mean that it would be, by tying the authentication of, yes, this is me wanting to use SQRL to log in, that just, I mean, it becomes even more magical.  And it's already kind of freaky as it is.  You'll remember a long time ago when I showed Jeff's client your web page through Skype of the QR code, and it logged you in over Skype, I mean, optically.  So, yeah, that makes it even easier.  I mean, even more magical.



LEO:  Yeah, it's cool, yeah.



STEVE:  So the cryptographic toolkit which is the foundation of SQRL's crypto, which I chose three years ago, and which I recommend without hesitation, it's the state-of-the-art crypto, the elliptic curve crypto.  It's designed to help people who aren't experts make no mistakes:  Libsodium.  It has been independently audited by a friend of the podcast, Dr. Matthew Green of, I'm looking here, Cryptography Engineering is in the coverage of this.  The organization Private Internet Access released the results of its Libsodium audit.



They wrote that:  "Libsodium is an open source cryptographic library that is used far and wide in projects such as Zcash, as well as internal applications at Private Internet Access.  Private Internet Access is proud to have another audited tool in its software suite.  The Libsodium security assessment was conducted by Dr. Matthew Green of Cryptography Engineering.  Dr. Green previously completed the TrueCrypt audit with the Open Crypto Audit Project, as well as an OpenVPN 2.4 audit on PIA's behalf" - which we talked about before, or both of which we talked about before.  "The assessment found no critical flaws or vulnerabilities in the Libsodium library.



"Matt, who authored the Cryptography Engineering Libsodium Security Report, summarized the audit goals and results, saying:  'Over the past several months we conducted a detailed audit of the Libsodium cryptographic library at the request of Private Internet Access.  Our goal in this effort was to help ensure the safety of an increasing number of applications that rely on Libsodium for cryptographic operations. We are pleased to report that our review did not uncover any critical flaws or vulnerabilities in the core library.  Overall, we believe that Libsodium is a carefully implemented, secure cryptographic library.'"  And you can tell Matt is a cryptographer because of that language.  "Overall, we believe," that is, they've looked at it.  They understand they're not perfect, either.  But they're independent, and they looked at it trying to find problems, and they found none.  So great news for Libsodium, not only for PIA's use, but for everybody else who's using it, including me.



There was a story about data stored at Amazon in an AWS cloud bucket.  And I just wanted to note that this is - and so what happened was, sorry, the permissions on the bucket, which can be private or set to public, were accidentally made public, and the cloud-based content was available on the Internet.  This is not, obviously, Amazon's fault.  This is the fault of the user.



LEO:  It's misconfiguration; right?



STEVE:  Exactly.  It's a classic configuration error.  But what was interesting to me is that, you know, this is the danger inherent in easy-to-use power, where what we're getting are sort of like toolkits, which are astonishingly powerful for their ease of use.  But with that great power comes great responsibility.  So just sort of a cautionary note that, yes, it's wonderful to have things in the cloud.  It's wonderful to be able to have access to them anywhere.  But it's not wonderful if you don't intend for everyone to have access, and they do.  So permissions are important.  And as you said, Leo, configuration matters.



Okay.  There's this company.  I don't even know what Turtl is, T-U-R-T-L.



LEO:  I do.  I use it.



STEVE:  Oh, good.



LEO:  Turtle Hop?



STEVE:  Turtl, no, Turtlapp.com.



LEO:  Yeah.



STEVE:  I was so impressed with their disclaimer.  I've got the link in the show notes, for anyone who's interested.  But I also have the entire thing on one page of the show notes.  And, you know, completely open kimono, as I say, they say:  "When is Turtl not [in italics] secure?"  They said:  "Here are some possible scenarios where Turtl's security measures will fail you.  We try to provide an exhaustive list so you're aware of the dangers of relying on Turtl."  And I love it.  The first thing they said under "When is Turtl not secure?  When we make mistakes."



LEO:  Good.  I like it.



STEVE:  Bravo to them.



LEO:  It's a large - it's not a commercial enterprise.  It's a large open source project.  So it's easier for them to be honest than a business.



STEVE:  True, true.



LEO:  Right?



STEVE:  And so they said:  "When we make mistakes."  They said:  "That's right, we're human.  It's entirely possible that bugs in the Turtl client leave your data exposed, especially," they wrote, "at our early stage."  And then I won't take everyone through its length, but they...



LEO:  Yeah, maybe it is a business.  Maybe they do intend to make it a business.  But it's open source.  I download it and run it on my server for no cost.



STEVE:  What is it?



LEO:  It's basically - so when Evernote started charging and stuff, I looked at a bunch of different Evernote replacements.  It's a Note app that you host.



STEVE:  I like it.  I want one.



LEO:  And then they have, well, what's nice is they've written really nice apps for the mobile platforms.  So it really does give you a lot of the capabilities of Evernote at no cost to you.  But you have to be able to run your own server.  That's the...



STEVE:  I can do that.



LEO:  Yeah, I thought you might.



STEVE:  So other examples of when it's not secure is, they say, okay, "When we make mistakes," as I said, but also "when you use a bad password."  Bravo for saying that, of course.  "When you invite someone to a board over email."  And then they go into some depth of essentially they use a shared secret technology where, if that got loose, that would be a problem.  "When someone you shared data with is compromised.  When you have malware installed."  Right.  I mean, yeah, that's correct.  As we keep saying, if something infects your client, all bets are off.  "When your operating system is compromised.  When your hardware is compromised."  And, finally, "When someone is holding a gun to your head."  So, yes, they covered all the bases.  And as we know, excessive coercion is sometimes the path of least resistance, unfortunately, for security.



Two pieces of errata.  Delphi Ote said:  "Road sign classifier misstatement."  And this is from my discussion of that learning neural network training.  He caught me in a mistake.  He said:  "You said it had 91% accuracy on data used to train it."  He said:  "That would be training set, not test set."  And he's completely right.  I completely missed that and skipped over it.  The way neural nets are trained is with two separate sets that are disjoint.  They're non-overlapping.  And I said last week that it couldn't recognize the same stuff it was trained on.  Wrong.  They had a separate training set which is different from what it was tested on later.  And so its recognition accuracy was 91% on the testing set which it had never seen before.  So thank you, Delphi, for helping me correct that.



And, secondly, Emett Speer caught me in a subtler mistake.  He said:  "You misquoted the availability of S3."  And I was having fun with it, the number of nines, but there actually are a gazillion of them.  The 99.999999999, I think I got the number right, percent is the data durability.  S3's availability is 99.99.  So only four nines on availability, but a gazillion nines on - and, frankly, I'd rather, if you ask me, I can wait for my data.  I just don't want it to go away forever.  So I care more about the 10 nines of durability than I do - well, for my particular application.  I'm not saying everybody.  But for me, I'm just using it as a backup archive.  So if I have to wait a minute, fine, if it blinks off briefly.  But it never has.



Okay.  And the Tweet of the Week, Leo, you'll get a kick out of this.  And our listeners, it's a perfect tech nerd Tweet of the Week.  Andy, who tweets from @remixman, tweeted:  "LOL, @SGgrc."  He's retweeting something from a Jim Birch, who said:  "The sun is currently undergoing a denial of service attack.  We hope to have full service restored soon."  And of course that was from Eclipse Day yesterday.



And I was speaking in the last couple weeks, actually two weeks ago, about the availability of the third book of the third trilogy of Richard Phillips' Rho Agenda work through the end of August.  So here we are on the 22nd.  So is August a 31-day month, Leo?  I think it is.



LEO:  Yes, it is.  Don't you know the old doggerel, "30 days hath September?"



STEVE:  Yeah, I count the knuckles.



LEO:  I have to go through it each freaking time.  Oh, you use the knuckle method.  Ah.



STEVE:  Actually, I can sort of remember.  I remember August 31st happening before, so it's probably going to happen again.



LEO:  Yeah, yeah.



STEVE:  So anyway, through the end of August, all previous eight of the nine books, which I love and which many of our listeners have loved, back when I was discovering them and telling people about them, I was getting a lot of positive feedback.  These are the teenagers that discover the second ship and get changed.  And, oh, it's fun.  Anyway, sorry, 99 cents each.  So I created a bit.ly link for this, for this show:  bit.ly/rhoagenda, R-H-O-A-G-E-N-D-A.  I've also got the full link that that expands to.  And what that link takes you to is an Amazon page where you can buy, for less than a dollar, well, a penny less, 99 cents each, all of the earlier eight books.  And they are all fun.  I mean, it's terrific.  So if you thought maybe that sounds like a good idea, but you were waiting, through the end of the month - which we now know is August 31st - you can purchase them for 99 cents each.



Jan asked:  "@SGgrc Could you talk about how to encrypt your storage on Amazon S3 for next week's episode of Security Now!?  Or has it been done already?"  And I'll just say again that I am a fan of CloudBerry Lab.  I looked at their crypto years ago, talked about it here.  I've looked at it recently.  And it's the tool that I've chosen for GRC and for my own use.  As I mentioned before, Sue is still using our original Jungle Disk account to back up her machine.  CloudBerry Lab is a full-featured suite.  So that's what I use.  And, boy, as I mentioned before, they support every cloud provider you ever heard of and about three times that many that you've never heard of.  So really good coverage.



And then, finally, on the question of the teapot, Leo, the HTTP 418 code.



LEO:  Yes.



STEVE:  Remember that the 400 series, 4XX, are errors.



LEO:  Right.



STEVE:  So the famous one is 404, and it's that Page Not Found.  418 was the I'm a Teapot error.  Well, a lot of listeners knew more detail.  I did, too, but I didn't want to go into it last week because as it is we never even got to any of our closing-the-loop feedback because we ran out of time.  But there's a site called Sites Done Right that blogged back in 2013 in March:  "What is 418 I'm a Teapot status code error?"  And they said:  "If you look through" - and there's some fun history here that I just wanted to share, and then that's the end of our podcast.



"If you look through the full list of HTTP status codes, you'll see one that really stands out:  '418 I'm a Teapot.'  So what's that all about?  I'll explain in a bit," this writer says.  "But first, a really quick introduction to status codes.  Whenever a page or file is accessed on your site, whether it's a user accessing it in a browser or a search engine crawling a page, your server" - so he's taking the server end view.  We look at the client end view, typically, where it's our browser fetching from a remote server receives a status code telling it how that HTTP fetch went.



So he writes:  "Your server returns an HTTP status code in response to the request, and it provides information about the status.  For example, some popular codes are '301 Page Moved Permanently' and," as he notes, "the ever-ubiquitous '404 Page Not Found.'



"So what's the '418 I'm a Teapot' all about?  Well, the group of people who make these codes and set standards is the IETF, the Internet Engineering Task Force.  To propose new standards, the members release RFCs, or Requests for Comments, to the community.  Every year, since 1989, they release a few humorous RFCs for April Fools Day; and, on April 1st, 1998, RFC 2324 introduced the 'Hyper Text Coffee Pot Control Protocol,' HTCPCP v1.0."



LEO:  I love it.



STEVE:  Yes, the Hyper Text Coffee Pot Control Protocol.



LEO:  And then that goes back to that Cambridge Coffee Pot.  I guarantee you; right?



STEVE:  And it was fully developed, Leo.



LEO:  Really.



STEVE:  Because these guys aren't messing around.



LEO:  These are real engineers here, yeah.



STEVE:  Oh, yeah.  So, he writes:  "This was a brand new protocol for controlling, monitoring, and diagnosing coffee pots.  Now, the RFC is pretty funny with lines like 'Coffee pots heat water using electronic mechanisms, so there's no fire.  Thus, no firewalls are necessary."



LEO:  Oh, how funny.



STEVE:  "If you have never read it, it's definitely worth a read," he says. "I added a link in the description below.  Now, you may be asking, if this is a Coffee protocol, why the Teapot code?"



LEO:  Yes.



STEVE:  "This is answered in Section 2.3.2 in that, quote, 'Any attempt to brew coffee with a teapot should result in the HTTP error code 418 I'm a Teapot, and the resulting entity body may be short and stout.'"



LEO:  Oh, dear.  Oh, dear.  Oh, dear.



STEVE:  It's pretty bad, Leo.  I mean, it'll hurt you.  "And just like that, the '418 I'm a Teapot' code was born."



LEO:  Was born, yes.



STEVE:  "Since then, it's been used in all sorts of wacky ways. Google even referred to 418 as a different error in their 2013 April Fools Joke 'Google Nose,' saying that '418 Scent Transfer Protocol Error indicates system congestion.  Please try again later.'"  So, yes.  A little fun from the crazy RFC authors.



LEO:  Really funny.  "Halt and Catch Fire" is back.  I forgot.  You like it or don't like it?



STEVE:  I fell off of that wagon.  I got a little bit into Season 3.  It's like, okay, Cameron's hot, but it's like not worth it.



LEO:  That's why I stopped watching from the very first episode, because they went there right away, and I thought, this isn't going to be good.  Although I just noted Woz tweeted or Facebooked or something that it's his favorite show.



STEVE:  Wow.  Well, I also abandoned "Twin Peaks" finally.  It's like, oh, this is just...



LEO:  It makes no sense, yeah.



STEVE:  Oh, it's a fever dream.  I mean, it is just...



LEO:  Yeah, that's a good way to describe it, yeah.



STEVE:  It's just like, okay, Lynch, what have you been smoking, and change brands because, boy.  Oh.



LEO:  Yeah.  On the other hand, "Game of Thrones" is getting better and better; isn't it.



STEVE:  Leo, it's, oh, yeah.  In fact, you know what's sad?  And I was glad to see this.  I encountered this somewhere in a link as I was researching the show.  Some site was talking about how it's unfortunate that now they've sped it up so much.



LEO:  Right.



STEVE:  It's like they're now in a hurry.  I mean, it was...



LEO:  Right, they have to...



STEVE:  ...painfully slow.



LEO:  They have to bring a million threads together; right?



STEVE:  Oh.  And the dialogue as those guys are walking through the ice was just...



LEO:  Isn't that funny?



STEVE:  It was just delightful.  But, boy.  And now, you know, we're already through with our little half season.  Next week is the end of this chunk.  Do you know when the next one is?  Do we have to wait till...



LEO:  I think we have to wait.  I don't think they begin shooting, or they've just begun shooting, so it's going to be another, yeah, year or something.  I thought they were going to do half and half, like they'd shot the whole season and spread it out.



STEVE:  Spread it out, yeah. 



LEO:  But I don't think that's the case.  I don't know.  Some fan will tell us.  But the thing that's interesting to me is this is the payoff for your devotion.  I mean, they always said it's going to be a 73-hour movie.



STEVE:  Yeah.



LEO:  And you've watched 60 hours.



STEVE:  My nephew, my sister's son and his wife are - I don't know how I happened to run across it.  We keep in text touch, or touch text, I don't know, something.  Anyway, so I just happened to mention it because, I mean, they've been so good, I've been trying to tell people who I think would be interested.  And he said they never saw it.  They're now on, like, Season 3 just a couple weeks ago.  So maybe they're up to 4 or 5 now.  And I just said to Evan, I said, it's worth it.  I mean, binging is probably so much better than the way we've had to do it over the course of seven years.



LEO:  Probably, yeah.



STEVE:  Which is like, oh, who was that again?  Who?  What?  What?  What?  Because, I mean, you know, you needed a guide just because it was so stretched out.



LEO:  What other TV show in the world would bring back a character you haven't seen in five years and expect you to know him?



STEVE:  Yes.



LEO:  Right?



STEVE:  Yes.



LEO:  Only GoT, baby.  Only GoT.  Yeah.  I don't know if I'll have the energy, stamina.  I'm going to have to wait until I'm in the nursing home and don't have anything else to do.  But I would love to rewatch it.  You know, Lisa's only seen, like, a bit of it.  So I keep saying, you know, if you want to start at the beginning and go through the whole thing, I'm willing to try it.  But 73 hours.



STEVE:  I know.  I'm looking for someone who hasn't seen "Breaking Bad" yet because that would be fun to share.



LEO:  Same here.  And I've been thinking of rewatching that whole thing.



STEVE:  Yeah.



LEO:  But again, you're right, it'd be more fun if somebody else - because that was probably the best TV series ever was "Breaking Bad."



STEVE:  I think it was, Leo.  I mean, I really do.  It's top of the list.



LEO:  ScooterX says the "Game of Thrones" next season won't start production until October.  So it's, yeah, it'll be next fall.



STEVE:  I guess they do have to wait for winter somewhere, don't they.



LEO:  Winter is coming.  I think winter is here.  Steve Gibson, he's at GRC.com.  That's where you can go to get, oh, so many great things.  Everything's free there except one.  And that's SpinRite, the world's best hard drive maintenance and recovery utility.  That's his bread and butter.  So do him a favor.  Do us a favor.  Do yourself and your hard drives a favor.  Get a copy.  GRC.com.  You can do all the other stuff free - ShieldsUP! to test your router and your firewall.  You can Shoot the Messenger and DCOMbobulator and Trouble in Paradise.  I'm going way back now.  You can also, of course, find out more about SQRL.  Somebody in the chatroom is saying it's not clear from the website that it's a server-side technology.  I mean, it's both.  It's client and server.



STEVE:  Leo, at this point nothing is clear from the website.  It is a mess.  And so the plan is the installer technology is nailed.  We're now happily tracking down little remaining things like should this text be red or green, and the focus doesn't work on 64-bit Win10 if you don't use IE and Edge.  You have to click the dialogue.  I think I fixed that, but I haven't looked yet because I've been working on the podcast since I posted what I hope is a fix for that.  So, and I have a bunch of other - I have a whole to-do list of little debris to clean up.  But, I mean, it's, like, it's very close.



And what I'll do is, once it really seems done, I've built a FreeBSD server for the forums because I'm going to be running public forums to host forums for all the clients and developers, so there's a public place for this interchange to occur.  So I'll get the client finished.  And the problem is unfortunately I built it using an older x86 server, not realizing that ZFS and enough RAM needed an x86 platform.  I've got one just sitting there, but I built it on the wrong platform.  So I'm going to rebuild it, move everything I already did.  The forums exist, believe it or not.  They're there.  I've just kept them offline until I'm ready.



So I'll stage the forums, we'll get that tested, and then we'll release the client publicly.  The demo sites, there's multiple demo sites.  There's iOS and Android clients and my Windows client.  There will be then a place for people to ask questions and a community to solve problems and so forth.  And while that's happening, then I go back and catch the website up that I haven't looked at since the beginning, so that it then represents a current statement of the spec.



The core pages I've been keeping current.  But all of the intro background stuff I just thought, okay.  For a while I kept rewriting it over and over.  And I thought, okay, just stop.  Let's get the technology finished, the clients running, everything going.  And then I'll explain it all and describe it.  So that's my planned sequence.  And of course the second that's done it's back to SpinRite 6.1 and much more compatibility and a lot more speed, which everyone will enjoy.  And a free upgrade for all users of 6.0.



LEO:  Yay.  So get over there, GRC.com.  He also has the audio of the show and really nicely written transcripts, so you can read along.  A lot of people like to do that.  It makes it easier to understand for many of us.  For some of us, we'll never understand.  But you can also get audio and video on our site, TWiT.tv/sn.



We stream live, if you want to watch live and be in the chatroom.  The chatroom is irc.twit.tv.  The live stream is TWiT.tv/live, and we do that every Wednesday, right after MacBreak Weekly.  Or, I'm sorry, Tuesday, right after MacBreak Weekly, roughly 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to come by and say hi.  Otherwise, on-demand audio and video at Steve's site, my site, and, of course, in every podcatcher known to man.  So no matter what you use, subscribe.  That way, you know, you'd like a complete set.  You want it all, Security Now!.  Thank you, Steve.  We'll see you next week.



STEVE:  Okay, my friend.  Till then.  Oh, and by the way, this is the first episode of Year 13.



LEO:  Year 13.  That's what I told the guy in the chatroom.  You really needed to listen to all 13 years to understand what SQRL is.



STEVE:  And you'd better start now because, you know...



LEO:  Seventy-three hours?



STEVE:  We're going to keep them coming.



LEO:  That ain't nothing.



STEVE:  Make "Game of Thrones" look like a walk in the park.



LEO:  I would guess the average length is probably an hour, although almost all the shows in the last few years have been two hours long.  But we started really short.  So you've got at least...



STEVE:  How could we do this in less than two hours?  It's not possible.



LEO:  There's probably a thousand hours' worth of show.  Somebody knows, and they will tell us.  Thank you, Steve.



STEVE:  Your caffeine-powered podcast.



LEO:  We'll see you next time.



STEVE:  Bye, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#626

DATE:		August 29, 2017

TITLE:		Shattering Trust

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-626.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we cover a bit of the ongoing drama surrounding Marcus Hutchins; examine a reported instance of interagency hacking; follow the evolving market for zero-day exploits; examine trouble arising from the continued use of a deprecated Apple security API; discover that Intel's controversial platform management engine can, after all, be disabled; look into another SMS attack; bring note to a nice-looking TOTP authenticator; recommend an alternative to the shutting-down CrashPlan; deal with a bit of errata and miscellany; then look into an interesting bit of research which invokes "The Wrath of Khan." 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here. We've got a lot of security news; a surprising update on the Marcus Hutchins legal defense fund.  Steve's going to explain some of the latest news about the Intel management engine.  It turns out, thanks to the NSA, we now know how to disable it.  And he'll explain about how "Star Trek 2:  The Wrath of Khan" applies to your security policy.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 626, recorded Tuesday, August 29th, 2017:  Shattering Trust.



It's time for Security Now!, the show where we cover your security and privacy online with this, well, normally it would be with Steve Gibson, but I don't - somebody else is sitting in for Steve.  No, it is Steve.  And for those of you listening, he'll sound just like Steve.  But there is something...



STEVE GIBSON:  Yes, it has not changed my voice at all.



LEO:  There is something a little different.  We're going to call you Babyface from now on.



STEVE:  That actually has been the consistent response from various women who have seen me since is, "Wow, you look younger."  It's like, well...



LEO:  Steve shaved his - how long have you had that moustache?



STEVE:  Forever.  It was briefly removed while I was married.  Juliette wanted to see what was under there.



LEO:  And then she said, "Put it back, put it back.  I don't know who this is."



STEVE:  She just wanted to change everything to sort of make it hers.  Anyway, so it came back.  But I don't know.  So I've been thinking about it for a few months.  And finally I said, well, if I don't like it, I can always let it grow back.  But, yeah, I think the reaction has generally been favorable, so what the hell.  Time for a change.  Now the problem, it's weird, too, because I didn't realize that in this new world we live in my picture is everywhere.



LEO:  Well, we have to redo the mugs.



STEVE:  Twitter feed.  Right.  My Twitter feed, and you and me as Jean-Luc, as Picard and Number One, I can't think of his name.



LEO: The guy with the beard, Frakes, Jonathan Frakes. 



STEVE:  Yeah, exactly.



LEO:  What we're going to have to do here - actually, I think it makes the mugs more valuable.  This is the...



STEVE:  Ah, the Collector Edition, yes.



LEO:  ...Collectors' Edition with Steve with his moustache.



STEVE:  Yes, for 12 years of the podcast, never to be revisited.  Get them before they sell out because, you know, who's going to make more of those now?



LEO:  We will.  TWiT.tv/store.  They have Security Now! mugs with - and the best thing about this, and I think this was Nitrozac who did your original illustration, a little editorializing on her part, you look quite confident and cocky in these.  And I love them.



STEVE:  I look snobby.  I've never liked that picture.  I look aloof and, like, you know, you know what doesn't stink.  It's like, no.



LEO:  No, you look like I'm secure, and I'm proud of it, or something.  I don't know.  I like it.  To me, I know it's not your personality, but I do like those.  And we have T-shirts and mugs, and they're available at TWiT.tv/store.  When you see, like, last day to order and all that, that's just the way that our supplier Teespring works.  We will always make more.  It's just a little weird thing about - we've sold 74 of those mugs, by the way, Steve.  I think that's a store record.



STEVE:  People are tweeting me themselves sipping coffee from them.  So thank you.  I mean...



LEO:  And we're not - I don't think we're making money.  If we are, it's just a couple of bucks on these.  They're $15 for the mugs, and I think $18 for the T-shirt.



STEVE:  Yeah, that's below Starbucks' going price for their...



LEO:  Yeah, yeah.  Less than Starbucks.  And we have a variety.  I think we even have, yeah, we have a hoodie.  Now, that, if you're going to DEF CON or Black Hat, you should wear a Security Now! hoodie, for sure.  That's $38.99.



STEVE:  That'll keep you warm.



LEO:  So what's coming up today on the show?



STEVE:  Okay.  So today's podcast, No. 626, I titled "Shattering Trust," after the title of the coverage we're going to give at the end of the podcast of a research paper titled "Shattering Trust," which is really interesting.  But of course, as always, we've got a bunch of stuff to talk about.  We're going to cover a little bit of the ongoing drama surrounding Marcus Hutchins.  Another little wrinkle has surfaced.



LEO:  Yeah.



STEVE:  Examine a reported instance of interagency hacking, which I wouldn't have expected.  But after you hear about it, it's like, well, sad, but probable.  We're also following the evolving market for zero-day exploits.  Examine trouble arising from the continued use of a deprecated, actually a long-deprecated, four years now, Apple security API.  Discover that Intel's controversial platform management that we've talked about here often can, after all, be disabled.  Look into another instance of SMS attack, you know, multifactor attack.  Bring note to a nice-looking time-based one-time password authenticator.  Recommend an alternative to the shutting down CrashPlan, which a lot of our listeners apparently subscribed to and said, oh, my god, where do I go?



LEO:  Carbonite, of course, our sponsor, yeah.



STEVE:  Carbonite even recommended, I mean, they recommended Carbonite as...



LEO:  CrashPlan did, yeah, yeah.



STEVE:  ...one alternative.  We're going to deal with a bit of errata.  And this one really caught me off guard.  And when I saw the first mention of it, I thought, oh, I probably misspoke once.  And then it turns out, no, apparently I just used the wrong word through my entire litany.



LEO:  And I didn't catch it?  I apologize.



STEVE:  Well, I didn't catch it.  And so I just - I used it, I must have used it initially wrong, and then I just kept on using it wrong.  So we're going to fix that.  And then we're going to take a look at this interesting bit of research which, of all things, invokes "The Wrath of Khan."



LEO:  Whoa.



STEVE:  So I think a good podcast for us.



LEO:  "The Wrath of Khan."  And by the way, I've just been handed this note.  Alex Gumpel says we're working, our R&D division is working on new Steve Gibson Security Now! T-shirts with removable Velcro moustaches.



STEVE:  Ah, perfect.



LEO:  So you could have it on or off.  So we're going to get that out of R&D, and we'll let you know.



STEVE:  Or if you have some of that Wite Out, since the mugs are white, you can just cover right over there.



LEO:  Moustacheless Steve Gibson.  It's going to take me a while to get used to this, I'll tell you.



STEVE:  Oh, I don't spend much time looking at myself.



LEO:  Right, you don't know.



STEVE:  This happened around middle of the day on Saturday.



LEO:  What was it like shaving it?



STEVE:  It was weird.  And I did it again this morning for the podcast.  And I have to be careful because - although it's better.



LEO:  Your lip is tender there, yeah.



STEVE:  You have to kind of get in underneath and get the upper lip and push that...



LEO:  Oh, it's hard, I know.



STEVE:  Yeah, so anyway, yeah.



LEO:  But your coffee tastes different, I bet.



STEVE:  Okay.  So our Picture of the Week reminds us of that classic dumb joke from the old days when we were in elementary school.  Is your fridge running?  Which I guess the idea used to be...



LEO:  You'd call somebody.  You'd call somebody.



STEVE:  You'd call someone, exactly.



LEO:  You'd say, "Do you have Prince Albert in a can?"  And they'd say, "Yes, we do."  You'd call a tobacco shop.  See, that's how old-fashioned this is.  "Yes, we do."  "Well, let him out."



STEVE:  Yes, let him out.



LEO:  Or you'd call a housewife, "Is your refrigerator running?"  "Yes."  "Well, you'd better go get it.  It's getting away."  Ha ha ha.



STEVE:  Right.  So updated to this world of 2017 is our Picture of the Week that poses the question:  "Is your fridge running?"  And then when they say, you know, whatever, it's "Because I can't log into it anymore."  So, yeah.  Our brave new world.



LEO:  Do you watch "Silicon Valley" on HBO?



STEVE:  Oh, are you kidding me?  Yes.



LEO:  That was one of the big plot points this season was hacking into - I can't remember their names.  The kid who was in the incubator bought a very, very, very fancy computerized refrigerator.



STEVE:  Oh, right, right, right.



LEO:  Remember?  And then...



STEVE:  Yeah, the younger Asian guy who...



LEO:  [Jian Yang] or whatever his - and then of course it drove Gilfoyle crazy, and so he spent many hours and ended up using all the CPU cycles in their server to crack this refrigerator, and then it just said nasty things.  It was very - I love that show.  Anyway, I'm sorry.  Go ahead.



STEVE:  Well, and I think what's fun is for techie geeks like us, I mean, it does tie into all of the experiences that we're having and sharing.  So, I mean, yes, it's very relevant.



So, okay.  In our Marcus Hutchins news is a sad story with I think an interesting takeaway.  Kevin Collier, who is BuzzFeed's News Cybersecurity Correspondent, reported some unfortunate news surrounding the high-profile arrest that we've been covering for the last couple weeks, post-DEF CON, of the so-called WannaCry hero, Marcus Hutchins.  It turns out that the overwhelming majority of money which had been raised to pay for his legal defense turned out to have been donated with stolen or fake credit card numbers.



LEO:  Oh, man.



STEVE:  I know.  And as a consequence, all donations, including the legitimate ones, because there was no way to tell them apart, are being returned, according to Tor Ekeland, who's the attorney who was managing the fund.  He reported, I think it was last week, that at least $150,000 of the money collected came from fraudulent sources, and that the prevalence of the fraudulent donations effectively voided the entire fundraiser.  He said he'd been able to identify only about $4,900 in legitimate donations.



LEO:  Now, who would do that?



STEVE:  I know.  But he couldn't be certain even of those.  He told reporters:  "I don't want to take the risk, so I just refunded everything.  One person had five different charges to his one credit card," Ekeland told BuzzFeed News.  "Odd number values, the kind you'll glance over when looking at your bill," designed obviously to hide them and obscure them.  And so clearly what happened is people began getting their credit card statements and saying, wait a minute, what's this charge?  I mean, it happens to us all the time.  It's mostly all Sue does for me is someone will call up and say, "I didn't ask for any research."  And Sue says, "Did you buy SpinRite?"  "Well, of course."  "Well, that's where you got it."  "Oh."  And then they are happy, and they go away.



LEO:  Oh, I get it, Gibson Research.  I get it, I get it.  "I didn't buy research."



STEVE:  Yeah, nobody thinks of us as the SpinRite people.  So basically Sue sits around explaining to people what that charge is on their credit card statement, and then they're happy.



LEO:  Yeah, yeah, that's funny.



STEVE:  Anyway, in this case, Ekeland, who was in charge of this, started getting all of these calls from people saying, who are you, and why did you charge me X amount of money?



LEO:  Right.



STEVE:  And he said, "Did you donate to the Marcus Hutchins legal defense fund?"  And they said, "Who's Marcus Hutchins?  No."  And so, anyway, big problem.  He said he's still determining what to do with a bitcoin wallet which was set up to take donations for Hutchins, which has received 96 small donations worth a total of about $3,400.



So the problem, of course, is that there's no doubt some legitimate, well-meaning, good-hearted people among the noise; and the noise, however, swamped it all and ended up, I mean, I don't know if the bad guys intended this to happen in order to kill the fund or thought they were doing Marcus a favor.  But of course, if that was the case, it ended up completely backfiring.  And as we know, his arrest shocked the community of cybersecurity researchers, who have since largely rallied behind him and tried to raise money for experienced attorneys.  At the time, Ekeland stepped in to host the fundraiser, after GoFundMe refused to host it, citing concerns with its terms of service.  I don't know what that was about, but okay.



So of course, as we know, Hutchins, who pleaded not guilty to all six charges against him on August 14th, has retained Brian Klein, an L.A., that is, Los Angeles-based trial lawyer, and Marcia Hoffman, an acclaimed expert on U.S. hacking laws, both as his attorneys.  And as we discussed, I think it was last week, a judge told him he could not return to the U.K. before his trial, yet he had free roaming through the U.S.  I'm sure they pulled his passport, and they also GPSed him.  And his trial is set for a little more than a month from now, in October.  And in the meantime he's hanging out in Venice Beach and enjoying the weather and waiting for trial.



A woman named Tarah Wheeler, who is a friend of his, previously had told BuzzFeed News that she planned to set up a new legal defense fund for him, but did not immediately respond to their follow-up request for comments on this story.  Ekeland said he believes he has refunded every donation to the fund, but that anyone who hasn't heard from him can email info@torekeland, T-O-R-E-K-E-L-A-N-D, dotcom for their refund.



And of course, right on cue and apropos of this, we have the just-posted note from Brian Krebs, who posted a piece with the headline, "Beware of Hurricane Harvey Relief Scams."



LEO:  Oh, no.



STEVE:  I know.



LEO:  What's wrong with people?



STEVE:  Brian writes:  "U.S. federal agencies are warning citizens anxious to donate money for those victimized by Hurricane Harvey to be especially wary of scam artists.  In years past we've seen," writes Brian, "shameless fraudsters stand up fake charities and other bogus relief efforts in a bid to capitalize on public concern over an ongoing disaster."



And our listeners may recall that, when I mentioned this Hutchins legal defense fund, I was inherently cautious about it.



LEO:  Yes.



STEVE:  I looked at the site.  I scrutinized it as best I could for, like, any signs of legitimacy and illegitimacy.  And I even said, you know, I can't vouch for this.  But I did look at it, and it appears legitimate.  Now, I don't know what that even means because I guess it would be obvious if it didn't appear legitimate, if it was broken English or like any of the cues we're used to seeing in poorly produced spoofs.  But as they say, there's no way to, what is it, disprove a negative?  I don't know what that is, but...



LEO:  Yes, that's exactly right.  You can't prove a negative.



STEVE:  Yeah.  So it's like, okay.  It's there, and it seems like the right one.  And it was, as it turns out, but that didn't save it.  So it occurs to me that we're seeing something of an emerging trend with this, which is high-profile online things of any kind that attract outsized attention of any sort are not left alone and allowed to function as intended.  By being high-profile, they become targets of mischief.  And if those sites are not adequately prepared to actively fend off mischievous or malicious attacks, disaster results.



And of course we had another example when the FCC put up the Net Neutrality feedback site.  It was so swamped with fraudulent postings and other nonsense as to become virtually useless.  They had to pull it down, take it offline.  Then they tried to put it back up, but of course it had been ruined by a ton of insincere crap that had also been put up.  So, and now here we have an entirely well-meaning online fundraiser similarly rendered useless by fraudsters who stole money from others' credit cards to illegally contribute to Marcus's defense fund, of course ultimately not helping him at all, and as a result collapsed the entire thing.



So I don't know how you could protect against this.  Maybe question, for one thing, question multiple donations from the same credit card.  That you could certainly check.  But they weren't doing that.  Because if nothing else it would have raised an alarm immediately to put this thing on hold when a duplicate charge from the same person - and then, you know, follow up, verify with the person and so forth.  But that wasn't there.  So, for example, perhaps in both cases this was sloppy and immature website design that put up a minimally functional web surface that wasn't sufficiently proactive against malicious abuse.  We've seen uncovered many examples of such short-sighted design in the past on this podcast.



And this is sort of the logical extension of the how do websites store our passwords question.  As we know, there are ridiculously wrong ways and very strong ways to solve exactly the same problem.  And the approach taken doesn't really matter if everything always goes well, and the strategy never is put under stress to test its strength.  Because when weakly designed and/or implemented systems are put under stress, then they will collapse and fail.  So I think that helps us to sort of put this into context, that is, there are lots of sites with bad security; but because no one really cares about them, it's like, well, okay, it works.  And it's not until, though, something happens that draws a lot of attention, and then, boy, disaster strikes quickly.



And we're operating, when you think about it, in a completely unregulated environment where there are no controls.  Anyone can do anything they wish, set up any kind of website they choose using whatever technology occurs to them first.  I had a more colorful metaphor there initially.  I thought, no, that's inappropriate.  I doubt that imposing regulations upon the industry will help.  But I think we do need to make use of more well-thought-out solutions, easier to deploy, things like we were talking about last week, the libsodium cryptographic library, which was recently audited and which had bindings to virtually every language.  So there's no excuse for not deploying strong crypto.  And it brings useful drop-in encryption to a much larger audience.



The next step, then, would be to incorporate similar libraries into drop-in problem-solving packages, that is, provide all the rest of the plumbing so that much of the underlying work can become standardized around solid solutions.  And in such a future, accepting and storing passwords don't need to be written ad hoc because the right way to do it would be built into every platform that a developer might choose to use.  So essentially those things become commodities, as they absolutely should be, so that it's just done for a developer, rather than it being up to them to just do something which is as likely to be incorrect as not because not everybody is experienced in secure design concepts for security.  Our listeners are now, but we're still a small subset.  And yes, Leo, I just, you know, so sad.  As you said, what's wrong with people?  It's like, well...



LEO:  I feel like there's a break, there's kind of a breakdown in - maybe this is what old people feel like - in the moral order somehow.



STEVE:  I'm right there with you, yup.



LEO:  And people don't care anymore, or they just - it's like - I feel like, you know what it is, it's almost anarchy, kind of, a strain of anarchist, people who will do it for the lulz, who do it because they don't - they just - they like to see the world burn.  And that's what I suspect is going on.  It's just like, yeah, wouldn't it be funny if we did this?  They don't care.



STEVE:  Well, and, you know, there's also this aspect of getting away with it and anonymity.  That is, a lot of the mischief that we see is done by people hiding their identity.  And this of course is a classic mixed blessing of the Internet.  There are so many valid, useful, constructive purposes for allowing people to be anonymous, to protect them, for example, from oppressive governments and the like.



But that anonymity gets abused, as often as not.  And the Internet provides a uniquely powerful anonymizing capability.  We know it's not perfect.  We've talked about the fact that it wasn't designed to provide that.  But there's a big difference between throwing a physical rock through someone's window in the real world, where you can be bodily grabbed by a well-meaning neighbor or by a passing police officer, versus hiding in the basement in some remote country and having a great deal of power to do harm.



LEO:  I guess that's the question.  Is this just - there have always been people like this, and just now they have the means?



STEVE:  This is definitely an enabling technology.



LEO:  That's for sure true.  But I also feel like there are more people like that than there were before.  I think we've raised a generation of kind of asocial, as a result amoral people who are good with these technologies and happy to use them because they don't have any empathy at all.  They just lack that gene, or they've never been trained to because they spent their whole time playing Call of Duty in Mom's basement.  I don't know.  Maybe that's just me.  I sound old.  Forget it.  I'm just, you know, old person.  Get off my Internet, you kids.



STEVE:  Yes, well...



LEO:  Who knows?  No one knows.  It's one of those questions for the era, the ages.



STEVE:  I love the position we're in, able to watch this happen, to cover the evolving news.  To, like, we've all been here.



LEO:  This is what we do, yeah.



STEVE:  Yes.  We've all been here, for example, pre- and post-Snowden.  And so we're riding through, I think, a fascinating period where we're going from the concept of a global autonomous network, and it's becoming real.  It's becoming vital.  It mean, in some cases having access to the Internet is considered, you know, not a privilege, but a right, and a necessity.  It's like, I mean, it's that big a deal.  I'll never forget when I first proposed GRC taking credit cards to my little group.  They looked at me like I was crazy.  And I said, "What?"  And they said, "Well, no one buys anything on the Internet."  And I said, "Well, I do."  And they said, "Well, you're weird."



LEO:  Isn't that funny?  Boy, that's changed.



STEVE:  Yeah.  I used to get Christmas presents from Amazon because I was like...



LEO:  Everybody buys everything on the Internet now.



STEVE:  ...one of their launch customers.  And, yes, and so look at just in this period of time how much has changed.  I mean, you look around restaurants, when everyone's just holding their phones up and tapping into them.  It's amazing.



LEO:  What a world.  What a world.



STEVE:  So I think this is a perfect time for us to be watching this.  And I love the podcast because it is chronicling, week by week...



LEO:  Absolutely.



STEVE:  ...the nature of these changes.



LEO:  That's why I've always loved doing this; right?  We've got ringside seats at the revolution.  Yeah.



STEVE:  Yeah.  Okay.  So get a load of this one, and this is one I did not see coming.  The question this poses is why did the CIA, assuming that we believe another Vault 7 doc dump from WikiLeaks, why did the CIA create a bogus software upgrade?  The answer, apparently, to steal data from the FBI and the NSA.



LEO:  Oh, god.  Oh, I hope this is not true.  Do you think this is true?



STEVE:  Okay, so here's what we think.  So we sort of assume that we're all on the same side.



LEO:  Yeah.  We know this rivalry, but it's like rivalry of the Marines versus the Army or something.  Collegial rivalry, yeah.



STEVE:  Precisely.  That's what we want.  And that, for example, there's automatic interagency cooperation and data sharing, despite the fact that they're different organizations.  But we've also observed the tendency of agencies to protect their own interests, sometimes at the expense of the larger picture and the greater good, doubtless justifying their actions by telling themselves that this is what they need to do.



So if we are to believe the recently leaked, so-called "ExpressLane documents" as part of WikiLeaks' most recent and ongoing Vault 7 document dump, it appears that the CIA embedded its own trojan into an interagency biometrics collection system installed at so-called "liaison services" and intelligence gathering partners, including the NSA, the Department of Homeland Security, and the FBI.  Again, if these documents are to be believed, the CIA didn't trust its security service partners to fully share biometric information with it, so it created a bogus software upgrade to steal the data.



This data-stealing trojan was created as part of a CIA project called ExpressLane, a piece of software installed by the CIA Office of Technical Services, the OTS, under the guise of upgrading the previously installed, CIA-provided, biometric collection system.  The CIA installed the biometric system at partner offices around the world and expected them to voluntarily share biometric data with the CIA.  But in case they didn't, it also installed ExpressLane to, quote, "verify that this data is also being shared with the Agency," Agency with a capital "A."  It also had a feature to cut off the liaison service's access to the system if it didn't provide the CIA with access.



The documents note that:  "The systems are provided to liaison with the expectation for sharing of the biometric takes collected on the systems.  Some of these biometric systems have already been given to the liaison services.  The OTS plans to revisit these sites with the cover of upgrading the biometric software to perform a collection against the biometrics acquisitions."  So that OTS agents could install the trojan in the presence of partner agents, ExpressLane included a splash screen with a progress bar made to look like an authentic Windows install.  OTS agents would install the software with a USB stick and could set the installation time of the update as well as a kill date before visiting the target.



Once installed, the trojan collects relevant files and stores them in a secret partition on a specially watermarked thumb drive that the OTS agent inserts during a subsequent maintenance cycle.  The biometrics system itself was provided by U.S. identity management firm CrossMatch.  It specifically didn't want the update to reference CrossMatch's software.



So I suppose that our lesson here is that spooks will be spooks.  And while we are all still on the same side, the fact that we have agency divisions inherently creates some degree of interagency rivalry and tension.  And so something like...



LEO:  On the other hand, I don't fully trust Julian Assange and stuff from WikiLeaks.



STEVE:  I know.  And this is the...



LEO:  We know that Julian is also very interested in sowing discord among the agencies.  And what better way to do that than this?



STEVE:  Yes.  Yes, I'm glad you mentioned that because this is the great danger with anything coming from WikiLeaks.  And all security people appreciate this.  Back when all of the Hillary email controversy was up in the air and stirring, the problem was that it would be easy for a nefarious actor to alter legitimate email or to slip in particularly damaging email among a bunch of otherwise boring stuff about her daughter's wedding and so forth, and be able to ride on the credibility of the rest of the known good stuff.  So, yes, I'm glad you said that, Leo, because we do need - that's why I made a point of saying, if we are to believe this, then this is what it's saying.  So we don't know.



Okay.  One more before our next break.  And, oh, boy.  There's a company we've discussed in the past, Zerodium, which is kind of a fun name, Zero as in zero-day, Zerodium, offering now, get this, half a million dollars for secure messaging app zero-day vulnerabilities which are exploitable.  The discovery and private reporting - and this is coverage from Threatpost.



"The discovery and private reporting of exploitable zero-day flaws in mainstream secure messaging apps now brings with it a payment of half a million dollars.  Last week, Zerodium" - whom we discussed before - "a vendor operating in the nebulous exploit acquisition market, updated their pricing structure to put a premium on zero-day vulnerabilities in secure messaging applications.  Both remote code execution and local privilege elevation zero days in messaging apps - including WhatsApp, Signal, Facebook Messenger, iMessage, Telegram and a few others - can fetch as much as $500,000 from the company's new program."



As we've covered here before, Zerodium purchases zero days, then makes them available in a feed of exploits and defensive capabilities to its paying subscription customers.  And of course those attacks and vulnerabilities are never shared with the affected vendor since they would then be promptly patched and made valueless.  So in this dark ecosystem it is in the interests of everyone within the "supply chain," in quotes - the discoverer who wants top dollar for their discovery, Zerodium who is in this for profit, and the exploit purchasers who want to use the exploits obtained from their subscription as long as possible, until they become patched.



So it's in everyone's best interest to closely guard and protect the secrecy of the vulnerabilities they are leveraging.  Thus this chain survives.  Zerodium's official policy is to sell only to democratic and non-sanctioned governments.  But one does wonder how tightly that policy is maintained when cash is dangled, since profit is the underlying motivation here.  So they could well just be saying that for face saving.  I mean, again, they're buying exploits and selling them and not reporting them.  Their pricing changes were primarily aimed at the mobile platforms.  And it's interesting, too, that that's the premium dollar now is mobile messaging.  This suggests that's where the action is.



The company is also offering half-a-million-dollar payouts for remote code execution and local privilege escalation bugs in default mobile email applications; $150,000 for baseband, like we were discussing recently the Qualcomm firmware problem for baseband; and media file or document remote code execution and local privilege escalation attacks; $100,000 for sandbox escapes, code-signing bypasses, kernel local privilege escalation, WiFi remote code execution, and also SS7 attacks, which we know is the Signaling System 7 for the whole cellular network.



Zerodium's founder, Bekrar, told Threatpost that government customers are in need of advanced capabilities and zero-day exploits that allow them to track criminals using these secure mobile apps.  And of course governments have the deep pockets required to foot the hefty exploit subscription cost that Zerodium must be levying in order to offer such substantial payouts, that is, they're paying half a million dollars for an exploit that doesn't have a super long lifetime, as far as we know.  Again, we don't know.  But so they've got to have multiple governments on the back end willing to pay serious money.  And who knows, like, what granularity.  Like do they have one contract with the U.S. government that then shares it among our multiple intelligence services?  Or is it a per service agreement?  Who knows?



Anyway, Bekrar said:  "The high value of zero-day exploits for such apps comes from both a high demand by customers" - I choke on that word, by "customers" - "and a small attack surface in these apps" - meaning they're secure, and there's not much to go after.  It's not like an OS where it's way, you know, like a desktop OS - "which makes the discovery and exploitation of critical bugs very challenging for security researchers."  In other words, because Google and Apple are really doing, like, they're putting a lot of focus and concentration on security, the consequence is greater security.  Over time it keeps getting better.  So that attack surface gets smaller and smaller, and consequently it's harder to find problems.  But when they are, they're worth more money.  So in other words, they're both rare and valuable.



Zerodium also announced that it would offer $300,000 for Windows 10 remote code execution zero days, specifically remote exploits targeting default Windows services such as - yeah, wouldn't you like these - SMB and RDP.  That's, of course, the Server Message Blocks that WannaCry exploited to such powerful ends, and RDP is the Remote Desktop Protocol, the remote console services.  Also web zero days they're paying for, specifically Apache on Linux and Microsoft IIS remote code execution attacks, are now fetching $150,000, while a Microsoft Outlook remote code execution is worth $100,000.  Mozilla Thunderbird remote code execution and VMware ESX guest-to-host escapes, which are of course in the case of VMware sandbox escapes from the VM, both pull $80,000.



Threatpost notes that Zerodium also doubled, or nearly doubled, payouts for Chrome, PHP, and OpenSSL attacks; while Tor remote code execution on Linux and Windows climbed from $30,000 to $100,000 and $80,000, respectively.  Nearly a year ago, Zerodium tripled the bounty it offers for an Apple iOS 10 remote jailbreak to 1.5 million, after previously offering 1 million for iOS 9 zero days.



So what we're seeing here is the emergence and continuing evolution of a predictable for-profit ecosystem created by the tension which currently exists between uncrackable crypto and multiple governments' perceived need to monitor whatever they choose of their own and other citizens' activities.  I feel this is not a stable state of affairs.  One way or the other, this is a transient, created by the post-Snowden and comparatively sudden rush to encrypt after the world learned exactly how much monitoring was actually going on.  It's going to be fascinating, as we were saying before, to watch how this fundamental tension resolves itself in our near future because, one way or the other, I think it's going to have to be resolved.  In the meantime, this happens.  And, boy, apparently the market is strong.



LEO:  The market is strong.  Well, and this is what happens.  Then you don't get - the companies don't get the information.



STEVE:  Right.



LEO:  The bad guys get it.



STEVE:  Right.



LEO:  Steve?  Now what?  Now what, Mr. Gibson?



STEVE:  Now what?  Ah, yes.  So, okay.  We have a deprecated, insecure Apple authorization API that can be abused to run code with root privileges.  This is some more coverage from Kaspersky's Threatpost and paraphrasing from their coverage.  They write:  "A deprecated Apple authorization API, invoked by third-party installers, is still developers' preferred choice for updating apps and services on macOS, which has a problem due to a massive security issue that could be abused by local attackers to elevate privileges to root with a little unwitting help from the user."



The situation is known and was raised again last month during DEF CON by noted Mac security researcher Patrick Wardle, who's the chief security researcher at Synack - who happens to be down here in Southern California not far from me - due to the ongoing use of the API.  It's a function called AuthorizationExecuteWithPrivileges.  Installers for many popular applications including Slack, Google Chrome, Google-owned Dropcam, VMware Fusion, numerous security software updaters, and open source update library Sparkle all use this deprecated API during installation and updates.



At DEF CON, Synack's Patrick Wardle explained that the API causes the system to display the familiar authentication dialogue box.  So, I mean, this is part of the OS, which is handled by a separate daemon running in the OS, meaning that the user doesn't have to entrust the application installer with their password.  So you're giving your password to this other piece of the operating system, which then works on behalf of the installer in this area so that you're not trusting the installer, which is, you know, that part is good design.  The operating system passes that trust to the daemon, and any functionality needing admin or root privileges upon installation may then proceed without the installer having it.



However, this AuthorizationExecuteWithPrivileges does not validate what is about to be executed on the machine to verify that it was not maliciously modified.  Therefore, an attacker that is already present on the computer and has some code running can wait patiently for one of these third-party installers to call upon this insecure authorization API and piggyback off the user's credentials as they are entered into the dialogue.  So let me be clear that this malicious software also cannot access the credentials themselves.  But, as I'll explain in a second, due to the fundamentally insecure design of this, something that's co-resident in your system can use this window of opportunity, thanks to the fact that this API does not verify what it's being asked to do, to do the malware's bidding, as well.



Okay.  So Wardle said, quote:  "Normally what happens is these applications ask the operating system to execute something as root, and what they ask to execute is writeable by everyone, like something that's in a temp directory or the downloaded application bundle."  And of course it has to be writeable by everyone because they needed to be able to write it.  So there's unprivileged access to the thing that the installer wants the OS to then trust.



"But this means that any running and patient local code, malware, or a local attacker who already has some access to the device, can alter what's about to be executed with root privileges because, by definition and necessity, non-privileged processes have access to what's about to be executed.  Since the OS does not verify what the application requested to have executed wasn't modified, when the user puts in their credentials and clicks install, the system will execute whatever was requested, even if that has been maliciously modified."



Wardle said the situation extends to Apple's own installer, as well, which suffers from a separate but related issue.  He explained that, as a user double-clicks on a package file, a .pkg, Apple's installer app executes and looks for plugins in the file and copies them to temp, loading those libraries into the process context of the installer application, which is signed by Apple and trusted by the OS.  But note that the libraries themselves aren't either signed or trusted, yet they're still stuck into this trusted application space, giving it a lot of power.



Wardle said that a local attacker can win a race condition and modify those packages before they're installed on a local system.  The malicious and unsigned dynamic libraries, since they're not verified by the installer app, will be blindly loaded, Wardle said.  Those malicious "dylibs" can create their own easily forged authentication popup that will be trusted by the OS.  The user will then enter their credentials, unwittingly giving malicious code permission to run as root on the machine.



Now, here's the sad thing.  Apple understood the problem and consequently deprecated this AuthorizationExecuteWithPrivileges API four years ago, back in 2013, and now recommends that developers use a different solution for the same problem called SMJobBless.  SMJobBless works by copying whatever is being asked to install into a higher privileged directory.  And once it's in that secure directory, SMJobBless cryptographically validates that it has not been tampered with, and only then asks the user to authenticate.  In other words, they really did solve the problem.



So somewhere in 2013 this came to their attention, and they said, ooh, you know, the way we've been doing this, not so good.  So they fixed it; right?  Okay, so what's the problem?  Using the much more secure SMJobBless comes at a cost, literally, because developers choosing to use SMJobBless must obtain an Apple developer certificate to perform the required cryptographic signing.



Synack's Wardle, who is clearly no slouch, said, and I like this:  "The problem is that there is a secure solution" - meaning SMJobBless - "but it costs money and," he wrote, "it's incredibly complicated.  It took me," he wrote, "several hours to get it working; whereas the deprecated and insecure API is literally, like, three lines of code."  He said:  "It's really easy, and that's why everybody does it still."



Wardle said he privately disclosed to Apple that its installer was loading unsigned libraries, but Apple did not respond.  He also reported the API issue to Google with regard to Chrome; and Google responded that it was aware and that, quote, "No good replacement exists," including, in their opinion, SMJobBless.



Wardle concluded, saying:  "Looking at the secure replacement, it's such a pain to get it working.  There's a massive tradeoff.  In an ideal world, everyone would use the secure replacement, or Apple would provide a secure replacement that would be easier to use.  When you have the Chromium guys saying this isn't a valid replacement, they generally know," he says, "what they're talking about."



So this is admittedly a difficult problem to solve.  We know there is no replacement for credentialing developers.  It's just like credentialing web servers, which we all do.  That's what TLS and HTTPS is for.  If we don't do that with web servers, there's no way we can trust their assertions.  And this is an example of why Let's Encrypt is an improvement, but not a replacement, for full credentialing.  As we know, Let's Encrypt promises nothing more than the re-use of the existing HTTPS TLS system for transiting traffic within encryption privacy.  But this is, and has been, massively abused by malicious actors.  Which is why Let's Encrypt hasn't and doesn't threaten the established certificate authority industry, who continue to provide a service that cannot be automated because, if it could, it would be abused.



Similarly, Apple cannot blindly offer free developer certificates to all comers; otherwise, all would come, some would abuse, and no assurance of enhanced security would result.  And just as traditional certificate authorities need reasonable payment for the non-automatable and necessary scrutiny they provide, so, too, does Apple.



But that said, it does seem as though Wardle's point about the comment from the Chromium developers is a very good one.  Some money needs to be paid, but then the rest should be easy.  And certainly Apple is clever enough to make it so, if they gave it the priority that I hope the light that has been shined on it now at DEF CON, thanks to the work of these Synack guys, may light a fire under them in order to make it happen.  It feels as though their first pass at this, or I guess really it's their second pass, was not sufficiently well thought through.  They may have been in a hurry to fix the problem that was discovered.  What they came up with was something that, because it's so burdensome, nobody bothers using it, even though it exists.



So let's hope they get that fixed and reduce it to a line or two of code and then a potential vulnerability that we're all living with now.  I mean, this isn't patchable.  This is not something Apple can fix.  They need to fix the underlying API and ultimately deprecate, go beyond deprecate, they need to remove this long insecure, but still available and preferred, API.



And of course Apple gets heat for this.  A lot of developers complain that this is one of the problems with Apple is that they're rolling forward, and they're killing off old stuff that's breaking existing code.  It's one of the things that Microsoft has not chosen to do over time.  As a consequence, you know, they're dragging all this legacy stuff behind forever.  But at least old stuff still works.  Apple has decided to take a different approach.  And here's an example of where, well, at some point they're going to have to start warning people, this is going away.  And we mean it.  But before they do that, they need to come up with a sufficiently useful replacement.



So, but again, this exists today; and now a bright light has been shined on it.  And so, yikes.  Maybe there can be some sort of a compromise.  I don't know.  Maybe they can strengthen the existing API that everyone's using.  After failing to get everybody to switch over to the new one, go back and say, well, okay, we've got to do something about this.  But the problem is unsigned code.  If you don't require signed code, and thus make the developers who signed it responsible for their own code's action, then you're never going to have a secure ecosystem.  There's just - there isn't a way.



So again, it ought to be, you know, get a certificate to sign your work, and it ought to be a reasonable price, but then easy to use.  All of my code is signed with an Authenticode certificate from DigiCert, of course.  And it's easy for me to do.  When I'm developing code, for example, SQRL, every release I've offered of SQRL has been signed.  So I'm working myself.  When I have something that I want to push out to the gang to test, I just type "sign it," and it does.  I mean, so not a big problem if it's designed right.



Okay.  We have spoken many times about what I would call - I guess my first instinct is to call it a "mixed blessing," but I'm not sure it's so mixed.  It's an evil necessity, how's that?  The Intel Management Engine, the IME, this separate processor which we know is running in the southbridge chip with its own firmware, which has to be there, that is, our motherboards have become so ridiculously complex now that the processor needs a processor.



And that's what we've got.  We've got a processor processor.  We've got something that fires up when the plug is plugged in.  Not even when you turn it on.  The machine isn't on.  The fans are not spinning.  The main processor is not on.  But the motherboard's processor is lit up and thinking about something.  And, you know, there are things it's easy not to appreciate.  Leo, you and I will, and all the old-timers among us will, notice that all of the interrupt jumpers have gone away.  The IRQ...



LEO:  Thank god.  Thank god.



STEVE:  Yes.  You know, the COM ports.  Oh, my god, I ran out of COM ports, and I can't get any more.  Or I've got four COM ports, but only two IRQs; and my COM software is confused because I'm trying to share one IRQ on two ports; and I have to rejumper and rejigger and blah blah blah.  You know, all of that.  And then there was the memory jumpers.  We had to be flipping little dipswitches back and forth in order to move segments of memory around.  Those were the good old days.



LEO:  Are you saying that the Management Engine replaces that?  Is that how they did it?



STEVE:  Yes, yes, it just all disappeared.  Just like, okay, we'll just, you know, people shouldn't have to worry about this.  Code should do this.  And so, yeah.  So it sets all this up.  It enumerates the bus.  It goes out and scans around.  It finds boot ROMs in the different PCI slots, and it figures out what order they should be executed in based on automatic interdependency resolution.  I mean, we plug the plug in, but that thing is busy.  And it also runs the clocks.  It's the reason, for example, BIOSes are, like, amazing.



You look at this thing, it's like, wait a minute.  I could just type some numbers and change the frequency of everything.  I can change weight states on DRAM and everything.  That's all soft now, thanks to this stuff.  So the point is you really need it after you plug your motherboard in.  But the concern has been you really don't need it afterwards, yet it hangs around.  And our listeners will remember that I chased down, I chased for a week or two a bizarre problem I had when I added another server to my network about a year ago now.  Something was causing an ARP collision because two different things were, I mean, I never even really did figure out what the problem was.



What I ended up doing was finally thinking, okay, I give up.  I don't know what is happening.  So I just moved the network interface from the primary NIC to the secondary NIC, and all was well again.  The problem just went away.  And then it was because I realized that primary NIC was the Intel Management Engine interface, and it was busy messing with my network.  I mean, there was nothing I could configure.  It was all magical and transparent, and in this case wrong.  So we've talked in the past about the concern people have had about the security of this, the fundamental security, because Intel won't tell us what's in there.  It's all proprietary and magic and closed.  And they've gone to some measures to keep it that way.  And as we know, there have been exploits.  There have been defects found.  We talked about one a few months back.



So the guys over at Positive Technologies have discovered that it is in fact possible to shut down the vast majority of Intel's worrisome IME motherboard integrated subsystem.  The story is the same one we often tell here.  Some things can be protected; others cannot.  The microcode firmware in Intel's preboot environment must be decompressed when it is pulled from storage.  And that decompressor code cannot itself be compressed because it's got to do the decompressing.  Therefore, if the decompressor could be located and reverse-engineered, then the secretly compressed preboot system firmware can be obtained in decompressed form.



It turns out the Intel firmware is compressed using something we haven't discussed for a decade, but we did once:  the variable-length Huffman coding system, which as it happens every version of SpinRite has employed for the past 30 years to compress its own highly compressible user interface data.  So I built my own Huffman compressor for SpinRite 1.0 because I know that a user interface is highly compressible, and that's just who I am.  I want to compress things when I can.



Huffman coding is an incredibly elegant and efficient scheme.  It does not provide high levels of compression.  So it shouldn't be confused with state-of-the-art block sort compression and zlib and things, which are far more sophisticated, but far more complex to implement.  Huffman coding, I mean, my algorithm is just, like, 20 lines of assembly language.  So, you know, it's just it's tiny, which is why it makes so much sense to use it.



What it does is it takes an alphabet of compressible things, like, for example, machine language bytes, or in my case user interface text and tokens, and it performs a frequency analysis.  It turns it from - you could think of it sort of as in time domain to frequency domain.  That is, it determines how many of each different token occur.  And if they're absolutely uniform, then it's not possible to compress it at all because the scheme relies on the fact that text and, to a big degree, machine instructions are highly non-uniform.  As we know, "T" and "E" occur with great frequency in English.  And, you know, "Q" occurs with much lower frequency.



So what the Huffman system does, just magically, it's just so cool, is it automatically assigns the optimal length number of bits, which are shorter, for the tokens that occur most often, and longer for the tokens that occur least often.  So it converts from a fixed-length encoding, where for example all bytes are 8 bits, to a variable length encoding, where those 8-bit bytes which occur most often end up being represented by just a few bits, and the right number of few bits, based on the inter-token competition that automatically exists.  Anyway, so that's what Intel did, for the same reasons I did.  They chose Huffman because it made sense just to squeeze their firmware down.  But in the process, it also encoded it.  It turned it into gibberish.



Now, it's not super strong encoding.  A cryptographer would just cut through that in a moment.  But what these guys did was they managed to find the Huffman coding table.  Because what you do, when you compress with Huffman, you end up creating this translation table.  That has to go along with the compressed result because that's the key used for decompressing what was compressed with it, unlike for example the Lempel-Ziv-based compressors, where their elegance is that you don't need to ever send the dictionary, as it's called, along with it because it dynamically builds a dictionary on the fly.



In this case, because this is such a simple technology, you do have to package the dictionary along with the decompressor.  These guys found the dictionary, understood what it was, and then took the Huffman-compressed and -encoded firmware for the microcode and decompressed it.  And then they began looking around.



So the page that I've linked to in the show notes, if anyone's interested, provides a painfully deep and very comprehensive dive into the weeds of what they did and what they found.  But the short version is they discovered something known as the High-Assurance Platform, or HAP for short, and a bit flag for enabling it, whatever it is.  They were unfamiliar with the term.  But as they wrote, a bit of googling quickly revealed the truth.  The second search result said that the name belongs to a trusted platform program linked to the U.S. National Security Agency, our NSA.  Not, apparently, for any nefarious reason, which I'll explain.



After additional research and lots of experimentation, they determined that the setting of this bit flag caused the Intel management engine to be shut down once it had performed all of the necessary work of powering up, configuring, and starting the system, and giving control to the platform's bootable operating system.  After that, bye-bye.  In other words, the NSA is no more happy than we are to have this potentially buggy, undocumented, secret, and exploit-prone hidden subsystem always running in the background on our motherboards while it has full access to everything in the system.



The NSA, however, has a bit more negotiating leverage than we do.  So they were able to get Intel to acquiesce and provide them with this undocumented and well-protected option for completely shutting down the IME immediately upon the completion of its power-up work.



Wanting to be absolutely certain of their findings, the Positive Technology guys reached out to Intel for comment and received the following reply.  Intel quote:  "In response to requests from customers with specialized requirements, we sometimes explore the modification or disabling of certain features.  In this case, the modifications were made at the request of equipment manufacturers in support of their customers' evaluation of the U.S. government's High Assurance Platform program.  These modifications," Intel writes, "underwent a limited validation cycle and are not an officially supported configuration."



Except they are.  But, yes, we understand what Intel is saying.  In other words, yes, you found it, but we take no responsibility for its use.  And, after all, we did do everything we could to hide it from everyone to prevent its discovery.  But it's there.  I don't know if what will surface will be a tool that lets us do that.  I wouldn't be surprised.



Positive Technologies' current posting is detailed.  And they wrote at the beginning that that posting is only the first of several.  They're apparently going to tear this thing down to its constituent bits.  And it may well be that - I don't know if it will ever be user-accessible without hardware.  There's something called the SPI, which is the Serial Programming Interface.  And Leo, you're familiar with it.  You actually use it because...



LEO:  I do?



STEVE:  ...that's how I've updated my firmware of my various gadgets over the years. 



LEO:  Oh, okay.



STEVE:  That's that programming interface, very common.  It's a standard.  But it is hardware.



LEO:  Oh.  Oh, yeah.  That's the thing.  Okay.  I know.  I use it for that little whatever that is, the match chip or whatever, the EEPROM.  Yeah, the EEPROM, yeah. 



STEVE:  Exactly.



LEO:  Steve and I have a secret thing, a little bit of hardware.  And every once in a while he'll send me - yeah.  Okay, cool.



STEVE:  Yeah.



LEO:  And I have a little Linux command-line utility that does it.  But there's [crosstalk].



STEVE:  Yeah.  And so that is - but it does require an SPI programmer, not something that you can do by setting a jumper on the motherboard of our PCs or so forth.  So anyway, interesting that in fact all this grief that we've been suffering, and the angst, and here all along there is a program that says, you know, we don't want that left running for no good reason after you've got everything up and going.  So turn it off.



LEO:  There'll be an open source tool that takes advantage of this soon.



STEVE:  I wouldn't be surprised.  I think we're going to see something surface.



LEO:  Yeah.  So to speak.



STEVE:  So anyway, nice piece of work from the Positive Technologies guys.  So I wanted to keep this on people's radar because this is important.  So this is not news, per se, except it's another "this actually happened" event.  John Biggs, who is a writer, consultant, programmer, and the former East Coast Editor and current contributing writer for TechCrunch, wrote, and I lightly edited:  "At about 9:00 p.m. on Tuesday, August 22nd" - so a week ago today - "a hacker deauthorized my phone's SIM card, replaced it with his, presumably by calling T-Mobile.  This, of course, shut off network services to my phone and, moments later, allowed the hacker to change most of my Gmail passwords, my Facebook password, and to impersonate me in texts sent under my identity."



LEO:  Oy.



STEVE:  I know.  "All of the two-factor notifications went, by default, to my phone number, so I received none of them; and in about two minutes I was locked out of my digital life."  He wrote:  "I noticed all of this at about an hour later, around 10:00 p.m., when I assessed the damage and called T-Mobile.  By 10:30 I had restored my phone's SIM and began the process of changing all of my passwords, hardening my two-factor accounts and my T-Mobile account, hopefully ensuring that this would not happen again."  But he wrote:  "Sadly, I worry it will."



He said:  "My hacker was thorough.  In the course of a few minutes he or she did a quick search of my Facebook Messenger messages and assessed that I was originally from Ohio and that my Dad was sick.  He or she used this information to approach people I knew in the cryptocurrency space with a story that was arguably ludicrous.  The hospital would pull the plug on my father if they didn't get payment of a bill; and that I, in my anguish, needed to borrow and sell 10 bitcoins immediately and would pay the friend back 15 the next morning.  Luckily," he writes, "my friends weren't idiots and immediately texted me and my wife.



"The hackers' IP (173.239.232.29), which points to LogicWeb of Plano, Texas, along with a breadcrumb noting a login from Florida, suggests that the hackers were from the United States.  They clearly had the modus operandi down because they also hacked two other friends of mine in the space of a week.  This all came about, I believe, after another friend in the cryptocurrency space was hacked last week.  That hack" - so that would be two weeks ago.  "That hack bore all the same hallmarks as this one except the SIM hijacking.



"First, the attacker grabbed access to my friend's Facebook Messenger and contacted everyone on his list that was interested in cryptocurrency, including me.  In the ensuing melee, the hacker asked me to send 10 bitcoin and that he would send me 11 back in the morning.  Confused, I told them that I had some bitcoin, but not that much. I then realized the ruse and asked" - I love this - "'Did you talk to Wallace Shawn yet?  He can help.  I think he's having dinner with Andre right now.'"  Now, of course, for those who don't recognize the references, Wallace Michael Shawn is a well-known actor and comedian who starred in "My Dinner with Andre."  "Anyway, the hacker claimed that Wallace wasn't available.  I knew I'd been had.



"This interaction led to my own subsequent hacking.  Once it was clear that I had some bitcoin somewhere" - because that he had admitted in the interchange - "the hackers decided I was their next target."  He says:  "Ultimately I got away lucky.  Nothing major was stolen as of today, and I took control of my accounts back fairly quickly.  I had," he writes, "some two-factor set up; but because my phone was compromised first, I lost access to most of it. I've since activated authentication apps" - and this of course is our takeaway lesson here - "for all of my accounts.  The biggest question is how the hackers took control of my SIM card in the first place.  This is the most troubling; and T-Mobile is, quote, 'looking into what happened.'"  Uh-huh.



LEO:  They have recordings of it.  And what they'll find is what always happens.  A customer service rep...



STEVE:  Yup, yup.



LEO:  ...wants to serve the customer.



STEVE:  Social engineering.



LEO:  They have very nice, very friendly customer service reps who work hard to make you happy.  They believe it's you, even if it's not.



STEVE:  Even if you don't provide sufficient evidence.  Oh, I forgot my mother's maiden name.  I just - I used to know it, but it's been so long.



LEO:  Well, I did that recently, and all they required was the last four of my Social.



STEVE:  Wow.



LEO:  And that isn't that hard to find.  I mean, you know, and it's not a really good way of validating, frankly.



STEVE:  Well, and of course we know that in the U.S. the Social Security number has long been used as an identifier when it's specifically supposed to be excluded from that purpose.  But too many people ask for it, and you don't often have a choice not to give it.  So our takeaway is this:  While it may be unlikely for everyone, targeted attacks can be quite powerful and successful.  The new hacking method is to arrange to take over the communications channel, whatever it is, used for account verification and/or second-factor authentication.



This is why time-based one-time passwords, not communications-based one-time passwords, are the only safe choice to make when a choice can be made.  And this is exactly what he did when he said he switched to authentication apps, meaning those apps which automatically give you a changing six-digit code.  Then it's not communications-based after the initial setup, which is over a secure channel, typically a QR code presented to a web page.  You snap it with your phone, or in my case you print it so that you're able to take it offline and then populate other apps in the future.  And we'll be discussing that, actually a variation on that here in a second.



So the problem, of course, is that not all services offer this.  And we can hope that, as more such instances of this exploitation do surface, more providers will begin offering time-based one-time password, or so-called TOTP, authentication options.  So I appreciate him sharing this and helping to keep this awareness because it's sad, but the reality is, as you just verified, Leo, that there just isn't enough protecting the ownership of our cell phone accounts.  And if sending anything, either to email that can then be intercepted by the mail on the phone, or by a text sent to the then-registered number of the phone, then that can be a problem.



And speaking of time-based one-time passwords, we have Authenticator Plus, which was brought to my attention just by reading feedback from our listeners.  It's at www.authenticatorplus.com, and it looks very nice.  Everyone knows that my preference is to print, maintain, and manage the QR codes for my one-time password authentications in order to manually curate and install them into new devices.  But I get it that some people may choose the convenience of networked linkage and secure cloud backup which this offers.



So in their feature overview they said:  "Secure:  256-bit AES encryption and PIN lock for additional security, along with hardware-backed encryption where available, which protects your account even in rooted devices.  Syncing across devices:  Apps are available for Android phones and tablets, iPhone/iPad, Android Wear and Apple Watch."  So broadly cross-platform over in the mobile space.  Under organization, group accounts and categories and the ability to reorder frequently used accounts to the top, change the themes, and display account logos for easier lookup.



And actually I really like that feature.  It just shows you the icon.  So, like, for Facebook and Google and LastPass and so forth, I just have text now under my Google Authenticator.  I'm going to take a look at this.  If it's possible to turn off the cloud backup, I would turn it off because I'd rather continue to manage my things offline on paper.  But this otherwise looks like a nice piece of work.



On iOS it's free with in-app purchases.  And I haven't looked any further than that.  And so I don't know what - which makes me a little uncomfortable, like is it going to limit me to five sites unless I buy it?  I mean, I'd just rather buy it.  If I'm going to use it, if it's going to be a few bucks, fine, I'll buy it.  So I would want to trust this person; but, as we know, that has to be earned.  Their FAQ says all the right things about their intentions and policies.  It's got all the jargon and so forth.  So again, I can't recommend it because it doesn't have any reputation as far as us having some reason to really believe it's right.  But I don't have any reason to believe it's not.  And the site looks nice and polished; and the FAQ, as I said, is saying everything right.  So anyway, it's there, and I wanted to point to it.



LEO:  It's the same as Authy, which I recommended, as well, which does very much the same thing and is free.  Highly recommend Authy.



STEVE:  Good, good.  And I agree.  And Authy provides the various interdevice sync and cloud sync also?



LEO:  Yeah, yeah.



STEVE:  Good.  CrashPlan has shut down, or announced that it is shutting down, its home offering.  They will stop accepting subscription renewals for the consumer version.  They're just going to go focus on enterprise.  And 9to5Mac had some coverage, writing:  "Code42, the company behind CrashPlan, has officially announced that, as of today, they are pulling out of the consumer market.  CrashPlan for Home users will have to begin migrating away from the service as it will no longer be available starting in October of 2018."  So that's responsible.  They're giving people more than a year.



In a message posted on CrashPlan's website, the company wrote:  "Effective August 22nd, 2017" - so that's last week, last Tuesday - "Code42 will no longer offer new - or renew - CrashPlan for Home subscriptions, and we will begin to sunset the product over several months.  CrashPlan for Home will no longer be available for use starting October 23, 2018."  So that means that they will honor a subscription created just previous, a one-year subscription just previous to their decision to stop, through the end of its life, but you can't get it anymore as of last week.



"So Code42 seems to be handling the situation as delicately as possible by giving users ample time," writes 9to5Mac, "to prepare for the transition, and offering discounts on alternative services," which is very nice.  "CrashPlan for Home users can transition to the Small Business plan and receive a 75% discount over the next 12 months."  Of course then the price would jump up to full price after that.  If a user wants to completely move away from CrashPlan, the company recommends Carbonite's services.



Now, many of our listeners have asked what they should do, so I'll repeat my recent advice from my own research and findings.  First of all, I'll just say that Carbonite offers a similar system, that is, an all-in-one, integrated, drop it in and go.  As I've mentioned before, that's perfect for Jenny.  Jenny has that installed on her laptop.  She doesn't know what it is or really why it's there.  But it's just keeping her safe.



My advice, I just like the idea of some bifurcation.  I get it that there's a place for an integrated, all-in-one drop-in.  And I gave a perfect example with Jennifer.  But I like to have these two things separate.  Once upon a time, Boxcryptor was what I was liking because they got the crypto right.  I took a deep dive into it, looked at it, verified that it was right.  But they have since transitioned from a product to a service.  So anyone's use now of Boxcryptor as a service is just as susceptible to them changing their Terms of Service or canceling it outright, exactly as CrashPlan just has.



This is why I don't see the benefit for a more technically sophisticated user of an integrated bundled service, again, unless you just don't want to worry about it.  What I choose is a TNO software solution whose crypto was done right, then choose among any or all of the available cloud storage providers and be free to change your mind or to use any different provider or providers for different purposes at any time.



And that's why, as I said recently, CloudBerry Labs has become my current choice.  Their crypto's done right.  They offer one-time purchase and lifetime use of a product and not a service; and they support every cloud provider any of us has ever heard of, and many we haven't.  I looked at their list, and it's like, whoa.  I mean, they must have a team that does nothing but figure out the APIs of providers.  And in fact, it may be that we're now beginning to see some merging of API.  So as long as providers support this emerging standard way of accessing their services, well, then, you can use one thing everywhere, which is great.



But anyway, so Carbonite, if you want a good alternative turnkey solution.  Or CloudBerry Labs is what I use personally, what I use at GRC.  And Sue is still using Jungle Disk just because she always has been, and it still works, which is a turnkey solution also.



So my slip of the tongue, I can't believe I did this, and so the first note came from a Daniel, wow, and Daniel, I'm sorry I can't pronounce your last name, looks like Szmulewicz.  Anyway, sorry, Daniel.  He wrote:  "You meant atomicity, not monotonicity, in the section explaining compare-and-swap."  He said:  "I hope you don't mind me saying."  And I actually thanked him for his note and said, "Of course I don't mind you saying."



And then I thought, okay, that must have just been something I said once.  Whoops.  And then Chris Frederick, who has a more easily pronounced last name, said:  "You talk quite a bit about 'monotonic' operations and functions in the latest podcast.  I wonder:  Did you mean to say 'atomic' operations?"  And so, okay, as I said at the top of the show, I must have just started off saying monotonic, which is a term I use often when I'm talking about counting.  Counters that only count upwards, incrementing, are monotonic.  They increment monotonically.  And so I apparently started off saying "monotonic," and I just let her rip and kept saying it.  Yes, I meant "atomic."



The reason they're called "atomic operations" is they are indivisible by an interrupt.  That is, if you do a separate compare instruction, then a swap instruction, that entire instruction pair is not atomic, it's a molecule, and so composed of two atoms.  So you can split them apart with an interrupt which might occur between them, thus the problem.  So these are called "atomic operations" because they are operations that are not splittable any longer.  Anyway, thank you for bringing it up.  And for anyone else who was confused, I apologize.  I just, okay, maybe I do need my moustache.  Oh, that was pre-moustache.  That was when I still had the moustache. 



LEO:  No excuse.  No excuse.



STEVE:  No excuse, yeah.  Meanwhile, another of our listeners whom I hear from from time to time, the AspiringLockPicker, said:  "I used my AutoIt UDF to break apart Steve's Security Now! pages to get at the minute and episode number of all shows and do the math.  A fun few minutes.  Security Now! has, as of this moment, 908.05 hours of total content, spread across 623 episodes."  So not quite a thousand hours, 908.  Just a shred over 908 hours of content.  Yup.



LEO:  A goodly number.



STEVE:  And growing at the rate of about two hours a week.  And two interesting quick little points about SpinRite.  I saw a note from an "xoff00."  Boy, I haven't seen XOFF for a long time.  XON and XOFF are two reserved ASCII non-printable control codes which were used to start and stop the paper tape reader.  Or I guess maybe sometimes - no, I don't think it was optical.  It was back in the teletype days.  Something would send an XOFF if its buffer was filling, and it needed to stop the reader from going [making chunking sounds] as it was reading the holes in paper tape.  Then the reader would catch up and then start again.  And it's still - it's a "software flow control" is how it's better known.  And so it exists in some terminal applications and so forth.



Anyway, he said:  "A coworker just told me he regularly runs SpinRite on USB thumb drives.  Your thoughts?"  And it's interesting.  I think it's a great idea.  One of the things we know is that thumb drives have become incredibly inexpensive, super cheap.  Unfortunately, they're cheap in both senses of the term.  They're both inexpensive, and they are low quality.  No expense beyond the bare minimum has been put into them, and people have problems with them.  Running a Level 2 scan will not fatigue the thumb drive, but it will help to keep it alive.



And the idea is that, you know, the more robust SSDs are highly over-provisioned, sometimes as much as 40% over-provisioning, meaning there's an additional 40% of storage space you don't see.  So it's actually, even though they say it's 64GB, that's the 100%.  It's actually 140%, for example, in terms of physical storage.  And the reason is that gives it resilience and life.  And as spots go bad, as they will, then the reserved area gets pulled into use, making that aging and fatiguing transparent over time.



SpinRite, as we keep seeing, is great on nonvolatile solid-state media, almost perversely as good as it is on spinning media for which it was designed, because the underlying technology is still very similar.  So, yes, if you are a heavy thumb drive user, giving a thumb drive a SpinRite Level 2 scan from time to time makes lots of sense.  And I imagine doing so regularly will help to keep them from ever dying.  



LEO:  I should mention that our official count is 968 hours.  I'm not sure where the other 60 hours come from.



STEVE:  Interesting.  I wonder - I don't know whether he...



LEO:  We may have counted all the special episodes.



STEVE:  ...actually listened to the, I mean, whether his data pulled from the metadata of the MP3s, or if he parsed my pages.  Because I'm normally rounding down to - I just know I'm rounding.  So I don't know.  It could be rounding error or what.  But that's a large difference.  So I don't know what...



LEO:  Patrick Delahanty, the keeper of the stats, says 40 days, seven hours, 59 minutes, and 18 seconds of content.  Average running time 01:32:20.



STEVE:  My goodness.



LEO:  Yeah.



STEVE:  And that average is going up over time, too.



LEO:  Yeah.  629 episodes, including special episodes with nonstandard numbers.  So the episode count is roughly right.



STEVE:  Yeah.  Interesting.  Well, I did say - I mentioned a couple weeks ago, I think it was recently, that one of the nice things about SpinRite was that it was so small.  You could run it in a 640K VM, and it would be completely happy because it pulls in DOS, and it pulls in itself.  I mean, it used to run and can run in a 640K memory environment.  The problem is I don't think VMs go that small.  You can't turn the dial down that much.



LEO:  I'm sure they're at least a megabyte, yeah.



STEVE:  Anyway, so I got a tweet from a FlitBee, and I have the photo.  I wasn't able to grab it off of - he just sent it in my Twitter stream, but I captured it.  But I got a kick out of it.  He sent:  "SpinRite scanning two drives in a 128MB VM."  And it shows two SpinRite windows side by side with SpinRite running away.  And this is what's cool is that, unlike running whole operating systems that require a gig or more each, SpinRite is actually very practical to run in a virtual machine because it requires just hundreds of K.  I may think about this maybe for the future.



My plan for after v6 is to, you know, the v6 series, the idea is to use it as the platform for developing the next generation of hardware interface technology.  So all SpinRite 6 users will get the benefit of that immediately.  But the goal is to come up to speed with the state-of-the-art hardware support, finally getting rid of the BIOS and all that.  And that will then be a proven base upon which SpinRite 7 gets written to, and that's where we lose the textual DOS interface and DOS completely, and I roll up my sleeves and probably do a hybrid of assembly language and Python because everyone would rather have it sooner rather than later, and it just makes sense at this point to move us off to a higher level environment.



One quick closing the loop note from a Pete Blumenthal, who listens.  And then we'll talk about "Shattering Trust."  And I liked this.  This caught my attention just as we were getting ready to go on-air, so I dropped this in at the last minute.  He asked about my thoughts on voiceprint for phone authentication.  And then he gave a link to a Fidelity.com something or other, TD Bank something.  He says:  "I see no technical details."  But we don't need any.  And I have a couple thoughts which I think are useful.  First of all, it's clearly a bad idea if the phrase you are asked to repeat for your voiceprint is unchanging because anyone could make a recording and play the recording.  But if instead you are shown something on the screen which you then must speak so that every utterance for authentication is unique, then it's interesting.



It's not that someone can't impersonate someone else's voice.  We know we've heard very good impersonations.  And we know that software, unfortunately, is rapidly acquiring that ability, the ability to do exactly that.  But what I liked about those two counter examples, repeating the same thing versus being challenged with a unique statement to utter is one is like a password, where your password is unchanging.  You always put the same thing in; so, for example, a keystroke logger can capture it.



The second one, the better security, is the challenge/response model where you are challenged in some form with something that has never been offered before and will never be repeated, to solve the replay attack problem.  And then you must respond to that unique challenge properly and correctly.  So anyway, I just liked that, Pete's question as a jumping-off point, because it's interesting to consider that, voiceprint as a biometric.  Makes me nervous, especially if it was done wrong, and it could be done wrong if it was the non-challenge-and-response authentication, if it was just, you know, say your magic phrase, again, because that's trivial to capture and to repeat.  So, cool question, though, Peter.  Thank you.



And, finally, "Shattering Trust:  When Replacement Smartphone Components Attack" is the title of the paper.  Very interesting piece of work these guys did.  Their abstract explains the concept.  They said:  "Phone touchscreens and other similar hardware components such as orientation sensors, wireless charging controllers, and NFC readers are often produced by third-party manufacturers and not by the phone vendors themselves."  Okay.  So in other words, our modern smartphone is a system.  It's got a bunch of peripherals which are all in the same package, and we hold it all in one hand.  But in fact those subcomponents come from somewhere else. 



"The third-party driver source code to support these components is integrated into the vendor's source code.  In contrast to 'pluggable' drivers, such as USB or network drivers, the component driver's source code implicitly assumes that the component hardware is authentic and trustworthy.  As a result of this trust," they write, "very few integrity checks are performed on the communications between the component and the device's main processor."  Which is where we insert:  What could possibly go wrong?



They write:  "In this paper, we call this implicit trust into question, considering the fact that touchscreens are often shattered and then replaced with aftermarket components of questionable origin.  We analyze the operation of a commonly used touchscreen controller."  In this case it was from an Android device.  "We construct two standalone attacks, based on malicious touchscreen hardware, that function as building blocks toward a full attack:  a series of touch injection attacks that allow the touchscreen itself to impersonate the user and exfiltrate data, and a buffer overflow attack that lets the attacker execute privileged operations.  Combining the two building blocks, we then present and evaluate a series of end-to-end attacks that can severely compromise a stock Android phone using standard firmware.  Our results make the case for a hardware-based physical countermeasure."



So in other words, there's currently an assumption of trust among the hardware subsystems of Android smartphones.  I'm reminded once again of that classic scene, and here's where we talked about "The Wrath of Khan" from "Star Trek:  The Wrath of Khan," where Khan was approaching under radio silence, and Kirk had not raised his shields.  Spock cited regulations about the need to adopt a defensive posture wherever the intentions of an approaching ship had not been confirmed.  Kirk waves Spock off.  One of Khan's own minions commented that Kirk was not raising his shields, to which Khan replied, "Of course not.  We're all one big happy Federation."  Whereupon Khan blasted the crap out of the Enterprise.



LEO:  What?



STEVE:  Oh, yes.  So as we have seen many times, it's all about security boundaries and where those boundaries are placed.



LEO:  But wait.  Why didn't Kirk raise his shields?



STEVE:  Because, you know, he knew, he thought he knew the captain of the ship that was approaching, and they hadn't been able to establish coms.



LEO:  Okay.



STEVE:  But he thought, huh, that's odd.  He was sort of, hmm.



LEO:  A strange lapse on the part...



STEVE:  That's when the first photon torpedo hit, and it's like...



LEO:  ...of James Tiberius Kirk, a strange lapse of judgment.



STEVE:  Yeah, well, it was funny, too, because I think it was Scotty or somebody else on the bridge chided Spock for, like, telling Kirk what he well knew.  And then Kirk afterward said, "You just go right on quoting regulations."



LEO:  "It was only logical."



STEVE:  Not that I've memorized the...



LEO:  I think you have.



STEVE:  ...dialogue of the show.  As we've seen many times, as I was saying, it's about security boundaries and where they're placed.  If a system's security boundaries are all-inclusive, that is, if there are no internal boundaries, then any adversary who manages to slip inside the system's outer boundary will potentially obtain unfettered access to the keys to the kingdom.



In their research paper, way later on, and I've got the link to the paper in the show notes if anyone's interested, they say:  "Counterfeit components have been in existence ever since the dawn of the Industrial Age.  Their effectiveness as attack vectors is also well known.  What, then, is unique about the particular setting of a smartphone?  We argue that our specific attack model is feasible because we assume only a specific component with an extremely limited hardware interface is malicious, while the rest of the phone, both hardware and software, can still be trusted.



"Furthermore, we assume that the repair technicians installing the component themselves are not malicious, and will perform no additional operations other than replacing the original broken component with an unknown-to-them malicious one.  Hundreds of millions of devices satisfying this attack model exist in the wild.  One can assume that these limitations make this attack vector weaker than complete hardware replacement.  We show it is not.  On the contrary, the nature of the smartphone ecosystem makes this attack model both practical and effective."



In other words, people are shattering their screens.  They are taking them to third-party service organizations.  They are having them replaced with aftermarket screens of unknown or non-OEM, not the original equipment manufacturer origin.  And the firmware, what these guys did is, just by changing the firmware programming, they were able to completely commandeer and take over Android smartphones from the setting of the processor.  Ah, you found the scene.



LEO:  Just like Khan.  I love Khan's extended family.  There you go.  We're all one big happy family, aren't we.



STEVE:  Yeah.



LEO:  Yeah, mm-hmm.



STEVE:  So the authors followed responsible disclosure practices by disclosing the Synaptics device driver vulnerabilities to Google back around the middle of February of this year.  The disclosure includes the details necessary for understanding, reproducing, and fixing the issues discovered during their research.  Google acknowledged the reported issues and issued a CVE (the Common Vulnerability index) of 2017-0650, with critical severity.  The vulnerabilities discovered in the Atmel device driver, so there was a little Atmel chip in the keyboard, are being compiled into a responsible disclosure report at the time of the submission of the paper.  And they conclude, saying:  "The threat of a malicious peripheral existing inside consumer electronics should not be taken lightly.  As this paper shows, attacks by malicious peripherals are feasible, scalable, and invisible to most detection techniques."



Yeah, nothing would - you could scan the firmware of the phone till the cows came home.  I mean, it's up there in the keyboard, where you would never think to look.  "A well motivated adversary," they write, "may be capable of mounting such attacks in a large scale or against specific targets.  System designers should consider replacement components to be outside the phone's trust boundary and design their defenses accordingly."  So really interesting, I mean, fully fleshed-out proof of concept.  Go to a service place, have the screen on your phone replaced, and at the moment it has the ability to take over the device.  Trust boundaries.



LEO:  And this is, by the way, why Apple is very reluctant to let third-party replace phones.  And if they do, remember there was a problem.  There was actually a bug in Apple's firmware that crashed, bricked the phone if the screen's been replaced.  Remember that?



STEVE:  Well, remember, it was the Touch ID.



LEO:  Because they felt it had been compromised.



STEVE:  Right.  The people were getting their iOS devices swapped for screens.  But since they couldn't reproduce Apple's proprietary fingerprint sensor, they were non-fingerprint sensor Touch ID, I mean non-Touch ID.  And the phone said, "What?"  And, you know, said, "No way, I'm not going to."  Which is good.  There is a strong security boundary.  It said, "Third party?  No."  Now, mixed blessing, of course, because some people would say, wait, it's so much cheaper to use the third party than to use Apple certified service.  And unfortunately, sometimes you get what you pay for, or even more than you paid for.



LEO:  Well, part of this is also because Apple doesn't offer any third parties the official parts.



STEVE:  Right.



LEO:  I mean, that might be another way to solve it.  Apple prefer that you go to them.



STEVE:  Yeah.



LEO:  Interesting, yeah.  Don't use third-party parts for security devices.



STEVE:  And if an untrusted ship is approaching, always raise your shields.



LEO:  Yeah, raise your shields for crying - especially if some guy not wearing a shirt.  Well, I guess you didn't know that yet.  That came about later.  "Khan."



STEVE:  He was very macho, Leo.



LEO:  He was Ricardo Montalban.



STEVE:  He was genetically engineered, yup.



LEO:  Steve Gibson has been generally engineered to be moustache-free.



STEVE:  I wish.  I hate shaving, so it'd be nice if it weren't going to grow.



LEO:  I know.  I know.  Well, let's just all grow beards.



STEVE:  No.



LEO:  Let's just do it.  Let's just do it, just take it out of the equation.  Of course then we would lose our sponsorships.  But that's okay.  That's okay.  Steve is the man in charge at GRC, the Gibson Research Corporation.  So if you get a bill for research, it's probably because you bought SpinRite, the world's best hard drive maintenance and recovery utility.  That's at GRC.com.  Everything else is free, though.  You won't get a bill for the ShieldsUP! service or the work he's doing on SQRL or Perfect Paper Passwords and Password Haystacks, all that stuff is there and browsable at GRC.com.



So is this show.  He's got the audio of the show and full transcripts at GRC.com.  We have audio and video at our site, TWiT.tv/sn.  But really the best way to do it is to subscribe.  Use your podcatcher, whether it's Overcast or Pocket Casts or whatever you - iTunes, whatever, Stitcher, Slacker.  Subscribe to Security Now! so you can have every episode, all 638 or whatever it is.  It says, see, this is where he went wrong.  It says 626, but there are actually more episodes than that. 



STEVE:  Yeah.  And there was like an "a," and some we skipped, and there was a little renumbering.  I mean, you know, things have been very smooth for the last decade, but we got off to a little bit of a rocky start there.



LEO:  When Leo was running it, it wasn't so good.  Now that I have trained professionals...



STEVE:  That would be the case for my enterprise, were I going solo.  I got an email from Sue, don't forget to get the car smogged because - it's like, oh, thank goodness for Sue.  I mean, because, you know, I worried...



LEO:  She told you to get your car smogged?



STEVE:  Oh, she runs my life.  Yeah, she's wonderful.



LEO:  Wow.  That's a very handy person.  Thank you, Sue.  And thank you, Steve.  We'll be back here next Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to tune in and join us.  Look forward to that.  And I guess that's all there is to be said about this show except we'll see you next week, Steve.



STEVE:  Thanks, Leo.  See you for Episode 627 next week.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#627

DATE:		September 5, 2017

TITLE:		Sharknado

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-627.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  Although there are an unbelievable FIVE Sharknado movies, this will be the first and last time we use that title for a podcast!  This week we have another update on Marcus Hutchins.  We discuss the validity of WikiLeaks documents, the feasibility of rigorously proving software correctness, and the fact that nearly half a million people need to get their bodies' firmware updated.  Another controversial CIA project is exposed by WikiLeaks.  A careful analysis is done of the FCC's Title II Net Neutrality public comments.  We talk about a neat two-factor auth tracking site, the Stupid Patent of the Month, an example of a vanity top-level domain, a bit of errata, and finish up with the utterly unconscionable security mistakes made by AT&T in their line of U-Verse routers.



SHOW TEASE:  It's time for Security Now!, and it is a "SharknAT&To," this episode dedicated to a zero-day revealed by security researchers in every single U-Verse Arris modem, and it's a bad one.  Lots to talk about.  Also some interesting research which just came out by Brian Krebs on the Marcus Hutchins story.  All is not as it appears, perhaps.



We apologize in advance.  You'll notice during the show, and I hope it's not too bad, that there are audio breakups.  We do know about it.  Steve and I spent a long time trying to troubleshoot.  I think we know what it is, but we weren't able to fix it for this episode.  So my apologies for the occasional glitches and burps.  Nothing we can do about it.  We will not have this problem next week, though, I promise you.  Stay tuned.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 627, recorded Tuesday, September 5th, 2017:  Sharknado?



It's time for Security Now!, the show where we protect your privacy and security online with this cat right here, this groovy individual named Steven "Tiberius" Gibson, the man at GRC.com.



STEVE GIBSON:  With the extra ebullient Leo Laporte...



LEO:  Yes, I am.  I must have had a little something something.



STEVE:  ...as our copilot.  I think you did, Leo.



LEO:  Oh, wait a minute, I did.  What was that weird drug you sent me?  That thing that's supposed to improve your alertness?  What was that, that weird vitamin?  Wasn't PABA, but it was like it, that was...



STEVE:  Oh, I've forgotten now.



LEO:  Yeah, remember that?



STEVE:  Oh.  Hmm.



LEO:  You need some, Steve.  No, it's so funny because you said earlier that I was kind of peppy on Security Now!, I mean on MacBreak Weekly.



STEVE:  Oh, my goodness.  Obviously so.



LEO:  I didn't notice it.  But it must be that thing.  I only took 325 milligrams.  Was it lysergic acid diethylamide?



STEVE:  No, that's not it.



LEO:  Not it.  It's probably...



STEVE:  I think I did give you a norepinephrine precursor; didn't I.



LEO:  You did.



STEVE:  Yes.



LEO:  Completely legal in most states, I might add.  Wow.  You know, I forgot I'd taken it.  And that explains the peppiness.



STEVE:  Okay, yes.  Back off from that, Leo.  I think maybe...



LEO:  I took a little dose, Steve.  Okay.



STEVE:  Okay.  So I have to apologize.  I think this is the first podcast I've ever felt I had to apologize for the title.  And although there are an unbelievable five Sharknado movies, this will be the first and last time we ever use that title for the podcast.



LEO:  Well, you'd better have a good reason for it.



STEVE:  Oh, well.  Whoever named this disclosure, it's like Honey Monkeys or Heartbleed.  In this case it's Sharknado.  It's the name of the set of vulnerabilities that were found in AT&T U-Verse modems.  And they are, I mean, I was going to say heart-stopping, but that's a different story we have today.  It's unbelievable.  So the guys who wrote this up disclosed it immediately when they realized how bad this was, just to deliberately put pressure on AT&T because it's unconscionable what they have done.



LEO:  I know what you're talking about.  This is the problem with the Arris modems.



STEVE:  Yes.



LEO:  Yes.



STEVE:  It's the Arris modems.  And then AT&T OEMs them from Arris and then alters their own firmware.  So there's some question as to - because it looks like it may be both.  But when our listeners - I'm going to put it at the end of the show.  We'll get to it.  When our techie listeners understand how bad this is, in some cases in every single one of the AT&T U-Verse systems, for example, allows anyone on the Internet anywhere to access the ports on any of the machines behind the router.  So essentially not requiring port mapping, but just kind of going straight through.  It's just incredible.  So I thought, okay, we have to further shine light on this.  And it just is another perfect example of, boy, you know, doing it wrong.



So this is Podcast 627.  We're going to talk about that.  We've got another update, an interesting one, this time from Brian Krebs, just went live an hour or two ago.  And so but it's an interesting take.  Brian spent the last three weeks researching Marcus's background and drew, coincidentally, but he did the work, we were just guessing, exactly the same conclusion we had for the last few weeks about sort of the back story here.  We're going to discuss the validity of WikiLeaks documents; the feasibility of rigorously approving software correctness.  Nearly half a million people are getting a part of their body's firmware updated.



LEO:  Ha ha, he said "firmware," ha ha.  I'm sorry.



STEVE:  Calm down, Leo.



LEO:  I'm sorry.



STEVE:  Don't ever take any more of that than you did.



LEO:  It's DLPA.



STEVE:  Yup, DLPA.  Yup, that's exactly right.



LEO:  I've got to get a new source for this stuff because it's awesome.



STEVE:  DL-Phenylalanine is two forms of an amino acid that are precursors, yup.



LEO:  Completely harmless.  But better than coffee.



STEVE:  It's actually is very good for you.



LEO:  Yes.



STEVE:  Anyway, another controversial CIA project exposed by WikiLeaks.  A careful analysis of the FCC Title II Net Neutrality public comments comments.



LEO:  Oy.



STEVE:  And it's really funny, too, because this was commissioned by the ISPs, and they did not like the results.



LEO:  No.



STEVE:  A neat two-factor authentication tracking site.  The Stupid Patent of the Month, which is actually the EFF's title for this, not even ours, being snarky.  They were.  And, boy, you won't even believe what that Patent Office has done this time.  An example of a - we were talking about vanity top-level domains, like I could get .grc?  And one just surfaced, and I thought, oh, that's just a perfect coincidence.  A tiny bit of errata and a little bit of miscellany.  And then we're going to discuss these utterly unconscionable security mistakes made by AT&T in their line of U-Verse modems.



LEO:  And I have an apology to make because I know our audio, if you're listening now, and you hear it, is a little choppy.  And we have no idea what's going on.  We've been working on this, and we...



STEVE:  For 40 minutes.



LEO:  Well, not just for 40 minutes, for months, because we sent you a new audio interface and...



STEVE:  Right.



LEO:  ...we're trying to figure it out.  But we'll keep working on it.  I apologize.  And we've decided that the content takes precedence over the quality in this case, and we wanted to get you that information.  If you can't stand it, there will be handwritten transcriptions available at GRC.com in a week or so, or a few days, anyway.  Do you want to take a break?  I know, you've got to find some DLPA, man.  This stuff is great.  It's so funny because I did not - I forgot I took it this morning.  And until you said "You seem unusually ebullient..."



STEVE:  Have other people noticed it?  Has anyone...



LEO:  No.  Have you noticed it?



STAFF:  You take it before you go to bed?



LEO:  Well, I don't like to take it before I go to bed.  It gives you nice dreams, elaborate dreams, but I like the energy that it gives me.  Am I supposed to take it before I go to bed or before a show?



STEVE:  No, before a show.



LEO:  That's what I say.  Whoa.  I'm going to calm down now.



STEVE:  So check out this Picture of the Week.  It took me a while to figure out what it was I was seeing.  This is the most embattled ATM I have ever seen.  Oh, I mean...



LEO:  Is this in the Caribbean?  Where is this?



STEVE:  No, I can't - you know, the link did have the address, and I don't remember now where it was.



LEO:  Maybe it's Venice Beach or somewhere.  It's got...



STEVE:  Oh, my lord.



LEO:  It's got, like, boards all around it.  And then there's graffiti.



STEVE:  And that big red bar across is obviously to, like, keeping something from backing into it, or to keep you from ripping the ATM out and taking it away with you.  I mean, it just - I just, like, first of all, one thought is why even bother having an ATM there?  I mean, do you really have to have an ATM there?



LEO:  Who would use it?  Right?  Don't you...



STEVE:  Exactly.  It looks like you'd get a disease just walking by.



LEO:  It's like the old phone booths.  That's what phone booths ended up looking like.  Holy cow.



STEVE:  Oh, yikes.  Anyway, I just got a kick.  It was like, here is technology trying to struggle against all odds.  And, you know, at this point skimming, you know, sticking your card in, having a skimmer is the last of your worries.  I mean, I just - I wouldn't even get near that thing.  It looks like it would bite you.



So Brian Krebs, bless his heart, spent the last three weeks digging deep.  And this is, you know, the moment I saw this I thought, oh, this is perfect for Brian.  This is what he does.  As we know, he's interested in going back and really doing, like, the dark corners of the 'Net, taking the time to create identities that are not linked to him so that he can participate in the dark underbelly chatrooms and so forth.  And then he brings his results to us.  So I was just, as I said at the top of the show, just made aware of his most recent posting, which I've linked to here for anyone who wants to dig in.  And I haven't had a chance.  I just, like, I understood what it was he said.  So I've not been able to dig in.  I will go through it before next week to see if there's anything more worth sharing.  But to quote just the headline summary, here's what he said.



He said:  "At first" - I'm sorry.  I forgot.  We're talking about Marcus Hutchins, who as we know was arrested by law enforcement as he was trying after DEF CON to go back to his home in the U.K., and he was detained.  He's now, speaking of Venice Beach, Leo, he's now in Venice Beach with two attorneys, waiting for trial, I think next month, in October.



So Brian said:  "At first, I did not believe the charges against Hutchins would hold up under scrutiny.  But as I began to dig deeper into the history tied to dozens of hacker forum pseudonyms, email addresses, and domains he apparently used over the last decade, a very different picture began to emerge.  In this post," writes Brian, "I will attempt to describe and illustrate more than three weeks' worth of connecting the dots from what appear to be Hutchins' earliest hacker forum accounts to his real-life identity.  The clues suggest that Hutchins began developing and selling malware in his mid-teens  only to later develop a change of heart and earnestly endeavor to leave that part of his life squarely in the rearview mirror."



LEO:  That kind of makes sense, yeah.



STEVE:  It does.  It makes total sense.  So yay to Brian for doing that work.  Thank you, Brian.  And of course it exactly corresponds with our own, what was just gut feeling.  Like, well, you know, all the evidence was he wasn't happy that his recent work was being abused, but there had to be something more.  And I'm just hoping - I don't know what - whose laws apply where, if there are exceptions for underage, that is, non-adults.  But maybe that will give him some shield.  If he's kept his act clean and been a good guy as an adult, maybe his earlier teen antics are shielded by not being at the age of consent and so forth.  So I don't know.  But anyway, I just wanted to provide Brian's really useful research feedback.  Again, thank you.



Okay.  Somebody following up on our comments last week about needing to remember - and you highlighted this for us, Leo.  We don't know if WikiLeaks document dumps, which are so damaging to law enforcement actual technology use and reputation, are real because we just don't know.  But someone sent me a link which initially looked like there was a way to prove it.  It turns out that's email being proved, not documents secreted somehow from the CIA.



So one of our listeners responded to my caution about placing possible unwarranted trust in WikiLeaks documents with a link.  Now, this is an older posting from about 10 months ago, October 2016, and by somebody with a lot of cred, Robert Graham, who's the guy at Errata Security.  And of course he's our old buddy fro the BlackICE personal IDS firewall days, in the early days of Windows firewalls.  Theirs was a different kind of product that's called BlackICE that was more of an IDS.  It was more of an intrusion detection system for a PC than just a simple firewall.  And so it wasn't rule-based, and it wasn't my kind of solution.  I just believed you should close the ports down and open them wherever you wanted.  Theirs was sort of more automatic, but a useful product.



Anyway, WikiLeaks documents are not leaked emails.  So Robert's blog posting is only relevant to email, but due to a technology originally developed as an antispam solution, as a direct consequence incorporate cryptographic signatures that, as cryptographic signatures will, can be used to authenticate a document and prove with cryptographic certainty that it has not been altered.  So we can validate WikiLeaks emails, or some WikiLeaks emails.



Okay.  So before I go any further, I want to remind everyone, because this is a political case, that this is not a political podcast, and I work scrupulously to keep us from falling into that controversy.  I don't know what Robert's politics are, even if he is political, but this isn't about that, though it touches on something from our last election cycle.



Robert writes in his posting last October, October 2016, he said:  "Recently, WikiLeaks has released emails from Democrats.  Many have repeatedly claimed that some of these emails are fake or have been modified, that there's no way to validate each and every one of them as being true."  He writes, "Actually, there is, using a mechanism called DKIM."  That's DomainKeys Identified Mail.  "DKIM is a system," he writes, "designed to stop spam.  It works by verifying the sender of the email.  Moreover, as a side effect, it verifies that the email has not been altered."



He goes on:  "Hillary's team uses HillaryClinton.com, which is DKIM-enabled.  Thus," he writes, "we can verify whether some of these emails are authentic."  Now, there's a caveat there that I'll get to in a second.  He said:  "Recently, in response to a leaked email suggesting Donna Brazile gave Hillary's team early access to debate questions, she defended herself by suggesting the email had been 'doctored' or 'falsified.'"  And he writes:  "That's not true.  We can use DKIM to verify it."



Then he provides a link to the specific email and notes:  "It's not a smoking gun; but, at the same time, it both claims they got questions in advance while having a question in advance."  And then he said:  "Trump gets hung on similar chains of evidence, so it's not something we can easily ignore."  I don't understand what he meant by that, but regardless.



Anyway, this post isn't about the controversy - thank you, Robert - but the fact that we can validate email.  When an email server sends a message, that is, a DKIM-enabled email server, it'll include an invisible header.  They aren't, you know, email headers aren't specifically hidden.  Most email programs allow you to view them.  It's just the way, you know, there's a bunch of debris that confuses people, so we don't want to see them.



And then he goes on to explain that normally this email validation is server to server, that is, from SMTP server to [dropout], meaning that when a sender of email sends their email from their POP or IMAP client to an SMTP server, then it's got a private key.  It takes a hash of the entire email - subject line, time and date, body of the email.  It hashes that, and then it digitally signs the hash to create a signature which cannot be forged.  Then [dropout] to its destination.  The receiving SMTP server receives it, sees that it has a DKIM header, meaning that this particular piece of email is verifiable, it is validatable affirmatively.



So if it doesn't already have the private key in its cache of candidate recipients - and, you know, there are so many domains in the world it probably doesn't - it does a DNS lookup.  And this is one of the reasons, you know, our listeners know that I'm so excited about the idea of DNS being this generically useful database, an Internet-wide cacheable index.  And thus when DNSSEC finally happens, to be able to have that robustly secured is going to create an incredible asset for the world.



So in this case there's a text record which is an additional record for a domain.  So, for example, HillaryClinton.com had a text record which allowed that domain to publish its public key.  So it signs outgoing email with its private key, which it keeps secret and in control.  But then any recipient can go, oh, let's make sure this really did come from HillaryClinton.com, and not a single character was altered.  In that case, the receiving SMTP server performs a special DNS query, not asking for an IP address, but asking for the text records which are just textual that are associated with that domain name.  That allows it to get a base64 encoded version of the binary public key.  It decodes that into binary, then it uses that public key to verify the signature.  So all of that happened.



Well, it turns out there's an extension which you can get.  He uses Thunderbird.  So he said:  "How do you verify this is true?  There are a zillion ways with various DKIM verifiers."  And he writes:  "I use the popular Thunderbird email reader from the Mozilla Firefox team.  They have an add-on designed specifically to verify DKIM."  Meaning that in this case your email client itself does that same verification.  It queries the remote DNS, gets the public key, and shows you as you're looking at email whether it's verified or not.  So it's kind of a cool add-on for an email reader.  I mean, it doesn't get in your way at all.  It'll let you know if this is spam or not.  If it's DKIM-signed, the chances are way lower.  At least it's not spoofed.  You could have a DKIM-signed spam email, but it can't also be a spoofed domain.  So once you decide you don't want any more email from that server, you can put it on a blacklist, and there's no way it could jump around.



So anyway, he says:  "Downloading the raw email from WikiLeaks and opening in Thunderbird with the add-on, I get the following verification that the email is valid."  And he provides us with a screenshot.  He says:  "Specifically, it validates that the HillaryClinton.com sent precisely this content, with this subject, on that date."



So that's all true.  The problem is this evidence appears to be very damning, but this is - we have here a security and technology podcast, so we always ask the question, how sure are we?  Cryptographically, we're absolutely as positive as we could be that the signature is correct.  However, as we've often discussed in the case of HTTPS connections with TLS certificates, and this applies [dropout] with DKIM, the certificates, that is, the private keys and the security of the system absolutely relies on the assumption that the signing server is uncompromised because it's the signing server that has the private key that allows it to perform this authentication or to provide the authenticatable content.



But the whole point of this, in this case the Hillary email debacle, was that their email server was compromised.  And as we know, if in addition to the 3,000-some emails that were reportedly obtained, if that private DKIM key was also obtained, then we're right back where we started, with a somewhat harder to substantiate claim of document doctoring, but still a way that it could have happened.  So this stuff, we get down in the weeds with the technology.  But we've never really talked about DKIM in the entire 12-plus years of the podcast, so [dropout].



And this doesn't, of course, speak to WikiLeaks documents and their forgeability or not, only to email, and only to email that is DKIM-signed, and only in that case where you can reasonably assume that the DKIM signature could not have been spoofed.  But unfortunately, in this case, where a server compromise was part of the story, you really can't make that assumption.  So [dropout] we did before in terms of secure crypto-level assurance, which we would like to be able to have in this case, but the facts really don't support that.  But still, an interesting twist.



We've talked some time ago about this questionable deterministic random bit generator which used the dual elliptic curve technology which the NIST had standardized on against all reason.  There are four that they have in their kit.  And this is - and I don't have the - oh, yes.  It's NIST document 800-90A.  [Dropout] hash-based version, an HMAC-based version, meaning that it's a keyed hash.  There's a counter running through a cipher based version.  And then of course the infamous dual elliptic curve DRBG, which is based on the elliptic curve cryptography.  And remember that the problem with it that surfaced a couple years ago was that there was no - the NSA provided the curve parameters with no substantiation for where they came from.  They just said, "Here are these magic numbers."



Well, the problem is those magic numbers could have been hiding a backdoor, which could have allowed someone who had some of the random numbers that were being produced to get the other ones.  And then, to make matters worse, it was the slowest of the four.  There's, like, four different algorithms, and the other three are obviously strong.  But, for example, RSA chose the slowest one, that one.  So it was like, again, we had no proof of anything, but it just seemed like a compounding of suspicious circumstances.



So all of this by way of the fact that Matt Green and a team went to the trouble of absolutely verifying everything about one of these.  They took the HMAC-DRBG and have published a paper out of Princeton, verified correctness and security of mbedTLS, that's M-B-E-D-T-L-S, which is an embedded TLS crypto that happens to use this HMAC deterministic random bit generator to generate its pseudorandom numbers.



Okay.  So I'm going to explain this a little bit, but I want to talk more - what I really wanted to do was to use this as a jumping off point to talk about software verification.  So they write:  "We have formalized the functional specification of HMAC-DRBG, and we have proved its cryptographic security, that its output is pseudorandom" - which sounds like not a big deal, but that's a high bar.  And when these guys say "prove," they're not saying "we're of the opinion," or "five of us all looked at the code."  They're saying, no, "We have proved it," they said, "using a hybrid game-based proof.  We have also proved that the mbedTLS implementation written in C correctly implements this functional specification."  And of course that's exactly what we want.  We want proof that both the theory and the practice, which is to say in this case the implementation, are both verifiable, both solid.



So they continue:  "That proof composes with an existing C compiler correctness proof to guarantee, end-to-end, that the compiler's output machine language program gives strong pseudorandomness."  They write:  "All proofs - hybrid games, C program verification, compiler, and their composition - are machine-checked with the Coq proof assistant."  So there's some automated assistance to help unwind any spaghetti in a code and absolutely nail down what's going on.  They said:  "Our proofs are modular.  The hybrid game proof holds on any implementation of HMAC-DRBG that satisfies our functional specification.  Therefore, our functional specification can serve as a high-assurance reference."



And so what I wanted to bring to this is that this is the future.  We're not there yet.  But computer programs are just math.  They're not a crapshoot.  They are a deterministic processor.  Assuming there are no bugs in the processor, so we have had them in the past, so we have to make that caveat.  But if the processor properly follows the instructions, each instruction does exactly what it's supposed to.  And then there are other problems like cache conflicts or instructions interfering with each other.  So it gets complicated quickly.



But computer programs are not flowers and trees and fields of stars.  They are absolutely and utterly subject to pure and perfect rigorous analysis and verification.  The problem is it's difficult.  It's unclear how fast we're going to see progress in the area.  But this is a larger chunk of verification than I remember seeing recently.  So it feels like we're getting better at it.  I don't know if it'll happen in our lifetimes.  On the other hand, we're seeing exponential growth in AI capability, so that might be a factor.  Ask Watson if the program is provably correct.



And this is why, for me, as a writer of code myself, always chafed at the lay statement which unfortunately is true, that all software contains bugs, which we've all heard that said before.  We may have said it ourselves.  I would back off from that, though, and say that trivial software can be easily bug-free.  However, more complex software, for example, like the U.S. Shuttle's flight guidance and navigation computer, which needed to be bug-free, can be bug-free, if it's sufficiently important to then pour incredibly extensive resources into it in order to make it so.



So my point is that it's the nature of software to get rapidly insanely complex as it grows in size.  The interactions within a program just become crazy.  So very quickly a trivial small program, like a loop that adds numbers.  Okay, I want to compute a Fibonacci.  Okay, that's three lines of code.  It has no bugs.  I mean, it's so simple, it's clearly perfect.  It's provably correct.  But you start adding bells and whistles to it, and things go out of control quickly.  And that's the point.  The complexity just exponentiates.  It goes insane, and so out of our ability to perfectly know that it was exactly correctly implemented.



So I think what's destined to happen - and again, I have no sense for timeline, but we need it to happen.  We are getting the capability for it to happen, and so I think it will - is that what will happen over time as the computing resources increase, and they're clearly going in that direction, is that the cost of rigorous software correctness proofs and verification will fall dramatically so that we're no longer guessing that it's okay because it stops not being okay, and so we go, whew, ship it.  Instead, we tell Watson, okay, here we go, does this do what we think it's going to do?



And, I mean, this is not a small problem to solve, which is why it has so far been virtually intractable, and it is so insanely expensive because of the fact that we just don't know how to do it yet, that none of our software, I mean, virtually none of it has been proven to be correct.  We kind of limp along and get updates and patches and say, well, okay, because that's a lot cheaper.  But imagine a day where we have AI that is able to do what we cannot do, which is to find the bugs that are there, that are not manifest, and say, whoops, you meant to do this over here, didn't you.  Oh, yeah, thank you for telling me now.



So anyway, so Matt and his group did that.  They wrestled a substantial piece of work to the ground, producing mathematical certainty that it is correct, which is very cool.  And you and I, Leo, may not live to see the day.  But in the future you could imagine that, because it is just math, because these are deterministic systems, they could be, given sufficient focus, made perfect, which would be nice.



LEO:  Wow.  Yes, for those of you watching at home, Steve has lost the caterpillar that used to haunt his face.  We commented on it last week, but just in case people didn't tune in last week, or didn't watch the video last week, he knows his moustache has gone missing.



STEVE:  Yes.  And it requires maintenance to keep it so.



LEO:  Yeah.  Hey, I got a Dollar Shave Club for you.  I'll send it out.  It's right here somewhere.  Yup.



STEVE:  So when I saw this title I thought, wait a minute, how do you do that?  The FDA is recalling 465,000 pacemakers.



LEO:  Uh-oh.



STEVE:  But that's the definition - that's an embedded computer.



LEO:  Yeah, literally.  In your chest cavity.



STEVE:  And so this story will make your chest thump.  The United States FDA, the Food and Drug Administration, is recalling 465,000 pacemakers to which attackers - get this -- attackers can gain unauthorized access - yes, to your pacemaker - to issue commands, to change settings, and maliciously disrupt its function.



LEO:  Didn't we talk about that hack some months ago?  It feels like we talked about that hack.



STEVE:  It's probably coming back around again, yeah.  Oh, in fact, this is not the first time that Abbott Labs has had a problem.  You're exactly right, Leo.  There was a related problem that they had a few months ago, so good memory.  According to the FDA, the recalls of affected pacemakers are tied to research by MedSec Holdings.  I guess they were the researchers that originally brought the St. Jude medical equipment flaws to light about a year ago, and Abbott Labs acquired St. Jude Medical in January.



Okay.  So I snipped a bunch of nonsense out of the reporting because the details of the exploits are chilling.  And I didn't even realize, we've talked about US-CERT, the Cyber Emergency Response Team?  Turns out there's an ICS-CERT which is the Industrial Control System Cyber Emergency.  And apparently that's the category for pacemakers.  You've got an industrial control system in your chest, implanted or embedded.  So the ICS-CERT manages industrial control system vulnerabilities, catalogs them and tracks them and so forth.  They cite three vulnerabilities in their advisory regarding Abbott Laboratory's pacemakers, which were manufactured prior to August 2017, meaning last month.  So unless your scar is really fresh, you may have one of these puppies.



The highest rated of the three vulnerabilities is [dropout] authentic - it's hard to even say this - the pacemaker's authentication algorithm, the authentication key and timestamp can be compromised or bypassed.  So there's an authentication bypass on your pacemaker which could allow a nearby attacker, because this is RF, so it's radio, so someone has to get in your proximity, to issue unauthorized commands to the pacemaker over the air, wirelessly.  An additional bug could significantly reduce battery life of the pacemaker.  You would like your implanted pacemaker's battery to get the full benefit of its life so that you're not having to be reopened again.



The CERT advisory said the pacemakers do not restrict or limit the number of correctly formatted RF wakeup commands that can be received, which may allow a nearby attacker to repeatedly send commands to reduce the pacemaker's battery life.  So in other words, as we know, receiving radio is a low power task compared to sending it.  And this is a bidirectional interface between the outside world and the pacemaker in a person's chest.  So the pacemaker is either always or periodically receiving, but that is very low power, doesn't take much to receive.  But when it's explicitly awoken, then that fires up the transmitter, and transmitting energy is far more power consuming than receiving it.



So the second half, so [dropout] problem with the firmware is that there is no limit on an attacker's ability to drain the battery of an affected pacemaker by keeping its transmission radio running all the time.  So you can imagine an attack scenario where an office worker who's generally going to be in a certain location is having his pacemaker drained without his knowledge, and certainly that's not what he wants.



And then, finally, the third flaw is related to the fact that the devices - again, really?  The devices transmit unencrypted patient information via RF communications to programmers and home monitoring units.  So they didn't bother to run a cipher on this.  It's just plaintext coming out, which is problematic, they write, oh, because the pacemakers store patient data in the clear and transmit it in the clear.  So that just feels like sloppiness.  I mean, you could say, well, some things are a mistake.



But as we've discussed often, it's important to separate policy from error, and it's hard to defend a person's pacemaker transmitting plaintext to any receiver within range, which apparently these things have some range.  If you've got a home monitoring unit, maybe you can walk around while it's monitoring you.  So anyway, they have been recalled, 465,000 pacemakers.  Now, the good news is they are firmware-upgradeable by that same radio link.  So we're not talking about any more open hearts or open cavity surgery.  It's just, okay, fine, we need to see you in the doctor's office to update your body's firmware.



"The vulnerabilities could be exploited," the advisory writes, "via an adjacent network.  Exploitability is dependent on an attacker being sufficiently close to the target pacemaker as to allow RF communications."  I'm not talking - I'm sure it's not Ethernet commercial WiFi.  You know, it's going to be some wacky medical band RF.  So it's not like your neighbor is going to receive it on their television set or something.  It's definitely in its own world of RF spectrum.  But if anybody were targeting someone, that would not represent any kind of a problem because we also have now very inexpensive SDRs, Software-Defined Radios, that you just set to whatever you want.



So anyway, mitigation of these problems require patients to visit their doctor for a short-range wireless update.  Abbott warns...



LEO:  It's cool that they can do that, though, I have to say.



STEVE:  Yes, yes.



LEO:  And it's a relief, too.



STEVE:  And I'm sure that this is all super regulated so the devices are serialized, and I'm sure there's a database so that they're able to get notices to the - I was going to say the "end user" - but to their doctor, you know, and follow up on this to bring everybody back in and get it taken care of.  But yes, Leo, I agree.  The idea that you can - I'm sure they just put something on your chest, and it has a dialogue back and forth.  Abbott did warn that the firmware updates should be approached with caution, that is, they really don't want anyone messing with an already in-place pacemaker.  They said:  "Like any software update" - well, we don't want to think that this is like any software update, but that's what they said - "firmware updates can cause devices to malfunction."  Gulp.



LEO:  That wouldn't be good.



STEVE:  Gulp.  "A botched update could result in a loss of settings to complete loss of device functionality."



LEO:  Uh-oh.



STEVE:  So, now, the good news is you will be in your doctor's office.  And not all pacemakers are, like, replacing a bad sinus node which has stopped functioning.  Some of them are just like demand pacers, where if you had a little bit of a systole, it notices it and goes whoops and gives you a pacing spike.  So it's not like stopping would immediately kill you, but it would be good that you've got your doctor standing by with some paddles.  So, interesting.



LEO:  Paddles.  I hope that's not required.  We'd better book some surgery this minute, yeah.  Wow.



STEVE:  Okay.  So here we have a case of another WikiLeaks dump where, boy, the press got this wrong.  The headline is "CIA Caught Planting Malicious Software in Windows."  Okay.  But, no, that's not what happened.  And the codename, as all these things have fun codenames, is AngelFire.  So the truth is that a team of hackers at the U.S. CIA, our Central Intelligence Agency, allegedly used - again, from WikiLeaks, so maybe - used a Windows hacking tool against its targets to gain persistent remote access.



LEO:  So what's the surprise here?  I mean, of course they did.



STEVE:  Exactly.  Exactly.



LEO:  What would you have them do?



STEVE:  And I'm glad they can do that.  I'm glad we have...



LEO:  Yes, break into the house?  Put a glass against the wall?



STEVE:  Yeah.  So as part of its Vault 7 leaks, WikiLeaks last Thursday revealed details of a new implant developed by the CIA named AngelFire to target computers running Windows.  The AngelFire framework implants a persistent backdoor on target Windows computers by modifying their partition boot sector.  In other words, it's a high-end, well-designed rootkit, specifically for intelligence and law enforcement data gathering.  It's got five components.  I mean, it's a beautiful piece of work.



AngelFire is the umbrella.  There is something called Solartime, which is the thing that modifies the partition boot sector to load and execute the Wolfcreek, which is the kernel code, every time at boot up.  So that makes it persistent over the long term.  Wolfcreek, in turn, is a self-loading driver that runs in the kernel, which loads other drivers and user-mode apps.  Keystone is the component that utilizes DLL injection technology to execute the "malicious" user applications.  And again, "malicious" should be in quotes because it's intended for this purpose.  They execute the purpose-specific applications directly into the system memory without needing to drop them in the file system, so there's no sign of them.



There's something called BadMFS which is a covert file system which attempts to install itself in non-partitioned space available on the target computer and stores all drivers and implants.  And we've seen rootkits do this.  I remember talking a long time ago that, as a consequence of the weird history of hard drives, there is this cylinder alignment.  Cylinders don't exist anymore, like logically, the way we see them.  We used to have cylinders, heads, and sectors.  And partitions used to have to end.  They didn't have to start on a cylinder boundary, but they had to end on one, meaning that if the drive - and a cylinder, especially when you have lots of heads or virtual heads and sectors, a single cylinder could be - I used to know this by heart, but it's number of heads times number of sectors times 512.  That many bytes, a big chunk of space.



So the idea would be a hard drive itself, which is now actually just a linear array of bytes, it's not going to be an even multiple of the OS's arbitrary cylinder size.  So there will be slack at the end.  And in fact in 6.0 of SpinRite I understand that, and I specifically test all of the slack space and the intercylinder space by rounding up and down appropriately and then clipping to size so that SpinRite doesn't miss anything.  But the OS does.  And so these guys are using something that will be completely invisible.  It's on the drive, but you can't see it.  You can't get to it.



And then the last bit - those were the first four.  The last piece is the Windows Transitory File System, which is a new method of installing AngelFire that is a method these guys came up with which allows the CIA operator to create transient files for specific tasks like adding/removing files to and from the AngelFire system.  So there's a user manual for it that WikiLeaks downloaded.  The 32-bit version of the implant works against Windows XP and Windows 7, while the 64-bit implant can run on Server 2008 R2 and Windows 7.



So as so many of these things are, it feels a little bit dated, okay, so like 7, but not 8.1 and 10.  But, you know, it's very likely that they adapted AngelFire 2 or 3 or something if they wanted to use this.  But once again we've seen overblown and clickbait coverage of this in the press.  For example, I read one report which opened with, quote, get this, I mean, I was worried when I read this because this was the first thing I encountered.  I thought, what?  It says:  "All Windows machines have been infiltrated by the CIA under a project..."



LEO:  Well, thank goodness.  I think maybe they mean all Windows versions or - I don't know.  That's weird.



STEVE:  Yeah, but the headline [dropout]:  "All Windows machines have been infiltrated."  It's like, no, they haven't.  And then it's "allowing the U.S. government to load malicious programs onto a person's computer without their knowledge."  Yes, a foreign adversary's computer.  I mean, no one suggests that this should ever be perpetrated against U.S. citizens in contravention of U.S. law and the U.S. Constitution.  And no one is suggesting that they ever did.  This is very likely part of their toolkit for remote intelligence gathering.  I mean, and we don't have any evidence one way or the other.  But that's the assumption.  And so I'm glad they've got power tools, you know, as long as they point them somewhere else.  And I presume they do.



Okay.  So in a little bit of turnabout is fair play, a consortium of ISPs commissioned an independent third party, because they wanted clean hands, to analyze all of the submissions which were made on the topic of Net Neutrality and Ajit Pai's threatened or planned anti-Title II plan.  Ars Technica has a nice report about this, falling under the category of "be careful what you ask for."



Ars wrote that:  "A study funded by Internet service providers has found something that Internet service providers really don't like.  The overwhelming majority of people who wrote unique comments to the Federal Communications Commission want the FCC to keep its current Net Neutrality rules and classification of ISPs as common carriers under Title II of the U.S. Communications Act, according to the study which was just released."  I've got a link to the study, for anyone who's interested, in the show notes.



And get this:  98.5% of the unique Net Neutrality comments said don't change anything.  The study was conducted by a consulting firm Emprata [E-M-P-R-A-T-A] and was funded by the so-called Broadband for America coalition whose members include AT&T; CenturyLink; Charter; CTIA - The Wireless Association; Comcast; NCTA - The Internet and Television Association; the Telecommunications Industry Association (TIA).  Oh, and USTelecom.  So, you know, a lot of big players.



When Emprata analyzed all 21.8 million comments, including spam and form letters, 60% - if you just looked at them all, without discrimination, 60% were against FCC Chairman Ajit Pai's plan to repeal the Title II classification, and 39% supported the repeal.  But the numbers shifted starkly, Ars writes, in favor of keeping the Title II rules when excluding duplicates in order to analyze just unique comments written by real people.



Emprata wrote:  "There are considerably more 'personalized' comments, appearing only once in the docket, against repeal," meaning in favor of leaving everything alone.  That is, 1.52 million were "leave it alone" versus only 23,000 personalized, individual, probably real comments supporting repeal.  "Presumably," Emprata says, "these comments originated from individuals that took the time to type a personalized comment.  Although these comments represent less than 10 percent of the total, this is a notable difference."  Meaning, if you look at them all, it's a 60/40.  If you look at ones that are probably authentic, it's a 98.5.



LEO:  Yeah.  I wrote a personal long comment, as long as I could fit in there.



STEVE:  Yes.



LEO:  Saying why Net Neutrality needs to be preserved for my personal business.



STEVE:  Yeah.  So I don't know if it'll make a difference.  I just wanted to report on this interesting step because we've talked several times now about this flood of spam that, I mean, and DDoS attacks that basically just knocked it off.  They had to take it down off the 'Net for a while in order to let things cool off, and then they put it back on again.  So again, unfortunately we're in a world where, as we know, the guys with the big bucks can pay the big lobbying firms to apply strong pressure on our politicians to do what they want to have done.  So we'll keep our fingers crossed.



Leo, you're going to love this one, a fabulous site:  TwoFactorAuth.org, T-W-O-F-A-C-T-O-R-A-U-T-H dot org.  I recommend it without hesitation to our listeners:  TwoFactorAuth.org.  The URL is in the show notes.  It is a comprehensive listing.  Now, Leo, click one of those icons.  What happens is it opens the row below and gives you a breakdown of the type of authentication offered by a huge range of services and sites within the category that the icon uses.  So there's communications, there's cloud providers, there's all kinds of different categories.  But it shows you, for example, that row on the far right is what we want.  That's the software time-based solution.



LEO:  So that's Authenticator or something like it, Google Authenticator.



STEVE:  Exactly, exactly.  The one in the middle is what we're now sketchy about, and that's the SMS authentication, where they're sending it in real-time.  But this is just a cool reference for, like, I can imagine somebody really security conscious, when shopping for a service, a bank or a cloud provider or whatever, might say, well, I want one with time-based token two-factor authentication.  This lets you shop the industry for that.  And you'll see the links there.



They also provide you, in instances where a provider or a service has none, you've got links to make it easy to tweet your request that they add it to help apply pressure for second-factor authentication where it doesn't exist.  So TwoFactorAuth, spelled out, dot org.  And I commend it to our listeners.  Just take a time and scroll around.  It's well organized, and it's got GitHub behind it, and it accepts user submissions of things they don't have yet, and it's constantly being maintained.  So it's a really...



LEO:  It's a GitHub repo, so that's an interesting way of keeping it up to date.



STEVE:  Right.



LEO:  So you can submit - which means that people are continually keeping it up to date.  I was curious how old it is.  Last commit was a day ago.  So people are - and last commit was Stripe added UTF support.  So you can add a commit, if you know enough how to use GitHub, when something changes, to do it.  Which is great.  Which is really great, yeah.  And in fact you could install your own local copy of it, if you wanted.  I'm not sure why you'd want to, but because it's GitHub you could do that.



STEVE:  Nice. 



LEO:  You could run it locally.



STEVE:  Well, yeah.  I mean, maybe - you're right.  I guess you typically wouldn't want this offline because you're inherently - it's an online process.  But, yeah, to make a snapshot, that'd be cool.



LEO:  I'm seeing more and more of this kind of collaborative stuff.  Instead of using a wiki or some other solution, using GitHub and git commands to keep it up to date.  It's kind of a cool way to do it.



STEVE:  It's a cool application, yeah.  So the EFF labeled this in their posting, as in their URL, the Stupid Patent of the Month.



LEO:  Oh, boy.



STEVE:  And, Leo, JPMorgan got a patent, I'm not kidding you [dropout], just make sure you're centered, on interapp permissions.



LEO:  Oh.  I'm glad they invented that finally.



STEVE:  Oh, god.  Somebody had to, Leo.  It was just staring us in the face.  Why didn't anyone think of that before?



LEO:  A system and method for communication among mobile applications.



STEVE:  Groundbreaking.  Groundbreaking.



LEO:  Groundbreaking.



STEVE:  Shocking.  So the EFF writes:  "We have often criticized the Patent Office for issuing broad software patents that cover obvious processes.  Instead of promoting innovation in software, the patent system places," they write, "landmines for developers who wish to use basic and fundamental [and, by the way, old] tools."



LEO:  Well, as I remember, this is how a modem handshake works.  They've patented [mimicking modem sounds].



STEVE:  Yes, the sound of geese mating.  "This month's stupid patent," they write, "which covers user permissions for mobile applications, is a classic example.  On August 29," so last Thursday, "the U.S. Patent Office issued Patent 9,747,468" - which they now refer to as the "468 patent" - "to JPMorgan Chase Bank," the geniuses there who came up with this patent "titled 'System and Method for Communication Among Mobile Applications.'"  It's a breakthrough, Leo.  Who ever heard of mobile applications communicating with each other?



"The patent covers the simple idea," they write, "of a user giving a mobile application permission to communicate with another application.  This idea was obvious," they write, "when JPMorgan applied for the patent in June of 2013."  So only four years ago.  How old is the iPhone?  It had the little thing that popped up and said, you know, do you want this app to have access to your photos?



LEO:  Right, right.



STEVE:  "Even worse," they write, "it had already been implemented by numerous mobile applications."  And built into iOS.  "The Patent Office" - I'm surprised that Apple didn't get a patent on it, but anyway.  They probably knew they didn't invent it, either.  "The Patent Office handed out a broad software monopoly, while ignoring both common sense and the real world."  They provide in this the full text...



[Leo playing modem sounds in background]



STEVE:  Yup.  They provide the full text of Claim 1 of the 468 patent.  There are just five clauses, so I'll read them:  "A method for a first" - and this is like legal speak patent talk - "A method for a first mobile application and a second mobile application on a mobile device to share information, comprising the first mobile application executed by a computer processor on a mobile device determining that the second mobile application is present on the mobile device; receiving, from a user, permission for the first mobile application to access data from the second mobile application; the first mobile application executed by the computer processor requesting data from the second mobile application; and, finally, the first mobile application receiving the requested data from the second mobile application."  Again, Leo, shockingly brilliant.  It's like, wow.



LEO:  How did they think of that? 



STEVE:  That's it.  "The claim," they write, "simply covers having an app check to see if another app is on the phone, getting the users' permission to access data from the second app, then accessing the data."  Anyway, and, see, for those who - we've talked about patent trolls and the East Texas nightmare and all that, Samsung putting their logos and supporting the local sports team, trying to beg for, you know, trying to say we're not evil to this wacky judge that just immediately, you know, who knows what kind of house he's living in and where the money is coming from.  But it just seems all so slimy.  The problem is, what JPMorgan now has is a license to sue.  And you faced this with the podcast patent, Leo.



LEO:  Yup.



STEVE:  It's like, so suddenly someone can strong-arm small players who face the expense of defending themselves against a specious claim in patent court, in patent litigation.  And the way our system works, because JPMorgan Chase in this instance has the patent, you can't call this a malicious prosecution.  So you're not able to sue for damages.  They have a right to sue somebody who's infringing the patent they received legitimately from the U.S. Patent and Trademark Office.  So it's just - it's a disaster when this happens.



And so the problem is this will end up being overturned.  But a huge amount of cash is going to flow back and forth within the system to attorneys and patent attorneys and courts and dockets and everything, in order for a secondary process to kill something at great expense and time and emotional anguish on the part of the people who are targets of the patent holder, only to undo what should have never been done in the first place.  And I know that it's a rough job.  But unfortunately, patents are now being abused like this.



And what happens, of course, is that so many large companies have "patent portfolios," as they call them.  And we've seen the Apple thing.  I mean, Apple's patenting the sunrise in order to have it so that then they can have a cross-license agreement with IBM and Microsoft and Google so that they'll all basically agree to be able to not sue each other over the contents of their respective patent portfolios.  But again, the system is just broken in a way that is just so demoralizing and sad.



Okay.  And finally, before we take our last break, we talked - you and I were talking, and you surprised me last week by saying that, oh, yeah, anybody can get a top-level domain, a vanity top-level domain.  You know, I could get...



LEO:  If you've got the money, yeah.



STEVE:  If you've got the money.  Oh, and the thing that occurred to me after we talked about it, Leo, is I'll bet that's not just a one-time fee.  I'm sure you're paying every year for this thing.



LEO:  Possibly, yeah, yeah.



STEVE:  In order to maintain it.  And actually suggesting that is the fact that, believe it or not, the .montblanc domain...



LEO:  Wow.



STEVE:  Montblanc had their own TLD.  The IANA had a - I have a link to it in the page.  Essentially somebody commenting on it said:  "We rarely see entire top-level domains killed."  Because of course typically you've got so many second-level machines in there.  Like, you know, .com is never going to go away.  It would kill the world.  So Montblanc, someone thought, oh, that'll be fun.  Let's grab our own top-level domain.  It was removed last Friday.  So not only did they apparently no longer want it, but they must have no longer decided it was worth the money that they were having to pay in order to hold it.  So I just - this was just a coincidence that this came along.  I thought that was kind of cool.



LEO:  I wonder if it's the Montblanc pen company?  Must be. 



STEVE:  That's immediately where my mind went.



LEO:  Yeah.  Unless it's Switzerland.



STEVE:  So you can't go there now because it - yes.



LEO:  Maybe Switzerland registered it.



STEVE:  Yow.  Last break.



LEO:  Nope.  There's no more ads.  We've run out of ads.



STEVE:  I forgot.  You and I were so...



LEO:  We didn't talk about it, yeah, no.



STEVE:  ...tied up with the quality of the connection.  Okay, cool.



LEO:  I know, I'm sorry to disappoint everybody, but there's no ad, sorry. 



STEVE:  You can't get up and pee yet.  You've got to keep listening.



LEO:  Dammit.



STEVE:  Okay.  So I can't even tell you.  Every week there's a most tweeted thing.  This time it was me screwing up "The Wrath of Khan" story, of all things.  I had said that when Khan was approaching with no contact and radio silence, and Kirk was sitting there with his hand on chin thinking, huh, this is mighty peculiar, I had said that Spock had told Kirk.  Well, the first time I saw a comment, it's like, "Oh, of course I knew that."  But, you know, one of those.



It was Kirstie Alley playing Saavik, Lt. Saavik.  And being junior and very Vulcan, she of course was going by the book.  So the fact that Kirk had not raised shields, she was telling him.  And then Spock, who's been around the block a few times and on many adventures, he sort of backed Lt. Saavik down, saying "The Captain is well aware of the regulations."  So of course after getting blasted by Khan, Kirk said, "You go right on quoting regulations to me."  So anyway, thank you, everybody, for the correction.  It was the...



LEO:  How could you forget Kirstie Alley?  My god.



STEVE:  She was a hot little Vulcan, too.  That was - well, anyway.  So thank you.  And then I loved the second part.  Someone else noted that there was another connection between that episode and this podcast, which was that at one point, and it's been so many years since I've seen it, I don't remember the sequence now, but all Federation starships have an override code.  And every Starship knows every other Starship's override code.  In other words, the ship that Khan was on was using the default password, which was never changed.  And as a consequence, the Enterprise was able to log into Khan's ship and bring his shields down and then return the favor of blasting him into space, like he was already there, so not where he got blasted to.



But anyway, I got a kick out of, like, yes.  And in all fairness to Khan, he wasn't an Enterprise officer, I mean a Federation officer.  He'd been on some dirtball for a long time and was pretty upset about that.  So he wasn't sure how his ship worked.



LEO:  And, by the way, we have verified with the ICANN wiki that .montblanc did belong to the pen company.



STEVE:  Ah.  Now, that's got to be the definition of a vanity domain.



LEO:  It's also a long vanity domain.  Seven letters is significant.



STEVE:  Yeah, exactly.  And I have news, Leo.  The guy who did the really nice artwork for us where Riker and Jean-Luc's faces were removed and ours were put in their place, he has requested a photo of me with no moustache so that he can update...



LEO:  Oh, very important.



STEVE:  So that it's not looking too dated.  So thank you very much.



LEO:  Very important. 



STEVE:  Okay.  I got a nice note from Steve Holden about SpinRite.  Apparently [dropout] Eurythmics fan.  And he said:  "I wanted to let you know that SpinRite was able to help me recover some 11-year-old [so that's 2006] podcasts from the Eurythmics."  And then he gives me a URL to them.  So thank you for sharing that, and I think that's cool.  And I have a couple of closing-the-loop pieces.  Yeah, exactly, there's the link. 



LEO:  They only had four episodes.



STEVE:  Ah.  They're probably not very long, either.



LEO:  No, 17 minutes, 26 minutes, 13 minutes, and 19 minutes.  That's not even half of one Security Now!.



STEVE:  So I've got one tweet saying that "I assume you would recommend LastPass authentication over others," meaning the LastPass app.  And I just wanted to note to Mark and everybody else, no.  The fact that I'm a fan of LastPass as a password manager does not mean by default, and in this case is not the case, that I like their authenticator.  You like Authy.  I'm using Google's Auth.  Although I've downloaded but haven't yet found time or the need to configure another one that I talked about last week, was probably what prompted Mark's question.



But no, LastPass, the problem I had was that there wasn't enough on the screen.  That is, I like to have a bunch of websites on the screen without having to scroll a long way.  And LastPass's UI was just taking up way too much space.  So I thought, okay, that's, you know.  And they're all the same.  As long as they're written well and they've been maintained, it just doesn't matter which one you use.



LEO:  Yeah.  My concern was, and I don't know if the LastPass Authenticator does this, but if it syncs the authentication information back to LastPass, that would obviate the whole point.  I have two-factor.  One of the points of having two-factor, which is if somebody got my LastPass, it's a single point of failure now.  If somebody got my LastPass bundle, they would, you know, they can still use the Authenticator.  Now, I don't know, I didn't check, but I just thought, you know, let's get some heterogeneity in the security here and not use two products from the same company.



STEVE:  That's an important word, yes.  Oh, and I was also talking about XON/XOFF.  And I was referring to it as, you know, like teletypes, where it was being used - and XOFF was sent by the computer to stop the paper tape reader so the computer could catch up and then start up again.  And Simon Pickup said, regarding SN-626, he said:  "XON/XOFF live on."  He says:  "I use it every day pressing Ctrl-S to pause scrolling on the tty output," you know, T-T-Y, teletype output, "of an exterm."  He says:  "Surely you do, too."  And I'm thinking, I'm trying to think if there's any terminal I have that runs so slowly now that that actually is useful.  I mean, you know, we use like more pipe or something in order to...



LEO:  I think it's the other way around, that it runs so fast you can't keep up, so you're pausing it.



STEVE:  That's my point is that, like, by the time you enter the command, you're at the bottom, you're at the end of the output.  And it's like, okay, fine.   So anyway...



LEO:  Some keyboards, though, I think, still do have - maybe it's the Scroll Lock or something.



STEVE:  No, it's just Ctrl-S to Ctrl-Q.



LEO:  Oh, Ctrl-S and Ctrl-Q.  Yeah, I used that for years.  I remember that, yeah.



STEVE:  Yup.  Yup.  And so, and they work today, but nothing is so slow that you have any time to move your hand over to the keyboard, or like get the Ctrl key down by the time it's done.



LEO:  Yeah, you've got to use more or less or something like that.



STEVE:  Okay.  And Leo, you'll get a kick out of this.  I cannot tell you how many people, fewer than corrected me about Lt. Saavik, sent, "What the hell is getting your car smogged?"



LEO:  People don't know what that is?



STEVE:  We have an international audience, and I saw that over and over and over last week.



LEO:  Oh, that's interesting, yeah.



STEVE:  So I didn't - yes.  So for those of you who don't have, who aren't in the U.S., here, with CO2 and other nasty chemical emissions from the tailpipe of our cars, in order to renew the annual registration, it's necessary to prove that your car still meets the regulations and the guidance for emissions of smog, thus getting your car "smogged," as it's called in slang, for the year model.  So I bought a Jeep back in '84, in the early days of SpinRite, which believe it or not is still on the road.  It's a six-cylinder inline, and it refused to die, so I sold it to my best friend for a buck.  I said, "Here you go, here's your spare car.  I'm not going to sit around and wait for this thing to die."



But the point is that it's not clean, but it's old, so it only has to be held to the emissions requirements at the time, so it's all kind of grandfathered, and that's fair.  But anyway, a huge number of people heard me and said, "Getting your car smogged?"  And when you think about it, that would make no sense at all unless you...



LEO:  It's an "e-test" in Canada.  They call it "e-test," apparently.



STEVE:  Ah.



LEO:  And I would imagine that there are something along those lines...



STEVE:  Equivalents, yeah.



LEO:  ...in emissions tests in other countries, as well.



STEVE:  Maybe not in China.  I don't think China has one.



LEO:  Can you see the smoke?  Okay, well, maybe you should get that fixed.



STEVE:  Yeah, when you have to wear a mask to walk around outside it's - yeah.  In that case you're the one who's getting smogged.



LEO:  And there are states that apparently don't do this.  Somebody in the chatroom's saying Wisconsin doesn't do it.



STEVE:  Really.



LEO:  Yeah.  Here in Cal...



STEVE:  Oh, that's right.  It's a California thing.



LEO:  It may be a California - I think some states probably also do it.  It may be a Cal - yeah, Minnesota, Michigan, no.  Emissions check in U.K.  Florida has no emissions test.



STEVE:  How about Texas?  Does Texas do that?



LEO:  Texas does not, no.



STEVE:  That was my guess when it came to mind.



LEO:  I'm guessing, yeah.  Anything goes there.  By the way, you have a right to carry a sword now in Texas, so that's good.



STEVE:  Well, you know, there might be snakes in the wild.



LEO:  You never know. 



STEVE:  That's right.  Okay.  So in what is probably the most awkward of, like, okay, how can we justify this, I mean, Heartbleed, that was not a big stretch.  That kind of worked.  MSBlast, that named itself.  This one uses the AT&T at the end of Sharkn, and then add "o."  So it's SharknAT&To.  Okay.  We knew they were going with Sharknado, fine.  They did win the podcast, the once and only time this will ever be called the Sharknado podcast.  Have you seen any of those previews, Leo?



LEO:  I have never, I am proud to say, seen one "Sharknado" movie.



STEVE:  Oh, I couldn't watch the movie.  But even the previews, even the commercials are just...



LEO:  But they're intentionally bad; right?  I mean, they're not trying to be good.



STEVE:  Yes.  Like Leslie Nielsen in "Airplane."  I mean, well, you've got sharks flying through the air, biting people.  And it's like, okay.  Fine.  So, but regardless of that, unfortunately a ridiculous name was given to a serious problem.  And so to offset the ridiculousness of the name, we had the serious problem.



When I encountered this in doing the research for today's podcast, and the near criminal negligence this demonstrates, or should be regarded as, on the part of AT&T, I was stunned.  And it immediately became our focus for this week, and unfortunately the title, not only as yet another useful walkthrough showing just how much can go wrong, but also to advise this podcast's listeners themselves who may be AT&T U-Verse subscribers and might therefore be directly affected; and to continue to support the explicit intention of this publication, who did the research to further highlight AT&T's negligence and the extreme danger the negligence has placed their paying and trusting customers in.  So, yes, Sharknado indeed.



The paper is so well written.  I corrected a few typos here and there that I found, but as I was reading it I was thinking, wow, this is written in a very, well, this is an English-speaking voice.  And as we know, we've covered some in the past that weren't.  So I'm just going to share this.  I did cut out some and edit it down for length.  But it's beautifully put together.



They write, and of course I've got the link in the show notes:  "When evidence of the problems described in this report were first noticed, it almost seemed hard to believe.  However, for those familiar with" - [dropout] or I'm sorry, "the technical history of Arris" - and that's what you keyed in on, Leo - "and their careless lingering of hardcoded accounts on their products, this report will sadly come as no surprise.  For everyone else, prepare to be horrified," they wrote.



"In all fairness, it is uncertain whether these gaping security holes were introduced by Arris, the [dropout], or if these problems were added after delivery to the ISP, in this case AT&T U-Verse.  From examining the firmware, it seems apparent that AT&T engineers have the authority and ability to add and customize code running on these devices, which they then provide to the consumer, as they should.



"Some of the problems discussed here affect most" - in fact, there's one that affects all - "affect most AT&T U-Verse modems regardless of the OEM, while others seem to be OEM-specific.  So it's not easy to tell who is responsible for this situation.  It could be either; or, more likely, it could be both.  The hope behind writing this is that the problems will be swiftly patched and that, going forward, peer reviews and/or vulnerability testing on new releases of production firmware will be implemented prior to pushing it to the gateways."  Again, so that's a summary of basically let's [dropout] and clean up their act and do this right from now on.  So we can hope.



"Security through obscurity is not acceptable," they write, "in today's high-threat landscape.  And this is especially true regarding devices which, A, route traffic, sensitive communications, and trade secrets for millions of customers in the U.S.; B, are directly reachable from the Internet at large; and, C, have wireless capability and therefore have an additional method of spreading infection and releasing data.  Regardless of why," they write, "when, or even who introduced these vulnerabilities, it is the responsibility of the ISP to ensure that their network and equipment are providing a safe environment for their end-users," meaning, yes, the responsibility unquestionably is AT&T's.



"This, sadly, is not currently the case.  The first vulnerability found was caused by pure carelessness, if not intentional all together.  Furthermore, it is hard to believe that no one is already exploiting this vulnerability at the detriment of innocents."  I mean, they are that bad.  Shodan is, like, already knows them all.  So this is horrific.  "Which is why," they write, "this report is not passing Go, not collecting $200, and is going straight to the public domain.  The vulnerabilities found here will be ordered roughly from least to most prevalent."  There's four of them.



Okay.  So we've got a little bit of trouble with attribution.  We're not sure if it's Arris or AT&T.  But regardless of where the problems were injected, where this happened, they wind up being in the victim's home.



Okay.  The first one, so we're going from least to most, believe it or not, SSH is exposed to the Internet with the superuser account hardcoded with a username and password.  They write:  "It was found that the latest firmware update" - not an old one, like the one now - for two of these modems, the NVG589 and NVG599 modems enable SSH - which of course we know as the secure shell service and server, meaning that an SSH client anywhere can connect to your U-Verse modem - enable the SSH server and contains hardcoded credentials - okay, get this, and they're here, right here.  "Remotessh" is the password.  I'm sorry, "remotessh" is the username.  The password is "5SaP9I26."  There you have it.  Anybody want to log into someone's U-Verse modem, scan the 'Net for SSH services and try using "remotessh" as the username and "5SaP9I26" as the password, and maybe you'll get in.



So they write:  "...using those credentials, which can be used to gain access to the modem's cshell client over SSH.  The cshell is a limited..."



LEO:  Oh, that's nice.  I'm glad they use "cshell."



STEVE:  Isn't that convenient, Leo.



LEO:  I far prefer that to "bash."



STEVE:  You don't want to have to grab the text and the reference to figure out, like, how do I blank the line?  Yes, yes.  Wow.  "Cshell," they write, "is a limited menu-driven shell which is capable of viewing/changing the WiFi SSID and password, modifying the network setup, reflashing the firmware from a file served by an FTP server anywhere on the Internet, and even controlling what appears to be a kernel module whose sole purpose appears to be the injection of advertisements into the user's unencrypted web traffic."



So if you do go to a website that's not over HTTPS, as we know, you have not established a secure tunnel that transits through the router.  So if this module were in use, it would be able to install U-Verse's own ad banners into pages as they were returned from the remote server.  "Although," they write, "no clear evidence was found suggesting that this module is actually being used currently, it is present and vulnerable.  Aside from the most dangerous items listed above, the cshell application is also capable of many other privileged actions."



To reiterate the carelessness of this firmware's release - remember, the most recent firmware - the cshell binary is running as root, and so any exploitable command, injection vulnerability, or buffer overflow will result in a root shell.  He says:  "Yes, it is running as root and trivially susceptible to command injection through the use of the menu's ping functionality; and, due to not sanitizing parameters, one can execute arbitrary commands through the menu."  That is, you use the semicolon to append two commands.  The ping accepts, you know, ping, and then the IP address you want to ping.



Then you put a semicolon and, like, echo /bin/nsh and pipe that to /etc/shells.  It's like you're executing Unix commands as root by adding whatever you want to on the end of the ping command, where it's expecting an IP.  You just give it a little bit more.  Incredible.  So then, after doing that, and they give some examples here, you exit and then reconnect via SSH.  The prompt you get changes, and you now have a root shell.  You type pound, you know, bang, I'm sorry, not pound, bang, the exclamation point, and you receive a busybox root shell of AT&T U-Verse routers everywhere.  Wow.



And then he adds:  "Please note that the cshell binary was only examined briefly" - not that he needed to examine it any further because he already had everything you could want - "and only until the easiest exploit was found."  So they thought, no, let's just try adding a shell command to the end of the ping's IP.  Oh, what do you know, that works.  Okay, we're done here.  Wow.



So they say:  "Judging by the binary's repetitive use of unsafe C functions, one can guess that hundreds of additional vulnerabilities most likely exist.  However, we find it highly amusing that the first vulnerability found was so trivial that it looks like it came out of one of those hacking tutorials that were so popular in the '90s."  I mean, it's like it's so bad, it's like from an example in a textbook.  Don't even ever think about having, like, not checking your parameters to see if it's actually an IP address and only an IP address that's been handed to the ping command.  And these people didn't.



Censys, which is an Internet security firm, reports 14,894 IP addresses having hosts which are vulnerable.  They say this is no guarantee, there's no guarantee expressed or implied in terms of this number will be all inclusive.  Almost 15,000 people discoverable on the Internet currently have this port listening, waiting for someone, an anonymous anybody, to connect.  Which will then allow them to take over your residential router with root privilege and do anything they want.  And this is now in the public.  Everybody knows.



LEO:  Yeah, you just gave it out.



STEVE:  Yes, exactly.



LEO:  If you were a bad guy, you knew it already.



STEVE:  You knew it already.  This has already been published.  And what we need to do is make sure AT&T doesn't screw around for a few months deciding how they're going to cover their tail and whether or not they want to fix this or not.  I mean, pressure needs to be applied.  And I agree with their position.  This is not a small mistake.  This is criminal negligence.  I wouldn't be at all surprised if a class-action suit is filed, and it ought to be.



LEO:  So it wasn't Arris.  It was AT&T, which has the rights to modify Arris firmware for these U-Verse modems.



STEVE:  Yes.  Well, what they said at the start was because both parties are involved - and we don't know.  We don't have clear attribution.  So it could have been built in, or it could have been added, or [crosstalk].



LEO:  But it's not present in other Arris modems?  Because I use an Arris modem.  I think a lot of people use Arris cable modems.



STEVE:  That one is present in two.  When we get to number four, it's in all.



LEO:  Oy.



STEVE:  So we're going in order of increasing pervasiveness of the problem.  Okay.  Second problem.  That was [dropout].  Second problem.  [Dropout] called "caserver," C-A-S-E-R-V-E-R, which is an HTTPS server present in the NVG599.  That was one of the two.  The previous one was the 589 and the 599.  This one is in the 599.  They write:  "An HTTPS server of unknown purpose was found running on port 49955 with default credentials.  The username 'tech'" - T-E-C-H, just in case anyone wants to go out sporting around - "and an empty password field conferred access to this highly vulnerable web server, which used only a basic authentication scheme," that is, nothing fancy.  You just have to give it "tech" and an empty password, and you're in.



"The server seems slightly unstable with its authorization capability, denying access on the first attempt even with valid credentials, and eventually completely locking up with an 'unauthorized' message.  It remains unclear," they write, "whether this is just poor coding or more security through obscurity, but either is unacceptable.  And," they write, "there's a trivially exploitable command injection vulnerability here."  So first it's open, we know what the credentials are, and it's exploitable.



They say:  "The exact intended purpose of the caserver is unclear, but its implications are not.  Caserver is an HTTPS server that runs" - and I was immediately wondering, okay, wait a minute, an HTTPS server, that means it had to have a private key.  And we're talking about a private key in a horribly insecure modem.  So that's never a good idea, but okay.  Maybe keeping its traffic encrypted was more important.  But there's, like, other ways to do encryption than over TLS.  So laziness abounds here.



"Caserver is an HTTPS server that runs on port 49955 of affected devices, which seem," they write, "to only be the NVG599 modem.  Caserver script takes several commands, including upload a firmware image" - so, yes, anybody can replace your firmware if they like.



LEO:  That's a good one.  I like that.  That's a useful - that'd be - come in handy, yeah, yeah, yeah.



STEVE:  Be handy for field service, Leo.  "Requests to a get_data handler which enumerates any object available to its internal SDB databases with a lot of fruitful information," they write, "and requests to a set_data command which allows changes to that database to be made."  And they show a screenshot of the request which caused the command injection.  And so it's all documented here.  Again, as the root user.  And again they note that, for the first request the server will probably reply, "You are not authorized to access this page."  They say:  "This can simply be ignored, and resubmitting the request shown will yield command execution."  And, you know, change the firmware.



LEO:  This can't be Arris.  This has to be some nitwit at AT&T.



STEVE:  It's incredible.  Yeah, you're right, it's too egregious.



LEO:  Yeah.



STEVE:  The service can be a little quirky, they say.  It locks up as I said before, after about five requests, blah blah blah.  Okay.  So I'm going to skip the rest and go to number three:   Information Disclosure/Hardcoded Credentials.  The next vulnerability involves a service also running, in this case on 61001, which will give an attacker a plethora of useful data about the device.  The attacker, however, will need to know the serial number of the device ahead of time.  So not so bad.  Once this information is acquired, the request can be made.



On the other hand, consider that AT&T U-Verse knows the serial number of every device every customer has.  And so if law enforcement or intelligence services had a valid reason, they could get the serial number and, with assistance from AT&T, could use this in order to attack the device.  Well, the mixed blessing of this I'll get to in a second.



So they provide the format of the command of what you do on port 61001.  You have to provide this weird set of hex characters, 001E46, and then a serial number in order for it to authenticate.  When the correct serial number, the OUI is the Arris organizationally unique number, and username and password are submitted, and they document that, the server will hang for several seconds before returning a response.  Afterward, several pieces of invaluable information are returned about the modem's configuration, as well as its logs.  So it says, here you go.  Here's everything you could ever want to know.  The most sensitive pieces of information are probably the WiFi credentials and the MAC addresses of all internal devices in your network.  And they could be used to exploit the next vulnerability.



Oh, forgot to mention, for anyone who's interested, the hard-coded username and password credentials are bdctest/bdctest.  So again, username and password, both are the same, bdctest, lowercase.  They write:  "This is the second-most vulnerability at the moment.  It is not the biggest threat since the modem's serial number is needed to exploit it.  This may change if an attacker were to find a reliable way of obtaining the serial number.  If present, an attacker could use the aforementioned caserver to retrieve the serial number."  Ah, I forgot that, yes.  So the previous vulnerability which anyone can exploit does return the serial number, which you're then able to leverage for use in the third vulnerability to get the log dump and a dump of all of the useful information that you might - about what's going on.  Just incredible.  



Number four, and this one no one is going to believe, the most prevalent of all.  They write:  "The most prevalent vulnerability, based solely on the high number of affected devices, is the firewall bypass" - that is to say the incoming traffic NAT router bypass.  We talk about NAT routers as being inherent firewalls because unsolicited traffic [dropout] to an initial outgoing traffic, so it's not return traffic, is blocked by default.  So we all have firewalls.  This bypasses that for any external attacker, allowing them to access the devices in your home.



So they write:  "The most prevalent vulnerability, based solely on the high number of affected devices, is the firewall bypass that is made possible by the service listening on port 49152."  This program takes a three-byte magic value \2a\ce\01 followed by the six-byte MAC address and two-byte port of whichever device your VCR, your webcams, your Ring Doorbell, you name it, whatever they want that's behind there would like to connect to from anywhere on the Internet.  What this basically means is the only thing protecting - and remember, this is the most widespread one.  Apparently they're all affected.  The only thing protecting an AT&T U-Verse internal network device from the Internet is whether or not an attacker knows or is able to brute-force the MAC address of any of its devices.



However, the first three bytes, that is, the first six characters of the 12-character MAC address, as we've often discussed, the MAC address is 48 bits, so it's 24 bits of the manufacturer and then 24 bits of a unique serial number for the MAC address to make them globally unique.  They are very predictable since they correspond to the manufacturer, which in this case would be Arris.  Maybe AT&T changes it themselves, but probably not.  Or maybe it's the manufacturer of the submodem subsystem, if that were the case.  Given this, an attacker could start out with this scheme with the unknowns marked.



And so basically they show us a template for that starting up, the \2a\ce\01.  And then they use an \ab\23\ed, which is the known high 24 bits of the MAC, and then you have to guess the other ones.  On the other hand, there aren't that many.  But they say:  "To make matters worse, this TCP proxy service will alert the attacker when they have found a correct MAC address by returning a different error code to signify that either the host didn't respond on the specified port or that an RST, a reset packet, a TCP reset was returned.  Therefore, the attacker is able to attack the MAC address brute-force and port brute-force problems separately, greatly decreasing the amount of keyspace which must be covered."



So essentially this is an access method by MAC address.  It allows a remote attacker to scan for the MAC addresses of the devices behind the router, essentially performing a device scan on your LAN from anywhere on the Internet.  And this, I have to say again, is present in all AT&T U-Verse modems.



And they have some more examples of ways to exploit it that they used.  And they get to:  "At which point it is now feasible for a determined attacker to use a brute-force attack.  Aside from the brute-force approach, there are other methods for obtaining the MAC addresses such as the previously mentioned vulnerability."  I mean, notice how these things even link together?  Like for number three you need data from number two, and now for number four you need data from number three.  Like almost like this was a plan, the way these exploits, the data provided links them together.  You have both plausible deniability, but it ends up being horribly frightening.



So they said:  "Aside from the brute-force approach, there are other methods of obtaining the MAC address, such as the previously mentioned vulnerability, or using a wireless device in monitor mode in order to sniff the wireless client's MAC addresses."  Good point.  Remember that, if you did promiscuous sniffing within the area of the user's WiFi network, the MAC addresses are not encrypted because they have to be on the outside of the packets in transit for them to get from point A to point B.



So anyone passively obtaining any network's traffic will, after a few minutes or half an hour, have eventually seen all the devices that are periodically transiting, some keep-alive traffic or beacons or pings or whatever, and obtain a list of all - now, that does require you to be local for a while.  But as they're saying, you could also use the method from the previous exploit in order to have it dump out the log of all the devices that are talking to it right now.



So they say:  "Going off of the example above, if the device's  MAC address," and they give it, "has an HTTP server running on port 80," which, you know, some device in your house, "and the attacker wants to connect and issue a GET request on the web root, the command will be," and they provide it, where essentially you are connecting to TCP servers running which are normally protected by the NAT router that lets you cut right through them.



And they say:  "This will open," in their example, "an unauthorized TCP connection between the attacker and the 'protected' web server, despite the user never authorizing it."  They say:  "It is believed that the original purpose of this service" - that is, essentially, the ability to break through the de facto firewall and get to a device behind it, and their presumption makes total sense - "that the original purpose of the service was to allow AT&T to connect to the AT&T-issued DVR devices which reside on the internal LAN."  That would make sense because they would know what the MAC address is, and that would allow them to immediately poke right through and access a device at a known MAC address at a known location.



"However," they write, "it should be painfully obvious by now that there is something terribly wrong with this implementation.  Added to the severity is the fact that every single AT&T service observed has had this port" - I've got to rewind.  "Added to the severity is the fact that every single AT&T device observed has had this port 49152 open and has responded to probes in the same way.  It is also important to note that the gateway itself cannot be connected to in this manner."  So only devices behind it in the home, on the LAN.  "For example, an attacker cannot set the MAC address to that of the modem's LAN interface and the port to correspond to the web configuration console.  This attempt will fail.  This TCP proxy service will only connect attackers to client devices behind."



So they wrap it up, saying:  "In conclusion, in 2017, when artificial intelligence runs the largest advertising firm on the Internet; when only last year the largest leaks in American history occurred; and where vehicles are self-driving, autonomous, Internet-connected, and hacked, why do we still find CGI injections, blank default passwords with root-privileged services exposed and what will most likely be termed 'backdoored' credentials?"



He writes:  "Developing software is no trivial task.  It is part of this company's core services.  But carelessness of this magnitude should come with some accountability.  Below are some workarounds for the vulnerabilities described in this write-up.  The time of full disclosure is gone, mostly; but let the time of accountability begin.  Accountability.  Or is it okay to continuously accept free credit monitoring from vendors and governments and corporations who have 'accidentally' exposed your privacy; and, in this case, maybe that of your family's, too?"



So a nicely put together piece and documenting just a horrific situation.  I would hope that our listeners, this podcast's listeners have their own NAT router behind their AT&T U-Verse box.  That solves most of these problems.  If you don't rely on its NAT routing, but rather have the Arris device, just plug it into one of our little favorite $49 EdgeRouter X's, and then you're done.  You're safe.  There's still the problem of somebody getting into the Arris modem itself.  That they could do.



But if anyone is concerned, and I didn't keep going with this because I just wanted to cover this, but the workarounds are there.  And the good news is the outcry that this has to produce will force this to get fixed soon.  But in the short term, I would immediately make sure you have your own NAT inboard of the AT&T U-Verse "router," unquote, and that way at least your internal network is protected.  Somebody could, using that earlier, but also much less widespread - the big problem is the widespread one.  So it was like, as they said, every single device they looked at had that port, 49152, open, listening, and with that proxy service running.  So you want your own NAT inside.  And as we know these days it's not difficult to do that.



LEO:  Well, but if you're using U-Verse TV service, you may not be able to NAT it.



STEVE:  Oh.



LEO:  That may be a bigger problem.  I'm not sure.  I seem to remember that people who use U-Verse have to use the router provided by AT&T, the TV service.



STEVE:  Ah.  In that case, maybe there's a way to run your own LAN, put your TV on...



LEO:  Put your LAN separate from your TV, yeah.



STEVE:  Yes.  Yes, yes, yes, yes.  



LEO:  And it sounds like, it's not clear, I mean, from reading the Nomotion post, they do ding Arris for previously having problems like hardcoded passwords in the firmware.  But it's not clear if this affects Arris modems on other than U-Verse systems.  I'm still not quite getting whether that's the case or not.



STEVE:  I completely agree.  We don't know.  But AT&T U-Verse is  large footprint.



LEO:  Yeah.  Oh, absolutely.  It's, yeah, number two or number three.  But, yeah, I would guess that most of the people who listen to this show have their own router in between them and the modem.



STEVE:  If not, then now.



LEO:  I would hope so, yeah.



STEVE:  There's never been a better lesson for why it's always good to do that anyway.



LEO:  Yeah.  Well, again, I once again apologize for the sound quality in this show.  I think what we're going to start doing, Steve, from now on is a double-ender.  I'll give you a week to figure out how you can record your audio at your end and then send it to us.  And that would solve the problem, not for the live folks, but it would solve the problem for the podcast.



STEVE:  Good.  I will do that.  And I should mention I've been watching.  I've still got the traceroute running.  And it's actually not three hops down.  That was sort of an initial artifact.  It looks more like, I don't know if there's, like, if there's a problem everywhere.  But it looks like it's the first hop.  It's like the other end of my cable connection is not returning.  So I'll just run through all my stuff again.  And I'm able to do this.  During one of our sponsorship breaks, I wrote down the IP that is the other end of this Skype connection, and I'm going to start poking, I'm going to probe to it.  And anyway, so it'll be fixed next week, one way or the other.  I'll get it nailed.



LEO:  Okay.  But, yeah, worst-case scenario we'll do a double-ender, and that way at least the podcast will sound okay.  And I do heartily apologize to everybody.  And for those of you missing one word in five, they'll all be in the transcript.  Well, maybe not.



STEVE:  Well, if Elaine can figure it out...



LEO:  The only one that was weird, and I just want to make sure Elaine gets it right, is when you said AngelFire, it sounded like the "g" was missing.  And that changes the meaning entirely.  So I just want to make sure that Elaine understands it's AngelFire with a "g."



STEVE:  Oh, yes.  I had to wait for a minute to figure out what it was you meant, but now I understand.



LEO:  Yeah, think about it, you'll know.



STEVE:  Now I understand.  You really don't want fire there.



LEO:  No.  Steve is at, and the podcast, too, GRC.com.  It's where you'll find the world's finest hard drive maintenance and recovery utility, of course SpinRite.  If you have a hard drive, you need SpinRite.  Everything else freely available including transcripts of the show and audio, 64Kb audio.  We do audio and video at our website, TWiT.tv/sn.  You can get that on demand.



Watch live, though, if you're around Tuesday at 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  A lot of people watch the show live, like to get the latest security news hot off the presses.  And if you do do that, join us in the chatroom.  Always a good bunch of people talking behind the scenes at irc.twit.tv.  Thank you, Steve.  We'll be back next Tuesday.  It'll be my last time, and then I'm going on a couple weeks' vacation.  But I'm sure that you'll have a good time with Father Robert.



STEVE:  We'll do it, and I'll talk to you next week, friend.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#628

DATE:		September 12, 2017

TITLE:		The Equifax Fiasco

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-628.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss last Friday's passing of our dear friend and colleague Jerry Pournelle; when AI is turned to evil purpose; whether and when Google's Chrome browser will warn of man-in-the-middle attacks; why Google is apparently attempting to patent pieces of a compression technology they did not invent; another horrifying router vulnerability disclosure including 10 zero-day vulnerabilities; an update on the sunsetting of Symantec's CA business unit; another worrying failure at Comodo; a few quick bits; an update on my one commercial product SpinRite; answering a closing-the-loop question from a listener; and a look at the Equifax fiasco.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And he, you know, it's funny when something like the Equifax breach happens, I don't know about you, but I just wait all week just to hear what Steve has to say about it.  Well, your wait is over.  Equifax and a lot more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 628, recorded Tuesday, September 12th, 2017:  Equifax Fiasco.



It's time for Security Now!, the show where we cover security, now, with this guy right here, Steven Gibson of the GRC, Gibson Research Corporation, GRC.com.  Steve is a security researcher, discoverer of spyware.  He also wrote the first antispyware tool, and he's been doing this show now for 13 years.  We've been doing this show for 13 years, since we were young men, all about security and privacy.  Hi, Steve.



STEVE GIBSON:  Yes, Leo, it was about 12.5 years ago we were in Toronto between filming breaks, and you said, "What do you think about doing a weekly, maybe like a half an hour podcast on, like, the week's security events?"  And I said, "A what cast?"  No one had ever heard of that before.



LEO:  You hadn't even heard of podcasts at that time, yeah, wow.



STEVE:  Yup.



LEO:  I see you're growing the moustache back.



STEVE:  Well, I don't know what I'm doing.  I'm lost.



LEO:  Well, Jerry Pournelle, I never saw Jerry Pournelle without a moustache, and I was even looking at old video of him from the '70s, and he had that pencil moustache that he was famous for.



STEVE:  Yup.  And his bolo tie, remember, I don't know what you call it, sort of he had like the big...



LEO:  Yeah, bolo tie, that's it, yeah.



STEVE:  Yeah, exactly.  Anyway.  So today's podcast, 628, is of course titled "The Equifax Fiasco."  And so we'll pull all this together.  A couple of our foreign listeners said, "What's a credit bureau?"  So I guess they do things differently elsewhere.  So but this is of course the big security news of the week, so we're going to cover that.  And we wanted to talk briefly about Jerry Pournelle, our very good friend and colleague who passed away Friday, peacefully in his sleep, at the age of 84, about a month after celebrating his 84th birthday.



Also about when AI is turned to evil purpose, which unfortunately apparently is going to be a thing before long.  Whether and when the Google Chrome browser will warn of man-in-the-middle attacks.  There was a lot of interest among our listeners about this news that Chrome is going to do it.  It turns out that's a little bit of a red herring.  And it kind of has to be because this is being done so much now.  Also why it appears that Google is attempting to patent pieces of a compression technology that they did not invent, which I imagine you and Jeff and Stacey will expand on tomorrow.  But there's sort of some interesting technical angles to it.



Another horrifying consumer router vulnerability disclosure, this time including 10 zero-day vulnerabilities.  An update on this forthcoming sunsetting of Symantec's certificate authority business unit.  Another worrying failure of operations over at Comodo.  We just keep coming back to those guys.  A couple quick bits.  And then we're going to - I've got to close the loop on one listener question, and then we're going to talk about the Equifax fiasco in detail.  So I think another - oh, and the perfect Picture of the Week that ties in, but we'll get to that in a second.



LEO:  Nice.  I can't wait.  Always enjoy those.  For those of you listening live, yeah, we know Steve is having a little glitching still.  But for those of you on the podcasts, apologies again for last week, but I think we'll have a good-sounding podcast for you because Steve's recording his end locally.  He'll probably sound better than ever before.  We'll see.



STEVE:  I don't think it'll help, but okay.



LEO:  Your moustache will be crystal clear 4K UHD.  All right, Steve.  Let's see that image you were talking about.



STEVE:  Oh, boy.  The Picture of the Week instills real confidence in Equifax.



LEO:  Oh, boy.



STEVE:  On their security encryption page, it starts:  "In the United States you can order all Equifax products online with confidence using Netscape and Internet Explorer."



LEO:  What?  Netscape?



STEVE:  Oh, yes.



LEO:  The Netscape?



STEVE:  Netscape Navigator, Leo.



LEO:  What?



STEVE:  Under SSL and 128-bit Encryption:  "If you have Netscape Navigator, simply select 'Help' from the menu bar."  And down lower it explains about the broken key that Netscape Navigator shows if you don't have encryption.



LEO:  Is it possible there are still people using Netscape?  No.



STEVE:  This company is on the leading edge of security technology as...



LEO:  Holy cow.  I can't - that's unbelievable.



STEVE:  Yeah.  Talk about being inattentive.  So, yikes.



LEO:  This is on their current website?



STEVE:  Yeah.



LEO:  Oh, lord.



STEVE:  Yeah.



LEO:  Well, there were lots of mistakes that they made in the various pages they put out.



STEVE:  Oh.  Oh, boy.



LEO:  I'm really looking forward to hearing your dissection of this one.



STEVE:  Yeah, yeah, yeah.  So I did just want to take a moment for a little bit of memoriam to our very good friend and colleague Jerry Pournelle.  I knew him pretty well, back in the early days of the PC industry.  And I mentioned before that his wife Roberta is a real hugger.  And it always used to seem to really annoy him that whenever the three of us would run into each other at a computer show, Roberta and I would give each other an extra-long hug.  And he'd sort of stand there looking a little flustered, kind of like, okay, when is this going to be over?



LEO:  I love it. 



STEVE:  So he was born on August 7, 1933, so he had his birthday, his 84th birthday last month.  And his son wrote last Friday, he said:  "I'm afraid that Jerry passed away.  We had a great time at DragonCon.  He did not suffer."  So, and of course, as we know because he was around TWiT during the time that he was fighting that brain cancer that he did bounce back from and recover from, I mean, so the guy was tough.  And, boy, what a pivotal personality through the whole formation of the early PC industry and Byte magazine, Chaos Manor, and all of his contributions over the years.



LEO:  Yeah, he was a great, treasured contributor on TWiT - 20 different TWiTs.  We also did two Triangulations with him, which if you want to know more about his life and times, they were great.  He had some great things to say.



STEVE:  Yeah.  And he and Larry Niven, another sci-fi author, put themselves together, and they wrote a number of books together.  My all-time favorite, though, was "The Mote in God's Eye."



LEO:  Yeah. 



STEVE:  In fact, I think all of their books, come to think of it - and it may have been Larry's influence, but I know that Jerry was the same way - there is like a core concept that is, you know, sitting around coffee or something, they come up with an idea that's really good.  And it just - it stands alone.  And then they encapsulate this perfect, pure surprise in a fully fleshed out sci-fi setting.



And that's what "The Mote in God's Eye" is.  You're reading along, and it's really interesting, and they're leaving clues along.  So it's sort of a mystery, kind of.  But there's, like, a big "oh, my goodness" that, like, jumps out at you about three quarters of the way through.  So just a really pleasant read.  And for what it's worth, if anyone has never had the chance to read "The Mote in God's Eye," it is a sci-fi classic and definitely worthwhile.  So Jerry, you will be missed.



Gizmodo covered an interesting story, although I did the math that they did and came to a different conclusion than they.  Although it shouldn't surprise us that the bad guys will be weaponizing AI for their own nefarious purposes.  Last year, Gizmodo writes, two scientists from the security firm ZeroFOX conducted an experiment to see who was better at getting - well, "who" in quotes - better at getting Twitter users to click on malicious links, humans or AI.



"The researchers taught an AI to study the behavior of social network users and then design and implement its own phishing bait.  In the tests, the artificial hacker was substantially better" - and I'll take issue with that in a second - "than its human competitors, composing and distributing more phishing tweets than humans, and with a substantially better conversion rate."



The AI, which was named SNAP_R, "sent simulated spear-phishing tweets to over 800 users at a rate of 6.75 tweets per minute, luring 275 of those 800 into the phishing scheme.  By contrast, Forbes staff writer Thomas Fox-Brewster" - we've often quoted him on the podcast - "who participated in the experiment was only able to pump out 1.075 tweets a minute, as opposed to 6.75, making just 129 attempts and luring just 49 users."  So as I'm reading this, immediately my rough math says, okay, wait a minute, that's about the same conversion rate.  Different gross rate, but the same fraction.



Anyway, Gizmodo finishes, saying:  "Thankfully, this was just an experiment, but the exercise showed that attackers are already in a position to use AI for their nefarious ends.  And in fact they're probably already using it, though it's hard to prove.  In July [a couple months ago] at Black Hat USA 2017, hundreds of leading cybersecurity experts gathered [as we know] in Las Vegas," Gizmodo writes, "to discuss this issue and other looming threats posed by emerging technologies.  In a Cylance poll held during the confab, attendees were asked if criminal hackers will use AI for offensive purposes in the coming year, to which 62% answered in the affirmative."



Okay.  So I guess this should not be surprising.  But I checked the math, and the "capture rate" or the "conversion rate" was almost the same for both the human, that is, Thomas Fox-Brewster, and the AI.  It was 34.4% versus 38%, and those are not statistically significant differences for such a small sample size.  So, I mean, so that difference with such a small sample set could have just been chance, luck of the draw.



So the takeaway here is that it's certainly worrisome that an AI could be at least as good as a human because that's zero effort, and it's automatable, and it's infinitely scalable.  So not good news for us.  But again, also not very surprising.  But we're sort of just putting on our radar because we do put things on our radar, and they come back to bite us almost invariably.



In fact, we talked about this whole credit freeze thing two and a half years ago, Leo.  And we'll get to that later.  But I thought, okay, yeah.  Our listeners who took our advice then weren't in any danger from this, or at least comparatively so.  There's even a bit.ly link I created that I verified was still valid:  bit.ly/freezecredit.  And it still works.  And that's two and a half years old.



LEO:  Does it link to all three of the reporting agencies?



STEVE:  It takes us to a page which has been maintained which walks a consumer through the processes for all three bureaus, yes.  And so it was good then, and it's still good today.  So again, this is one of the fun things we get to do on a 12-plus-year podcast is see how these things, these threats and issues evolve over time.



Another sort of a red herring, as I said at the top of the show, was this news that Chrome, Google's browser, of course, would soon be warning us of software that performs man-in-the-middle attacks.  Well, first of all, I didn't know how it could do that because, as we have now been discussing a lot, many enterprises are doing so-called "middleboxes."  That is, they're doing HTTPS proxying on their border in order to be able to inspect the traffic that's moving into their Intranet and protect themselves from it in case it's malicious because, as we know, post-Snowden, the world is going encryption.



So in digging into the story, which confused me, it turns out that there was a summer intern, just this past summer, who for her project looked at adding this technology to Chrome, which will get incorporated.  But it seems like what is actually going on is that there's a gray area in improperly or poor-written middleboxes which can do some sort of breakage.  And I wasn't able to get any details.  Maybe when it actually rolls out we'll be able to come back to this and understand what's going on.  But I did want to note to our listeners because I got a bunch of people saying, hey, look, this is going to prevent snooping.  It's like, no, it won't.



And remember, the other really unanswered question is whether, like what Chrome does because remember that Chrome pins the signatures of all of Google's certificates in itself.  And this is how so many fraudulently issued Google certs have immediately come to Google's attention because any person who uses Chrome, who goes to a Google asset which is coming from a fraudulent certificate, immediately sends a beacon back to Google saying, hey, I just saw a Google cert that you guys didn't produce.



So the point is that middleboxes are synthesizing their own certificates on the fly.  Thus the signatures would not match, and Chrome would not be happy.  So it may be that those middleboxes whitelist the Google domains and pass them through without trying to intercept them in order to keep Chrome from - I just don't know.  But that would be one way to do it.



So this feature that Chrome is adding is some sort of quality metric where - and again, I'm just - unfortunately there isn't anything that I was able to find that had the nitty-gritty, which everyone knows that I prefer to work from.  But it's if some threshold of errors are occurring, then the user is warned with a big intercept screen, big scary thing that comes up and says something to the effect of your firewall or AV is mis-installed, is what it says, and inducing errors, you know, you need to fix that.



So the idea is it will be tolerant of the, like, well-written, properly performing interception, but not poorly done interception.  And again, I hate that I don't know exactly what that means.  But hopefully at some point we will get some clarification.  It says this will appear in Chrome 63, and so on the horizon, and as a result of some work by a summer intern.  But again, I did want to back our listeners away because many people picked up on this and said, hey, that looks great.  It's like, well, okay.  Apparently it's just to help clean up the industry.  Which is, again, a good thing.  As we know, Google often uses its clout in order to discipline people who are behaving badly.



And this story is interesting, too.  Google has been accused of trying to patent public domain technology.  This surrounds an arithmetic data compression approach which was deliberately placed in the public domain by its developer, a Polish assistant professor who goes by Jarek Duda, D-U-D-A, who has accused Google of trying to patent technology he invented and deliberately placed into the public domain, specifically so that no one could own it.  I mean, his research was paid for by his university, and he just wanted to give this away and keep it from being locked down.



We were talking just recently, when we were talking about the decompression of the firmware in the Intel Management Engine and Huffman coding, how it uses variable-length tokens to represent variable frequency occurrences.  That's a form of arithmetic encoding and arithmetic compression.  This guy has a much, much more advanced form of, as it's called, "entropy coding," which is known as an - it's an Asymmetric Numerical System, or ANS, which he's been working on from 2006 through 2013.  And a number of companies have jumped on this and adopted it.  There's a variation called tANS and another one called rANS.  It's used in Apple's LZFSE compressor.  Facebook uses this in their Zstandard compressor.  And Google is employing it in their Draco 3D compressor.



So patents are weird.  And from this distance it's impossible to judge.  I don't know what Google is thinking, or if this professor is feeling very proprietary about his technology, and in fact Google may have had some innovations they've added.  There is a third party involved, which is there's an international patent group that is siding with the professor, believing that what Google's claims are alleged to be are just already in the public domain, well known, already in practice and so forth.  But again, as I said, patents are tricky.  Sometimes you initially go in with a deliberately overbroad application; and then, as we were talking just last week, sometimes you just get lucky - well, unfortunately for us - and the Patent Office says, yeah, okay, fine.  And then of course you have to litigate that in court, which would not be easy to do when Google is on the other side of that.



Or, more often, the U.S. Patent and Trademark Office will come back and say, well, claims 13, 14, and 15 we will allow; but one through 12, no, we're going to disallow those.  And so there's some, you know, it's why it takes many years, typically, and there's some back and forth.  So it could be that Google has innovated on top of the existing technology, which many patents do grant.  We'll have to see how this thing plays out.  Apparently Google is being silent.



There's some belief in the industry that they have pulled this back, and they're going to rework it in order to make it more clear what they've done and what they haven't.  So we'll see how this evolves over time.  It's not absolutely clear where things stand although, as I said, some independent judges have looked at this and don't feel that there's anything that really merits the label of invention and intellectual property protection.



Add to our list of routers behaving badly a recent state-of-the-art D-Link router, the 850L, which is a Wireless AC1200 Dual-Band Gigabit "Cloud" Router.  I put "cloud" in quotes because it has built-in cloud features.  It's got something called "mydlink Cloud Services" which allow you to access, view, and control the devices on your home network from anywhere.  So that's a powerful capability, and you would like the router to be secure.



Unfortunately, a researcher initially examined this router for his participation in a contest and was quickly appalled by what he found.  Then he found 10 different worrisome zero-day vulnerabilities of varying concern, which I'll run through here in a second.  He tried to responsibly disclose to D-Link, who never worked with him, kept silent.  He waited for months with no apparent action.  And then these was some disclosure where they didn't acknowledge that this came from him, but said, well, a researcher happened to discover a couple problems.  And so it's still not clear.  And in the most recent updates they still haven't fixed everything.  So anyway, the thousand-foot view of this is, if you have a D-Link 850L, I read several accounts saying just unplug it because, well, you can see why here.



So there's two different firmware releases, a version A and a version B.  A is not protected in any way, so firmware images can be forged by attackers.  There's no protection.  B added password protection.  Unfortunately, the password is hard-encoded and never changed, so that doesn't really afford any protection.  Okay.  So in the revA firmware, both on the WAN and the LAN side - and this researcher in enumerating these problems was careful to differentiate the two because, of course, as we know, LAN-side vulnerabilities are not good.  If something gets into your network, or depending upon like how large your network is, if you are an enterprise using one of these 850L D-Link routers on your perimeter, then any employee in a sufficiently large corporation would have access to the router from the LAN side.



Of course we know the WAN side is a much bigger concern because that's the world that has access.  And as we've seen, and we've been talking lately or recently, there are search engines like Shodan that can find all these things.  So the revA firmware exposes both on the public WAN and the private LAN cross-side scripting vulnerabilities with PHP files, which can be exploited as a consequence of these cross-side scripting vulnerabilities to steal authentication cookies from the router.  That's in the revA firmware.  On the revB firmware, again both WAN and LAN, it's possible to retrieve the admin password for the router to gain then full access using this custom mydlink cloud protocol.



And apparently without breaking the D-Link's terms of use, this researcher found vulnerabilities which could allow attackers to abuse this mydlink cloud protocol and register the router to their own accounts, to then gain full and unfettered access to the user's D-Link cloud and all the internals of their network.  So just abominable security.  It turns out also that this cloud protocol is fundamentally weak, so independent of firmware version there's a WAN side exposure.



The researcher wrote that the mydlink cloud protocol is little more than a basically TCP relay system with no encryption by default.  Traffic is sent over TCP to the Amazon servers that host the cloud without encryption.  So it's fully susceptible to sniffing and to man in the middle because, remember, if you don't have encryption, that is, if you don't have HTTPS and TLS encryption, you have no authentication of the endpoint, meaning that anybody could intercept your connection and have at what's going on back and forth.



Also the router interface allows users to enter credentials for their email accounts, which are then sent from the router to the server without encryption or verification.  And the passwords are stored in cleartext.  So, I mean, the more you look at this, the more it looks like a whole bunch of features that look fine on the surface, but absolutely no attention to underlying security and protocol.  Just none.



There's a backdoor on the LAN.  There is a secure tunnel both on the WAN and the LAN side, but they use hardcoded private keys which anyone can determine they never change.  And so even though you use stunnel in some cases to talk to this thing, the keys are known and fixed, so no actual protection.  And nothing like Diffie-Hellman dynamic key exchange.  Just no one's caring here.



There's also, I mean, it just goes on and on.  There's a way to brute-force the DNS configuration to allow the DNS config to be changed without admin user authentication checks, which allows anyone to reroute your DNS to a malicious DNS server.  And we know that that's like the first thing you want to do if you want to spoof a website's identity is return fraudulent IP addresses for legitimate DNS queries.  There's weak file permissions, and credentials are stored in the router in cleartext, which a remote attacker has access to.



There are remote code execution vulnerabilities where you have the ability to run as root; the DHCP server, which we know that a router will [dropout] get its IP address and gateway and other setup details over the Dynamic Host Configuration Protocol.  And it turns out that that daemon in the router has all kinds of vulnerabilities that allow that query to be manipulated to allow a remote code execution with root privileges.  And there's also the ability to DDoS or DoS some of the daemons that are running in the router.



So D-Link, if you do have an 850L, be very wary.  This is one of those situations where just replace it with something like the $49 EdgeRouter X or with a pfSense router or something.  It's just, you know, we're seeing these things being sold which, from companies that evidence no interest in the underlying security of their offerings and of the concern for their customers.  And end-users don't know.  It's up to security researchers and vehicles like this podcast to help spread the word and just to create, I think, a level of awareness that unfortunately has to be present.



It's going to be interesting to see how this evolves over time, how we create more responsibility, except maybe for consumers over time to just wander away from companies whose reputations have not afforded them the kind of demonstration of concern over security that they should have.



So, okay.  We've talked about the problem that Symantec had with mis-issuing certificates and, in one case, one of their partners that they had delegated responsibility to not being responsible with the issuing of certificates.  And the industry has decided that they are going to sunset trust.  So what happened yesterday was that Google's security blog updated the industry on where they stand.  And it also included some important information for site owners which I want to cover, and then also bring up something that I don't think we talked about before, which is the potential impact on IoT devices that this would have.



So Google wrote in their blog, they said:  "This provides a bit of Google-side perspective about the Symantec/DigiCert agreement and what it means also practically for website operators."  They said:  "After our agreed-upon proposal was circulated, Symantec announced the selection of DigiCert to run its independently-operated Managed Partner Infrastructure, as well as their intention to sell their PKI (Public Key Infrastructure) business to DigiCert in lieu of building a new trusted infrastructure."  And this is what we talked about before is that Symantec just said, okay, we're going to transfer our business unit, essentially, and the public key infrastructure, to an already trusted certificate authority, in this case DigiCert, because that just makes the most sense.



So Google explained for website operators what this means.  We were just talking about how this poor man-in-the-middle implementation detection will work in Chrome 63.  Starting with Chrome 66, which is around March 15th of 2018 in beta, and the stable track users will get it about a month later on April 17th, 2018.  So starting with that version, Chrome 66, Chrome will remove trust in Symantec-issued certificates issued prior to June 1, 2016.  So prior to this summer, June 1st of this summer.  And so that happens like after the first quarter of next year.



So Google writes:  "If you're a site operator with a certificate issued by a Symantec certificate authority prior to June 1, 2016, then prior to the release of Chrome 66 you will need to replace the existing certificate with a new certificate from any certificate authority trusted by Chrome."  So that's, I mean, hopefully all sites who are using such certificates are aware of this and getting this news because it'll come as a rude awakening after the first quarter next year when Chrome users are no longer able to connect to them.  They'll be getting errors from Chrome.



Okay.  And in addition, by December 1st of this year, so a few months from now, December 1st, 2017, Symantec will transition issuance and operation of publicly trusted certificates to DigiCert's infrastructure, and certificates issued from the old Symantec infrastructure after this date will not be trusted by  Chrome.  So that says that there is a period between June 1st and December 1st, so through this summer into fall, where certificates issued then are trusted, but Symantec's operations will be shut down and fully transferred to DigiCert by December 1st.  So now certificates should be issued by Symantec; and, in any event, Chrome won't trust them because, again, remember, there's just a concern that we're not really sure the extent of what happened with Symantec.  Reports have varied.  But it's better in this case just to say, okay, we're no longer going to trust those.



And then finally, around the week of October 23rd, 2018, so more than a year from now, about 13 months from now, Chrome 70, seven zero, will be released, which will fully remove trust in Symantec's entire previous infrastructure and all of the certificates that have been issued.  So essentially what Chrome will be doing until that time, until October 23rd, will be looking at the not-valid-before date, that is, the date of issuance, and making a judgment about when that certificate was apparently signed, whether or not to trust it.  And then 13 months from now, October 23rd, 2018, the Symantec root will be pulled out, will be removed from Chrome so that no certificates anywhere ever that were signed using that root will be considered valid.



So we're seeing sort of this drama unfold in real-time.  We've talked about these things before, and this is not certainly easy for the industry.  It creates some upheaval.  But it's what we have to do because, as we've said, the whole system only functions, I mean, and it functions precariously as it is because we're trusting implicitly the signatures by so many different certificate authorities where any certificate can be signed by any certificate authority.  And if that signature is trusted, then the certificate is trusted.



But there is a concern here that could have some impact on consumers, depending upon the design of IoT-style systems, because remember we're all used to thinking in terms of our client-side OSes, which have a massive store, hundreds of trusted certificate authority root certificates, which as I said will be used to trust anything that they sign.  But IoT devices, which are inherently lean, may presume the certificate authority of the remote server that they trust.  That is, our web browsers, for example, have to trust everybody because we're roaming around the Internet.  We want our browsers and our systems to be able to go connect to any server anywhere and authenticate the identity and get a secure connection.



But an IoT device inherently has a different architecture.  Many of them, for example webcam, are inherently tied to a specific remote server at a specific domain.  And because they're trying to keep their costs down, they're trying to keep their support overhead down and not waste RAM and flash memory and so forth, they may well have a single certificate because they know which certificate authority will always be signing the trusted certificate of the remote site.  If the authority was Symantec - and the other problem is these root certs are very long-lived.  They've got expirations decades from now.  So there may well be IoT devices where - and remember that Symantec was a major CA, historically.  I mean, they're the ones I used.  They were like the one, back in the day.  And I of course happily moved away over to DigiCert quite some time ago.



But it may well be that there are hardware devices that only trust Symantec certificates.  And there will be no more Symantec certificates for them to trust before long.  And the idea would be that, because an IoT device's root cert has a multi-decade life, the server side could keep refreshing its certificates every two or three years as required, as we know,  yet still be trusted by this single long-lived certificate burned into the firmware of some device.  So there's really nothing we as consumers can do.



The upshot is that some devices, if they have a single trusted root, and that root is Symantec, and hopefully they are establishing secure connections rather than just unencrypted and unauthenticated connections, they're going to die next year, which will create an additional unexpected problem.  We on our much more dynamic, full-spread, client-side OSes, this won't affect us at all.  But it does affect the server operators who have to make sure that the certs they're using remain trusted.  But again, I'm sure that news will spread sufficiently.



And speaking of this problem of any trusted signer being able to sign a certificate for any domain, note that pinning certificates creates an exception, which we were talking about with Google's Chrome.  There is a - I was going to say an "upcoming standard," but actually it already went into effect.  Last Friday, on September 8, 2017, the requirement for certificate authorities to check the new, relatively new, actually it's been around for a while, but we've talked about it before, the so-called CAA, the Certificate Authority Authorization record that is being published by a domain's DNS, went into effect.



So the CAB Forum, the CA Browser Forum, established these guidelines.  All certificate authorities starting - they have been able to deploy this technology earlier, but it was mandatory last Friday.  And what that meant is that, before a CA, a Certificate Authority, would grant a certificate to an applicant, the certificate authority would query that domain's DNS for a CAA record, which was a newly defined DNS record type, which specifies the certificate authorities that that domain uses to sign its certificates.



So, for example, GRC would have a CAA record that says my certificates come from DigiCert.  So the idea would be that other CAs should, before ever issuing anyone a server certificate for GRC, query GRC's DNS and see whether there is a CAA record.  If there isn't one, then that means there is no block.  That is, okay, this person hasn't said who they buy their certificates from.  If there is one, that is, if there is a CAA record which says these are where we get our certificates at this domain, and it does not include that CA, that Certificate Authority that is being asked to issue a certificate, then they must not.



So, okay.  This is obviously when, as we've seen, this is not strong protection.  But we haven't got a way yet to create strong protection.  This is better protection.  It's very lightweight.  It allows anyone who wants to increase their own domain's security to add one of these records so that any other certificate authority who is responsible and does follow the CAB guidelines, the CA Browser Forum guidelines, will refuse to issue a certificate when they should not issue it because they've not been given permission to explicitly by the presence of one of these CAA records.



So unfortunately, the day after this became mandatory, a curious German researcher went over to Comodo, and he has a long-published CAA record for his domain, stating that Let's Encrypt is the source of his certificates.  And everyone knows where this is headed.  Comodo happily issued him a certificate in contravention of the CA browser guidelines, which had gone into mandatory effect the day before.  And this guy got his certificate, verified it.  Since then, many other people have confirmed the same thing, so it's not just a single source of reporting.



What's odd is that Comodo has a very nice-looking page on their site where they go on at great length in their knowledge base about the CAA record and how it works and what it does.  And they've got links to various types of DNS servers and what you need to do.  But unfortunately, they're not, I mean, this is not just a single report.  This is multiple reports now that have verified that this is happening.  And they're just deciding, oh, well, this will be a nice technology.  Hopefully they will figure out why they issued that certificate by mistake and start honoring that because, again, this doesn't prevent a certificate authority from maliciously or deliberately ignoring that.



But of course, as this researcher just demonstrated, this is also easily tested because, as it is now, any CA - or as it had been, any CA would happily take your money in return for giving you a certificate.  Now that shouldn't happen, if there's a CAA record.  And again, this is easily tested.  So I think what we'll see is quickly that things come into compliance because we've got serious companies like Google who have demonstrated that they're willing to subject companies to whatever degree of pain is necessary in order to keep the system up and functioning and trusted, which is what we need.



I've got two quick little bits, some follow-ups on some previous coverage of ours.  We talked, boy, about a year ago, whenever it was that Volkswagen got caught basically with special case software which was able to detect when its emissions were being tested because the back wheels were spinning, but the front wheels were not.  And that sort of just sort of disappeared.  It just resurfaced again.  It turns out that, unfortunately, one of the engineers who was involved in this, he's in charge of the group that produced the software that did this.  He's been given 40 months of jail time...



LEO:  Whoa.



STEVE:  In prison, yeah.



LEO:  I think the executives should get the jail time.



STEVE:  That's my feeling, too.  That was my immediate reaction was, wait a minute, you know, this guy was probably following orders.  So also he's paying a $200,000 fine.  TheRegister.co.uk picked up on this.  They said:  "As head of the VW Diesel Competence unit in the U.S." - and the engineer's name is James Liang.  He "oversaw the software function that enabled the cars to cheat the emissions tests.  He is also the most junior of the eight current and former VW executives that have been charged so far."



LEO:  Fall guy.  



STEVE:  So, yes, there is some, you know, a bit of a - there is responsibility being taken.  They called it the "defeat device," which was designed to recognize when the car was being tested and to switch to a lower emissions setting.  When the car was running normally, that setting was removed, and emissions were measured at up to - get this - 40 times higher than the permitted levels.



LEO:  Yeah, but the response was great.  That car could go.



STEVE:  And this, of course, was put on 11 million cars.  The Register writes that:  "The engineers knew full well what they were doing and attempted to hide their tracks, even calling the device by a variety of pseudonyms including 'acoustic function,' 'cycle-beating software,' and 'emissions-tight mode.'"  So it turns out that, in terms of, like, sentencing guidelines, basically the judge threw the book at these people.



LEO:  Good.



STEVE:  The federal prosecutor said that Liang's prison sentence would send "a powerful deterrent message to the rest of the industry."  And apparently not just the auto industry will hear that message.  "Software engineers across the country," write The Register, "will have to reflect on the fact that they may be held personally responsible for creating something that knowingly breaks the law."



LEO:  Good.



STEVE:  So, yes, exactly.



LEO:  Although I really want to see the executives go to jail.  I don't...



STEVE:  I agree.  I agree completely.



LEO:  I'm sure he was just following - "I was just following orders."



STEVE:  Well, and I guess he's going to say no, and then get - I don't know how it's [crosstalk].



LEO:  Well, from now on I think that's the hope is that people will think twice and say no when they're asked to do stuff.



STEVE:  Yeah, I'm not going to prison for you pencil necks, yeah.  One other little bit of news about iOS 11 came across my radar actually last week, but we didn't have time to talk about it, and that was that we'd talked previously about the problem of MAC address leakage from our iPhones, and that Apple had taken some - was aware of the problem, that is, in the Ethernet protocol, we know that MAC addresses are used to carry packets.  And MAC addresses are globally unique identifiers by design because the way Ethernet works, that's something that you need to count on not having an address collision with.  There was some provision for Apple to obscure the MAC address, that is, if you were probing versus connected, as I recall from our previous discussion of this.



Anyway, what Apple decided to do with iOS 11, and bravo to them, although there are some packet capture people that are annoyed, and that is they are removing access to that information completely so that you will - apparently apps have previously had access to the ARP table.  That's the Address Resolution Protocol table where essentially that's a rich source of information.  You're able to see all the MAC addresses and IP addresses of all the machines that you're currently seeing around you in your environment, including your own.



So essentially they're going to just remove that from the application space so that applications will no longer be able to query that, to further increase the privacy.  So that's, you know, most applications have no need for it.  There are some developers who are chafing at this news because their particular packet-sniffing, packet-capture widget had access to it, so they're not happy that they're losing it, but tough.  I think from a privacy standpoint it is underlying plumbing information which definitely can be used to breach privacy, and that's a problem.



I had this in my notes last week, but we didn't have any time.  And the news was pretty much focused on Equifax this week, so we have a little more time.  I wanted to take a minute to do something I only do every couple of years because this sort of - what I see is that, as I'm talking in passing and sharing testimonials about SpinRite, I start beginning to get email from newer listeners who are saying, well, you talk about SpinRite every week, but you never explain really what it is, just like it does data recovery stuff.  And so over the course of the 12-plus years we've done the podcast, every couple years I've said, okay, let me just quickly explain what this is.  I also want to talk about briefly, catch everybody up on the 6.x plan, my plan for 7, and also the pre-release for the 6.x stuff.



So for people who don't know, for listeners who are new to the podcast, SpinRite was born more than 30 years ago in arguably a very different era of the PC industry.  Hard disk drives were very small, like 10, 20, 25 megabytes, not gigabytes, and not terabytes.  And they used much less technology, so there was much more [dropout] placed on the user and on the [dropout].  The drives themselves had no brain in them at all.  All the brains were in the controller.  And SpinRite started off with the goal of improving the performance of the drive because drives at that time were unable to read the data as fast as it was coming off of the disk.  The disk was spinning too fast.  The data was already too dense.  There wasn't a chance to get it into the computer, into its memory.



So instead there was this interleaving of sectors was a technology that had already been applied in the non-PC industry, where instead of successive sectors being adjacent, successive logical sectors were put downstream.  They were interleaved so that, like, every sixth sector would be the next one.  That allowed one sector to be read and then some time to pass while that got into RAM so that you could then ask for the next sector to be read.  The problem was there was no provision for optimizing that interleave.  That is, for example, IBM set theirs to six, that is, a 6:1 interleave.  But that meant that it took six revolutions of the drive in order to read all the data from one track, and then you could go to the next one.



What I discovered 30 years ago was that that was not optimal; that many systems could do it in three, that is, a 3:1 interleave, and some cases a 2:1 interleave.  What that meant was that you could reduce it from requiring six revolutions in order to suck in the data to just two, so it made your drive three times faster, and that was a big benefit.



The problem was, in order to do that, it required that the drive be low-level reformatted, to actually physically change where the sectors were.  Well, that was something you could not do once the drive had data.  Except that, back then, nobody had anywhere to put their hard drive data except keep it on drive.  So the big innovation that I created back with SpinRite 1 was the world's first and only non-destructive re-interleave.  And I did that by tackling it one track at a time.  Turns out you could low-level reformat just one track.  And so I would read all the data off it, and then I would change the sector ordering to optimize it for data transfer, and then put all data back.



But being a perfectionist, I thought, okay, I need to make sure I get all the data off because, once I low-level reformat that track, there's no more data there to get off.  So part of the responsibility I undertook was to absolutely, positively do everything I could to get any data off, especially if the drive wasn't wanting to give it.  You know, drives back then, as I said, didn't have a lot of technology.  They had much lower levels of error correction.  They didn't depend upon error correction nearly to the degree we do now, which has happened as bit density has increased.



So I built into the very first version of SpinRite arguably the world's best data recovery as a side effect of the fact that this was going to be my last opportunity ever to retrieve data from the hard drive before reformatting it at the lowest level and then putting the data back.  So consequently, SpinRite always had world-class data recovery.



So years go by.  Drives get more dense, much more data on them.  And at some point the controller moves into the drive with what we called the IDE drives, and SCSI drives were also very much the same way.  They became smart drives and began to encapsulate more and more technology.  Of course there was also huge pressure to squeeze as much data onto the drives as possible.  And as we've talked about, there were much more technology applied in order to always keep the drive kind of on the verge of working, where now more error correction was being used, almost continuously, but much more capable error correction also, in order to deal with the inherent softness.



So today's SpinRite no longer re-interleaves drives.  I can't remember where I took that out.  I think I took it out when I went to SpinRite 5.  I said, okay, I mean, just we're not doing that anymore.  All drives are now 1:1 interleave.  The channels from the drive to the processor and to main memory are all fast enough to handle taking the entire track off in a single revolution.  So SpinRite has evolved with the industry over time, no longer optimizing sector interleave, but actually falling back on and becoming useful going forward.



And amazingly, even as we've spoken of, on SSD drives that don't spin, but do use error correction, which is where SpinRite still comes in, it's evolved into a powerful data recovery system.  Which is what we talk about when people's files are in danger, run SpinRite on it.  Drives die, run SpinRite on them, bring them back to life.  You have to make a value decision, how much you want to trust it after it died once because, if it's trying to die, and I've said this before, ultimately SpinRite will fail because if the drive is determined to stop being a hard drive at all, it gets the final say.  But SpinRite certainly pulls you back out of a gray zone and keeps your data there typically long enough for you to then back it up or image it or get another drive and copy it over.



So we've been at SpinRite 6.0 since 2004.  It's now 13 years.  And it's [dropout] need to update it.  That's what I will immediately return to as soon as SQRL is behind me, and that's looking really good.  We're struggling with a couple last details, but we're getting very close.  So my plan is to - ultimately I want to go to SpinRite 7 and offer a whole bunch of new features because SpinRite is still just sort of a run-it-in-place system.  It made sense back then, but now people have drives everywhere.



So there are a lot of other features I want to add.  But I'm going to use the 6.x series - 6.1, 6.2, 6.3 - as sort of a technology development platform to create the technological foundation for 7.  So 7 will basically be a complete rewrite, but using the foundation that I develop with 6.1, 6.2, and so forth.



So what 6.1 will get us is a final removal of the BIOS and BIOS dependence, which has been a compatibility benefit in the long term, but recently a problem because BIOSes have not kept up with the drive technology.  So that'll go away.  SpinRite will talk directly to the hardware.  And I had all that running when I suspended the work on 6.1 in order to develop SQRL.  I'll go back to that.  That's how I know how fast SpinRite was running, and we've talked about that technology before.



But the idea will be the 6.x series will create this new technological foundation that will be useful immediately for 6 owners and will create the foundation for 7 moving forward.  And I will, as I have committed, make all of the 6 series available at no charge to all existing SpinRite 6 owners.  And because I want to give a thanks to the listeners who have been supporting me and making this possible by purchasing SpinRite 6, it will also be available to us on a pre-release basis.  That is, that always tends to be the way things work out.



Like SQRL has actually been working, not in finished form, but working for two years, or a year and a half at least.  And Leo, you'll remember a long time ago I looked at the screen with SQRL and logged you in over our Skype connection.



LEO:  I do, yeah.



STEVE:  So I'm sure the same thing will happen.  SpinRite will be running, that is, [dropout], well before I'm able to say, okay, I've got all the UI, or the packaging for the typical end-user.  So I'm going to make that available early to the podcast's listeners as a special thanks to everybody.



LEO:  Very nice, thank you.  Well, somebody just joined the show, and he said, "Have you talked about Equifax yet?"  That's next.  Can't wait to hear Steve's take on EquiF'd is what we call it.  You've been "EquiF'd."



STEVE:  I was tempted to call it "Equihax," but I thought, nah.



LEO:  Equihax, yeah, I like it.  Yeah, no, it's good.  We're going to get into it in a second.  Steve, I'm not going to be here for the next couple of weeks.



STEVE:  I know.



LEO:  I think Father Robert is going to be taking the helm, depending on whether he's got work for the Pope or not.  If not, it'll be Jason Howell.



STEVE:  And you're not kidding when you say that.



LEO:  And I am not kidding.  But Robert loves doing this show, and I know you love doing it with him, so I think it'll be Robert, Father Robert Ballecer in the next couple of weeks.  I'll be back in October.



STEVE:  And where are you going?



LEO:  France.



STEVE:  Oh, nice.  So I did have one closing-the-loop item.  An Ed Zucker asked what do I think about Tarsnap as a replacement for CrashPlan?  And I've spoken a little bit about Tarsnap in the past.  The short version is absolutely yes.  It was designed and created by a very good guy, Colin Percival.  And it's T-A-R-S-N-A-P dot com.  It runs on Unix-like OSes, so not for Windows, although it can run under Cygwin.  So the BSDs, Linux, macOS, and so forth.  And it's security done right.  It's full TNO, and it's a service as opposed to a standalone product.  So Colin hosts the cloud side, but it's very affordable.  And he shows you how to compute your costs, and things are measured in picodollars.



LEO:  I love it, 250 picodollars per byte-month.  



STEVE:  Exactly.



LEO:  It's hysterical.



STEVE:  Yeah, he's a neat guy.  In fact, he's the one who designed the scrypt memory-hard function for Tarsnap which I took and created EnScrypt for use with SQRL.  So that gives you a sense for how much I respect his crypto stuff.  He has a very correctly implemented block deduplication system.  You can't do deduplication after you encrypt.  So he maintains a hash of blocks on your local client.  And then, when you're doing an update, new blocks are hashed and checked against the cache to see whether it's already been stored.  If so, it doesn't do it again.  If not, then it hashes the block, adds it to the hash cache, then encrypts it locally and sends - oh, I'm sorry.  It then compresses it and then encrypts it and then sends it up to the server.  So if anyone is looking for a replacement to a cloud-based system, who has a Unix-like OS - BSD, Linux, macOS and so forth - Tarsnap I can recommend without hesitation because Colin definitely knows his technology.



Okay.  So a listener wrote, he said, actually tweeted:  "@SGgrc, not an American so I don't really understand what Equifax does.  Mayhaps there'll be an explanation on this week's Security Now!."  Let alone the title. 



LEO:  There must be something.  There's no way, I mean, I'm not a fan of the credit reporting agencies, but I can't believe that in Amsterdam you would just walk in and say I need a car loan, and they go okay.  They're going to check your credit.



STEVE:  Don't know how they do that.



LEO:  There must be some way to do it.  They just don't call them the same thing.  But there's got to be.



STEVE:  Ah, maybe that's it.



LEO:  Yeah.



STEVE:  So for our listeners who don't know, what has sort of evolved in the states, in the U.S., is a symbiotic relationship between credit grantors and these credit bureaus, as they're called.  We have three of them:  Equifax, Experian, and TransUnion.  And so the idea is that someone who's going to grant you credit, a bank or a credit card company, wants to have some warm and fuzzies about your history of paying your debts.  Are you good for the loan that they're going to give you in one form or another?



So these credit agencies, they aggregate the history of individual U.S. consumers, and apparently not only just U.S.  There's some Canadian and I think some U.K. that somehow get caught up in this.  So they maintain this history.  And so there's sort of, I guess, a Hobbesian bargain going on because the credit grantors say, well, we want the ability to know the history of people we don't already know.  And so these aggregators say, well, we'll tell you the history of people you ask us about in return for you telling us the delinquency or failure to honor the credit, oh, and their credit lines and everything else, of the customers you already have.  So there's this bidirectional flow of information.



What ends up happening is that these three agencies aggregate over time a huge amount of very personal data and background.  In fact, in order to do their job they're a database that has to store detailed information, private information about consumers.  And notice that this is not anything that a consumer asks for.  I never signed up with these companies, yet they've got complete records of my entire credit history without me doing anything, without me giving them permission.  This is all from - what?



LEO:  Well, you don't give them permission.  But whenever you acquire a loan or buy a car or rent an apartment or get a credit card, in all of that documentation you give permission for them to report all of your activity to a credit reporting agency.



STEVE:  Right.



LEO:  So you don't give it to them specifically, but you've agreed to it.



STEVE:  It's in the fine print, essentially.



LEO:  In the very, very fine print, yeah.



STEVE:  Well, and the other thing I'm seeing, and I'm sure you are, too, Leo, is from time to time you get letters from your bank or from investment firms or whatever who have [dropout] saying, you know, we're updating our privacy terms.  And if you don't do anything, then you're implicitly giving us permission to share your information with others, with our marketing partners or our financial partners.  And so unfortunately a U.S. consumer has to be very proactive if they want to minimize the amount of information which is leaked about them and hungrily aggregated by these companies.



And of course what is then evolved is the so-called "credit score," which reduces all of this through some sort of equation to a number.  I don't know, is it like zero to a thousand or zero to...



LEO:  850, something like that, yeah.



STEVE:  850?  Okay, yeah.



LEO:  Maybe it's 900.  I've never seen 900, but I guess it's theoretically possible.



STEVE:  And I did, coincidentally, I was talking to someone the other day who was having a problem, and her score was like in the 400s...



LEO:  Yeah, that's pretty low, yeah.



STEVE:  ...because she was considered not a very good risk.  Okay.  So what happened?  Equifax lost control of this database.  Now, and again, we don't have complete disclosure yet.  I'm sure Congress is going to be getting complete disclosure because, I mean, this was, in some cases, I know that I saw a piece that Dan Goodin wrote for Ars Technica, wondering whether this might not be the worst personal information disclosure so far in history.



I mean, it's bad:  143 million U.S. customers, which is approximately 44% of the U.S. population, had highly personal data, including of course the names, but also their Social Security numbers, their dates of birth, their physical addresses, in some instances their driver's license numbers.  And a subset, about 209,000 also had their existing, their current credit card account numbers; and there were also some sort of dispute documents where you go back and forth in order to say, wait a minute, you've got some bad information about me.  And I've had that happen.  I've never fought them.  But, for example, they had some bogus physical addresses for me in one of them, I don't remember who it was.  But it was like, okay, I never lived in Montana ever.  But they seemed to think that was a previous address for me.



So there is, unfortunately, they're sucking all this stuff up, and there's no feedback unless you go and pull your report from these people and then argue with them about things that they got wrong because otherwise it's just sort of being done passively.  Oh, yes, and information also about U.K. and Canadian residents were also there.



So what we know from still kind of fragmentary information, but I've now found it in multiple places, and in some cases financial ratings, that is, stock market equity summaries that are very careful about being correct with their facts.  We believe that Equifax first became aware of the incident on July 29th - so, what, five weeks ago, six weeks ago, okay - while the breach is believed to have occurred somewhere around mid-May, meaning that half of May, all of June, and all of July, two and a half months before they were even aware.



So that's raised throughout the industry a big red flag.  It's like, whoa, wait a minute, this company that we're trusting with this kind of information could have somebody accessing their data - and apparently this didn't happen all at once, it was over time - for two and a half months, undetected, until they figured it out.  But then, of course, once they did, on July 29th - so this only came to light last week, which means [dropout] five weeks after knowing that they had had a catastrophic breach in their security affecting 143 million U.S. consumers, putting our credit at risk.  Because the idea is that, with this information that they had that got loose, other people could impersonate us and apply for loans and credit under our name and get credit cards and change addresses and so forth, and thus really cause havoc.



So it's also a concern, not only that they didn't know about this for two and a half months, but that they waited for five weeks before letting us know.  And there was also some reporting that tried to make a deal or make an issue of the fact that three Equifax executives cashed out $1.8 million of their stock during this interval, but there has been a formal statement from Equifax saying that none of those three executives had any idea.  They were not in the loop, so this was not them trying to cash out before Equifax's stock crashed.  And no one's expecting this is the end of Equifax.  The one equity firm whose report I read said, well, maybe it'll take a 10% hit, but they'll recover over time.  It's not the end of the world.  Still, it's disturbing that this is the way they behaved.



So, okay.  What do we know about how this happened?  That was sort of what happened.  Well, what we know is that the Equifax system is based on a well-known open source server-side Java-based web design framework called Apache Struts.  It's a sophisticated system - that is, Apache Struts is - that needs to be used responsibly.  You know, Java is a serious industrial-strength language, but it's not a toy.  You need to know how to use it correctly.  So it doesn't do a lot of handholding for you, requires competent developers.  And as we'll see in a bit, when we talk about the generation of the per-user unlock authentication PINs, the competence and attention to detail of Equifax developers easily calls their competence into question.



So because Apache Struts and the Apache Struts project, which is part of the Apache Foundation, has been called into question, Ren Gielen, who is the VP of the Apache Struts Project Management Committee, went on the record to address the fact that they seem to have been implicated, their code appears to have been implicated in this.  And there's some interesting information here.  So he said:  "The Apache Struts Project Management Committee would like to comment on the Equifax security breach, its relation to the Apache Struts Web Framework, and associated media coverage.



"We are sorry [of course] to hear news that Equifax suffered from a security breach and information disclosure incident that was potentially carried out by exploiting a vulnerability in the Apache Struts Web Framework.  At this point in time it is not" - and I edited this a little bit.  "At this point in time it is not clear which Struts vulnerability would have been utilized, if any.  In an online article published in Quartz [qz.com], the assumption was made that the breach could be related to" - and there's a CVE number, we'll call it the "9805."  It's 2017, so it's this year, 9805, which was publicly announced eight days ago on September 4th, along with new Struts Framework software releases to patch this and other vulnerabilities.  So Apache Struts is being immediately responsive and responsible.



However, they write, the security breach was detected by Equifax in July, which means that the attackers either [dropout] an earlier announced vulnerability on an unpatched Equifax server, okay, so that would say that there may have been something else that was previously known and patched in the open source community which Equifax may not have bothered to keep current.  That's a theory.  We don't know that yet.  Or they exploited a vulnerability which was not known at the time, thus they may have independently discovered this 9805 known problem that was disclosed last week, or something else.  Again, we don't know.  No doubt there's forensics people looking at this now to get a better sense for what was going on.



Frankly, given what we will talk about in a second about the construction of the PIN, the secret passcode that users are given when they lock down access to their credit reports, it really doesn't inspire much confidence.  Not to mention the fact that they are explaining about how to use Netscape Navigator on their site.  So if the breach was caused, they write, by exploiting this 9805 known, now known as of last week, it would have been a zero-day exploit at that time.



And the Quartz [qz.com] article also states, and these guys didn't respond to that, probably because they're embarrassed, that this 9805 vulnerability has existed for nine years.  So it's been there, like, forever in Internet time.  And if it were independently found, that would be a problem.  But again, as we know, mistakes happen, and Apache Struts immediately fixed it and issued an update.  We don't know what Equifax was using on their servers and whether they were being as responsible or not.



Okay.  So this goes on for a while, but it ends with properly phrased good security advice, just to sort of wrap things up, which was keep your frameworks and libraries up to date.  Make sure that you're staying current, that is, that you're using the latest that is available.  A note that complex software always contains flaws.  And so don't depend, as Apache Struts guys, don't build your security policy around the assumption that the supporting software products are flawless, but rather use a defense-in-depth approach, a multilayered approach.



And, importantly, monitor.  As we've said often, it is ultimately monitoring to see if what's going on on your network makes sense.  If you can explain all of the traffic that you're seeing, that's crucial.  No matter what your defenses have, you also have to watch.  So anyway, so that's the techie open source web framework side of this.



So finally, the consequence in the public is, not surprisingly, that in the wake of this news, everyone has been rushing to lock down their reporting.  As I said, the danger to consumers is that a huge database, 44% of U.S. consumers have enough information now loose, including their Social Security number, in some cases credit card numbers, their date of birth, their physical address, I mean, everything you need, typically you provide to a credit grantor to grant you credit, is now available.



So our listeners who followed our advice many years ago, two and a half years ago, may already have been protected.  On Security Now! Episode 495, which you and I, Leo, recorded on February 17th, 2015, that podcast was titled "HTTP/2."  And I said, and Elaine transcribed, I said, "Okay.  Lastly, and this is - I probably should have done this first, but these other stories were just too interesting, and the industry's been buzzing about them.  I created a bit.ly link for this, and it's important:  bit.ly/freezecredit, all lowercase.  This takes you to a page where the guy explains that a service that is available to anyone for all three major credit bureaus - Equifax, Experian, and TransUnion - allows consumers who are not the victims of identity theft to lock their credit reports.  It's not free, but it's not too expensive.  The cost varies..."



LEO:  This is Clark Howard's page.  It's a good page, yeah.



STEVE:  Yes, yes.  "The cost varies, depending on where you are, from $3 to $10.  In California it was $10.  And because I was curious to do this, although I wasn't worried about this recent Anthem breach" - and that's what brought this current was it was the Anthem breach two and a half years ago.  I said:  "I locked my credit reports.  It was $10 for each."  And you, Leo, responded:  "You may have to pay to thaw it, as well, and that's important."



So we talked about this two and a half years ago.  I, my friends, and family locked ourselves down.  So what that meant was that it is impossible, given that the lock is honored, impossible for anyone to apply to use these three firms' reports in order to get credit granted.  And so, for example, if I needed a loan for something, I would have to explicitly unlock access in order to permit it.



And of course there's been a lot of controversy about this, naturally, in the last week because this has put a big spotlight on the whole locking/unlocking process.  And many consumers are annoyed that they're being charged to do something that they never asked for in the first place.  But in truth, and as you said, Leo, in order to be granted credit, you need to prove your creditworthiness.  So this is indirectly a service for consumers, although we certainly need the people who maintain all this data to be responsible with it.



So, weak PINs.  Someone tweeted to me:  "@SGgrc How much entropy is there in a timestamp?  Would be great to hear the breakdown in tomorrow's show."  The answer to the question, how much entropy is there in a timestamp:  none.  Zero.  It is entirely deterministic and predictable [dropout] about a timestamp.  It increments uniformly.  I mean, it's like none.



Okay.  So Equifax said, in response to people noticing that the PIN was a timestamp, they responded:  "While we have confidence in the current system" - okay, stop right there.  What?  They said:  "...we understand and appreciate that consumers have questions about how PINs are" - geez - "how PINs are currently generated.  We are engaged in a process that will provide consumers a randomly generated PIN."



LEO:  No.



STEVE:  What an innovation, Leo.



LEO:  No.



STEVE:  Imagine that.  "We expect this change to be effective within 24 hours.  A consumer has an option, and will continue to have an option..."



LEO:  Such idiots.



STEVE:  Oh, Leo.  Get this.  The PIN is mmddyyhhmm - month, day, year, hour, minutes.  And when I saw that...



LEO:  Couldn't possibly be a collision.



STEVE:  Well, could you guess it?  No, you couldn't possibly guess it.



LEO:  No, never.  Never in a million years.



STEVE:  I went back and looked, and now I'm a little annoyed with myself for not actually looking at the PIN.



LEO:  You didn't notice it, yeah.



STEVE:  Yes.  But I know the day, date, and hour, and minute that...



LEO:  That you applied.



STEVE:  Because it's right there in my record of my, quote - you know.  Oh, my lord.



LEO:  So terrible.



STEVE:  Yeah.  Again, so this is what I was saying.  If a company is issuing you a secret token which is a time and date stamp of when you submitted your request online, it's no security.  And it is obviously brute-forceable.  It's just - it's incredible.  So in the best case it's very low entropy.  And I can't explain this.



I mean, we already know you could take a counter and hash it and give somebody eight digits or 12 digits or whatever you wanted of that as a PIN.  Or if you wanted to be able to relate it [dropout] take the timestamp with a nonce, because you want to have a nonce, and symmetrically encrypt it with a secret key.  That would give the consumer something unpredictable and completely random.  But if they gave it back to you, you could decrypt it back into that original timestamp, if for some reason you needed a timestamp.  I mean, it's just like it's so obvious how to do this right.  And these people just thought, why bother?  So again, it's difficult to give them latitude.



And of course, finally, we have the expected class-action lawsuit arising from this.  A couple of plaintiffs who were tweaked by this found themselves a class-action firm.  I'm sure they were lining up to go after Equifax.  And so there's a class-action in the works seeking as much as $70 billion in damages for all of the consumers that were affected by this.  Okay.  So the bad news is that, in the wake of this disclosure of this disclosure, consumers did, I mean, like my best buddy Mark has been frantic on the phone with me, "What do I do, what do I do?"  I said, you know, I'll have a full readout by the end of the podcast.  I'll be up to speed, and I'll have dug into this.



Everybody has been running to lock their reports down, as we discussed two and a half years ago.  And for what it's worth, again, that bit.ly link, bit.ly/freezecredit, is still valid.  You just brought the page up, Leo, and it's a good page to talk people through how to do that.  The problem is that the three bureaus have been crashing because of the demand on their sites for everyone who wants to pay $10 or whatever it is, $10 in California, to each of the three agencies.



LEO:  In some states it's free.  In Maine it's free.



STEVE:  Ah, good.



LEO:  So don't assume it's going to cost you anything.  It's just, you know, each state has its own rules on that.



STEVE:  Yeah.  And in fact in our reporting two and a half years ago I said between $3 and $10.  So, but again, zero and 10.  Probably as a function of what the state allows these guys to charge.  Maine probably just said, no, you're not charging anybody anything for that.



LEO:  Exactly, yeah.



STEVE:  Because they would certainly like to, if they could.



LEO:  Oh, yeah.



STEVE:  So the final takeaway is, depending upon how concerned you are, remember that there's an inconvenience associated with this.  I've been locked for two and a half years because I haven't needed to apply for credit in two and a half years.  I like my credit cards.  They're fine.  And so I'm at a stage in my life where I'm not buying things that I need loans for any longer.  So that lock I applied, I mean, I know exactly when because my PIN tells me, back when I was researching this after the Anthem breach.  But it is an inconvenience if you need to be unlocking it all the time, especially if they're going to ding you again for the privilege of releasing the lock on your account, and then you're going to want to go back and re-stealth yourself by locking it again.



The system's a little broken.  But unfortunately it's an asset of some value because it does allow people whose credit has been established to have the freedom of having a third party to represent that, yes, you've been making your payments on time.  But with it comes a responsibility that Equifax, I mean, at worst they made a mistake.  I think maybe, as more facts come out and this unfolds, we'll get a better sense for just how irresponsible they were.  We don't really have a calibration on that yet.



LEO:  Amazing.



STEVE:  Yeah.



LEO:  There is, you know, we talked, didn't we, about that chatbot that you could use to fight traffic tickets in the U.K., and they launched it in the U.S.  Apparently he's added a feature now that you can sue Equifax using that.  I think it's for like $35,000 using the chatbot.  So I think Equifax is going to get some heat for that, quite a bit of heat from this.  Congress is investigating.



STEVE:  Yeah, there is an automated - there's something that I missed, also, Leo.  I don't know if you picked up on this.  But there was something about something that you did which caused you in the fine print to lose the ability to sue them.



LEO:  Yeah.  That's actually not the case.  Well, it might be the case.



STEVE:  I didn't think so, and that's why I didn't cover it was I was never quite clear on what it was.



LEO:  So if you went, and no one should ever do this because no reason to deal with Equifax any further.  But if you decided to go to their goofball site where you would figure out if you'd been hacked, Brian Krebs went there and entered random name and numbers and got the same responses if you didn't.  So it's a goofball site.  And apparently its real purpose is to move you through to their free credit monitoring TrustedID.  If you agree to that, there is an arbitration clause in that.



But both, first of all, I don't know how binding it would be.  Secondly, Equifax has said, well, this only applies to our TrustedID service.  It does not apply to the breach.  We're not going to assert that you can't sue us.  Although they've been fighting hard.  They've been giving lots of money to members of Congress to eliminate the ability for consumers to sue and eliminate the CFPB, the Consumer Finance Protection Bureau.



STEVE:  God help us.



LEO:  I mean, these guys are as bad as it can get.  They're just as bad.  They collect, I mean, we talk about Google and Facebook invading privacy.  These guys do it.  They collect all your data, they don't secure it, and then have the gall to try to make money off of this breach by signing you up for free protection which will of course on year one plus a day start charging you.  It's just - it's kind of - the gall of these guys is mindboggling.  Mindboggling.  I would like to know what they do in other countries for this kind of credit reporting.  They  must have some system.  Maybe it's run by the government?  I don't know.  But there must be, there has to be a way to verify your creditworthiness.



STEVE:  [Dropout] free enterprise.



LEO:  Well, that's the thing.  We live in a capitalist society.  And this is not only legal, it's kind of encouraged and supported by the government.  Well, Steve, what a good job you've done of synopsizing this.  And you warned us all a long time ago, freeze your credit.  It's kind of a pain.  If you're young, and you're doing a lot of credit, you know, buying stuff, getting credit cards, getting set up in life, it may not be worth the cost and the hassle of freezing and unfreezing.  But us old farts should definitely do it.  Besides, we've got more to lose.  We've got assets.  What's your opinion on credit monitoring?  Have you ever kind of thought about that or delved into that?



STEVE:  Yeah, and that was another question that my buddy Mark asked.  He said, "What about LifeLock?"  And I said, well, when the FTC stops suing them for overstating their benefits to the consumer, maybe.  I don't know.  I mean...



LEO:  There's a story with LifeLock.  I'm not convinced that that's LifeLock, by the way.



STEVE:  Ah.



LEO:  I should mention that I think they have been in the past a sponsor of some of the shows, maybe the radio show.  But I use them and pay for them.  The reason LifeLock got sued, and it didn't get sued by the feds, but it got sued by many states - I think the FTC might have ended up going after them - is that Experian, TransUnion, and Equifax went after them because what LifeLock originally did was put credit freezes on your account for you and maintain them.



STEVE:  Ah.  Nice.



LEO:  And that means that those companies can't - Equifax, TransUnion, and Experian can't make money off of you.



STEVE:  Can't sell your data.  Right.



LEO:  So they went after these guys.  And what they did, and we've seen this methodology used before, they went to individual states attorneys general and said, you know, it would be nice - here's a little campaign contribution - if you were to go after these guys.  So it's kind of too bad because LifeLock ended up having to stop doing the credit freezes.  They succeeded.  But what they did was I thought quite smart.  They bought one of the big backend companies that does the transactions for most credit card companies.  And this is the company that does the fraud alerts for your credit card company.  So they do have access to a huge data flow.



But even then the attorneys general and the FTC said, well, you can't say you can watch all transactions.  Well, obviously.  Cash transactions don't go through these services.  There's stuff that happens you can't see.  The bigger question is if credit monitoring does you any good.  I mean, I have LifeLock, and I get periodically, oh, we just saw your driver's license on the dark web.  Well, what am I supposed to do about that now?  Nothing.  There's nothing I can do.  How do you act on it?



STEVE:  Yeah.  And we've all heard these horror stories about identity theft.  I mean, how it's almost impossible, like it just ruins your life to have your identity stolen.



LEO:  That's what these companies offer, by the way, mostly, certainly what LifeLock offers is insurance and help in restoring your identity.  Maybe that's what you're really paying for.  I don't know.  I pay for it, and I get it for my kids.  When this happened I got it for Lisa and Michael, only because why take a chance; you know?  Because it's a lot cheaper to buy that than it is to fight identity theft.



STEVE:  Yeah.



LEO:  Anyway, yeah, I think the jury's out on whether LifeLock really did anything wrong, or if this was just the Big Three trying to put them out of business.  By the way, there's a fourth called Innovis that is not on this Clark Howard page.  But you can go to Innovis and freeze your credit there, too.  Interesting, huh.  What a world we live in.



Steve's show is available in a fine, high-quality 64Kb MP3 on his website, GRC.com, as are those fine transcriptions Elaine Farris writes for us.  You'll also find SpinRite, the world's best hard drive maintenance and recovery utility there and many, many, many free things, GRC.com, including ShieldsUP!, SQRL, his supplement history, all that supplement info.



STEVE:  The Healthy Sleep Formula.



LEO:  The Healthy Sleep Formula.  All you've got to do is go to GRC.com, Gibson Research Corporation.  And as for us, we have it at TWiT.tv/sn, Security Now!.  And we also have video should you wish to see the growth of Steve's moustache over the years.



STEVE:  As it returns to the world.



LEO:  As it returns to the world.  And you can also subscribe in whatever podcatcher you use; you'll find Security Now!.  Any show that's been around 13 years is probably in every directory.  Please do subscribe.  That way you'll get every episode.  I won't be here next Tuesday at 1:30 Pacific, 4:30 Eastern, 20:30 UTC; but Father Robert Ballecer will.  And Steve Gibson certainly will.  He never gets sick.  He never misses a day.  He's just the Iron Man of security podcasts.  Next Tuesday we'll see you.  Thank you, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo, and we'll see you in three weeks.



LEO:  See you later, yeah.



STEVE:  All tanned, or whatever you are after that.



LEO:  I don't think it's going to be one of those tanning.  I might be a little fatter.  Might have my liver force fed.  But other than that, yeah.



STEVE:  Okay, my friend.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#629

DATE:		September 19, 2017

TITLE:		Apple Bakes Cookies

HOSTS:	Steve Gibson & Father Robert Ballecer

SOURCE:	https://media.GRC.com/sn/SN-629.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Padre and I discuss what was up with SN's recent audio troubles, more on the Equifax fiasco, the EFF and Cory Doctorow weigh in on forthcoming browser-encrypted media extensions (EME), an emerging browser-based payment standard, when two-factor is not two-factor, the CCleaner breach and what it means, a new Bluetooth-based attack, an incredibly welcome and brilliant cookie privacy feature in iOS 11, and a heads-up caution about the volatility of Google's Android smartphone cloud backups.



SHOW TEASE:  It's time for Security Now! with Steve Gibson, and we are laughing in the face of security armageddon.  Equifax protected your security with an executive who didn't know anything about security, the EFF walked out on the W3C, browsers get payments done right, your Bluetooth devices are spying on you, and Apple gives you a cookie.  Security Now! is next.



FATHER ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode 629, recorded September 19th, 2017:  Apple Bakes Cookies.



It's time for Security Now!.  It's the part of the Internet in which we trust you, if you trust no one.  Now, the person who trusts the least and therefore is the most trustworthy is Mr. Steve Gibson.  Of course he is the big brain behind Gibson Research, ShieldsUP!, SpinRite, and our coming non-passworded overlords over at SQRL.  Steve, my friend, it's been a while; and I've got to say I've been looking forward to this day for quite a bit.



STEVE GIBSON:  I have.  I know our listeners have.  And what's better, we have an extra treat for people.  You can actually understand what I'm saying today.



PADRE:  Yeah, I noticed you were having some audio issues the last couple of weeks.



STEVE:  Ooh, boy.



PADRE:  What was that?



STEVE:  Yes.  So first of all, it wasn't Cox.  They've been amazing and fabulous through this.  And what I think happened was a combination of things.  I hadn't messed with my own coaxial infrastructure forever because I'm sort of a, you know, if it's not broke, don't fix it.  But that sort of decays over time.  So I believe that there was a problem from the delivery of the external cable into my residence.  I did find an unterminated run which, you know, you're supposed to have a 75-ohm terminating resistor on all of those, so that may have been a problem.



So I think the initial problem was a slowly accumulating problem between - essentially with the cable modem that I had getting a strong enough signal, through no fault of anyone's except just indirectly my own from having left everything alone for so long.  But then what we did was, thinking that maybe it was the audio interface, we switched to a much more advanced audio interface.  And unfortunately the machine that I was using was an old Win7 box that I think was underpowered.  So we had overlapping problems.  The audio interface was actually having a buffering problem with the machine getting the data, getting the audio off of the USB bus fast enough for this newer, higher powered audio interface.  That overlapped with the networking problem.



So anyway, as everyone can hear, that's all gone now.  And I got a lot of, of course, feedback from our listeners saying, oh, my god, your content is great, but it's barely worth listening to at this level of quality.  So yes, we know.  I apologize.  It did take a couple times, a couple weeks to get down to this.  But as I promised Leo at the end of last podcast, I said, okay, done.  I mean, I'm not resting until this is resolved.  So anyway, again, I've switched cable modems a few times.  Even the people at Cox who don't know me, like random tech support people that I had to talk to in order to do some of these experiments, were fabulous.  So I have got nothing but praise for my bandwidth supplier, and I'm happy that we've got a glitch-free connection now, for now and hopefully into the future.



Oh, and I wanted to mention that I was recording at my side, but the problem was, when I sent the audio up to TWiT, the time base was enough different that my audio kept falling behind or getting ahead, I never - it wasn't really clear which way it was.  So my own recording wasn't useful.  But since the problem was in the USB audio interface, it wasn't any better either.  It was the same as what I was sending up to TWiT.  Anyway, problem solved.  I appreciate all of our listeners' patience, and we're good to go again.



PADRE:  Well, it's a great lesson for anyone who's troubleshooting because what you just described is - it's a perfect storm.  It's almost a worst-case scenario where you have an issue that crops up, but it's not one problem, it's actually two discrete problems that masquerade as a single problem.  That is the most difficult thing to troubleshoot ever.



STEVE:  Right, right.



PADRE:  And also I had this problem not too long ago when I used a USB audio interface, which I thought would make things much more clean because it's free and it's isolated and it was relatively expensive.  But it turned out that because it didn't have its own onboard DSP, it was requiring the CPU to put in a lot of its power to do the calculations.  And that's just no good.



STEVE:  Yup.  So this is Episode 629.  And I didn't have a title until the very end, as I was pulling everything together.  But as I dug into a surprising story, I thought, okay, this one is the title for the podcast, ABC.  The title is Apple Bakes Cookies, which we will be getting to later in the podcast.  I am so excited about something that Apple has done in iOS 11 which has reciprocally and for the same reason really upset the entire Internet advertising community.  And our listeners know that the whole issue of tracking and cookies and third-party management, it's been a particular hobbyhorse of mine for years.



I have, at GRC, I built that whole cookie forensics system in order to generate statistics from the browser configurations of GRC's visitors.  It's not something that I ever made widely public, mostly because it was sort of an R&D project.  And we've talked about it often.  So we will, later in the show, address what Apple has done, which is just - it is a brilliant solution to this problem.  And so Apple Bakes Cookies is our topic, but we've got a whole bunch of stuff to talk about.



We're going to talk about, we already have talked about the solution to this podcast's recent audio troubles.  We're going to do a little more follow-up on the Equifax fiasco because I heard you on a different podcast, Padre, talking about some additional steps beyond just locking your credit with the various bureaus that I wanted to give you a chance to share with our listeners.  The EFF and Cory Doctorow writing for them have weighed in on the forthcoming browser Encrypted Media Extensions issue, the EME issue that we talked about, about a month or two back.  And, boy, they're not happy.  In fact, I'll just step on this a little bit.  They've resigned from the W3C over this.



PADRE:  They've taken their ball and gone home.



STEVE:  We need to talk about that.  There is an interesting emerging web browser-based payment standard which has got full adoption by all of the browser vendors that we need to cover.  Also, something finally hit me, I don't know why it took so long, but it's another story about the danger and the mixed blessing of using SMS, the Simple Messaging Service, for second-factor because it turns out it's actually not a second factor.  Then big in the news was the breach at CCleaner, one of everyone's favorite utilities for Windows, to go in and do a much deeper cleaning of junk we don't need than Microsoft's own little disk cleanup utility.  They had a big problem that affected people for about four weeks.  There is a new Bluetooth-based set of vulnerabilities.  Again, the great solution that Apple has come up with in iOS 11 we'll talk about.  And then a few other random tidbits.  So I think lots to talk about.  And maybe we'll actually finish today.  I'm not sure, though.



PADRE:  Well, this has always been a unique thing about the dynamic between the two of us.  We get so into the stories that we always end up doing maybe half of the content.  I think maybe, maybe this time we might get to the full docket.  It's a nice set of stories.  All right.  It's a topic that we've been covering for a while, the Equifiasco, Steve.  What have we got?



STEVE:  Well, first we talk about our Picture of the Week.



PADRE:  Oh, that's right.



STEVE:  It is one that - it's just one that I had around.  There was nothing really compelling that synchronized with the stories of the week.  So somebody put together a fun mug.  We've recently had my mug on a mug that a lot of our listeners have been purchasing.  I just got a kick out of this.  And someone sort of synthesized a SQRL mug where we've got the logo for SQRL, and then the subtitle, "Protect Your Nuts."  So, yes.  And in fact it's a little-known side effect is that there is in fact a nonce in the protocol which is required because SQRL works by signing a unique challenge that the server presents.  So thus we need a nonce.  But we didn't want to call it "nonce" for the SQRL protocol, so it is formally known as the "nut" in the SQRL protocol.



PADRE:  Steve, you're having way too much fun with your naming conventions.



STEVE:  So, yes, okay.  Padre, it's your turn.  More that we can do about Equifax.



PADRE:  Right.  So of course everyone by now has heard all the details, and they continue to come out.  And Steve, I've got to tell you, some of it is just head-scratchingly bad.



STEVE:  I know.



PADRE:  It's as if someone designed a program to fail - bringing in the wrong people, bringing in the wrong technology, and bringing in the wrong security measures.  But what we wanted to do, and what we've done on a couple of the TWiT shows, including Know How and TNT and New Screen Savers and TWiT, is we've talked about some of the strategies to sort of minimize the Equifiasco.  Now, I know that you and Leo talked about the credit freeze; right?



STEVE:  Right.



PADRE:  I mean, that's - he advocates it.  And the reason why he advocates it is there's a little bit of revenge.  Because if you put a credit freeze on your finances, you're essentially telling the CRCs, the Credit Reporting Companies, that they are not allowed to make money off your information.



STEVE:  Well, and in fact, 30 months ago, after the Anthem breach, that was our advice on this podcast.  I created a bit.ly link, bit.ly/freezecredit, which takes you over to the Clark.com page that lays out the details per reporting agency, how to get them to shut down.  So, yes, that's certainly the first step.



PADRE:  But we went over a couple of different strategies because there are options available to you, some of which are free and some of which are paid.  Now, the most well known and the easiest is the fraud alert.  Let's actually talk about this because there's three levels of fraud alert.  One only applies to U.S. military, and that allows them to put an alert on their account for one year, free of charge, giving them access to two credit reports per agency per year.  But the ones that our audience are probably going to run into are the standard credit fraud alert, which lasts for 90 days.  And unlike five years ago, you don't actually have to call in.  All you have to do is go to the website of one of the CRCs.  I would suggest TransUnion, just because it's really simple to use.  You do have to give some personally identifying information, which I'm always a little risqu about it, but at least I'm not giving it to Equifax.



STEVE:  To authenticate yourself to them.



PADRE:  Precisely.  But once you place a credit fraud, they must report it to the other CRCs.  So you only have to do it once.  You don't have to go from CRC to CRC.  It works for 90 days.  You can go ahead and get yourself a free credit report.  And here's the thing.  You can get a free credit report every year, but that's not cumulative over the CRCs.  So I could actually get one every four months, going from CRC to CRC.  That's actually an important step.



Now, after 90 days, if you do the online version of it, you can just renew.  It's just a simple click, and they'll renew it for another 90 days.  There is one for active exploitation of your ID.  If you are confident that someone has tried to open up credit in your name, you can ask for an extended fraud alert.  And what that does is it works for seven years, and you don't have to renew every 90 days.  That works much better.



But the difference between a freeze and a fraud alert is simple.  In a fraud alert, anyone coming to one of the CRCs and asking for your credit report, they can get the information, but they are given a warning that this account has been put under fraud alert, and here's a number which you provide that they should call to verify that you're actually the person requesting credit, a new mortgage, a car, whatever.  But they can still get the information.  Which sounds bad, but it actually can be good.



Let me explain that.  In a freeze, a freeze is different because, when you put a credit freeze, no one can access your credit report unless you specifically allow them to.  So you have to whitelist any company that wants to look at your credit.  Now, you may say, well, that's good.  We're all about whitelisting.  We're all about trusting no one.  Except in modern society a lot of people don't realize how many times your credit report is accessed.  And it's not just when you want a loan or a mortgage or a credit card.  It could be the next time you try to get a phone, the next time you try to get a job, you try to get an apartment.  All of those are going to ask for your credit report.  And if you have a freeze, you have to whitelist every single one of those.  It's kind of a pain in the butt.



Additionally, the rules for freezes differ from state to state.  In some states it's indefinite.  And when you put a freeze on, it's frozen until you unfreeze it.  In other states it will last for up to seven years.  And there can be a fee both putting the freeze on and taking the freeze off.  So they can get you both coming and going.  We don't tell people not to do a freeze.  But at the very minimum, the easiest thing to do is to put a fraud alert.  And, yeah, so that's really the start of the spiel.



On Know How, if you take a look at Episode, I think it was 344, we went over a bunch of different apps that can train you to take better care of your credit.  A lot of people don't actually do the maintenance, the bare maintenance that they need to do to make sure that their credit isn't being used, that there isn't some fraud going on.  And also, Steve, I don't know how you feel about this, but there are so many people, and not just young people, there are adults who don't know the basics of finances.  And this makes it so much easier to exploit you when you don't actually understand how your credit works.  Have you seen that?



STEVE:  So the one thing I'm puzzled about, you said that the fraud alert, you place a phone number.  So it's the company that is querying them for your credit must call you, except there's no enforcement for that; right?



PADRE:  There's no enforcement.  But here's the thing.  If they opened up a line of credit on an account that had been flagged for a fraud alert, and they did not call that number to verify you, then if anything does happen...



STEVE:  Then they're liable.



PADRE:  ...they're liable.  So no company, no legitimate company that's trying to grant you credit would dare not call.



STEVE:  Well, and what we're trying to prevent is a malicious other party - I don't know whether it's third or fourth or fifth.  We've got too many parties going on in here.  But we're trying to prevent the case of somebody else applying for credit in our name.  So the company verifying that application would be the one to call us to make sure that that's really us who is applying for the credit, rather than that malicious party.  So if the system works correctly, that does close that loophole.  The person who is having credit applied for in their name gets the notification from a valid credit grantor that this is really them.  And so I can see how it works, as long as everything fits together.



PADRE:  Right.



STEVE:  And I guess it's more practical for people who are dynamically needing credit.  For example, all of mine have been locked since that podcast 30 months ago, and I've never - so nobody has successfully been able to look at my credit reports.  I just haven't needed any.  But I'm 62, and I'm at the point in my life where hopefully things have settled down, and I'm not needing lots of credit now, unlike somebody in their 20s or 30s who's actively churning around, jumping around, moving, buying things and so forth, where locking all of the access to your reports is much less practical.



PADRE:  Right, exactly.  If you're not super dynamic, a credit freeze works really, really well.



STEVE:  Right, right.



PADRE:  But if you are, and you're relatively young, and you're moving around the country a lot, a credit freeze, it's actually a huge hassle because, again, it's every single request has to be whitelisted.  And imagine if everything you wanted to do in a modern society required you to call up, give a PIN, and whitelist an organization.  It's actually a lot more hassle than people think.



STEVE:  Well, and probably just renting an apartment requires that your credit be checked because the landlord wants to make sure that you're not a deadbeat.  So all kinds of things.



PADRE:  Yeah, and getting a job.  When I was still working HR for the church, that's one of the first things we did.  I mean, we'd check social media, but we also checked the credit report because that kind of tells you whether or not someone's trustworthy.  If they declared three bankruptcies, and their credit report is in the garbage, you start thinking, okay, this person doesn't have control over their personal finances.  Maybe that's not a position that they should be in.  It sounds horrible, but that's a publicly available piece of information.  Of course an employer is going to take advantage of it.



STEVE:  We did also - naturally more information is emerging about Equifax.  And one of the worrisome things we discover is that the person in charge of security, the CSO, the Chief Security Officer, was apparently a woman by the name of Susan Mauldin, who is a music major and, the best anyone can tell, has no background in technology or security.  And immediately after this happened, her name on her LinkedIn profile - first of all, the last name was just changed to Susan M.  And I looked just this morning, and it's completely disappeared.  There's also been some evidence from people who have been digging into this that all information about her has been proactively scrubbed from the Internet.  YouTube videos have been pulled down, and interviews have disappeared, and blog postings have gone away.



And so it looks like there's a real sort of a CYA effort here on the part of Equifax to bury the fact that somebody whose qualifications for the position were dubious at best is, like, trying to be covered up.  And apparently she's also retired from Equifax now.  So I think one of the things that I think I saw after last week's podcast was the number of inquiries and lawsuits gearing up to respond to this breach is daunting, to say the least.  Although it hasn't affected their stock price very much.  I think everyone, you know, the official opinion is, well, this is bad, but bad stuff happens, and they'll survive.



PADRE:  We kind of wish they wouldn't.



STEVE:  Boy.



PADRE:  You almost kind of want them to fail just so that the other CRCs will take their responsibilities seriously.  I mean, they have one job.  They get access to massive amounts of incredibly sensitive information that could destroy people's lives, and their one job is to keep it secure.  And in exchange for that, they are allowed to sell it, package it, do the analysis so that they can make money.  They didn't take care of the former, they shouldn't get the latter.  Isn't that, I mean, that sounds fair.  But let's talk about Susan, though, because, Steve, what strikes me about this story is Equifax put more effort into scrubbing the web clean of any references to who she was or what her background was, they put more effort into that than providing the website so that people who are affected could check.  That's just mindboggling.



STEVE:  Well, and there was also - there was some, and I didn't have a chance to pursue this, and there was some controversy about it, and the information seemed to be from kind of a sketchy source.  But there was also, you know, we talked last week about how perhaps it was one of the libraries that they were using for their server-side application design.  There's also some question about whether they had left their web-based control panel configuration set to a username and password of admin and admin.



PADRE:  That's secure, yeah.  That works.



STEVE:  Okay.  So - okay.



PADRE:  Steve, that's worked on my Linksys router for 20 years.  Why should a website be any different?



STEVE:  That's right.  And besides, if it was fancy, then people wouldn't be able to log in when they want to, and it would be some big hassle for everyone.  So, yes, I agree.



PADRE:  It was reverse psychology.  People were thinking they couldn't possibly use a default username and password.  So therefore it was safe.



STEVE:  That's right.  They're going to have a super strong one, so don't even bother guessing.



PADRE:  Oh, my goodness.



STEVE:  But you're right.  It is disturbing to see them now taking a lot of actions for their own sake, for their own benefit, when they arguably weren't taking that kind of action on behalf of their 144, 143 million customers whose data was escaped.  And that has just happened.  We haven't started to see the consequence of that.  But that's probably what starts to happen next is when people actually, you know, certainly the people who are clued in, listen to this podcast, listen to the TWiT network, people who are tech savvy are recognizing they need to take some action.  And of course this did go mainstream.  So the news is out there.  Still, there will be many, many millions of people who don't actually follow through and are vulnerable to exploitation from this breach.  So that's unfortunate.



PADRE:  Eric Duckman in the chatroom was saying we've covered this on every TWiT show, can we stop?  And I understand that.  There's a little bit of fatigue.  However, it's amazing, every time we do this, how many people still haven't done the bare minimum.  The bare minimum is logging into a website, spending five minutes entering personally identifying information, and putting a fraud alert.  And you have to do that because at this point it's a race condition because, if I were to be looking to do something nefarious, I would find where they're selling these lists on the dark web, and I would start putting fraud alerts and putting my own number so I could lock people out of their finances.  I mean, that's a nightmare scenario.



STEVE:  And I think here's how I would respond to that is, yes, we're beating a dead horse.  But this is our responsibility.  So having done that, and this is the last time we'll mention it unless some other interesting information comes up, but now it's the responsibility of the people receiving this information, whether they want to follow through and take care of themselves or not.  So I have no problem coming back to this and saying, one more time, this is the liability that people are exposed to.



PADRE:  Right.  One quick note because I do have security friends who they kind of bristle anytime we make fun of the fact that Mauldin had a bachelor's degree in music appreciation.  That doesn't necessarily mean that she would be a bad security person.  I know people who have their degrees in the fine arts, and they are fantastic security people.  But I think, if you look into Susan Mauldin's background, what you find out is not only did she not study security, she didn't have any practical experience with it.



STEVE:  Correct.  And I would argue, I would counter the other security people who are bristling by saying, okay, so why is all evidence of her being removed from the Internet?  If they're proud of who she was, just leave her there.  I mean, we obviously all know her name.



PADRE:  Yeah, yeah.



STEVE:  So, okay.  So we talked a couple months ago about the EME, which has been adopted by the W3C, the World Wide Web Consortium.  And I want to follow through on that.  I have to read the letter that Cory posted, Cory Doctorow posted on the EFF site yesterday, and then we need to talk about it.  So, and he addresses this to Jeff, Tim, and colleagues.  Tim is certainly Tim Berners-Lee.  I don't know who Jeff would be [Dr. Jeffrey Jaffe, W3 CEO].  Did you have an idea?  That just didn't...



PADRE:  Yeah, no, I was looking at that.  That did not strike any memory for me.



STEVE:  Anyway, so Cory writes:  "In 2013, EFF was disappointed to learn that the W3C had taken on the project of standardizing Encrypted Media Extensions, an API whose sole function was to provide a first-class role for DRM [Digital Rights Management, we know] within the web browser ecosystem.  By doing so," Cory writes, "the organization offered the use of its patent pool, its staff support, and its moral authority to the idea that browsers can and should be designed to cede control over key aspects from users to remote parties."  So he's sort of laying down their fundamental gripe with the idea of standardizing encryption and media content delivery through the browser.



He says:  "When it became clear, following our formal objection, that the W3C's largest corporate members and leadership were wedded to this project, despite strong discontent from within the W3C membership and staff, their most important partners, and other supporters of the open web, we proposed a compromise."  And this is important because this is where we came down also on this a couple months ago, when we first covered this.



He writes:  "We agreed to stand down regarding the EME standard, provided that the W3C extend its existing IPR policies [Intellectual Property Rights] to deter members from using DRM laws in connection with the Encrypted Media Extensions" - and then he quotes a section of the U.S. Digital Millennium Copyright Act or European national implementations of Article 6 - "except in combination with another cause of action."  In other words, and this has always been our take, we must provide an exception that allows researchers and non-malicious attack of this protocol for everyone's benefit.



Anyway, and so I said in my notes here our listeners know quite well how every lesson we're learning shows us how clearly crucial it is that academic security researchers be allowed to have verification oversight over proprietary encryption systems upon which many people depend.  This must not be closed.



PADRE:  Right.



STEVE:  So Cory says:  "This covenant would allow the W3C's large corporate members to enforce their copyrights.  Indeed, it kept intact every legal right to which entertainment companies, DRM vendors, and their business partners can otherwise lay claim.  The compromise merely restricted their ability to use the W3C's DRM to shut down legitimate activities, like research and modifications, that required circumvention of DRM.  It would signal to the world that the W3C wanted to make a difference in how DRM was enforced; that it would use its authority to draw a line between the acceptability of DRM as an optional technology, as opposed to an excuse to undermine legitimate research and innovation."  And you can imagine where this is going because they're not happy.



He says:  "More directly, such a covenant would have helped protect the key stakeholders, present and future, who both depend on the openness of the web," and blah blah blah.  I'll skip the rest of this because it's more of the same.  Basically they are incredibly unhappy, and the EFF has resigned from their participation in the World Wide Web Consortium, where they have long been a member, over the fact that the way this came down, that covenant to protect research was withdrawn.  Well, I should say it's been shelved for some period of time, and there's a big concern now that essentially the rights holders' lobbying power has won, and that even the exemption for research is now in danger as this Encrypted Media Extensions moves toward standardization and widespread adoption.



PADRE:  You know what worries me about this story, Steve?  It's the fact that a lot of news agencies are selling this as, well, the EFF doesn't like the corporatization of this group.  That's not really what it's about.  As you've been trying to say, the clear analog here is how the CFAA, the Computer Fraud and Abuse Act, has been misused...



STEVE:  Yes.



PADRE:  ...since its inception.  I mean, yes, ostensibly it's to keep people from accessing systems that they shouldn't access.  But we have seen time and time again that researchers get brought to court because they point out a flaw in a system.  They were never attempting to exploit it.  They were very open about their research.  In fact, many of them offered their research in secret to the company before they went public.  And time and time again the CFAA was used to hit them over the head and say, well, no, you accessed our system without authorization and therefore your research is null and everyone should not pay attention to the huge gaping hole that you just pointed out in our security.



What the EFF is saying is let's not repeat that mistake.  That was a mistake in the first one.  It was an unforeseen consequence.  Why don't we write this one so that that doesn't happen?  That's the actual story.  It's not about whether or not corporations are involved or not.  It's about whether or not honest academic research can be had in a digital world.  And it boggles the mind why the W3C won't allow that.



STEVE:  Well, yeah.  And the problem is we all know that what the whole EME is trying to do is impossible anyway.  We have example after example of the problem being, if you need to decrypt for display, whether it's a DVD or an HDMI video signal, the decryption has to be there in order for the end-user to see the decrypted content.  Which means protecting it is impossible.  I mean, every single attempt to do this has failed, over and over and over, way back when it was a VHS tape, and they were trying to screw up the vertical blanking interval in order to prevent VHS tapes from being duplicated.  You just stuck a little box in there that reconstructed the vertical sync, and then you could copy the VHS tape.  I mean, it doesn't work.  Yet the content holders keep trying to force this on us.



And I was lamenting this when Leo and I were talking about it a month or two ago, that, for example, as a consequence of the encryption in the HDMI signal, when you switch around, once upon a time you were able to flip sources or flip destinations on your media system, and it was an immediate switch.  It would just jump between devices or between sources.  Now you sit there and wait for the encryption to do its resync and handshake and everything, and both ends to agree.  And then, after 10 seconds, finally you get a picture, if you're lucky.  And sometimes it sort of hiccups, and you have to go away and come back again.  It's just, I mean, there is a cost to this to the consumer of inconvenience, even for people who are playing by the rules.  It's just so frustrating.



And I get it that, if we didn't build this into the browser, then we would always be forced with add-ons, which arguably would be less secure, prone to failure.  I'd rather have there be, if we're going to be forced to have encrypted content delivery, and the browser is the container for that, I'd rather have it well done once than every content provider providing their own with lots of opportunities for problems.  So again, I think where the EFF came down was exactly right.  I hope that their withdrawal doesn't minimize the effort that they're able and the pressure that they're able to bring because we really do need academic researchers and security researchers protected from lawsuits which are too easily launched by media companies that don't want anyone poking around inside their code and their system.  We need it for security.



PADRE:  Steve, I'm with you.  That's what I want.  I just don't know if that's reasonable to expect.  I mean, we have an analog; right?  The analog is all of the discussions we've had, not just in the United States, but also in Europe, on encryption, about whether or not there should be a backdoor that government can access.  And we've had experts.  We've had computer security experts.  We've had mathematicians try to explain as simply as they can why you can't do that; why putting a backdoor into any encryption system will necessarily break it, bust it.  Not weaken it, but destroy it.  And yet you still have politicians and corporations sort of ignoring that and saying, well, but this is what we need.  I see the exact same thing with the W3C here, saying we understand that there needs to be an exception for academic research, but this is what we need.



STEVE:  Yup.



PADRE:  And now that the EFF has withdrawn from this consensus group, what's the next step?  There is no next step; right?  I mean, they've got no input.  The W3C is not going to reverse their course.  No one is going to have a voice for the consumer anymore.



STEVE:  No.



PADRE:  That's kind of depressing, Steve.  I don't know how to take that.



STEVE:  Yeah, so Cory's letter ends saying:  "We will renew our work to battle the media companies that fail to adapt videos for accessibility purposes, even though the W3C squandered the perfect moment to exact a promise to protect those who are doing that work for them."  He writes:  "We will defend those who are put in harm's way for blowing the whistle on defects in EME implementations."  And finally:  "It is a tragedy that we will be doing that without our friends at the W3C, and with the world believing that the pioneers and creators of the web no longer care about these matters.  Effective today, EFF is resigning from the W3C."



PADRE:  And it's a great letter, and it's an incredible sentiment, and he's absolutely right.  I just - I need to know what happens next.  I mean, it was the right thing for the EFF to pull out.  It does send a message.  It does now have a tension, eyeballs on this, maybe even in the mainstream media.  But what's the next step?  What needs to happen?  Is there something that could possibly happen that will reverse course for the W3C?



STEVE:  My feeling is that things like the DMCA need to be revisited.  I mean, that's the ultimate source of all of this problem is that that was improperly conceived.  And again, unfortunately, in the U.S. we keep seeing that our politics, the politics of our legislators, are for sale by well-moneyed lobbying groups that have proprietary interests behind them.  I don't know how we get on the other side of that.  I mean, it is a challenge.



But I think the only solution is to go back to the root of the problem, which is a fundamental misunderstanding about the - again, which we keep seeing demonstrated over and over about the need to create exemptions for academic research because this stuff is hard.  And prohibiting anyone from looking at it is not the way to make it better.  I mean, we keep seeing that.  I don't know how this can be made any more clear, but all we can do is hope and keep sending the message.  And speaking of...



PADRE:  I've been catching up on my - sorry.



STEVE:  Go ahead.



PADRE:  I was just going to say I've been catching up on my Black Hat and DEF CON talks because I missed them this year, and so I'm watching them on the Internet.  And there was one talk that was specifically on the DMCA.  And they were saying, what we've realized is that the DMCA has one purpose, and that's to put white hats in jail.



STEVE:  Yeah.



PADRE:  Because black hats, they're not going to be honest about their research.  They're not going to share it with anybody.  Gray hats, they're working in a datacenter somewhere, so as long as they keep their particular part of the Internet safe, they don't care.  White hats, their entire existence is to share knowledge.  And it's the sharing knowledge that gets them in trouble with the DMCA.



STEVE:  Yeah.  And if they don't responsibly disclose, then I can understand.



PADRE:  Absolutely.



STEVE:  But when they do, don't put them in jail.  It's like that teenager, was it in Budapest, who found that the new e-ticketing system had a problem?  I don't remember where it was.  And he got a visit a couple days later in the middle of the night and was arrested.  It's like, what?  I mean, he didn't even use the ticket that he was able to issue by this poorly engineered system, and he brought it to their attention.  And they said, oh, bad.  We don't want any bad news.  We're going to arrest you.



PADRE:  Right, right.  There has to be something like safe harbor for researchers, saying, if you stick to this protocol - so this is what you can do while you're researching, and this is how you responsibly disclose.  And as long as you stick to that protocol, you are legally in the clear.  That is how you get white hats to be white hats.  And that's how you encourage people to be responsible.  I mean, all of our shows are about encouraging responsible Internet citizenship.  Well, if you want responsible Internet citizenship, you need to provide some rights to responsible Internet citizens.



STEVE:  Yeah.



PADRE:  All right, Steve.  Bring me out of this funk.  Give me some good news.



STEVE:  Okay.  So I have some good news.  Does my audio sound as good to me as yours does to me?  I mean, wait.



PADRE:  You always sound good to me, Steve.



STEVE:  My audio sound as good to you as it does to me.  It just seems flawless.



PADRE:  Yeah.  This is the fun part about working with you.  It just brings out the best in everyone.  All right.  So we've got the EFF resigning from the W3C.



STEVE:  Yeah, we do.  However, the W3C did something good.  Just yesterday was the announcement that all of our browsers are onboard with a new API, which is known as the Payment Request API.  And I thought, uh-oh, what?  Because, I mean, there are so many ways this could be done wrong.  I thought, okay, what is this?  Okay.  So the good news is it looks like it's a really good thing.  We're all familiar with the annoyance of purchasing things online.



And, for example, I'm happy when I buy something with Amazon that I just say, yes, I want that tomorrow, and magically it appears.  Or if there's some used minicomputer on eBay, I go, oh, I have to have that, or a circuit board, or who knows what.  And I just, you know, I click on the button, and because eBay is tied into PayPal, it just, you know, something spins around a couple times, and then it says, okay, done.  So that's a nice transaction.  Of course, those are siloed instances.



Now, we do have, for example, in my case with LastPass, LastPass has my credit card information.  And so to some degree it works, if you trust LastPass with that information.  Because LastPass is already able to populate form fields, it's able to say, and it does bring up a little extra dialogue, "Are you sure you want this page to receive your payment information?"  And you say yes, and then it goes blip and all that, you know, it does as good a job as it can filling in a form.  And of course the problem is since there's no standardization of user-facing web payment, there's a problem with automating that process.



So that's what this new standard does.  And it looks like they did it, they thought it through, they understood the problem, and they came up with a nice compromise which is lightweight, which is really what we want, this Payment Request API.  So the way this works is, if you go to a site, and you navigate to the place where you would normally need to fill out the form, now the site is able to, using this Payment Request API, send something to your browser that says here is the payment information we're requesting.  The browser then itself, not an add-on, like just as I was giving the LastPass example, or whatever add-on you have, the browser itself is a database which would contain an instance of this information, obviously securely encrypted and protected.



So the website, this Payment Request API, allows the remote site to say here are the pieces of information we need.  The browser checks its database and presents to the user essentially a detail of what the site is asking for and gets the user's permission to forward that to the remote site.  So essentially this unifies and smoothes that process.  It unifies it because it creates an industry standard query and response mechanism for this class of information where the user is in the loop, saying yes, I want you to provide my relatively unchanging information to that site.  You simply say yes.  The remote server gets the information, and then it processes all of the payment with its own back end.



So this is just sort of a clean information provision system.  It doesn't change the payment flow in any fashion.  And, for example, if the site supports PayPal purchases on the back end, it would be able to tell the browser that it accepts credit cards or PayPal.  So then the user would be able to say, ah, yes, then that's what I want to use, and you select that option and confirm that you want the information provided, and it just happens.  So it'll take a while for this to get supported, first by the browsers and then certainly by the back end.



Since I wrote my own ecommerce system, I fully intend, as soon as this thing is mature, to send that query to visitors who are wanting to purchase a copy of SpinRite in order to smooth the process.  And it'll just be much easier to do the same things we've been doing, but essentially solve the problem that no two sites do this in exactly the same way.  It will allow a unification of that process.  So I think it's very welcome.



PADRE:  Okay.  So let me break this down a little bit.  Correct me if any of this is incorrect.  My browser acts as a database, so it's going to have my credit card information, maybe my PayPal account information, address, all that good stuff.



STEVE:  Zip codes, your address and so forth.



PADRE:  Zip codes.



STEVE:  Yeah, shipping information, billing information separately, yup.



PADRE:  All my sensitive information.



STEVE:  Yup.



PADRE:  If there is a request, a payment request from a site that's using this API, they will get a tokenized bit of data that allows them to have what they need for the payment that I select.  So instead of having enter your name and your credit card number, it'll say, hey, I've got these four different payment options.  Which ones do you want to use?  I can click one of those.  That automatically gets sent over.  But where is the database actually maintained?  So let's say I'm using Chrome.  I am accustomed to being able to log into my account on any of my computers, and I get full access to everything that browser might contain - the sites I visited, my history, my cookies, et cetera.  Will that still work?  And if it works, where is the actual database of personally sensitive information stored?



STEVE:  Well, it is in the browser.  And we can think of it as an extension of the "Remember my password for this site" sort of feature.  So, for example, if you don't have a third-party password manager, but for example you're using Chrome, and you just say yes, Chrome, remember my password for this site, that's stored.  And then if the browser uses cloud sync, then it's automatically synchronized among your other instances of Chrome, wherever you are.  So this information would be treated similarly.  So it's browser-side data.  And whereas right now the browser is responsible for recognizing a username and password field - and as we know, that mostly works, but kind of doesn't.



What these guys have done is they've said, okay, we're going to make this explicit.  We're going to remove any ambiguity.  We're not going to require that the browser use heuristics in order to realize, oh, look, there's a credit card field.  And as we've been talking, there have been some exploits, for example, where fields are being - malicious exploits where fields are being moved offscreen and filled in by the browser behind the user's back in order to allow sites to obtain information without the user's authorization.



So this is a response to that problem, too, so that essentially this will be the way that websites can query the browser in sort of the same way they do so implicitly, with username and password form field recognition.  This will surface it into an on-the-wire protocol that allows a browser to say, ooh, this site you're visiting has just asked for payment information.  And it offers these options.  Which one do you want to use?  And then you say "That one," and you say "Yes," and it just happens.



PADRE:  You know, if Google is smart here, they will take this payment API idea, and they will just integrate it into Google Wallet because Google Wallet, they've already designed it and semi-abandoned it to be sort of the place where you keep all your credit card and all your payment information.  If they made a couple of tweaks, so now that is automatically offered whenever payment is requested through this API, then they make Google Wallet compelling again, and they make it a reason for you to actually keep your payment information.  I have a Google Wallet, but I don't think I've updated the credit cards in there probably for five years.



STEVE:  Yeah.  And for what it's worth, Google is 100% behind this.



PADRE:  Yes.



STEVE:  One of the articles that I ran across when I saw this was that Google is gung-ho.  They're all - they have a nice page on their site, and they're up and running.  So they will certainly be among the early leaders.  And I'm sure that - who knows when Microsoft will do it, but they certainly will.  And I'm sure that Firefox will be right on it, too.



PADRE:  Right.  Now, we've got to play devil's advocate here.  Is there a downside?  Do you see right now an obvious venue to exploit this?  Because there will be malicious calls to the payments API, what do they have to do to make sure that this is going to be secure coming out of the gate?



STEVE:  Well, the obvious attack is on the browser's storage of this.  But that's present somewhere anyway.  I mean, our passwords are in the browser.  If it's remembering our passwords and filling them in for us, then it's in the browser.  And password managers, same sort of story.  So, I mean, again, ultimately there's a tradeoff that we always face between convenience and security.  And so if you want absolute maximum security, you don't let your browser ever have any of this information.  You always enter it in every time.



On the other hand, notice that one of the great risks is keystroke logging that we're always facing.  And this moves the information away from the UI.  That is, it's no longer something - you're not entering your 16-digit credit card number every time you want to purchase something.  That happens behind the scenes, which you could argue is a benefit for an obvious class of vulnerabilities, by just having it saying, oh, do you want to use the Visa card ending in 2319, and you say, oh, yes, I do.



Well, nobody watches you do any of this, whether it's keystroke logging or any other malware, gets all that information.  They just see, oh, something just got permitted because this is over TLS encrypted between your browser and the remote server.  So I don't really see a downside.  I don't see that we're opening up a new avenue of attack beyond what we have.  And I would argue that just smoothing the way for allowing browsers to securely interact with remote sites for ecommerce makes sense.



PADRE:  Right.  If you've got a compromised browser, you have a compromised browser.



STEVE:  Right.



PADRE:  Nothing is going to help you from that.  But you're right.  As long as it doesn't open up a new venue of attack, a new vector, then this is a net good.  I don't see anything wrong with this.



STEVE:  And we have the advantage of the web browser vendors clearly focusing on the security of this.  So they're going to do as good a job as possible to make sure that it's secure and safe.



PADRE:  Well, I mean, unlike making sure there's an exception for academic research, they have a vested interest in making sure the payments API works perfectly, the experience is good, it's secure, because that's money.  I mean, money talks.



STEVE:  Right, right.



PADRE:  Okay, now, Steve, here's a thing.  Can we jump in a time machine and go back, say three years?  I was a big proponent, when I was still helping out with IT for my organization, of enabling two-factor authentication.  And the big thing was, look, you all have phones.  We're going to set it up so that you get an SMS to give you the second factor.  It's super secure.  It's easy for you to remember because you're always carrying your phone.  You're never going to forget it.  You're always going to have access to that second factor.  I can't really say that anymore.  I can't tell people to use SMS as a second factor because, as it turns out, it's even more insecure than I thought it was.



STEVE:  Not only that, but it's not a second factor.  That's the crux of the problem.  Remember, the whole idea of multifactor authentication is an "and" conjunction between the factors, your password and a second factor, not "or," or not "instead of."  And what's happened is, over time, this thing has changed.  It's morphed into "I forgot my password, send me a text message."  That's not second-factor.  That's multiple first factors joined with an "or," rather than an "and."  And it's weaker by definition.  I mean, it's clearly weaker.  If you have more different things you can use to authenticate, and any one of them can be used, then you immediately reduce the security of the whole system to the weakest among them.



And now we see that, thanks to the problems with the Signaling System 7, which is the international system that glues all of our telecom providers together, due to that it is very vulnerable to interception.  So what's happened is we've moved away from using this second factor as additional security to replacement security.  And so thus what is in the news recently is that yet another set of researchers have demonstrated how people's bitcoin, I think it was Coinbase Wallet, an account could be compromised by using the SMS second-factor on Gmail for account recovery.  It should not be account recovery.  It should be additional password verification, the idea being that you want to prevent password being brute-forced.  So you say, oh, they know the password, but now they also have to, rather than, oh, they forgot their password, so instead.  So yes.  It's not a second factor.



PADRE:  So it's a 1.1 factor.  Maybe not even .1, no.



STEVE:  It's a .5 because it's less because you've reduced it to any of these can be used.  And one of them is, I mean, this is worse than a password.



PADRE:  Yes.



STEVE:  Password you at least have to try, you have to brute force.  It might be a really good password, and then they can't get in.  Now it's intercept the SMS message on the fly, and that's all you need in order to do multiple different accounts.  It doesn't even matter if you have different passwords on your Gmail and your Coinbase account.  No, because they're both using SMS.  So one single exploit allows them to intercept the text messages being sent out.  It's ended up the whole system has just collapsed because it ended up being misused.  It isn't multiple factor.  It's semi-factor.



PADRE:  I've been following a project from a maker that I know.  I met him at DEF CON, like six or seven years ago.  He has created what is essentially a dirt box using a software-defined radio that is about the size of a laptop, maybe like an old Toshiba laptop, and it's got all - the power supply and everything that he needs to turn this thing on and start intercepting.  And so I was asking about, well, so what would you intercept with it?  You would intercept calls.  He goes, no, calls are boring.  I don't care about calls.  What I want is I want messages because messages are easier to parse, they're easier for me to filter, and it's easier for me to look for exactly what I'm looking for.



And he couldn't turn the thing on because of course you're getting in trouble any time you want to do that.  And he actually is a bona fide researcher.  But he was trying to show me how easy it would be for him to, say, walk into an office and just wait for SMS messages to start rolling into his dirt box.  And that's a little scary, honestly, because it could happen in real-time.  This is not one of those hacks that takes someone time to set up or maybe social engineer themselves into a place where they can get in the middle.  This is literally someone walking by your office, and 30 seconds later he's got a data file filled with in-the-clear passwords.  That's - yeah.  So what's the solution?



STEVE:  So I began talking about this when I was setting up my Hover account.  I moved my domain names over to Hover.  And while I was setting it up I wanted maximum security, and they gave me these options.  And immediately it was clear to me from our own coverage on this podcast:  SMS, bad; the time-based one-time password, good.  And so back then, at the time, we talked about this.  And I said it is time for us to abandon SMS.  That is, it is a vulnerable system which is no longer trustworthy.  And at least someone like Hover is using it for multifactor, not alternative factor.  But even so, what we want is we want to use the authenticator app that generates a time-based token.



In that case, the sensitive cryptographic information is only exchanged once.  It's on your browser.  Everyone knows that I like printing out the QR code.  I have a little sheaf of printouts of the QR codes of all of my different time-based tokens.  And so when I'm setting up a new device, I just run through, literally, the paperwork and snap a picture of each QR code in order to import the keying for the time-based token into the authenticator app, and then I'm good to go.  But that's where we need to head.



And the disturbing thing is that, for example, even if you set things up that way with Twitter, Twitter still wants to send you an SMS message just because they're biased towards ease of use.  They don't want tech support calls.  They don't want people saying, oh, I'm using a new phone, and it doesn't have my authenticator.  And it's like, okay, well, then we're going to send you an SMS message.  It's like, no, because that's a back way in, a backdoor, essentially, into your password recovery if you don't have the password.  So anyway, we need - it's going to take a while because everyone thinks this is, oh, super security.  No, it's worse than what we had when you had to brute-force a password.  Now it's like, oh, I forgot my password, send me a message.



PADRE:  Now, "HiQ1" in the chatroom does point out that you can turn off SMS authentication with Google.  And actually I have.  I'm not sure how much better the new system is.  So the way it works is anytime I log in on a new browser - and I actually have mine set in to time out every three days.  So every three days, any device that wants to access anything from Google, so that could be my YouTube account, my Gmail account, it could just be the browser, when I log in, I get a pop-up.  It's not a text, but it's a pop-up saying, hey, this device with this name has just requested access.  Do you want to grant it?  And it's just  literally a yes or a no.  That's not SMS; right?  That's something else.  That's some other protocol. 



STEVE:  Yeah, that's their own backend messaging technology.



PADRE:  As far as you know, is that secure?



STEVE:  Yes, because it's TLS, it's authenticated, and it's over the Internet, not the telephone system.  And so it's the telephone system that is the problem because this Signaling System 7 is old.  And there's no authentication in SS7, none, which means it's completely vulnerable to man-in-the-middle attacks.  There's no verification of the endpoint identities among telecom providers.  It's like the early Internet.  And because we've layered all of the authentication on top of the underlying protocol of the Internet in order to get HTTPS with certificates, but the telco systems never did that.



PADRE:  Right.  I will say that they also give me a backup because of course the fear is, well, what happens if I break my device, or I don't have my device?  My device is stolen?  Now I'm locked out.  You're going to hate me for this, Steve, because this is about as insecure as you can get.  In my wallet right now there is a card that is printed.  It has one, one-time-use authentication.  Now, you still have to have my username and password, so it's not - it doesn't get you in.  But it does...



STEVE:  That's very good.



PADRE:  It does mean that one time, one time I can get in and maybe print out another one-time authentication code.  But even if I don't have this device, I can still get into my data.



STEVE:  Yup.



PADRE:  Again, but the wallet's probably not the most secure place to put that.



STEVE:  I don't know.  I mean, Bruce Schneier is famous for saying that you should have complex passwords and write them down because we are good with managing little bits of paper, as Bruce put it.  And it's true, you don't want to write them on the underside of your keyboard because that's now the first place everyone looks.  But still, no one's rummaging through my wallet.  And so, yeah, you have to have your username, your password, and something else.  I think your solution is bulletproof.



PADRE:  There was a company at CES, and I wanted to do a video with them, but it just seemed so strange.  They were making a product specifically for this.  So the way it would work is you would print on a very special piece of paper, and it would go into like this laminate pouch.  And then you'd crack something, and it now stays in your wallet.  It still looks like a business card.  To access the code, you had to tear open the seal so you could see it, and the print would fade after 10 minutes.  And I was thinking, okay, that's really geeky, super impractical, probably doesn't actually add any security because I can still write it down.



STEVE:  No, because we all have cameras.  We all have cameras.  You just take a picture of it.



PADRE:  But I thought, that would make for 30 seconds of really cool video.



STEVE:  Agreed.



PADRE:  It's almost the "Mission Impos-" - I wanted it to self-destruct, honestly.  I was like, oh, the ink just fades?  I wanted it to burst into flames or something.



STEVE:  Sort of like your MRE heating up the other day.



PADRE:  Oh, my gosh.  Steve, I have to say this.  If any of you are having gastric problems, eat an MRE, and you'll be bound up for a couple of days.  They're still better than the ones they had in World War II, but oh, my gosh, there's so much salt.



STEVE:  Yeah.



PADRE:  You know, the next time you come into the studio, we actually have a box of MREs.



STEVE:  No.  No.



PADRE:  No?



STEVE:  No.



PADRE:  You don't want to test internal security?



STEVE:  Our listeners, well, and the industry at large was very upset over this CCleaner breach, which was recently exposed.  For a period of four weeks - and the good news is there's a way to tell if you got bit by this.  For a period of four weeks, from the middle of August, so a month ago, through September 12th, the downloadable 32-bit version of CCleaner, v5.33, was compromised with the Floxif, F-L-O-X-I-F, Floxif malware, which infects Windows executables and DLLs, backdooring the machine to install additional malware.  2.27 million CCleaner users inadvertently downloaded that 32-bit version of CCleaner v5.33 during that one-month window.



Okay.  So the earlier versions were fine.  It was updated to 5.34 on September 12th, closing that window.  Yet 2.27 million copies of that were downloaded.  In the show notes I provide a registry key.  So our more tech-savvy listeners, if you look under HKEY_LOCAL_MACHINE, then the software folder, and then Piriform, those are the publishers of CCleaner, P-I-R-I-F-O-R-M.  If you have a key under there, Agomo, that's not good.  That's the key that this Floxif malware creates.  And under there are two data values, an MUID and a TCID, which are used by the installed Floxif infection.  So the bad news is that CCleaner does not proactively remove this.



So I just wanted to say, again, I mean, I've got CCleaner.  The good news is I didn't download an update or a copy of it.  CCleaner does not have an auto update facility, so you do need to get a newer version.  Bu you definitely want to make sure, if you think you may have obtained an update or a download during this window from the middle of last month to the middle of this month, then you want to make sure that you didn't get this stuff installed in your machine.



What was found was - and this was found by the Cisco Talos security group.  They were testing some of their own antimalware software, and alarms went off, to their surprise, caused by CCleaner.  They then dug into it and discovered that the alarm was going off because of this Floxif malware.  What happened was that the distribution server was compromised, and this was added to the download.



PADRE:  Well, how did they sign it?



STEVE:  It wasn't.  It was added to.  So the CCleaner was properly signed, and the signature validates.  But there was another chunk that went along with it that was the infection, and it was not signed.  But it still you know, someone clicked yes when they were installing it because they thought they were just installing a trustworthy copy of CCleaner, but it was bringing something additional along for the ride.



PADRE:  I mean, yes, that's evil, and it's horrible.  But props to whoever thought of that because, yeah, I mean, if you download something like CCleaner, which has a reputation, is a legitimate piece of software, it's security software...



STEVE:  It's a great utility, yes.



PADRE:  Great utility.  And you attach something to it, they're still going to think it's CCleaner.  They still see that it's signed.  And anything that pops up during the installation process, they're going to assume it's legit, it's valid.



STEVE:  Right, right.  So anyway, the takeaway is, if you think you may have been exposed, you can check that registry key because this Floxif infection will create a key there.  And if you've had CCleaner from before and haven't updated it through that window, you're probably fine.  And so, yikes.  But it happens.  The Piriform guys, I wasn't really happy with their disclosure.  They worked a lot to minimize the threat.  They believe nobody was compromised.  It's worth mentioning that you don't want this thing in your machine.



But what the Talos guys found was a dynamic domain name generator so that this would be in the future looking for domains which would be registered for the command-and-control server, where it would connect up.  The server was brought down.  It is believed that none of these infections were ever able to contact the mothership in order to get instructions and then download other things.  But you definitely want this Floxif stuff off of your machine.  So everyone should just make sure, if you think you may have been open to downloading it during that time, that you're sure it's not there. 



PADRE:  And as far as I can tell from the bulletin, it is only the 32-bit version; right?



STEVE:  Yes, yes, for sure.  Oh, and there is a cloud version that is also susceptible.  So it's the CCleaner Cloud had the same - during the same window of opportunity, that same one month, either of those could have been a problem.



PADRE:  Now, JammerB, if you go ahead and go to my screen, so this is - I just reformatted this machine, so I don't have it yet.  But you would say it would be under LOCAL_MACHINE and then software.  If you have CCleaner, there would be a new key here for Piriform.



STEVE:  Piriform, yes.



PADRE:  And what am I looking for again?



STEVE:  And then under there is Agomo, A-G-O-M-O.  Normally under Piriform is just CCleaner.  But there'll be a second entry, Agomo, A-G-O-M-O, which is not from Piriform.



PADRE:  Got it.



STEVE:  And that'll have two other keys, an MUID and a TCID, values.



PADRE:  And that tells you that, yeah, it has installed itself.



STEVE:  You've got to do some cleanup.  And you want to look around for, like, Floxif remover stuff.  Probably Malwarebytes or something will scrub it right off.



PADRE:  Question, Steve.  Do you know where I could get a copy of the infected file?  Because I'd love to install it and just see what it does, maybe just watch the traffic, see if it's phoning home.



STEVE:  I don't know, but I'm sure it's around.



PADRE:  That could be fun.  That could be a Know How.



STEVE:  Yeah.  So one more, and then we'll do our last break.  A security research company, Armis Incorporated in Palo Alto, has found and responsibly disclosed some time ago, which is why we're now learning of it, a series of very worrisome Bluetooth-based compromises and exploits.  They're calling it BlueBorne, sort of as in airborne, but in this case using Bluetooth.  Our listeners know for years my advice on the podcast has been always turn off radios you don't need.  And I know that this advice has been heard because iOS has had this habit of always reenabling Bluetooth whenever you update any iOS device, just like, oh, you must have turned that off by mistake.  No.  I turned it off on purpose because I don't - there's nothing, no Bluetooth thing that I need my phone connected to, thank you very much.  But Apple always turns it back on.



So these guys did a deep dive into the Bluetooth stacks throughout the industry and found and responsibly disclosed a large number of zero-day vulnerabilities that is just - it's breathtaking in scope.  Basically, all Bluetooth platforms - including iOS, but not recently, props to Apple - had these problems.  So in their coordinated disclosure, Google was contacted back on April 19th of this year, after which details were shared, and Google immediately addressed the problems and pushed security bulletins and updates out earlier this month on September 4th.



Microsoft, same thing, contacted back on April 19th, details were shared, updates were made back in July, and then public disclosures in the middle of this month, like about a week ago.  Apple was contacted on August 9th because there were no vulnerabilities, and I think it was from - I think iOS 9 may have been, but 10 and now today's update to 11, Apple was completely clean.  Sadly, Samsung was contacted on three separate occasions, in April, May, and June; never received a response.  These guys were never able to get a response back from Samsung, whose head is apparently deeply buried in the sand because they're rife with exploitable vulnerabilities in their Bluetooth stack.



Linux, the Linux Project was contacted back in August, around the middle of August, and they jumped on this and fixed their problems.  So, I mean, across the board - Windows, Linux, the mobile platforms - all vulnerable to a series of very serious attacks.  The Bluetooth stack runs down in the kernel.  It's got lots of privileges.  There are remote code exploits.  And, I mean, it's like as bad as it can get.  Also, these attacks work without needing Bluetooth association.  So it's just the fact that the radio is on and listening.  A non-associated attacker within 30 feet is able to compromise a device that has not been patched that has Bluetooth on.  So if you've got a Samsung device, turn Bluetooth off because those jokers have not responded to any of this.



So anyway, I just did want to let everybody know.  The good news is it was handled, as long as you're keeping your devices up to date.  As long as you're using Google and you're current, and Windows and Linux and Apple's iOSes from 10 on, you're fine.



PADRE:  Unless you own a Samsung device.



STEVE:  Samsung, not so much, yes.  Yikes.



PADRE:  Well, that's what so nefarious and so impactful about this story.  The vulnerabilities that they're targeting are typically the older versions of Bluetooth, which we keep because that's the compatibility.  We all have devices from way back when.



STEVE:  Yup, yup.



PADRE:  And so, yes, it's easy for me to update this.  In July, Microsoft updated Windows, so that's not going to affect me anymore.  Same thing with this.  This is a OnePlus 5.  This actually came patched, pre-patched.  But there are also millions of devices that are using older versions of Bluetooth that will never be patched.  Think of how many car stereos have Bluetooth.



STEVE:  Televisions, televisions.  Yes, yes, yes.



PADRE:  They will never be patched, ever.  Now, what you can do with that depends on how they actually designed the entertainment system in the vehicle into the actual controlled area network of the car.



STEVE:  [Sounds of distress] Yeah.



PADRE:  But, I mean, that's, yeah, this is a vulnerability that will keep giving for decades.



STEVE:  And this is the problem is that we have a very sophisticated protocol which is hard to get right.



PADRE:  Yes.



STEVE:  And we keep tweaking it and adding to it and extending it, and mistakes get made.  But unfortunately, exactly as you said, there is an ecosystem of dynamic patching which affects some of these devices, but certainly not all of them.



PADRE:  I actually had a family member who read about BlueBorne, and she called me in a panic.  And she says, oh, I have a car, and it's got Bluetooth audio, and I keep my phone on so it automatically syncs up every time.  And I said, okay, well, there's a few things.  First, turn off Bluetooth on your phone.  You don't use it for anything else.  And then I was about to say, well, just go ahead and use a 3.5mm jack to get to - and then I realized, oh, you've got an iPhone.  Okay, so that's not going to work.  Get yourself a dongle, that will work.  But then I'm like, the Bluetooth on the stereo system, I was going over the model that she had, it can't be turned off.  There is no setting to turn it off, so it will always be on and always be vulnerable.  Forever.



STEVE:  Wow.



PADRE:  So she just needs a new car.



STEVE:  Yeah.



PADRE:  Easy, yeah.  Fixed, fixed.



STEVE:  Or an old car.



PADRE:  And old car like mine, which just has the 3.5mm jack.



STEVE:  Like mine, like mine.  I have no technology.



PADRE:  Well, I've got the little OBD-II bus, and that's plugged into a laptop so I can monitor what's going on with my engine.  But yeah, I don't want - especially since we've heard that some of the high-end automakers have done wonderful security things like allowing cruise control and the brakes to be controlled by the entertainment system.  Which sounds like a really good idea.



STEVE:  What could possibly go wrong?



PADRE:  All right.  Let's get away from BlueBorne, let's get away from wireless exploits, and let's get back to good old-fashioned HTTPS.



STEVE:  Well, this is just a quickie.  Someone, one of our listeners thought I would be interested in this and forwarded it to me, and it's very cool.  One of the problems that users of the iOS platform have is that it's so closed.  You can't see what's going on.  And then there's a reason for that.  Most users don't care what's going on.  They wouldn't know how to interpret what they saw if they could see more of what's going on.  So, fine.  Except we're not all those users.  Certainly not listeners of this podcast.



There is a very neat extension named Inspect, just I-N-S-P-E-C-T, for iOS devices.  I've got the link to it in the show notes.  It allows you to inspect the HTTPS certificate of any site you're visiting.  So it uses the familiar UI, where you send something somewhere, like you copy the page, you send it to email, you send it to message or so forth.  This adds a "send it to inspect."  And so what happens is the Inspect add-on captures the URL and displays full certificate information.  So anyway, just a cool little gizmo.  Again, nothing you need, but there's no other way to get that information when you're just using the standard Apple Safari browser.  This allows you to check out the certs of the sites you're visiting.  So I just wanted to put it on people's radar because I know that some of our listeners would think that was a cool add-on.



PADRE:  I'll actually say that that's a big deal.  For years now you've been telling people to inspect the certs, just check it out, make sure everything's on the up and up.  If you didn't have a tool to do that, then that's not usable advice.  Now you can.  



STEVE:  Well, and sometimes you'll get a warning where it's like a go, no go.  It's like, oh, there's a problem, but it doesn't - you get no details.  And so how can you make an informed decision about whether you want to use it anyway?  For example, what if the certificate expired last night at midnight?  So it's like, okay, well, I really want to go to this site, but their cert expired 12 hours ago.  So it's probably fine.  They just forgot to renew their certificate.  So this would allow you to see what the problem was and then make a decision about what your behavior should be after you have that information.  So seems useful to me.



PADRE:  Well, Steve, this next item is about Android.  This actually affects me because I have multiple backup devices I keep on hot standby.  Maybe I'll power them up every once in a while to make sure that they still work, make sure that they're updated.  But there's a new policy at Google that might make that a bit more important to power up every once in a while.



STEVE:  Yeah.  And I just wanted to, again, sort of in the same way, put it on our listeners' radar.  I wasn't aware of this, either.  And that is that Google will purge your Google Android device backups if you haven't used them in 60 days.  So there is an expiration notice that will show after a couple weeks.  It'll warn you that you've only got X, like 54 days left.  But if you're not - so the point is that you cannot rely on a backup of your device persisting longer than two months from your last use of the device.



Again, it's not like the device is going to die, or that everything's going to disappear.  But again, your backup will be expunged from Google's cloud after 60 days of non-use.  So again, just something to keep in mind.  You cannot rely on it being there forever.  Which is weird, too, because in doing some investigation of this, people are - it's not like their storage is over commit or over limit or anything.  They've got lots of available Google cloud space.  But Google's just saying, I don't know, we don't want to store something for I guess a device maybe that they assume is retired.  Be nice if it was more like 180 days than 60.



PADRE:  Right, right.  And I understand what Google's doing because they're probably looking at their datacenters and realizing, over half this stuff are for devices that no longer exist.



STEVE:  Yes.  



PADRE:  Do we really need it.



STEVE:  Exactly.



PADRE:  Although, when you wrote down the Google "purge," I was thinking that for 24 hours all apps can be downloaded to your phone, and nothing will be illegal or something like that.  It's a really bad horror movie.  Wait, did you catch that reference?



STEVE:  It actually had a sequel.  They made a second one.  It's worse than the five Sharknado movies.  It's like, oh, goodness.



PADRE:  But it made money, Steve.  It made money, so they'll make another one.



STEVE:  Yeah.  All right.



PADRE:  Know what I want to do?  When's the last time I did Security Now! with you?  It's got to be months; right?



STEVE:  Has been months.



PADRE:  Well, I haven't talked about SpinRite in months, and that's just not right.



STEVE:  Well, and I thought of you because I got a tweet from someone whose handle is @FoodLovingProg, so I guess that's programmer.  And he said:  "@SGgrc Your 'stache is back."  And it's like, yeah, it's kind of coming back.  I'm not sure what I'm going to do.  And he said:  "Do you reckon I'd be able to recover a broken Nexus 5X phone with SpinRite?  Could I get non-encrypted data off it?"  And I was reminded of you because, first of all, there's a good chance.  And then I was recalling that you had had some success in using SpinRite to recover from a phone.  So since you were here, I figured I'd let you remind us.



PADRE:  Yeah.  So it was with an old OnePlus 1.  And unfortunately it doesn't work on all OnePlus 1s.  It depends on whether or not you've updated the software because on some of the early versions there was more or less a USB dumb mode that you could put it into, where it literally looked like a USB drive.  There was no connector.  



STEVE:  And that's what SpinRite would see, and it would be able to treat it like a drive and thereby recover the lost data.



PADRE:  Precisely.  Unfortunately, in some future versions, because they added a couple of features, especially in encryption, it looks like a USB drive, but only once you install some adapter software.  And if you have that version of the operating system, it just won't work anymore because it no longer looks like a plain USB drive.  It only looks like a USB drive if you have that piece of software, and SpinRite can't use that piece of software.



STEVE:  Right.



PADRE:  So same thing for the Nexus.



STEVE:  So the takeaway would be, if your device looks like a USB storage device, or if you can put it into storage mode so that plugging it into a computer sees it like a drive, then yes.  SpinRite will be able to treat it like a drive and recover the data in the way it does.



PADRE:  Right.  Unfortunately, the way the tweet seemed to be worded is that it's a broken Nexus.  And if it's a broken Nexus, then I don't see how you could flip it into USB mode.  If it doesn't even power up...



STEVE:  Ah, right.



PADRE:  Unless you want to - you could desolder the chips from the phone.



STEVE:  No, no, no.



PADRE:  Yeah.  I mean, SpinRite does many things, but wirelessly reach into dead devices, that's not one of them.



STEVE:  Well, yes.  And I've often said that what people are succeeding in doing, and we're often reporting, is that drives are dying, and they're using SpinRite to bring them back alive.  And sometimes they're pushing their luck.  It's like, oh, now it only lasts three months and I have to run SpinRite again.  Oh, now only two months.  It's like, okay, look.  Ultimately SpinRite is going to fail at this job because, if the drive absolutely refuses to continue being a drive, it gets the final vote.  So yes, by all means, use SpinRite to pull yourself back from the brink.  But limit the number of times you do that because ultimately it's just going to say, sorry, we're now a doorstop here.



PADRE:  I actually had an email exchange with a fan of the TWiT TV network, and she was upset because she said, you know, "You've been advocating SpinRite for so long, and it doesn't work."  And I wrote her back.  I'm like, "Well, what's the problem?"  And she said, "Well, I've got this drive, and about 18 months ago it had a problem, and on your advice I used SpinRite, and it came back.  But every time it comes back it lasts less and less.  Now I have to run SpinRite every two or three days."  I'm thinking...



STEVE:  Oh.



PADRE:  And I'm like, oh, okay, I think I've spotted the problem.



STEVE:  Perfect example, Father.  Perfect.



PADRE:  Again, it's a great piece of software.  It's not magic.  It just seems like magic.



STEVE:  It'll save your butt over and over and over, but not infinitely.



PADRE:  Right, right.  Unlike the Dollar Shave Club butt wipes.



STEVE:  Oh, I can't believe...



PADRE:  That's a callback, Steve.  I learned that from Leo.



STEVE:  Yeah.



PADRE:  All right.  Steve, we do need to talk about the main topic, and that is Apple's wonderful, brilliant cookie solution. What did they do that has you so excited?



STEVE:  Okay.  This is why I'm excited.  First of all, it's been a constant recurring theme, probably for all 12-plus years of the podcast because, as I've explained it, we know why cookies were created.  Cookies were created because browser transactions have no memory.  That is, a browser says "Give me this page," and the website gives the browser back the page.  Then the user clicks on something, and that's an entirely separate event.  Now the browser says "Give me this page," and then the server gives it back that page.



So cookies were designed back by Netscape in the beginning, in the dawn of browsing, to create some cross-query connection so that, when the server sent back the page, it also gave the browser a cookie, just an arbitrary token, any string, such that subsequent queries made by the browser would send that cookie back.  That created the possibility of session persistence.  That's the way, that's entirely the way we are able to log onto a site is that your browser now has a nonce, this token.  Then you say, oh, here's my username and password.  You feed those in, and now this site associates your account with this token, and you are logged onto the site, entirely because of this mechanism of the cookie.



The problem is, as with the evolution of the 'Net and the cleverness, we introduced the notion of third parties, that is, you and the server being the first and second parties.  Now there's a third party because, when the server returns your page, the browser will reach out to retrieve resources from wherever, not only the same server, but also a third party's, many times now, ads.  And those ads are able to provide your browser with a token, which the browser will dutifully save and return every time the browser requests an asset from that same domain, from the advertising domain.  And that's where we get tracking is that then, as you move around to different first-party servers, which all present ads from the same third-party site, the same cookie gets returned by your browser, which allows the advertiser to track your movements around the 'Net.



And my biggest problem with this is that it's never really been shown that this is useful.  I mean, what it is tends to be unnerving because many people have anecdotal reports about how they were over on Amazon looking around for, I don't know what, tennis shoes or something.  Then they go somewhere completely different, and they start seeing tennis shoe ads.  So it's like, whoa, you know, I mean, it's like we have this illusion that, because this tracking is all underneath, it's behind the scenes, it's unseen, unless you add tracking awareness add-ons to your browser, in which case it's really in your face.  You get a real sense for just how much of this is going on.



So this has been the problem that we've been facing for a long time.  We've talked about NoScript had some cookie blocking.  We've talked about turning off third-party cookies so that ads don't get those.  We've talked about the do-not-track beacon, the DNT, which was hopeful for a while, but it was voluntary, and the advertisers didn't want to support it.  So that sort of fell by the wayside.  Then we've got things like uBlock Origin that gives power users more control.  With, what was it, iOS 10 we began to get in iOS apps the explicit support for third-party ad and cookie controls with add-ons.



Okay.  So now, as of this morning, all my iOS devices are updated now to iOS 11.  It was a busy morning, and lots of multi-gig downloads.  Apple has done something amazing.  They call it "Intelligent Tracking Prevention."  There is a link in the show notes, but I need to take us through this.  We have time.  And our listeners will understand what this means.  So I'm quoting now from Apple's disclosure on WebKit.org of Intelligent Tracking Prevention.



They said:  "Intelligent Tracking Prevention collects statistics on resource loads, as well as user interactions such as taps, clicks, and text entries.  The statistics are put into buckets per top privately controlled domain," or what they refer to as "TLD+1."  Okay.  So for example, as we know, TLD is like .com or .gov or .org or .net.  TLD+1 is one level down, like TWiT.tv or GRC.com.  So it's the privately controlled domain.



"A machine learning model," they write, "is used to classify which top privately controlled domains have the ability to track the user cross-site, based on the collected statistics."  Okay.  So that means that they've implemented a heuristics monitor which will see this third-party advertising tracking happening.  Again, remember, add-ons could do that.  Attentive users could do that.  Apple has built this in.



Based on the collected statistics:  "Out of the various statistics collected," they say, "three vectors turned out to have strong signal for classification based on current tracking practices:  sub-resource under number of unique domains, sub-frame under number of unique domains, and the number of unique domains redirected to.  All data collection and classification happens on the device."  So it's all done locally.



Then they say, under "Actions Taken After Classification" - so first there's this classification.  Then they say:  "Let's say Intelligent Tracking Prevention classifies example.com as having the ability to track the user cross-site.  What happens from that point?  If the user has not interacted with example.com [itself, that is, as a first party] in the last 30 days, example.com website data and cookies are immediately purged and continue to be purged if new data is added."  That's the key.  Meaning that this heuristically determines if you are actively visiting domains.  And if you are not, if you haven't for a month, that is, if you don't go to a third-party advertising domain deliberately after a month, those cookies are removed.  You stop being tracked by places you don't proactively visit as a first party.



PADRE:  Now, Steve, that's an important part because a lot of people don't realize that's happening in the background.  Even if I'm not actively using that cookie, that doesn't mean that new information and new data isn't being added to that cookie.  In fact, that's a very popular way for advertisers to maintain a cookie indefinitely.



STEVE:  Oh, and believe me, they're not happy about this.  We'll get there in a minute.



So Apple says:  "However, if the user interacts with example.com as the top domain" - that is, as a first party - "often referred to as a first-party domain, Intelligent Tracking Prevention considers it a signal that the user is interested in the website and temporarily adjusts its behavior as depicted in the timeline."  And there's a shot that you're showing on the screen now, Padre, which sort of explains how this works.  And essentially, if the user interacted with example.com the last 24 hours, its cookies will be available when example.com is a third party.  This allows for "Sign in with my X account on Y" login scenarios.



In other words, they had to figure out how not to break the log in with Google, log in with Facebook and so forth, the various OAuth flows which are third-party login technologies.  I don't like them because they're also, as we've often discussed here, a privacy problem, since those people you're logging on with know where you're logging on with them.  That's part and parcel with it.  But it is a popular means of authenticating now, until we have something better, and we all know what that may be someday.



"This means users only have long-term persistent cookies and website data from the sites they actually interact with, and tracking data is removed proactively as they browse the web."  Yay.



And, finally, under what they call "Partition Cookies," they say:  "If the user interacted with example.com in the last 30 days, but not the last 24 hours" - so there's like a middle ground - "example.com gets to keep its cookies, but they will be partitioned.  'Partitioned' means third parties get unique, isolated storage per top privately controlled domain or TLD," that is, tracking is blocked, which is brilliant.  So they say:  "For example, account.example.com and www.example.com share the partition of example.com."



They say:  "This makes sure users stay logged in, even if they only visit a site occasionally, while restricting the use of cookies for cross-site tracking."  And then they say:  "Note that WebKit already partitions caches and HTML5 storage for all third-party domains."  So this is just - this is a wonderful, automatic, background, low-impact, privacy-enforcing heuristic that iOS 11, as of today, brings to the Apple platform.  And not surprisingly, advertisers are freaking out.  There are six major industry advertising consortiums that have just published an open letter blasting the new tracking restrictions Apple has unveiled and just implemented today, saying that they are, quote, "deeply concerned," unquote.  And just two paragraphs I'll read, just so you get a sense for this.



They said:  "The infrastructure" - this is the advertisers.  "The infrastructure of the modern Internet depends on consistent and generally applicable standards for cookies, so digital companies can innovate to build content, services, and advertising that are personalized for users and remember their visits.  Apple's Safari move breaks those standards and replaces them with an amorphous set of shifting rules that will hurt the user experience and sabotage the economic model for the Internet.



"Apple's unilateral and heavy-handed approach is bad for consumer choice and bad for the ad-supported online content and services consumers love."  Oh, yes, we love those.  "Blocking cookies in this manner will drive a wedge between brands and their customers and will make advertising more generic" - oh, darn - "and less timely and useful."  Oh, double-darn.



PADRE:  Ninety-nine percent of the time, if you hear a release from any corporation or consortium that talks about consumer choice, they're normally talking about the opposite of consumer choice.



STEVE:  And notice they still serve us ads.  Everything still works.  Ads will be there, impressions and clicks and all that.  They just won't be able to as easily compile massive portfolios of our identity, which they're selling behind our backs and saying, oh, it's all anonymized.  No, it's not.  Not at all.



PADRE:  Well, that's the thing about the solution.  If they're only doing what they are publicly claiming that they are doing, which is serving you more relevant ads...



STEVE:  This won't hurt them.  This won't hurt them.



PADRE:  Won't hurt them at all.  However, if they're one of these businesses that realizes that the data collection business can actually be more lucrative than the ad-serving business, then yeah, this is going to put a huge dent in their wallet.  And guess what, it should.  And they shouldn't have that.  That should not be a source of revenue for them because I have not given them that permission.



Now, I love this idea of buckets.  In other words, yeah, your cookie can still work.  It just can't see any of the other cookies.  And it can't see any of the other activity.  And if you are being honest and saying you just want to make sure that when they come to your site that they get the experience that they've set up, then that still works just fine.  Wow, you know, I can't see this approach not being copied by Google and Microsoft.  This is going to be the norm; yes?



STEVE:  The only way around this is if we start seeing redirects to the advertisers, which would be annoying.  That is, you click on a link, and the site you're visiting redirects you to DoubleClick.net, which then redirects you back because then that's a first-party visit which is transient, but that would serve to refresh your DoubleClick.net first-party presence.  And again, this is Spy vs. Spy or cat and mouse.  So if that started to happen, if there was an explicit attempt to work around this, then I'm sure Apple and WebKit would adjust in order to say, oh, those don't count, bouncing through, doing a non-user-driven redirect through what would nominally be a third-party site to allow it to register itself as a first-party domain.  It's like, okay, that also we can have technology to prevent that.  And as far as I know, maybe they already do.  This just occurred to me as a means of bypassing it.  Certainly they already thought about this.  So maybe it's already protected.



But yes, Padre, I am - this is just yay.  And I agree with you.  Given that this happens, users are going to want this, and we're going to want it pervasively.  And the system will adjust.  It's not like, again, as you said, they're leveraging our trust without ever asking for permission, and they are ignoring request after request after request to stop it.  They just say no, you can't make us.  Well, yes, maybe we can.



PADRE:  Yeah, exactly.  And the thing about this approach is there's a lot of room for adjustment.  They've got that three-day parameter right now, which it might not be long enough.  Maybe people are going to find out that four-day or five-day is what they want.  Or maybe others are going to realize, hey, you know what, if I haven't visited in 48 hours, it's probably not one of my primary sites, and I don't really care about it.  And Apple has the ability to play with those parameters.



But what I love, and this is what, when I was reading the story last night, this is what told me that they'd actually put serious thought into this.  The idea that, even if a cookie receives new information, if it hasn't been actually interacted with by the user in 30 days, then it still gets eliminated.



STEVE:  Yeah.



PADRE:  That means, yeah, Apple looked at what are the tactics being used that really kind of bend the rules, and that was one of them.  That was one of the biggest tactics, which is I'm going to refresh my cookie every couple of hours.  And then people wonder why things are working slowly or not working at all.  It's because you've got a bunch of third-party cookies that are doing things they shouldn't be doing.  If I put them in their own bucket, I let them play in their own little world, and they all think they're alone, then those third parties stop getting the information that they want, and they starve, which will cause me to play the world's smallest violin.



STEVE:  Oh, darn, yes.  Oh, darn.



PADRE:  Steve, that is brilliant.  You know what, sometimes we have Security Now! moments of brilliance that go over everyone's head.  But I think everyone can understand this, the fact that you've giving us, or Apple is giving us, more choice on how we handle cookies.  The fact that Apple can give us an experience that is far more secure and yet really kind of accomplishes 99% of what we want, yes, that fits into the "brilliant" category.



STEVE:  Yeah.  I mean, it solves the problem.  It provides users with more privacy.  It breaks this long-term persistent tracking model which has been - and this is - I've always said, this whole third-party cookie thing was kind of a mistake.  It's like the idea was the cookie was associated with a domain.  It was intended to allow persistent browser sessions.  It was intended to allow us to log onto sites, not to allow sites we never visit to track us around the 'Net.  So this has always been abusive, but there hasn't been a good solution.  And now I think we've got a very useful heuristic.  So bravo.  As I said, and thus titled this podcast:  Apple Bakes Cookies.



PADRE:  Steve, it's been an absolute pleasure.  And I can't believe this is the very first time ever that I've done the show with you that we have actually reached the end of the show notes, and we've gone through everything in the show notes.  That doesn't normally happen.  Normally we have to skip a few things.



STEVE:  Padre, I've learned to scale my ambitions accordingly because I know we're going to have much more fun with each topic. 



PADRE:  Oh, it's because I slow you down.  Be honest.  It's all good.  Of course, folks, Steve Gibson is the mastermind behind GRC.com.  If you haven't been there, then you haven't seen the best part of the Internet.  It contains everything from ShieldsUP!, which you're going to have to run.  Folks, if you don't know why you should be port scanning your own network, then watch Episode, I think it was 320, of Know How, and figure out what kind of information we can gather from you if you don't mind your own security.



He's also the purveyor of SpinRite, which you've already heard me talk about several times as the number one tool that I keep in my IT toolbox to recover drives that are dying, or maybe even service those SSDs that are going a little bit more slowly.  And still anxiously awaiting SQRL.  I'm looking forward to this, especially since there's a new renewed sense of purpose for replacing online identity, some way to confirm that we are who we are.  Steve Gibson, you might actually end up being far, far ahead of the curve on that one.



STEVE:  Well, we'll see.  We're going to make it possible and then hope that the world understands what we've done.



PADRE:  That does it for this episode of Security Now!.  Don't forget that we are live here on TWiT.tv every Tuesday at 13:30 Pacific Time.  Steve will always be here to inject you with some healthy paranoia and help you understand the wonderful world of security.  You can find all of our shows - I'm sorry, go ahead?



STEVE:  I was going to say, and we get you next week, as well.



PADRE:  That's right, that's right.  And next week actually, because I've got connections, I've already figured out what's going to be in the news.  Next week, Equifax has fixed everything; Microsoft will announce that they're fully patched, and they never have to patch another version of Windows; and, oh, Google is going to announce that every Android phone dating back 10 years will be running Oreo.



STEVE:  And that would be the last episode of Security Now! then.



PADRE:  Ever.



STEVE:  Because we'll be done.



PADRE:  Indeed.  Don't forget, you can find all of our shows at TWiT.tv/sn, as well as iTunes, Stitcher, and wherever fine podcasts are aggregated.  You can also find high-quality audio downloads at GRC.com, which is also where you will find, as we mentioned, all of those wonderful projects, all those wonderful products that Steve has made over the years to keep you safe.  Until next time, I'm Father Robert Ballecer with the one, the only Steve Gibson.  And remember that, if you want to keep your data into the future, you need to think Security Now!.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#630

DATE:		September 26, 2017

TITLE:		The Great DOM Fuzz-Off

HOSTS:	Steve Gibson & Father Robert Ballecer

SOURCE:	https://media.GRC.com/sn/SN-630.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week, Father Robert and I follow more Equifax breach fallout, look at encryption standards blowback from the Edward Snowden revelations, examine more worrisome news of the CCleaner breach, see that ISPs may be deliberately infecting their own customers, warn that turning off iOS radios doesn't, look at the first news of the FTC's suit against D-Link's poor security, examine a forthcoming Broadcom GPS chip features, warn of the hidden dangers of high-density barcodes, discuss Adobe's disclosure of their own private key, close the loop with our listeners, and examine the results of DOM fuzzing at Google's Project Zero.



SHOW TEASE:  Equifax is the awful gift that keeps on giving; the world has stopped trusting the NSA; what we thought to be a relatively ham-fisted CCleaner malware attack turns out to be an incredible piece of exploit engineering; and your favorite browser just got fuzzed.  Security Now! is next.



FATHER ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode 630, recorded September 25th, 2017 for Tuesday, September 26th, 2017:  The Great DOM Fuzz-Off.



It's time for Security Now!.  We're like a kinder, gentler, benevolent advanced persistent threat, but for your brain.  And of course, in the world of security, there is none more benevolent or more persistent than Steve Gibson.  Steve, of course, is the big brain behind Gibson Research, ShieldsUP!, SpinRite, and our coming non-passworded overlords through SQRL.  Steve, so good to see you, my friend.



STEVE GIBSON:  It will not be an overlord relationship.  It will be entirely beneficial.  And optional.



PADRE:  The funny thing about this is when the whole Equifax disaster first started coming out, SQRL was mentioned quite often as, yeah, you know what, we do need a better way to identify ourselves than some static numbers from a bygone era.  And that sort of approach of being able to use dynamic bits of information was something that was pushed forward by a lot of cryptography experts.  So I'm thinking, hey, you know what, you all should probably talk to Steve.  He's been working on that for a while.



STEVE:  Well, I love the slogan that the project has about that.  I just think it's got such a nice hook to it, which is "SQRL gives websites no secrets to keep."  And that's key because that's what's always happening with these information disclosures is that essentially our password is a secret.  And we make it up, and we give it to a website, and we say, please keep this a secret.  This is our secret.  Keep it.  Which is why it's so upsetting for people when, like you say, oh, I forgot my password, and they email it to you.  It's like, no, that's not how you're supposed to do this.  You're supposed to send me a link that allows me to set a new password, not send me my current one in the clear.



So the way SQRL works, the only thing the website has is your public key, and it's only valid for that one domain, for that one site.  So they could publish it if they wanted to.  It wouldn't do anybody any good.  It's not of any use at all because all it does is it serves to identify the user because it's a per-user and per-site public key.  And it's used then to verify a signature that they return of a nonce that the server sends.  So it makes up a random bit of noise, sends it off, and says "sign this in order to prove you have the matching private key."  That's all there is to it.  I mean, yeah, a lot of details, which is why it's taken us a few years to get it all nailed down, but it's just there now.  So, but, I mean, it's just like the right way to do it.



PADRE:  And actually it all fits in with the concepts of security that are going to come up later in the episode, especially when you start talking about the evolving Equifax disaster, of the death of perimeter security.  We have to assume that the bad guys are inside the walls now.  That's just the era in which we live.  And so as you mentioned, the best way to deal with that is to make sure that, when the bad guys get the information, they can't do anything with it.  That's where we're at.



STEVE:  And, I mean, that's useful, but it requires an evolution of technology, which is what SQRL represents.  The other thing I would say is that - and this came up, I'm just back from several days of a private security conference, which was under NDA so I don't have any need to or desire to talk about what went on.  But one of the points that I found myself making was to remind everyone of the importance of monitoring.  That is, for example, we know how long the bad guys were rummaging around inside of Equifax, for, like, weeks, many weeks, without them - and I did see one little bit of news, I didn't have a chance to follow it down, but it suggested that that had actually been going on for, like, four months earlier than was reported.



The point is that, yes, you have defenses.  You've got your walls and your moats and your multilayers and everything.  But you then must also watch what's going on.  You must monitor because - and then the idea would be to be able to account for all the traffic that you see.  That is, you know, take a look at it and say, okay, what's that?  Okay, that's that.  Oh, and what's that?  Okay, that's that.  But if you see something anomalous where you can't ascribe its valid role in your organization, that's, I mean - okay.  So obviously, if you brought that to the attention of the security people, they'd have a meltdown.  Well, you can't know it's going on unless you're looking.  So you have to be looking in order to have the opportunity to see something that is anomalous and then bring it to the right people's attention.



Anyway, so I think - and the point is that, yes, we need to evolve our technology over time.  But today anybody who is willing to put the resources in can at least be monitoring what's going on, be watching what's happening, and with the goal of making sure they can explain what they see.



PADRE:  That's just a little bit of a teaser.  As you can tell, we're a little bit passionate about this because, Steve, I think you can agree with me, it just keeps getting worse.  Every week you figure this is it, okay, this is as bad as it's going to get.  And then a couple of days later it's just one of these, what?  



STEVE:  And we'll be talking today also about what appears to be an increasing focus on state sponsorship.  And so we've seen over the course of the last decade the evolution of hacks from teenagers, who unfortunately are being arrested and now tried for what they did in their earlier years, specifically Marcus.  But now we're really beginning to see that this notion of cyberwar and cyberwarfare, which just - I remember it sort of struck me as science fiction.  It's like, what?  But no, it's real, and it's not good.  [Crosstalk].  



PADRE:  You've got to bottle that anger.  You're got to push it way down like a Klingon, let it explode forth.



STEVE:  So the title for today's podcast is "The Great DOM Fuzz-Off" because how could I not name the podcast "The Great DOM Fuzz-Off"?



PADRE:  You just like "fuzz-off."



STEVE:  Yes, which is the title of a Google Project Zero blog posting where a new member of the team built - essentially I guess it would be for him a third- or a fourth -generation DOM fuzzer that we'll be talking about, like what's a DOM fuzzer, and produce some surprising results.  So that's at the end of the show.  But you and I are going to talk about more of the Equifax breach fallout, a look at some interesting - and almost this would have been predictable, but it actually happened -  encryption standards blowback from the Edward Snowden revelations.  We're going to examine some worrisome news, actually additional worrisome news about the CCleaner breach that we first started talking about last week.  More has come to light since, and of course it's not good news.  There's also been some reporting by ESET of their discovery of some ISPs deliberately infecting their own targeted customers with malware, which is a new bad bit of news.



PADRE:  But it's friendly malware, Steve; right?  I mean, it's a good version.



STEVE:  The recipients don't feel that way.  Also we need to talk about a warning that has come to light post-iOS 11, that turning off your radios doesn't.  Some look at the FCC's lawsuit against D-Link over their poor security.  We've talked about that previously.  A judge has thrown out half of the case.  We're going to talk about that.  Broadcom is introducing a new GPS chip with some cool new features that'll appear next year.



Also I wanted to talk just briefly about the dangers of high-density barcodes which has come to light.  And I sort of blew this one off.  This appeared during the security conference I was at, and I thought, well, that's interesting.  But then as I caught up on my Twitter feed, all of our listeners were sending it to me.  And this is of course Adobe's disclosure of their own private key, like posting it on their website.  Whoops.  Then we're going to close the loop with some of our listeners and then get into this "what is DOM fuzzing" over at Project Zero.  So I think another great podcast for us.



PADRE:  Steve, I have no idea if we're actually going to be able to get through that because that's a lot.  That's a lot of material.  But we're going to give it a good try.



STEVE:  Especially when I have you on the other end of the phone, Padre.  We always engage significantly.



PADRE:  I've noticed that.  We tend to dwell a bit more when I'm here.  I'm just enjoying it.  I mean, I really get to geek out on security, and that's why I enjoy doing the show with you.



STEVE:  At least everyone can hear us this week, so that's an extra bonus.  Suddenly you appear, and the audio clears up.  I wonder what that's about?



PADRE:  I've got some pull.  With Comcast.  Oh, Steve.  One week there will be no more bad news out of Equifax.  This is not that week.



STEVE:  Well, we have to talk about our Picture of the Week.



PADRE:  Oh, that's right, that's right.  I always jump over that.



STEVE:  Which was just fun.  I had to remove the F-bomb from the quote from the NSA guy, but it's just a great cartoon.  It's just three frames.  In the first frame we have Apple's CEO holding the phone, saying "In the new iPhone X...."  Then in the second frame, "We want to introduce face recognition!"  And then in the lower frame there's a guy in a suit and tie with a CRT or display screen labeled NSA on the back, and he's quoted saying, "We love these guys!"



PADRE:  Now, over the weekend I actually got some hands-on time with the iPhone X.  I got to play with it for about three hours.



STEVE:  I'm afraid if I saw it, I would have to have it.  But right now I'm still sticking with my 6.



PADRE:  You won't, honestly.



STEVE:  No?  Oh good.



PADRE:  Here's the thing.  It's beautiful.  It's really nice, and they've done an incredibly good job on engineering it.  I even like the notch.  The notch is not a problem.  People are mocking the little notch at the top of the screen.



STEVE:  The little rabbit ears?  I hate them, yeah.



PADRE:  Yeah.  But, I mean, it's not like that takes away from the functionality of the phone.  Normally that would just be a whole bar of the screen that you wouldn't have.



STEVE:  Right, right.



PADRE:  What bothered me, however, was the lack of a home button.  So they had to change everything.  So now everything is run by swipe.  So something you used to be able to do maybe with a long press or a double-tap of the home button, now you do it with, like, three or four swipes.  That felt very weird.  And I don't use iOS a lot, so it was really noticeable for me.  And I'm thinking, if someone is very much into iOS, they're going to not like that at all.  It will take so much getting used to.  And at some point it means you split iOS.  You're going to have two different interfaces.  I don't think Apple wants to do that.



STEVE:  Well, and in fact they've also done that to a much greater degree because I've got it on all my devices as of last Tuesday morning.  And so already, because of the difference in screen size, there's a bigger change between the phone and the tablet functionality, things that are gone from the tablet that you used to have, that the phone still has because they couldn't - those UI features, or the UX stuff, couldn't translate.  But I'll just say now, I actually have it later down in the show notes, this is the buggiest release of iOS I've ever experienced.



PADRE:  It is, very much so.



STEVE:  Every single one of my devices does bizarre, some of them repeatable, like bright flashes of the screen when I'm trying to scroll something.  And at one point I had some of the balloons in a message stuck on the screen with other ones scrolling underneath it from a different stream.  It's just like, okay.  And I've had the little bar of icons, the tray, stuck on the vertical right side of the tablet.  And everything else rotated around, but it just stuck there.  And it's like, I mean, it's constantly doing broken things.  So I do think that we'll see those fixed.  I'm sure they're collecting them now, and they'll fix them, and 11 will stabilize at some point.



PADRE:  Oh, yeah, they'll figure it out.



STEVE:  But it's a mess.  It's never been such a mess.  I've never seen a version of iOS that is this buggy out of the gate.



PADRE:  But Steve, I mean, you've got to say it with me:  It's not a bug, it's a feature.



STEVE:  Eh, it's a bug.  So, yes.  Just referring briefly to Equifax, I picked up an interesting conversation over breakfast the second day of this private security conference that was something that hadn't occurred to me in all the discussions we've had about Equifax.  There is, among the upper-end security community, a clear, I don't want to say "knowledge" because it doesn't quite clear the bar of being knowledge.  But it is believed that this was China that performed the Equifax attack, that this was not some opportunistic random hacker somewhere, but that this was state sponsored, and that the 143 million of us Americans whose personal information was disclosed are probably not in the kind of immediate danger we would be if the goal were to sell this information on the gray market.



Now, maybe that will eventually happen.  But the theory of this is that, if this was China state-sponsored attack, the goal was to obtain the very private, personal identify information of specific individuals, government employees, corporate higher-ups, you know, C-level people, to enhance and increase the power and success of targeted attacks against them. 



PADRE:  That's interesting.



STEVE:  So rather than all this information just being made widely publicly available, there may be more to it than that.  And it may not have the kind of sweeping effect it could have because in some senses, if it's a state sponsor, then maintaining the privacy of that is of more value to them.  And also then everyone running around locking our reports, while that's what we have to do, it suggests that there won't be many attacks against those locks as a consequence of this, but rather this was a deliberate personal information database acquisition by a state actor.



PADRE:  Now, there's a couple of interesting things about that.  One, typically when you hear "state actor" in combination with any sort of attack, it means it was some sort of advance attack.  I think that's one of the things about this particular breach that did bring a lot of state actor talk at the beginning because this wasn't an advanced breach.  This was not an APT.  This was just a company that didn't know how to run security.  But that doesn't mean it's not state sponsored.



STEVE:  Right.  Exactly.



PADRE:  What lends credence to this, however, is I've spent the last two weeks crawling all the usual spots, looking for an increase of credit card numbers and identities on sale, and I haven't really seen it.  It's been sort of the regular flow of traffic.  And you would figure, if this was for gain by selling IDs...



STEVE:  And because it's time sensitive.



PADRE:  It's very time sensitive.



STEVE:  As people are locking things down.



PADRE:  Precisely.  And I haven't seen a great increase.  So either they didn't get a lot of information or, as you mentioned, this was particularly a targeted attack.  They were going after some very specific people, and the rest of us were collateral damage.  And again, because Equifax had no controls...



STEVE:  Were just swept up in the acquisition, yeah.



PADRE:  But as you mentioned at the beginning of the show, if you're not listening, and you don't have any controls in place to see what's actually leaving your network, then that makes breach recovery so much more difficult because we don't know.  I mean, it could literally have been one person or no people that they took the data of, or it could have been all 143 million.  That's just - that's unacceptable.



STEVE:  Right.



PADRE:  My goodness.  And it just, I mean, it just keeps getting worse and worse.  We've already heard some of the details about how this happened, how it was a struts vulnerability, how they have at least one server that had default admin and password credentials.  We know now that they actually did get hacked back in March, and they didn't do anything about it, so that could have been seen as a dry run for this.  We know that their security executives had no business being security executives, one of which was receiving like a $3.2 million salary per year to do nothing, literally nothing.  And then of course this last week there was the embarrassing and yet somehow very appropriate advisory that they had been forwarding people to a site that wasn't theirs.



STEVE:  Yup.



PADRE:  So, yes, it's all around good news, Steve, I think, yeah.



STEVE:  Well, and in the wake of all of this attention, Brian Krebs reported also another little bit of news, and that is that Experian was not well protecting the PINs of their previously locked customers.  I remember very well 20 months ago, after the Anthem breach, when we first discussed the need to lock our credit reports.  I and my friends and family - because I turned them all onto this at the time, and I'm sure a subset of our listeners here all did this - we went to the various bureaus and obtained locks and PINs.  And in every case the fear of god was put into us about not losing this PIN.  It's like, this is it, and you'd better not lose this because there's nothing we can do.  And I take that very seriously.



So it's like, okay.  So I printed the web pages and took photos of them and zipped them up and put them away and really preserved and protected that information.  Now we come to find out that, well, that was not very practical.  It's like, oh, well, people are going to lose their PINs.  What are we going to do about that?  So Experian has a web browser-facing PIN recovery.  And unfortunately, you only have to give it things that are always available, if you've got this problem in the first place.



PADRE:  Name, social security, address, driver's license, and a little bit of your credit history.  Those five bits of information will let you get a new PIN.  And it's like, oh, but that's all on the web now; right?



STEVE:  Yeah.  So it says it's Experian - it's called the "Request Your PIN," which, first of all, you shouldn't be able to do.



PADRE:  No.  



STEVE:  The whole point is you don't want anyone - because they don't know it's you.  The point is how do you authenticate yourself when you get your PIN to unlock the thing you're trying to lock?  So it says:  "If you have a security freeze on your credit report and wish to obtain your forgotten or misplaced PIN," you know, they just ought to say, well, you're SOL.  But no.  They say:  "You may request it here.  A PIN is needed to remove a freeze from your credit report and to provide a creditor access to your credit report."  Right.  That's why no one should be able to get it.  But anyway, they make it quite easy.  You fill out the form, providing information that's readily available - your Social Security number, your date of birth, your name, and your address.  Bang.  Here's your PIN.



PADRE:  You know, Steve, this is the tension between the various audiences that we have.  There's the hardcore Security Now! audience, and they understand.  They get it, that if you use a new service that says we protected everything and you control all your data, and they have a way to recover your password, then that means they're lying to you.  Our audience gets that.  However, there's another audience, and that's who this is for.  This is security theater.  It's to make them feel like they've protected themselves, but they don't get that if there's a way to recover something that is supposed to be unrecoverable, that somehow the service is broken.  And how do you get to that second group?  You can't really because [crosstalk].



STEVE:  I know.  And this is my one worry about SQRL because, as its designer, I have refused every temptation to cheat the system in that way, and I've made it extremely safe.  I mean, I've done everything I can to help people not hurt themselves.  Like for example when the system, when you create your identity, it gives you a 24-digit rescue code.  And that sounds like a lot, but it's only one and a half credit cards long because a credit card number is 16; this is 24.  So, I mean, it's not bad.  But it makes you write it down.  And I'm sure people are going to go, eh, no, and just hit "next."  Well, then it says "type it in."  And they're like, oh, damn.  And they have to go back and actually get it.  And it won't let you put it on a clipboard because it's not supposed to be in your computer.  So it's like, it won't let me copy this.  It won't let me cut and paste it.  No.  Write it down.



And so, again, it's a two-party system because no one wants to trust a third party.  Okay.  But that means there's no third party to go crying to when you do forget it.  So maybe it's going to end up being that SQRL is just not for everyone.  If you cannot handle - all you have to do is write that down.  But if you refuse to do that, I'm sorry, there's nothing we can do for you.  You just step away from the computer, maybe unplug it, and go back to basket weaving because the Internet is not where you should be.



PADRE:  It's funny, it brought up a situation I had over the weekend.  I gave my dad a new phone, and he's an Android user, so we were switching him over to the new phone, and he couldn't remember his Google password.  And I'm thinking, that's the one resource you use regularly on the Internet.  How can you not have that?  And then I realized, no, that's actually pretty normal.



STEVE:  It's so sticky now.  Yup.



PADRE:  Yup.  And I'm thinking, yeah, so that actually is a problem because people get so accustomed to their device automatically being authenticated that, when they're asked to remember it, it's not even possible.  And I would love to have that solid security because, like you, I believe that helping others with security helps me with security.  When people around me are more secure, I'm more secure.  That's just how circles of trust work.  But at the same time, I can't ask my father to use a service that absolutely positively will never allow him to recover a password that he's lost.  So I guess I'm just going to have to live with getting owned every once in a while?  Is that it?



STEVE:  Well, the point you just made reminded me of the iOS 11 upgrade process because you are forced to give your cloud identity in order for it to synchronize in.  But that's before the rest of the system is up and running, where you have access to your password manager.  So you would have to have it somewhere else and then have a password that you have a chance of entering through the frustrating touch keyboard with upper and lowercase and symbols and everything.  It's like, god help you if - and so it happens that I do have that.  But, boy, if you're someone who has become totally dependent upon your password manager, there are still places where even that can't come to your rescue.  It's all messed up.



PADRE:  That's where we are.  But, see, that's why you're here, Steve.  You're going to make it better.  I think, I truly believe that when people understand security a bit better, and when it gets brought down to a level that they can intake, then they're more likely to make a good security decision.  People will still forget, and people will still be lazy, but at least they understand the stakes.



STEVE:  One of the very first acronyms that we developed on this podcast was TNO, Trust No One.  And there didn't seem to - there hasn't been any blowback against that.  And I think it's because only the people who understood the responsibility of - the other one was PIE, Pre-Internet Encryption, PIE.  You encrypt before you put it on the Internet.  Only the people who were willing to go to that extra level also understood that there was no recourse, that their encryption system was putting noise up in the cloud, and they couldn't ask the noise holder to please turn that noise back into data.  Just no.  There's just no way to do that.



And so the problem that I face with SQRL is it is both seductively easy to use, I mean, when you actually have the experience you immediately think, wait, this is secure?  I mean, it just - it's so easy that everyone is going to want to use it.  Unfortunately, to your example, your dad shouldn't because, if his SQRL identity gets away from him, all of the system for recovering it is in danger.  Although I will say that, because SQRL is a proxy for you, that is, you give it permission to authenticate on your behalf, I do require you to constantly reenter your one SQRL password when you begin using it each session.  You don't have to do it during your multiple hours of sitting there logging into different places.  There you only have to give it just the first few characters to make it easy.  And that, too, is deliberate.  It reinforces your memory of that one password.  That's all you have to give it.



And even so, we've got some weasels - now I'm sorry I used that term - in the newsgroup whose signature line on all their postings is "The SQRL password must be removed" because they're annoyed by that.  And it's like, I'm sorry, no.  There's a reason for everything I have done.  It's been well thought through.  And so this doesn't let your dad forget the password because he does have to use it every day, just once, and then it won't be in his way.  But he has to keep using it.  And so anyway, the system has been designed not to be brittle and to have tons of recovery, but ultimately it's only you.  And so we'll see how that works out.  Apparently it didn't work out very well for the Identity Lock PINs.



PADRE:  Absolutely not.  Oh, by the way, when he says "weasels," he actually means nuclear vessels.  There are those who are floating by in the night, and you're doing your own thing, so it's all good.  Now, Steve, can we talk a little bit about encryption because you brought up that one of the very early acronyms was Trust No One, TNO, and that makes sense.  Except the U.S. government has been saying to our allies, "Trust no one except us," and that's not turning out so well anymore.



STEVE:  Well, and our listeners, our longtime listeners will remember well the controversy that arose around the dual elliptic curve random bit generator which there were four different pseudorandom number generators that were NIST approved.  And what was weird is that the slowest of those was the one that RSA had as their default in their BSAFE package, which was their crypto library that they've been selling for years.  I've got the documentation behind me on the bookshelf.  I owned a set of that in the early days.



And so there was never any proof; but the problem with, for example, algorithms that have magic numbers is that the magic numbers have to be set in stone.  Everyone has to agree on them.  But they have to have certain properties.  And so if someone just hands you a magic number that looks like gibberish, but it's like special gibberish, the question is where did the gibberish come from?  That is, did you roll dice all weekend and come up with this?  Or did very smart people in the backroom say, ooh, look, we can hide some special bits among the gibberish which will give us an edge that nobody would suspect.



And so in this dual EC, this dual elliptic curve technology were some magic numbers, some gibberish, but there was no explanation of where it came from.  For example, say that you ran a counter through a hash, and you took the output gibberish and tested each one until you've got one that met your criteria.  That's a perfect example of being able to demonstrate you didn't design this gibberish.  It emerged from a hash equal to all the other gibberish, and you just chose one that met the criteria you had.  There you could say, ah, we know where you got the gibberish, and we like it because it's as good as anybody else's.  Yet instead the NSA just presented their gibberish and said, okay, everybody, take it.  And it's like, uh, we're not really sure that's clean gibberish.



So now we have, many years downstream, what has come to light through some reporting that Reuters did, is that after three years of ANSI committee work on some next-generation ciphers, and this is the ISO standards body, there were two encryption algorithms named Speck and Simon which were sourced from the NSA and analyzed and believed to be good.  But the NSA was also saying, oh, and you know, in power economy environments or time-critical environments, you might want to use some weaker versions.  And we've talked about how ciphers work where they typically are iterative, and so you have to run some number of iterations in order to deeply mix the bits enough that they cannot be unmixed.



And in fact attacks on ciphers are done by deliberately reducing the rounds, the number of rounds, as it's called.  And, for example, there was even in Rijndael, our current AES standard, there have been attacks against, like, seven rounds because that hasn't given the bits time enough to get thoroughly mixed, that it is possible to still statistically see some leakage.  But you're never supposed to do that.  You're supposed to always run 11 rounds or 13 rounds or whatever, depending.



So anyway, what happened is, as a consequence of what everyone now remembers of the Snowden disclosures, where some of the documents that were leaked by Edward Snowden after he left the NSA demonstrated that there were budgetary records which were seeking funding to insert vulnerabilities into commercial encryption systems.  So, I mean, that news is out there.  And of course it's made the global community skeptical of accepting designs of what would become critical encryption infrastructure which is offered on a plate from the national security agency without...



PADRE:  As it should.



STEVE:  Yes, exactly, as it should.  Now, you could argue that in the past we were too trusting and just saying, okay, fine.  Now that trust is much more conditional and has been pulled back.  And so what happened was the U.S. ANSI standards organization, which was proposing the standardization as an ANSI standard of varying strengths of these two new encryption algorithms, finally agreed, after essentially the global community said no, agreed to remove all of the weaker variants so that only the maximum-strength versions of Speck and Simon, which are believed by academic researchers to in fact be super strong, those are the only ones that have been allowed to make it into the standard because the weaker ones looked okay, but unfortunately trust has been compromised in where they came from.



And again, unless you can clearly demonstrate every bit of the sourcing of something like this so that you can just say, yes, it dropped from the sky and we realized, oh, look how good this is.  Or it just truly came out of staring at lava lamps and writing down how many bubbles were coming up over time and using that as your source of entropy.  No.  If we don't know where it came from, it really needs to be treated with skepticism.



PADRE:  When we were developing our own type of encryption for different types of communication, we would actually use atmospheric static.  But we had specific rigs that would look past the ionosphere, the mesosphere.  So we would actually get intergalactic static.  And it was one of these things, yeah, that's crazy random.  There's no one that can manipulate that.



STEVE:  That would be manna from heaven; right?



PADRE:  Precisely.  But, see, Steve, the thing about the story that gets me is it's a tragedy that you've introduced so much distrust into a standards body that is responsible for some serious heavy lifting for security because, now that everyone doesn't trust the NSA - and again, rightfully so because the NSA tried to use the trust that other countries had in them to push forward something that they knew was broken.  Now you start to ask, well, if the NSA is doing it, who else on the standards body is doing it?  Everyone has an agenda.  Everyone's trying to get a leg up.  Everyone wants to have that backdoor.



STEVE:  State actors certainly, yes.



PADRE:  Yeah, exactly.  It would be naive to think that the United States has the only institutions interested in doing this.  But once you have that level of distrust, then you don't really have a standards body anymore.  You have a very adversarial process.



STEVE:  I think what will probably evolve from this is that we change the way these things are presented, very much using the example I did before where we had accepted an elliptic curve algorithm with magic numbers whose origin was unknown.  That just has to be ended.  We will never do that again.  A bunch of people, a bunch of academic cryptographers from different countries will all get together and watch a random number generator generate random numbers, and they will collectively as a team choose the one that they decide is the best, not because it was - I mean, so that by a priori knowledge it has no special characteristics.  That's the way we're going to have to do this  moving forward, very much like the way keying is done of the top-level domains or DNSSEC and things, where, I mean, those secrets are so crucial to be kept pure that it's done in a way that cannot be compromised by any single entity.  It cannot be biased.



PADRE:  Right, right.  And of course that is exactly the type of solution that members of our own governing agencies are very much against.  They don't like the math.  They like to be able to control those magic numbers. 



STEVE:  They don't want to lose control, exactly.



PADRE:  Yeah.  Because we know from the past that trusting a government entity, especially the United States, with a magic number, or in this particular case of the TSA a magic key, that there's no way that they're going to let it leak out and potentially become public knowledge.



All right, Steve.  I know we're not going to have any good news because the next topic is...



STEVE:  This is not the happy good news podcast.



PADRE:  At one point we have to have a Security Now! podcast where it's just everything is happy.  It's like, oh, my gosh, Microsoft has everything together.  They'll never have to be patched again.  Apple has a great new iOS 12.



STEVE:  Bug-free, yes.



PADRE:  Yeah, bug-free.  But, no, bug-free is far in our future because we need to talk about a malware that at first seemed like it would have minimal impact, but over the last two weeks has been building into something quite dastardly, actually.  What's going on with CCleaner?



STEVE:  I don't remember now if I shared on the podcast last week my feeling that Piriform, the publishers of CCleaner, were downplaying the severity.  I felt when I was reading their own disclosure.  I was thinking, you guys are really trying to make this really seem like a non-event.  Whereas the more independent coverage, I mean, this was originally found by Cisco's Talos security group.  And their coverage was, I think, much less biased.  Again, any security researcher is playing up their cleverness at finding something and so that tends to be maybe a little overstated, how bad this is.  On the flipside, the company that has been found to be at fault is like, oh, well, you know, yes, 2.23 million people downloaded it, but that was only there for a moment; and when 5.34 was replaced, that all went away.  So it's like, okay, but what happened in the interim is what we want to know.



So to recap, the 32-bit installer of the v5.33 of CCleaner was  maliciously modified to install and include a backdoor which reached out after it was installed to a remote C2, as the community calls it, a command-and-control server.  That backdoor malware was able to receive commands and download additional malware payloads.  That we knew last week.  And we also know that significantly more than 2 million copies of that, the number I have in my mind, I didn't look it up again, but I think it was 2.23, if I recall, were downloaded.  And we don't know that they were run, but people downloaded them.  And most people, because there's not an auto update in the free version of CCleaner, which most people use, someone who went to get it almost certainly ran it.



So, okay.  So that's everything we knew last week.  Cisco has continued, with their Talos security division, continued looking at this.  And they obtained a set of files which were ostensibly from the malware command-and-control server.  They were skeptical and careful about trusting the files, as they should have been.  But they were able to determine that, to the best of their belief, they were absolutely authentic because among the files were logs which showed their own early traces of their activity with the server.  So, for example, they probably set up a cleanroom environment and infected a system and watched it reach out to the server.  So they were able to later forensically detect their own earlier initial discovery among the files.  So they came to the conclusion that, yes, these are authentic.  What we have is real.



Then what they discovered was a bit chilling.  They found, in the code on the command-and-control server, 20 domain names of targeted companies, among them Cisco.com and a bunch of others.  If anyone's interested, the link is in the show notes, and there's a list of the 20 domains in the Talos blog coverage.  And they discovered that, if an infected machine contacted the command-and-control server, the source IP of that incoming traffic was looked up and its domain checked against the list.  And if there was a match, the command-and-control server woke up and delivered a second-stage payload.



PADRE:  Wow.



STEVE:  Yes.  And they also determined that that had been done - and I'm scrolling through my notes here, and I don't see the number in front of me.  Let's see.  Talos wrote:  "The use of domain-based filtering..."



PADRE:  Twenty machines, yeah.



STEVE:  Was it 20?



PADRE:  Twenty machines.



STEVE:  Yes.  Oh, yeah, there it is.  More than 20 machines, they've determined, have received the second-stage payload.  So now we get a better picture of what was really going on.  Some attacker - and again, this is now believed to be state-sponsored because this is not, I mean, this is a big project that was put together.  And the nature of the attackees, that is, the targeted domains, also suggests that somebody with state-level interests had specific agendas.  So they picked a very popular freeware utility, and I don't remember what the number was.  We talked about it last week.  The total number of downloads is just astonishing.  I mean, I use CCleaner.  Everybody uses it.



PADRE:  It was over two million.



STEVE:  Yes.



PADRE:  In the limited amount of time.  And remember it was only one variant of CCleaner, was just the 32-bit version.



STEVE:  Correct.



PADRE:  So two million machines to get about 20, just over 20.  Now, Steve, unlike the Equifax breach, this feels state sponsored.



STEVE:  But the point I was saying was that CCleaner itself, see, they didn't know they were going to get 2.2 million.  CCleaner itself had been downloaded - and that's the number I don't remember - hundreds of millions of times, I mean, because it's so popular.  And so you're right, in this brief window they got 2.23 million downloads.  But in terms of the overall plan, the idea was to find a highly popular piece of freeware whose supply chain, and that's the term that you're seeing how, we're talking about a supply chain attack, that is, an attack on getting something to someone else and interfering with the chain of delivery.



So take a highly downloaded piece of freeware.  Infect the supply chain so that you are then using a scattershot.  Essentially you're infecting everybody who downloads it with a first-stage attack.  But you're only actually looking for 20 domains.  You don't know you're going to get any, but they did.  So you have a list of actual targets.  And when from this scattershot, this 2.23 million downloads, just by pure luck and opportunity, some of those people who downloaded that version were among the 20 intended targets.  And they got hit with the real McCoy with this second-stage shell code which, again, even though there was both 32- and 64-bit shell code, it still looks like it was a 32-bit attack.  So the attackers apparently had reason to believe - or maybe it was vulnerabilities in those attacked machines that were only available in the 32-bit variant.



But still, now we have a better picture for what was going on.  This was a big project, and there were specific companies targeted, using a purely random, hopeful, maybe this is so popular that some people working in these particular organizations will be CCleaner users and will, while the infected one is in the supply chain, will download it, will run it.  We'll identify them, and then we're going to zap them with our actual malicious payload.  Wow.



PADRE:  Now, Steve, I'm trying to think back.  Now, it wasn't quite a two-stage attack.  But Stuxnet is one of the first of these types of attacks where you shotgun to as many machines and clients as you can, but you only activate when you get on the right system and, even then, only when the conditions are exactly right for you to do the most damage.  That was an incredibly advanced state-sponsored attack back in the day.  That's what this feels like.  I mean, if this was just some kids looking to be script kiddies or just some Black Hats looking to compromise financial data, they would want to get as much as possible.  The fact that these were looking for specific hosts within specific domains, it screams, first of all, industrial espionage; and, secondly, state sponsored.



STEVE:  Yes.  And so you can kind of forgive Piriform's initial statement of saying, well, we looked at what happened, and that command-and-control server was taken down before it did anything wrong.  Well, that was not true.  But you could forgive them for wanting to fudge in their favor because it also, exactly to your point, it wasn't the case that all 2.23 million people got the big enchilada.  They only got the little icebreaker that started this.



And also remember that the risk of discovery is all part of this.  So that first stage tried to do as little as it could.  It didn't want to get discovered.  And that's why it just made a little query out to the command-and-control server, basically to say, hey, I'm here.  Did we get lucky?  And if not, no, okay, go away, just shut up.  But in the case of, oh, yes, you're calling in from Cisco, bingo.  And then that's when the big payload was delivered.



So this is the way you organize it, exactly as you were saying, Padre.  You organize this so that you don't expose yourself.  Absolutely, you keep that exposure minimal so that you can continue operating as long as possible before someone like Cisco Talos or ESET or Kaspersky or any of these companies that are now on the watch for this become wise to it and then basically shut down that particular campaign.  There will be others, but this was one.



PADRE:  Oh, yeah.  This is going to be more common, especially as they start building malware to time exfiltrations because that's the next step.  Once people actually start looking like, okay, we need to look at any particular malware, any C&C system, and actually compare it across multiple domains all across the Internet, it's going to change the way that they look for that.  But once they have that down, it will be, well, if I'm going to design an APT, I'm going to have it stay dormant in the system until I get exactly what I want, and then I'm going to exfiltrate all at once.  And that is such a difficult attack to defend against because it can be literally over in a matter of minutes.



STEVE:  And we're going to do it at 2:00 a.m. on a...



PADRE:  On a Saturday.



STEVE:  On a Sunday morning, yes, exactly, when no one is around, and everyone's asleep.



PADRE:  Oh, my.  Now, Steve, this type of attack - let's start with the infection.  They were able to make it look like authentic software because they didn't actually change the install package.  They just piggybacked on it.  How easy is that to do?  Can I piggyback on basically any install package? 



STEVE:  Well, it requires access to the system.  So the idea is that you had to have a broken server.  But normally what's happening is the installer is not signed, but its payload is signed, because the installer is not produced by the software publisher, it's produced by the installer company.  And so it was the case that CCleaner had a valid signature, but the installer was modified to install something in addition to CCleaner.  So checking the CCleaner signature, everything was fine.  But there was also something that was brought along with it, that was added to the installation package.



And so you absolutely want to protect the security of the supply chain, in this case the download server that is supplying all this.  And of course you do want to make sure that you're connecting to the proper download server, which brings us into our next story because ESET - and this is very troubling.  ESET, which is a well-known security company, has found instances of a WikiLeaks previously disclosed malware known as FinFisher, F-I-N-F-I-S-H-E-R, which is advanced surveillance malware, capable of snooping on webcams, monitoring and recording keystrokes, microphone audio, and the victim's web browsing behavior.



So it's advanced surveillance malware.  And it's like, okay, so that's not good.  It was found in seven countries which they have blacked out in their reporting.  And they specifically say we're not going to talk about where we found it because I guess they're working with those organizations or companies in the background.  But they did say it was found in seven different countries.  The disturbing thing is that they also found evidence that it was being delivered to the endpoints, the endpoint users, by their ISPs.



So the ISPs were redirecting their customers' attempts to download WhatsApp, Skype, Avast, VLC Player, and WinRAR.  Those apps were found to have been compromised, not the actual supply chain download server itself, that is, not the real one.  But what the ISP was doing was performing a targeted man-in-the-middle attack so that, when specific customers downloaded any of those very popular apps, the ISP redirected the download to a malicious source which provided FinFisher-infected versions of those popular applications in order to get them installed onto their systems.  So this is the first time we had any knowledge and public reporting of this happening, that is - and we don't know which ISPs and which seven countries.  I sure hope it's not ours.



PADRE:  It better not be ours.



STEVE:  Yikes.



PADRE:  I mean, that's actionable.  That's illegal.  At the very minimum, that's a violation of the Computer Fraud and Abuse Act.  And it actually could be worse because now you could be held by the 1986 Telecommunications Act because you're essentially launching a man-in-the-middle attack against a customer who paid you for service which included security.



STEVE:  Yeah.



PADRE:  Wow.  And the thing is, that's not hard.  I have at least four or five different packages that you buy commercially that do just that.  Now, it's supposed to be so you can do network management on your own enterprise segment.  You can make sure that if anyone - one of the reasons is you want to reduce the amount of load on your externals.  Maybe there's a file that a lot of people inside your organization are downloading, and this is an easier way for you to cache it.  It also could be that you are trying to keep people from going to particular sites or getting particular software, and there's plenty of software that stops you from doing that.  So this is just the application of totally legitimate software.  What makes it illegitimate is the fact that they did it without the users' consent.  That's - wow.  Steve.



STEVE:  I know.  I know.  And we've been talking about ISP trust and people talking about using VPNs in order to get through their ISP.  And our fallback has been, well, yes.  But as long as you're over HTTPS, you're okay.  But of course...



PADRE:  It jumps off, yeah.



STEVE:  We know that's not absolutely true.  If this is a state sponsor, we know that it has to be the case, with so many CA root certificates in our system's trust stores, you know, four or 500 of them - maybe it was eight.  I can't remember the number now.  But, I mean, it was like a ridiculous number, where our browser trusts anything signed by any of them.  There is no way we can believe that every state government who wanted to cannot have a certificate made for any site they care about.



And so if they provide a server which has spoofed TLS certificates for WhatsApp and VLC and the rest, and Skype and so forth, then all the ISP has to do is rewrite the IP packet headers that are, even though it's encrypted, even though it's HTTPS and TLS, all they have to do is redirect the traffic to the malicious server which is set up to answer a TLS connection correctly with a fraudulent but valid HTTPS certificate, and a user can download WhatsApp, Skype, VLC, whatever, with everything looking fine over an HTTPS connection, and still be getting a fraudulent, malicious payload.



PADRE:  I got a question about this in my show email from Know How, and they wanted to know if using something like a Tor server would prevent this.  And I'm like, well, Tor jumps off.  At some point it has to jump off the encrypted node, and it's now, if it jumps off on a client that is connected to an ISP that's doing this, then it's a false sense of security because you're only as secure as your most insecure link.  Now, if I have a direct encrypted connection to the server from which I am pulling the file, then I can say with certainty that, yes, I control what's being put onto my computer, and I control what's being pulled from that server.



But unless I actually own the server, and I know all the segments that I'm jumping through to get to that server, then I have to assume that it's insecure, even if it does say HTTPS.  Which is kind of scary; right?  Because we've been telling, I won't say the "less schooled," but we've been telling the less tech savvy that as long as they see the little lock in the browser bar, that they're safe.  And in reality that's no, not so much.



STEVE:  Well, and we need to remember that we get two things from those links.  We get encryption, and we get authentication.  So encryption is easy.  We always get encryption.  And, for example, Let's Encrypt, that gives you encryption, but Let's Encrypt is so heavily abused these days by spoofers that it's like, well, it's not really that well authenticated, and they don't try.  They're only trying to be the weakest form of so-called DV, Domain Validation certificates.  And that was the goal.  That was the original goal was just we just want to encrypt everything.



Unfortunately, with that comes the presumption of identity, that is, you're also authenticating the server endpoint.  And that turns out not to be true, I mean, largely in the case of Let's Encrypt; but also that's a weak assumption when you've got somebody with sufficient power like a state actor that has the ability to just synthesize whatever certificate they want because they have control of a trusted certificate authority just by virtue of the fact that they are a government.



PADRE:  Right.  And also they have the power to actually reach into the infrastructure and make sure that you're routed in a way that they want.



STEVE:  Yeah.



PADRE:  Which at that point it's game over.  They're mentioning in the chatroom that, too, it's time to make sure you always check your hashes.  And, yes, that's very, very important.  Make sure it's SHA, MDA.  Go ahead and check the values after you've downloaded a file.  But even that, would that have stopped the CCleaner attack?  Because the actual install file was still fine; right?  It was the piggyback file that was infecting.



STEVE:  Certainly it depends upon what hash you're checking.  And also notice that I've always been kind of skeptical about this whole "here's the hash of the file" because you're getting the hash from the same page as you got the infected file.



PADRE:  Exactly.  Still have to trust somebody, yeah.



STEVE:  So any hacker worth their salt is going to fix the hash to match the infection.  So it's like, oh, look, the hash checks.  Well, now you're even worse off because now you think you're extra secure, and you've verified the hash, and so you're going to give that to all your friends.  So, no.



PADRE:  Yeah, yeah.  It's nice.  In theory it is a great way to protect yourself.  But again, as you're mentioning, as we are getting more and more into the age where the really sophisticated hacking is being done by state actors, A, it's not easy to detect; and, B, once you detect it, there's not a whole lot you can do unless you own all the infrastructure between the resource you're trying to access and your client.



STEVE:  Yeah.  Well, let's go to something...



PADRE:  More we can control, Steve, because I think we're putting our audience into a tailspin.  How about this?  You've got a brand new iOS 11 device.  It's got all these great features.  I love the little dashboard.  I love the fact that Megan Morrone showed me how I could set up my iOS device so that it doesn't let me use it while I'm driving, which I love.  But it also does this other thing which, again, is not a bug.  It is officially a feature.  And that is, when you turn off WiFi and Bluetooth in iOS 11, it doesn't actually turn off WiFi and Bluetooth.  What's this about?



STEVE:  That's right.  That's right.  So, okay.  From early days of the podcast we've talked about how annoying it is that iOS continually turns Bluetooth back on because, as we know, Bluetooth is a short-range inter-device communications link, which is very useful.  If you have a hands-free phone in your car that is Bluetooth-enabled, if you've got a Bluetooth headset or earphones, I mean, there are certainly use cases for it.  But many people don't have that.  And so there's nothing for their phone to Bluetooth to.  And standard security practice, and we were just talking about - what was the blue attack last week?



PADRE:  BlueBorne.



STEVE:  BlueBorne, exactly.  We were just talking about how, yes, if you don't actively need something, especially if it's a radio, turn off the radio.  And so our advice had always been turn off Bluetooth if you don't know you need to have it on.  And so it was annoying that iOS constantly turns it back on again, even though it requires your deliberate act to turn it off.  They think they know better.  Well, with iOS 11, this presumption has grown in strength.  One of the cool features, and I was using it during my recent travels, is that you're able to swipe up from the bottom.  If you swipe up a little bit, you get sort of an application switcher.  You get your little launch dock.



If you keep going, that takes you to - they call it the Control Center, which you used to be able to get by swiping up from the bottom, but this is different now.  It's also got an application switcher.  You're able to customize its contents using the Control Panel to change what's there.  And I did add a few things and took some things away to make it the way I wanted it.  And there you're able to do things like turn the brightness up and down, turn the flashlight on, go quickly to the camera, rotation lock, other stuff like that.



And there's a little set of icons that allows you to put your phone, in the case of a phone, or your tablet into airplane mode and/or turn off WiFi, Bluetooth, and the cellular connection.  And so it's very handy.  They made it, like there it is.  You can get to it quickly.  Swipe up and tap tap.  What comes to light a week later is that, well, no.  If you turn them off there in the right-in-front-of you convenient and easy location, it doesn't turn them off at all.  It disconnects their connections that you have to things, giving you the impression that it's turned them off, like fooling you.  Just like, okay, it's off.  I mean, and the icon goes dark, everything looks off.



But they're not off.  They're not off at all.  They're not stopping communicating.  They're not reducing power, which is the other reason you turn off unused radios is you want power conservation.  No.  It just disconnects things so like your speaker goes doo-doo-doo and says, oh, yeah, look, Bluetooth's disconnected.  Yes, but your Bluetooth is still running on your phone.  It just disconnects.



So as people began to realize what was going on, they got upset.  Apple was forced to produce a formal explanation.  And what they said was, well, yes.  All we're doing is disconnecting things because we want - we, Apple, have decided for you that we know better.  And so AirDrop, AirPlay, the Apple Pencil, the Watch, then there are continuity features like Handoff and Instant Hotspot and Location Services, we think those are more important than you being able to turn your stuff off when you want.  So we're going to ignore you when you turn off WiFi and Bluetooth and have all those things still work.



The good news is you cannot - well, the bad news is you cannot do it from the Control Center.  The good news is you can still do it from the Control Panel, the original root of control, if you go to the little rotating gear icon, go in there.  When you turn them off there, they are actually truly off.  No more broadcasting, no more receiving, no more potential security vulnerabilities which otherwise exist constantly, thanks to things like BlueBorne - although in this case Apple was not a victim of those attacks.  Everybody else was.  So just a heads-up to our listeners.  You don't save power, you don't get security if you turn these things off from the Control Center.  You've got to go the root Control Panel in order to get what you thought you were getting from the Control Center.



PADRE:  Right.  And we've got people in the chatroom who are mentioning what you did, which is why Apple did this.  And we know why Apple did it.  And there is an engineering decision behind it.  They believe that you want to stay connected to peripherals, and the average user will not want to lose connectivity to the keyboard and the Pencil and the Watch that they might be wearing.  And I get all that.  But here's the thing for me, Steve, the fact that they put the icons for Bluetooth, the universally recognized icons for Bluetooth and WiFi, and you can gray them out by pressing them, just like you would in iOS, just like you would in Windows or OS X or Android, and it doesn't do anything.



STEVE:  Right.



PADRE:  That part, that's the betrayal to me.  That's the whole, well, if you weren't going to turn it off, why even put those icons there?  That makes no sense.



STEVE:  Yes.  For example, they could have had a three-state button where you press it and it gets a ring around it or something that says, okay, other things that Apple doesn't make are disconnected.  But Apple-trusted things are still there.  And then you hold it longer, or shake it or do something, and then it goes, like, totally dark, which is, okay, now, sorry, you can't type anymore on your Bluetooth keyboard.  Do that a couple times, you'll figure out what's going on.  But no.  They just said - and get this.  The Bluetooth spontaneously turns itself back on at 5:00 a.m.  I have no idea why.



PADRE:  Because at 5:00 a.m. you - we what?  Wait, I'm sorry, I didn't hear that part.  Seriously?



STEVE:  Yes, yes.  At 5:00 a.m. Bluetooth, bing, comes back on.  Every morning.



PADRE:  That's a very curious decision.  Wait, wait.  But if I turn it off in settings, it's off off; right?  That's like the actual way it powers down.



STEVE:  Yes.  If you turn it off in the Control Panel, it's shut down.  But if you do the Control Center sleeper, it wakes up at 5:00 a.m.  And also apparently if you wander too far.  It realizes, oh, he's somewhere else.  We maybe should turn on to look around again.  So there's both location and 5:00 a.m.-ness, which completely overrides what you've done, just because.



PADRE:  You're kidding.



STEVE:  And I've seen that.  I've noticed it's back on the next day.  And before I understood what was going on, it was like, okay, what is this, a bug?  No, this is a feature.



PADRE:  And I will mention, because they're asking about airplane mode in the chatroom, airplane mode does shut it off, but it also shuts off the cellular radio, which is normally what people don't want to do.



STEVE:  Right.



PADRE:  I mean, more than just security, and you mentioned this, Steve, I sometimes need to save power on my device, and so I'll shut down all the radios, the GPS, the WiFi, the whole - shut everything down.  And I can do that at a glance on Android, just by dropping down the menu and graying out the buttons.  I would be very upset if I learned that those radios just stayed on because Google decided that they should just be on.



STEVE:  And get this.  If you have some of those things turned off, then you go into airplane mode, when you come out of airplane mode - you can already guess where I'm headed - they're all back on again.  So it doesn't even remember the pre-airplane mode preferences that you had.  It says, oh, we're just going to err always in the direction of turning everything on all the time.  And when you turn them off, remember, you get warnings.  Oh, warning, location services are less accurate now.  It's like, yes, go away.



PADRE:  I will say that when the next version of BlueBorne comes out, and iOS devices are once again vulnerable, just remember, walk around with your scanner at 5:00 o'clock in the morning, and you'll probably find a few.



STEVE:  That's right.  Target-rich opportunity at 5:01.



PADRE:  Steve, there's been an ongoing battle in the enterprise/prosumer world with securing the devices that we use every day.  And unfortunately there's been some laxity among the manufacturers who don't think that supplying security past the first 90 or 100 days is really of any benefit to them.  D-Link had a case filed against it by the FTC where they were going to be held to account for some very shoddy network gear that they put out, specifically routers, wireless access points, and cameras.  But now there's a development in the case.



STEVE:  Well, yeah.  And we talked about several weeks ago the really atrocious discovery of - I've often said anybody can make a mistake, but people should be held accountable for their policies, the things that are done deliberately, the things that some committee decided.  And the D-Link router revelations were the latter of that.  They were just evidence of no one caring.  When you have fixed and publicly known administrative usernames and passwords exposed to the public Internet, that's unconscionable.  I mean, that's someone's decision to do that.



So we've often talked about the dilemma that we have in the industry and wondered how this gets fixed.  How do you get a company like D-Link to care?  How do you get those deliberate policies to change?  Because consumers just look at the prices and the features and how many stars it has on Amazon and go, oh, that looks good, and they click it, and they end up being victims often when what they clicked on has a security catastrophe.



So one solution, which we talked about at the time, was that the FTC, the U.S. Federal Trade Commission that has legal oversight and the goal to protect consumers against this kind of misconduct, filed a suit, a big suit against D-Link as a consequence of their clearly sloppy security.  The suit had six claims.  And what's in the news that you're referring to, Padre, is that a judge immediately tossed out three of those six.  And I looked at it, and first of all I didn't think that was good news because I want a company like D-Link to be held responsible.  This may not be the mechanism, but it is a mechanism that in the U.S. we have.



It turns out, though, I agreed with the judge, I agreed with his decision because - and the FTC should have known better.  Essentially, the judge was discarding the claims which alleged without proof that there was damage that resulted as a consequence.  And that's kind of the gotcha in this kind of situation.  The plaintiffs have to have standing.  And if you're alleging damage, you've got to have some.  You've got to have some proof.  You've got to have some evidence of damage.  And the FTC was just saying, "Bad D-Link, bad D-Link."  But you can't sue anybody over bad.  



PADRE:  Right.



STEVE:  Now, you could sue them if we had laws, but we don't have laws.  That is, if there was a law that any device that was being sold to a consumer could not contain a fixed static known admin username and login, fine, have a law.  Then when they do that, they're breaking the law.  But we don't have any laws.  We just have "Bad D-Link" and, unfortunately, "Go to your corner."  That's just not enough to sue.  So hopefully the other three remaining surviving claims will be sufficient to continue this legal case.  Maybe the FTC was just throwing everything at the wall, hoping that some would stick.  But the judge isn't buying it in this case, saying, well, no.  Yes, we don't disagree that they're bad, but there isn't a law against badness at this point.  So fix that, and then we can proceed.



PADRE:  I think the FTC was feeling the heat from the botnet attacks, of which we believe there were a lot of D-Link devices included.  And let me be clear, D-Link is absolutely at fault for a lot of this.  Some of the most basic vulnerabilities have existed in their products for over 10 years that they haven't  patched.  And that's a pretty - yeah, that's bad.  But as you mentioned, it's not illegal to make a bad product.



STEVE:  Right. 



PADRE:  And in fact the judge specifically said, look, this might be, quote, "an annoyance and an inconvenience," unquote.  But the FTC had not provided any specific instances of damage, monetary damage, and that's what the law is about.  Now, there are going to be people say, oh, well, this judge doesn't understand it.  This judge is actually quite good at this.  He's been very consistent in his positions.  He had a case in July of 2016 against Pokemon Go.  Some property owners were saying there was damage because there were portals put on their property.  And his thing was, look, unless you can show me what was actually damaged, I'm not going to award you $5 million.



He did the same thing in March of 2017 with the Telesocial case, who said that they were hacked by Orange, and they had some magical math that led up to, like, $100 million worth of damage.  And the judge was saying, I don't see a single dollar that's not completely theoretical.  And in June of 2015 there was OpenText's suit against Box, and it was the same thing.  It was like, okay, you allege that there was a hacking.  But before you even try to argue whether or not there was a hacking, where's the damage?  We're not going any further until you show me where the damage is.



And Steve, we've argued the other side because we've both talked about people, researchers, who have been caught, and then you have some trumped up charge that's brought against them alleging $50 million worth of damage because you sent an email, and a lot of our employees read it, and it cost time, and that's $50 million.  He's the judge you want on cases like that because he'll say, "Show me."



STEVE:  Yes.  And we know that there's a problem with, as they're called, "frivolous lawsuits."  And so there has to be pushback against that because the system can be abused in this way.  So as I said, I agree with the judge in this case.  As our listeners know, I am incensed that D-Link is conducting themselves this way.  But unless our current laws can be applied to create an environment where this is not permitted, then we need new laws.  We need to make it illegal to sell clearly irresponsible security to consumers.  When we have that, then that's what the FTC can use.



PADRE:  Precisely.  It's just our laws have not caught up yet.  People are saying in the chatroom that, oh, but there are laws against harm.  Yes, but in this particular case harm is defined by monetary loss.  And if you don't have monetary loss, then by the legal standing there's no harm.  Yes, it's irresponsible.  Yes, it's horrible.  Yes, you shouldn't buy from D-Link until they fix this.  But it's not illegal.  Again, the big push is the market sorts it; right?  If D-Link makes such a bad product, why are you buying it?  Why do you keep purchasing their gear?



STEVE:  Well, and of course I've always often talked about the license agreements that we click through which are complete hold-harmless agreements.  So everybody using Windows has given up all of their rights to ever complain about anything.  That's what it says in the fine print is no matter what this does, we're not responsible.  I mean, cars don't get that luxury.  The wheels fall off, that manufacturer's in deep trouble.  VW knows all about that with their emissions.  So the computer industry is in this weird blessed bubble of, sorry, you're using it, so good luck, even though we got your money.  It's incredible.



PADRE:  Okay, well, let's move to another story that has both good things and bad things.  We'd all like more accurate GPS; right?  I mean, I love it when my GPS doesn't have me jump from one side of the city to the next.



STEVE:  Or like you're following your mapping software, and it tells you to get onto the freeway at the next opportunity.  It's like, wait, I'm on the freeway.  But then you realize, oh, there's a road over to the side.



PADRE:  And they think I've, yeah, veered off.  No.



STEVE:  And that's where it thinks you are.  Anyway, so yes, all this by way of noting that Broadcom has - and this is just fun to know.  I just picked this up as I was putting things together, and I thought, oh, this will be just neat to share real quickly.  They've produced the first low-power, commercially feasible, next-generation GPS chip.  It's the BCM47755.  They started sampling it.  It will be appearing in next year's smartphones.  They're not saying which ones or whose.  But in 2018 this next-generation chip will start happening.



The current generation of smartphone, cell phone GPS chips use the original GPS ranging and positioning technology designated as L1.  And L1 technology uses a chirp which is lower complexity and longer, and a lower frequency.  All of those things, they work, but they have a problem with resolution, that is, your actual location resolution.  And they have a problem when you're in an environment with lots of reflections, like you're in downtown of a city, and the signals might be bouncing off of buildings, and that's a problem known as multipath.  That is, there are multiple paths by which a satellite signal can reach you due to reflection off of other objects, which tend to confuse things.



So the next generation is known as L5, which uses a much higher frequency, a much more complex burst pattern, which is, as a consequence, much shorter.  The shorter burst means that it's over by the time a secondary or tertiary or whatever reflection might reach you.  So the phone is smart enough, or this chip subsystem, the GPS subsystem, to only pay attention to the first one.  So the higher frequency has a reduced problem with multipath interference in general.  The shorter burst further improves that.  And the higher complexity, higher frequency signal, which can be carried in a higher frequency carrier, means yo get much better location accuracy - so much so that whereas what we've been using has five-meter accuracy, next year these chips will give us 30-centimeter accuracy.  So, wow.  Very cool.



PADRE:  That is such a jump.  I mean, military GPS has had increased accuracy since the start of the system.



STEVE:  Right.



PADRE:  But 30-centimeter resolution for a consumer product?  I didn't even think that would be allowed.



STEVE:  Yeah.  Yeah, it's going to be very cool.  Oh, and I forgot to say, half the power.



PADRE:  Oh, well, yes.



STEVE:  Half the power.  I think it's a 25-nanometer process or something.  And so they're reducing the size and cut the power in half.



PADRE:  Anybody who's ever used Google Maps on a phone and watched your battery just drain, or something like Pokemon Go or any AR game that has to tag their location in the real world, you understand that the GPS chipset actually pulls a lot of power, way more than you think it would because it doesn't actually transmit anything.



STEVE:  But it's doing so much work.  In fact, this has a dual-core ARM as part of it in order to do all the background math required in order to make this happen.



PADRE:  That's where the power saving is coming from, because you're not tagging the main processor anymore with doing all the triangulation.



STEVE:  Right, right.



PADRE:  Okay, that makes sense.  All right.  Well, hey, Steve, I don't want to jinx it, but I think we just had a positive story.



STEVE:  Oh, good, yes.  In fact, we have a few more coming up.  Although we do have another caution.  I meant to put a picture of this in the show notes, Padre.  It is on the page if you click the link in the show notes, so at least our video watchers can see it.  But this is just a reminder.  One of the things that's happening is that things consumers get are containing high-density barcodes, specifically boarding passes.  And there is on the screen now.  That is a very high-density barcode.  And as an engineer and a techie, I would look at that, and I would think, ooh, there's a lot of information in there.  That's more than my gate number and my flight number and my last name.  That's so many bits, that could contain a lot.



It turns out it does.  So one of the things that people are beginning to do is, because of the ubiquity of cameras and Snapchat and Facebook and everything, where it's so easy to post stuff, people are taking pictures of their boarding passes and putting them online, saying look where I'm going.  Well, it turns out, yes, and what you're also posting is, "and look who I am."  It's like your passport number, your date of birth, passport expiration, all kinds of stuff which you don't want to be posting publicly is encoded in that high-density bar.



So if you've got any photo manipulation software, fuzz that out.  Blur it into a gray rectangle or a gray ellipse, or scribble over it, or do something.  Or just snip it out and don't post the portion of your boarding pass that contains a high-density barcode unless you know what it contains because it could have way more information and currently does than you believe you are posting, much more than just is on the face of the boarding pass itself because it's computer-to-computer information, and they're like, hey, we got four kbits.  Let's use them all.  Right.  Just don't...



PADRE:  How easy it would be to modify people's high-density barcodes. 



STEVE:  Oh, yeah, true.  You could get into some mischief.



PADRE:  I mean, it's just a bunch of blobs of ink on my boarding pass.



STEVE:  Right, and no one can see them.



PADRE:  I don't want to suggest that, Steve, because someone will create some device that will thermal print a new boarding pass that you could put on people's - no, let's not do that.



STEVE:  Yeah.  So we also had, and this is - this kind of went by, and I thought, yeah, okay.  But it came to so many people's attention.  Again, my policy is always mistakes happen.  Policies are what people should be held accountable for.  Mistakes, yeah, they happen.  Maybe this demonstrates a process failure where you might say, okay, we should have a process to  have someone check postings before they're made public.  The news is that Adobe inadvertently publicly posted their private PGP key.  Whoops.



PADRE:  [Sighs]



STEVE:  Yeah.  And in backtracking it turns out that they were using a PGP extension for Chrome and Firefox known as Mailvelope.  And there's a web page interface where there's three blue buttons.  The left-hand one says "Public," the middle one says "Private," and the third one says "All."  And the person who did this was wanting to post the public PGP email key, but "Public" wasn't pressed.  "Export All" was pressed.  And so the extension dutifully posted both:  in the first half, the public key; and, in the second half, the private key.



PADRE:  Why does that button even exist, Steve?  When would you want to post both your public and your private key?



STEVE:  And not only that, why isn't there, like, a warning?



PADRE:  Are you really sure you want to do this?



STEVE:  Big, red, "Hey, you're asking for your private key.  Are you really...."  And then I'm sure the person would have gone, ooh, crap, thank you, thank you, thank you, thank you, thank you.  Instead, just cut, copy, and paste, and there it is for the world to see.  Needless to say, they took it down.  They rekeyed.  And now they posted their new public key.



PADRE:  This is the security equivalent of drunk texting.  Oh, no, no, I didn't want that.  That's when you really wish that there was a recall button for that message you just sent out.  Wow.



STEVE:  Once it's on the Internet, it's loose.



PADRE:  I mean, okay.  It's easy to make fun of this.  But this could happen to people.  People post the wrong stuff all the time.  It does really make me wonder about that interface, though, that you've got those three options right there, and it is so easy to post any of them.  That's just bad UI.



STEVE:  Yeah.  I mean, I'm sure the Mailvelope developers must be getting feedback that a techie in Adobe let the wrong data be published.  You're right.  It's unconscionable that this thing doesn't say, wait, you've got to stand up, click your heels together three times, and hold Ctrl-Alt-Del down in order to enable this button.  But no.



PADRE:  And again, people will say, well, the developer of the software never knew that someone would be stupid enough to - but we're past that point; right?  We now have to be at the point where security comes first, not at the end.  We're not trying to tie up the things that might be a problem.  As we develop a solution, you always have security at the forefront.  And when I'm designing that UI, I'm thinking to myself, is it a good idea to have the public key right next to the private key?  And the answer is no, of course not.  There would be no situation where I would want those posted at the same time.



STEVE:  That's a very good point.  Good, yes, that's even better.  You're right.  The UI should not even offer in that context to put out the public key, I mean the private key.  It should be deeply protected so that it's easy to expose and provide your public key, which you're having to do all the time.  But the things you never want to do shouldn't be next door to the thing you always want to do.



PADRE:  Yeah.



STEVE:  Two quick notes about SpinRite, just very...



PADRE:  Oh, yes, please.



STEVE:  They are quick.  Vigen Galustian, I hope I pronounced your name right.  If not, sorry.  Vigen, maybe Vigen?  Yeah, I think Vigen Galustian in West Hills, California.  He said:  "I'm an avid user of SpinRite.  It has saved me several times.  I hope in your future upgrade you can make it to support USB connection and independent of OS."  So I just wanted to say, I did mention this last week, but that's absolutely in the - I want to say "floor plan."  That's not the right word.  The development - there's a word for that.  The development timeline.  I'm going to do that second.



First comes AHCI support, which will be native hardware and OS dependent.  So it'll be able to run across platforms on Macs and Linux and so forth.  So we will allow people to download an ISO that they can burn, and I'll get to the next point in a second about that.  So, yes.  But because USB is slightly less important than AHCI, which is now the universal standard platform hardware - and I don't want to delay SpinRite, that is, the 6.1, a second longer than I have to.  And as I said, I'll be making it available even before its release to our podcast listeners as thanks for their long-term support.



6.1 will happen immediately when we get the crazy performance boost and expanded compatibility.  And then 6.2 will be similar low-level hardware UHCI, and there's an EHCI, I think it is.  Anyway, it'll support the USB 1.1, 2.0, and 3.0 hardware-level chips to provide the same kind of raw, low-level, high-speed performance over USB links.  And that'll be in 6.2 that I will immediately start on.  This is all going to happen in a single development continuum.  But my intention is to drop out intermediate technology in a highest priority to incrementally lesser priority sequence, all of that being intended to create the next-generation hardware and driver foundation for SpinRite 7.0, which will be where we go crazy with features.



So first is hardware compatibility and performance and OS independence, and then 7 will be like all the stuff people want for multiple drives, doing multiple drives at the same time, extracting files themselves rather than fixing the surface, lots of file system recovery and reconstruction.  Basically 7.0 is where I think SpinRite will stay for a while as we produce all those other features.



PADRE:  Of course.



STEVE:  And then Greg - and he put his location as "Somewhere."  So Greg is somewhere.



PADRE:  He trusts no one.  Good man.



STEVE:  Yes.  His subject line was:  "Will new SpinRite make it easy to run SpinRite off a USB drive?"  And I thought, Greg, it is now.  So he wrote:  "It is more trouble to always have to get out a CD drive to use SpinRite.  Sure would be handy if it was easy to put SpinRite on a USB stick and run it from there.  The current method for doing this is too complicated for me.  Hopefully new version will make this easier."  Okay.  You put the USB stick in your computer, and you press the drive letter...



PADRE:  Yeah, it does it right now.



STEVE:  ...of the USB, and it does it right now.



PADRE:  I just did that, like, three days ago.



STEVE:  So anyway, Greg, we're not laughing at you.  There are no bad questions.  I just wanted to tell you, read the user interface.  And when that screen comes up, you'll find that it says to install it on a USB drive, just press the keyboard key for the drive letter, and it'll confirm that's what you want to do because it does wipe out the contents that you currently have.  It formats your USB stick for you, making it bootable.  And it boots the USB and runs SpinRite.  So it's already in there.



PADRE:  I will say, though, there are some flash drives out there, typically the really cheap ones...



STEVE:  That will not boot.



PADRE:  That will not boot.



STEVE:  Yes, yes.



PADRE:  So if you put it on that, it won't boot.  And it just means try a different drive.



STEVE:  Right.



PADRE:  And unfortunately there's not a really good guide about which chipsets will boot and which ones will not.  It's just trial and error.  I've separated all - because I have literally hundreds of flash drives from the different events I go to, and I'm sure all of them are safe.  But I have two tubs, and one says "Bootable" and one says "Not Bootable" because I'm always making bootable USBs.



STEVE:  Ah, nice.



PADRE:  Oh, Steve, I did want to add in one thing because I won't be able to do it next week.  But I have a Brother from - he's Nigerian, but he's living in Zambia.  And he runs a school there.  And they had lost all of their semester's data and the yearbook pics on a bad drive.  And he was telling me about this over the summer.  I was in Portland doing that two months of off-the-grid type thing.  And so I actually gave him my - in my toolkit I had the USB drive with SpinRite.  I said, "Try this once and see if it works."  And he wrote back to me just a week after he got home, and he said, "Oh, my gosh, I tried it.  It got it back.  We copied all the data off.  I'm going to go buy a copy now."  I said, "Yes, that's what we want."



STEVE:  Perfect.



PADRE:  From one IT Jesuit to another.



STEVE:  Interdigital Jesuit.



PADRE:  Indeed.  Now, that's going to close the loop.  And I can't believe this, we might actually get through all the material, which I didn't think was possible.



STEVE:  Actually, no, we're going to skip that.  Let's go to the DOM fuzzing.



PADRE:  Even better.



STEVE:  Because, yeah, because we've got eight minutes to go, and we are actually going to get through all of our material right on time.  So this is the title of the show.  And I just, again, I said, okay, if I have the opportunity to call the podcast "The Great DOM Fuzz-Off," how can we not do that?



So an engineer named Ivan Fratric joined Google's Project Zero.  And in the past, prior to joining Project Zero, I don't know if he was at Google or where he was.  But the way he explained it, he had a history of having explored DOM fuzzing.  And we'll explain what that is in a second.  And so he had already implemented a number of previous systems.  So he decided that, upon joining the Project Zero team, that he wanted to start fresh using all the experience he had acquired to create a next-generation and open-source - this is all up on GitHub - DOM fuzzer.



Okay.  So what's a fuzzer, and why are they useful?  Well, the Document Object Model, which is what DOM stands for, is this increasingly complex, but beautifully unified means of programmatically accessing the contents of a web page.  So you're able - there's this notion of the body of the page, and the body contains sections, and the sections contain paragraphs, and the paragraphs contain content and tags and forms and so forth.  All of that has been formalized in a structure, and it's inherently a hierarchy so that you have lists of things.  And then those objects contain lists, and those objects contain lists and so forth, forming something that can be - it can be enumerated, that is, it can be stepped through.  It can be examined by code.  It's a beautifully powerful system, which is what enables today's dynamic web pages and all of the stuff that we're doing.



And of course part of that is JavaScript, which is the default language which can be run on the page to be the thing that looks at the DOM, the Document Object Model, and participate in it, and be part of it also because you can have JavaScript embedded in these objects, or invocations of JavaScript.  It's very complicated, but it's a system which now works and is well-defined.  The problem is it all uses the "I" word, which we talk about from time to time.  It is one big massive interpreter.  And as we know, interpreters are hard.  Interpreters are difficult to get right.  And the reason is that they're built to interpret what their designers build them to interpret.  But it's very different to do that than to have them robust against abuse of interpretation.



Fuzzing is an intriguing idea.  That is, and we talked about this, I remember one of the early cases was Eeye Systems in Aliso Viejo, Southern California.  They had a whole bunch of PCs, I think they were Windows machines, that they were fuzzing.  And they were finding all kinds of problems that had never been found because, after all, an API is an interpreter.  It's a set of calls, and you give it arguments, and it does something with the arguments.  It interprets what you give it, and hopefully it doesn't crash.



And so what was happening was Eeye Systems, they were crashing all these Windows machines, and then they would look at the log of the fuzzing of just the nonsense that they had given to the machine, and then they would go, ooh, why did that crash?  And that's the beginning of finding an exploit is because it should never crash.  And so what Ivan did was he wrote a next-generation DOM fuzzer.  That is, he wrote technology to create crazy, insane, non-logical web pages containing web content and JavaScript, designed to find crashes, designed to crash the web browser.  And that should never happen.



PADRE:  Steve, let me ask you this.  So the fuzzer is just generating random elements.  It's just putting everything in there, trying to see how it will be interpreted.  Do I get to control the randomness of those elements?  Can I ensure that it's running all the elements that I've tried in the past?  Or is this just truly pile everything into a blender and let it stream out?



STEVE:  Yes.  It needs to be structured.  And in fact in his blog posting he talks about the fact that you could do random stuff, but random stuff isn't going to mean anything.  And if you did deterministic stuff, well, it would all mean something, but it wouldn't be random enough.  So you have to have a constrained system that still has sufficient degrees of freedom to, like, poke into the corners and find the dark spots and the soft spots and the stuff that hasn't been caught by the authors.  And, I mean, these are mature browsers that are in use right now.  We're all using them.



So what happened was no browser survived.  Everything collapsed under the attack, essentially, the benign probing of this next-generation DOM fuzzing technology.  As it happens, Google's Chrome Blink engine was found to have two bugs that were - they were Project Zero.  They were zero days.  They were unknown.  Firefox's Gecko engine had four.  The Trident engine in IE also had four.  The Edge HTML engine in the Edge browser was found to have six.  And the WebKit engine in Apple's Safari browser was the "winner" in that sense.



PADRE:  The winner, yeah.



STEVE:  The big loser had 17 different problems.  All of these have been assigned Project Zero bug IDs and have been responsibly disclosed to their vendors, and I'm sure are in the process of being fixed.  But anyway, it was just really interesting.  He wrote:  "We tested five browsers" - those five I just said - "with the highest market share:  Google Chrome, Mozilla Firefox, IE, Edge, and Safari.  We gave each browser approximately 100,000,000 iterations with the fuzzer."  That's the other thing.  With this kind of automation, you can just do far more than you could ever do, even with a bunch of monkeys typing on the keyboard.  I mean, this thing is fast, and so it gives you great coverage.



So it says:  "We gave each browser approximately 100,000,000 iterations with the fuzzer and recorded the crashes."  Again, no browser should crash when you give it a web page.  It's not supposed to do that.  When it does, that's a reason to go look and find what it was that made it crash; what part of the fuzz that it received it was unable to handle.



PADRE:  Was there any consensus on a particular fuzz that was causing the most crashes?  I'm looking at the numbers, and maybe like 10, 11 I see a couple times.



STEVE:  Ah, good point, yes, repeats of the same debris, yes.  So he said:  "If we fuzzed some browsers for more than 100,000,000 iterations, only the bugs found within this number of iterations were counted in the results."  So it may not have been the first hundred million, but a window of a hundred million somewhere.



He said:  "Running this number of iterations would take too long [yeah] on a single machine and thus requires" - and I love the phrase - "fuzzing at scale."



PADRE:  Overfuzzing.



STEVE:  Yeah, fuzzing at scale.  Overfuzzing, that's good.  "But it is still well within the pay range of a determined attacker.  For reference, it can be done for about $1,000 on Google Compute Engine, given the smallest possible VM size, preemptible VMs," he says, "which I think work well for fuzzing jobs as they don't need to be up all the time, and 10 seconds per run."  So you can spend $1,000.  And if you are good enough to write this code - and it's now in the public domain, it's now open source - you, too, can find zero days in popular web browsers.



PADRE:  That's scary.  That's absolutely scary.



STEVE:  And fuzzing is cool.



PADRE:  Now, so the zero bug IDs that they identified in this test, those are just ones that crashed a browser.  But if it's listed here, does that mean that they can reliably crash the browser with that bug, or only in a certain sequence?



STEVE:  It means that - okay.  Yes.  In order to qualify for a Project Zero zero day, it has to be a zero day.  I mean, so they opportunistically found it; but then they said, whoa, what just happened?  Because - and this is always the case.  You threw some monster crap at the browser, and it collapsed.  But it turns out it's only one number that had a semicolon or a back tick or something in it that was the culprit in this much larger page.



PADRE:  So you rewind the sequence and keep playing back little bits and pieces until you find a particular sequence.



STEVE:  Exactly.  Exactly that.



PADRE:  That makes sense.



STEVE:  Yup.  And then you have found something that a bad guy could exploit if you didn't fix it first.



PADRE:  I remember looking for zero days back when I was in high school.  And the idea that sometime in the very near future I would essentially be able to spin up the equivalent of tens of thousands of workstations and test against them for basically pennies...



STEVE:  Wow.



PADRE:  Yeah.  I always thought it was going to be, oh, I'm just going to be working on maybe one or two workstations, and maybe I'll luck into a vulnerability here or there.  But this is a blueprint for making zero days in sort of an assembly line.



STEVE:  Yeah.



PADRE:  Which, again, scary.



STEVE:  Yeah.  And I think what this also demonstrates, we saw this before with Eeye.  We've talked about fuzzing a number of times.  It is too valuable not to do.  It ought to be part of any security-oriented project's development chain, is when you think you've got it all nailed down, throw a bunch of crap at it and see if it survives because, if it doesn't, you need to find out why.



PADRE:  Well, Steve, I think our audience has been thoroughly fuzzed by that last story, so this might be a good time to end.  My friend, it is always a pleasure to work with you.  And again, I miss Leo when he goes away.  It's always fun having him in the studio.  I learn so much from him.  But when he does go away, it means I get to play with you, and that's always a privilege.  So thank you very much for letting me participate in what has been a grand pageant of security awesomeness.



STEVE:  Has been.  And Father, it goes both ways.  I know that Leo is enjoying his vacations.  He announced earlier this year he plans to be taking a lot of them.



PADRE:  Yes.  I'm looking forward to that.



STEVE:  So you are certainly welcome back.



PADRE:  Folks, that does it for this episode of Security Now!.  Don't forget that we're live here on the TWiT TV Network every Tuesday at 13:30 Pacific time.  Steve will always be here to inject you with some healthy, healthy paranoia, which is good in this day and age, and help you understand the wonderful world of security because the more you understand, the less security seems like a black box. 



You can also find all of our shows at the show page at TWiT.tv/sn, as well as iTunes, Stitcher, and wherever fine podcasts are found.  You can also find high-quality audio downloads on GRC.com, which is also where you'll find everything GRC has created - SpinRite, ShieldsUP!, and, coming soon, SQRL.  I'm Father Robert Ballecer, the Digital Jesuit, saying that if you want to keep your data safe going into the future, you're going to need some Security Now!.



STEVE:  Thanks, Padre.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#631

DATE:		October 3, 2017

TITLE:		Private Contact Discovery

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-631.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss some aspects of iOS v11, the emergence of browser hijack cryptocurrency mining, new information about the Equifax hack, Google security research and Gmail improvements, breaking DKIM without breaking it, concerns over many servers in small routers and aging unpatched motherboard EFI firmware, a new privacy leakage bug in IE, a bit of miscellany, some long-awaited closing-the-loop feedback from our listeners, and a close look into a beautiful piece of work by Moxie & Co. on Signal.



SHOW TEASE:  It's time for Security Now!.  I'm back.  Steve Gibson's here.  And we have a lot to talk about, including a little more information about how Apple's Face ID works.  A judge who says, no, the FBI doesn't have to tell you anything about how it unlocked that iPhone.  And Moxie Marlinspike in another discovery, this time Signals the victim.  Plus the secret life of bees.  It's all coming up next on Security Now!.  



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 631, recorded Tuesday, October 3rd, 2017:  Private Contact Discovery.



It's time for Security Now!, the show where we cover the latest news from the security front.  It is a front.  It's a war out there.  Thank goodness General Gibson is here, Commander in Chief.



STEVE GIBSON:  Welcome back from your two-week cruise around the world, wherever it was you went?



LEO:  No, I just went up the river, that's all.



STEVE:  Up the river, okay.



LEO:  Up the river.  But it was a nice vacation.



STEVE:  Well, fortunately you came back down the river.



LEO:  Yeah, and thanks to Father Robert, who everybody agrees should be hosting the show from now on.  It's okay.  It's okay.  It doesn't hurt my feelings much.  But I am glad to be here.



STEVE:  Well, you did announce you were loving your vacations and your travel.



LEO:  I'm not giving up this show.



STEVE:  No.



LEO:  We've been doing this 13 years.



STEVE:  Yes, we are in our 13th year.



LEO:  Unbelievable.  And the good news is no security problems at all this week.



STEVE:  Yeah, yeah.  We're just going to...



LEO:  Did you see, there's one that just broke that is hysterical.  It has nothing to say about it.  But Yahoo now admits that every account was hacked.  Three billion, with a "B," accounts were hacked in that 2013 exfiltration.



STEVE:  And I liked your comment on MacBreak Weekly, that that would be the largest ever.  And one reason is that there used to be much more concentration into a single provider.  And now we have a much more heterogeneous environment, where not everybody is at Yahoo the way they used to be.  So I think it is the case that no one outfit will be able to be as devastating.



So Episode 631 today.  As I was pulling things together, one interesting bit stood out that acquired the title, which is the reason this is called "Private Contact Discovery."  Our friend Moxie Marlinspike and his gang working on Signal have tackled the problem of concealing contact metadata, which is an unsolved problem, really, until now.  We've talked, for example, about how, even when, for example, email content is encrypted, the fact of the mail moving from person A to person B cannot really be hidden.  And so it's often the case that who you're talking to is as much a useful piece of information and, unfortunately, a breach of privacy as what it is you say.



And in the world of instant messaging, which Moxie's Signal protocol addresses - and we've talked about Signal's ratchet mechanism and what a perfect job they have done, so much so that other platforms have adopted the open Signal Protocol for themselves because it's so good.  One of the remaining problems has been how does a new person join the Signal ecosystem, or the Signal universe, and discover who in their contacts are other Signal users without the central repository that glues all this together knowing that they've done so, like knowing what contacts they have in common.  Anyway, they figured how to do this.  So we'll wrap up the podcast talking about that.



But of course we do have our week's worth of news.  We're going to talk about - I have to talk a little bit about my experiences with iOS v11.  Won't take up much time, but I just want to kind of go on record with what I have found.  Also, for a couple weeks there have been a number of stories about various instances of browser hijack cryptocurrency mining.  And so I want to talk sort of generically about the emergence of this as a thing and what it means.  We have some additional information about the Equifax hack, some amazing coverage thanks to Bloomberg.  Google's security research is uncovering new problems.  They have also, anonymously, some people at Google have announced some plans for improvements of Gmail security as a consequence essentially of this emerging awareness of how much Russia was involved in this past presidential election.



There were some specious rumors, well, actually stories about breaking DKIM, which is the email authentication protocol that we've actually been talking about not too long ago.  Turns out it wasn't broken, but we'll explain what happened.  Also there's some new concerns over many small servers in routers which are exploitable.  And you also were talking on MacBreak about motherboard EFI firmware concerns, mostly focused on Apple, as you said, because the researchers were able to more easily explore various Apple Mac systems and the firmware that they had.  But this - we're going to broaden it, of course, for this podcast and talk about also the impact on Windows and Linux-based hardware.  There's a privacy leakage bug in IE.  We'll do some miscellany.  We've got a bunch of interesting, and I think pretty quick, closing-the-loop feedback from our listeners.



And then we're going to take a close look at this beautiful work that Moxie and his group at Signal have done in order to protect the discovery of other people in your existing contacts who you want to converse with over Signal without disclosing the fact that you have found them, which turns out to be a much harder problem than one would expect.  So I think a great podcast.



LEO:  Yes.



STEVE:  Our Picture of the Week just cracked me up.  It caught me by surprise.  Someone tweeted it to me, and it's apropos of the data disclosures that we've been having lately.  It shows some bees flying around in a hive, and the caption says, "Security Failure:  EpiPen's Database of Everyone Who's Allergic to Bees Has Been Obtained by Bees."



LEO:  [Laughing] That's good.  That's good.



STEVE:  I just thought it was perfect.



LEO:  Oh, you're in trouble now, boy.  The bees know who you are.



STEVE:  Yeah, the bees know who you are.  They know how to selectively sting because they only get one shot at that, I think.



LEO:  That is funny.



STEVE:  I thought that was great, yeah.



LEO:  Wow.



STEVE:  So just briefly, on iOS 11, I don't know yet how 11.0.1 compares.  But right out of the gate it was the buggiest release of iOS I have ever experienced.



LEO:  What problems did you experience?



STEVE:  Oh, my god.  Like everything I tried, every one of my devices - I have, I don't know, like seven pads and my phone - and they were all doing bizarre things.



LEO:  Oh, dear.



STEVE:  Like I remember in iMessage one of the balloons got stuck so that, when I changed to a different message channel, the other ones were scrolling underneath this...



LEO:  Oh, I've seen, yeah, I've seen weird cosmetic things like that, yeah.



STEVE:  Yes.  And the dock got stuck on the vertical side.  And as I rotated, everything else rotated around, but the dock just stuck where it was.  And on one of my pads, whenever the preview, when the screen is locked where you're able to see things, every time I touch it to scroll it, it flashes white really briefly and then kind of settles down.  I mean, just if Jobs were still around, heads would be rolling, or they wouldn't have shipped it.  I don't know.  But, I mean, I was just, you know, I love iOS.  I use it as much as I use Windows.  And it just completely collapsed.



LEO:  So what did you do?  Did you roll back?



STEVE:  No, I just kind of shook the pads more and turned - I often turn them off and back on, and then it's okay for a while.  I mean, it's really - and I'm hoping that 11.0.1 that came out, what, late last week, hoping that it's going to be an improvement.  And I think it'll take a few times.  I mean, it's a big change.  And I've heard you loving the Control Center, as I do, too, now that I've kind of got the hang of switching and going to the Control Center and all that.  So there's some learning stuff.



But the other problem I have, and this is not something they can fix, is it's really slower on older hardware.  And so, I mean, again, this is sort of a natural thing that's happening is they're wanting to do more aggressive, deeper features that are inherently synchronized with the ever more powerful chips that they're releasing in their newer hardware.



The problem is they're updating older hardware with the newer software.  And for a while I was misunderstanding its lack of response as it wasn't getting my screen clicks.  And there is no feedback.  It's not like it goes click when you tap the screen.  The assumption is its response will be immediate, so you get visual confirmation that you pressed the right thing.  And so what I've learned on my older pads that are all now, I mean, I moved to iOS 11 on everything, is you tap, and you just kind of be patient.  You wait for a while, and then it goes plink and does whatever you ask it to.  But, I mean, it's a noticeable delay.  It's beautiful on the latest hardware; but, boy, you can really feel it on the older hardware.



So I don't think there's a solution for that.  Apple continues to want to be on the absolute bleeding edge with their hardware.  They're wanting to leverage the newer hardware.  But they're also not wanting, for example, to continue to support iOS 9 or 10 in perpetuity.  And I understand that.  But, boy, the older hardware just doesn't have the horsepower to keep up with what the latest hardware is doing.  You really do need the newer hardware.  And of course Apple wants you to buy that because they're in the hardware-selling business, as we know.  So anyway, just my few cents' worth on iOS 11.  I love what they've done, but you really - and I can see myself maybe moving to a newer phone, not because there's anything wrong with my 6S whatever it is, or 6 Plus.



LEO:  Just the speed, yeah.



STEVE:  It's just getting older, yeah.



LEO:  I mean, you look at the benchmarks for the new processor, the A11...



STEVE:  The 11, yes.



LEO:  And it's through the roof.  It's almost MacBook Pro speed. 



STEVE:  Yup.



LEO:  And there's no real need for that.  But, yeah, you have to figure that, once they've got it, people will find ways to use it.  We've seen that, remember, with Windows; and we've seen that for years.



STEVE:  Yes, yes.  And I actually think that's one of the things that happened where we were having the audio problems was that we moved - we thought that maybe the old USB interface I was using was getting just a little glitchy.  It was 12 years old.  I hadn't changed it through the entire life of the podcast.  So we switched to a newer, more aggressive interface, and that older machine didn't have the horsepower to drive it.



LEO:  Interesting.  That makes sense, yeah.



STEVE:  So, yeah.



LEO:  That's the way of the world, alas.



STEVE:  Yeah, it is.  And you know me, I stick with what I've got until I'm finally driven off of it.



LEO:  Get off of it.  You've got to get off of it.



STEVE:  I'm now feeling like, well, okay.  And I'm sure that the iPhone X is going to be gorgeous.  So I will have skipped the 6S and the 7 and the 8 and probably...



LEO:  Oh, you're going to see a big difference.  Oh, my gosh.  You're on a 6?



STEVE:  Yeah.



LEO:  Oh, my.  Oh, my.



STEVE:  And I'm waiting.  I click, and I touch and wait.



LEO:  Oh, my, yeah.



STEVE:  Yeah.  So Apple published an interesting security guide for Face ID that didn't have a ton of new information in it, but it was interesting.  We know that it employs what they call their "TrueDepth camera system."  But one of the things that I appreciated that I want to share with our listeners, because I read the whole thing carefully, is that it demonstrates a truly gratifying focus, if you'll pardon the pun, on security, where things they did demonstrated that, I mean, it was above and beyond.



For example, we know that they project an infrared dot grid onto the user's face that an IR camera sees.  And they use that essentially in order to put little spots all over the person, like chickenpox spots, in order to use that as part of their recognition.  Well, for example, the sequence in which the dots are drawn is device-specific on purpose so that - because what they're doing is they're actually scanning with a dot sequence, and so the camera picks up where each dot is in three space; and the sequence is device specific so that you cannot capture the data from one device and then somehow arrange to play it into another device in order to spoof it.  It won't get fooled because the sequence will be wrong.  So they use a per-device pseudorandom sequence.  I mean, and just that demonstrates, okay, somebody really spent some time to think about how to make this as robust against hacking as possible.



So, and other information, just to sort of assure people, is that it is explicitly local authentication, meaning that the facial recognition stuff never leaves that device.  It has this crazy, as we were just talking, this A11 processor with a hardware assist neural network, what they call their "neural engine."  And so what do they call it, a "bio neural chip" or something.  The reason the bio-ness is in there is thanks to the fact that it uses a neural network as part of the processing of this image data.  But, for example, it's been controversial that devices like Amazon's Echo, after you do the "hello" word in order to activate it, then it streams your voice to the cloud for recognition.



And so the point is that the phone has so much horsepower, so much processing power, the iPhone X, that it's all done locally.  So none of your facial images, none of that that the camera sees during this process, ever leaves the camera.  It stays there, and all the processing is done locally.



And then the other thing that they have done, and the way they described it I wanted to quote this because, again, it demonstrates a real thoughtful set of tradeoffs.  They said:  "To use Face ID, you must set up iPhone X so that a passcode is required to unlock it.  When Face ID detects and matches your face, iPhone X unlocks without asking for the device passcode. Face ID makes using a longer, more complex passcode far more practical because you don't need to enter it as frequently.  Face ID doesn't replace your passcode, but provides easy access to iPhone X within thoughtful boundaries and time constraints.  This is important," they write, "because a strong passcode forms the foundation of your iOS device's cryptographic protection."



And what that put me in mind of was that I had made exactly the same set of tradeoffs in the SQRL client.  That is, you are encouraged to create a strong passphrase to authenticate yourself to SQRL.  But once you've done so, you are then, within certain conditions, allowed to use a much shorter, in fact, you just use the first N characters of your much stronger passcode to reauthenticate yourself once you have first authenticated yourself.  And Apple has done exactly the same thing.  For example, we know, we're all familiar with how they handle Touch ID and how you must, if you haven't used a Touch ID-based device for two days, you must then reenter your full passphrase.



Well, that has a couple consequences.  One is it's good for security.  It also keeps you from permanently forgetting what your longer passphrase is, if you're not constantly using your device.  So they've done the same sort of thing, even on a device which is Face ID unlocked, so that the passcode is still required in a set of different circumstances:  after it's just been turned on or restarted; again if it hasn't been used for two days, for 48 hours; if the passcode hasn't been used to unlock the device in the last six and a half days, so just shy of one week (for some reason they chose 156 hours); and Face ID has not unlocked the device in the last four hours.



So again, a set of interesting sort of heuristics that they settled on in order to create sort of a flexible security boundary based on how active you are, but also to continue refreshing your use of your face and your weekly use of your original passcode because that's the ultimate unlock.  And then they have more accessible, shorter term unlocks as long as you do it often enough.  And I get that because it's the same sort of thing that I have just recently done in order to reach a compromise between having a passcode which is strong enough to be resistant against cracking, yet offers a tradeoff, much as Face ID does, to still allow you to use your device frequently and conveniently every time you pick it up.



And as for this whole one in a million that they're now claiming for facial recognition versus one in 50,000 for Touch ID, it's like, well, I think we'll have to see how that goes.  I'm sure people will be experimenting with trying to hack it, much as we saw with Touch ID, where they were immediately using gummi bears and things in order to lift prints and create fake thumbs and so forth.  So I think we'll have to sort of see how this plays out.  But, boy, I couldn't be happier with the clear attention that Apple has paid to getting this right, right out of the gate.  And of course we haven't seen it yet.  It'll be some time before we do, I guess.



But anyway, very, very impressed.  And that grid is 30,000 infrared dots which are used - not just 100, 30,000 - in order to form what they call a "depth map" in order to differentiate your actual physical face from a 2D infrared image.  And they also do pupil tracking dynamically to determine that you are looking at the camera, so you can't be looking away or have your eyes closed and have this function.  So that's how the kids are prevented from holding Mommy's camera up in front of her while she's sleeping and unlocking the phone.  Nope, Mom's eyes have to be open, and they have to be pointed at the phone, so pupil recognition is used in order to verify that your eyes are focused on the device itself.  So again, I think they did everything that they reasonably could.  We'll have to just see over time how this evolves into the actual user experience.



So in the last couple weeks there has been a number of articles about "malicious" ads doing cryptocurrency mining on users' browsers unwittingly, that is, unknown to the user, when they were visiting a site.  And then most recently Showtime, several Showtime domains were found to be also covertly mining cryptocurrency.  This wasn't Showtime trying to augment their revenues, this was some bad guys got into the Showtime server and added crypto mining technology to what was produced when a user went to the Showtime properties and downloaded a Showtime page.



So, okay.  So I just sort of wanted to address the whole idea of this a little more broadly.  So first of all, what we remember from our initial discussion of bitcoin mining is that a variable, we could call it "variable hardness," but it's really a variable probability problem is being solved by a cryptocurrency miner.  That is, the idea is a random number is being chosen, and over time the probability of guessing correctly is decreased, making it harder and harder to guess the magic number.



And thus miners that have gone from just CPUs to GPUs to FPGAs to custom hardware, they've gotten faster at guessing, the idea being that the probability of guessing correctly is vanishingly small now, so the more times you're able to guess per second, the greater the chance that this diminishing percentage of chance will be correct.  And if you guess correctly, you win a coin or a fraction of a coin, and that's changing over time, too.  Everybody remembers how, when I first did the podcast on bitcoin, I started up a miner - in fact it was on that machine, Leo, the machine that's now too slow even to run audio for Skype sufficiently well.  I woke up the next morning and there were 50 bitcoins sitting there.  It was like, oh.



LEO:  But that was kind of luck; right?  It wasn't that it was working so hard.



STEVE:  Precisely.  Precisely.  I didn't expect it.



LEO:  It was like a jackpot.



STEVE:  And it's luck for everybody, and I'm glad you said that because that's exactly what's happening here.  So the idea is that bad guys are putting cryptocurrency-mining JavaScript into users' browsers, into ads or stuffing them onto websites that are hackable, like apparently Showtime's was.  And they're getting all the visitors of Showtime or all the people who display this hijacked ad to do a little crypto mining for them.  And the chances are, I mean, first of all, a browser is a bad miner.  I mean, it's like so far down the curve you can't even find it.  You don't want to be doing cryptocurrency mining in your browser.



LEO:  No, you want custom ASICs and massive GPUs and SLI, yeah.



STEVE:  Precisely.  But it costs them nothing to...



LEO:  Well, that's the other way to do it.  Go massively parallel; right?



STEVE:  Exactly.  Exactly.  And so here's the other problem, and I realized as I was putting this together that I haven't checked in with Mark Thompson...



LEO:  I was going to ask, yeah.



STEVE:  ...who's my good friend and fellow massive miner, because as we know there's been a huge growth in the value, for example, of bitcoin recently.  And last time we talked, as I have said on this podcast, you could not mine in California because our power costs more than the electricity required to earn/mine bitcoins.  But in Phoenix, for whatever reason, power is cheap.  And so it made sense for him to be mining in Arizona, but you can't do it in California.  But you can do it under Niagara Falls where, again, power is very inexpensive.  And so I don't know how that has changed as bitcoin, for example, has gone crazy in value to the point now where those 50 bitcoins, I really do have to find them because they're now approaching a quarter million dollars' worth of value.  Of course, I've...



LEO:  What?



STEVE:  Yeah, it is, believe it or not.  It's like $4,300 now for a bitcoin.



LEO:  But you don't want to be the guy who had 50 bitcoins and traded in for a quarter of a million when in five years it's worth 100 million.



STEVE:  That's the problem, too.



LEO:  It's like any stock.  You've got to know when to sell it.



STEVE:  But I'm also a Boy Scout.  How does that go?  Is that the Boy Scout salute?  So I will be reporting my bitcoins to the IRS because I'm not going to screw around with that.  They're now getting very annoyed with people who are cashing in their bitcoins and not reporting them as income.



LEO:  Is it regular income?  I guess it is.



STEVE:  Yeah.



LEO:  It's like winning the lottery.



STEVE:  Exactly.  So, yeah.  Very much so.  So, yes.  I'm not in any hurry to do anything with them.  But it would be nice to know where they are.



LEO:  You should find them.  I know I have my wallet backed up somewhere, and I can't remember the password.  I only have five, though.  If I had 50, I might make more of a concerted effort.



STEVE:  Yeah, it's a little motivation.



LEO:  Yeah.



STEVE:  So anyway, what I wanted to say to people is that - so what's annoying is that somebody is using your CPU.  On the other hand, okay, so are you.



LEO:  So is every ad.  So is every - yeah.



STEVE:  Exact - yes.  And the autoplay videos, those are pegging people's CPUs just as much as cryptocurrency mining is.  So our browsers...



LEO:  How much time do you spend on the Showtime site, anyway?



STEVE:  Precisely.



LEO:  Is it the watching a movie Showtime site, or just the logging into Showtime site?



STEVE:  It was several different domains, so probably going...



LEO:  Showtime Anytime or...



STEVE:  Yeah.



LEO:  See, if it was...



STEVE:  Well, it's probably when you're looking at something in your browser, and they're having a chance to run JavaScript.  So anyway, so it's an annoyance.  I guess it's not surprising.  So I guess the point is that our browsers are hardened against JavaScript being able to hurt us, like as in malware.  But they're not hardened against JavaScript being able to do work because, I mean, that's what it's for is doing work.  And so they're like, well, okay.  Maybe this user is at a website that's going to pay them if their browser scores a coin.  No, but it could be.  So anyway, that's what's going on with that.  It's like, yeah, okay.  So, yeah, it's using some of your power and some of your processor.  But so are you when you're doing anything.  So no biggie, really.



We covered extensively over the years the San Bernardino terrorist attack where Syed Farook had his two phones, and of course Apple famously refused to help the FBI to unlock this phone, which apparently...



LEO:  I wonder if they would still have that attitude today.  Seriously.



STEVE:  Really.



LEO:  What if the Vegas shooter had a phone?



STEVE:  Oh.



LEO:  Seriously.



STEVE:  Good point, yeah.



LEO:  It would be politically very difficult for them to say no now.



STEVE:  Really, yes.  Yeah, that's a good point.  What happened in the wake of this is that the AP (the Associated Press), USA Today, and Vice Media all sued under the Freedom of Information Act (FOIA) in the U.S. for disclosure by the FBI of who it was they worked with and how much they paid because those are taxpayer dollars.  And the argument was, hey, this is a taxpayer-funded operation.  We want to know.



Now, James Comey, as we'll remember, the former FBI director did indirectly disclose that they paid something around $1.3 million for this hacking tool from an undisclosed company.  So we really don't know who it was.  We never got an exact amount.  But these press companies, these press agencies wanted to know.



On Saturday of last weekend, a U.S. district judge in D.C., the District of Columbia in the U.S., Tanya Chutkan, disagreed with the suit that was brought by these companies and said, no, the FBI has no obligation, even under FOIA, to make this disclosure.  She said in her decision that, "It is logical and plausible that the vendor may be less" - the vendor meaning the one that the FBI paid - "that the vendor, undisclosed vendor may be less capable than the FBI of protecting its proprietary information in the face of a cyberattack."  Therefore, an argument for allowing this vendor, who received taxpayer money, to remain anonymous.



She also wrote:  "The FBI's conclusion that releasing the name of the vendor to the general public could put the vendor's systems and therefore crucial information about the technology at risk of incursion is reasonable, so they might be subjected to an attack."  And then regarding the cost of the hacking tool, Tanya agreed with the U.S. government that revealing the price the government paid for unlocking the iPhone could harm national security, saying:  "Releasing the purchase price would designate a finite value for the technology and help adversaries determine whether the FBI can broadly utilize the technology to access their encrypted devices."



Okay.  So translation is the FBI asked to keep this information private, and it got its way.  So we're not going to find out who they got it from and what they paid, and that's sort of closed.  And remember also that it's not at all clear that even then, if it weren't an iPhone 5c, that it would have been possible to make this happen.  What I remember from the time was that it was the fact that it was a 5c that fit together with maybe some known exploits against that particular make and model that allowed a third party to get in and to have a big payday for themselves because this was, as you said, it was at the time very politically sensitive.  And you raised a good question, I think, Leo, whether in the wake of the Las Vegas shooting, if a similar thing happened, whether Apple would be able to say no.  Or would.  



LEO:  Be hard for them; right?  Be hard.



STEVE:  Yeah.



LEO:  Be difficult.



STEVE:  Yeah.



LEO:  By the way, this just in, and you'll love this, the IRS has just awarded a $7.25 million fraud prevention contract to Equifax so that Equifax will let them know if any refund requests come from compromised Equifax information.



STEVE:  Boy, talk about making money coming and going.



LEO:  But we knew they'd make money on this breach because they were pushing people towards a for-one-year-free identity theft monitoring service, when of course in another year you'd be paying for it.  It's just, god, this is [crosstalk].



STEVE:  I know.  So I know that everyone is, like, fed up with hearing about Equifax.



LEO:  Oy.



STEVE:  But while you were on vacation, Leo, I attended a private security conference.



LEO:  I heard you say that, yeah.



STEVE:  Where there was some inside scuttlebutt suggesting that this was looking more and more like a state-sponsored attack.



LEO:  You thought it was China.



STEVE:  Yes, and that seems to be the consensus.  That's beginning to surface.  And the good news - and there's sort of a mixed blessing that it suggests that, if it's a major actor, they really don't care.



LEO:  It's not about credit card for them, credit card fraud, yeah.



STEVE:  Yes.  They don't care about the 143 million of us.  What they do care about are specific people who are high-profile targets.  And Bloomberg had some fabulous coverage.  I'm not going to go through the whole thing because it's long, but the link is in the show notes for anyone who wants more.  But I want to give our listeners a sense for just four paragraphs from their coverage.



They write:  "Nike Zheng, a Chinese cybersecurity researcher from" - and this establishes sort of the history and the background, which wasn't clear before - "from a bustling industrial center near Shanghai, probably knew little about Equifax," writes Bloomberg, "or the value of the data pulsing through its servers when he exposed a flaw in popular backend software for web applications called Apache Struts."  Which of course we talked about weeks ago.  That's the Java-based server-side platform for developing web applications that Equifax, among many other people, are using.



They write:  "Information he provided to Apache, which published it along with a fix on March 6 [2017] showed how the flaw could be used to steal data from any company using the software.  The average American," Bloomberg writes, "had no reason to notice Apache's post, but it caught the attention of the global hacking community.  Within 24 hours, the information was posted to FreeBuf.com, a Chinese security website, and showed up the same day in Metasploit," which as we know is a popular free hacking tool.  "On March 10th" - so four days after the disclosure - "hackers scanned the Internet for computer systems vulnerable to the attack and got a hit on an Equifax server in Atlanta, according to people familiar with the investigation."  So four days after the disclosure, Apache did everything they could responsibly.  They said, "Whoops, we have a problem.  Here's the fix."  And four days later Internet was scanned; Equifax was identified as a target.



"Before long," Bloomberg writes, "hackers had penetrated Equifax.  They may not have immediately grasped the value of their discovery; but, as the attack escalated over the following months, that first group" - get this - "known as an 'entry crew' handed off to a more sophisticated team of hackers.  They homed in on a bounty of staggering scale:  the financial data - Social Security numbers, birth dates, addresses and more - of at least 143 million Americans.  By the time they were done" - and here's news - "the attackers had accessed dozens of sensitive databases and created more than 30 separate entry points into Equifax's computer systems."  Meaning that this initial Apache Struts was just a means of gaining a foothold.  And once they did, they established 30 other presences that would protect them in case of the discovery.



So Bloomberg writes:  "The hackers were finally discovered on July 29th, but were so deeply embedded that the company was forced to take a consumer complaint portal offline for 11 days" - because it was so massively compromised - "while the security team found and closed the backdoors the intruders had set up."  And then, finally:  "The handoff to more sophisticated hackers is among the evidence that led some investigators inside Equifax to suspect a nation-state was behind the hack."  That is, the people who discovered it weren't the people who continued exploiting.  Rather, they found it, and then they basically passed it upstairs to a more sophisticated next-level group, who then crawled inside and began to exploit.



They write:  "Many of the tools used were Chinese, and these people say the Equifax breach has the hallmarks of similar intrusions in recent years at the giant health insurer Anthem" - which of course is, remember, 18 months ago is the reason we first told everybody to lock down their credit reports at the time - "and the U.S. Office of Personnel Management.  Both were ultimately attributed to hackers working for Chinese intelligence."



So this is the way these things are now being done.  And, I mean, it's not good that 143 million Americans and some Canadian and U.K. citizens had this detailed personal data exfiltrated from Equifax.  But as you said, Leo, it suggests that we're not all in immediate threat.  It's not as if some hacker was looking, some money-oriented, financially oriented hacker would be selling this in bulk in order to immediately turn it into profit.  It seems much more likely that, if this is in fact China, among this 143 million are a lot of, to them, very valuable information, which they could then use for targeting specific individuals.  So not good for those targeted people, but somewhat better for the rest of us, we can hope.



LEO:  Geez.



STEVE:  Yeah.  We've talked over the course of the last month or two about the Broadcom firmware problem that essentially beset all of our mobile devices.  Both Android and iOS devices were vulnerable.  Google and their Pixel devices were first to patch, and patched very quickly.  I'm not sure, I don't remember now where Samsung was on this.  But we covered several, a series of patches that iOS released and finished with 10.3.3 was the most recent before jumping to 11.0.1.  This comes back to our attention because a Google researcher has published a proof-of-concept exploit that is functional against Apple iPhone WiFi Broadcom chips prior to iOS v10.3.3.  So the good news is you don't have to update to 11 to be protected against this.



LEO:  Is that because Apple discovered it and fixed it?  Or just in the process of updating it fixed some issues?



STEVE:  No.  They did discover it.



LEO:  They knew about it, okay.



STEVE:  Or it was reported and fixed.



LEO:  Got it.



STEVE:  So they did it in 10, in 10.3.3.  Remember we had 10.3.2, and we thought - and there was some Broadcom fix there.  And we presumed that that was this big one.  But it turns out, no, it wasn't until 10.3.3, which was the final v10 iOS before jumping to 11.  So for people who, for whatever reason, are holding off on moving to 11 - and despite all my complaints, like you, Leo, I really like 11.  I'm liking the things that they did, although as I said it comes at a cost of performance on older hardware.  But somebody who wants to stay at 10, as long as you're current, which puts you at 10.3.3, then this is fixed.



But the significant thing is that now with a public proof of concept, that means anybody who wants to attack a pre-10.3.3 iOS device has a template for doing so, and it is a potent attack.  Remember that you don't need to be associated with the WiFi device.  Your phone just has to be within range, that is, radio range.  Your phone and the attacker's radio need to be able to reach each other, and so you don't have to log in or use a malicious access point.  You just have to be able to be enumerated, essentially, by the Broadcom chip.  And that's enough for your phone to be taken over.



So I imagine, I mean, the good news is Apple pushes these updates out, and so they're really good about keeping their phones' OS current and patching the firmware in the related devices, like this Broadcom chip.  But this is something everybody wants to do because it does make the attacks much more likely now.



And I heard you talking on MacBreak Weekly, Leo, about just in general about this evolving awareness of how much Russia had their tentacles into the most recent U.S. presidential election.  We've seen news in the press about Facebook taking real steps to understand how much Russia was purchasing ads on Facebook.  Twitter has done the same thing, recognizing the extent to which they were unwitting accomplices in Russian involvement.



And now Google has said, although it's not official yet, and the Google personnel who have been talking to the press have requested anonymity because the plans are not yet officially public.  But Google will be offering what they call the APP for Gmail, the Advanced Protection Program.  And the idea is that it is going to be a super secure service that will be an optional feature for Gmail, targeted at corporate executives, politicians, and others with heightened security concerns.



And this is probably to keep people from leaving Gmail.  Google would like to not have people think, oh, well, it's free, and so it's not very secure.  If we really want secure email, we've got to go somewhere else.  Instead, Google is saying no, we're going to require two separate physical security keys of some sort.  Right now, as we know, they do offer a second-factor technology, for example using Yubico's technology and one of the variants of FIDO as an option.  They're going to, as part of this Advanced Protection Program, require two different physical security keys; and no third-party apps will be able to gain access to your Gmail service.



So your freedom and your flexibility of what you can do will be dramatically curtailed.  But for people who really want security, that's a requirement.  You have to silo mail and not allow apps to be able to get access to your account, and then they're going to really crank up the authentication level to create this level of security.  Again, no idea when.  It's not official yet.  And if it happens, apparently it's going to be called Advanced Protection Program.  So again, in the wake of what we learned, Google is saying, okay, we're going to provide as much, arguably more security than anybody else, and a lot for a cloud-based service.  And I don't know whether it'll be free, or whether it'll be...



LEO:  I think it's for the Google Apps owners.  It's for G Suite.  I don't think it's really for us.



STEVE:  Ah, okay.  That does make sense, yes.



LEO:  I think it's for business folk, yeah.



STEVE:  It does make sense.  There was a bunch of mistaken news, but clickbait headlines, talking about how DKIM had been broken.  That's the domain key technology that we've been talking about recently, the idea being that a mail domain - I'll use mine, GRC.com - can publish through DNS its public key.  And as it's sending email from its SMTP server, it signs the outgoing mail.  That is, it adds a signature using its secret private key to every outbound mail.  And then the receiving SMTP server is able to similarly use DNS to get the matching public key and then verify the signature on email claiming to be from GRC.com in order to verify it.  This was designed as an antispoofing measure, that is, the idea being that no one could say that email was from Apple.com or Microsoft.com or any major presumably trusted sender.  The recipient would be able to verify.



And as we said when we were talking about this a few weeks ago, there are some email clients or browser add-ons or email client add-ons that will independently check the DKIM signature.  Normally that's done at the SMTP server level, where incoming mail will be rejected if the signature doesn't match.  So what has been claimed in the news recently is that it's been broken.  Well, it has not been broken.  The crypto is strong and was done right.



What was broken is that email itself has always had a problem sort of with its own security.  We have this notion of multipart extensions and the concept of a mail envelope, that being the thing, sort of an abstraction of the email content and the headers forms an envelope.  Well, that's what the sender's private key is signing.  Well, it turns out that it is possible to spoof the signature, that is, to spoof the contents of the envelope so that it's possible to add your own different message, keep the real message from being seen, yet still have the DKIM signature shown as valid despite the fact that what the user is seeing is not the contents of the envelope that was securely signed, but something that a spoofer attached to the email.



So the upshot is that the assurance that you believe you're getting from DKIM is bypassed so that, if your email reader is telling you that it's checking the email signature, it could show it being valid, but the contents of what it shows you is not what was signed in the envelope, so it's a spoof attack.  And this is one of the things we're seeing is sort of like, as we keep getting better and better with security, we keep closing loopholes and closing backdoors and getting better, the whole problem of spoofing remains elusive because our clients are still, in this case, still not strong enough to be hardened against that because they're trying to be flexible enough to give us the features that we want in email which were extended over time without thinking about security as closely as they should have been.



So anyway, I got a bunch of tweets from people saying, oh, my goodness, DKIM is broken.  Well, no, but it is spoofable, unfortunately.  And over time I think we'll see that get hardened up.  But for the moment, it is the case that someone could send you something where what you see is not what was signed, and therefore it could say anything and cause you to be misled.  For example, it could contain a malicious link where you trust the link because you're looking up at your DKIM monitor, and it's saying green.  It's saying, yes, this is trusted email.  And so you go, oh, fine, and then you click the link, and you could get compromised as a consequence.  So, I mean, it's not good, but it is not the case that the DKIM system itself was broken.  It was basically just bypassed.  Someone ran a bypass.



And, finally, there was another Google researcher sort of got around to taking a look at a very popular small suite of services.  It's known as DNSMASQ, D-N-S-M-A-S-Q, DNSMASQ.  It's very popular in small networks, in small routers, anything where you have a small footprint server where you want to provide local DNS and DHCP services.  So, for example, I mean, it's what we probably have in many of our home WiFi routers or standalone routers.  There are lightweight services to provide local DNS caching and lookup and dynamic host configuration protocol, DHCP, which is the obtain IP address automatically service.  It's widely used for tethering smartphones and portable hotspots.  Many virtual networking virtualization frameworks use DNSMASQ as their virtual machine DNS and DHCP systems.  It's supported under Linux, Android, the various BSDs, and Mac OS X.  It's included in most Linux distributions, and there are ports for it for FreeBSD, OpenBSD, and NetBSD.  So it is widespread and popular.  Ubuntu uses it, for example, by default for their services.



So what was found was, I think it was seven or eight remote code execution vulnerabilities, but remote in the sense of remote from the device, not in the sense of Internet facing.  These are all local LAN-side attacks so less to be concerned about.  It doesn't mean that suddenly there's a new vulnerability for all of our routers that we need to worry about.  These are, because these services are LAN-facing for clients on a LAN, clients of a router on a LAN typically, it means that, if you had a malicious device, maybe a malicious IoT device that had been updated with some malicious firmware, it could arrange to execute code on your router.  Well, that's not good.  That's certainly not good.  But it's less bad than if anybody anywhere in the world were able to scan for this vulnerability, which is probably very widespread.



So our takeaway - oh, and this is just yesterday that this came to light.  Our takeaway is that, to be looking for firmware updates for our various devices, hopefully people will start pointing fingers at specific vendors.  This is just - because this is less than 24 hours old, we don't yet know model numbers and makes and firmware versions and so forth that are vulnerable.  I think probably we'll be talking about this next week as more information comes to light.



It'd be fun also to find something that can identify whether the routers that we're using are vulnerable.  Much as malicious clients could exploit them, we may be able to do that.  But they are remote code execution, so what I think we'll need to find is be looking for updates to firmware for the devices we use.  And in this case it would be a good thing to apply them because you don't want a single malicious device to be able to take over your router and then potentially have access to both your internal network and open it to the outside world.



And Leo, I heard you talking about the EFI firmware vulnerabilities.



LEO:  Yeah.  I want to get some clarification from you on that one.



STEVE:  Yes.  So a tech firm has taken a look at the pervasive problem throughout the entire industry.  The problem is not just Mac-specific.  It's Duo Labs.  And these guys took a look at a whole bunch of Mac machines.  It was, like, more than - it was tens of thousands of Mac machines.  I want to say 43,000.



LEO:  Yeah, I think it was something like that.



STEVE:  Something like that, yeah.  And they found, like, an overwhelming number of specific Mac models were vulnerable, meaning that they were running EFI code today with known vulnerabilities.  Now, okay.  That's not immediately a problem because an EFI vulnerability, first of all, is very difficult to exploit.  It's more someone gets physical access to your machine and tinkers with your system in order to get something installed in EFI.  So it's difficult to exploit, though very powerful if exploited, but difficult for the average user to deal with because it's sort of - it's preboot.



But what it means is that AV software can't really see malware there because it's underneath the OS.  And what it enables, what EFI exploits, if they're exploited, enable are virtually undetectable super rootkit-style preboot attacks where power being turned on, before the system boots, EFI is running around setting things up, getting the hardware going, and has full access to the system.  It's what then chooses which drive to boot, finds the bootable partition, and loads it into memory and gives it control.  So if that's been made evil, that's really bad.



So the problem is that, first of all, there's maybe a lack of focus and attention on this.  It's also the case, as you mentioned during MacBreak Weekly, Leo, that attempts to upgrade firmware sometimes fail silently.  And once it fails, like it doesn't pass the required checks, there may not be room in available flash memory to install the firmware because a lot of other drivers have been added over time, I mean, there are just many different failure modes.



And so it's sort of like, I mean, I have the experience because I have a small iPad where it complains, it was complaining for a while that I couldn't update iOS because there wasn't enough room.  Now Apple has solved that by allowing some apps to be removed temporarily to make space for an update, and then they're brought back in, which is kind of a clever workaround.  We don't have that capability for EFI.  So as a consequence, what these guys found, what Duo Labs found was that there is a widespread lack of current firmware in Macs, but not only Macs.  All they looked at was Macs.



But I would argue that, because the general PC market, whether it's running Windows or Linux, has a greater disconnection between the OS and the motherboard.  For example, Microsoft doesn't care about EFI because they're providing Windows.  Apple cares about EFI because they're selling the hardware and all the software that goes on top.  So it's really up to your motherboard manufacturer to deal with EFI, and motherboard manufacturers don't really care that much.  I mean, they may have updates, but then it's up to the user to go get them.



So we're sort of in a mess with this, which is why it's good that the vulnerabilities, if they exist, are difficult to exploit, and unfortunate that it's difficult for average users to deal with.  The best takeaway I have is that, for the Mac's EFI firmware, there is a free tool that you can use to check what's known about your EFI underneath your Mac.  It's called EFIgy as a cute little play on EFI.  It's E-F-I-G-Y dot I-O.  EFIgy, E-F-I-G-Y dot I-O.  That's a shortcut that takes you to a GitHub page where these guys are using an EFIgy API in order to tell users about the state of their firmware.  This is still under development, and what we need now is a mature database to connect Mac makes and models to this firmware they should have in order to compare to the firmware they do have.  This thing will show you what you do have.  It provides a means for gaining some insight into that.  And I think over time we'll be seeing this mature.



Unfortunately, this is Mac-specific - well, unfortunate for PC users, non-Mac users.  It's Mac-specific.  All you can really do is take the effort to check in with your motherboard manufacturer and almost, I mean, if you haven't checked for a couple years, and really who does because it's not automatic, check to see.  Normally your boot screen will show you what version of EFI you've got while your system is booting.  Go to your motherboard manufacturer, see if there's new firmware.  The old wisdom I would argue, and this was the wisdom for motherboard firmware, if it's not broken, leave it alone.



Well, unfortunately, all EFI firmware is probably broken.  It's working, but it's vulnerable now.  So we've moved from a world of, if it's not broke, don't fix it, to a, if it's old, it's probably broke, and you don't know it, so it's worth patching it with the latest version.  So I think the advice now needs to be specifically - because, as we know, attacks never get weaker, they only get stronger.  So it's worth knowing that your motherboard is running the most recent EFI firmware that your motherboard's manufacturer makes.  And the good news is...



LEO:  You know, the interesting thing on the Mac, I don't think there's any way to manually force an EFI update.



STEVE:  No.



LEO:  It's not like a PC motherboard where you can go to the manufacturer and download the firmware.



STEVE:  Yes.  And that's the perfect segue because I was just going to say that the good news is Apple has responded to the press coverage, and they've said - they're kind of dodging it at the moment because they don't have an answer immediately.  But this has shined a bright light on this, and Apple can and probably will take responsibility.  So arguably Mac users have a little bit of a spotlight on them at the moment, but Apple can step up and give this better attention.  And then you're glad you're a Mac user because Apple can take care of it for you.  You'll just get an update, and you won't know why, but things will spin around on the screen for a while and then settle down, and you'll be safe again.  So that's good.



LEO:  Yeah.  They've put that in High Sierra, too.  So if you have High Sierra, or you don't have High Sierra, might be another reason to upgrade to the latest version of macOS because there's an EFI checker.  They check weekly now.



STEVE:  Just a real quick little mention of an IE bug.  I'm not, you know, IE is sort of, well, I guess those of us who are not yet on 10 are still using IE on everything before - unless we're using Chrome or Firefox or Opera or anything else.  It came to light that there is a bug in Internet Explorer's handling of the URL bar.  And it's kind of cool and subtle.  And again, it's one of those things that everyone is going to jump on until it gets fixed.  JavaScript running on any ad or page you're visiting can intercept when you hit the Enter key after entering a new search term or URL as you're leaving that page.  Whoops.



So it means you're somewhere, and you go, okay, you're done being there.  So you type something new into the URL field, either as a URL to go to a website or as a search term.  When you hit Enter, before leaving where you are, script on that page can capture what you typed and send it back to the site you were visiting or any of the ad suppliers, which is not behavior that you want.



Only IE does this.  It's a bug.  It'll get fixed.  I don't know how soon, probably not by next Tuesday.  That's going to be the second Tuesday of the month since today is the first Tuesday of the month.  So it seems like that doesn't give Microsoft much time.  Maybe they can do it in time.  It would be nice.  But again, it only affects IE users.  But it's the kind of thing that might provide some information the bad guys would want.  And if you're using IE, they can get it until Microsoft gets that patched.



I had one little bit of miscellany.  Leo, while you were away, two sci-fi shows premiered.



LEO:  And I haven't seen either.



STEVE:  "The Orville" and...



LEO:  And "Discovery," yeah.



STEVE:  ..."Star Trek:  Discovery."  I was negative about "Discovery" with Father Robert because I'd only seen the first five minutes of it, and it was so painful that I just said, okay, let's switch back to whatever...



LEO:  It got a lot of bad reviews, though.  It wasn't just you.



STEVE:  Yeah, I know.  And I'm not surprised.  I did sit down and watch the whole thing again.  I gave it a try.  And then I'm remembering how bad the first few episodes of "The Next Generation" were.



LEO:  Oh, okay.



STEVE:  Even with Patrick Stewart and Frakes and the whole, you know, the core crew, it was bad for the first, oh, maybe half a season.  They were, like, shouting at each other across the bridge, and Picard was being really annoying.  They found their groove, and it was arguably one of the better franchises that we had.  So I want to give them the benefit of the doubt.  And I also - my plan is to wait for the first season to be finished and then do my CBS all-access and watch, not only all of "Discovery," if I end up hearing that it's worthwhile, but also the "The Good Fight," which was the continuation of "The Good Wife" that was on CBS for so many seasons and so worthwhile.  And Leo, I didn't make it past 10 minutes of "The Orville."



LEO:  Oh, really?  Oh, that's the only one I really want to see.  But you have to be a fan.  If you don't like his humor, Seth McFarland's humor...



STEVE:  Yes.  And I recognize I'm very finicky about humor.  I like "Seinfeld" and...



LEO:  I love "Family Guy," so I think I would like "Orville."



STEVE:  Give it a try because it certainly is sci-fi setting for that.  But it wasn't my kind of humor.



LEO:  He's an acquired taste, to say the least.



STEVE:  Yeah.  I had a really fun story about SpinRite and Drobo from Steven Perry, who's a CISSP, or has his CISSP.  He's in Charlotte, North Carolina.  And the subject, it was dated email incoming September 28, so just last week, said:  "SpinRite Saves a Drobo."  And there's an interesting lesson here.



He said:  "Hi, Steve.  Have a SpinRite testimonial for you.  You could title this one 'SpinRite Saves a Drobo.'  I have a Drobo FS NAS device that is about five years old now.  When I set up the Drobo, I made sure that each of the five 2TB WD [Western Digital] Black drives passed SpinRite before they were installed in the Drobo.  This past Sunday the first drive bit the dust."  Okay, now, remember, the Drobo is a RAID, so that's okay.  You can lose a drive, and it'll say, whoops, we got a problem here, but we can still read all your data.



So he says:  "I swapped out my spare 2TB drive" - so he had another one standing by, as he should - "that had also passed SpinRite, and during the rebuild process one of the other drives..."



LEO:  Oh.



STEVE:  Yup, "went offline, and Drobo was setting off alerts that too many drives had failed or been removed."  That is, the drive just completely died.  He says:  "I was able to safely shut down the Drobo, remove the drive it thought was missing, run it through a Level 2 pass from SpinRite," he says, "which took 11 hours to complete.  The drive was then returned to the Drobo, powered on to find the Drobo was now seeing the drive and was able to successfully rebuild the new drive, and all is good again."



So essentially, as we know, a RAID 5 allows you to lose one drive.  Or another way to look at it is any given sector on one drive can be sort of recreated by using the other drives to calculate the lost data in a single sector of one drive.  Or you could lose all the sectors of one drive, and then the other drives can be used to calculate the lost drive's contents.  That gives you the redundancy of RAID 5.  But you cannot lose two.  And that's why now we're seeing increased use of RAID 6 because drives have gotten so huge that exactly what happened to Steven Perry is beginning to be seen, and that is the first drive dies, you swap in a good one, and during the rebuild process, where you temporarily have zero tolerance of any failure, another drive dies.



And that's why, for example, at GRC I'm running RAID 6.  So I've got - it's expensive in terms of you don't get the data now of two drives.  On the other hand, you get double redundancy.  With RAID 6, I have a four-drive RAID 6, and any two can fail, and I'm still okay.  And remember, because we've talked about the Drobo before, although again it's expensive, the Drobo does allow you to use extra redundancy.  It'll take away a chunk of your storage; but you can say, no, I need extra, extra safety.  Most people don't do that.  They figure, as Steve did, hey, I've got RAID 5, and I've got SpinRite.  As it happens, thanks to having SpinRite, he was able to recover from that window where there was zero tolerance.  He could not afford to lose another drive, and he did.  But SpinRite got it back.  The Drobo was able to rebuild.  Now he's back up to full redundancy.



LEO:  And he should turn on the two-drive backup.



STEVE:  And maybe that would be good.



LEO:  I do that on all my Drobos and my Synology, as well.  If you have that as [crosstalk]...



STEVE:  Belt and suspenders.  Belt and suspenders and SpinRite.



LEO:  Although I suppose three drives could fail.  But it gets diminishingly small.  We had a funny - go ahead.



STEVE:  Yeah, go ahead.



LEO:  Well, we had a funny thing happen because we had a drive that was a little flaky.  We used a Western Digital Gold for rerun playback.  And it was flaky.  And they pulled it.  They were going to get a new one.  And then somebody, I think Russell, very sharp-eyed, noticed that the label on the Gold spelled "thailand" with a lowercase "t."  It's a made in [lowercase] thailand.  And he thought, hmm.  We got them on Amazon, but from a reseller on Amazon.  And he thought, hmm.  So they open it up, and it is a counterfeit Western Digital drive.  We bought it on Amazon, but from a third party, not from Amazon directly.  And you'd have to look really closely with a magnifying glass to see "Product of thailand."  But, wow.  I mean, really.  So somebody's taking crap drives and just slapping a sticker on them and selling them as enterprise-grade 2TB Western Digital drives.



STEVE:  Wow.



LEO:  So be careful.  You know, I've said this - by the way, this is the sticker they put on it so we wouldn't use this, says "Fake News."  I mean, you would just assume, well, I got it on Amazon.  But really you've got to be careful on Amazon because a lot of times you're not getting it from Amazon, you're getting it from a third party.  And it means sometimes getting it on Amazon doesn't mean anything more than getting it on eBay.



STEVE:  Right.



LEO:  And you wouldn't really buy hard drives on eBay, would you.



STEVE:  No.



LEO:  No.  It's not that it was made in Thailand CR1, it's that it's a lowercase "t" in "thailand."  I had a watch for a long time, a Rolex that said "Made in Jeneva" with a "J."  That was how I knew.  But it was only $10 in New York.  All right, Steve.  I think it's time to get to the topic at hand.



STEVE:  Well, we've got a couple of closing-the-loop bits from our listeners.



LEO:  Yes, sir.



STEVE:  So Mark Havas sent:  "Regarding your printing 2FA codes to have them on paper, would you keep the images in a password manager like 1Password?  Thanks in advance."  And, okay.  So it's interesting.  I don't understand, and I'm not picking on Mark at all because many people, I mean, he's just asking, but everyone seems so resistant to the idea of printing their QR code for their authenticator.  And I don't understand it.  It may be that we're all hooked on electronics or automation or cloud or something.  But so we just sort of keep seeming to circle around this.  I get it that people want a shortcut.  They want more ease of use.  But offline means offline, not online.  If it's offline, it cannot be hacked.  If it's online, it can be.  So I'm not saying it will be, but it can be.



So if you've got all your two-factor authentication QR codes in anywhere online, then Russia has a connection.  They may not be able to use it, they may not be able to decrypt it, but online means online.  And so I have no problem if people say, oh, well, I want to store them in my password manager.  Okay.  I'm not storing them in my password manager.  While you were talking, Leo, I looked at this question.  And I'm not facing them toward the camera, but here's my set of QR codes.



LEO:  Where do you stick those, Steve?  It worries me that they were that available.  Are they just in a pile on your desk?



STEVE:  Yes, and Russia cannot get them.



LEO:  Yeah, but maybe you can't either if you put some more stuff on your desk.



STEVE:  No, I know exactly where they are.  They are offline.



LEO:  Yeah.  No, that's an excellent point.



STEVE:  And that's my point.  And I'm a little worried about this because SQRL has some of the same imperatives.  And it's like, no, write this down once, print this once, because then it's offline.  It's like people don't get that anymore.  It's like, oh, everything's supposed to be online.  It's like, yes, Russia wants that.  So okay.



LEO:  And I use Authy, which means it's online.  But you're absolutely right.  I mean, I had a little scare this morning.  I got a call from Hover.  James, very nice guy from Hover said, "Your account's been compromised, and somebody has changed your email forwarding."  I use Hover to forward my email address because they own, they register the domain name.  And they added, somebody added another email address.  They didn't take the old one out, thank goodness.  But I guess the idea was to be surreptitious; right?



STEVE:  So they were cc'ing themselves on everything.



LEO:  They were cc'ing all my email to them.  And the idea was that they would then ask for password resets, and they'd get them.  And I might notice, gee, there's a lot of password resets, but there always have been.  I mean, people are always attacking me.  But I have my email address, and they can't get them.  So they found it.  I guess they had monitoring.  I know you were talking last week about monitoring, how important monitoring is.  They must have been monitoring, and they discovered the hack.  They reversed it and notified me immediately.  Hover, God bless you.



STEVE:  Yup.



LEO:  I went and looked, and there was one Twitter reset email an hour before he called.  But I have two-factor turned on, so it didn't do them any good.  They got my password, but they couldn't get my account because I had two-factor turned on.  And it just really underscored for me, not only how important two-factor is but, to your point, how important it is that two-factor be really separate.  So if you're compromised in some way - and I'll give you an example.  They could have tried to get my Authy login; right?



STEVE:  Right.



LEO:  And then, now, in this case it's fine because Authy uses Trust No One encryption.  Only I know the passphrase that unencrypts my Authy database.  So even had they, I'd be safe.  But if they had - but that's an example.  These things cascade.  They get your email, and then they can use your email to get something else.  And if they were able to figure out what your passwords and your two-factor would be, then there would be no point in having two-factor.



STEVE:  Well, yes.  And as we've been saying, one of the most troublesome things is that email is the typical password recovery system.  That is, again, this is one of the tradeoffs that SQRL uses is there's no one to complain to.  There's no one to say, oh, I forgot my password.  I've lost my identity.  I mean, we make it really hard to get yourself in trouble, but not impossible.  And there's just - if you want security, then you cannot trust a third party, or they could be subject to a subpoena.  So it'll be interesting to see how this plays out.  But I've not cut any corners.  I've said, no, I'm going to offer the world a truly secure system.  We'll see if that's too much for everyone to handle.  And it's not like everyone has to.  People who are security conscious can choose to do that, and everybody else can continue using their email.  But the problem is that, how many times, "I forgot my password."  What do they do?  They email you a link.  Well, if somebody had your email account...



LEO:  That's exactly right.  That's right.



STEVE:  ...and they then said they've lost their - they forgot your password, because they're bad people, then you would get a password recovery link which they would grab.  They would click on it before you knew any of this was going on and take over your account.



LEO:  Yeah.  I just - I'm so glad I listen to this show and I have two-factor turned on everywhere.



STEVE:  Yeah.



LEO:  I feel much more secure.



STEVE:  This is really a quickie.  I just got a kick out of this.  Jamie, whose handle on Twitter is @LinkLayer, he said:  "My CS prof anticipated fuzzing in 1976."  We talked last week extensively about fuzzing.  And he said:  "He said to test input handling with punch cards from the trash and swept up from the floor."



LEO:  That's a good idea.  Random data.



STEVE:  It may not be true, or maybe it was.  Yes, exactly.  Just grab punch cards from wherever you can and feed them into your program and make sure they don't crash it.



Michael Johnson asked, he said:  "How about a segment on taking a laptop on a flight out of the USA, working, and returning?  What's the best way to do this with no border hassles?"  Well, okay.  So there are two problems with taking your laptop somewhere.  One is the privacy of having its contents somehow exposed.  The second is possibly getting it infected and bringing an infection home.  So privacy could be obtained using an add-on that we've talked about often, like VeraCrypt, which encrypts the whole hard drive.



But the other thing you could do would be to make a copy of the hard drive, if the drive is removable from your laptop.  Clone it.  And then leave the original drive home, which is the entire state of your laptop before your trip.  Then encrypt the one you're traveling with for its safety, then stick it in the laptop and use it.  Now you have the benefit of encryption, so only under your control can anyone see into it.  And if, while it's decrypted, it were to get malware infected, then the idea is, when you bring it home, you just take it out of the laptop and put it aside.



And then, oh, you also want to, depending upon what work you do, put the work up in the cloud before you take the drive out so that you've moved your transient work somewhere safe.  Yank out the drive, which is encrypted, which may not be convenient for you normally and might have unknown contents on it.  Maybe some foreign government briefly got it at the border and installed some malware on it or something.  So the point is you can no longer trust it.



Well, you don't have to.  So you yank it out of your laptop.  You put the drive that stayed home in safety into your laptop.  So now you've restored your laptop to exactly the way it was before you ever flew anywhere, or before you traveled.  Now, grab the work you did back from the cloud, and you're safe, having essentially removed any possibility, removing the inconvenience of having the drive encrypted if you don't normally want it to be, and any possibility that you brought more back with you than you intended.  And, finally...



LEO:  Of course, if they put an EFI virus on, you're screwed no matter what.



STEVE:  Yes.



LEO:  In other words, don't let anybody have your laptop.



STEVE:  Really, if you can avoid it, you don't want to lose control, physical control of your laptop.  And I did want to just add, if removing your drive is not possible, then you could use a tool like Image for Windows to take a full drive snapshot to an external drive before your trip.  When you get back, and before you hook it to your network - do not hook it to your network - restore the full drive snapshot back to your laptop in order to wipe out anything that may have happened while you were traveling.  So that's really the best, I think, you can do.  You get encryption to keep anyone who might be curious from poking into the drive; and you're also protected if, while it's unlocked and decrypted, as it has to be in order to use it, from picking up anything malicious.



Many people, naturally, because of the all-time favorite podcast, "The Portable Dog Killer" episode, have been tweeting me and writing to me about the U.S. Embassy in Cuba, I guess it was in Havana, where all of the diplomats are suffering serious hearing loss.  And of course they're saying, wow, does somebody there have the equivalent of the portable dog killer technology?  And of course, well, no.  It's certainly different.  Remember that this was high-frequency, but audible to dogs and us, which is why Mr. Archibald heard it when I zapped him with it at the end of the day in the story.  Those who don't know what we're talking about may want to look back into and find that episode [SN-248].  But anyway, I just wanted to acknowledge everybody's, I mean, I've gotten so much input from people saying, hey, Steve, looks like they had a version of the PDK.  And it's like, well, whatever they're doing is unfortunate and certainly very malicious, to impair someone's hearing by who knows what they're doing.  Scary.



James Parsons tweeted:  "If SQRL requires a password re-entry every session, it encourages weak passwords, especially on mobile."  He says, "Special chars are difficult on phones."  Again, I'll just say, because I already did cover this, the long password is only needed once to tell the system you're you, which we have to have to make it hackproof.  And then afterwards you do a per-authentication short version.  And the good news is everyone will be able to play with this very soon.



And as for phones, in the case of Jeff Arthur, who is the author of the iOS client, you do need, again, to authenticate yourself once.  But then that enables Touch ID, and no doubt he'll be supporting Face when it comes along.  And so then you're able to just authenticate with your fingerprint, just as you would expect.  So again, a set of tradeoffs exactly like what Apple has designed is the same thing we're doing for the sake of security and to make it not cumbersome.



I mentioned the payment API a couple weeks ago, and someone tweeted the question, does this mean all adopting online retailers can stop storing our credit cards?  Breaches would be far less harmful.  No, it doesn't mean that.  All the payment API means, which will be coming to all of our browsers, is that rather than us having to redundantly fill out payment forms across the Internet, our browsers can be a secure repository of that information, can contain our billing address, the credit card number, the expiration date and so forth.  And the API allows a website to request through an established protocol that information.  It'll pop up a standard dialogue which we'll get used to.



So the standardization at the user side is very useful, will verify the stuff we're allowing our browser to send, and then just say yes.  And that will go then to the remote site in a uniform format to essentially circumvent the whole form fill-out thing.  So all it is, is a unification and sort of a standardization of our submission of repetitive similar information to make it secure and uniform to the user and smoother, to smooth the checkout process and the payment process for the website.  So it's a good thing.



But what the server does, what the remote web server does once it gets the information is still out of our control.  For that you need PayPal or some third-party payment processor if you want to keep the sites blinded to that information.  But still a very nice thing.



So now let's talk about what Moxie Marlinspike and his gang who are developing Signal have done.  Okay.  Matthew Green, our friend and cryptographer at Johns Hopkins, said of what I'm going to discuss, he said:  "Private contact discovery for Signal.  Make no mistake, what Moxie is doing here is going to revolutionize messaging."  I've got a link for all the details, which I'm not going to go into because it's lots, like pseudocode where they're talking about showing sample code to implement these different things.  But here's what's going on.



As I said at the top of the show, we recognize that it's one thing to encrypt the communications, and a different thing to encrypt the metadata, meaning that I could have encrypted email which I send to somebody, but the fact that I'm sending it is not encryptable.  The envelope that contains the mail has to have headers for the destination.  And typically the sender, in order for it to go from point A to point B, it has to be addressed.



And this is true everywhere.  We've talked about, for example, even how HTTPS, the standard encrypted protocol for the web, it encrypts the contents and encrypts some of the metadata, but not all.  People still know what IP you're going to because your traffic is going there.  And in some cases the first packet identifies the website that you're asking for, which is how the server knows which certificate to match for your connection.  So there's still - there's always still some metadata leakage.



That wasn't good enough for Moxie and his gang.  So they said, okay, we need to solve this problem of a new Signal user wanting to find out which people in their own contact list on their phone, for example, are Signal users.  How do we do that in a way that leaks nothing, even to the server that is providing the intercommunication?  Because, remember, this is all end-to-end encryption, meaning that they've designed a system so that the server is blind to the contents.  They also wanted to blind it to this aspect, this social graph metadata.



So one thing they could do, for example, is to hash the phone numbers of everybody in your contacts list.  And so what you would be doing is you would be submitting a list of hashes of all the phone numbers.  Send them up to the central server, and it then looks through your hashes for the hashes of all the other subscribers of this system for any matches.  And if the hashes match, then we know that the phone numbers match.  And so that says that you have the phone number in your contacts list that matches another phone number of a subscriber; and so, yay, they're a Signal user, and you could connect to them.



The problem with this is that there isn't enough entropy.  There's not enough randomness in a phone number to prevent it from being brute-forced.  We know that you cannot go backwards from a hash, from the hash back to the phone number.  But you can put in all possible phone numbers and get all hashes that match those phone numbers.  And that wouldn't be that hard.  There's just not that many phone numbers, and hashing is very fast now, thanks to the cryptocurrency people that have, like, moved all this down to silicon.  So it doesn't work to hash a low-entropy item like a phone number and use that for security.  It's just not enough security.



So Moxie and company understood that.  What they came up with was very clever.  Intel has a set of features for everything from Skylake on, which is like three generations ago.  Remember, Skylake was the laptop I purchased, my last Lenovo.  And the Windows 7 machine that I built a couple years ago when we heard that Microsoft was not going to be supporting the older processors moving forward - or, wait, yeah, was not supporting the - oh, wait, the newer processors on the older OSes.  I want to still be using Windows 7 for a long time, but I thought, uh-oh.  I've got to get a Skylake processor now, which I did for both my next main machine and my laptop.



So Skylake and, what, there's a Haswell and something since.  They all have this, what's called SGX, Software Guard Extensions.  And this is very similar to what the ARM guys have built into, known as Trust Zone.  It's an extension to the Intel architecture which allows the creation of protected software containers.  The total size is limited to 128MB, so it's not gigs, but it's still large.  And it creates a protected enclave in an Intel chip, much as Apple has done on their iOS devices, where they have a cryptographic enclave where they can store secrets.  This is a means, supported since Skylake, for doing the same thing on any of our systems that we have.



So it was designed to support DRM, which none of us are big fans of.  But the idea was that the actual contents of this region of RAM would be encrypted so that it could not be inspected.  If it was inspected from outside the enclave, it would just be gibberish.  It would be encrypted.  But from code running inside the enclave, inside of this software guard, it would be Intel instructions and data, which could be viewed.



The other part of this is it can be - the technical term is "attested to."  It supports attestation so that someone connecting to this from outside, like in the case of DRM, a content provider could be assured that no changes had been made to this software guard extension enclave so that the encrypted data going in is being decrypted and is only going to be played once - not, for example, exported as a torrent and then put on the Internet.  So all of this exists and is in place.  But this is all client side.



What Moxie and company figured out was that this could also be done on the server side to create a provably trusted, essentially - the term we've used is an HSM, a Hardware Security Module - create a virtual HSM just using Intel chip features to create a container where this hashing could be performed in a secure fashion.  That is, a bunch of phone numbers would be hashed, and then that blob would be encrypted to prevent the hashes from being reversed.  This encrypted blob would then go in up to the server, up to the signal centralized server, into the Software Guard Extensions enclave, where it would be decrypted back into a list of hashes.



Now, they worked all this out.  The problem is that, even though you cannot decrypt - even though something outside cannot see in, it can detect memory access patterns.  So they recognized that that would create, technically, as we know, known as a side channel leak.  So just by inferring from the pattern of memory fetches, there's enough information there to leak what's going on inside.  So then, and this is where I get into the pseudocode that they show in their coverage of this, they worked out, they carefully worked out a way to sort of design a meta hash matching system that deliberately decouples the work being done to match the hashes with the memory fetches which are used to do the work.



And the set of tradeoffs they came up with was one where there's a relatively large setup time, which is required in order to obfuscate where everything is in this scramble.  That takes a while.  But once that's done you can batch all of the various clients that want to compare their hashes against this master list because it's the master list that needs to be obscured.  That can be set up once so it doesn't matter how long it takes.  And what they ended up verifying was that they could do - they could essentially scale to over a billion users using this technology, essentially making the algorithm aware of the caching and cache lines, explicitly aware so that it would defeat the ability of anyone outside to observe memory fetch patterns and timing, thus the need to be caching aware, to be able to make sure that cache hit and misses aren't also leaking any information.



And essentially what this means is that Signal users using a Signal server with this technology - oh, and by the way, it's on GitHub.  It exists.  It's in beta at this point but it is freely published.  The idea would be that the Signal server would implement this in the server hardware using Intel software guard extensions, and the client would be able to verify the integrity of this container before encrypting the hashes of its contacts' phone numbers and sending them into the container for decryption and matching and then weeding out.



What then happens is what's returned is a subset of the context of the user's contacts, which are a subset of hashes, which are then reencrypted and returned to the client, where it's then able to compare the hashes of all the phone numbers in its contact list to the smaller subset that are Signal users and flag all the contacts, all of the users in the contacts list as Signal users where they are, and then they can connect with them with absolutely zero leakage that any of the user's social graph has been exposed.



So bravo to Moxie, and this is what we want.  I'm just thrilled to see this kind of attention being paid to security.  Over the course of this podcast we've gone from sort of training wheels security, where it's like, oh, look, it's encrypted, we're done, to oh, no, no, no, it's encrypted and we're just getting started.  Because our understanding that encryption is just a small piece of the whole problem has really matured over the last decade plus.  Very cool.



LEO:  Once again, Moxie Marlinspike to the rescue.  Steve, I'm so glad to be back.  I missed this show, and I missed the chance to ask you...



STEVE:  Glad to have you back, Leo.



LEO:  ...all the questions about my security woes and share my issues with you.  We do this show every, well, actually we're not constrained by time quite as much as we used to, so that's kind of nice.  But the theory is we'll do this Tuesdays at 1:30 Pacific.  We're a little late, and now you know why we were late.  I was busily going through all my accounts to make sure I hadn't been compromised.  I didn't want a Mat Honan.  I was afraid.



STEVE:  No. 



LEO:  Yeah.  I tell you, two-factor, baby.  If you're not using it, use it.  We do this show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to tune in live, by all means also join us in the chatroom.  They're a great group.  Mashed Potatoes says nice to have the full moustache back, Steve.  Irc.twit.tv for the chatroom.  You don't need to have an IRC client; but, by god, if you're listening to this show, you damn well better.  I couldn't understand why you wouldn't.  You also can get on-demand versions at Steve's site, GRC.com.  He has a nice audio version.  Plus it's the only place you can get a written transcript of everything Steve says.  Does Elaine take out the uhs and the, you know - you don't say "uh."



STEVE:  I told her to do so, and she balked a little bit 12 years ago because she wanted it to be exactly - I said, "This is not a medical..."



LEO:  It's not court.  We're not talking court here.



STEVE:  Right, right, right.



LEO:  She's a court reporter.  I think that's probably where she gets that.



STEVE:  And I should mention that her husband is having some medical challenges.



LEO:  Oh, no.



STEVE:  Yeah, Bennett.



LEO:  Sorry, Elaine.



STEVE:  So the transcript, she warned me yesterday, may be not available till the weekend.  So I already notified the people in the Security Now! newsgroup at GRC.  And it'll probably affect her for the next week or two.



LEO:  Oh, I'm sorry, Elaine.



STEVE:  So it's kind of major, yeah.



LEO:  Okay.  Well, we're thinking about you.



STEVE:  Yup.



LEO:  She does such a good job.  Twelve years she's been doing this?  That's kind of amazing.



STEVE:  Yup.



LEO:  She's been doing it almost as long as we have.



STEVE:  Every one.



LEO:  Just go to GRC.com to find that and all the other great free stuff Steve gives away.  And of course his bread and butter, and if you like Steve, support him by buying a copy of SpinRite.  You'll be glad you did.  GRC.com.  It's the world's best hard drive maintenance and recovery utility.



Now, we have audio and video.  I know I'll never - it baffles me.  You didn't understand why we're doing video, I don't understand why we spend all that money on video.  But people do like video.  And if you want to watch, if you want to see Steve wave and do his "live long and prosper" properly with the thumb out, he does it right, and he does it with the right hand.  No, you're doing it with your left hand.  Should I do it with my right or my left? 



STEVE:  I don't know.



LEO:  Does it matter?  I can't do it with my left.



STEVE:  Unfortunately, yeah, my right hand is behind the...



LEO:  Yeah.  You could do it with both hands, which shows one thing:  a misspent youth.  If you want the video go to TWiT.tv/sn for Security Now!.  Or I think the best thing to do is subscribe.  You could subscribe to audio or video versions in any podcast application:  Overcast; Pocket Casts is I think number two after iTunes; Stitcher; Slacker; TuneIn.  You can even listen, you know, if you have an Amazon Echo or a Google Home, you can just say, on the Echo, you say "Echo, listen to Security Now! on TuneIn," and you can hear the latest episode.



Increasingly, I think that's how people are going to be listening to podcasts at home.  It's just asking for it on their Echo or their - it works with Google Home.  The syntax is slightly different.  I can't remember what it is.  Used to be you could watch it if you had an Echo with a screen, but YouTube's turned that feature off.  But listening is fine.  You could also listen to our live stream if you want to join us live by saying, "Echo, listen to TWiT Live on TuneIn."  TWiT Live is the live stream.  Does that take care of all of our business?  I think it does.  I think it's time for me, sadly, to say goodbye, and live long and prosper.



STEVE:  Well, until next week, my friend.



LEO:  What do they say on "The Orville"?



STEVE:  Luckily, I don't know.



LEO:  You have no idea, and you're glad.



STEVE:  I'm never going to know.



LEO:  It's probably profane.  Thanks.  We'll see you next time on Security Now!, Steve.



STEVE:  Thanks, buddy.  



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#632

DATE:		October 10, 2017

TITLE:		The DNSSEC Challenge

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-632.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we take a look at a well-handled breach response at Disqus; a rather horrifying mistake Apple made in the implementation of their APFS encryption (and the difficulty to the user of fully cleaning up after it); the famous "robots.txt" file gets a brilliant new companion; somewhat shocking news about Windows XP - or is it?; Firefox EOL for Windows XP support coming next summer; the sage security thought for the day; an update on "The Orville"; some closing-the-loop comments, including a recommendation of the best Security Now! series we did in the past; and, finally, a look at the challenge of DNSSEC.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got lots of questions, lots of feedback.  We'll talk about why your passwords should be like your underwear.  Or maybe not.  And the newest thing with all the kids is replacing or supplementing robots.txt:  security.txt.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 632, recorded Tuesday, October 10, 2017:  The DNSSEC Challenge.



It's time for Security Now!, the show where we protect you and your loved ones online with the help of the man in charge at GRC.com, Gibson's stuff.



STEVE GIBSON:  Someone sent me a tweet saying, "I'm disturbed by the fact that you're saluting with your left hand."



LEO:  Oh.  But you're not.  You're live-longing and prospering.



STEVE:  That's right.  That's right.  And the problem is I've got the microphone here blocking my right hand.  And it's like, oh, okay.



LEO:  I never even thought of that; but, you know.



STEVE:  Yeah.



LEO:  Did you ever serve in the Armed Forces?  I don't think so; did you?



STEVE:  Never did.  Boy Scout was as close as I got.  And I seem to remember that it was - the Boy Scout salute was left-handed.



LEO:  Was it?  Maybe it is.



STEVE:  That's kind of what I'm doing.  I don't know.  I don't know.  They both seem - trustworthy, loyal, helpful, friendly, courteous, kind, obedient, cheerful, thrifty, brave, clean, and reverent.



LEO:  That is our man, Steve Gibson, from GRC.com.



STEVE:  Haven't said that for quite a while.



LEO:  But you remember.  It's burned into your brain, isn't it.



STEVE:  It is, yes, indeed.  So we've got Episode 632 today, which I titled the DNSSEC Challenge.  DNSSEC is of course DNS Security.  And the occasion of this is the punting for what was planned to be tomorrow's major key rollover, it was going to be the 2017 key-signing key rollover where we've had seven years since 2010 was where the existing keys were first put online.  And the ICANN punted because they realized, whoops, we're not ready yet.



LEO:  What?  Oh.  This is that big key exchange ceremony where they do the weird thing?  Is that the one?



STEVE:  No, no.  That's a whole...



LEO:  That's different.



STEVE:  ...like in plain sight, no way for any one party to compromise because it's so important that we get that right.  This is an automated process for rolling keys.  And anyway, so that's what we're going to get into.  I want to talk about, sort of remind people where we are, why this is - it's 20 years old.  I mean, it was 1997, 20 years ago that the first RFC for DNSSEC appeared because it was clear that we needed to be able to secure the domain name system.



And I've often talked about all the advantages, if we ever get to a truly secured global lookup system, which is what this promises, how many things we can do.  So, I mean, for example, among them we are able to move away from this problem with there being literally many hundreds of root certificate authorities, all of whom our browsers must trust, because if DNS were secure a website domain could publish its own public key, rather than having this whole chain of trust system where the public key is signed by an authority and, because we trust the authority, then we trust its signature.  So all kinds of things can get done that we can't do now.  But we're not ready yet, after 20 years.  And so I want to sort of put that into context and talk about that.



And of course we've got a lot of news.  We're going to take a look at a well-handled breach response at Disqus.  Or "discuss," I'm not sure how you pronounce it.



LEO:  I think they say "discuss."



STEVE:  They probably do.



LEO:  I had this whole conversation with them years ago.



STEVE:  Yeah, I don't think it's an Olympic event, I think it's a discussion board.  So, yes.



LEO:  Spelled with a Q, not to confuse you, yeah. 



STEVE:  And we have a rather horrifying mistake that Apple made in the implementation of their newer APFS encrypted file system and the difficulty of, for each individual user, in cleaning up after it.  We've talked sometimes in the past about the famous robots.txt file.  It gets a brilliant new companion coming soon.  We've got somewhat shocking news about Windows XP.  Or is it?



LEO:  There's news about Windows XP?  Wow.



STEVE:  Believe it or not.  My jaw dropped.



LEO:  That's like saying there's news about Abraham Lincoln.



STEVE:  Precisely.  And wait till you hear it.  So we also have a declared end of life for Firefox for Windows XP, which a lot of our listeners tweeted to me, saying oh my god, Steve.



LEO:  It's a big deal.  That's the last browser; right?



STEVE:  Well, I'm staying with it.  But it's not until next summer, so I'm going to - hopefully by then I will be moved over.  And actually there's an event which will be causing that to happen.



LEO:  Wait a minute, moved over from Windows XP?



STEVE:  Yes, to Windows 7.



LEO:  Wow.  You are so modern.



STEVE:  Kicking and screaming all the way, yeah.  So we also have the Sage Security Thought of the Day, courtesy of Matthew Green; an update on "The Orville," thanks to many of our listeners who said, okay, wait a minute.  We also have some closing-the-loop comments.  Oh, including a recommendation of the best series we ever did on Security Now! in response to one of our listeners' questions about, like, what would I recommend he go back and listen to.



LEO:  Oh, I know.  I think, well, this is good because I would - I know what I would nominate.



STEVE:  I'll bet it's the same.



LEO:  So good.  I bet it's the same.  All right.  Okay, good.



STEVE:  And then, once we get all that done, we will finish the podcast, taking a look at why it has taken 20 years to still not yet get DNS secured.



LEO:  Yeah, because I feel like - I thought DNSSEC was, like, done.  Apparently not.



STEVE:  And in fact I've got charts and statistics and numbers and things about where it is.



LEO:  Wow.



STEVE:  But it's just a heavy lift.



LEO:  It fixes a multitude of woes.



STEVE:  And then, believe it or not, when you think - you would think in our 13th year, Leo, we would have pretty much seen any possible Picture of the Week.  But we're going to be introducing underpants.



LEO:  Okay, Steve.  I've got it queued up.



STEVE:  So our Picture of the Week appears to be authentic.  Several people sent it to me, of course.  It's got the logo in the upper left-hand corner of Shell Oil, and it's got the coloration of Shell Oil's corporate colors.  So it looks to me like it's real.  And the big message here is that they have some site they call Think Secure.  And that's certainly a good concept to be promoting.



LEO:  This is probably an internal document, right, given to Shell employees.



STEVE:  Yes, yes.  And in fact the photo that I saw that I snipped this from was sort of like a poster that you'd pass by as you were walking down the hall and think, okay, where am I working?  Because this says:  "Treat your passwords like your underpants."  I'm not kidding.  "Treat your passwords like your underpants."



LEO:  Well, wait a minute.  Like your - what?  Like take them off before you go to bed?  What?  I don't - what?



STEVE:  Change them often, which sounds like a good idea.



LEO:  Okay, simple, yeah.



STEVE:  Don't leave them lying around, just the whole personal hygiene tip.



LEO:  Yeah, put them in the hamper, yeah, yeah.



STEVE:  And this, I think, is really - a third one I'm really agreeing with:  don't share them.  So keep them to yourself.



LEO:  Somebody in the chatroom said, oh, long underpants, long passwords?  No.



STEVE:  Yes, boxers or briefs?  Do we - yeah.



LEO:  I think the point of this is to get the attention of the employees, to get...



STEVE:  And it would do that, yes.  It got the attention of the Internet, and it certainly got the attention of this podcast.  There is, I mean, I was tempted to dig in further.  It may have been an Intranet website...



LEO:  That's what I guess.



STEVE:  ...that wasn't publicly available.  But, for example, they say use different - this is down below:  "Use different passwords for business and personal purposes."  It's like, no.  Use a different password...



LEO:  For everything.



STEVE:  For every single thing you do.



LEO:  Everything, yeah.



STEVE:  Yes.  As we are learning from breach after breach after breach, you just have to use unique passwords.  Anyway, I just [crosstalk].



LEO:  Oh, I know.  Treat your passwords like your underpants.  Avoid crappy passwords.  Yeah.  That's one way of putting it, yeah.



STEVE:  This is going to be the start of a whole new meme.  Wow.  



LEO:  And actually I don't think changing your passwords often is really very good advice, but we've talked about that before.



STEVE:  No, exactly.  That was one of the nice things that the IETF finally updated their recommendations for.



LEO:  NIST.



STEVE:  NIST, right, the NIST updated their recommendations for a couple months ago was, okay.  Oh, and the guy who originally, as we discussed on the podcast, the guy who originally just made that up out of whole cloth.



LEO:  Seems like a good idea.



STEVE:  He said, yeah, it was never really a good idea, so I apologize.  Wait, you apologize?  You screwed up the whole world.  Oh, lord.



LEO:  Twenty years later, oh, yeah, I just made that one up.



STEVE:  Yeah.  And of course we'll take credit here from the day one of saying, uh, what is that supposed to achieve?  What is the possible logic behind that?  Because, yeah.  As we've often discussed.



LEO:  It is common in corporate environments.  In fact, my corporate bosses at iHeartMedia make you change your password every six months.  So it's still common practice, even though I don't know what it achieves.



STEVE:  Well, and we've discussed how employees end up working around that because they'll, like, change it, and then change it back.  Unless the system is maintained...



LEO:  Or they'll add a "1" to it or - yeah, yeah.



STEVE:  Exactly.



LEO:  That's my favorite.



STEVE:  Or, yeah, okay, fine.  So Cory Doctorow reported for BoingBoing about what he described as a well-handled security breach at Disqus, D-I-S-C-U-S.



LEO:  Q-U-S.



STEVE:  Oh, it is.  Oh.  Q-U-S.  Oh, I just missed it.  How did I miss that?  Well, okay.



LEO:  I used them for years, so that's how I know.  And I remember talking to them when I first...



STEVE:  Oh, D-I-S-Q-U-S, sure enough, I see it down later.  So he noted that five years ago, back in 2012, the Disqus commenting service suffered, at the time, an undetected breach of 17.5 million user accounts.  Once upon a time that was a big number.  Of course we were just talking about 145 million.



LEO:  Three billion.



STEVE:  And then Yahoo's three billion.



LEO:  Three billion.



STEVE:  It's like, eh, how many zeroes does it have is the only thing we're really wondering about these days.



LEO:  Well, I use Disqus.  So the idea is it's an add-on, like it was for my WordPress blog, that lets people comment on the blog, and it pulled in social media.  It was actually a really neat idea.  But in order to comment on Disqus you had to have an account with Disqus.  So this is a lot of end-users commenting on blogs, basically.



STEVE:  Right.  So Troy Hunt, who is a security researcher we have been following and often who comes up in our news - he's the guy who created the HaveIBeenPwned site and service.  It was through his work that he discovered and disclosed the breach just recently, this five-year-old breach.  So these passwords had gotten loose, and nobody knew about it.  So Troy was extremely happy with Disqus's response, so much so that his blog posting was titled "Disqus demonstrates how to do breach disclosure right."  Title of his posting.



And he said:  "Twenty-three hours and 42 minutes from initial private disclosure to Disqus to public notification and impacted accounts proactively protected."  And then, just to sort of put this in context, he said:  "Think about everything that had to happen within this less than 24 hours, just shy of 24 hours."  He says:  "I had to get a response and establish a communication channel."



He says:  "I had to get the data to them securely."  And he said, parens, "(over Australian Internet speeds).  They had to download and review the data.  They had to establish the legitimacy of the data.  They had to ensure that there was no ongoing risk in their system.  They had to invalidate passwords that had been exposed," 17.5 million.  "They had to contact the impacted users" whose passwords have been exposed.  And they had to prepare the communication of this disclosure.  All which happened within a day.  Thus, relative to the other organizations that he has informed in the past, he was very impressed.



He wrote:  "When I look at how Disqus handled their intent, they ticked so many of the boxes.  It was easy to report to them."  And, by the way, this comes back to the soon-to-be-adopted companion to the robots.txt file that we'll be talking about.  So it was easy to report to them.  "They applied urgency," he said, "more than I can honestly say I've seen any company do before under similar circumstances.  They disclosed early, earlier than anyone could have reasonably expected."  Again, he says normally he thinks of 72 hours as the "Gold Standard."  They did it in less than 24.



"They protected impacted accounts very quickly by resetting the passwords of those that had been disclosed as a consequence of this breach.  They were entirely transparent," he wrote.  "There was never a moment," he said, "where I thought they were attempting to spin this in their favor at the expense of the truth."  And of course, as we've often covered here, I'm rough on companies where they really seem to be hedging.  It's like, okay, come on.  When they're trying to downplay the severity, and later it comes out, whoops, it was much bigger than it was originally believed to be.



He said:  "They provided details.  The passwords were salted SHA-1 hashes," he wrote, " which is not a pretty story to tell in this day and age, but they told it truly regardless."  And in their defense, five years ago, yeah, okay, SHA-1.  If you salted it and you iterated it, that was probably good.  I don't know that it was iterated, but at least it was per user salt, so that was good security practice at the time.  And who knows what they're doing now, but it was those hashes back then that were the ones that were disclosed.



And, finally, he wrote:  "They apologized."  He said, "It was one of the first things they said.  They owned this incident from the outset and didn't attempt to divert blame elsewhere."  So bravo to those guys.  You couldn't ask for - as we've often said here, mistakes happen.  Anybody can make a mistake.  What you could be held to account for, I think, is first the policy that you set forth, and also how you deal with a mistake.  And these guys did the right thing, so bravo.  And it's not as if losing a Disqus password was life-threatening.  It's not like Equifax.  It's an add-on discussion board system.  Still, you'd like to keep it secure so the people aren't impersonating you or messing around with your account.



LEO:  And they may have had credit card numbers for, I mean, you have to pay for it as the website user.  So they may have had, for a much smaller number of people...



STEVE:  If the breach included that information, yes.



LEO:  Yeah.  I don't know if it did or not.  So they had my credit card number, for instance, I mean, an old one.



STEVE:  Yeah.  And while we're on the topic of anyone can make a mistake, some are more embarrassing than others.  Apple issued an update to its High Sierra desktop OS last week, last Thursday.  They called it the "macOS High Sierra 10.13 Supplemental Update," which repaired two dangerous bugs in that OS, both which exposed user passwords, but in different ways.  In the first case, if you had created a new APFS, that's the Apple File System, with an encrypted volume under High Sierra, and then set anything at all as the password hint, so how you create a password and oftentimes you're offered the opportunity of giving yourself something that would be meaningful to you to help you remember, hopefully, which of the billion you have.  Even better, you can't remember it, and you've stored it in a password manager somewhere.



On the other hand, if you can't get into your encrypted volume in order to get to your password manager, that won't help you.  So you'd have to store it somewhere else.  But the point is, if you used the hint feature, then whoops, your password itself was stored as the hint in plaintext.



LEO:  Oh, that's terrible.



STEVE:  It's not good.  That means anyone, I mean, it's like, ooh, how did this get by QA?  I mean, just yikes.  That means anyone could have gotten your passwords simply by clicking on the "Show hint" button.  They really should have just relabeled it:  "Show my password."



LEO:  Show password.  Yeah, there you go.



STEVE:  And, I mean, so yes, it's a bug, obviously.  Somewhere the intent was to take what was in the provided hint field and copy it into hint storage.  Instead, they used the wrong variable.  They copied the unobfuscated, unencrypted, I'm sure they take the password and hash it like crazy, iteratively, in order to do a really great job of securing it.  Unfortunately, they took the pre-obfuscated, user-provided plaintext password and stored that in the hint record. 



LEO:  It's more embarrassing than dangerous.



STEVE:  Um, well...



LEO:  Well, I'll tell you why.  First of all, somebody obviously has to have physical access to your system.



STEVE:  True.



LEO:  But also, now, Rene was talking about this on MacBreak Weekly.



STEVE:  That is what you're protecting.  You are protecting physical access to your system.



LEO:  Yeah.  It's File Vault.  It's protecting, encrypting the hard drive.  But I think it only showed up on a second encrypted partition, not the first, because in order to see it, you'd have to be able to boot the first, run the disk utility, and then you'd get the password for the second one.  That's what I think Rene told me.  So I may be misquoting this.  Anyway, it's not, I mean, somebody has to have physical access to your system.



STEVE:  Well, but remember, that's what it's protecting.



LEO:  Right.  No, that's why you encrypt.



STEVE:  It's no protection for what it's protecting.



LEO:  Right, right.  Yeah, it's [crosstalk].



STEVE:  And what's a little disturbing is that it's quite burdensome to recover from that.  You have to install the 1013 supplemental update.  Then you have to create an encrypted backup of the data in your affected APFS volume.  Then you open Disk Utility, select the encrypted volume in the sidebar, unmount the volume, then erase it, then type in a new name for the volume; change it to APFS and then back to encrypted in order to sort of, like, flush things out; then enter a new password in the dialogue, now under the fixed system.  Add a hint if you choose.  This time it's safe to do so.  Then you apparently click "erase," and then you watch it do that.  Then, once it's done, you restore your data from the backup to the volume.  So, yikes, it would have been nice if there were some way to automate it, but I guess there was nowhere to put that data while it's doing all of this.



LEO:  A lot of people won't be bit because it shows up in the Add APFS Volume Command in Disk Utility.  So it isn't for your primary volume.  If you turn on File Vault...



STEVE:  And you only have one, yes.



LEO:  Yeah.  Which is how most people do it.  Very rarely do you add a second APFS volume and then encrypt it.  And the good news is you do it probably on an external drive, so this isn't as laborious of a fix as it sounds because you'd have a second external drive and move it over, blah blah blah blah.



STEVE:  And that does also explain kind of how it got loose, I mean, how it got - it's a more subtle bug than if it was the primary.



LEO:  Not many people would use this; right.



STEVE:  Right, right, right.  Good.  And the second problem was one that was discussed a couple weeks ago about the problem in the password chain, I can't remember what Apple calls it.



LEO:  Keychain.



STEVE:  The Keychain, yes, which was also fixed in 10.1.3, which improved the protection that was being offered.



LEO:  I use Keychain, so that's a fairly important technology.



STEVE:  Yeah.  And in fact I don't think it's possible to use macOS, is it, without Keychain?



LEO:  No, it automatically puts stuff in there.  But I use it, and many do additionally, as a kind of a password manager.



STEVE:  Ah, right, right.



LEO:  Because it will generate passwords and store them for you.  I also use it as a certificate manager for when I use S/MIME certificates.  So it's a pretty important technology.  Not any more important than File Vault.  That's important, too.  Both of these are...



STEVE:  And it's nice that it's built into the system so users - I mean, and it's something that has received a lot of Apple security attention over the years.  So it's a safe place to put things.



Okay.  So we've talked in the past, but not for a long time because it's not high tech, but good answers, good solutions don't have to be, about the robots.txt file.  What happened in the early days of the Internet, when we began to have automated web crawlers, also called "spiders," which is for example how Alta Vista and the early search engines and of course Google all worked is they send automated agents out to go rifling around through people's websites, looking at pages, indexing them, and thus being able, when we make a search query, to say, oh, those words you're looking for, we happened to find over here.



Well, no human did that.  A bot roaming around the Internet did that.  The problem then arose that there were places on a website where a robot should not go.  First of all, you could have dumb robots that would follow every link, even if they were circular.  So a robot could follow, could go to a normal page, then record all the outbound links from that page, which might refer to page two, and page two might refer back to page one.  So if the logic wasn't good, the robot could just get stuck in an infinite loop and be essentially pounding on someone's server with no end.  There were other places where, especially in the early days, when servers were not high-powered, or when they had very inefficient interpreters, doing some active content stuff put a big burden on the server.  And then, for example, you might have an ecommerce site, or an ecommerce area, where it made no sense for a bot to go in there because there was nothing in there for them to see.



So all of these problems got handled with a convention that was established, the robots.txt file.  By convention, it sits on the root, that is, www.grc.com\, the root of the server.  And well-behaving bots and, I mean, basically today any bot, any automation, when it wants to go to a site, it assumes the existence - normally there are not links to it.  It just assumes the existence of a robots.txt file in the root, in the base directory of the site, which it retrieves.  So it asks for that, and the robots.txt file has a simple but well and time-honored, well-established format, where you're able to specify which so-called user agents you want to apply a set of rules.  So you are able to sort of tune the rules for who's coming in.



Again, it's all informational.  None of this is enforced.  It's basically saying, "We would appreciate you robots out there if you respected our wishes and did not go digging around in the following places."  And so, for example, GRC.com has one where there's just a bunch of stuff where it's better if they don't go wandering in there because there's nothing very interesting, and it could screw things up.  So we've had that for years.



In what I think is a brilliant concept, we're going to be getting security.txt.  Security.txt will similarly be a text file located in the root directory of a website, which is meant not to inform robots, but to inform researchers.  It will create a unified, centralized place for a security researcher to determine who to call, who to send email to, who to inform if a security problem is found with the site.  And it is so simple, it is just brilliant.



LEO:  It's also a promise, in a way, which I think security researchers are rightly so paranoid these days because you can get arrested for pen testing or working on a system and finding exploits.  So it's a promise to the researcher, no, it's okay, let us know.  I mean, I don't know if it's a binding promise, but that's the mean reason I like this.  A researcher won't be put off from looking for flaws and reporting them.



STEVE:  Right.  Well, and even Troy, in his previous note about Disqus, he said, "I was able to contact them more easily because I had a preexisting relationship."  But very often obscure researchers don't have a contact at some random company.  And knowing that you could put into the browser www.domain, whatever it is, .com or .it or who knows what, .ru, /security.txt, and immediately be shown a couple lines that that provide you the information.



So, for example, there are - you can have one or more contact fields, where there can be an email address, like security@example.com; a phone number to call; a page that refers you to their security policy or for additional information.  Also there's an encryption field.  You can say "encryption:" and then, for example, they can provide their PGP public key, which you're then able to use to encrypt your report to them so that it can be sent to them.  And as we enumerated in the previous story, Troy needed to, because this is sensitive information, he needed to have a means of sending this information to them securely.  So this provides that.



And also, what is the site's disclosure policy?  And so, for example, in the formal emerging RFC, it says it allows a company to specify the disclosure policy where the type could be "full," which would stand for full disclosure, "partial" for partial disclosure, and "none" means you do not want to disclose reports after the issue has been resolved, meaning how does the company intend to manage the information that you have provided?  Will they fully disclose it, partially disclose it, or just not say anything at all?  And then, also, what is their acknowledgment policy?  The fourth type of field is acknowledgment, where they provide a page to, like, their Hall of Fame or whatever, where people who have helped them are being disclosed.



So I just think it's so simple.  As I said, really good ideas don't have to be complicated.  And this is just great.  Again, voluntary.  Nothing's requiring it.  But you can imagine at some point in the future we'll start getting a list of sites that do provide a security.txt file in their root and those that don't, and be sort of asking those who don't, hey, why not?  It's so simple to do.  So, yay.



Back sort of on the topic of old OSes are oh, so very hard to kill.  As we were talking at the top of the show, there's actually news about Windows XP.  Believe it or not, Leo, and I know you're sitting down, and we don't have to worry about you being centered over the ball anymore, so no worry about you being capsized. 



LEO:  Okay, I'm bracing myself.



STEVE:  Windows XP is going to be getting TLS v1.1 and 1.2.



LEO:  Oh, my god.  Well, that's actually great.



STEVE:  I know.



LEO:  Is that Microsoft doing that?



STEVE:  It's Microsoft.  In their official blog, and I've got the link to it in the show notes, Microsoft has backported TLS v1.1 and 1.2 to support XP, and it will be appearing in the Windows Update channels.  Okay, now, we need to add a little bit of a caveat because remember that this is for the POSReady, the Point of Sale Ready...



LEO:  Oh, it's not all XP.



STEVE:  Right.



LEO:  I was going to say, that's quite an acknowledgment that people are still using XP, but it isn't.  It's POS XP.



STEVE:  Correct.  And that better explains the context of this.  So what's happening is Microsoft has recognized, and no doubt there is pressure behind the scenes on them, that there are turnkey kiosks and point-of-sale systems still in operation throughout the world which are beginning to experience connectivity problems due to the aging TLS protocol support in Windows XP.  I was finally forced to move to Service Pack 3  because SP2, where I had been happily for years, did not understand SHA-256.  And I was back on SHA-1 support in the cryptographic libraries of Windows XP.  I'm on Service Pack 3 with no problems.  SHA-256 is understood.  But I'd like to have TLS 1.1 and 1.2 support.  So when I restart my system soon, I'll be getting it because my system has been flagged as POSReady.  I have several Win XP machines around that are that.



LEO:  That was that thing you did; right?



STEVE:  Yes.  It's just a registry key entry.  Anybody who is still using Windows XP can google "WinXP POSReady 2009."



LEO:  I'm amazed that's still working.  I would have thought people would have - that Microsoft would have turned that off right away.



STEVE:  I don't think they really want to.  I mean, I think...



LEO:  No, they acknowledge there's people using it.  And this is good.  That's good.



STEVE:  Yeah.  Yeah.  And on the topic of XP, Firefox has formally announced on the 4th, so it was last Wednesday, the day after last week's podcast, that next June of 2018, which gives me time - and I need a deadline.  We know I need a deadline.  I was forced to Service Pack 3 of Windows XP because SHA-1 was finally dying, and I thought, okay, I'm going to have to do this.  So that'll be good for me.  I'm not leaving Firefox.  I still like it.  It's getting much faster recently.  They're continuing to do things.  I just like it more.



And so they said:  "Last year we announced that Windows XP and Vista users would be automatically moved to the Firefox Extended Support Release (ESR)" - and that's what mine is now - "ensuring them continued updates until at least September 2017."  Okay, well, that was last month, and I'm still alive.  "Today," they wrote last Wednesday:  "We are announcing June 2018 as the day Steve" - oh, no, I'm sorry - as the final end-of-life date...



LEO:  You are the best-known Windows XP user, though, I must say.



STEVE:  The day Steve will finally be forced off of XP, kicking and screaming.



LEO:  But it's the last major browser not to update; right?  I mean, Chrome stopped a while ago.  Of course IE stopped a long time ago.



STEVE:  Yeah, and Chrome keeps, yeah, every time I fire Chrome up, just like if I need to do something with it, it's like, hey, what the hell are you still doing on XP?



LEO:  Who are you?



STEVE:  It's like, ah, yah, yah.



LEO:  Is this Ted Kaczynski?  What is going on here?



STEVE:  So they said:  "...as the final end-of-life date for Firefox support on Windows" - now, notice it doesn't stop working.  So technically I could maybe get into June, I mean into July.  But anyway, no, I'll move - "on Windows XP and Vista.  As one of the few browsers that continues to support Windows XP and Vista [little trumpeting sound], Firefox users on these platforms can expect security updates until that date. Users do not need to take additional action to receive those updates."



So, okay.  That's good for me.  I will be, over the next few months, I'll be establishing a remote outpost where I will need to be doing work.  And so that'll be good for me.  I haven't needed to really take Win7 seriously yet, but that will cause me to do so more deeply than I have.  So I think I'll be kind of okay by the time I have to move.  And I've got this box sitting over here that's just like a dream machine that I haven't had to use yet because that's my Win7 box.  So it'll be good to move.



LEO:  You're going to like Windows 7.



STEVE:  Oh, I do.  No, I have it everywhere else, just not my main system.  Because Mark Thompson scared me when he made the move because all kinds of stuff broke, because both of us have a lot of 16-bit code, and Windows 7 formally abandoned support for 16-bit code, which is like, oh, crap, I mean, like Brief, my editor is, I'm sure.  So I'll have to make the change.



LEO:  Somebody's saying the portable Firefox will continue to run.  But there's no reason to think it won't run.  It's just it won't get patches.



STEVE:  Oh, yeah.  Precisely.  Precisely.  And, okay, the Sage Security Thought for the Day.  It's sort of a play on the famous Arthur C. Clarke quote.  Arthur C. Clarke is famous for having said:  "Any sufficiently advanced technology is indistinguishable from magic."  Love that.  Matthew Green, our cryptography friend, said:  "Sufficiently advanced incompetence is indistinguishable from malice."



LEO:  Very nice corollary.  We're going to call that "Green's Corollary."



STEVE:  Yes.  "Sufficiently advanced incompetence is indistinguishable from malice."  Which is actually an important lesson because - and we often encounter that and phrase it less eloquently than Matthew just did, where, okay, it's not clear that this is evil.  It just could be dumb, like really dumb.  So, yeah.  "Sufficiently advanced incompetence is indistinguishable from malice."



LEO:  Love it.  And we see a lot of it, god knows.



STEVE:  So I wanted to give an update after panning "The Orville" after not being able to tolerate more than about five minutes of it.



LEO:  Seth McFarland's kind of "Star Trek" homage, a little bit; right? 



STEVE:  Yes.  And I put it back in TiVo, and I missed the first five, I think.  But I may go back and see.  Anyway, I just wanted to follow up.  Dan Edwards wrote:  "First episode of 'The Orville' was a train wreck.  Episode 2 is much better, and 3 and 4 are fantastic.  Very Roddenberry-esque."



LEO:  Which you can't say for "Discovery"; right?



STEVE:  No.  Al Spaulding said:  "'The Orville' had eye-rolls at first.  I pushed through, and Episode 3 turned out to be good, with a Roddenberry-worthy twist at the end."  And finally, Adam van Kuik said:  "You said 'The Orville' wasn't your cup o' tea after viewing a few minutes of the first episode.  You may start to enjoy it from Episode 2 on."  He said, "I find 'The Orville' more drama than comedy.  I think Episodes 3 through 5 would hold your interest."  So I just wanted to say, for the record, thank you to our listeners.  I will give it another try because, I mean, if it's gotten more serious and not just so goofy, then, yeah, I think that sounds like it could be worthwhile.



LEO:  All right.  I haven't watched it, but I will.  Should I just skip the first episodes and go right to three?



STEVE:  Sounds like you should.  Sounds like you're not missing anything.  And just save yourself some pain because, oh, boy.  I mean, even these guys agree, ouch, number one was just like, okay, wrong.  But, boy, if it's Roddenberry-esque, that says a lot.



LEO:  [Crosstalk] in the chatroom says it's amazing.  More Star Trek than any of the modern renditions.  And of course it doesn't take itself too seriously.



STEVE:  Wow.  Sounds worth watching.  So, okay.  We've often covered through the years my interest in energy storage, you know, supercapacitors, batteries in the lab, all kinds of stuff that have never happened.  And so I just wanted to touch on, just sort of for the sake of not not saying anything about it, some researchers at Rice University who have discovered, believe it or not, that adding a bit of asphalt to existing lithium metal batteries can allow them to charge between 10 and 20 times faster than our current lithium ion batteries.  The lead researcher was a guy named James Tour, who was quoted in the press coverage saying:  "The capacity of these batteries is enormous, but what is really remarkable is that we can bring them from zero charge to full charge in five minutes, rather than the typical two hours or more needed with other batteries."



And he revealed another significant benefit:  "The asphalt additive mitigated the formation of lithium dendrites," which we've talked about.  That's the cause of these explosions because what happens is, when those dendrites, which are lithium and thus conductive metal, when they end up creeping through the electrolyte to bridge the two electrodes, that's when you get fire and explosions and so forth.  So the addition of the asphalt mitigates that.



Anyway, so I just - who knows whether this will ever happen.  Like so much of the other power storage stuff that we've touched on here over the years, it's still stuck in the lab.  It may never see the light of day.  On the other hand, maybe it's almost better if it doesn't because we know, if we end up with asphalt in our batteries, we're going to have to be putting up stories saying things like "Paving the Way to a Better Battery."  So we'll see whether that happens or not.



While I was going through all of my Twitter backlog, I ran across a couple mentions of SpinRite that I wanted to share and thank our listeners.  Someone named Floris apparently used SpinRite to really help himself.  He said:  "I dug out my very, very old executable" of SpinRite.  He said "@SGgrc executable."  I'm sure he meant SpinRite - "to try to fix a broken drive. Hooked it to an old WinXP [yay] machine.  After only four hours of constant rattling noises, it made massive progress, finding tens of gigs and tens of thousands of pictures."



And in his tweet he then posted something that looks like macOS.  I guess that's what it is.  I mean, based on the three different shaded buttons in the upper left that are close, minimize, and maximize, showing it compressing 3,200 items into Pictures-2017, 5,398 into Pictures-2016, and so forth.  So tens of thousands of photos that he says SpinRite helped to recover from that drive.  Also Tyson, I don't know how to pronounce his last name, Clugy, C-L-U-G-Y.  He tweeted:  "PC locked up..."



LEO:  Hope it's not kludgey.



STEVE:  I hope not, yes.  But that's the handle that he chose for himself, so...



LEO:  It could be, yeah.



STEVE:  Yeah.  He said:  "PC locked up several times this week.  Wouldn't boot today.  Less than an hour with SpinRite, and it's made a full recovery.  Thanks."  So Tyson, whatever your last name is, thank you for sharing.



And finally, NeutronJon, who I don't think is the name his parents gave him.  Maybe.  What should we call our son?  How about Neutron?



LEO:  NeutronJon, I like it.



STEVE:  That sort of sounds - but wasn't that the name of a dog on "The Jetsons"?  Was that Neutron?



LEO:  Well, there was "Jimmy Neutron."  That was a cartoon show.



STEVE:  Johnny Neutron.  Yeah, okay.  So he said:  "Just bought SpinRite and wanted to give a shout-out to a tutorial that helped me to use it on a Mac."  The tutorial's titled "Running SpinRite 6.0 on MacOS."  Link in the show notes.  And looks like it's very recent:  KevinStreet.co.uk.  Yeah, you found it, Leo.  And it's dated 9/11, so September 11 just a month ago, running SpinRite 6.0 on MacOS.  "It's got very nice walk-through instructions."  And Kevin in his blog posting says, you know, I found various things on the 'Net about how to run SpinRite on a Mac.  So he pulled it all together and packaged it nicely.



LEO:  This is great.  This is great.



STEVE:  Yeah, very nice walkthrough.



LEO:  And you approve.  This is Gibson-approved.



STEVE:  Yes.  In fact, he found something called PlayOnMac.



LEO:  Yeah, I use that, yeah.



STEVE:  Yes.  And that's what I've been looking for for SQRL because we also want SQRL to be able to run on a Mac until someone writes a native SQRL client.  So I had asked the SQRL gang earlier, has anyone packaged the WINE subsystem that would allow a Windows app to be used easily by a Mac user?  And so PlayOnMac is exactly that.  So when I ran across...



LEO:  And it's free, and that's nice.



STEVE:  Yes, it's free and, exactly, no Windows license required and so forth.



LEO:  It's just like WINE.  It's an API.



STEVE:  Correct.  Well, it is WINE.



LEO:  Oh, it's WINE.



STEVE:  It is a packed, yeah, it's sort of a packaged version of all the WINE work, which itself is just amazing.  And I am verifying that SQRL, the Windows SQRL client runs under WINE.  We've got guys using it under Linux now.  But for Windows users, being who they are, we just sort of need to make it easier to use on a Mac.  So it looks like PlayOnMac will be able to do that.  So NeutronJon, thank you for bringing that to our attention.



And of course it probably won't be 6.1 because I'm not going to delay its release at all.  The instant I get it running at high speed and direct to the hardware using the AHCI hardware, I'm going to push that out.  And so that'll be the first of a series of updates.  But getting it to run on the Mac, that'll probably be 6.2, so it runs natively on the Mac.  And then I'm thinking 6.3 will be adding native USB hardware support in order to bring USB attached drives up to the same maximum performance.  So anyway, subject to change, but the goal is to essentially apologize for this having taken me so long to get SQRL finished by accelerating the release schedule for SpinRite and not holding it back while I put more things in because I can just always do more of them.



And we have a couple of interesting closing-the-loop bits, including what we talked about and teased at the beginning of the show.  Josh Freeman asked:  "Been listening for years, but what are some of the key great episodes from the early years of Security Now! that I should listen to?"  And I would argue that it's our - we started it with Episode 233 titled "Let's Design a Computer."



LEO:  Yup, same one.  Yup.



STEVE:  Yup.  I thought so.  And what we did over the course of, let's see, one, two, three, four, five, six, seven, eight, nine episodes was we went from first principles and starting, all the way through.  So 233 was "Let's Design a Computer."  In fact, my description from back then was:  "To understand the advances made during 50 years of computer evolution, we need to understand computers 50 years ago.  In this first installment of a new Security Now! series, we design a 50-year-old computer.  In future weeks, we will trace the factors that shaped their design during the four decades that followed."



And so 235 introduces "Machine Language."  237, "Indirection:  The Power of Pointers."  239, "Stacks, Registers & Recursion."  241, "Hardware Interrupts."  247, "The Multiverse - Multithreading, Multiprocessing, Multitasking, Multicores."  250, "Operating Systems."  252, "RISCy Business," R-I-S-C-Y, where we talk about RISC processors.  And then, finally, 254, "What We'll Do for Speed," where we talk about the insane things that our chip manufacturers have done in order to improve the performance of their machines, as users kept pushing for more.  So that, I think, is a reference set of really sort of a classic walkthrough of the evolution of computers over time, which I would recommend, both to Josh and anybody else who, against all reason, is looking for additional content.



LEO:  And the reason it's alternating numbers is, back then, back in the day, every other show would be a Q&A show.  So we couldn't do it continuously.  We don't do that anymore.  They're all kind of...



STEVE:  Well, there's just so much news, Leo.  I don't know how we could, like, skip - how could we skip a week?



LEO:  Those were better times, simpler times, Steve.  This is seven years ago now.  But, I mean, this was a great series that you did, really, really useful.  And, by the way, because it's seven years ago, it's audio only.  We didn't do video at the time.



STEVE:  Well, thank goodness.



LEO:  Yeah.  You don't like the video.  Never did, I know.  And you were right.



STEVE:  Makes me shave every week, so...



LEO:  Yeah, that's a good thing.



STEVE:  Well, and Lorrie appreciates that [crosstalk].



LEO:  And bathe once a week, it's very important.



STEVE:  Yeah.  So Paradigm Concepts said:  "Steve, have you had a chance to review Bitdefender BOX?  It describes itself as a total home protection device for unlimited IoT devices and lets you roam using your home connection through VPN to your home." And so I have looked at it, and I've talked about it briefly, but I'll just remind people because for some reason it must have been in the news or came to people's attention.



So of course BitDefender is a well-known longstanding antimalware software solution.  BitDefender BOX is a hardware system.  It's $99 at the moment, but I think it also involves a subscription, so it's a service-oriented solution.  So I'm always a little skeptical about these turnkey things that you stick in your home, and they always make claims which are difficult to support because, as we know, as more things go to HTTPS and TLS encryption, it's not possible for a drop-it-in turnkey box to see into those connections unless they were to do a man in the middle.



Now, BitDefender has the advantage of being able to deploy clients on all of your devices.  So that's where it gets an advantage over any of the other "drop it in and we protect everything," meaning that you can have a BitDefender client running on your iOS devices, your phone and your pads; on your Android devices; on your Mac and your PC and your Linux machines.  So that gives them coverage that a box wouldn't otherwise have.  But there is no solution for arbitrary webcams and baby monitors and security systems, you know, the IoT devices, where they are not able to install a client.  They're making broad IoT-inclusive claims.



Now, what they can do is look at where the connections are going and what URLs are being looked up, and they do that.  So they have a cloud-based bad URLs directory.  So their box could flag worrisome remote URLs and domains and blacklist them in order to prevent, not only your regular clients, but also all the things you got in your internal network from accessing worrisome places.  So I think in general, with the slight caveat that they're wanting to protect IoT, but I don't see how they can offer the same level of protection, at least, with IoT as they do with all of our other endpoints.  Most of the problems that people want protection from are the things they're interacting with more, like their iPhone, their iPad, and their desktop machines and laptops.  And if you install the companion BitDefender client that goes with the box, then you get that, too.



So, yes.  If you are a person who wants more of a turnkey solution, you don't mind paying some sort of a subscription fee, and you've got an environment where you'd like - I would argue maybe the best protection you can get, though you do have to pay for it, this looks like a good solution.



LEO:  I'll have to try it.



STEVE:  Yeah.  Simon Zerafa, who frequently keeps me informed of things I would otherwise miss, noted that Netgear had a lot of updates.  And I went to Netgear.com/about/security because I was curious, and wow, yes.  Just, I mean, like a raft of updates.  So I know that Netgear is popular with a lot of our listeners.  I wanted to just point people at Netgear.com/about/security for anyone who wanted to check to see if these recent things were affected.  Maybe, if you can go to your Netgear devices management page and just say "check for update," it's worth doing so.  They normally can't be proactive because there's no way for them to put that news in your face, unless maybe they send you email.



So post a big wave of updates, it makes sense to just go, if you have a Netgear device on the front line, like a Netgear router with combined WiFi, for example, now would be a good time to go check, see if there's an update because there might be, and it could be important, especially if the device is there Internet-facing, because we know what the challenge is of keeping those protected.  We're talking about it all the time.



Kyle Hardin said:  "Excellent show.  What do you think of RAID 10 as an alternative to RAID 6 regarding drive failure during rebuild?"  We were talking last week about - in fact, this was our SpinRite story last week, where somebody had RAID 5, so they had a single drive of redundancy, a drive died, which was okay because the whole system was still running, able to recreate the lost data from all of the other ones.  But during the attempted rebuild - he put a new drive in, and it was rebuilding the RAID - one of the surviving drives then died.  Now he was in trouble, potential full loss.  Fortunately, he was able to run SpinRite on that one and bring that one back to life, then reconstruct the RAID and get another full drive of redundancy once again.



That led me to talk about RAID 6, where you get two drives of redundancy and, due to the fact that RAID arrays are now so massive and that the rebuild process can be problematical, the industry has begun to move to RAID 6, which offers two drives of redundancy.  And, for example, that's what I use at GRC.  All of the GRC servers are RAID 6, just because suspenders and belt.



Okay.  RAID 10, which is actually sort of a - it's a RAID, think of it as RAID 1 and RAID 0.  As we know, RAID 0 is striping, where essentially you take two drives, and you see them as one, which is the sum of their sizes.  So the idea being you can get a performance benefit by pulling from both drives at once in order to maybe get double the speed, depending upon the controller and bandwidth and things.  The danger with RAID 0, which gives you, if you have two identical drives, the appearance of one drive of twice the size, is that if either of them now dies, you're in trouble.  That is, you've increased the probability of a failure.  So by comparison RAID 1 is mirroring, where you don't get any drive size increase, but you get 100% redundancy.  So that, if either one fails, the other one is a mirror image, or they are mirror images of each other, that is, they contain a complete duplicate of the data, so you're protected against failure.



So RAID one oh is really the way to think of RAID 10.  RAID one oh is both of those things.  You take four drives.  You mirror them, and you RAID 0 them, that is, you stripe across each pair and then mirror the pairs.  The advantage of that is that there's very low computational overhead.  That is, since you're just writing the same data out, you're able to do that very quickly.  And when you're reading it, you're able to do it very quickly.  Any of the RAID 5 and RAID 6, there's a computational overhead because you're having to do fancy exclusive ORing functions in order to maintain the less than 100% redundancy which RAID 1 gives you.  The more drives in RAID 5 or RAID 6 that you have, the more efficient it is because you only have one drive redundancy over however many total drives.  In RAID 1, you've got essentially 50% data loss because you're mirroring the two drives.



So anyway, with that background, the benefit is performance.  The problem is you're still vulnerable to some failures.  RAID 6 can tolerate two failures.  RAID 10, well, it can tolerate some two-drive failures, but it can't tolerate two drives which are half of the mirror.  So if you sort of think of it as a square, drives mirrored across and then striped down, if the right two drives fail, you're okay.  If the wrong two fail, you're hosed.  So it's sort of a poor man's higher performance, yet still kind of risky RAID 6.  If you can, just working with RAID 6, I think, is better.  Or using 5, but be sure to monitor it.  You absolutely need to monitor your RAID because you want to detect the first moment there's a problem.  And in general what we're learning overall is that monitoring is a good and important thing to be doing. 



Two people asked about NAS.  VipX1 asked:  "What's the best Synology NAS for regular home," he said, "OS image backups using Acronis?"  And then somebody else, SgtWilko said:  "Hiya.  I know you've mentioned it on Security Now!, but I can't find it.  Which NAS do you use and recommend?"



Well, so, first of all we should mention that Drobo is a sponsor of the TWiT network, and I love it.  That's what I've got here, and I know you do, too, Leo.  



LEO:  Yeah.



STEVE:  I had an experience that I'll share at GRC's Level 3 service.  I played around with FreeNAS on Unix, thinking that it was like some magic, amazing thing.  And I soon realized that it was just a thin little coating on top of just standard Unix which already supports all of these various Network Attached Storage protocols.  I mean, you have NFS and Samba.  There are DLNA servers for Unix.  And what I really wanted was ZFS.  I mean, I am so impressed with ZFS that it's what I'm standardizing on moving forward for my Unix-based machines.  And so my feeling is taking any machine - I mean, so I guess I would separate by user type.



Certainly Drobo gives you a sponsor of the network, gives you a simple-to-use, no-muss-no-fuss, drop-in solution with features that nobody else had.  Not even ZFS allows you to arbitrarily change the size of one of the drives just on a whim, and the system sort of says, oh, look, we have more space.  And then it, like, reallocates it with this whole amazing RAID-based system that the Drobo sports.  But if you're more techie, and you like the idea of taking your own retired PC and bringing up a solution, I would look at whatever Unix system you like, maybe it's Linux, for me it's FreeBSD, and just setting up a ZFS system, and just rolling your own, basically bringing up the various protocols that you need for your network.  And you end up with something that is all yours, that you understand, that you built, and thanks to ZFS, which just nails redundant file storage in a highly scalable way.  Although it needs a lot of RAM.  That's the glitch I hit as I was learning about it.  This particular system I was trying to bring it up on was an x86 Xeon.  And while it had a lot of horsepower, it was limited to 4GB.



LEO:  Well, that's not nearly enough.



STEVE:  You really do need - you need a lot of RAM.



LEO:  Yeah, this is a 32GB machine that I built for my FreeBSD, and that's kind of minimal.



STEVE:  Nice, nice.



LEO:  Yeah.  I mean, this is nontrivial.  That's the other side of it.  



STEVE:  Correct.



LEO:  Not to give a plug to Drobo or even something like Synology, which I really love.  It uses Btrfs, which is also a copy-on-write file system.  But ZFS is the king of the hill here.  This is what Apple is supposedly offering now with APFS.  It's got snapshots, it's got, I mean, I feel like a modern file system...



STEVE:  A state-of-the-art OS, yes.  I mean a state-of-the art file system.



LEO:  File system, yeah.  So one hopes that we'll see some improvements across the board.  I'd love to see ZFS on Linux.  It's really kind of not ready for prime time, in my experience.  It's really a FreeBSD thing, yeah.



STEVE:  Interesting, yeah.  So, and finally, David Lemire, he said:  "Regarding huge root trust stores, any practical way to clear out, then add back individually what's actually needed by the user?"  And that's a great question.  If I weren't way overcommitted with finishing SQRL and then getting immediately back to work on SpinRite, I could solve that problem, or someone else could.  The key is that the initial handshake of TLS cannot by definition be encrypted.  So it's possible to passively monitor outside of the system, for example a router could do it, or something attached to the router like a Raspberry Pi could passively monitor all the traffic on your network and compile a list of the signers of the certificates that you're actually using.



So you could, for example, set that up and watch for a year, for example.  Because, I mean, we're not - CAs and the trusted root store, they're not going away anytime soon.  So you could watch for a long time, until you'd notice that, oh, look, it had gone sort of exponential, that is, you hadn't added any new ones for the last three months.  Then you could say, okay, I now have a snapshot of the roots all of our systems in the network actually use, and remove all the others.  In which case you'd probably be able to function with a minimal trust.  And especially if you went to a site that you knew you had visited before and got a warning that this certificate was not trusted.  Be like, whoa, wait a minute.  I was there yesterday.  That would immediately clue you to the fact that, I mean, to the possibility that you had received an illegitimate certificate somehow from what should be a valid domain.



So the only way I can see to do this practically would be to audit for a while, which is feasible to do monitoring the network because, as I said, that initial certificate handshake is in the clear.  It isn't until after that happens that you then bring up the encrypted tunnel between the endpoints.  So someone passively observing could build a list of all of the certificates and their signers and then move that into the trust store, pulling all the other ones out.



You'd still want to have, I mean, it's certainly the case that you could occasionally encounter a problem.  But you could deal with that on an as-happens basis.  And if you ended up getting surprised, it would be a good way of noting that, wait a minute, why is this site suddenly asking for a root cert that it didn't ask for yesterday or a few days ago?  So that would bring that to your attention.  Anyway, I don't know if such a thing exists.  If it does, I'm sure our listeners will let me know, and I'll share it on the podcast.  So great question.  And it would be handy to have.



Okay.  So why, 20 years after RFC 2065, which was titled "Domain Name System SECurity Extensions," which, by the way, is what DNSSEC stands for, DNS SECurity, why don't we have it today?  The goals even back then were well understood.  Twenty years ago, in 1997, the abstract for that RFC starts by saying:



"The Domain Name System has become a critical operational part of the Internet" - yeah, no kidding - "yet it has no strong security mechanisms to assure data integrity or authentication.  Extensions to the DNS are described" - it's funny, too, because every time I see "the DNS," is it "the DNS" or "DNS"?  Well, technically, since DNS stands for Domain Name System, it's "the Domain Name System."  So, okay.  "Extensions to the DNS are described" - that is, described herein, they meant - "that provide these services to security-aware resolvers or applications through the use of cryptographic digital signatures."  In other words, signing something in DNS.  And we'll be talking about exactly what in a second.



They write:  "These digital signatures are included in secured zones as resource records.  Security can be provided even through non-security-aware DNS servers in many cases.  The extensions" - which they proposed in 1997 - "also provide," they write, "for the storage of authenticated public keys in the DNS.  This storage of keys can support general public key distribution service as well as DNS security."  Of course, we've talked about that, too, and I will in a second.



"The stored keys enable security-aware resolvers to learn the authenticating key of zones in addition to those for which they are initially configured."  Meaning they're able to acquire new information through the system and know that that's also authenticated and has not been modified.  They finished, saying:  "Keys associated with DNS names can be retrieved to support other protocols. Provision is made for a variety of key types and algorithms."



So, okay.  So that was RFC 2065.  Today, 20 years later, we have a set of three:  4033, 34, and 35.  Which obsolete not only 2065, the original one, but 2535, 3008, 3090, 3445, 3655, 3658, 3755, 3757, and 3845, while updating 1034, 1035, 2136, 2181, 2308, 3225, 3007, 3597, and 3226.  In other words, this has been a hard problem to solve.  And they didn't even come close to getting it right out of the box.  In fact, it was so badly wrongly done the first time that the original design was completely scrapped.  It required, I think I remember six back-and-forth messaging transactions which, when they started thinking about how to deploy this and get it working at scale, they realized, well, it won't.  It would not scale to the size of the Internet.  So it was completely scrapped, and work was begun again.



For a while, the IETF was calling it DNSSEC-bis to sort of differentiate it from what was originally created, and finally just ended up replacing that wholesale among all of those RFCs which had been scrapped and replaced and obsoleted and amended over the last 20 years.  And we've talked about this promise, for example, of DANE, D-A-N-E, as a maybe someday possible future replacement for the existing certificate hierarchy that we're using currently, where we have all these many, many hundreds of trusted certificate authorities whose signatures we trust on anything that they sign.



The promise of DANE, which stands for DNS-based Authentication of Named Entities, DANE, is that, if this were to happen, a website like, for example, GRC could itself securely publish its server's public key.  That's right now what the certificate is that GRC sends to a client, and which has been signed by someone that the client trusts so that it can trust the certificate.  Well, if we had a secure way of distributing those public keys through DNS, which we don't today, even 20 years later, then the domain itself could say, here's the public key of our web server, so that clients that were able to get that through DNSSEC, that is, a secure DNS channel, would be able to say, oh, good, now I've got the public key directly from the source, rather than through a level of indirection through the certificate hierarchy system that we have today.



And I've often waxed on about just what it would mean to have a truly secure global directory system, where using a DNS name-based system, it would be possible to distribute more than IP addresses, more than references to email, more than DKIM signatures.  I mean, do something really secure.  It's a foundation we could build a lot on top of.



So what's the problem?  Why 20 years?  Why is this so hard?  The problem is that DNS itself, standard DNS, that is, the original DNS that, frankly, we're still using today, but increasingly less so, that is, as DNSSEC is beginning to happen it was incredibly lean and elegantly designed, very lightweight.  You know, UDP was the protocol.  So you just sent a query, and you got a response.  Just it couldn't have been simpler.  They came up with clever ways of compressing the data so the packets were small.  Most of the queries and responses fit in one packet so you don't have to set up a communications protocol in order to deal with out-of-order packets and the sort of stuff that TCP does.



The problem is that doing DNS with no security is elegant and bordering on trivial.  It's just so simple.  But adding security is complicated.  It requires a huge addition, not just adding a couple things, a massive addition to the existing simple and lightweight DNS system.  And a perfect analogy is TCP, where TCP allows you, the Transmission Control Protocol, to establish a communication between endpoints over the Internet, which protects you against packets arriving out of order and packets being lost and momentary hiccups.  It handles all that work for us.  But it was never in the beginning secure.  Security was an afterthought for the Internet.



So a perfect example is then we said, oh, well, we like TCP.  But, boy, we'd sure like to know who we're talking to and have what we're saying private.  So of course SSL was the way we said, okay, let's add security to TCP.  Well, that was SSL, and we had all these versions of it, and all the mistakes made, and all the fixes over time, and now TLS.  And we're still working on getting it right.  So that sort of puts into perspective how simple it is to do something that just works without a concern for security, and how surprisingly difficult it is, how many problems arise when you decide you want to add security.  And it's not just that it's the addition of security.  That is, it wouldn't have been any easier back in the beginning, except we would have had it from the start.



LEO:  I once asked Vint Cerf if he had any regrets designing TCP/IP and the Internet protocols, anything he would have done differently.  And that's exactly what he said.  He said:  "I would have put in crypto."  I mean, they didn't feel like it was necessary.  It was just universities talking with one another.



STEVE:  Yes, well, back then it was a miracle that it worked at all.



LEO:  That it worked at all, yes. 



STEVE:  It's like, wow.



LEO:  They probably couldn't have technically added reasonable crypto just because there wasn't the horsepower to do it.



STEVE:  Correct.  Back then we didn't have the processing power.  I mean, and so the advantage to it being there from the beginning would have only been that we wouldn't have the adoption inertia.  And that's really what we're seeing.



LEO:  They could have put the hooks in without implementing it, something like that.



STEVE:  Yes.  And what was interesting is that in reading a little more into this in order to talk about it for the podcast, I ran across several notes that noted that Dan Kaminsky's discovery of the problems with DNS, which we of course covered back at the time, I think it was 2008, really gave a good kick in the butt to DNSSEC, in the same way that Snowden's revelations about the NSA and how much true wiretapping was going on suddenly made us get a lot more serious about HTTPS and encryption.  Similarly, I mean, it sort of took realizing how bad things were for the community to say, oh, okay, maybe we've got to dust this off and think about it again.



Okay.  So the thing that put this on my radar was that October 11th - today is the 10th.  We're recording this on October 10th, 2017.  On October 11th, tomorrow, was to be the ICANN, what's called the KSK Rollover, the Key Signing Key Rollover.  We are currently using keys from 2010.  The plan was to roll those over to 2017 keys, just because it's a good thing to do.  It allows us to have keys that are not so dated.  And so what happened was they said, uh, we're not ready.  So ICANN's announcement stated that the KSK, that's the Key Signing Key Rollover is being delayed.



They said:  "...because some recently obtained data shows that a significant number of resolvers" - meaning DNS resolvers - "used by ISPs and Network Operators are not yet ready for the Key Rollover.  The availability," they wrote, "of this new data is due to a very recent DNS protocol feature" - see, they're still messing with this, it hasn't even settled down yet - "a very recent DNS protocol feature that adds the ability for a DNS resolver to report back to the root servers which keys it has configured."



So, translation:  After 20 years, DNSSEC is still being changed, messed around with, and not yet settled.  They're adding things.  And in this case they added a feature which a few servers adopted, which then began reporting back about their keys.  And although the data was incomplete and far from comprehensive, an analysis of it spooked them because what they realized, the one thing they cannot do, I mean, DNS has been challenged enough as it is.  The one thing they cannot do is break anything because that would be a setback that, if avoidable, has to be avoided.  So there is an annual meeting of the so-called DNS Operations Analysis & Research Center, O-A-R-C, known as DNS-OARC.  It's one of the DNS organizational bodies.



And Friday before last, I think it was September 29th, during this DNS-OARC annual general meeting, an engineer from VeriSign, Duane Wessels, presented the result of VeriSign's passive monitoring of DNS and DNSSEC in his talk, which was titled "A Look at RFC 8145," which is yet another RFC out in the 8,000s, which is Trust Anchor Signaling for the 2017 KSK Rollover.  In other words, that's this new protocol which had just recently been added that allowed there to be some signaling of what was going on down from the top-level domains, down from the root servers, essentially.  And the short version of his talk was that we're not ready.  He said:  "This RFC describes how recursive nameservers can signal up to authoritative servers the trust anchors that they have configured for DNSSEC validation."  That is, are they configured?



Shortly after its publication - there is a very popular DNS server called Unbound, which is sort of a play on the previous Unix standard server, which was BIND.  So we have BIND and Unbound.  So they said:  "Shortly after its publication, both Unbound and BIND implemented the specification.  As organizations begin to deploy these new software versions, some of this 'key tag data' is now appearing in queries to the root name servers."  In other words, Unbound and BIND added this RFC 8145 Trust Anchor Signaling.  Then, as those versions of Unbound and BIND started to sort of percolate out into the world, as sort of a side effect of their support for RFC 8145, they began sending this data back to the roots.  And that's what allowed VeriSign to look at the data coming in and saying, oh, we'd better not do this.  The world is not ready.



So they wrote:  "This is useful data for Key Signing Key rollovers" - that is, this KSK rollover - "and especially for the root.  Since the feature is very new, the number of recursive name servers providing data is not as significant as one might like for the upcoming root KSK rollover.  Even so, it will be interesting to look at the data."  And so he does so.  And essentially, in I think it was a 27-slide presentation, he demonstrates like where we stand.  And based on this, ICANN soberly decided to kill the long planned rollover from the 2010 Key Signing Keys to new 2017 Key Signing Keys because the data demonstrated, though it was fragmentary, that we weren't ready.



Okay.  So where are we today, if we're not ready?  We're actually making pretty good progress.  There's a map here in the show notes, Leo, that might be interesting if you wanted to put it up, very colorful, showing from red to green, from 0% to 100%, of the domains which are signed.  So we have the top-level domains, things like .cc, .ru, .com, .gov, .edu, basically the far right-hand side:  89% of the top-level domain zones are signed.  So that's very good, 89% of the top levels; and just shy of half, 47% of the country code top-level domains, like .it for Italy, .ly for Libya and so forth.  So half of those are signed.  Not nearly as good as the main top levels, but still pretty good.



Second-level domains are far lesser signed.  For example, GRC is not yet signed.  It's, like, on my, yeah, I'll get around to that after I stop using Windows XP.  But, for example, 2.5 million .nl, that is, The Netherlands, are signed.  That's about a little shy of half of theirs.  Eighty-eight percent of .gov are signed, which is nice.  It's like, you know, the government is trying to set an example by signing its.  Over 50% of the Czech Republic, .cz, are signed.  A quarter of .br, Brazil, are signed.  However, they note, while only 0.5%, okay, half of 1% of the zones in .com are signed, still there are so many .com domains that the half a percent is 600,000 zones.  So a good chunk of zones are signed.



The major DNS authoritative server software and libraries now support DNSSEC and have several years of development experience.  And as I was browsing around in more detail, basically all the different languages have libraries, Python and Perl and Java and .NET, and all of them now have libraries which offer DNSSEC as a feature of them.  And it's all public and open source.  So that's a good requirement.  Also management tools have started to come online to assist in the deployment.  I'm sure I'll take advantage of some of that when I get ready to do this.  I do need to make sure that I'll be able to leave subdomains unsigned because I have several that are dynamic, where the DNS is varying arbitrarily, and it's a pain to sign something that is completely dynamic.  But I'm sure that there's a way to set that up correctly.



Also encryption algorithms are maturing.  We were at shorter key lengths.  We're moving to longer key lengths.  We're also beginning to move to elliptic keys, which is a huge win.  As we know, the huge advantage of an elliptic key is that, because the problem that it represents is harder than factoring, we're able to use shorter keys to get the equivalent strength, as with RSA factoring-based problems.  And shorter is better for DNS.



One of the biggest problems is that, to get cryptographic security with RSA, which has been the primary algorithm used, the keys had to be big.  Well, but the actual DNS query is tiny.  So this is, again, one of the problems.  Suddenly the bandwidth was going to jump up.  The load on the servers was going to jump up.  DNS servers, as we've often discussed here, are notoriously overworked, and they get no respect, much like Rodney Dangerfield.  They're in a closet somewhere, and they never complain, so no one ever bothers them.  So anyway, it's just we're moving forward, but with a lot of inertia.



In terms of validation, one of the problems that still exists is that, to get end-to-end security, we need to also encrypt the "last mile," as it's called, from the ISP to the end-user.  So it's one thing for a DNS server that requests on behalf of its user some information and for that server to be able to verify the information that it got was correct.  But if it then sends it to the user unencrypted, unprotected, then that last mile could technically still be subverted.



So what we still need to do is to come up with a means of either encrypting that - and there are some non-DNSSEC approaches for doing that - or have the client itself verify DNS.  So that instead of just saying "Give me the IP for this," it says "Give me the records I need to verify the response that you're going to give me," and it performs that.  In other words, we need DNS validating resolvers in the clients.  And those are coming, too.  Those are a little bit further behind, but they're on the way.



So anyway, we're moving forward.  I think it's a function of it's a hard problem to solve.  It's had a deleterious and dramatic impact, and there's been this sense of, well, what we have now is working.  If it's not broken, don't fix it.  And what that just means is it's just taking time.  But nothing could happen - because this is a hierarchical system, it had to start at the top and then filter down.  The roots are all signed.  We're seeing that the top-level domains and the second-level domains are getting signed.  And we're at the point now where it's just my own inertia doesn't have GRC signed, but DNSSEC is finally beginning to happen, and where are we?  We're at Episode 632.  I think by 999 we'll probably be pretty well established.



LEO:  Because you're retiring at 1000, so...



STEVE:  I run out of digits, Leo.



LEO:  We'd better do it by - really, it's going to take that long?  Wow, 999.



STEVE:  Yeah.  I just think it's going to be - yes.  I think what we'll see is we'll see high security use places where it'll be available, and it'll be used where it really matters, very much like HTTPS.  We had HTTPS for a long time, whereas lots of blogging sites said, oh, I don't need that.  And it wasn't until Let's Encrypt made it absolutely simple - or, for example, WordPress said, yeah, we're just going to support it everywhere.  You know, bang.  So it'll start being used, but even then it won't be used pervasively.  That'll take more time.  But, yeah, it is clear that it's happening.  It's not that it's not happening.  I would argue it's happening more than IPv6 is.  And there's another example of something that's not in a hurry.



LEO:  Yeah, no kidding.  Well, I think it shows, unless there's something forcing it, it's just not - there's too much inertia.  It's just not going to happen.



STEVE:  Yes.



LEO:  Steve, did it again.  Lovely show.  If you want to get a copy of the show and share with your friends, well, first of all, if you're listening at home, you probably already have a copy.  But for future reference, Steve puts this show on his website, GRC.com.  That's where you'll find SpinRite, his daily bread, the world's best hard drive maintenance and recovery utility.  You'll also find information about SQRL and Perfect Paper Passwords and Password Haystacks and, oh, I mean, it just goes on and on and on.  All that stuff's free, GRC.com.



The podcast is there.  So is a transcript so you can read along as you listen.  A lot of people find that's a little easier to do if you read it and you understand it better, whatever way suits you.  We have audio and video, as well, on our website at TWiT.tv/sn.  You can download it there, or you can also subscribe from there.



And I encourage you to subscribe, whatever podcast appliance you use, a subscription means you'll have every episode automatically.  Our feeds only include the most recent 10, so you want to start now, start building your collection.  It's very important.  Overcast, Pocket Cast, very popular.  iTunes, of course.  Stitcher, Slacker.  And you can even listen on your Amazon Echo.  You just say, "Echo, listen to Security Now! on TuneIn."  TuneIn's our purveyor on the Echo.  Or watch TWiT Live, if you want to watch the live stream on TuneIn.  We do that every Tuesday, 1:30 Pacific, 4:30 Eastern, for the next couple of weeks 20:30 UTC.  But we are going to slide back when we fall back and go back to daylight - not daylight, standard time, Western Pacific Standard Time.



Steve, thanks once again for a fabulous episode, and we will see you next week.



STEVE:  We have no idea what next week will bring, but we will cover it, whatever it is.  And I really do thank our listeners for sending me things they find that are of interest.  It helps to pull all this together, and I just want to say thanks.



LEO:  I forget to mention that, and I should do that.  Since we don't do the Q&A anymore, I forget that you can go to GRC.com/feedback.  He's got a form there.  Or tweet him.  That's probably the best most instant way to do it, instantly gratifying.  It's just 140 characters, some of you have 280 characters now, @SGgrc.  And he does accept DMs as well as public tweets, @SGgrc.



STEVE:  Yes.  In fact, someone sent me, last night I ran across clearly a 280-character tweet.  I was so jealous. 



LEO:  I know.



STEVE:  It's like, oh, look at that.



LEO:  I know, I don't have it yet.



STEVE:  It's amazing what a difference it makes just to double.  I mean, it's like, okay, this is better.



LEO:  I wonder, though, how it's going to change Twitter.  I mean, the stream will now be a bunch of long posts.



STEVE:  True, true.



LEO:  I don't know, I don't know, I don't know.



STEVE:  Interesting to see.



LEO:  Yeah, yeah.  Thanks, Steve.  We'll see you next time.



STEVE:  Okay, buddy, thanks.  



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#633

DATE:		October 17, 2017

TITLE:		KRACKing WiFi

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-633.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine ROCA's easily factorable public keys, the surprising prevalence of web-based cryptocurrency mining, some interesting work in iOS password dialog spoofing, Google's Advanced Protection Program, and some good loopback comments from our listeners; then we take a close look at KRACK, the Key Reinstallation AttaCK against ALL unpatched WiFi systems.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  This is, boy, this is a banner week for security flaws.  Here we are Tuesday, and already there are two major flaws:  ROCA, which affects public key crypto in a very serious way; and, of course, you've probably all heard about the WPA2 KRACK - K-R-A-C-K.  Well, Steve's got reassuring news in both cases, and information on what you need to do to protect yourself, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 633, recorded Tuesday, October 17th, 2017:  KRACKing WiFi.



It's time for Security Now!, the show where we talk about your security and privacy.  And, man, everybody is just dying to hear what Steve has to say this week.  Hello, Steve Gibson.



STEVE GIBSON:  Thank god this did not happen on Wednesday.



LEO:  Yeah.  We would have had to do a special redo.



STEVE:  Or later today, yes.  I mean, it's wonderful when these things drop on Monday because then everyone's all revved up, and there's enough information for me to talk about.



LEO:  Yeah, if it had happened Tuesday that would have been bad, too, really.  Monday's the best day.



STEVE:  Yeah.  Please, if we could have the major security problems hit on Monday.  There was even a little bit of windup Sunday evening.  Sort of the news of something about to happen the next day began to leak out on Sunday.  And it's like, okay, this doesn't sound good.



LEO:  Unh-unh.



STEVE:  So consequently, Security Now! Episode 633 for October 17th, 2017:  KRACKing with a K, of course, WiFi, which will be our main topic.  But there was another problem, also yesterday.  Both of them happened.  And the other one we're going to talk about first; and we'll deal with the KRACK crack, or the KRACK attack, or the release of the KRACKen or whatever you want to say, at the end of the podcast.  But ROCA (R-O-C-A) is, I would argue, much worse.



LEO:  Interesting.



STEVE:  But hasn't gotten nearly as much attention because it's not as sexy.



LEO:  Right.



STEVE:  And doesn't have as good a name.  You've got to give these things a good name, remember, like Heartbleed.  Oh, my god.  So we're going to talk about ROCA, which is a massive industry-wide failure of an embedded cryptographic library in five years of Infineon's chips, which are everywhere, which has until now been unknowingly producing weak public keys that can be factored, which is the whole thing you don't want in a public key.  And they're everywhere, like BitLocker is not safe.



So this is, as I said, arguably a much bigger problem.  The KRACK attack is not good, but we'll explain all about that.  It definitely needs to be fixed, but it's not the end of the world.  We don't need WPA3 or anything else.  Also, I read an interesting piece by an adblocking company which started off talking about the surprising prevalence of web-based cryptocurrency mining, which we were talking about last week.  And remember I said, I think it was during our closing-the-loop loopback with our comments from our listeners, that it's sort of annoying that your computer is being commandeered while you're visiting one of these websites, but it's not a security risk.  They're just taking your processor time.



But these guys - and as I was reading this, I had a thought that hadn't occurred to me last week, and then they ended up presenting it.  And I thought, oh, well, okay, good.  So we'll talk about that.  A really interesting, worrisome look at iOS dialog password spoofing.  Also, Google has formally announced the Advanced Protection Program that we expected to come out and we talked about a couple weeks ago.  We've got some comments from our listeners.  Then we're going to take a deep dive into KRACK, K-R-A-C-K, which is an acronym for Key Reinstallation AttaCK - that's where they get the C-K from at the end of the attack - which is effective against all not-yet-patched systems, or those by Windows and iOS that never implemented it correctly in the first place and, as an interesting side effect, aren't subject to this main problem.



LEO:  Isn't that funny.  So that was what Rene was speculating is that maybe the reason that some of these devices aren't vulnerable is they don't do it the proper way.  Which is crazy.



STEVE:  Correct.  They screwed up.  And it's like, oh.  The attack doesn't work here.



LEO:  Turns out it was a good idea.



STEVE:  But even so, Windows did quietly fix this officially in last Tuesday's Second Tuesday of the Month update.  So they got it.  And we know that Apple has betas in the works, and we expect to have probably 11.0.4, or maybe it'll be 11.1.



LEO:  Yeah.



STEVE:  And of course we just hope they keep fixing iOS because, boy, it's still a catastrophe.



LEO:  Yeah.  It's 11.1, and I have 11.1.  So everybody who has a beta, public or developer beta, should be up to date and secure.  It's only devices.  And then, you know what, eero's doing the same thing.  I checked with our sponsor to see what they were going to do, and they're in beta.  So I guess the mitigation isn't hard.  But everybody reasonably wants to test it a little bit before they push it out to everybody.



STEVE:  Well, and what's interesting is that access points don't need to be updated.  That's the other thing everybody has missed.



LEO:  Oh.



STEVE:  So there's no need at all.  It's the clients that are the problem.



LEO:  Oh, okay.



STEVE:  Not the access points.



LEO:  Oh.



STEVE:  Yes.  But access points are still going to fix their protocol.  So this is one nice way of measuring how you feel about your access point provider.



LEO:  Yes, yes, yes.



STEVE:  I heard you mention, for example, that DD-WRT is already fixed.  It is.  And Ubuntu is, and a lot of them are.  But I'll explain why this is the case.  It's only if access points were connecting to each other because, in that case, one of them is a client of the other, and the attack is against the client, not against the access point.



LEO:  I did not understand that.  See, this is why we needed you.  Fantastic.



STEVE:  Well, that's what I'm happy to provide.  So I think we've got a great podcast, with lots of information.



LEO:  That is real, boy, that's - okay.  Man.  As always, Steve's the best.  And when things like this happen, invariably the first thing I hear is, okay, okay, Steve's going to talk about this; right?  Yes, he is.  And ROCA, too, which I'm happy to say all my keys are secure from ROCA because I checked them right away.  Okay, Steve.



STEVE:  So we have a fun cartoon for the week that is about this problem.  Unfortunately, it's incorrect, but it is fun nonetheless.



LEO:  Oh, dear.  Oh, dear.



STEVE:  So we've got two guys.  One's got a cup of coffee in his hand.  The other is eating a burger.  They're in an office.  And a third guy comes rushing in, swings the door open in a panic and says, "Oh, no.  They found a vulnerability in WPA2.  We can't trust any WiFi anymore.  We need to change every router, starting with ours."  And so the bearded guru who's sitting down with his burger says, "Calm down, guys.  We'll just need to update the firmware, and everything will be fine."  The guy who was initially panicked says, "Are you sure?"  And the bearded guy says, "Yes.  We just need to wait for the manufacturers to release a patch for all their router models, and then for all the sysadmins to flash all the new firmware."  And then the fourth frame shows three skeletons.



LEO:  They need spider webs.



STEVE:  Exactly.  And in fact in the initial three there was a small flower in the plant.  And in the final one there's, like, five flowers, and the vine has wandered off the desk because so much time has passed.  So the point being, uh-huh, and that'll never happen.  So the good news is, as we just said at the top, and I'll explain why, it doesn't matter.



So ROCA first.  So both of these problems, the KRACK with the "K" and ROCA, are subjects of the upcoming ACM Conference on Computer and Communications Security known as the CCS, the ACM CCS, which is the major annual ACM Conference of the special interest group on security, which this year promises to be a barnburner.  So not only will...



LEO:  Only you would call a security conference a "barnburner."  But I know what you mean.  I know what you mean.



STEVE:  That's right.  That's right.  No barns were harmed during the production of this podcast.  So not only will the presentation of the KRACK attack be there, which as I said we'll be talking about in detail at the end of the podcast; but so also will a, I would argue, even more unsettling discovery by a team of researchers in the Czech Republic, the U.K., and Italy from several universities.  ROCA, R-O-C-A, as in almond, stands for Return of Coppersmith's Attack.  



LEO:  Wow. 



STEVE:  Yes, R-O-C-A, Return of Coppersmith's Attack, a practical factorization of widely used RSA.  So, okay.  Coppersmith, Don Coppersmith is a well-known cryptographer with a long rsum of cryptographic accomplishments to his name.  He developed one of the many improvements of factorization, of factoring, which of course is a concern everybody has because we rely on the intractability of performing a prime factorization of two very large primes after being multiplied.  We're relying on the fact that it is computationally infeasible to demultiply them, to factor them, to break them back apart after the two large primes have been multiplied.



So it turns out that we're often talking about the importance of that problem, how crucial it is.  And in fact the worry of quantum computing is that there will be a way to apply the power of quantum computing to this problem, which is why cryptographers and academics are already talking about a next generation of crypto that will be quantum proof because the worry is factorization may not longer be quantum proof, once quantum computers develop enough strength.  So I'm sure in the future we'll be talking about lattice-based cryptography, which is already known to be quantum proof.



Okay.  So one of the things that we also often talk about is the distinction between theory and practice.  That is, it's important that you put into practice properly what the academics have figured out in theory because of course implementation is where the rubber hits the road, and that's what actually matters.  What these guys uncovered was a doozy of a mistake, which exists in the embedded cryptographic library used by Infineon's products.  Okay, now, I mean, Infineon is everywhere.  They're one of the major embedded crypto people.  Their stuff is in the Trusted Platform Modules, the TPM, especially v1.2, which is the one that's around that most of us have in all of our machines today.



LEO:  Oh, well, that's a very big deal.  I didn't realize that.



STEVE:  Yes.  Yes.  And BitLocker uses TPM for its key.  And it turns out that, if you have BitLocker running on a machine with TPM 1.2, it almost certainly has Infineon in it, and the public key it would have generated can be factored.



LEO:  Wow.  That's really bad news.



STEVE:  It's really bad news.  These guys scanned the Internet.  They found thousands of public keys that can be factored.  They looked through GitHub and found a ton of public keys on GitHub that were being used to protect things, but were not providing protection.  So the problem is huge.  This impacts one flavor of Yubico, and Yubico responded instantly.  They knew about this a while ago.  They've been safe, I think, since July.



The researchers discovered the problem in January of this year.  January of this year; right?  So 10 months ago.  They informed Infineon in February and negotiated an eight-month silent period, knowing that they'd be presenting this at the ACM CCS meeting at the end of October.  It's October 30th through the beginning of November, the first few days of November.  So Infineon's had time.  Word's gotten out quietly to those people who are being affected.  And so there's been time for remediation.  Yubico, if anyone is a Yubico customer and concerned, they've got - Yubico immediately put up a page to help identify which of their products might be affected.  Most of them aren't.  I think it's the YubiKey 4 when you use it to synthesize the public key.



LEO:  That's what I thought.  Okay, good.  Because that is a feature of the YubiKey 4.  You can generate private keys and keep your PGP key on there.



STEVE:  Correct.



LEO:  And I never did that because I don't - for one thing, you can't put it anywhere else.  It's on the key.



STEVE:  Right.



LEO:  Okay, good.  All right.



STEVE:  So it's only in the case that you do that.



LEO:  Only if you use it to generate [crosstalk].



STEVE:  So it's a relatively small attack surface in that case.  But they also found a ton of insecure PGP keys.  So, I mean...



LEO:  Really.



STEVE:  Oh, yeah.  I mean, the problem, I mean, Infineon is just everywhere.



LEO:  Yeah, see, I generate mine in software.  So I'm okay.



STEVE:  Yes, and so you're okay, yes.  So this got introduced in 2012, so for the last five years this has been a problem.  Okay.  So let's take a look at what this means.  First of all, I think, well, it's a mixed blessing.  It's good news, unfortunately, for both people with keys and bad guys who want to attack them, that any public key can be almost instantly, like as in a millisecond, so effectively instantly, tested as a candidate for easy factorability.  And there are online websites.  There are online and offline tools.  Everything is open source, MIT licensed so the code that has been produced can be put into other tools or products.  And so there are a lot of resources to allow people to check their own public keys to see yay or nay whether that key would be vulnerable to this essentially factorability short-circuit.



So, okay.  So to give us some context here, the worst cases for the factorization of 1024 or 2048-bit keys would be less than three CPU months for the 1024-bit, or 100 CPU years for the 2048-bit.  Okay.  So, first of all, that's interesting.  And this is another important lesson for us is that doubling the bit length of RSA didn't double the difficulty.  It went from, what was it, that would be 400 times harder, three CPU months, which is to say one quarter of a CPU year, compared to 100 CPU years.  So 400 times harder to factor a 2048-bit key.  And that's on a single-core recent CPU.  So nothing GPU, nothing fancy, no hardware assistance, just a standard CPU, just to give us some sense for that.



But that's the worst case.  The expected time is half that because you're doing a bunch of guessing, and you may just get lucky.  So generally about half of that is what's expected.  However, this factorization problem can be performed in parallel across multiple CPUs, allowing for practical factorization in hours or days.  So, okay.  And I should say this is the - if your key is weak, that is, if this test reveals that you have a weak key, this is how quick it can be done.  It should be vastly harder.



A properly generated 2048-bit RSA public key should require several quadrillion years.  So that's hundreds of thousands of times the age of the universe.  We throw these large numbers around casually, but when you're at hundreds of thousands of times the age of the universe, you probably are safe, as long as nobody made a mistake.  And the point was somebody did.  The 2048-bit keys that this broken library has,  which has been embedded into Infineon's hardware, can be broken in not several quadrillion years, but 100 simple CPU years.



LEO:  Oh, that's a little different.



STEVE:  Yeah.  Yeah, exactly.  So to put some cost on this, in the case of the broken public key, the worst-case price of factorization on an Amazon AWS C4 computation instance for the 1024-bit key would be $76.



LEO:  Wow.



STEVE:  Uh-huh.  And the 2048-bit key costs more.  That's 40,000.  But obviously well within the price, well within the budget of a corporation, or certainly a state actor who's got essentially infinite money; and half that would be, again, typical.  So about $20,000 worth of Amazon AWS C4 computation time, and you get to crack one of these weak keys.



LEO:  So a scenario would be maybe you had a competitor's laptop, and it was an encrypted hard drive using TPM, and on that laptop were presumably secrets of interest to you.  You could, for 40 grand, unencrypt it, basically.



STEVE:  Yes.



LEO:  Wow.



STEVE:  Yes, exactly, yes.  So for a reasonable price in a short time, way down from several quadrillion years.



LEO:  How much is that in Amazon compute - actually, really this may be more of a statement of how cheap it is to get compute time.



STEVE:  You know, come to think of it, it's free because you'd never have a chance to pay the bill. 



LEO:  The bill would never come.



STEVE:  It's like, no, we're still working.



LEO:  I'll let my great-grandchildren pay that bill.



STEVE:  Right.



LEO:  That's hysterical.



STEVE:  Okay.  So they disclosed it, as I mentioned, to Infineon in February, so everybody's been scrambling.



LEO:  Well, as an example, Yubico says every key we've sold since June is fixed.



STEVE:  Yes, exactly.



LEO:  So they've known for a while.



STEVE:  Yes, exactly.  So Microsoft, Google, HP, Lenovo, Fujitsu, they've all already released software updates.  On the other hand, a laptop that doesn't get those isn't fixed.  So very much as with the KRACK attack that we'll be wrapping up today with, this is an instance where you really do want to sort of ignore the advice of, which is typical for firmware, of if it's working fine, don't mess with it.  In this case, actually it only appears to be working fine.  So you definitely want to make sure, especially if you have TPM v1.2, because that's where the problem is.  Apparently, v1 didn't have this because it's older than this library, which only appeared in 2005.  I mean, sorry, 2012.



Okay.  So these guys, as I mentioned, scanned the 'Net, found thousands of keys, found PGP keys, found keys on GitHub.  So apparently people have been using this Infineon library for producing keys, just figuring, oh, you know, hardware's better than software.  Not in this case.  They also looked at 41 laptop models that used TPM modules.  They found vulnerable TPMs, Trusted Platform Modules, from Infineon in 10 of the 41.



And in their own disclosure they note that, they said:  "The vulnerability is especially acute for TPM version 1.2 because the keys it uses to control Microsoft's BitLocker hard-disk encryption can be factored.  So anyone who obtains a Windows machine with a BitLocker-encrypted drive on top of TPM 1.2 may not be secure."  So what that says is you'll want to fix your hardware and then rekey your BitLocker using an updated key from the Trusted Platform Module.  And I'm sure there will be remediation details available as soon as this all happens.



So both online and offline detection tools have been provided.  They're open source and, as I mentioned, released under the MIT license so they can be put into other solutions.  The best vulnerability test suite is keychest.net/roca, K-E-Y-C-H-E-S-T dot N-E-T /roca.



So the researchers wrote:  "Our work highlights the dangers of keeping the design secret and the implementation closed-source."  Oh, I forgot to mention that the way they found this was just by having some Infineon hardware generate a whole crapload of keys, of public keys, all presumably safe, and these guys studied the output and found the weakness.  It would have been so much easier if, as academic crypto researchers, they had been able to look at the code.  But it's closed.  And so they had to go through the extra work of essentially looking at the output and reverse-engineering, discovering and reverse-engineering the problem from the result.



So they said:  "Our work highlights the dangers of keeping the design secret and the implementation closed-source, even if both are thoroughly analyzed and certified by experts."  They wrote:  "The lack of public information causes a delay in the discovery of flaws and hinders the process of checking for them, thereby increasing the number of already deployed and affected devices at the time of detection."  So of course Infineon's not happy.  They have acted as responsibly as they can, but unfortunately this is firmware embedded into hardware, so getting to it is a little less easy than it is for OS updates and app updates and the sorts of things we're seeing all the time, and also probably requires more user interaction.  And we'll be talking, of course, again, about how KRACK relates to this.



Yubico's page is Yubico.com/keycheck, and I've got a link in the show notes to the paper where they describe all this.  But so basically there was, five years ago, a weak library or a flawed library which was supposed to be producing strong public keys, wasn't.  And because it's from a well-known good manufacturer, I mean, I'm sure Infineon is not happy about this, these keys are well used and prevalent.  So it is possible to check any, if you've got them, to quickly determine instantly whether it's vulnerable or not.  And not all of them that are produced by this library are vulnerable.  So it's worth checking, rather than immediately panicking.  And if you're able to rekey once this is fixed, or just rekey from a software-based source, as you did, Leo, then that makes a lot of sense, too.



LEO:  Yeah.  I actually was looking into - because I thought, it's time.  I read a really good article saying don't keep your private keys on the Internet, which I don't do, or even on an Internet-accessible device.  Like put them on a USB key and put it away.  And so I was thinking, another place to put it would be of course a YubiKey.  And then I noted that you could generate a PGP key on YubiKey, the YubiKey 4.  It has enough horsepower to do that.  But then you can't put it anywhere else. And if you lose that one, you're out of luck.  So what I decided to do is, yes, generate a new key, put it on the YubiKey, but generate it in software using open GPG, and then put it on the YubiKey and put it on a separate USB key in a drawer somewhere so that the private key isn't publicly available.  That's a little less convenient.  But if it's on the YubiKey...



STEVE:  I think you mean so that the public key is not publicly available.



LEO:  No, the public key has to be publicly available, or it wouldn't be very useful.  The private key is the one you use to authenticate the public key.  So that you want to keep very close to your chest.  You put the public key out.



STEVE:  And so that's the problem is it's the public key that gets factored.



LEO:  Oh, screw that.  But I'm going to make it with safe software.



STEVE:  Correct.



LEO:  Right, of course.  Yes, the public key is the weak link.



STEVE:  Right.



LEO:  Private key wouldn't be because nobody has it.



STEVE:  And that's why it's private.



LEO:  Right, exactly, yeah.  But, no, I want to keep my private key private, not because of ROCA.  But you're right.  But I checked, immediately went to that ROCA tester, which by the way is down right now, the one that you...



STEVE:  It's probably just submerged. 



LEO:  Yeah.  They say "KeyChest is upgrading our system."  Which means, yeah, it means it got clobbered.  But when I heard about this yesterday I ran all my keys through it, and all of them passed because they were all software generated.



STEVE:  Yes.  Good, good.



LEO:  Sorry, I didn't mean to interrupt.



STEVE:  Oh, no.  No, that's a good segue.  So the prevalence of web-based cryptocurrency mining.  And this has an interesting twist to it.  So there is a well-known popular advertising blocker, an adblocker called AdGuard.  And they took it upon themselves to just sort of do a little research into the prevalence of cryptocurrency mining.  We talked about it, I think it was just last week, or maybe the week before, about the fact that, while, yes, it might be annoying, if you go to an adult content site or a torrent or a pirate TV site or something, they tend to be the shadier sites on the 'Net.  If your processor gets pinned because you've got JavaScript trying to do mining - and I mean, oh, my god, I can't think of anything less efficient than using JavaScript for mining.  But as we said last week, it's not a security compromise.  It's just an annoyance.



LEO:  And it may even be a somewhat legitimate way of monetizing; right?



STEVE:  Well, that's where we're headed with this.



LEO:  Ah, okay.



STEVE:  Is that wouldn't you rather, since it isn't a security problem, wouldn't you rather let the site you're visiting run mining on your computer rather than have you look at ads that are annoying?



LEO:  Because it's not cycles you really care about.



STEVE:  Right.  Unless you're mobile.  If you're mobile, then maybe it does matter because your phone's going to heat up when you're there.  But for a laptop...



LEO:  They kill battery life, yeah.



STEVE:  Yes.  For a laptop and a desktop, if I had a choice of letting a site I visit borrow my computer for the sake of monetizing my presence while I'm on the site, I mean; and, for example, if the page is in the foreground, not if I switch away to a different tab, then it's like, okay, sorry, I'm not looking at you anymore.  So the idea is all of the proper influences align.  The longer time you spend, the more frequently you go back, the longer you are there looking at their content, the greater the chance that lightning will strike, because that's about what it is these days on scoring a coin, and your computer will generate a bitcoin on behalf and for the benefit of that site.



And for large sites with enough users, the numbers begin to make sense.  These guys note that, even though the incremental amount of revenue generated is not great, neither is it for ads.  And so ads are also working on the large population model, and so does mining.



So we have an interesting picture here in the show notes from their blog posting, from the AdGuard.com blog post, where they said:  Three weeks into crypto mining going viral, 220 of the top 100,000 websites are currently...



LEO:  What?



STEVE:  Are offering crypto mining scripts.  Yeah.  And those sites, based on the popularity of those sites, 500 million users were doing mining on behalf of those sites during that period of time.  And it's estimated that those sites earned about $43,000 in those three weeks.



LEO:  This is actually great.  Now, everybody in the chatroom's going, "I'm never going to do that, no way, I don't want to."  But if it's got no impact on you...



STEVE:  Correct.  It's not a security vulnerability.  You're already running JavaScript like crazy.  It's doing all kinds of tracking crap.



LEO:  Do these sites do it explicitly?  Are they, I mean, I've never seen this.  Is somebody saying, hey, if...



STEVE:  No.



LEO:  No.  See, that's what they need to do.



STEVE:  Yes, exactly.



LEO:  You have a choice.



STEVE:  Exactly.



LEO:  Do you want ads, or do you want us to use crypto mining software?



STEVE:  Exactly.  So the idea would be that, in the same way that our browsers had a DNT, a Do Not Track flag that they sent with every query, our browsers could have a "you may mine on me" flag.  And so you'd be saying - and so when your browser says that, the site says, oh, good, we'd rather mine anyway than annoy you with ads.  So you get an ad-free page, and your processor gets pinned, and there's some probability that your browser will succeed the bitcoin or the whatever cryptocurrency it is challenge, and the web server will score some cryptocurrency.



And it might even be possible to pool, that is, to use mining pooling to get incremental revenue just based on the total amount of time that a site is spent mining.  I have to talk to Mark Thompson, who really understands this stuff upside down and backwards.  But I agree, Leo, I think it's a really interesting monetization solution, one that makes absolute sense.



LEO:  It's going to be hard to convince people.  But I think, man, if I thought I could get away with it, I'd do it.  I mean, seems completely reasonable.  I mean...



STEVE:  I agree.  I can't see a downside.



LEO:  And people are saying, well, what about the electrical cost?  In aggregate, it will be high because that's the nature of bitcoin mining.  But to any individual, it's not even going to be a penny.



STEVE:  Yes.  It's so distributed.  The whole thing is so distributed that I think it makes sense.  And, I mean, if you put your hand next to your computer and looked at the heat that's being pumped out of the fan?



LEO:  It's working.  It's working anyway.



STEVE:  Yes.



LEO:  People say how much bandwidth will it use?  None.



STEVE:  None.



LEO:  It's all local.



STEVE:  Yes.



LEO:  I think we'd have to educate people if we were going to try this.  I'd do it in a minute.  The problem is I don't really - wouldn't it be great if I could figure out some way, as you're listening to a show, that it's bitcoin mining in the background.  Two hours later we've paid for Security Now!.  Costs us $1,000 an hour to do a show.  Could you, if you had enough listeners or viewers, make - I bet you could make a thousand dollars an hour.  But anyway, it's an interesting thought.  I'm happy [crosstalk].



STEVE:  It really is.



LEO:  Folks, I wouldn't do it without telling you, and I have no plans to do it.  But I think it's a great idea.



STEVE:  I do, too.  I just, I mean, and it hit me as I was reading along, thinking - and then it's the way they ended their blog post.  I mean, and here's an adblocking company.  Their whole business is to block this.  And by the way, uBlock Origin already does, and AdBlock Pro or Plus, Adblock Plus does.  So there's already been a response by the controllers in the industry to this.  But I'd be inclined to say, yeah, let's allow mining if it means that I get a better, you know, if I could support the sites where I'm visiting.  I mean, okay.  Wikipedia.  Fine.  Mine on my machine while I'm browsing Wikipedia in order to send some money to them.  Why not?



LEO:  Wow.  Oh, I would do that.  I already send them, like, $100 a month.  That would be a good thing for me.  Wikipedia won't put ads on.  Kudos to Jimmy Wales for not doing that.



STEVE:  Yup, yup.



LEO:  I think that's a very responsible thing to do.  But that means they have to beg.



STEVE:  And so the idea would be, in the Wikipedia model, they could present a banner if they see that your browser does not have a cookie explicitly allowing mining.  And they say, "Hi.  If you would like to support our site, please allow us to run bitcoin mining on your computer, in your browser, while you're on Wikipedia."  And so you click yes, and that's - so that sets a static cookie.  And from then on, every time you go to Wikipedia, the mining starts up.  Maybe there's a little banner that says, "Hi.  Thank you for your support.  We are currently mining bitcoin on your computer."



LEO:  Right.  It'd scare people like crazy.



STEVE:  It could be totally done.  Well...



LEO:  Somebody said you've got to - it's all in the naming.  Chatroom says call it "Ethical Mining." 



STEVE:  Ooh, I like it.



LEO:  Thank you, Dave Redekop.  That's Nerds On Site.  He says...



STEVE:  Ah, perfect, yes, David.



LEO:  EMO, Ethical Mining Operation.  I love it.  It's one of those ideas that's brilliant, and I just feel like people are so scared of the whole idea they wouldn't - they don't like the idea of somebody running a program on their computer.  We do SETI@home.  It's like SETI@home.



STEVE:  [Crosstalk] when you go to a browser.



LEO:  What are you doing?  That's what you're doing.



STEVE:  I mean, and see, that's just it, is that JavaScript is going insane every time we go anywhere on the Internet.



LEO:  Right.



STEVE:  I mean, and that's why I gave up.  Remember NoScript.  I was pushing NoScript forever until I finally said, okay, it just breaks everything.



LEO:  I know what you call it.



STEVE:  The reason it breaks everything...



LEO:  Clean Bitcoin.  No.



STEVE:  Or CleanCoin.  



LEO:  CleanCoin.  Ohhh.  CleanCoin.  We can set up a Dogecoin mining operation.  You'd never make any money, but - all right.  I'm not going to do it.  Don't worry, folks.



STEVE:  So this was an interesting piece of work by a guy named Felix Krause.  It got sent to me by one of our listeners, and I thought, oh, you know, this really does raise a very good point.  And that is, it is easy to obtain someone's iOS password.



LEO:  I've thought this for a long time, and I didn't want to say it.



STEVE:  Just ask. 



LEO:  Yup.



STEVE:  And, yes, on the show notes we have a side-by-side sample of something real from iOS, and then a sample pop-up generated by a phishing application.



LEO:  Can I just say I feel like this is a problem across the board - OAuth notifications, when people ask me to sign into Google - because there's no certificate attached to these pop-ups.



STEVE: Yes.



LEO:  It's very hard to validate.



STEVE:  Yes.



LEO:  What would you click on to know if this - it looks exactly the same.  



STEVE:  Yes.  And the problem is, as Felix notes, we have trained people to simply enter whatever is asked of them because there is, I mean, there's no model, no robust, no rigorous model for when we should receive one of these.  These little sign-into-iTunes things pop up all the time.



LEO:  Seemingly randomly.  We've talked about this before.  You say, in fact - I remember.  I said, "Why, Steve, why?"  And you said, "Well, I can only assume that Apple knows that there's a potential problem at this point, so they want to reauthenticate."  But end-users don't know why.  They just...



STEVE:  Nope.



LEO:  And we're trained to say, okay, fine, another Apple password pop-up.



STEVE:  Yup.  In fact, Felix says:  "iOS asks the user for their iTunes password for many reasons.  The most common ones are recently installed iOS operating system updates or iOS apps that are stuck during installation.  As a result, users are trained to just enter their Apple ID password whenever iOS prompts you to do so.  However, those pop-ups are not only shown on the lock screen and the home screen, but also inside random apps, for example, when they want to access iCloud, GameCenter or In-App-Purchases.  This could easily," he writes, "be abused by an app, just by showing an UIAlertController" - which is the name of that in the developer docs - "that looks exactly like the system dialog.  Even users who know a lot about technology have a hard time detecting that those alerts are phishing attacks."  And looking at it, there's no way to tell.



LEO:  You can't.



STEVE:  There's no way.  Now, he went on further.  I didn't show another sample.  But in this instance he is showing your email.  So the app would have to know that, which frankly is probably not hard or not impossible for it to know.  But there are other iOS real dialogs that don't provide your Apple ID as a reminder.  Again, a user - and even if there weren't, but there are, but even if there weren't, a user is still going to see that and go, okay, I don't know why, but my phone wants me to enter this.  So they're going to.



LEO:  Somebody in the chatroom, Don S., says, "When I see those, on the iOS, anyway, I press the Home button, maybe even twice, because iOS will ignore those.  It will sit on that screen until you sign it.  But apps cannot."



STEVE:  Yes.  And that is exactly what Felix...



LEO:  That's a great trick.



STEVE:  That is what Felix said.  That is his solution to this.



LEO:  Ah, okay.



STEVE:  Is when you see that, press the Home button.  It won't work if it's a real one.



LEO:  Very interesting.  Of course, it doesn't solve the Google OAuth pop-up problem or any of this.



STEVE:  No.  And in fact my work in the last week on SQRL has been exactly this problem, that is, because what we were just talking about here with the iOS pop-ups, that's an instance of the bigger problem of spoofing.  Spoofing is just - it's the intractable problem because it involves the human factor.  And so, as you know, we're down very near the end of the SQRL project.  And someone brought up the point that a web page could present a dialog that looked just like the standard SQRL login prompt, and how would anybody know?  And so essentially I've duplicated the UAC look that Windows adopted for this reason, to create something that a web browser can't do because the web browser never has control outside of its own borders.



LEO:  That's why Windows darkens everything and...



STEVE:  Yes.



LEO:  And, yeah, that's interesting.



STEVE:  And I'm doing the same thing.  I monochrome and darken the screen so the dialog stands out, and a browser can't do that.  But unfortunately, I mean, in general, spoofing leverages the human factor.  And there's just not a good solution to it.



LEO:  It's another reason why you should be concerned about this feature in Android that applications can write over other applications.



STEVE:  Right.



LEO:  A feature Google won't fix because it's used by Facebook for chat bubbles and other - yeah.



STEVE:  Exactly.



LEO:  But that's exactly how you'd use that.  You'd just write over another application.



STEVE:  Yeah.



LEO:  Making it even more hard to detect.



STEVE:  Exactly.  And you can't tell who you're talking to.



LEO:  Who's writing that, yup.



STEVE:  Yup.



LEO:  Wow.



STEVE:  So Google did follow through with what we were expecting.  We talked about it, I think it was just last week, the so-called APT, the Advanced Protection Program.  It has gone live.  I've got a screenshot of Google's announcing it with their Get Started button.  The screenshot reads:  "Google's strongest security for those who need it most."  And they're explicitly not suggesting this is for everyone because, as I mentioned last week, and now we have some additional details, there's a tradeoff.  So it's intended to protect the accounts of "those most at risk of targeted attacks - like journalists, business leaders, and political campaign teams."



The main defense is a physical security key used for authentication, and it requires the use of Chrome, that is, their browser, because that's the other side of this authentication requirement.  So you have to use Chrome, can't use a non-Chrome browser.  And there are several tradeoffs.  One is this you have to have a physical security key - so that's not free, I mean, there's some cost to that - which is used to guard against phishing because the physical security key will be required every time you log into a device.  You can't even do, like, "remember me."  So this will replace and disable all other forms of authentication, including SMS, thank goodness.  But even the Google Authenticator app, sorry, physical key required.  Also it limits data access and sharing.  Third-party apps will no longer have access to Gmail or your Google Drive.  And email is only available through Gmail or inbox clients.



So since iOS apps do not support security keys, Google has noted in their documentation that Apple mail, contacts, and calendar apps will not work, and users will be forwarded to standard apps on iOS.  And Google services that require a sign-in, like Photos, will only be available through Chrome.



And then, finally, the last measure is that it's designed to encounter impersonators who claim to be locked out of their account.  Again, we've talked about the weakest link.  The weakest link is account recovery.  It's like, oh, I forgot my password.  And it's like, oh, fine.  Here, we'll mail it to you, or we'll send you a link that, you know, blah blah blah.  So Google notes that these extra steps, like reviews and requests for more details, will be there in place during account recovery process.  So the recovery process will take a few days, they are saying.



LEO:  Wow.



STEVE:  In other words, yes.  So this is not free.  A physical key is required, and with it comes substantial, I mean, but correct - they're doing the right thing.  As we often talk about, there is a security versus convenience tradeoff.  If you have convenience, you don't have security.  If you have the convenience of one-click password recovery, there's no security there.  So Google has decided they're going to get...



LEO:  How do you get this?



STEVE:  ...serious.  Good question.



LEO:  I don't think they say.  They probably approach celebrities and at-risk people.



STEVE:  What happens if - yeah, because, well, there is this dialog.  What happens if you google Advanced Protection Program?



LEO:  Yeah, because I would do this.



STEVE:  Yeah.



LEO:  I'm actually doing, you can kind of do it anyway.  What I do with - in fact, this came about, remember I was talking a couple weeks ago about how somebody had hacked Hover, my - yours, as well, my domain registrar.



STEVE:  Yes, registrar.



LEO:  And added an email, because I use them for email redirection, and added another email account to the redirection so that they were getting recovery emails.  So one way to mitigate that, I decided to stop using my main email address for all of this stuff, but to create a special address on a server no one knows about as my recovery address.  That way you can't guess it, anyway, and you'd have to somehow get access to that server.  It's interesting.  With LastPass, which is of course the thing I secure most aggressively, it reencrypted all my keys when I did that.  So I had to re-log into LastPass everywhere.  I use a YubiKey for two-factor on LastPass which is great because you can't - unlike Google, you can't revoke it or say, well, let me use some other method.



STEVE:  Right.



LEO:  Once you turn on hardware two-factor on LastPass, you either have to have that or approve an email to the recovery address that says, okay, yeah, turn off the YubiKey.  So I feel like that's pretty secure.  Google unfortunately does, if you can't authenticate one way, Google gives you 20 ways you can authenticate, which I wish it didn't do.



STEVE:  Yes.  And that is the problem.  And so in this program they've said no.  It's not free, but we're going to give people an option to have really strong security with a bunch of tradeoffs.  That is, your Gmail is siloed, essentially, and cannot be accessed by other apps and other facilities.  And account recovery, we're going to really make that difficult.



LEO:  Looks like you can do it.  Just go to - it's landing.google.com/advancedprotection.  And then I see a Get Started button.  So it says you'll need two security keys.



STEVE:  Yes.  That's what we discussed last week.



LEO:  Buy one Bluetooth key that will work on your phone with a cable.  All right.  I already have a Yubico FIDO compatible.  And then they say buy a - wow.  So I need to go out and buy a key.  Wait for keys, and then you can turn on Advanced Protection.  Wow.



STEVE:  And is there a cost?  Is there a billing to it, or is it just...



LEO:  Let me see.  Let's see.  Oh, now it's going to - see, there's that pop-up that I just - I don't know how to trust.  I guess at this point I could look at a certificate because it's a page.  But there's the login.  First login.  I'll figure out what it costs.  I might do it.  I'm a target; right?  You are, for sure.



STEVE:  Yeah.  Okay.  So one note of miscellany, and that is I want to make peace with my listeners who are in love with "The Orville."  Because I'm not.



LEO:  You did try it; did you?



STEVE:  I tried it again.  I watched the "Krill" episode.  And when they finally went into the chapel for Krill services, I just though, this is just not serious enough for me.  And that's it.  Sci-fi for me is not humorous.  And I understand.  I've had some very good friends of mine recommend it, I mean, people I really respect, Trekkies saying, "Oh, Steve, make sure you're seeing 'The Orville.'  It's like, well, I did make sure.  I saw the first episode.  I got 10 minutes into that one, it was like, uh, no.  This one I just kind of hung in there, but finally I thought, no, it's just, you know, it's not for me.  But please, I get it that a lot of people like it.  But we don't all have to like the same thing.  So peace.



LEO:  So one roadblock on this.



STEVE:  Yes?



LEO:  The security key that Google recommends is currently unavailable.



STEVE:  Whoops.



LEO:  So I guess why you need two keys, one is - it says one that you can work with a cable connected to a tablet or a phone.  See, that's the problem with the YubiKey is there's no...



STEVE:  Ah, to power it, probably, in order for it to get power?



LEO:  They recommend this Feitian MultiPass FIDO Security Key.  I already have a YubiKey compatible with FIDO, which is my YubiKey 4.  But I don't - I'm not sure what the requirements are of this Bluetooth key that'll work on your phone.  I can't really use a YubiKey on my phone.



STEVE:  So if it's Bluetooth, it must have a battery in it.  And so it uses BTLE...



LEO:  Oh, I get it.  I get it, yeah, BLE for compatible mobile devices.  So there is an NFC.  You know the Neo, YubiKey Neo has NFC in it.  But I don't know if they make a - so I'm looking for a - I probably don't have to use this Feitian.  I can use anything that has BLE.



STEVE:  Right.



LEO:  This one has a rechargeable battery.  Yeah, you're right.  You nailed it.  Okay.  At least I know what I can look for here.  All right.  I distracted them from "The Orville."  That was my plan.  What?  Orville?  What?



STEVE:  Good job, Leo, good job.  So I got a nice note from Kristopher Ting of Powell Technologies in Johnson City, Tennessee.  This was yesterday, the 16th.  And of course the subject was "Another SpinRite testimonial for you."  He said:  "Hey, Steve.  I'm a longtime user of SpinRite and longtime listener of Security Now!.  Wanted to send you a quick success story, in case you wanted one for an upcoming episode."  Yeah, like the day after.  Thank you very much, Kristopher.



He says:  "My dad called me a few days ago, telling me his Windows PC was suddenly telling him his second hard drive (the one on which he stores all his important data) was not formatted.  It was the dreaded 'This drive is not formatted.  Would you like to format it?  Yes/No' message."  He said:  "After telling him to immediately click 'No' and remove the drive, he brought the drive over to my house, and I mounted it into my PC.  As I expected, my Windows PC had the same problem accessing any information on this drive."



So at this point we're talking a total loss of all of his dad's important data, which he explicitly and deliberately stores on this second drive.  And he writes:  "Saw it as RAW instead of NTFS."  He says:  "I immediately rebooted into SpinRite, ran a Level 2 on it. It was a standard 250GB SATA drive, so the estimated time to complete was under 40 minutes.  At around the 95% mark, SpinRite detected a sector that needed deeper analysis and, after about three minutes, it was ultimately marked as unrecoverable. The rest of the drive checked out okay.



"Thinking that the one unrecoverable item was the cause of my dad's original issue, I felt certain that I would reboot only to find I had the same issue and that his data was officially toast.  But to my pleasant surprise, the drive and its original partition were there and were now fully accessible.  I immediately copied all his data onto my PC and will be replacing his with another in the near future.  Dad's happy, and of course that makes me happy.  Kudos to such a great product, Steve.  Thanks, Kristopher Ting."



And I'll just note that, first of all, we've seen instance after instance where, even when SpinRite doesn't explicitly acknowledge that it fixed something, everything is fine afterwards.  But the other thing that I wanted to remind Chris and others of is that "unrecoverable" only means not every single bit was recovered.  But there are 4096 bits in a sector.  And if the sector is part of the drive's directory system, the file system metadata, you don't necessarily need all the bits.  And SpinRite does a partial reconstruction, even if it cannot do a full reconstruction.



So even if that sector was actually the problem, and we don't know whether or not it was, SpinRite will get you, for example, 4090 out of 4096 of the bits, and put them back and make the sector readable where it wasn't before.  So in many instances, that's enough to allow the drive to then say, oh, I have a partition again, and allow access to it.  So I often see a lot of people talking about how, oh, you know, it didn't perform full recovery, so it didn't do anything useful.  It's like, uh, well, actually that's not the case.  So we've talked about how large a gray area there is between everything working perfectly and nothing working at all, and how SpinRite is often able to pull a drive back out of the gray, back into the light.  So it did so in this case.  And Kristopher, thanks for sharing.



LEO:  So I've done some research.



STEVE:  Ah, good.



LEO:  Google, you're exactly right, Google's a little more clear about the two security keys that are needed for their Advanced Protection.  And the reason you need to, if you didn't ever use anything with iOS, you'd be okay.  Because NFC would be sufficient, USB and NFC would be sufficient for Android and most computers.  However, iPhone and iPad don't support NFC.  They only support Bluetooth LE; right?  So you need two keys, one like a YubiKey 4, which that's what I have already, and then another, as you said, with battery because to do BLE you've got to have battery.  It has to support FIDO UTF, that's the weaker Google version of SQRL.  And this does.  This is from - I don't know a thing about the company.  I've ordered one, and we'll see what happens, from VASCO Data Security.  They call it their DIGIPASS.



STEVE:  VASCO are good people.



LEO:  Okay.  And so this is a - it has a USB dongle, which is kind of interesting, but it also supports BLE.  So this will work with iOS, as well as all the other devices.  So I'm going to get this to supplement my YubiKey because I'm going to do it as an experiment.



STEVE:  Good, good, good.



LEO:  And I'll be your guinea pig on this.



STEVE:  And I thought that the newer, like from iPhone, was it 6?  I thought we all had NFC now in our iPhones.



LEO:  We do, but only Apple can use it.



STEVE:  Ah ha ha, okay, got it.



LEO:  And it's only used for Apple Pay at this point.



STEVE:  Right, right.



LEO:  So, yeah, and then I'll let you know.  If I can get through all of this, I'll let you know what the cost is in a second.



STEVE:  Yeah, cool. 



LEO:  For Advanced Protection.  But I think if I've got my Google and my LastPass really locked, securely locked down, I'm not worried so much about the rest of it; right? 



STEVE:  Yes.  I think you need an anchor which is secure, and then everything flows from that.



LEO:  Got it.  Thank god I've got you, Steve.



STEVE:  So closing...



LEO:  Have you brushed your tongue lately?  No, go ahead, keep going.  I'll talk about that later.



STEVE:  Okay, one second.  Closing the loop, we've got a couple nice bits of feedback from our listeners.  I guess he would call himself @crackruckles.  That's his Twitter handle, @crackruckles.  He actually tweeted to both of us, said:  "Just found a great site to show what info is leaked from your browser and torrent clients."  And I - you, Leo, and our listeners, IPLeak.net.  And it's a little sobering to see, just going to this site, you know, there have been many of these through the years that we've talked about.  But this is pretty comprehensive:  IPLeak.net.



LEO:  It kind of looks like ShieldsUP!.



STEVE:  And it just populates, and it keeps on going and shows you all kinds of stuff that's...



LEO:  Now, Father Robert said I shouldn't show my IP address on the air.  But I don't think there's anything particularly secret about the TWiT IP address.



STEVE:  Yeah.  The only thing I could see is that it would subject you to some DoS attacks, perhaps.



LEO:  Oh, I'm not worried about that.  We've got plenty of protection here.



STEVE:  Exactly. 



LEO:  This is good.  So this actually would be useful when I use my Tiny Hardware Firewall with a VPN and Tor.



STEVE:  Yes.  Yes, yes, yes.



LEO:  Get some idea about what's going on.



STEVE:  Go to IPLeak.net and see what it shows you.



LEO:  Very nice.



STEVE:  Steverino, whose handle is @DaMoisture...



LEO:  Okay.  Okay.



STEVE:  He said:  "Security at USAA Bank" - okay, this is security at USAA Bank - "just tried to tell me 'they have done the research and found that shorter passwords are more secure than longer passwords' when I complained about their website's 12-character password limit."  So Steverino, they're not used to talking to Security Now! listeners who know better.  They're used to talking to people's, I don't know, cousins who have just, like, what, okay, really?  Okay, well, that's good to know.  I didn't realize that shorter passwords are better.  Now I don't have to worry.  That's right.



Ryan Scullen said:  "How would DANE" - which is the technology we talked about a week or two ago where DNSSEC is being used to provide certificates.  "How would DANE prevent man-in-the-middle attacks?"  Well, and the answer is that one of the requirements for some man-in-the-middle attacks is DNS spoofing.  And so it's really not DANE as much as it is DNSSEC which prevents various DNS attacks which essentially are carried out by somehow arranging for someone to get the wrong IP address for a domain they believe is legitimate and then going there for spoofing.



So it's not exactly tied in.  But Eric Vollbrecht asked:  "How is publishing a public key via DNSSEC any different than a self-signed TLS certificate, other than the method of distribution?"  Okay, well, it's that last caveat.  So Eric, you're right.  It's not.  You essentially are publishing a self-signed TLS certificate.  It's TLSA is the protocol used.  And but the key is the method of distribution.  A self-signed certificate is less secure because you don't have a secure means of sharing it, essentially.  If you get a self-signed certificate from a site you're visiting, and your browser says, oh, this site's certificate is self-signed, you want to trust it, well, you say yeah.  But you could be at the wrong site, and now you're trusting the self-signed certificate from the wrong site, from a spoofed site.



So the beauty with DNSSEC is by securely and authenticatively obtaining the certificate for the connection, you solve the problem of distribution.  So Eric, you kind of answered your question.  Other than the method of distribution, there's no difference.  But it is the securability of the distribution of the key which that's as useful as the key being signed by someone you trust, which is the current certificate authority model that we're all operating under today.  So either you get a certificate from someone you hope is correct, but it's been signed by someone you trust, therefore you trust it, or you have a secure channel, a secure means of authenticating the source of the certificate, which is what DNSSEC and DANE under DNSSEC provides.



And Barbara asked, regarding the security.txt file that we discussed last week, remember, the one that will be coming soon to run alongside robots.txt, she asks:  "What stops bad guys from spamming the contact email address?"  That's exactly the thought that I had, and I forgot to mention it last week.  So thank you, Barbara.  One of the things you can put in the security.txt file is your email address.  You can also provide a place to pick up your PGP key, to allow people to send you encrypted secure email that only you can decrypt, which is useful if they want to establish a secure dialog with you.  But you can also give them a URL.



And so what I will do, because naturally bad bots, spam bots are going to be scrubbing the 'Net looking for security.txt files, I won't put an email address there.  I will put a link to a form which has "I'm not a robot" CAPTCHA stuff in order to prevent spamming.  So the solution is you can do many things, you can put many things in the security.txt file, one being not an email address, but rather a link to a web page where you can then do anything you want to.  So you are able to take it out of the email channel, if you choose.



Richard Petrie said:  "You asked on last week's episode why people were averse to printing password recovery tokens."  That is to say, actually talking about why they're averse to printing QR codes.  And he responded:  "I have no physical security."  And that's of course an absolutely valid point.  I do.  I'm able to, you know, my environment is just me.  And so I'm able to provide adequate physical security for the slips of paper where I have my QR codes printed.  But I completely get it that, if your environment has inherently got a lot of people coming and going, and you don't have control over it, then yes, I would absolutely agree that without physical security you would be better off printing them to a PDF or taking screenshots and then storing them in a safe fashion online, where you do have security over access.



And, finally, Brad Dux said:  "Regarding people wanting no password for SQRL."  He says:  "The protocol is open.  So do you not expect people to release implementations without a password?"  That's a very good point.  And the password requirement is for the client, that is, not only is the protocol open, the protocol never discusses passwords.  The passwords are not even in the SQRL protocol because, for example, biometrics, a fingerprint or facial recognition or whatever you want to do could be used.  It could be the physical proximity of, Leo, the Bluetooth key that you'll be getting.  Anything could be used as the additional factors.



The reason we need some control is that we are turning our authentication over to SQRL as a third party.  So we want to make sure that's not abused, that there's some interaction of some kind that permits SQRL to authenticate on our behalf.  Nothing in the protocol requires it.  And in fact even my client allows you to reduce it to a single keystroke, if you want to, after you provide the long secure password once.  So, I mean, and I think users are going to actually see that it is just not burdensome.  Normally, we call it the "quick pass," is the first four characters of your much longer password.  You have to provide that once.  And then, from then on, per authentication, it's just one, two, three, four, just you're able to just type the first four characters.  And you don't even have to hit Enter because the system knows that it's four characters.



So, I mean, we've really streamlined it.  I think people are going to be - won't feel that it's any barrier.  But technically, Brad, you're right.  Not only is there nothing to prevent people from doing non-password-based SQRL clients, I mean, I don't think anyone would want to use one, once they understood that anybody could come along and click on the QR code on a page and be logged in as them.  They would want some protection.  So it's available.



LEO:  Nice.  I'm using my Tiny Hardware Firewall, VPN, and BitTorrent.  And my IPLeak thinks I'm in Hungary.



STEVE:  Nice.



LEO:  I like that.  And it says it can't figure out what my IP address is at all, let alone leaking DNS requests or WebRTC requests.  It's just confused as all get out.



STEVE:  Perfect.



LEO:  Yeah.  It does, it's really amazing how much information the browser leaks, however.  That's really frustrating.



STEVE:  Oh, my goodness, I know.  It really is.  And they're just pouring out crap.  I mean, like which extensions you've got installed.



LEO:  Yeah, right.  And of course that's a fingerprint technique.  There's probably enough unique information in its configuration here to fingerprint me.



STEVE:  To still track you, even though you're coming out of Hungary.



LEO:  Yeah.  What's Leo doing in Budapest?



STEVE:  Well, he's been traveling a lot lately, you know.  He's liking those vacations.



LEO:  Why is his Surface laptop in Budapest?  I do, I have to say, at some point I'd love for you to just certify, if you can, WiFiConsulting and their Black Hole Cloud VPN because one thing we're learning, especially in light of KRACK, is that VPNs are not all made the same, and some of them are actually horrible.



STEVE:  Right.  In fact, there was a story that I let go by last week about VPN logging of everything that their own clients and their own customers were doing.



LEO:  Right, right.  I'm pretty sure these guys are great, and I've been using them for years.  And I love the fact that I'm going through hardware.  This thing appears to be an Ethernet dongle; right?



STEVE:  Nice.



LEO:  But then it has two WiFi radios in it.  It's just very clever, I think.  Anyway, it's also pretty fast, which is nice.  Continuing on, let's talk KRACK.



STEVE:  So, okay.  We've discussed over the years WiFi details, WiFi crypto, extensively.  But I need to do a little bit of a review because there haven't been any, like, horrific problems.  I mean, remember, Leo, back in the beginning of the podcast, I mean, there was stuff happening.



LEO:  WEP.  WEP.



STEVE:  WEP and oh, my goodness, and TKIP and RC4 and all these problems.



LEO:  That's what's so terrifying about this, because we had settled for five or six years now on the simple recommendation, you don't need to do anything else, just use WPA2, PSK, and your network's secure.  



STEVE:  Yes, actually since - it's been 12 years, since 2005.



LEO:  That long, wow, okay.



STEVE:  Yeah, we've just all gotten kind of happy.  So, okay.  So I have to lay a little bit of foundation for people to understand the nature of why what's happened is a problem.  In cryptography there are two broad categories of ciphers.  There are block ciphers and stream ciphers.  In a block cipher, which is, for example, what you use if you're going to encrypt your hard drive, the cipher algorithm itself, like AES, is itself a block cipher.  And in the case of AES it uses 128 bits, which is 16 bytes.  And so that divides evenly into the size of a sector, and everybody's happy because everything kind of works right.



Communications like WiFi use stream ciphers.  And stream ciphers and the ones that are used with WiFi use this magic XOR which is really kind of cool the way it works.  And we've often talked about both the power and the danger of exclusive OR.  The way to think of XOR is that it's the difference between two bits.  So if the two bits are zero, well, they're the same, so the XOR is zero.  If they're both ones, again they're the same, so the XOR is zero.  But if they're different, if the two bits are different, if it's zero and one or one and zero, then the XOR of those two bits is one.  So think of XOR as the difference between the two inputs.



And I'll just say briefly that that's the whole secret of RAID 5.  That's the way RAID 5 works.  If you have two drives, and you want to be protected against either of them failing, you add a third drive which is the XOR of the first two.  That is, it's the difference between those first two drives.  And so as data is written to either of these two drives, the third drive is kept synchronized.  It's always kept updated to be the difference between the first two drives.  Now, if the third drive, that one were to die, well, who cares?  It's like, okay, go get another one.  You just lost your difference drive, but it doesn't matter because you've still got your first two.



But if either one of the first two dies, think about it.  You can figure out what the other drive of that first pair was by comparing the surviving drive to the difference drive, to the XOR drive.  And that allows you to compute the entire contents of the drive that died.  That's how RAID 5 works.  So anyway, just something as simple as this XOR operation is very powerful.  And the other thing that we've talked about in the past, when we've talked about stream ciphers, is something very counterintuitive.  If you took plaintext, just a plaintext stream of bits that is readable, and it's clear for everybody to see, and you XOR that stream with noise, with randomness, the output is the best encryption in the world.  Which is, like, what?  It's that simple?  Yes.  Because XORing with random noise, it randomly inverts bits of the stream.  And there's no way to know from the output which bits were inverted.



And so that's what's sort of difficult conceptually to, like, believe.  It's like, well, if you just flipped the right bits back, then you'd have the original plaintext.  It's like, yes, but you don't know which bits.  And so it's weird that taking plaintext and just XORing it with a random stream of bits is unbreakable.  It's really strong crypto.  And here's the key when it comes to WiFi, because that's what WiFi does, is it simply generates a pseudorandom bitstream and XORs the data that's being sent.  And then the other end generates the identical pseudorandom bitstream and XORs what's received, which recovers the plaintext.  And that works.



Now, there are some requirements, though, for security because, powerful as this XORing plaintext with something pseudorandom is, you can kind of get a sense for the danger.  In the same way that the RAID drive was like a lost drive was recoverable, there is a danger, if you know what the plaintext was, and you can see what the encrypted data is, that reveals the stream.  If you XOR the plaintext and the ciphertext, you get the bitstream.  So this has to be used very carefully.  It's very fast, very lightweight, very secure.  We've all been happily using it for 12 years.  Well, and stream ciphers for even longer.  As long as your source of the pseudorandom data is good, you're okay.



Now, the problem is that we need to use a static key, that is, the key needs to be agreed upon by the endpoints and not change.  But the danger with a stream cipher is ever reusing the same stream.  That's the problem.  You must provide a mechanism from ever allowing the same pseudorandom stream to be reused against the plaintext because, if you do that, turns out it's trivial to crack it.  So to solve that problem, that is, not needing every packet to have its own key, but have long-lived keys, they introduced the notion of - and this is something we've talked about in the past - an initialization vector, an IV.  And the concept there is that it can be known, but it must never be duplicated.



So the idea is that, when you're starting to encrypt a packet, you increment this nonce, which can just be a counter.  And that's the initialization vector.  It mixes in with the key to produce a unique pseudorandom bitstream for that packet.  The other side has a synchronized nonce, and it's got the same key.  So it's able to increment the nonce, packet by packet, and reconstruct the same pseudorandom bitstream to get the data, to decrypt the data by re-XORing it against the same pseudorandom bitstream, and out comes the plaintext.



Okay.  So now that means never repeating the bitstream used to XOR the plaintext is crucial.  It turns out, not surprisingly, that's not guaranteed.  What these researchers, two guys from Belgium, figured out was there was a way with a lot of preconditions to force a reuse of the nonce.  Which, as I said, is absolutely disallowed.  So when a client - meaning iOS or iPad or Android device or a Mac or a PC, that is, anything that's not an access point.  That is, anything that wants to connect to an access point, that's the client, also known in the terminology as the "supplicant," as opposed to the authenticator.



When the client wants to connect to a WiFi access point, it sends an authentication request.  The access point sends an authentication response, that is, an authentication response back to the client.  The client sends an association request, and the access point sends an association response.  So that's the first back and forth.  That sort of just gets things rolling between the two.  That establishes their agreement to then negotiate secrets.  So that is just authentication and association to sort of establish the dialog between them, but no secrets yet.



So then they do what's known as the four-way handshake, that is, after those first, then they do the four-way handshake.  The access point sends the first of the four messages to the client containing its nonce.  So it makes up a random value, sends it to the client.  The client then uses its own nonce with the one it received from the access point to derive what's called the PTK, which is one of the keys used for WiFi.  And then it sends its nonce back to the access point.  That's the second of the four packets in the four-way handshake.



Now the access point has the nonce it sent and the nonce it received, so it's able to derive the identical PTK key.  So now both sides, having exchanged nonces, have this PTK key after two packets, the first one from the authenticator, from the access point; the second one from the client.  Now the access point sends message number three of the four, containing an additional key, and the client finally sends the fourth packet of this four-way handshake back to the authenticator.  And so that's the four-way handshake.  That's the way things normally go.



What these guys discovered, essentially, was there had been a mistake made from the beginning, from the first moment that this technology was designed, which arises if the client's fourth packet, that fourth phase of the handshake, it sending back essentially a confirmation.  If that never arrives, the access point will resend message three, thinking that, you know, not knowing whether message four got lost or dropped or collided with or whatever in transit; not knowing that the client didn't  get message three.  All the access point knows is that it didn't get message four, which is the result of the client receiving message three.  So it resends three.



It turns out that the protocol has an error such that, when any client receives message three, it resets the nonce.  And that's the vulnerability.  As I said, we cannot ever allow any encrypted data to be reencrypted under the same bitstream.  While the keys are not going to change, we're relying on the nonces to change.  So if we re-zero the nonce, we're going to reuse the same bitstream, and that collapses our security.  So this, it turns out, is difficult to implement in practice.



Okay.  So assuming that you don't have - no one assumes we have an evil access point because, after all, it's the access point that you're trying to talk to.  There's no reason to have an evil access point that is maliciously resending that third message in order to cause the client to reset its nonce because the access point knows what the client's saying anyway.  So one of the things that was not well covered is this requires a man in the middle.  That's one of the first things that has to happen.  This is man-in-the-middle attack, meaning that a third party has to somehow arrange to intercept the communications between the access point and the client.



Well, in a wireless mode with radio, that's not super difficult except that it's a little complicated because the MAC address of both of the nodes, both of these endpoints, is mixed into the crypto material.  So a man in the middle can't have a different MAC address if it's pretending to be the access point for the client, or none of the crypto will work.  It'll all break.  So the man in the middle has to have the same MAC address.  Except you can't have two of the same MAC addresses on the same Ethernet.  And remember that WiFi is just a version of Ethernet.  So the way these guys very cleverly solved the problem is they run their attacking man-in-the-middle radio on a different channel, with the same MAC address as the access point.  That way the crypto doesn't break, and there's no inter-MAC collision because they're deliberately on different radio channels.



So now this rather sophisticated man in the middle has to be within radio range of everybody and has to be quick on the draw, able to intercept, and essentially it's impersonating the access point to the client, and it's impersonating the client to the access point.  And it uses its man-in-the-middle position to maliciously resend that third message to the client in order to force the client to reset its nonce because of a protocol documentation error that's always been there.  And just that crashes the rest of the link security.  Now, it's true...



LEO:  Wow.



STEVE:  Yes, yes.  So, okay.  So several things.  First of all, notice that all of this is the client, is on the client side.  That's why I said it's not technically necessary for any of us to worry about our access points.  It's a malicious access point in the middle that is the problem, leveraging the client's logic on the client side.  So that's really what needs to get fixed.  It'll be good for us to update our access points because access points can behave as clients if they're connecting to another access point.  That is, when you want to connect like a mesh, if you have any kind of a mesh system, per the WiFi protocol, somebody is the access point; somebody is the client.  Or in WiFi terminology, one is the authenticator; the other is the supplicant.  So it's the supplicant side, the client side, where the vulnerability exists.



So it'll be good for us to update our firmware, but it's not crucial.  What's crucial is that the things that connect to the access points, the clients, that's what needs to get updated.  But also understand, I mean, somebody has to want to do this.  I mean, it's going to require - and there are no tools yet available for exploiting this.  It'll be nice to see whether there are some tests that become publicly available, and I imagine there will be some.



LEO:  He has a Python script he demonstrated, and he said as soon as it's mitigated, after a reasonable amount of time, he'll release the Python script.  But I wonder now, does he have to have some specialized hardware?  Or could he just use the laptop WiFi radio to do all this?



STEVE:  Well, for example, what we could do, you could put this as a - someone could build a test in an access point because it's the access point abusing the protocol to the client that is the problem.  But, for example, anything can be an access point.  So you could, for example, have a laptop...



LEO:  Do an ad hoc WiFi network, yeah.



STEVE:  Yes, exactly, a laptop running this Python script, which appears as an access point.  Then you would have your other devices, your tablets and your Android and your iOS client...



LEO:  Simulate the supplicant.



STEVE:  ...connecting to it as the WiFi radio and see whether they are robust against this attack.  So it turns out...



LEO:  But could you perpetrate the attack from a laptop, is what I'm...



STEVE:  Yes, you could.



LEO:  You could, okay.



STEVE:  Yes.  Yes.  So that's the way it would be done is somebody sitting at Starbucks, for example, to use my often-used...



LEO:  You could put it in a Pineapple, probably; right?



STEVE:  Correct, correct.  Or a Raspberry Pi.  Anything that had a WiFi radio would be able to do that, yes.  So by weird coincidence, Microsoft and Apple misimplemented the original insecure specification in a way that made them invulnerable to this.  The specs showed that - and the spec is a little unclear, too.  Matthew Green had an interesting statement about how this happened, saying that really, you know, the IEEE process - the IEEE is the organization which is responsible for the Wi-Fi Alliance and for managing the spec.  As Matt said, look at anything the IETF has done, and you'll see RFCs and everything in the open for everyone to see and inspect.  You can't get the specifications from the IEEE.  You have to be a member.  You have to pay dues.  I mean, I've often been thwarted by wanting to look at the WiFi spec.  It's not public, which is just crazy.



LEO:  That's weird.  That's ridiculous.



STEVE:  It's crazy.  It's absolutely wrong that we're all using a non-public, non-published specification.  It's just wrong.  But that's the game that they've always been playing.  Maybe something will change.



Now, on Android, from 6.0 on, it's actually the worst of all because there's a reading - oh, I'm sorry.  A particular implementation which appeared in one of the code bases that Android borrowed from, one of the open-source code bases, didn't only re-zero the nonce, it re-zeroed the key.  So when you do this to an Android 6.0 and later device, it's like worse than anything.  It actually switches to a null key, which of course everybody knows is all zeroes.  And so it instantly allows you to access the communications of the device.  So not good.



I'm just trying to think if there's anything else.  Oh, yes, remediation.  All that is necessary to fix this is that a Boolean flag be added to the software.  And I've already looked over some of the diff files of the source code which has been updated.  And it's simple to fix.  You just do a little bit of logic to say, if the key has already been established, and I'm getting another message three from the access point, don't clear the nonce.  Thank you very much.  That's all it is.  That's all you have to do.  Just don't do that.  And had that always been there, we would have never had this problem.  So it is...



LEO:  And that's on the client side, not the access point side.



STEVE:  Yes, correct.



LEO:  That's really critical, yeah.



STEVE:  Yes, yes.  So there need be no panic at all about access points that are not connecting to other access points.



LEO:  Although your mesh router does do that, in effect.



STEVE:  Correct, yeah.  And some of the routers actually have a checkbox where you can disable it to function as a client.  And so if you wanted to be safe until your firmware is updated, if you had the option in your access point, in your WiFi router, to disable behaving as a client, turn that off, and that will make you completely safe.  Then it can't be used as a client in order to connect.  In which case, I mean, even then the danger is minimal.



LEO:  But if you had an Android phone or an unpatched Android phone or unpatched iOS phone, it's a client, joining a WiFi network.



STEVE:  Except iOS was never vulnerable.



LEO:  Right, because it misimplemented it; right.



STEVE:  Exactly.



LEO:  But if you have an Android phone that properly implemented it, and now it's a client, what do you do?  You can watch the traffic between the two because it's unencrypted now?  Is that what's going on? 



STEVE:  Oh, so, yes.  And very good point, thank you.  I tweeted this this morning, and I wanted to remind people that, okay.  So what this would allow after an active attack, it would allow passive eavesdropping of some of the traffic.  But remember, a VPN, TLS, and HTTPS are all going to be encrypting the traffic anyway.  So WiFi encryption is an outer wrapping of encryption on top of what goes through it.  And now, increasingly, what goes through it is encrypted anyway.



LEO:  Although he seems to show on his - and I'm watching the video right now.  He seems to show that you can now do a man-in-the-middle on the certificate, as well.



STEVE:  Well, yes.



LEO:  And crack into TLS traffic.



STEVE:  You may be able to do that.  What initially they were saying, that if you captured the TCP SYN packet, that you could get the sequence numbers and intercept and essentially hijack the TCP connection.  I mean, so the idea is they're still sort of fleshing out how you leverage the ability to crack this.  And you're right, I guess if you were able to get the beginning of the...



LEO:  Oh, I see.  He's intercepted the OAuth is what he's intercepted.



STEVE:  Ah.



LEO:  And that's in plaintext because you've bypassed the encryption.



STEVE:  Correct.



LEO:  But a VPN protects you.  So that's really the real lesson here, yes.



STEVE:  Yes, yes.  A VPN protects you, and TLS and HTTPS protect you.  So most of our communications today are encrypted even without WiFi encryption.  For example, again, Starbucks.  That's a nonencrypted connection.  All everybody doing anything in an open WiFi access point is, you know, there's no password there.  It's all in the clear.  So we're relying on our own encryption, whether by VPN or HTTPS, to protect the data from other people who would otherwise be trying to spy on us.



So, I mean, this is going to get fixed.  The good news is Windows did patch it officially last Tuesday, even though it never really was a big problem.  There are, like, three different types of sort of subsets of this problem, and Windows was technically vulnerable to one of them, but not the main one.  And the same thing is the case of iOS.  As you said, 11.1 is now at beta, and we should have that soon.  And after reading this, I wanted to make absolutely sure I was right because I couldn't see how an access point needed to be patched.



So I checked their FAQ, and they say in their own - the authors' FAQ says, "What if there are no security updates for my router?"  And they write:  "Our main attack is against the four-way handshake and does not exploit access points, but instead targets clients.  So it might be that your router does not require security updates.  We strongly advise you to contact your vendor for more details.  In general, though, you can try to mitigate attacks against routers and access points by disabling client functionality, which is what I was talking about before, which is, for example, used in repeater modes, or mesh, and disabling 802.11r fast roaming.  For ordinary home users, your priority should be updating clients such as laptops and smartphones."



And so that's from these guys who, throughout their 16-page paper that I read in order to exactly understand what was going on, I mean, they're finding any possible little nook and cranny, any little kink anywhere.  So for them to say, yeah, you're probably fine with your router, I mean, that means you really are fine with your router.  But we do want to get our clients updated.  And as I said also at the top of the show, the response that our router vendors generate, even though we don't need to worry about it, it'll give us some nice feedback about how security-conscious they are and how responsive they are to these kinds of problems.



Ubuntu has been updated.  I've got a bunch of - DD-WRT is already updated.  OpenBSD is updated.  BleepingComputer - I've got a couple links here at the end of the show notes.  BleepingComputer has a list of the firmware and driver updates as they're happening, and there's another link for a page maintaining a list of who's done things and who hasn't.  So again, we're a little overheated on the router side because it's not a vulnerability against the router.  It's against the client.  And I think we're going to have those updated.  And this, of course, this is the challenge once again with Android is there are an awful lot of Android devices.  Literally anything that isn't updated for this has probably been vulnerable since 2005.  So all Android ever.



LEO:  As long as nobody knew about it, that's okay.  We just don't know.



STEVE:  Exactly.  Now we know.



LEO:  So just to be clear, and I know you've said this, and I'm reiterating, updating your access point is not a mitigation.



STEVE:  Correct.



LEO:  And that's the real problem is that we all have bunches of IoT devices whose companies are long gone or don't care and probably won't be updated.



STEVE:  Okay, now, the good news is this is the link key, that is, it is the key that's negotiated on the fly from point to point.  It does not reveal your WiFi password.  There's no...



LEO:  But let's say they hacked a camera.  They'd be able to see the camera traffic; wouldn't they?



STEVE:  Correct, yes.  So, yes.  So it would be the traffic to the device could be exposed.



LEO:  Whatever that IoT is sending to the access point is now unencrypted and visible.



STEVE:  Yeah.



LEO:  Unless it uses TLS or a VPN or something like that.



STEVE:  Correct.



LEO:  Okay.  Well, you said in your tweet yesterday don't worry, and I think you've helped people a little bit.  And, frankly, if your IoT device is - I'm trying to think of - doesn't send video and doesn't send - if it's just, if it's like a Harmony Hub or something, who cares?



STEVE:  Yeah.  Well, and the good IoT devices like the Amazon...



LEO:  Like the Ring and the Echo, yeah.



STEVE:  They're all going to be over TLS.



LEO:  Yeah.  Oh, that's a good point, too, okay.



STEVE:  Yeah.  All the good ones, the real heavy-duty IoT devices, are going to be encrypted separately.



LEO:  So when I looked at the video again, I realized what he was showing was he could see part of the OAuth process.  But if you've already got an account with this TLS-based website, that's encrypted, and there's no way you can kind of bypass that.



STEVE:  Yes.  Correct, correct.  So the only danger would be non-HTTPS, where things like passwords are going in the clear.  Exactly like back in the day with Firesheep, where we were able to see that stuff in the clear.  This crack breaks the radio encryption in situations where sensitive data is relying on the radio encryption from point to point, which we're actually relying on a lot less today than we have historically.



LEO:  Oh, boy.  I'm so glad you came on, as you do every Tuesday, and reassured us on this one.  This is good stuff.  I will mention that I have ordered the dual-dongle setup for  Google Advanced Protection.



STEVE:  You've got dual dongles.



LEO:  They still haven't told me how much it would cost.  You have to register your dual dongles.  So I have to wait till I get the dual dongles.  I have one.



STEVE:  Next week.  Next week; right?



LEO:  But next Tuesday I will be able to tell you.  I'll get them in Friday.  Amazon says I should have it by Friday.  I already, as I said, I already have an appropriate YubiKey.  I just need that BLE device that you charge and all that.  And I guess that means from now on I have to carry those around with me; right?



STEVE:  Yes, Honey.



LEO:  But that's a vulnerability, too, because if somebody looks and says, oh, he's got a YubiKey and that other thing, well, he must be doing Google there.  They could try it; right?  Well, we'll see what Google does.



STEVE:  Well, and again, the reason - what we're trying to protect from is Russia. 



LEO:  Yes.  So not somebody mugging me.



STEVE:  So they're over there.  Even if you're in Hungary still.



LEO:  I should be okay.



STEVE:  Yes.



LEO:  I can't wait, actually.  I think that - I don't care about my Twitter, but I'm going to lock - Google, that's my Gmail.  That's all my email.  I want to lock that down.  I want to lock down LastPass.  Those things really are...



STEVE:  And the idea that when you switch into this mode with Google, they silo your Gmail so nobody else can get to it.  Apps can't connect, I mean, everything else is like it's firewalled.



LEO:  Good.  That sounds pretty good to me.



STEVE:  Yup.



LEO:  We'll see.  I'll give you a report next week.



STEVE:  Yeah, now, I do remember, Leo, when you started using your YubiKey, that it was like never around when you needed it.



LEO:  Yeah.  Well, I've put it now on my car key.  It's on my car key keychain.  And so it's always in here.  It's always here.  But it's sometimes not in the same room.  And, yeah, you have to get up and go get it and put it in.  But LastPass only requires you authenticate once every 30 days.



STEVE:  Right.



LEO:  So that's not the end of the world.  And you just kind of get used to it.  And if you are the kind of person who carries your keys, you know, yeah.  Or maybe I'll just make a little broach, a dongle broach, something that I always have it with me.



STEVE:  That's right.  We want you wearing them around your neck.



LEO: I bet you people - I bet you sysadmins do that; right?  They wear...



STEVE:  Oh, I guarantee.



LEO:  Yeah, a little clip on your belt or a tie clip.  Oh, yeah.



STEVE:  Remember when people were wearing thumb drives.  Remember when people were wearing thumb drives on a lanyard.



LEO:  I remember Woz coming into The Screen Savers, saying - he had it on a lanyard.  He had a USB key, and he said, "That's 2GB on that thing."  And we all went, "Whoa."



STEVE:  You must be rich.



LEO:  Whoa, 2GB.



STEVE:  You're never going to fill that up.



LEO:  Never fill that up.  Steve Gibson's the greatest.  GRC.com is his website, the Gibson Research Corporation.  That's where you go for SpinRite, the world's best hard drive maintenance and recovery utility.  You must have it if you've got hard drives.  Even SSDs, you've got to have it.



STEVE:  Fixes them right up.



LEO:  Why don't we make that the new SpinRite logo?  Live long and SpinRite.



STEVE:  Ooh, I like that.



LEO:  Live long and SpinRite.  Or SpinRite and live long.  He also has lots of free stuff there.  You can find out more about SQRL, of course Perfect Paper Passwords and Password Haystacks.  And people use his site to generate 64-byte totally random passwords.  They use his site to validate the difficulty of unencrypting a password.  It's all sorts of great stuff.  ShieldsUP! to test your router.  GRC.com.  You can also go there and leave feedback or questions:  GRC.com/feedback.  But the easiest place maybe to reach Steve for that is on his Twitter feed.  He allows DMs:  @SGgrc.  Do you have 280 characters yet, Steve?



STEVE:  I don't because I'm using TweetDeck.



LEO:  Me neither.  I think it doesn't matter.  I think it will automatically give you 280 when you can get 280.  Maybe not.  I don't know.  But nobody that I know of has it except for Julian Assange for some reason.  So @SGgrc.  Keep it brief, I guess is the answer, although your DMs can be as long as they want.  Now, we also have the podcast on our site.  He has it at his site, and not just audio, but also transcripts.  We have it on our site, TWiT.tv/sn.  We have video, as well.  And sometimes the video is worth watching because like the Image of the Day and stuff like that, you can see what Steve's talking about, although he does a great job of describing it.



We also have a subscription form there so you can find a way to subscribe on iTunes, Pocket Casts, Overcast.  So that way you get every episode.  This is really the one show on our network that doesn't turn to fish wrap by the next week.  It's something you want to keep on hand.  There's always lots of very useful stuff.  Mr. Gibson, we'll be back next Tuesday, 1:30 Pacific, 4:30 Eastern time, 20:30 UTC for Security Now!, and I'll see you then.



STEVE:  Till then, my friend.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#634

DATE:		October 24, 2017

TITLE:		IoT Flash Botnets

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-634.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss some ROCA fallout specifics, an example of PRNG misuse, the Kaspersky Lab controversy, a DNS security initiative for Android, another compromised download occurrence, a browser-based cryptocurrency miner for us to play with (and Google considering blocking them natively), other new protections coming to Chrome, an update on Marcus Hutchins, Microsoft's "TruePlay" being added to the Win10 Fall Creators Update, and some interesting "loopback" from our terrific listeners.  Then we take a closer look at the rapidly growing threat of IoT-based "Flash Botnets."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and we are going to talk about something kind of creepy.  Somebody is building a bot army using compromised routers.  It's already two million strong and growing 10,000 a day.  But what are they doing with it?  What are the plans?  No one knows.  Plus we'll talk about Google's Advanced Protection and how it works.  I got my keys.  I tried it.  I turned it off.  I'll tell you why, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 634, recorded Tuesday, October 24th, 2017:  IoT Flash Botnets.



It's time for Security Now!, the show where we cover your safety and security online with my good friend, my compadre, our comrade in security arms, Mr. Steven "Live Long and Prosper" Gibson.  Hi, Steve.



STEVE GIBSON:  Leo, great to be with you again, as always, for Episode 634 of this weekly security podcast.  We're recording this on October 24th.  And something has happened in the last week which is - the right word would be "nascent."  The title of this podcast is "IoT Flash Botnets" because almost exactly on the anniversary, the first anniversary of the Mirai botnet, something much worse has been detected.



LEO:  Oh, no.



STEVE:  By several security firms.



LEO:  So this is in the wild.



STEVE:  This is.  And, okay, I'll give our listeners a tease.  We're going to cover this at the end.  Mirai brought down DynDNS, a major DNS provider, using 100,000 compromised cameras and DVRs.



LEO:  Right.



STEVE:  This botnet, which goes by two names, either "IoT Reaper" or "IoTroop," I-O-T-R-O-O-P, currently has two million.



LEO:  Oh, boy.



STEVE:  So 20 times the client strength, the attacker strength that Mirai had.  Anyway, we'll talk about it at the end of the show.  But, I mean, it could do anything it wanted to.



LEO:  What can it do?  Anything it wants.



STEVE:  Anything it wants, yes.



LEO:  Uh-oh.  Oh, boy.



STEVE:  So before that we're going to talk about some of the ROCA fallout specifics.  Remember that we opened the topic last week.  That was the defective Infineon library in embedded devices that has been there for a long time.  And since that had just happened, all we could cover last Tuesday was the fact of it.  We now are beginning to get a little more fallout specifics from that.  There's a real interesting instance, thanks to Matthew Green and two other researchers, about an example of pseudorandom number generator misuse which is really interesting.



LEO:  Oh, I saw that.  That was very interesting, yeah.



STEVE:  Yeah.  We need to talk about the Kaspersky Lab AV controversy.  I've sort of been ignoring it because it just seemed purely political.  But Eugene has responded in an interesting way that I want to talk about.  Also there's an interesting DNS security initiative which the XDA Developers have picked up on for Android.  Another compromised popular software download occurrence, which paints now another meme, essentially, that we're going to talk about because first we had Handbrake, then we had CCleaner, now we have - god, I can't remember.  It's in my notes.  It's a Mac media player, Elmedia or Eltima.  Anyway, it's in the notes.  So, but this is beginning to form a trend that we need to identify.



Oh, I also found this so cool cryptocurrency browser-based miner, bitcoin mining - actually it's not bitcoin, but it's one of the cryptocurrencies - that we can play with.  And when you do it, you're donating to a cause of your choice that they offer.  But it also shows the hash rate of your machine.  And I tweeted this earlier, and my Twitter followers have been having a ball comparing which browser is faster mining than the other, and which platforms mine at which speeds.  But then of course the other thing it lets us do is experiment with the mining blockers because now we have a benign way to play with mining on our machine.  So we're going to talk about that.  Oh, and the fact that Google, for their Chrome browser, is now considering natively blocking this.  So you could test that, too.  Also there are some other new protections coming to Chrome.



We've got a quick update on Marcus Hutchins and what's going on with him.  Something that just sort of caught me by surprise because Microsoft has nothing better to do than to block people from messing with gaming under Windows 10.  And I'm sure this is something that Paul would be all tuned up on, and probably you are, too.  But then as I read into it a little bit further, I thought, okay, well, that kind of makes sense.  So it's something called "TruePlay" coming for the Fall Creators Update that has a security angle, of course.



We've got some interesting loopback feedback from our terrific listeners.  And then we're going to take a closer look at this new, really worrisome threat of a flash botnet, that is, the emergence and the emerging threat of flash botnets, this one in particular that already is 20 times the strength of Mirai.  And it hasn't misbehaved yet, but that's what it's for.



LEO:  We're keeping an eye on it just in case.



STEVE:  I don't think we're going to have to keep an eye on it.  We're going to know when this thing gets targeted, yes.



LEO:  It's like a small child as we're waiting for it.  Any minute now it's going to live up to its potential.



STEVE:  Hold onto the chandelier.



LEO:  Well, as always, this will be a jam-packed episode, full of goodness and delight, thanks to Mr. Gibson here.  Somebody asked me something this week.  I wonder what you would say.  Is it getting worse or better with security?



STEVE:  I would say it's getting worse.  I think that, well...



LEO:  I said both.



STEVE:  It's getting more.



LEO:  It's more.



STEVE:  So it's more.  So you've got more researchers finding more problems.  There are more problems to find.  There are more devices connected to the Internet, and there is more pressure on developers to push stuff out so there are more mistakes being made.  So it's just more of everything.



LEO:  And then the other side of it is, which I brought up, is because these are searched for more, there's bug bounties and all sorts of things, so I think there are more security researchers looking for flaws to protect you than ever before and more patches than ever before and more regular patches.  More attention's paid.  I feel like, for instance, Windows 10 is more secure than it's been ever.  Things like that.  So it's both, in a way; right?



STEVE:  Right.  Well, and browsers are getting better.



LEO:  Browsers are getting better, yeah.



STEVE:  We no longer have, beyond belief, JavaScript running in Outlook that used to be able to infect you.  No more active desktop.



LEO:  Right, right.  So we're making progress.



STEVE:  Yes.  Flash is being phased out finally.  So, I mean, we're moving forward.  And, for example, IE got rewritten to make Edge, which is arguably now state-of-the-art secure.  Firefox is catching up.  Chrome is leading.  So there's definitely progress.  But we're not really seeing anybody winning.  That is, it's just more.



LEO:  Yeah, that's true.  Yeah, it's back and forth.



STEVE:  Yeah.  So there's more attention being paid.  What that's revealing is more problems.  But it's not like there aren't new problems that are coming along just as quickly to fill the gaps left by the old problems that have been fixed.



LEO:  True, true.



STEVE:  So, yeah, I don't think we're ever going to wrap this up.



LEO:  We're in a little rat race.



STEVE:  We'll just do the all-day Tuesday podcast here before long.



LEO:  It's not getting shorter, that's for sure.



STEVE:  And we'll just - maybe bring your sleeping bags, and we'll just get a fresh pot of coffee.  So our Picture of the Week is textual, which we've never done before, but I was sent this screenshot, which I tweaked and cleaned up a little bit.  I just sort of liked it because anyone who's ever read an RFC - and this also put me in mind, we were talking about the Wi-Fi Alliance and how their specs are secret.  And I gave the example last week of the IETF and how easy it is to take for granted, but how cool it has always been that all the specs that underlie the operation of the Internet are all developed in public, in public sight.



LEO:  Well, except for the IEEE and the KRACK thing; right?  It's behind a paywall.



STEVE:  But that's not.  That's not the IETF.  No, but that's not the Internet.



LEO:  Ah.  That's true.  It's a standard.  It's a different standard.  Yeah, yeah, yeah.



STEVE:  Right, right.  So the Wi-Fi Alliance, they hide their crap.  Bluetooth hides their crap.  I mean, those are all organizations where you pay a lot of money to be on the inside.  But as a consequence mistakes happen.



LEO:  People make the argument KRACK could have been discovered 10 years ago.



STEVE:  Yes.



LEO:  If the IETF wouldn't have put the spec behind a paywall, a very expensive paywall.



STEVE:  Well, not the IETF.  That was the Wi-Fi Alliance.



LEO:  Oh, okay.



STEVE:  And their effort.  And the IEEE and the Wi-Fi Alliance, so that's different completely than the IETF, which is the Internet Engineering Task Force.  So literally the name of the documents, RFC, stands for Request For Comment, which that acronym says, "We want to know what you think."  And throughout the last 12.5 years we've been talking about RFCs.  I'm referring to them all the time.  I wrote my own IP stack, my TCP/IP stack, using these specifications.  I mean, they're complete.  They're thorough.  They're well conceived.



At one point the structure was unified, and they came up with these - and it's always in all caps.  They'll say such and such and such must be done or must not be done or should be done or should not be done.  I mean, and so they formalized what is important and what is optional and what is recommended.  Oh, there is also "may," so it may do this or may do that.  But they're very explicit in, like, ranking both on the positive and negative end of the scale, where specific requirements fall.



And that's why this particular Picture of the Week caught me because it's titled "So, pretty much this is how you break a widely implemented protocol."  It's five steps.  Step 1, read the RFC.  Every time it says "must," check if they did.  Every time it says "must not," check if they did not.  Every time it says "should," assume they did not and test for it.  And, finally, every time it mentions a requirement that does not affect the functionality, assume it was done wrong by at least one company, and nobody noticed because it still works.



So, I mean, and this encapsulates so much, and sort of broadly and definitionally, so much of what we talk about every week because there are, well, in fact one of our stories about the misuse of a pseudorandom number generator specifically said this should not be done, or these numbers produced by this should not be used for these purposes, only for these.  And people ignored that.  And as a consequence, it's possible to look at some of the handshake traffic on very enterprise-scale VPNs and crack them because, whoops, "should" and "should not" were not honored.



So anyway, this was just a great little observation that I liked, and we'd never really talked about the structure of RFCs and, like, not only how significant it is.  I would argue, I mean, we've often talked about how surprisingly robust the Internet is, from day one.  I mean, consider it largely hasn't changed.  And while, yes, we know it is far from perfect, our requirements today were not the requirements then.  So it's not fair for us to impose how we wish it had been designed 20 years ago on the way we wish it were today because they had no idea.  I mean, this was sort of an experiment back then.



But its foundation was so rigorously thought out.  I mean, and that's how you get something that survives this well is that you use committees, small teams of smart people who argue with each other in email forums and back and forth, and they receive a response, they think about it overnight, and then they come up with something better.  So each spec ratchets forward slowly until it finally drops out of that process as an RFC that is then adopted by the Internet Engineering Taskforce, and that becomes a spec.  And if it was crap, we'd have a system that we couldn't, I mean, it wouldn't be today's Internet.  But it was almost immaculately conceived.  It's just phenomenal.



LEO:  It was IEEE that has it behind a paywall, not IETF.



STEVE:  Correct.



LEO:  I meant to say IEEE, yeah.  For KRACK, yeah.  So it's not so different.  I mean, admittedly, all these companies are heavily influenced by their paying members, i.e., the big companies.



STEVE:  I just think, because they're asking their - the only thing I can figure...



LEO:  No reason for IEEE to hide this behind a paywall.  That's crazy.



STEVE:  Correct.  Correct.  I mean, it's WiFi.  And I just think it's because they charge a lot of money, and so the benefit of membership is accessed to these specs.  I mean, what else are they going to offer them in return for all this money?



LEO:  Yeah, right.



STEVE:  So it's like, okay.  But the problem is then none of the rest of us who have a real need and are not enterprises able to drop tens of thousands of dollars on dues to these things, I mean, and you have to be a member of all of them.  So it's annoying.



Okay.  So ROCA.  We're beginning to see some specifics of fallout from the decade-plus use of very badly, poorly generated public keys by this broken embedded library.  And there was something that teased me that I just didn't run down, but there was some suggestion that a shortcut was made in the algorithm where they had some collection, it was described as a "structure of primes" that were built in.  And it's like, okay, it's hard for me to understand how that couldn't be really bad, so it must be different than I'm thinking.  I mean, it just...



LEO:  You don't want to use fixed, existing, well-known numbers as part of your factoring.  That would be foolish.



STEVE:  No, no.  That would not survive well.  So we've learned a few things.  First of all, Infineon, of course, within the industry, is a well-known company.  They're the guys that had certainly the best of intentions, but they produced this defective library which Gemalto, which is a major smartcard and security access OEM, was using.  They have a card which they began selling in 2004 which is called the IDPrime .NET card.  They won't disclose, because now they're embarrassed, how many they've shipped.  But it's believed to be as many as hundreds of millions.  So hundreds of millions of individual Gemalto smartcards, based on the broken Infineon library, are not secure.



Now, it's probably not a coincidence that this card was discontinued at the end of last month, just this last month, three weeks ago, four weeks ago, at the end of September.  Okay, we're done.  However, the problem is they're the manufacturer.  There are still third-party distributors with inventory still selling them.  In Ars Technica's reporting, they contacted a Gemalto representative who referred them to a company advisory which read, quote: "Our investigation has determined that end-of-sale IDPrime .NET products may be affected."  And so, okay, right.



LEO:  What a coincidence.  What a shock.



STEVE:  Yeah, we're not selling those anymore.



LEO:  Yeah, you don't want those.



STEVE:  Good thing you didn't ask us last week.  But now we're not selling them anymore.  Meanwhile, cryptography experts who are curious, I mean, now you can imagine in the wake of this revelation from Monday before last, the industry is trying to understand what this means in practical terms.  So cryptography experts who are now aware of this have been poking around.  And they've added that there is little doubt of Gemalto's cards being affected.  The CEO of Enigma Bridge, that is a security company, said he had examined 11 of Gemalto's IDPrime .NET cards issued from 2008 through earlier this year.  All of them used an underlying public key that tested positive for the crippling weakness.  So you can extrapolate that to hundreds of millions.



So essentially what we have is an incredibly widespread, difficult to remediate, vulnerability.  And this is the danger imposed by the use of widespread and defective consumer crypto.  The problem is everybody's got these.  There's hundreds of millions of them in people's wallets or pockets.  And how do you deal with that?  I think it's going to probably be up to individual companies to decide how they want to handle this, what they want to do.  I mean, if they've got Gemalto products, they probably need to say, okay, how do we fix this?  We need safe keys.  And these are embedded in inexpensive cards, so they're not flash reprogrammable.  They're dumpster discardable instead.



Also, and this news was on our radar last week, but I didn't mention it, that the government of Estonia that we've spoken of from time to time through the years because they're on the leading edge of deploying state-of-the-art identity for their citizenry.  Unfortunately, they're using an identity system based on the Infineon library; and 750,000 electronic IDs it has issued, that is, the government of Estonia has issued, are vulnerable, three quarters of a million.  And researchers have uncovered evidence that the ID cards issued also by Slovakia and Spain may also be vulnerable.



So essentially we have this widespread, deep, broad, and old vulnerability.  And hopefully this will come to the attention of those organizations who are using these products.  To be responsible, Gemalto should be sending a notice.  Unfortunately, if they work through reps, they don't know who all their customers are.  There's a layer of insulation between them.  So, for example, unlike, I don't even know, I was going to use Yubico as a direct seller.  I know that they know many of their end users.  I don't know, though, if they run through a second tier of distribution.  Oh, yeah, because you can buy them off Amazon.  So they don't know who all their customers are.



LEO:  Well, but they might be fulfilled by Yubico.  Just because you buy it off Amazon...



STEVE:  Ah, good point.



LEO:  My sense is they do, I don't know, that they know everybody.  But that may not be correct.



STEVE:  So of course what we would hope is that any supplier, directly or indirectly, of technology that uses these soft public keys would take responsibility for that and say, you know, we profited from your trust, which, whoops, we didn't know we didn't deserve, so let's fix that.  Let's hope that happens.  The real threat isn't broad because, unlike something like Heartbleed, where a huge number of servers on the Internet theoretically had a low probability of being attacked, but when it happened the server could lose its private key, and then it could compromise it a lot.



Here, each individual card would have had a different public key.  Each one could have, for on the order of $10,000 or so, have that public key cracked to determine the private key, which would then allow that card to be impersonated.  So this suggests a huge vulnerability for targeted attacks, not broad and sweeping.  But, for example, you could imagine any state actor who knows that a target is using, indirectly or directly, this Infineon library could be rubbing their hands together because for the first time, if they can capture the public key, which the card readily gives you, so you just...



LEO:  That's the point of the card.



STEVE:  Yes, exactly.  It's like, here's my public key.  And so that's distributed.  So you just capture that, you crack it down, and you can then impersonate the person because the idea is that it offers the public key, and then it responds to a challenge which can only be answered by the private key.  And that stays in the card.  So the idea is, if you get the public key, which it broadcasts, for all intents and purposes, and crack it in order to extract the private key, now you can impersonate the cardholder for anything that they use the card for.



Oh, and that was the other thing, too.  In some of the research I was doing for this, one of their customers - it was on the Gemalto site.  It was one of their success stories.  Some random wind farm company somewhere had equipped all 4,000 of their employees, and they were so proud, and they're emitting press releases because this one card does it all.  And I'm thinking, it sure does.  Now you only have to crack the public key once for a given card, and you can do everything.



LEO:  Is it the same public key on all the cards?



STEVE:  No, no.  Different public key.  And so that's why it has to be targeted.  You'd have to decide, okay, we want to get Willard.  We want to be able to do what Willard can do, pretend to be him.



LEO:  So you'd need Willard's card.



STEVE:  Yes, yes.



LEO:  Having gotten his card, you get his public key.  You defactor it.  You know his private key.  Give him the card back, and you can be Willard.



STEVE:  Exactly like any of the spy movies where you get Willard a little tipsy, and then you slip his card out of his wallet, run it through your reader, put it back in.  And he still has his card, but you've got what you need.  And then you send that up to a cracking farm, and in a short time you've got the ability to, without copying the card - because that's the whole point.  The physical card is meant to be your security.  And so the idea is there's only one of those that has Willard in it, essentially.  And as long as he physically is in possession, it's the physical possession that is his security.



This breaks that by being able to leverage something the card freely gives out, the public key, being able to leverage that into something it deliberately never gives out, which is the private key.  And notice, this is exactly the same model with the Trusted Platform Module 1.2, which many people have.  I think it was 11 out of 41 laptops or something that we talked about last week.  So not all, but many have exactly the same vulnerability.  The TPM will say, "Here's my public key, woohoo," and anybody who asks for it can get it, the idea being that it'll never give up the private key.  Well, now you don't need it to.



LEO:  That's why you need two-factor.



STEVE:  Okay.  In the interest - oh, I forgot to start my podcast timer, so we're about 30 minutes in.  Normally I have that in mind.  But okay.  So I think we started around 2:00 o'clock.  So, okay.  This is the DUHK attack, but spelled D-U-H-K, which is an acronym for Don't Use Hard-coded Keys.  Yeah, don't.  Anyway, so I was tempted to say it should be the DUH attack, just D-U-H.  But DUHK works better, and you need the "K" on the end anyway.



So two researchers at the University of Pennsylvania who Matthew Green often works with, and Johns Hopkins University's prolific Matthew Green, uncovered a significant implementation error in at least 12 commercial VPN and enterprise big-iron products.  Fortinet is one that they highlight, only because - and it's just one of the 12 that they found.  But Fortinet is a major supplier of industrial-grade, serious big-iron hardware.  So I'm going to read the abstract from the paper.  The link to the paper is in the show notes.  And it is wonderful.  I mean, the paper is really nice, for anyone who wants to really dig in and understand this.



So the abstract says:  "The ANSI X9.17 random number generator" - so this is an ANSI spec, an official random number generator - "is a pseudorandom number generator design based on a block cipher and updated using the current time," that is, current time of day.  "First standardized," they write, "in 1985" - so it's old - "variants of this PRNG [pseudorandom number generator] design were incorporated into numerous cryptographic standards over the following three decades.  It remained on the list of FIPS 140-1 and 140-2" - that's the federal information something or other, oh, Federal Information Processing Standard.  And the FIPS 140-1 and 140-2 are sort of the formal, these are what we approve of that anyone can use.  And if you do, then you can stamp "FIPS certified" or "FIPS compliant" on your device.  And so everybody does who thinks, oh, good, we want to be able to say that.



So it remained on the list until January of 2016, so about a year and three quarters ago they finally said, okay, this is old.  Time to remove it.  But as always, I mean, if we learned a lesson, any kind of lesson here, it's that, yes, you can say "stop using it," but you've got to make everyone stop using it.  And no one has.



"The design," their abstract reads, "uses a static key" - and the static key is okay, that is, the design itself, the official design uses a static key - "with a specified block cipher to produce pseudorandom output.  It has been known since at least 1998" - okay, so since this thing was 13 years old because it was born in '85 - "that the key must remain secret [yeah] in order for the random number generator to be secure."  Actually, it was probably known in 1985, but it became obvious and apparent, probably.



"However, neither the FIPS 140-2 standardization process in 2001 or NIST's update of the algorithm in 2005 appear to have specified any process for key generation."  That is, they just sort of say, oh, yeah, just a static key is fine, but they're not telling you how to make it.  And of course all anyone had to do would have been to choose any source of entropy at all that the system is able to generate from packet arrival timing or anything to come up with a new key every time.



Anyway, they write:  "We performed a systematic study of publicly available FIPS 140-2 certifications for hundreds of products that implemented the ANSI [that is, this thing], the ANSI X9.31 random number generator, and found 12 whose certification documents use static hard-coded keys in source code, leaving them vulnerable to an attacker who can learn this key from the source code or binary."  So what they did was they looked at the documentation specifically for devices and products that use these standards, suspecting that that might make them vulnerable, and then looked at what these guys were doing.



They said:  "In order to demonstrate the practicality of this attack, we," they wrote, "developed a full passive decryption" - okay, listen - "full passive decryption attack against FortiGate VPN gateway products using FortiOS," which is their OS.  "Private key recovery requires a few seconds of computation."  Ouch.  "We measured," they wrote, "the prevalence of this vulnerability on the visible Internet" - that is, just the public Internet, they were just out there looking at traffic - "using active scans and find that we are able to recover the random number generator state for 21% of HTTPS hosts serving a default Fortinet product certificate."



So Fortinet also terminates HTTPS connections, not just VPNs, but standard web connections; and, they write, "97% of hosts with metadata identifying FortiOS v4.  We successfully demonstrate full private key recovery in the wild against a subset of these hosts that accept IPsec connections."  So VPNs, HTTPS, and IPsec.



So, okay.  What's going on?  The problem is that this is old technology that has been deprecated.  And this is a pseudorandom number generator whose use would be safe if its output was never visible.  For example, if you used it to generate candidate primes for primality testing, and remember that a prime in such a situation is going to - one of them is going to be your private key.  So it's by definition private.  And if you got another prime, and you multiplied them, now you can't pull them apart practically, so both primes are now essentially secret, even though their product you can make public.  The point being that that would be a fine use of this.  Wouldn't matter if you had a hard-coded seed for this pseudorandom number generator because its output was never being broadcast.



What these instances - and I don't mean to just single out Fortinet, and these guys don't either because that was one of 12 different companies whose products were doing this - is one of the things that we've talked about, in fact during our discussion last week of the KRACK attack, part of the WiFi handshake is nonces being sent back and forth.  Well, the nonce, and this is part of what the Diffie-Hellman handshake magic allows, is that somebody can observe each side sending nonces back and forth, and even with full knowledge of each end's - the nonce is a one-time random token.  Even in full visibility, seeing these random tokens going back and forth, that doesn't allow a passive observer to determine the secret key that they end up generating, that each side generates, after receiving each other's nonce, which is really cool.  But if these nonces are coming from this pseudorandom number generator whose output is never supposed to be seen, whoops.



So this is a misapplication of a pseudorandom number generator.  And so what Matt and the two other researchers understood and figured out and then reduced to practice was capturing plaintext nonces which are being sent at the beginning of these various protocols as part of the key negotiation phase and the handshaking, grabbing those nonces, knowing that they come from this pseudorandom number generator whose output is never supposed to be seen, but that's what a nonce is.  It's a public disclosure of randomness.  And from just by capturing those, they're able to reverse-engineer the PRNG state, which is to say any secret information, like the key, essentially, that they may or may not know.  And in this case the key is available to you, that is, you can reverse-engineer the firmware.



Sometimes the source code is published in some of these specifications, assuming that it doesn't matter.  But if you know the key, and you capture a bit of its output, you can figure out any unseen state in RAM that the device has, which allows you to both go back in time to recover previous state, which is to say previous nonces that would have been generated to decrypt conversations that happened before the conversation that you captured and the future.  So you can go both directions in time because this thing generates semi-deterministic random numbers based upon a key which you then know.



So it's a beautiful piece of work.  And the real lesson here is that it is very important not to have application drift.  That is, 30 years ago, 20 years ago, 10 years ago people who were involved with this probably understood that this was very simple and convenient and, oh, look, it's FIPS-approved.  Everybody thinks it's wonderful for this application.  But you can't have application drift, where you start applying this to other things, without consequence.



So this has now been deprecated, but now it's going to have to be forcibly removed from the firmware wherever this is being used, thanks to Matthew and company's discovery that this not-supposed-to-be-seen pseudorandom number generator is not safe for generating public nonces.  And our crypto protocols today use public nonces.  You cannot have this generator generating them behind the scenes because we can determine the past and the future of the connections that it makes.  It's very cool work.



So I need to engage you on this one because I think this is interesting sort of from a sociopolitical standpoint.



LEO:  Oh, you're going to talk Kaspersky.  I knew you were.  All right.



STEVE:  Yes.  Yup.  Eugene says...



LEO:  I have a strong opinion on this.



STEVE:  Yes.  Eugene says, "We have nothing to hide."  So they are planning, they have announced that they are going to open their software to external review and verification.  So to give our audience, I mean, I know everyone knows about Kaspersky.  They've got - their software is installed on 400 million computers worldwide.



LEO:  Isn't that mindboggling, that number?  Geez.



STEVE:  Yes.  Yes.  That number, it caught me by surprise.  I thought, whoa.



LEO:  That's got to be like 20% of all Windows computers.  I mean...



STEVE:  0.4 billion computers.



LEO:  Amazing.



STEVE:  0.4 billion.  So in their announcement, Kaspersky did not name the outside reviewers, but said they would have strong software security credentials - whoever they are, but we'll find out - and be able to conduct technical audits, source code reviews, and vulnerability assessments, as well as to examine Kaspersky's business practices and their software development methodology.  And he said that they're going to open "transparency centers," because transparency is what they're saying they're trying to give us, in Asia, Europe, and the United States, where customers, governments, and others can access the results of these outside reviews and discuss any concerns about the security of their products.



And, finally, they're going to be expanding their bug bounty program from what was a paltry by today's standards maximum award of $5,000 to a more state-of-the-industry $100,000.  Now, of course, in the reporting of this and kind of coming up to speed on this, I noted that not everyone's concerns were mollified.  And I could argue that, I mean, I'm sure we could, that nothing Kaspersky could do would make any difference to those who just say, uh, we just don't trust them.  So what do you think, Leo?  I don't have an opinion either way.



LEO:  Oh, I have a strong opinion.  First of all, why, I mean, okay.  Everybody deserves the benefit of the doubt, I guess.  Eugene Kaspersky and his company have written software for the Russian secret police, the FSB.  Because they're a Russian company, it's I would say likely that the FSB or the Russian government, or you know how they work through these shady third parties, could very easily compel them to include code in there.



STEVE:  And they could also have planted employees.



LEO:  Absolutely.  Why take the chance?  I mean, already the Department of Homeland Security forbids, and the DoD forbids, any government agency from using Kaspersky.  So if you absolutely - first of all, you and I agree we don't really believe in antiviruses.  They just are another vector, frankly, into your system.



STEVE:  And in fact, yes, we've covered places where they introduce vulnerabilities that weren't already there. 



LEO:  Right.  In many of the biggest named antivirus softwares.  And, you know, our intelligence agencies are convinced that Kaspersky was instrumental in the exfiltration of NSA secrets from this contractor's computer.  He brought them home, had Kaspersky on there.



STEVE:  Ah.



LEO:  What they're not very clear about is necessarily where Kaspersky did it knowingly or merely provided a vector.  Maybe there was a flaw in Kaspersky's software.  But since it was the Russians that exfiltrated it, I'm just - I think that it's fine to give someone the benefit of the doubt if you're going to put them in jail.  But I think the biggest problem is that Eugene Kaspersky, Kaspersky himself is beloved by American tech journalists.  Dvorak loves Eugene and was always recommending Kaspersky.  I know a lot of tech journalists are reluctant to indict him because they know him.



STEVE:  Right, right.



LEO:  I don't know him.  I don't have any opinion on him.



STEVE:  Nor do I.  And again, his company has grown large.  He's no longer able, even if he's absolutely blemish-free, in a large company, I mean, it's like the executives of Sony weren't responsible for Sony's network being breached.  It was a administrative assistant who clicked on a phishing link.  And so you really can't hold the management of a company responsible for everything every employee in the company does.



LEO:  Dave Redekop in our chatroom from Nerds On Site says, "Any sufficiently advanced antivirus software is indistinguishable from malware."  That's somewhat true.  But here's my - now, and then people will say, but yes, but he's opening the kimono.  He's showing people his software.  Here's my issue with that.  That works in open source software because you can then download and compile the software yourself and assure that the binary you're using comes from the audited source code.



STEVE:  Right.



LEO:  That's not what's happening here.  He's giving source code to be audited, but there's no guarantee that's the source code that makes up the binary you're going to use; right?  Or maybe I'm wrong.



STEVE:  No, no.  I agree with you.  And in fact it turns out it's surprisingly difficult for even two different developers to  compile the same source and get identical binaries.  They're just - it's like, if you use different versions of some libraries, you won't get a binary match.  I mean, and if you hash it, you get something completely different.  So it turns out it's extremely difficult to actually, even in the best of cases, to get binary identical output from separately compiled code because there are so many dependencies now, and every single thing has to be identical.



LEO:  You modify one library, you give them the source code calling a clib, but then you modify a clib so that when it's shipped it has an extra line.  It just phones home.  Anyway, there's lots.  But the thing that bothers me is their assertion that doing this will reassure people.



STEVE:  Right.



LEO:  That makes me think they're up to no good.  I mean...



STEVE:  Well, okay.  My take is...



LEO:  Maybe it's the best they could offer, I guess.  I don't know.



STEVE:  Well, it's all they can do.  But Leo, think of the economic impact.



LEO:  Oh, yeah.  I agree.



STEVE:  So this is devastating to Kaspersky that this is happening.



LEO:  So that would argue, well, maybe we should give them the benefit of the doubt.  But I...



STEVE:  Well, no.  It just suggests that they're doing everything they can to hold onto market share because they've got a lot right now.



LEO:  Well, when this was all brewing, they started giving it away, you know.  Which made me even more - I was telling my radio audience listeners, I've been telling them for months, don't install any antivirus, but especially Kaspersky.  Especially Kaspersky.



STEVE:  Well, stepping back from this a bit, there's sort of a broader thing, too, because I've noted on this podcast for years how surprised I am that Microsoft is able to sell Windows into China and Russia.  I mean, when we hear they're using Windows, I always think, what?  Really?  They trust U.S. software?  I mean, there's a contentious relationship here.  And so, yeah, Kaspersky is selling their AV into us, and they're using Windows.  And it's like, wow, okay.  I mean, so it goes both ways.  And I think it's surprising to me that in this day and age of there being open source alternatives to a proprietary OS like Windows, who in their right mind would use a black box where the hard drive is thrashing around, and it's constantly connecting, making random connections on the Internet.  It's like, what's this doing?  No.  So, yeah.



LEO:  Yeah, yeah.  So I guess we're in agreement.  And I feel bad.  I mean, poor Eugene.  I mean, maybe - I'm sure he's made enough money now that he can retire to Belize if he wants to.



STEVE:  He can just go find a beach.  He'll find a beach.  Yes, yeah, go to Belize and join John McAfee.



LEO:  Kaspersky was good because it was one of the last antiviruses that didn't do a lot of heuristics, as I remember.  It was all signature, so it was very fast and light.  But nowadays I don't know how effective that is, even.  Anyway, good.



STEVE:  Yes.  So Google for Android, on their Android platform, is apparently, although we don't have an official announcement yet, going to be adding DNS over TLS, which is very interesting.  Four days ago, on Friday the 20th, some guys over at XDA Developers - which as probably a lot of people know is a huge mobile platform developer community.  I think it's something like 6.6 million members, and it's very active.  One of them noticed something interesting in the Android Open Source Project (AOSP) software commits, where some changes were committed into the source.



And so they wrote, over at XDA Developers, they wrote:  "It appears that DNS over TLS support is being added to Android, according to several commits added to the Android Open Source Project.  The addition in the Android repository shows that a new setting will be added under Developer Options, allowing users to turn off or on DNS over TLS.  Presumably, if such an option is being added to developer options, then it means it is in testing, and they arrive in a future version of Android such as v8.1."



Okay.  So what does this mean?  As we've often said, DNS, the Domain Name System, was never designed for security or privacy.  In fact, you could argue it's probably one of the least secure, least private, but also simplest and thus fastest and leanest technologies, protocols, systems that we have on the 'Net.  It uses UDP packets normally, which is also the lowest common denominator.  It's just a payload on top of an IP packet which you just send somewhere.  There's no setup.  There's no handshaking.  There's no, let's each of us agree on the numbering of the bytes that we're going to then be sending to each other in a continuous stream, none of that.  That's all TCP, which is built on these datagrams.



UDP just says, I'm shooting this off toward you.  If I hear back from you, I'll know that you got it.  And if I don't, well, either you are busy, or it didn't get to you, or your reply didn't get to me, so we'll just try again.  So, I mean, it just couldn't be any simpler.  There's no encryption.  There's no security.  There's nothing preventing somebody from intercepting it and lying to you, bouncing it back sooner.  In fact, we've talked over time about various ways of spoofing DNS, one being that, if you're closer to the person you're querying, and you see the query go by, and you inject a reply, if your reply arrives before the real one, yours is the one that gets accepted.  I mean, it just couldn't be less secure.



So the problem was, as we were saying earlier, this was, from day one of the Internet, the DNS we have today is the DNS they designed decades ago.  It was never designed for privacy and security.  But in today's era of not only surveillance, but extreme spoofing and phishing, DNS's startlingly absent security and privacy is an increasing problem.  And people are concerned about ISPs spying on them, seeing what they're doing.  And even if you do encrypt your TCP connections under TLS, DNS, unless you go to other measures like using DNSCurve or DNSCrypt, which you have to take some measures to do, you don't have any privacy.



So DNS can run over TCP.  By default it uses UDP packets on port 53.  That's the well-known port for DNS.  You can set up a TCP connection, though you don't have to.  There are a few DNS management operations which must go over TCP because they transfer much more information.  It's not just a simple query and reply.  You're transferring whole zones, as they're called, over DNS.  So there you need to do a zone transfer.  You need to use TCP. 



Okay.  So moving forward, we know that DNSSEC, which we've been discussing recently, prevents forgery by having the DNS server provide signatures along with its responses so that the recipient is able to generate a hash of the responses and verify that the signature the server also provided matches, and then you use the public key which the server publishes in order to verify that it had to have the private key to make the signatures correct.  So, however, that doesn't give you any privacy.  DNSSEC, let me say that again, DNSSEC, which hasn't even arrived yet, doesn't really work yet universally.  It's coming, but there's no privacy.  The replies are still in the clear.  You verify their signature, but they're not hidden.  So when we finally get DNSSEC, it's still no privacy.



Okay.  So back last May, what is that, about a year and a half ago of 2016, May of 2016, RFC - and here again Request For Comment from the IETF, publicly, beautifully, bunch of people all worked on this in plain sight.  They finalized RFC 7858, which is titled "Specification for DNS over Transport Layer Security (TLS)."  Unlike regular DNS, which as I said uses well-known port 53 for UDP and TCP, TLS DNS uses 853, so port 853.  They wanted to keep the 53.  They said, hey, nobody's using 853.  That one's ours.  So that's what it will be.



Now, TLS does require some handshaking.  So normally what happens where TLS is carried over TCP, you establish a TCP connection and then bring up a TLS tunnel, a secure tunnel, within that TCP connection, which gives you both authentication, so you know who you're talking to, and privacy, that is to say encryption.  So you authenticate the endpoint, and you encrypt.  So TLS, the use of TLS, even for DNS, still requires handshaking.  So this is not as lightweight as DNS, but it makes sense for a number of reasons.  For one, if there's no background bandwidth being drawn.



For example, Leo, if you and I were to go completely quiet and not move, there would still be substantial bandwidth back and forth between us because of the protocol that we're using.  It doesn't go to zero.  There's a constant background flux.  Not so for TCP, if you're not - and we talked about this years ago when we were talking about the way TCP works.  Basically, if neither endpoint is saying anything, there is no traffic at all.  Essentially, by setting up this transaction, you establish some agreed-upon numbering for the bytes that you will be sending.  And you can send a few, and the other guy can send you a few.  But as long as you're both quiet, no overhead.  Just there is some state, some memory that is committed at each end for maintaining an awareness of this agreement that you both have a connection.  But there's no overhead.  So that works in terms of does this make sense.



The other thing is that DNS tends to be bursty.  That is, when you go to a new page, especially in this day and age, there's crap coming from all directions of the Internet in order to fill that page with ads and discuss content and just all the stuff.  And so if those IP addresses for those domains, which may be a surprise to the browser, if they're also a surprise to your OS, then there's a burst of queries that goes out to your DNS server.  And that's the other thing that works with the notion of establishing a relationship with a DNS server, which you otherwise don't in normal DNS.  You just send off a UDP packet, and you hopefully get a response.  But if you have established a relationship by bringing up a TLS tunnel, then you're always sending your queries to the same server, which generally systems are doing anyway.



So what all this means is that we bring up a flavor of TLS.  This is not full HTTPS-style TLS with a certificate which is signed by a CA, which is trusted by the browser.  There are two modes that this can work in because they're trying to keep this from getting too heavy-duty.  They're just wanting to provide what they call "opportunistic privacy," where if you don't really - if you're not really worried about who you're connecting to, you'll still get a TLS tunnel for privacy.  You may not know for sure who the tunnel is connected to, but at least somebody passively observing the traffic can't see the queries that you're making, any of them.



The next step is to use key pinned privacy, that is, so that this TLS DNS server running on port 853, it'll hand you a certificate.  And but it's not signed by a CA, it's just a self-signed cert, just asserting, here's a certificate, the idea being that there's some out-of-band means for you to obtain the pinning for that certificate's key.  So the point is that maybe your ISP provided it.  Maybe you established a full-strength HTTPS trusted and authenticated connection to obtain the pinning for that key once.  Then you hold onto it, and then you just verify it.  So it's very quick, very lightweight.  Essentially you're verifying the signature of the certificate to verify that it matches the one that you have previously received because there's no way that we know of for somebody else to create their own spoofed certificate with a matching signature.  That's cryptographically infeasible.



So what this suggests is that Google is continuing to move forward to, sort of in this chicken-and-egg fashion, to increase the privacy of Android and in general of the ecosystem.  Now, this requires DNS servers that don't exist today.  I mean, this protocol just got standardized last May or, yeah, May before last, May 2016.  But the good news is it only uses already existing chunks of code.  So it would be very simple for the various major DNS servers to bring up protocol support for this, like in a few days probably, and then test it and make sure they haven't created new vulnerabilities.



And then I would imagine that a major version or two, or maybe even in the next major version, we'll say, yeah, now we support TLS DNS, or DNS over TLS.  And so that server would be opening a listening port on 853.  Of course, firewalls would need to open, too, in order to allow traffic to come through.  So there's some chicken-and-egg problem, but Google has, well, laid this egg.



And so we'll see what kind of uptake this has.  But this is a good thing.  It's lightweight, makes sense, and it looks like an Android version coming soon will at least, if it's on, probably attempt to bring up a connection to its DNS server on 853, which nobody else is bothering with yet.  And when those answers start, when those attempted connections begin succeeding, we'll start having - Android users will start having some privacy, at least.  An active man in the middle, because this is weakly authenticated, an active man in the middle could get in there; except that, if the Android user had previously obtained a key from that server, it could expect them to be the same.  So a transient man in the middle wouldn't be able to break in and spoof.



So it's not uber strong, but it's really good.  And it's lightweight.  And I salute Google for saying, yes.  We have a spec.  It's a year and a half old.  Let's add it to our client.  And I wouldn't be surprised if we see other clients like iOS in the near future saying, yes, us, too.  If Android's going to have it, we want iOS to have it.  And then some servers will start supporting it.  And before long, it'll be available.



And this is the way these things happen.  Probably not quickly, but inevitably.  And it seems like a good thing to do.  It's a lightweight, nice compromise for adding privacy to our DNS.  And again, ISPs, unless you VPN, they'll still know the IP address you're going to because that's still going to pass by.  But it's another layer of privacy.  And again, easy to do if it's just built into the infrastructure, as it will be before long.



Okay.  So we're beginning to see a trend emerge which is not good.  The name I couldn't think of at the top of the podcast was Elmedia.



LEO:  I don't know this, and I don't know anybody who uses it.



STEVE:  Interesting.



LEO:  So that's the good news.  Well, everybody uses iTunes on the Mac; right?



STEVE:  Right, right, right, right.



LEO:  And then there's [crosstalk].  



STEVE:  Although I tell you, Leo, it's a nice-looking player.  I took a look at it.  It's clean, supports lots of standards.  So what we're beginning to see is the emergence of what is being called "supply chain attacks," where something about the supply chain got compromised that caused malicious alteration of something, equipment, or in this case software downloads.  Remember that in early May a download mirror of Handbrake, the very popular media converter, was compromised to download a Mac malware known as Proton, which we'll talk about in a second because it's coming back again.  And of course a few weeks ago we talked about CCleaner.  Same thing, one of their servers got compromised, and one particular version of CCleaner was altered maliciously.



And now last Thursday researchers at ESET discovered that Eltima, E-L-T-I-M-A is the company, Eltima's Elmedia Player - oh, and also they have a download manager.  I have it in my notes here.  I don't see it in front of me.  Oh, there it is, Folx, F-O-L-X. 



LEO:  I'm not familiar with that, either.



STEVE:  Right.  That one is even a little bit further off brand.  But their media player, similarly compromised.  And it was delivering Proton.  Proton is a so-called RAT, a Remote Access Trojan.  And it's pretty nasty.  It first appeared last year as malware for the Mac and includes a range of features.  It's able to execute console commands, access the user's webcam, log keystrokes, capture screenshots, open SSH/VNC remote connections, able to inject malicious code into the user's browser to display pop-ups asking its victims for information such as their credit card numbers, their login credentials, and other things, or like anything else if it's able to do a code injection into the browser.  It's able also to hack the Mac user's iCloud account, even if two-factor authentication is in use.  And in March of this year, so about six months ago, offered for sale on a number of cybercrime forums for $50,000.  So it is potent sort of gray market, well, or black market cybercrime malware.



To their credit, Eltima was extremely responsive to ESET's report, immediately fixed the problem, notified their customers.  If anyone who's listening is a - and I don't have a range of dates.  It was just recently, so like in the last week or two, but I don't know when the attack, when this malicious change occurred, so I can't say if you downloaded this Elmedia player between these dates, that was the trouble.  But in the disclosure of this, Eltima's instructions explained how to scan for three things.  Under the temp directory, look for updater.app on your macOS.  Under the library directory, look for LaunchAgents and then com.Eltima.UpdaterAgent.plist.  Also under library look for .rand, and also under .rand look for updateragent.app.



They say:  "The presence of any of these files above is an indication that your system may have been infected by the trojanized Elmedia Player or Folx application which means your OS X/Proton is most likely running" in that machine.  The bad news is - oh, here it is.  "If you downloaded Elmedia Player or Folx on the 19th of October 2017, your system is likely affected."  So looks like it was found quickly and removed quickly.  But if anyone's concerned, if you are in Elmedia player, I've got the link to their page and disclosure.  I'm sure you can find it on their site also.



LEO:  They also recommend a total reinstall of your operating system.  Holy cow.



STEVE:  Yes, yes.  That's the problem.  This thing cannot be removed safely.  You just have to save your files and reload your OS.  It's the only way to get rid of it.  It is nasty.  Okay, Leo.  Donateyourtab.to.



LEO:  Okay.



STEVE:  Donateyourtab.to.



LEO:  They can have my tab.



STEVE:  Very cool.  This is a sample cryptocurrency miner, asks your permission before it mines, and very politely...



LEO:  Immediately crashed my browser.  Okay.



STEVE:  Whoops.



LEO:  You know, I wonder - this is Chrome.  I wonder if Chrome's saying, yeah, we're not going to do that.



STEVE:  That's interesting.  Well, it doesn't mine until you give it permission to.



LEO:  Yeah, it's crashed.



STEVE:  It ran for me on Firefox.  And I did have a bunch of people responding that it was working for them.  Do you have a uBlock Origin installed on Chrome?



LEO:  Oh, yeah, yeah, let me turn that off, yeah.



STEVE:  Yup.  uBlock Origin will block it.



LEO:  Thank you, uBlock.



STEVE:  Yes, exactly.



LEO:  Nope, it's still crashing.  I don't know what's going on.



STEVE:  Well, so this page, donateyourtab, all lowercase, no spaces, donateyourtab.to, runs a cryptocurrency miner.  You're able to choose whether you want to help Puerto Rico through the Hispanic Federation; fight breast cancer through the Breast Cancer Research Foundation; the Rohingya, I think I pronounced that right, which is the highly persecuted minority Myanmar; or help civil rights through the Southern Poverty Law Center.  You chose which of those groups you want to donate to, and then there's a huge iOS-style slide switch on the right-hand side of the screen.  You drag that over.  It then pops up a confirmation that we're going to run cryptocurrency mining on your machine.  You say yes, and then it starts.



Down on the bottom is a slider where you're able to decide how much of your machine you want to give it.  And in order to see how much performance you have, you can slide all the way to max.  And then in the far lower right corner it shows you your cryptocurrency hashes per second, that is, the rate at which your platform on this browser, when set to max, is able to mine this particular cryptocurrency.



So in their FAQ they ask themselves the question:  "How does the mining work?"  They answer:  "We use a service called Coin Hive, which is an API that allows browser-based cryptocurrency mining.  All of the hashes you compute in your browser go into their pool; and once we reach," they write, "a certain dollar amount, Coin Hive sends us the money.  We then cut a check to each charity based on how much was generated for each."  They write, "We make ZERO [all caps] dollars off this.  Coin Hive takes 30%" - and I choked when I saw that number, it was like, what? - "of all proceeds for running the mining pool and providing the mining services."  Then they also wrote:  "We're exploring ways to make that number lower by using a different or homegrown solution.  We also use money raised to cover fees to run this site (hosting, taxes, et cetera)."



And of course Coin Hive is alone at the moment, so there's no competition.  So 30% is what they're taking.  If this were to take off, or if major vendors were to experiment with using this as revenue generation for visitors, as we discussed at length last week, certainly, well, that would go to zero, although they wouldn't be donating to charity.  They'd be keeping the money themselves, rather than making us look at advertising.



But anyway, this followed perfectly on our discussion of this last week.  And I thought just it's a - for all of our listeners who are curious, donateyourtab.to lets you experiment and also see how good your machine, how good your browser.  Mine only got up to about nine hashes per second, which apparently is what an older iPhone can do.  So, yes, my machine does everything I want it to, but it doesn't do it quickly.  I'm seeing numbers of 80 and 90 being reported in my Twitter feed.  And anybody who's curious can check my Twitter feed and see how people are responding because it varies.  But only people with cell phones are as slow as my Windows, my old creaky, cranky, creaky Windows XP machine.



But very cool to see an example of this running.  And I don't see a downside to it.  It will be interesting to see if this ends up becoming something, if there is enough money in asking people to mine for the company that you're visiting.  That would be an interesting model.



Meanwhile, Google has responded to bug reports from several users whose Chrome was performing unrequested crypto mining.  It turns out that in the Chromium project they're looking at natively blocking in-browser cryptocurrency mining.  In the Chromium project, one of the recent bug reports wrote:  "If a site is using more than XX%..."



LEO:  That's what I'm using is Chromium.  That might be it.



STEVE:  If a site, well, I don't think it's deployed yet because they're still working on it.  And you would think they would give you a pop-up or do something that was a little more gentle.  "If a site is using more than XX% CPU for more than YY seconds, then we put the page into 'battery saver mode' where we aggressively throttle tasks and show a" - well, their internal term is a "toast," which is a notification popup, they call them "toasts" - "allowing the user to opt-out of battery saver mode. When a battery saver mode tab is backgrounded, we stop running tasks entirely."



They write:  "I think we'll want measurements to figure out what values to use for XX and YY, but we could start with really egregious things like 100% for 60 seconds."  And finally he finishes, saying:  "I'm effectively suggesting we add a permission here, but it would have unusual triggering conditions.  It only triggers when the page is doing a likely bad thing."  And then subsequently there was a lot of discussion back and forth.  No consensus yet reached.



But I appreciate that Google is being proactive.  And certainly, because of the battery drain problem, you certainly don't want mobile devices to be doing this without your knowledge and your permission.  And we would like our desktops to advise us that this is going on, like maybe pop up "This page is saturating your processor.  It's going to make other things sluggish.  Would you like us to throttle it for you, or shall we proceed?"  And "(It may be doing mining, which you're not getting any benefit from.)"  Who knows.



And of course there are, until Google and if Google should add in-browser blocking of mining, there is an AntiMiner extension, there's one called No Coin, and there's minerBlock as explicit blockers.  And then, as we mentioned last week, AdBlock Plus is adding it, or has now, and uBlock Origin has had it blocked for some time.  And apparently some AV products can also block those miners.  So anyway, even if you don't do this, having the ability to play with it I think is cool.  And I know that people who picked up on this earlier today from my Twitter feed thought, hey, this is cool.  And I began getting a flood of reports back about how many hashes per second people were doing.



Also on the Google track, they're adding what they're calling Advanced Protection to Chrome.  Google has partnered with ESET to beef up their detection of inadvertently downloaded and also removal under Chrome's "Cleanup" feature.  So Google explained, they said:  "We upgraded the technology we use in Chrome Cleanup" - which is already a thing - "to detect and remove unwanted software."



They write: "We worked with IT security company ESET to combine their detection engine with Chrome's sandboxing technology.  We now detect and remove more unwanted software than ever before, meaning more people can benefit from Chrome Cleanup.  Note this new sandboxed engine is not a general purpose antivirus.  It only removes software that doesn't comply with Chrome's unwanted software policy."  But now enhanced with technology from ESET, which again I think is very cool.



LEO:  That's not the Advanced Protection.  I think that's in the same article, but I think that's just a separate thing because Advanced Protection is that thing we talked about last week.



STEVE:  Correct.



LEO:  Do you want a report back on my experience?



STEVE:  Oh, yeah, yeah, yeah, yeah.



LEO:  Because remember I ordered - I already had a YubiKey.  These are unique...



STEVE:  Yes.  And that's the little round - the little BTLE.



LEO:  The VASCO DIGIPASS, yeah.



STEVE:  Yes.



LEO:  So these are both FIDO UTF keys.  YubiKey always was.  I had a YubiKey 4.  So I ordered, yeah, this is the Bluetooth one.  You press the button, and you have to have a Google trusted application on your device that recognizes this and authorizes you, and then you can use Google apps.  But there's some severe downsides to this.  In fact, so much so I turned it off.



STEVE:  Yes.



LEO:  So one is you can't use - and you mentioned this - third-party apps, which means none of your Apple iPad, your calendar, you have to use Google's apps entirely.  You can't use Apple's calendar, Apple's email, any of the Apple stuff, or any other third-party stuff.



STEVE:  To access your Google property.



LEO:  Which is as it should be.  I mean, if you want to really lock it down, Google can only lock it down if it doesn't give permission to any other apps, any third-party apps. 



STEVE:  And this is why it's always good to remind everybody this is not for everyone.



LEO:  Yeah.  Yeah, oh, boy, is it not.  The other problem that was the deal breaker for me is, as far as I could tell, it doesn't work on Linux.  So Chrome or Chrome.  You do have to use Chrome as your browser.  So I installed Chrome on Linux, and it still didn't work.  Maybe a bug.  I've seen this kind of authentication bug before with Google with their phone login techniques.  So it might be a bug.



STEVE:  Yeah, that's going to be an OS-dependent driver thing, too.  So you could see their...



LEO:  I tried it on all my different Linux machines with different distros and so forth, and it just - the YubiKey works fine with other apps.  But as soon as you put the YubiKey in, it bombs out.  So I don't know if that's something they'll fix.  They probably will.  If you can live entirely in the Google ecosystem, this is a great idea.  And it's not, I mean, it doesn't cost - to answer the question we weren't sure about last week, there's no additional cost, once you buy these keys.



STEVE:  Ah, right, right, right.  So there's no, like, service subscription or anything from Google.



LEO:  No, no fee.



STEVE:  Nice, nice.



LEO:  And you don't have to buy the ones they recommend, which is a good thing because this is out of stock now, as well.  But YubiKey and the VASCO DIGIPASS worked fine.  You need one USB and one Bluetooth LE key.  And you use one for mobile, any device that doesn't have a USB plug, and one for desktop.  It works well.  It's a really nice procedure.  You only have to authenticate it once on each device, just like always; right?  It puts a cookie on your system, says oh, no, he's him.



STEVE:  Ah, good.  That's very good.  So it's not a per-use reauthentication.



LEO:  No.  It's not burdensome in that regard.  It's only the first time you use a new device with your Google account.  The biggest burden...



STEVE:  And that makes sense, too, because the threat model is some third party somewhere else.



LEO:  Right.  Some guy saying, oh, I've got Leo's password.  Now let me log in.



STEVE:  Yes, yes, yes.



LEO:  I think that the only thing that makes this - the two things that make it different, one is there's no fallback.  So if you lose these keys, you have to go through this stringent account reauthorization which takes time and requires humans.



STEVE:  And, unfortunately, that's as it should be.



LEO:  Again, as it should be.  And you can't use third-party apps.  But really Google's two-factor you can narrow down to just - I turned off text messaging, for instance.  Just the Authenticator and something else, including a YubiKey.  So you can get many of the benefits just turning on two-factor and making it a little bit more locked down.



STEVE:  Right, right.



LEO:  Anyway, I didn't mean to interrupt, but this opens [crosstalk].



STEVE:  No, no, this is perfect.



LEO:  I thought I'd finish it.



STEVE:  Yeah.  That's good.  I'm glad you did.  So just a quick update on our buddy Marcus Hutchins, aka MalwareTech.  He may no longer be under curfew or GPS monitoring.  Marcia [Hofmann], I don't remember her last name, one of his attorneys, argued successfully last Thursday that the restrictions that had been imposed on him were needlessly burdensome.  He had never missed a court appearance.  Even when his GPS device failed during a trip to the East Coast, he did not attempt to flee, she pointed out.



And he apparently wants to surf and swim since he's in Venice Beach in the L.A. area on the West Coast of California, where the weather is fine.  And so this is needlessly, she's arguing, impeding him to have to wear a GPS tracker.  So the judge agreed and released him from those obligations.  And in fact he tweeted on Saturday, he said:  "Californians claiming it's cold, meanwhile I'm wishing there was a way to wear less clothes than I'm wearing without being arrested."  So it sounds like he's having fun in Southern California.



However, a day later, last Friday, the DoJ filed a motion to revoke the judge's decision.  So they're just going to fight.  Michael Chmelar, who is an Assistant United States Attorney said:  "While it's true pretrial services possesses the defendant's passport, it is unrealistic to think that the defendant could not leave the U.S. without travel documents."  So it's like, okay.  And what is the GPS's - I guess that forms a geographic tether?  So if he was at an airport, would alarms go off? 



LEO:  Oh, yeah, yeah.



STEVE:  Maybe TSA would wonder why there's...



LEO:  It's probably one of those house-arrest bracelets; right?  I don't know, but...



STEVE:  Yeah, well, it's GPS, so it's global.  And he was apparently traveling across the country with it.  So somebody wants to know where he is all the time.



LEO:  Yeah, yeah.



STEVE:  Yeah.  Anyway, he later tweeted, after the motion to revoke, that he was still in limbo.  So he doesn't know what's going on, and nobody apparently does.  So anyway, we'll see how this turns out.  I think we're probably correct, Leo, in that there's some history that he's going to be held to account for, unfortunately.



Okay.  I mentioned at the beginning of the show the Windows 10 Fall Creators Update adding something called TruePlay.  And I thought, what?  Anti-game cheating technology.  And I kind of grumbled to myself, "Because Microsoft had some spare time and nothing else to do?"  But apparently this is a thing, and I'm sure you probably know about it, Leo; and I'm sure Paul does.  I guess that, for example, something, the online gaming, for example, Grand Theft Auto, there are services, subscription-based services that provide the ability of players to do all kinds of cheats of the game, providing endless cash, teleporting...



LEO:  Huge problem, yeah.



STEVE:  Yes, teleporting them to arbitrary places, becoming invulnerable, walking through walls and so forth.  So it turns out...



LEO:  It's not so much a problem in single-player games.



STEVE:  Right, because you're just cheating yourself.



LEO:  Yeah, I mean, those are God Mode.  So somebody cheating in GTA, who cares?  But on online games, that's a pain in the tuchus. 



STEVE:  Right, right.  So it turns out that developers who went to the effort could increase the isolation of their game, that is, their existing technologies in Windows for creating better process isolation to prevent most of this from happening.  Microsoft has decided to incorporate that, essentially submerge that into the OS.



So there's a technology in Windows apps known as a manifest, which is a file that a developer can create - I have one for SQRL, for example - that declares some things to the OS about this application that it's getting ready to run.  So it is bound into the executable.  The whole thing is signed so that it can't be changed.  And when Windows loads the EXE, the executable, it looks at the manifest to learn things like what level of UI it's familiar with, how much security it wants, and various things.



Well, a new item, a new entry in this manifest can be the declaration that it wants TruePlay enhanced isolation.  And so Windows 10, I guess it's now in beta, but it'll be - I mean this feature TruePlay is in beta.  It will be incorporated into the Fall Creators Update.  It will recognize this manifest and automatically sequester this app from other cheats which would otherwise be able to get into it.



So from a security standpoint it's kind of cool because it makes it very simple for developers not to have to keep reinventing and reimplementing the same technology, but simply to say to Microsoft, "We would like this to be a protective process running on the platform."  And the other cool thing is that then, as Microsoft finds failures in their protection, that is, as the attackers work around what Microsoft has done, Microsoft can evolve the protection to match that and, as a consequence, all of the protected gaming technologies, or anything that wants to run in the TruePlay sandbox, essentially, gets the advantage of that.  So a nice advance from a security standpoint, too.  And solving a problem for online game players.



And speaking of solving a problem, Rich Williams in Daphne, Alabama, just yesterday on the 23rd, sent a note titled "One More SpinRite," and he meant success.  He said:  "Hi, Steve.  I know you receive a lot of feedback and have discussed SpinRite and its ability to repair many storage devices.  But I ran into something last week you and listeners may want to know."  And actually we have discussed this a couple times just coincidentally with Father Robert when he was on the show because the idea of SpinRite fixing smartphones happened to come up, and Father Robert had actually done that on one of his.



He said:  "My Samsung S7, which runs Android, has been having a problem recently where I would regularly be notified that my SD card was encrypted."  It turns out, he says:  "This is normal at boot, and everything seemed to run fine, so I didn't worry about it at first.  But after some research to stop the annoyance, I found that it was being caused by an unmount/remount operation happening when Android failed to read the card."  So there was beginning to be some flakiness, he's saying, essentially, in the card that was causing Android to react badly to a read problem.



He said:  "I store my podcasts, pictures, and videos on it, and it holds 250GB so I can't afford to start losing data gradually.  Needless to say," he wrote, "I pulled out my copy of SpinRite.  And after it completed running on Level 2" - which is what you want to run on solid-state media - "with no errors, it's been three days now and no more alerts.  Thank you for a great product and for the work you do on Security Now!."



So I would call that a refresh operation.  Much as we used to do on hard disks to just refresh the low-level format, this running SpinRite on an SD card that's beginning to get flaky refreshes the data, strengthens it, and actually we're not writing, remember.  Level 2 is just a read.  But the card's controller is watching.  And SpinRite does flip some switches that say we're doing maintenance things now, so pay attention here.  And when the SD controller sees that the card had trouble, it says, ooh, and will then do a rewrite only of those regions which need repair.



And that's why SpinRite says everything was fine because this was all down a level below the data access where the actual data, where the bits are being stored and the controller interacts and just reports, okay, everything was fine.  And then behind the scenes it makes sure that stays that way by rewriting the troubled data.  So another example of SpinRite's future on solid-state drives, although it sure doesn't look like spinning drives are going away anytime soon.  They just keep doing - the cost per bit just keeps falling, or per byte, and with no end in sight.



Looking at our time, I'm going to skip our feedback because we're coming up on two hours here, and I want to talk about Flash IoT Bots.  So, okay.  As I said at the top of the show, Mirai - this is literally, like, a few days ago, the one-year anniversary of the discovery of Mirai.  And as we know, it was about 100,000 devices that were sufficient to distributed denial of service, DDoS attack the DynDNS service.  That botnet pushed DynDNS off the 'Net and held it off long enough for the other DNS servers who had made a request some time ago for their caches of those requests to expire.  They went back to try to refresh the cache with updated IP address information, if any, and DynDNS was off.



So after a while those second-tier DNS servers removed the expired DNS data from the cache.  And now anyone asking that DNS server for the IP address of a domain that it had to go to ask DynDNS for got a "Sorry, we don't know that domain."  And so major sites, surprisingly major sites, were pushed off the 'Net by this attack on one DNS provider.  So that was with 5% of the number of bots that are now under control of a botnet that was discovered a week ago.  Because it was independently found by two different security firms, they each gave it their own name.  One called it the "IoT Reaper" because it is reaping IoT devices.  The other called it, having some fun with the IoT initials, called it "IoTroop" because it is that, too, a troop of IoT devices, although it's not...



LEO:  I like Reaper.  I think that's best.  Reaper's good.



STEVE:  The Reaper, yes.  So as we'll recall when we discussed this a year ago, Mirai - god, I can't believe it's been a year, Leo.  It seems like it was just the other day.



LEO:  I know.



STEVE:  Like where did a year go?  Mirai was doing telnet scans, scanning the 'Net, port 21?  23?  I've forgotten what telnet is.



LEO:  Is it telnet?  Yeah, 21.



STEVE:  Yeah, 21.  Scanning the 'Net for responses on 21, and then it had a list of default or weak admin WAN logon, admin credential logons.  And it would just guess usernames and passwords.  And for a frightening number of cameras and DVRs that were exposed on the Internet with their telnet port flapping in the breeze, the Mirai botnet was able to get in.



Now, a botnet which itself is a scanner is frightening because that's where the scale comes from.  That is, one gets launched.  It starts looking around for others.  And remember the old days of the worms on the Internet, where they just, like Code Red and Nimda and MSBlast, these things explode onto the Internet because each one that it gets infected starts looking for others to infect.  And so you get this exponential increase.  Now, that's also been recognized in the past as...



LEO:  Port 23, sorry.  FTP is 21.  I confuse my canonical ports.



STEVE:  Oh, 22.  That's right, FTP is 21.



LEO:  Telnet's 23.  SSH is 22.



STEVE:  23, there we go.



LEO:  It's so confusing, 23.



STEVE:  That's what I thought, 23.



LEO:  You were right.



STEVE:  So you get this exponential growth.  Now, this botnet is deliberately - it recognizes - the designer, the author recognized that there were going to be a lot of these, and so the scanning per is kept on the DL.  These things do not aggressively scan, probably to keep people from thinking, what the hell is going on with my bandwidth, if you had one of these that were infected.



Okay.  So here's the difference.  Mirai was looking for open telnet, guessing usernames and passwords using simple ones or default ones.  This nasty bot is using known weaknesses in existing products, many weaknesses known for years.  And they're in D-Link, Netgear, Linksys routers; GoAhead, JAWS, and AVTECH cameras; and a Vacron Network Video Recorder.  And this author is staying abreast.  In fact, in the show notes, second from the last page, Leo, I have a chart showing the nine vulnerabilities that are being used across these various devices.  However, Check Point also spotted the botnet attacking the MikroTik and the TP-Link routers, Synology NAS devices, and Linux servers.



LEO:  What?



STEVE:  So this thing is a - yes, it is aggressive.  And number five there on that chart, you can see where this Vacron NVR Remote Command Execution vulnerability was discovered on October 8th, and it appeared in the wild in use two days later.



LEO:  Oh, wow.



STEVE:  In this botnet.  Meaning that the author is watching the industry actively and immediately incorporating anything that is known.  And what's distressing, for example, is that the multiple vulnerabilities - number seven, multiple vulnerabilities in Linksys E1500/E2500.



LEO:  Look when that was discovered.



STEVE:  Yes.  February of - oh, no, I'm sorry.  Yes, in 2013.



LEO:  2013.



STEVE:  2013.  So more than four years old, and there are still Netgear.  These are...



LEO:  D-Link, too.  



STEVE:  Yes, yes.  And Netgear DGN devices, D-Link, yes.  And so here's the problem.  These are IoT.  They're appliances.  They're on the 'Net.  They're Internet-facing.  I mean, D-Link, Netgear, and Linksys, those are routers.  They're designed to be on the 'Net.  They've got longstanding known vulnerabilities.  Nobody ever patched them.  Now this botnet is finding them all and installing itself in them.  And we are now, we're at two million compromised devices, and it is growing at 10,000 devices per day.  It is going to find them all.



And there's a full Lua environment built into this thing, the full Lua execution environment, suggesting that the author intends to and can write very complex and efficient attack scripts.  And there are more than 100 Open DNS IPs in the firmware, meaning an Open DNS server, "open" meaning it will accept a query from anyone, rather than only like the customers of its own ISPs.  And of course that's what you want for DNS reflection attacks, where you spoof the IP of a query that will generate a much larger response.



And the unwitting DNS server will send the response to your attack target so that - can you imagine if more than two million of these well-connected routers on the Internet start sending queries off to someone that this person, just because they can, wanted to push off the Internet?  I can't, I mean, the bad news is we will be discussing in a week or two or three or whenever, because it's clear this is not just a research project.  If you've got 100 Open DNS server IPs built into this thing, the only thing they're good for is reflection attacks.



So we will, without question - a year ago it was DynDNS.  We don't know what this person has in plan.  But this is a very serious IoT net.  And I wanted to step back a little bit and just note that, you know, what are the characteristics here?  We've got devices with longstanding, in some cases more than four-year known vulnerabilities.  But people don't think of them as a computer.  They think of it as the box in the closet, the box on the shelf, the little piece of plastic they bought down at the electronics store for $49.  They hook it in and, oh, it seems to be working just fine.



Yes, and unfortunately there were problems that were found, and this thing isn't updating itself automatically, unlike the Ring products that you were just talking about.  Instead, you've got to go do something.  Nobody ever will.  So this is now an IoT device, sitting on the Internet, which is going to be found by this exponentially growing IoT Reaper flash botnet.  And how do you fix this?  How do you remediate this problem?  I just don't think you do.



And, I mean, we knew about these problems.  When I was doing the research, I ran across a posting on the Hacker News in early April 2014; right?  Early April 2014.  So just over 3.5 years ago.  It was titled "Millions of Vulnerable Routers Aiding Massive DNS Amplification DDoS Attacks."  Exactly this, 3.5 years ago.  Nothing happened.  So now something's going to happen, and we don't know what.



LEO:  Whew.  Wow.  



STEVE:  Yeah.  So we have a problem because we have an industry where an install base of non-self-updating devices which are vulnerable are sitting on the Internet and are going to become attack platforms for the future.  And I coined the term years ago, IBR, Internet Background Radiation, because there's still Code Red and Nimda packets out there from old Windows machines in the closet that are still out there hoping to get lucky.  They're not going to very often, but they're going to a little bit.  And, boy.  I mean, this is an ongoing problem.  I don't know how we solve this.  But the good - I was going to say the good news.  The bad news is it's going to take a catastrophe, and this is a potential catastrophe.



LEO:  Do you think a nation-state actor or individual?



STEVE:  No, there's been no claim.  I haven't read any supposition.  The IPs that are being used are known.  The botnet is generating candidates for infection and sending them to a command-and-control server.



LEO:  You could bring down the C&C server; right?



STEVE:  Except that new ones are being added constantly in order to scale.  I mean, just think about it.  Two million devices are phoning home to somebody.  You need a network of command-and-control just to ride herd over this herd.



LEO:  Yeah, yeah.



STEVE:  It's amazing.



LEO:  Wow.



STEVE:  You need a botnet just to control the botnet.



LEO:  It's amazing.



STEVE:  Yeah.



LEO:  And scary.



STEVE:  And, yes.  If this thing went peer-to-peer, if they moved away from a command-and-control model to a peer-to-peer model - and I hope I just didn't give the guy any ideas, although I don't think they need any ideas from me.  If they've put a Lua execution environment in there, this thing can support some code to do anything they want.  But if it switched into a peer-to-peer mode so that it became independent of command-and-control, then we've really got some problems.



LEO:  How would you control such a thing?



STEVE:  All you would have to know would be your own private access.  For example, you could send commands that were cryptographically signed by your private key.  Nobody reverse-engineering it could figure out what your private key was.  So nobody else could send their commands to your botnet in order to take it down.  And so you need a secret.  But once you have that secret, you could allow your bots to verify that commands are coming from an authority.  You can't have them talking to each other that way because then they would have to have a key that could be spoofed.  But the controller of this network could have a private key which is used to sign authenticated commands and then just spray them.  Just spray the commands.



LEO:  Or have them come and spread it automatically, peer-to-peer.  Just say, hey, if you get this command, make sure you tell your - it's like a cell structure or something.



STEVE:  Tell everybody you know, yes.



LEO:  Yeah, yeah.  Well, and that's of course in espionage what you use.  You use cells so that even taking out a cell doesn't take down the whole thing.  And no one cell knows more than two or three other participants.



STEVE:  Yup.



LEO:  I don't know why my mind's working this way.  I'm now thinking about how we would set this up, Steve.  But don't worry.



STEVE:  Any more good ideas, Leo?



LEO:  Don't worry.  I won't be - oh, as you say, I doubt very much we're telling people anything they don't already know.



STEVE:  No, no.  This guy...



LEO:  I really feel like these are - at some point, something this large, this is for taking down the grid or something.  I mean, the last time they used it to take down the PlayStation network.



STEVE:  Well, yes.  As I said at the top of the show, what could this do?  Anything it wants, literally.  Anything.  Two million.



LEO:  It could take down the whole Internet, I guess; right?



STEVE:  Two million bots doing whatever they want to do, yeah.



LEO:  You can't - the 13 root servers for the DNS system, those would be hard to attack.  I would imagine they're secured in a variety of ways.



STEVE:  Well, and there are only 13 logical, yes.



LEO:  Yeah, they're multi-routed, multi-owned.



STEVE:  Actually they're widely distributed, yes.  So it's only 13 IPs.  There's actually a ton of actual physical hardware.



LEO:  Right.  So it'd be hard to take that down. 



STEVE:  Yeah.



LEO:  Well, if it's North Korea, maybe they just go after Sony again, and then we can all rest easy.  Or it could be Elliot, and could be Stage 2 on E Corp, as somebody in the chatroom said.



STEVE:  I don't think we'll be waiting long to find out.



LEO:  It's Evil Corp's botnet.  Mr. Gibson, as always, you've scared us and amused us and reassured us all at the same time.  And I think the scare will last a little longer than the rest of it.  We will be back next week to do more of the same.  You'll find Steve...



STEVE:  If the world still exists, if the Internet still exists, yes.



LEO:  Wow.  It's really intriguing to think about.  



STEVE:  Yeah, I know, it's - yeah.



LEO:  How big was the old one, the Mirai?



STEVE:  100,000. 



LEO:  So this is...



STEVE:  Two million.



LEO:  This is 20 times bigger.



STEVE:  Growing at 10% of the old one per day.  They're adding 10,000 new devices per day.



LEO:  Oh, interesting.  Yeah, you don't build that for nothing.



STEVE:  No.



LEO:  Okay.  Well, thank you, Mr. G., for all your insights.



STEVE:  My pleasure.  Always happy to shine a little sunshine on something.



LEO:  You can find Steve at his website, GRC.com.  That's where you'll find audio and transcripts of this fine show, all 634 episodes.  You'll also find many of his fabulous programs, including of course SpinRite, the world's best hard drive maintenance and recovery utility.  You can find out more about SQRL, soon to be birthed into the world.  Also Perfect Paper Passwords and ShieldsUP! and DCOMbobulator and on and on and on.  He's done a lot of great projects.  His Healthy Sleep Formula.  There's all sorts of stuff.  It's fun.  It's like Grandma's attic or something.  You just want to browse around for a while.



STEVE:  And some of the things are that dusty.



LEO:  They're that old, yeah.  Like DCOMbobulator.  I don't know what made me think of that.  You can also find the show on our site, TWiT.tv/sn.  In fact, we have a link or links there to subscribe to all the favorite podcatchers, YouTube and all of that.  You can watch us live during the taping of the show.  We have an open studio policy.  Either come by and say hi, email tickets@twit.tv, or watch the live stream at TWiT.tv/live.  You can say "Echo, listen to TWiT Live," and it'll play it for you.



You can also join us in the chatroom, if you do, because that's the best way to talk, the kids in the back of the class, always talking around.  It's fun, and it's a great social network on its own of true geeks.  That's irc.twit.tv.  If you can't watch live, tweet live, chat live, you can always get on-demand audio and video after the fact at our website, TWiT.tv/sn or wherever you get your podcasts.  Make sure you subscribe.  This is one you want every episode of.  And don't forget you can reach Steve by tweeting him, @SGgrc.  He accepts DMs of any length.



STEVE:  I do.



LEO:  Thank you, Steve.  And we will see you next time.



STEVE:  Okay, my friend.  Back next week for 635.



LEO:  Yeah, baby.  Bye-bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#635

DATE:		October 31, 2017

TITLE:		Reaper Redux

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-635.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine the source of WannaCry, a new privacy feature for Firefox, Google's planned removal of HPKP, the idea of visual objects as a second factor, an iOS camera privacy concern, the CAPTCHA wars, a horrifying glimpse into a non-Net Neutrality world, the Coinhive DNS hijack, the new Bad Rabbit cryptomalware, a Win10 anti-cryptomalware security tip, spying vacuum cleaners, a new Amazon service, some loopback Q&A with our listeners, and another look at the Reaper botnet.



SHOW TEASE:  It's time for Security Now!, the Halloween edition.  Actually, every week is a scary show because we talk about security flaws.  This week we've got of course the wide range of things to talk about, but then Steve's going to talk in more detail about the Reaper worm that's making its way through mostly routers, millions of routers all over the world.  Actually, it's not yet a million, but it could be soon.  Steve explains why this thing is written better than most IoT devices.  That's coming up next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 635, recorded Tuesday, October 31st, 2017:  Reaper Redux.



It's time for Security Now!, the show where we get together with this guy right here and chat about security, privacy, the Internet, how computers work, anything on his mind - Steve Gibson.  It's really a rare opportunity just to sit down once a week and spend some time talking to Steve, and I always look forward to that.  Hi, Steve.



STEVE GIBSON:  Yeah, rare.  We're in Year 13, Leo, and this is Episode No. 635.



LEO:  Oh, I would do it every day if I could, so once a week seems rare to me.



STEVE:  And mostly what we talk about is the events of the previous week, that happened between the previous rare opportunity for us to talk and the current rare opportunity, which is pretty much guaranteed to happen since I think in the entire 13 or 12-plus years we've only missed one and got a lot of negative feedback for that.



LEO:  Not allowed to miss a show anymore, no.



STEVE:  Not going to do that again.  So No. 635 this week for Halloween, October 31st of 2017.  I titled this "Reaper Redux" because many other security firms have had a chance to weigh in on their own observations.  And whereas last week's initial reporting was maybe a little bit of hair-on-fire hysteria, we have now had a chance to examine it more, understand what Reaper is and, interestingly, is not doing to get a better sense for what the numbers are and what they mean.  Depending upon who you ask, the two million number was inflated, or maybe it's just pending and more.  So we're going to wrap up today's podcast by talking about that.



But we also have the attribution has finally landed for the WannaCry attacks from middle of May, earlier this year.  We've got a new privacy feature coming for Firefox from the Tor version of Firefox, which is not the first time that it's happened.  Google has announced they're removing an interesting high security feature from Chrome because it didn't really work out as well as they were hoping.



LEO:  Oh, yeah, I was hoping you'd talk about that, yeah.



STEVE:  Yeah, the HTTP public key pinning that they're going to be taking out.  I ignored this last week, although a bunch of our listeners sent me a note saying, hey, what do you think about this, the idea of using visual objects as a second factor for authentication.  So I want to sort of address that as this specific bit of news and also as a concept, and why I think it fails.



The guy who did that really interesting login spoofing demo for iOS that you and I talked about a couple weeks ago, where you just couldn't tell that it wasn't the OS asking for your credentials, it was a malicious app, Felix, he's done an interesting new blog post about a concern over iOS's camera privacy that I want to talk about.



We've got an interesting escalation of the CAPTCHA wars.  A horrifying glimpse into the world of non-Net Neutrality brought to us by an ISP in Portugal, where there is no notion of Net Neutrality.  And it's just like, oh, no, please don't let that happen here.  It's their page of services that they're selling.  We've also got the relatively high-profile DNS hijacking of Coinhive, which occurred for sad reasons.  You would hope that somebody like Coinhive would be doing their own security better than they were.



We've got a new cryptomalware has emerged known as "Bad Rabbit."  We've got Ed Bott bringing us a Windows 10 anti-cryptomalware security tip, a concern from the Israeli security firm whose name is escaping me right now, it'll come back, over spying vacuum cleaners.  LG, it turns out, had a problem with their whole home IoT system which would have allowed that.  A new Amazon service which is interesting, which is just beginning to happen, with some feedback from our listeners if we have any time.  And we will revisit the Reaper Botnet with the advantage of a week's more maturity in our understanding.  And of course a fun Picture of the Week.  So I think another great podcast for one of these rare episodes.



LEO:  As always.  You're the master of great podcasts, Steven.  All right.  Continue on.



STEVE:  So as we know, reliable attribution for cyberattacks is always problematical.  It's made difficult by the ability for attacks to be looped through or bounced off of remotely located innocent machines.  And, of course, I mean, it's a constant, it's a staple of detective novels, and you see it on TV all the time.  Oh, they bounced it through 13 different countries before it arrived so we don't really know where the bad guy is.  I mean, and that sounds questionable, but of course we know it's absolutely true.  It happens.  So this makes determining where things came from difficult.  But careful forensic research and reverse-engineering of samples of the attack tools which are caught often reveals some clues.  And we've talked about those over the years.



In the case of WannaCry, which as we know was a devastating cyberattack which leveraged a flaw in Windows SMBv1 which erroneously we believed it was affecting Windows XP, but it turns out that a later more careful analysis showed that a lot of Windows 7 machines and Server 2008, which is the equivalent, were also affected.  So the U.K. and other nations have arrived at a consensus that the cyberattacks which we've now labeled as WannaCry were sourced by North Korea.  North Korea is known to have about a 6,000-strong cyber hacker group which is doing these things.  And there was no attribution until just recently where finally, after the analysis was done, it's like, okay, we've got to hang this one on North Korea.



So although the SMB attacks are still occurring, we're about to talk about, a little bit later, Bad Rabbit, which is using a different version of an attack which escaped from the NSA's control.  WannaCry has been patched from Microsoft even in Windows XP, which surprised a lot of people.  But it generated so much negative press for them that they had to do it.  So but we know that it's very different to have a patch available and a patch applied.  Which is why the point you made, for example, Leo, about the light.house taking responsibility for its own firmware, that's going to end up immediately being required behavior for IoT devices, much like the Ring Doorbell does and other things are.



For example, we'll be talking about - I'm blanking on the name - the Reaper and the fact that it is functioning by commandeering a whole bunch of routers which are high power, more power all the time as they become more capable.  But typically routers are not auto updating.  They offer you the option, but you've got to log into the management interface and say Check for Updates.  That's not something they do autonomously, and that behavior has to change.



There has to be some means where the router figures out, okay, even though it means I'm going to have to go offline for a while - which is probably why they don't do that is it brings your whole network down and is very disruptive.  Or maybe they need to come up with a way of being able to load another version of their firmware and then do a less interruptive update in order to bring a new kernel online.  So anyway, some way this problem has to happen.  So anyway, we now know with strong certainty, as it was stated, that North Korea were the perpetrators of this WannaCry attack, which made a lot of press because it was so damaging.  And it was a cryptocurrency attack that was requiring ransom payment of bitcoin in order to get things decrypted.



In Firefox v58, which I think it's in January, yes, January 16th of 2018, so a few months from now, middle of January, we will be getting, in the normal public standard version of Firefox, a feature which has been four years in coming because it was noted as would be useful from the Tor version of Firefox.  The Tor browser is an ESR, the ESR version of the normal Firefox mainstream, where they've taken that and added a number of privacy enhancements for packaging as part of the Tor solution, The Onion Router that is all about protecting your privacy.  And we've also talked often about the problem of browser fingerprinting.



And I know, Leo, you've been talking about it on other podcasts recently.  I mean, it is a big problem.  But there are so many things about a browser which script running in the browser is able to send back to a site that wants to identify your browser uniquely.  Traditionally it's been cookies.  Cookies are a little controversial, of course, because especially in the EU we're beginning to see privacy requirements where sites have to disclose when they are just using cookies even the way they were intended to, just to create a persistent session on a first-party basis with their user.  So we're often now seeing sites bringing up a little banner that you are asked to click Okay on, just to acknowledge that you accept that this site is using cookies.



So what's interesting about fingerprinting is it sidesteps those EU privacy concerns because they're not storing something that the server has given your browser to return, which is what a cookie is.  Rather it's just a "passive," passive query of existing information about your browser.  For example, it turns out that one of the things that tends to be very unique from one browser to another, even the same browser make and model on the same OS, is what particular plugins that the particular browser's user has chosen.  I know I have a set that I use, and we've talked about them, things like uBlock Origin.



I have a YouTube downloader that I sometimes use.  I have something that does a nice job of capturing screenshots, where even if the page scrolls off of the screen, this thing somehow, it really works well, is able to give me a screenshot of the entire scrolling page.  I also use a session manager for Firefox, and of course I like tabs, so I've got the tab styling plugin.  The point is that it's probably the case that nobody else maybe in the world has exactly my same exact set of chosen browser add-ons.  Well, the browser add-ons, the plugins, is something that script running in your browser is able to enumerate.  And so it provides a passive, yet unique, fingerprint for me, sadly.  I mean, I'm not happy about it, but it does.



Another thing which tends to be sort of system specific are the fonts which your particular system has installed.  And I know, for example, that over time I'll say, oh, I like that font, or I like this font.  And so they tend to accumulate.  And so while they may vary a little bit, they also give a unique snapshot, a unique fingerprint into a specific system.



Now, as it happens, in the interest of privacy, Firefox 52 blocked the enumeration of system fonts by JavaScript because it's really kind of hard to make a convincing use case for why script running in your browser needs to rummage around in your system fonts in order to get a complete list.  The way the DOM works and the way CSS works is you're able in the CSS code to list in sequence of preference the font families that you would like the CSS and the browser to use and leave that up to the browser.  Yeah, okay.  Maybe it's interesting to make it more automated where the script is able to see what you've got and then rewrite the DOM on the fly.  But we could live without that.  And because system fonts are - it's hard to make a case for that.  Firefox 52 blocked that.



So Tor four years ago blocked something else which is also very powerful.  And that is, and it's something we've talked about in years past, the "canvas."  The canvas is a drawing surface which I have played with.  If you go to GRC.com/animation.htm, you will see a piece of my work.  That is the canvas.  That is JavaScript which I wrote using the canvas to create an animation of the way a hard drive converts data bits into flux reversals, and the way those are then picked up by the read head and turned back into data bits.  And so that is a 640-by-480 canvas where JavaScript is animating that in order to make that happen.



Well, what happens is it turns out that you can draw on the canvas with script, and then you can scan the pixels of the canvas to get the RGB values of each individual pixel.  And it turns out that that's something super valuable for fingerprinting.  As a consequence of browsers rendering details of text and lines slightly differently, like just the details of if the X and Y coordinates are even and odd, and the line is at a certain rate, how is the anti-aliasing performed where the line below begins to pick up some of the color from the line above as the line slowly crosses the raster scan, or the way the TrueType settings have been used in order to use the RGBness to anti-alias the actual pixels of text



And in the show notes here, I have samples from some of the coverage of this of the capital letter "T" rendered in different browsers on the same machine on exactly the same web content.  So that "T" was being displayed essentially by the same page on two different browsers.  And if you look closely, you can see slight differences in the coloration.  The very first upper left pixel of the left-hand "T" is a little pink, where it's not at all on the right-hand "T."  And in fact the whole left-hand bar of the left "T" is red, where it's more blue on the right-hand "T." So the point is that, if you were to render the two T's and then pull out the RGB and just hash the result, you don't care that it's a "T," you just hash the result, what you will get is a unique hash which is able to discriminate between those two browsers.



And it turns out that the OS, the GPU, if it's used for acceleration, the graphics driver, the display adapter, all the various things get involved in this and end up producing another unique fingerprintable thing which is not data being given to the browser, but just something that it's possible for script to read which is always unique.  And the Tor browser said, hold on here.  There is no good reason why JavaScript needs to read pixels from an offscreen canvas, which is what's being done is that a canvas is set up that you don't even see, where something is rendered, and then it is scanned and then sent back.  And so the version of Firefox which the Tor guys took and then modified shut that down four years ago.  That was just yesterday logged as a bug fix, which is just the terminology they use in their bug tracking, which we will see in Firefox 58 as a further advancement in Firefox privacy.



So that's something which is on the rise, that is, the use of browser fingerprinting is on the rise because it avoids the latest privacy legislation, which is beginning to shut down the  traditional means of tracking people, and so they're switching to passive fingerprinting.  And the good news is Firefox is beginning to sort of profile itself more as the privacy-protecting browser.  It'll be interesting to see whether we see Chrome and Google following over time, moving in that same direction.  We may see some differentiation between these two browsers over time, if Firefox continues in that direction and Google decides, well, we're going to focus on performance and security and not do anything to thwart tracking and profiling.  We'll see.



But speaking of Google and Chrome, it turns out that not all RFCs end up gaining traction and being a good thing.  There's a technology that we've touched on, but I've never really talked about too much because it always seemed problematical to me, and it looks like that's what the industry has decided.  It will be leaving Chrome.  It has been supported.  It will be leaving Chrome early next summer, in May of 2018.  And it's known as Public Key Pinning, or HTTP Public Key Pinning, or sometimes just by its initials, HPKP.  And it's ended up being problematical.  It uses a fun acronym, just because it sounds good, and that's TOFU, which is Trust On First Use, T-O-F-U.



The idea is that a website could offer in its reply headers some public key pins, meaning "pin" as in "pinning," meaning the SHA-256 signatures of the certificates that it uses, its own TLS certificates.  So the idea is that this would be a means for a website to supply compliant browsers, that is, HPKP browsers, with the upfront knowledge of these are the signatures of the certificates that we use.  Such a browser would then honor that assertion absolutely.  And that's the problem, is if anything happens to break the validity of that assertion, then a compliant browser will refuse to show you the site, period.  That is, the problem is that in order for this to be useful, because it's going to be received the first time a browser goes to a site, and cannot be replaced because if it could be replaced with an update, then that's what an attacker would do.



So the only way these signatures get accepted is if there's a blank space for that domain in the browser.  And once that blank space has been filled with the signatures, nothing can change them.  Now, they do expire.  But there again the max age has to be long, otherwise again they're not really of much use because, if they're expiring all the time, then they may have expired when a bad guy has performed a DNS attack or arranged to be a man in the middle or whatever, in which case again they're of no use.  So they're only of use if they have a really long expiration date.



But while they are in place, you cannot change your certificate under any, I mean, you can't.  There's no way to change this.  And so it's like, yes, it's really good security until it's really bad security.  And really bad means no one who's using a compliant browser who's ever visited you before can go to your site if your certificate's changed.  And we know that, first of all, certificates expire, and a new certificate has to have a new signature by definition.  There's all kinds of time and date stamps in certificates that is going to force a new certificate to be and to have a new signature.  And we do know that sites sometimes have their certificates expire without them knowing it.  So to use this mechanism responsibly and not have it bite you in the butt, it means you have to be very careful and cognizant and preemptive about bringing the expiration age down in anticipation of a planned whole Internet revamp of any PINs that exist in browsers.



And anyway, so what has ended up happening is it's too brittle.  It's too fragile.  It's too error prone.  It was a good idea, but it was more trouble than it was worth.  And it is going away.  So not all ideas that surface as RFCs end up being a good idea, and this one is dying.  It will be leaving Chrome.  And it turns out it never really gained much traction.  I remember seeing, as I was doing some research into this, something like 0.04% of the top bunch of Alexa's top million sites or something were using this.  I mean, it just - you really have to have a need.  And, boy, you've got to be responsible if you're not going to end up just creating your own denial-of-service attack on your own site by needing to change your certificates, but having pins not expired in browsers that are compliant.  So, yeah, steer clear of that.



I mentioned that last week a number of people were asking me about this idea of an image as a second factor.  This was a research paper that drove this, and the Verge has some coverage of it that I have a link to in the show notes, if anyone's interested.  It was never meant to be implemented.  It was just them experimenting.  And so the idea is that, in their coverage and example, that somebody might have some fancy Aztec bracelet with lots of detail and some rhinestones or aquamarines or whatever, and they would use that as their second factor.  So when in this concept, when something needed you to authenticate yourself, you'd go, oh, yes, here's my second factor.  And so you'd use your smartphone to show it this bracelet that you're wearing, or whatever.  Maybe you've got a particular wristwatch that you always have on your right or left wrist.



And so, again, you'd show your phone this wristwatch.  And the idea is that, through some process of feature extraction, this optical second-factor technology would decide somehow that, okay, maybe it somehow turns it into a hash.  But the problem is you're not going to be holding the image exactly the same distance the two times.  I mean, we all know the image is not going to be the same.  Focus is going to be coming in and out, as it tends to.  There's going to be some image stabilization problem because you're just holding your hands up.  It's not going to be the same size, the same orientation and so forth.  So there are some big hurdles to overcome in turning this into something like a hash.  And you don't want it to just be a go-no-go, like is this the same thing I've seen before, because that wouldn't be secure.



But the biggest problem in my mind is replay attacks because nothing about this is replay attack-proof.  That is, if something like a hash is being sent to, like as your second factor, then it's by definition not going to change.  It wouldn't be known to an attacker initially.  But if they captured it, then in the same way that a password is inherently static, the problem with a password is that it's inherently static.  It's your password.



And so the problem people have is in repeating their password properly because it is a static thing.  So if this is just a repeated hash of some feature extraction of an image, then that's just like a second password, which is better than only one password, but it's not nearly as robust, for example, as the existing infrastructure that we already have in place, which is a time-based one-time password, which is by its nature, and the reason it's time-based, is that it is replay-proof, the idea being that no server will accept the same token twice, and the token changes every 30 seconds.  So that's much better than saying, oh, yes, here's something that I showed you before, and I'm showing it to you again; right?  It's like, okay, but so was a password.  And this just seems way fuzzier and less robust to me.



And the other - so a time-based one-time password is better because it's replay-proof.  The problem it has, as we know, is that it does require that the server hold a secret, that is, there's a shared secret which does not go over the wire, but it is shared, and which is what allows the server to know the proper answer to the time-based one-time password.  So your authenticator has the secret.  The server has the secret.  In fact, remember that the way these things work is the server gave you the secret, typically in the form of a QR code, which your authenticator then captured so that now you're sharing the secret.



So the problem there is that, if the server loses its secret, then that could compromise all of its customer base's one-time passwords - which, remember, is exactly what happened with the RSA breach back in 2011, where all of their RSA SecurID product line, which are six-digit time-based passwords, was compromised because that was the huge concern was, if their secret key database escaped, and in this case it did, that was bad news.  So that's why the one step further is what the state-of-the-art authenticators use, like my own SQRL system, where we use a public key challenge/response system, where the server has the public key, not a shared private key.



And then the way the system works, the way you get replay protection is that the server generates a nonce, a unique challenge which it sends to the authenticator, which the authenticator signs using its private key; and then it returns the nonce and the signature and the server verifies, yes, that's the nonce I sent.  And, oh, look, it was signed properly, which it's able to verify with the public key.  That's the next generation which we are just in the process of beginning to move to.  So that sort of creates a hierarchy of authentication quality.  And unfortunately, interesting as the idea of showing your phone something, it doesn't have any of these characteristics that are replay-proof and ultimately proof against server-side compromise, which we really do want in this day and age.



LEO:  On we go.  Do you want to do the picture now?



STEVE:  Yeah, absolutely.



LEO:  It's a very good one.  Let me scroll back up here a little bit.



STEVE:  Yeah.  I'm tempted to wonder whether it's true or a setup.  But...



LEO:  Yeah, it could be a setup; right?



STEVE:  ...it is fun.  And, yes.



LEO:  I think it's a little old, too, just judging by the reaction from the chatroom.  I think a few of them have seen this.



STEVE:  Ah, okay, interesting.  So, well, we know that, at least based on the photography, it involves an iPhone, so it's not - so what's that, it's been 10 years now?



LEO:  No more than 10 years, yeah.



STEVE:  Yeah.  So anyway, it shows a car's stereo system, and the iPhone pushed into the docking slot of the stereo system, with the caption:  "My brother was upset because his car's 'docking station' for his iPhone wasn't working, and it was scratching his screen."  And those old-timers among us will remember the days when we had cassette players.  And it turns out that the iPhone is pretty much the same profile as the side of a cassette, yes.  So not the eight-track big crazy cartridges, but the cassette tapes.  And if we are to believe this, somebody thought, oh, look, a docking station for my iPhone.



LEO:  Fits perfectly.  Fits perfectly.



STEVE:  Yeah, that's right.



LEO:  You might wonder why [crosstalk].



STEVE:  I'm not sure what happens if you hit fast-forward on the iPhone.  Anyway, fun picture.  So our friend Felix Krause, who has been sort of working at improving iOS security by poking at it, as researchers do, noted some concerns over the way iOS handles the camera.  And our takeaway from this is it's worthwhile periodically doing a little privacy audit of your iPhone or iOS device - meaning I guess iPods and maybe, well, iPads - privacy settings.  Under Privacy is Camera.  And you're able to see which applications, over time, you have ever given permission to use the camera.  The reason an occasional audit is useful is that those things never turn themselves off, so they tend to accrue over time.  And you might find applications that you kind of wonder, wait a minute, why does that have access to my camera?



So the point is what Felix noted is that once any app has ever been granted access to your iOS device's cameras, it can access both the front and the back camera, record what it sees anytime that app is in the foreground, take pictures and videos without notifying you in any way, do anything it wants to do with that content, upload the pictures and videos it takes immediately, perform face recognition or detect facial features and expressions.  iOS 11 now builds some feature extraction, facial feature extraction into the underlying framework.  And all of that without indicating in any way that your phone is recording you taking pictures, taking videos of you and your surroundings.  No LEDs, no lights are shown, no indication of any kind.  Which is not to say that apps are doing that, or that they would be doing so maliciously.  And they can only do so when they have the foreground, when you're using the app.



But if you just - I think it's probably worth looking through, doing a little audit under Privacy > Camera, under your iOS device settings, just to see whether you still think it's a good idea for all of the apps that currently have that capability to keep them.  And of course there are switches there.  You can just flip them off, so to speak, if you decide that you don't want all of the apps that have access to your camera to retain that right because, as Felix notes, you are relying on the proper behavior of the application to which you have given - which if you ever gave it access, for example, sometimes an app may just want to, like during setup, acquire access to the camera to take a picture of you in order to establish an icon or an avatar for you.  But then there's really no need for it to have enduring access, but by default it keeps it unless you say you don't need that anymore.  So again, probably worth just keeping that in mind, just from a privacy standpoint.



LEO:  You could do a little audit in iOS.  You can go through.  There's a permission section, and I do this periodically, go through all the permissions and turn off ones you don't think are necessary because, you're right, could just be let's - Snapchat does that.  Let's just get a picture, well, I guess Snapchat continues to need access.  But there are other apps that do that.  And let's just get a picture of you or, hey, let me scan the QR code, that kind of thing.  Yeah, don't, yeah - revoke it.



STEVE:  And I actually feel that way also about location services.



LEO:  Same thing, yeah, yeah.



STEVE:  Yes.  And as we know, the location system in our iDevices uses a lot of power.  It's a big battery drain.  And if you look through location services, it's like, what the heck?  I mean, I want Google Maps, and I want Maps, and that's about it.  But you'd be surprised how much crap in there wants to know where you are.  And it's like, it's none of your business where I am.  So location services is another very worthwhile thing to turn off.  And you can say - there's never, always, and only when I'm using.



And so, for example, I have everything set for never except a couple of mapping apps where I have them only when I'm using them.  And what you'll find is you can recover an awful lot of battery life just by keeping things from just being able to monitor where you are all the time.  That's just - it's amazing.  And of course Apple's always trying to turn that back on.  And when you turn it off it's like, oh, my god, oh, oh.  Not everything is going to be able to know where you are.  It's like, yeah, it's none of their business.  So anyway, another thing worth auditing, I think.



One of the problems we have, as we know, with the 'Net is bots and botnets.  And on one hand it is super powerful to have them because, for example, Google revolutionized finding stuff on the Internet by turning bots loose, spiders, to crawl around the Internet and find everything and then index it.  So that's super useful.  But, unfortunately, with all the good comes the bad.  And we know that there are bots trying to get up to all kinds of mischief, creating accounts on sites.  For example, we've talked about how bots have abused Ticketmaster, and so Ticketmaster has had to come up with ways to prevent bots from buying tickets and then reselling them because that isn't what they intend.  They want humans to do that, not automation.  And then there's bots that scrape sites and copy their content.  I know that Craig's List has anti-bot scraping technology because, again, they want people to be viewing these pages, not automation to be abusing their intent.



So as we know, CAPTCHAs are the result of our attempt to determine what is a human and what is a bot.  And the CAPTCHA itself is an abbreviation for Completely Automated Public Turing test to tell Computers and Humans Apart, CAPTCHA.  And we've talked about CAPTCHAs and their evolution for years and how, for example, now Google is using much more of the intelligence they have about the history of you and your computer and your IP and the Google cookie that you're carrying so that you don't even have to solve a test anymore.  You just have to click on the "Yes, I'm Not a Robot," and they think about that for a minute or two, and they say, okay, yeah, we agree you're not.  But when they're not sure, then they will still give you some problem to solve, some text that you have to recognize.



If you have a problem with eyesight, then there's an audio option that you're able to use as part of Google's so-called "reCAPTCHA" system.  Well, just in the last week some research has come to light where two different groups have managed to, with a high degree of success, spoof the state-of-the-art reCAPTCHA system.



One group has named their solution unCAPTCHA, and they've taken six different speech recognition systems - the Bing Speech Recognition, IBM's, Google Cloud's, the Google Speech Recognition which is distinct from Google Cloud, the Sphinx system, and the Wit-AI.  They feed the audio puzzle from the audio flavor of Google's reCAPTCHA through all six of those and then aggregate the results and feed the most likely consensus answer back to Google with 85% success and far higher speed.  In tests that they performed, their experimental system was able to break 450 reCAPTCHA challenges, with 85.15% accuracy, in a little less than 5.5 seconds, 5.42 seconds.  Which is less time than one person could listen to and respond to a single reCAPTCHA audio challenge.  They did it 450 times with 85% accuracy.  So that's the audio side.



There's a different group of researchers who tackled the same visual CAPTCHA challenge.  They don't have quite the level of success, but they managed to develop an AI vision bot that is able to break various CAPTCHA systems with varying degrees.  Google's reCAPTCHAs where you say, okay, you're not a bot,  Google's not sure, so then they give you the visual challenge?  Their AI vision bot is now up to two thirds, 66.6% accuracy, it's able to solve the optical CAPTCHA.  There's another type of CAPTCHA known as BotDetect which they can do with 64.4.  Yahoo's seems to be tougher.  That's at 57.4.  And PayPal is about the same.  Their image challenge they're able to solve at 57.1.



So what we're seeing, and this is I guess foreseeable, is that bots happened.  We came up with a solution instead of puzzles that would stump the bots.  That worked for a while.  Then researchers said, okay, let's come up with more human solutions using existing technology and AI in order to push this bar further.  And they have, to the point where unfortunately it looks a little bit like we're beginning to lose that battle for this kind of puzzle.  Yet I really do like what Google has done where in the general case they're not presenting any puzzles.  They're just saying, based on our aggregate analysis of your past behavior, we already know you're not a bot.  So just click the "I'm not," and we'll move on.



LEO:  Although relevant to your earlier discussion, if they're not sure, they'll give you pictures.



STEVE:  Right.



LEO:  Right?  I always get the storefront.  Is this a storefront?  Click the ones that are storefronts.  I guess that's hard for a computer.



STEVE:  Oh, that's interesting, yeah.  I haven't seen that one.



LEO:  You don't get that one?



STEVE:  No, well, for whatever reason Google's happy with me.



LEO:  It always knows it's you, huh?



STEVE:  I don't move around much.  Okay, now, Leo, look at this next picture.  You're going to want to put this on the screen, too.  This is just horrifying.  This is an actual web page from a Portuguese ISP where Net Neutrality doesn't exist, showing how their customers are able to purchase which types of bandwidth they want access to.  Messaging, and we see the icon for Skype and FaceTime and iMessage and a few others.  And that's for, looks like, what, 4,99 euros per month, I guess?  So that's the Messaging package.  Or then you can also get the social engineering - the social engineering - the Social package, where we have Facebook and Twitter and Pinterest and LinkedIn and a few others, also for 4,99 euros per month.  Or the Video package, where you get access to YouTube and Netflix and a couple others; or the Music package; or the Email & Cloud.



And anyway, the point is that in this scenario, this has all been broken apart, and unfortunately it's reminiscent of what we're subjected to with cable TV providers these days, where you choose which things you want, not a la carte, but in lumps, and always end up with things that you're paying for apparently but not using.  And anyway, I just thought this was sort of interesting.  A bunch of our listeners sent this to me, and I thought, yeah, this is not the world we want to live in.



LEO:  This is a wireless provider, I'm guessing, because of the 10GB cap.



STEVE:  Right.



LEO:  That's weird.



STEVE:  Ugh, yeah.  Yikes.



LEO:  Well, here's the question, which is not clear.  Do they block these services unless you pay for them?  Or does it, I mean, I wonder what they're offering here?  Maybe someone from Portugal could tell us.



STEVE:  Right, who's able to read this.  I assume what this means is that you pay them if you want access to these services.  And so, yes, if you don't buy those, you don't get them.



LEO:  That's just terrible.



STEVE:  I know.  I know.



LEO:  That's just terrible.  Or is this offering zero rating?  Which it kind of feels like it might be?  In other words, if you pay for the video package, then none of the bandwidth you use for Netflix or YouTube or Periscope or Twitch goes up against your bandwidth.  That's kind of what I'm getting from my imperfect Portuguese, is it looks like this is saying, if you want this - you still get access.  But if you don't want it to count against your cap, you pay for it.



STEVE:  And in that move above it says something "Pago Unlimited."



LEO:  Yeah, yeah.  That's my guess.  But I don't [crosstalk].



STEVE:  Well, let's just hope this doesn't ever happen to us because I just want to pay Cox for my connection, and I am super happy with it.  Just let me do whatever I want to with it.



LEO:  Would it be okay with you if ISPs just had speed tiers?  Or, conversely...



STEVE:  Well, they do now.



LEO:  They do now.



STEVE:  They do now, yeah.



LEO:  Or, conversely, if they had, like speed tiers, but bandwidth tiers?  Like if you want - how much bandwidth do you want?  What if you just - in fact, what if you just paid by the byte?



STEVE:  The way we buy electricity.  The way we buy electricity.



LEO:  Yeah.  That would be okay, wouldn't it?



STEVE:  The more we use - I think it would because, well, but...



LEO:  Because then everybody's equal.



STEVE:  Yes.  And I tend to be a low bandwidth user.  I'm experimenting with cord cutting now for the first time in my life.  But until then, I was just using cable TV.  And so...



LEO:  But even if you use, I mean, that seems more fair.  Like if I'm 100GB or a terabyte a month user, it would make sense for me to pay a little more.  Maybe give us all high speed.  Or I guess you could still tier the speeds.  And the main point here is access is equal.  So you're paying the same amount per byte for Netflix that you'd pay for YouTube, that you'd pay for Skype.



STEVE:  And really, why shouldn't somebody who is downloading massive...



LEO:  Pay more.



STEVE:  ...tens of terabytes a month - they should pay more.  



LEO:  That's not against Net Neutrality because that is completely neutral.



STEVE:  Correct.



LEO:  It seems fair in the sense that you're paying for what you use, just like electricity or water.  It's more like a utility.  By the way, somebody in the chatroom, Dave, says he's Portuguese, and that's exactly what this is.  You pay for zero rating for these services.  So you pay 4,99 euros a month to get...



STEVE:  And then you're not billed.



LEO:  ...unlimited, you're never billed for video.  So if you watch a lot of Netflix, pay an extra five euros a month, that seems not too horrible.  But again, I prefer...  



STEVE:  It does mean that, yes, that does mean that they are getting involved in what you do.



LEO:  Right, the zero rating, which we don't like because I'm not on - TWiT's not on that list.  So you might be more likely to watch YouTube than TWiT because, hey, it doesn't cost me anything to use YouTube.



STEVE:  Ah, right.



LEO:  It might cost me something on TWiT.  That why it seems to me paying per byte is the way we should work this out.



STEVE:  Yeah.  I completely agree.  Absolutely.  I think that's, I mean, it just seems fair.  And it fits the - you know, you want models that make sense.  And so if your provider is going to run bigger pipes and fiber and so forth in order to be able to deliver, to beef up their own infrastructure, then they want remuneration.  And it should be the people who are requiring them to make that investment be the ones who pay for it.



LEO:  Yeah.  It would still impact TWiT, of course, because say we have very large files.  There'd be an actual cost for each show that you watch.  



STEVE:  Well, and I wouldn't be surprised also if there isn't a quantity discount.  That is, the more you - so there is a tiering system...



LEO:  As there should be, yeah.



LEO:  ...where big users - yes.  And there's that way with high power users, too.  Like with big electric users.



STEVE:  Oh, man, we get right up in that top tier every month.  That's why we ended up getting solar.  It got to be very expensive.  In California electricity is extremely expensive.



LEO:  I know.



STEVE:  That's why, as I was saying, actually it was Mark Thompson who explained to me that you can't do crypto coin mining anymore because you just can't pay for the power.  You're literally paying more than your miners are able to produce.



So speaking of mining, in a bizarre, but I guess not that unexpected attack, the Coinhive guys who are the premier web browser mining provider that we've been talking about the last few podcasts, they got their DNS hijacked.



LEO:  Ooh.



STEVE:  Get a load of how.  Three years ago, back in 2014, we covered, and there was, a major data breach at Kickstarter, which exposed a large number of passwords.  When Coinhive recently set up their hosting with Cloudflare - wait for it - they reused the same password which had been leaked by the Kickstarter breach three years ago.



LEO:  Well, that's why monkey123 never was a good idea.



STEVE:  It's never a good idea.  And I don't even want to know what the password is.  And they neglected to set up any other of the available second-factor authentication which would have prevented this problem.  This allowed attackers to get into their Cloudflare account almost with their permission because it was like, hey, here's the password that's been floating around the Internet.  What the heck?  Commandeered their DNS to point the Coinhive.com domain to their own servers which effectively commandeered the entire Coinhive cryptocurrency mining operation in one fell swoop.  So that suddenly all the websites that were hosting Coinhive's mining JavaScript were mining on behalf of the attackers and not on behalf of Coinhive or the sites which were hosting the script because Coinhive pays the sites for the success of their visitors' mining operations, the number of hashes per second that their visitors are performing while they're there.



And Leo, I thought you'd get a kick out of this.  I happened in my research to run across the script which anyone who wants to can put in their web pages to invoke Coinhive.  And it is simple as <script src="https://coinhive.com/lib/ coinhive.min.js"></script>.  That's it.  That invokes...



LEO:  Of course you're loading JavaScript from Coinhive.



STEVE:  Yes, exactly.  And so that's why, when the bad guys redirected Coinhive.com to their own servers, you were now loading their script rather than the script from Coinhive, which of course ran scripting, but in this case it accrued to the attackers' benefit.  But that's just how easy it is.  It takes a couple lines of JavaScript stuck in the browser headers in order to invoke mining.  Although it's worth noting that we're seeing  a strong backlash from the providers of adblocking, and even the AV folks are getting involved and stripping this stuff out of pages on behalf of their users who are saying, no, I don't want you monkeying around with my web page.



But as we've said here, I think that's a little bit misdirected, unfortunately.  As we know, it doesn't represent a security breach.  What it mostly means is that this is overt scripting.  And whereas most of the scripting being done is not visible, if something pins your CPU, whoops, that kind of makes it visible.  But there's a lot of scripting going on that is much worse than this, which is relatively benign, which is being used to track and aggregate and fingerprint and do all this other stuff, which people are just saying, oh, yeah, well, I don't care about that.  They're saying, oh, yeah, but I don't want anybody mining cryptocurrency on my browser.  Well, okay.  Really not such a security problem.  



Okay.  Bad Rabbit is new ransomware which is rapidly spreading across Eastern Europe, a lot in the Ukraine and in Russia.  Like WannaCry that we were talking about before, and several other recent families of malware, Bad Rabbit is leveraging one of the escaped NSA technologies, in this case EternalRomance, which was as we know released by the Shadow Brokers and was subsequently patched by Microsoft last March.  But as is unfortunately the case, as we know, not all machines are getting patched.  And in fact it's a little bit surprising to see how rampant this is.



EternalRomance was the remote code execution exploit that took advantage of a flaw, also another flaw in Microsoft's Windows Server Message Block, the SMB protocol, which allows, once it gets into a network, typically through a drive-by download, what the AV people are finding is that malicious Flash downloads - I'm just amazed this is still getting as much traction as it is.  I mean, Flash is pretty much gone.  But there are sites that say, oh, you need to update your Adobe Flash.  And people say, oh, okay, and click on it.  And what they're doing is they're installing a copy of Bad Rabbit, which then gets into their system.  It is cryptomalware, which is requiring - remember the quaint days, Leo, when it was one bitcoin, and that was about $400?



LEO:  Yeah.



STEVE:  Well, no.  Now it's 0.05 bitcoin, which is about $285.



LEO:  Well, that's a deal.



STEVE:  In current bitcoin...



LEO:  A fraction.



STEVE:  Yes, valuation.  So once it gets onto a single machine, then it uses the EternalRomance vulnerability to scan within the network and find other vulnerable machines and leap through an Intranet in order to infect all the machines within the network.  So this brings us back to our standard advice, which is, number one, NEVER, in all caps, NEVER download anything offered to you by any website.  And remember, even trusted sites may have been and often are transiently compromised.  So it doesn't matter who, that is, what site is making the offer for something your system "needs," in quotes.



One of the other things we're seeing is sites saying, oh, you don't have the XYZ font.  You need that font in order to properly, you know, click here to download that font into your system.  No.  They're offering you something.  Never download anything offered to you by any website.  And we learned this lesson back in the early email virus days.  It didn't even matter if the email came from Mom because we knew that Mom may have gotten her computer infected, and the virus was digging around in Mom's address book and sending out replicating virus email to everyone Mom knows, including all of her kids.



So, similarly, you trust Mom, but you can't trust email that apparently your mom sent you.  Similarly, you can't trust anything that any website tells you you need to download.  Just plant that one as a big flashing red neon sign.  Never download anything offered to you by any website.  That is now the leading attack vector.  It can be an ad.  It can be the site itself.  Just never.  And then, I mean, that's advice you can tell everybody you know, and should tell everybody you know because it requires no technology.  It requires self-control, but no technology.  Then we know, people who listen to this podcast, that we want our networks behind NAT routing firewalls, which are NAT routers which are inherently firewalls, with Universal Plug and Play disabled wherever possible.  That is, if it doesn't break something, turn it off, which is generally good security advice.  And also, if possible, we want to establish and maintain network isolation.



So if you can have, like, many routers are now having guest networks.  So you want, for example, your IoT devices and visitors to be on the guest network and keep isolation with your more valuable computers on a separate network, to do that as much as possible.  And, finally, keep up to date with all patching, not only your systems, which now patch themselves, but the IoT devices that aren't yet up to speed, that aren't yet patching themselves as really all future IoT devices are going to have to start doing.  We really - that just has to be a requirement for next-generation IoT is that they are able to preemptively update themselves, and they're being offered by suppliers, by manufacturers who are responsible in after-purchase maintenance of these things because they're all computers.



There's something that I wanted to put on everybody's radar that will be appearing in the Windows 10 Fall Creators Update.  And it's - I didn't write the version down.  It's something 1702?



LEO:  Nine, 1709.



STEVE:  Nine, okay, good.  Ed Bott made a note of it, and I wanted just to bring it to everyone's attention because it's a very nice feature which is not currently enabled by default.  And it has the feeling of something that Microsoft will eventually turn on once they have a sense of what it breaks and what they have to do.  It's known as Controlled Folder Access.  And it's a feature of Windows Defender, so Windows Defender has to be turned on.  You have to be using it, and so it cannot currently be used with third-party AV.  It's got to be Microsoft's built-in Windows Defender.  Under Virus and Threat Protection, it's called Controlled Folder Access.  Off normally, you want to turn it on.  But do so - I wouldn't tell all your friends yet to turn it on.  See how it works for you because the goal, Microsoft's goal is that it adds powerful and apparently effective anti-cryptomalware features.



What they're trying, what Microsoft is trying to do is to identify software which should not be reaching into what then become protected folders, which are the stuff that cryptomalware wants to encrypt because it's the unique stuff, your own documents and pictures and music and things that you may need to get back.  So there are, after you turn it on, there are two other options below there.  One is Protected Folders, which displays the list of folders whose contents are then being protected from tampering by malicious or suspicious apps.  The default list includes data folders from the current user's profile and from the public profile, so that you're able to do some curation there, if you want to.



And then the second thing is which apps are allowed.  So there's an "Allow an app through Controlled Folder Access."  You may find that something that you know and that you trust, but that Windows Defender doesn't yet, is having a problem.  So you can go in there in order to whitelist things.  So this is, I mean, I can understand this not being turned on because it could upset things.  I have a picture on the next page of the show notes of this page from the dialog from Windows 10 showing this thing switched on and those two other items that you're able to click in order to open and explore.



So I just wanted to make sure people knew it was there.  Seems like a good thing.  It'll be interesting to get some feedback over time.  And Microsoft tends to do this, historically.  For example, Windows XP was the first version of Windows to have an actual firewall built in, but it was turned off until, what was it, Service Pack 2 where they finally turned it on.  So they generally roll these things out kind of gently to make sure they don't break something, let experts turn it on at their own risk, and then they get some feedback on how it's doing and allow it to mature a little bit.  So a nice feature.  And it would be nice.



Oh, and I did also see in my digging into this a little bit, several AV systems were being blocked as they should have been.  I saw some reporting saying, whoa, well, this thing did stop us, and we're glad because our tests of ourselves demonstrated that it was catching unknown software trying to make modifications.  So that's a good sign.



LEO:  I like this Controlled Folder Access.  It just underscores more and more why you don't need an antivirus because, well, you're breaking the antivirus; right?



STEVE:  Yes.



LEO:  So, yeah, why should I let a third party with in some cases dubious security themselves access to my system?



STEVE:  Yes.  I completely agree.  I think we really have moved, I mean, again, another example of Microsoft sort of bringing out a feature and moving it slowly forward.  What has it been, a decade since we're beginning to get - we have the little house down in our tray, and it's downloading updates.  I mean, the writing has been on the wall.  AV companies, you don't have an infinite opportunity here.  It just doesn't make any sense at this point.  And you and I have been singing this tune for quite a while now.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  So Check Point was the Israeli company that I couldn't remember the name of at the top of the show.  They found a worrisome vulnerability in a widely deployed LG SmartThinQs, smart home devices.  LG calls it the SmartThinQ, S-M-A-R-T-T-H-I-N-Q, smart home devices.  Unfortunately, there was a design problem which Check Point found.  Had it not been fixed before disclosure - and they worked with LG.  LG, Check Point reported, was very responsible and immediately fixed this and has a system which was able to patch in place, which is what we want.



So after the fact, after this was fixed, and the fixes were pushed out, Check Point disclosed what they had found.  And essentially what they found was what they called HomeHack, as they named their vulnerability, gave the attacker the potential to spy on users' home activities, among other things, via the Home-Bot robot vacuum cleaner, which has a video camera.  And there are, as of the end of 2016, so about a year and a half ago almost, more than 400,000 of these camera-equipped Hom-Bot robot vacuum cleaners.  And essentially an attacker located anywhere in the world could get into someone's home remotely, commandeer this vacuum cleaner, and turn on the camera and drive it around the house in order just to, literally, you would have a mobile camera wandering around.



And you kind of look at it thinking, well, it doesn't look like it's following its normal vacuuming path.  No.  Some bad guy is steering it, going wherever they want to go to see what they want to see.  So anyway, sort of, I mean, but this was a true, a real-life example of an in-place active vulnerability that probably by now easily more than half a million homes had before Check Point found it, responsibly disclosed it, LG fixed it, and thank goodness they were able just to push a fix out to their entire install base of these connected devices and close this problem.  This is a perfect example of why it's crucial that IoT devices moving forward be able to manage their own security.  It just has to happen.



And so Leo, get a load of this one.  As I was doing the reporting on this, I'm thinking, okay, how long, count the seconds, before you have this ordered up?  Because this is a new service.



LEO:  Not at 250 bucks.  No way.



STEVE:  Oh, okay.  It is a little pricey.



LEO:  Yeah.  And, no, I can't get it here, either.



STEVE:  Oh, okay.  So it's not available.  Is it regionally exclusive?



LEO:  Yeah, well, he's talking about the Amazon Key service, which lets the delivery person in.  The reason it's only available in 37 markets, it has to be a market where Amazon deliveries occur from Amazon's own Amazon Logistics truck.  They don't let UPS, FedEx, or the U.S. Postal Service do it.



STEVE:  Oh, that was a big question I had.



LEO:  Yeah.



STEVE:  Okay.



LEO:  And we're not in one of those areas, that's all.



STEVE:  I am, but I'm not letting them in.  Unh-unh.  No.



LEO:  Why did you think I'd let them in?



STEVE:  Well, because, Leo, well, you know, you have...



LEO:  You're home all day.  This is why people would do it, if stuff's getting stolen from their porch.



STEVE:  Yes, yes, yes.  And so having an Amazon bonded delivery person - okay.  So let's explain what this is.  New service from Amazon called Amazon Key.  First of all, it relies on Amazon's new Cloud Cam and a compatible smart lock.  The camera functions as the hub.  It's connected to the Internet via of course the residential WiFi.  It's positioned to show the front door and entryway from inside the house.  So it's showing the, what do you call it, the region inside the front door.



LEO:  The foyer.



STEVE:  The foyer, thank you.  The camera talks to the front door locks, either manufactured by Yale or Kwikset, over the ZigBee wireless protocol.  So when a courier, when an Amazon courier arrives with a package for in-home or at-home, well, actually in this case in-home delivery, they scan the package's barcode, which sends the request to the Amazon cloud.  If everything checks out, you've signed up for the service at this location and so forth, the cloud grants permission by sending a message back to the camera, which then starts recording the video.  Once the video is started, the courier gets a prompt on their app and swipes the screen.  The home's front door unlocks.  The courier opens the door, puts the package in the foyer, closes the door, does something else on their scanner which causes the front door to lock.



The customer receives a notification that their delivery has been made, along with a short video showing the whole drop-off process from before the door was unlocked to after the door was relocked, in order to confirm that it was done.  And of course the Amazon delivery person realizes that they're on candid camera so they'd better behave themselves.  So it's as good as system as it could be where you want autonomous delivery and security, and everybody, all of the various components to be able to be held responsible.  And as you said, Leo, if you're in an area where there's a high likelihood that the packages could be or have been or would be stolen from your front porch, then I can see this could make sense.



LEO:  It's a big problem.  I mean, even here in Petaluma it's a big problem.



STEVE:  No kidding.



LEO:  Yeah.



STEVE:  Wow.



LEO:  Because they say Amazon.  They have a big smile.  They're big.



STEVE:  It's going to be something good.



LEO:  It's something good in there.



STEVE:  It's going to be a goodie, yeah.  Interesting.  So I just wanted to revisit the question, still unanswered:  Who is Satoshi Nakamoto?  As we know, we now believe this is a pseudonym for the mysterious and still anonymous and unknown original creator of the Bitcoin, the world's first cryptocurrency that we did a podcast on years ago [SN-287] because the technology was so cool, and I was just enraptured by it, whoever this was, or whatever group did this.  We've had, as we know, some false alarms about who this person is.  I just wanted to note that, at this time, since this unknown person or persons is believed to have about 5% of the current bitcoins that have...



LEO:  How much would that be worth?



STEVE:  $6 billion.



LEO:  Holy cow.



STEVE:  Yes, my friend.



LEO:  This is why I've always thought bitcoin and other cryptocurrencies are essentially a pyramid scheme because one of the features of a pyramid scheme is the person who starts the scheme makes the most money.



STEVE:  Yeah.



LEO:  And the people who enter late, like if you bought bitcoin today, make the least money, or in fact no money, and end up subsidizing the guy who invented it.  So good going, Satoshi.



STEVE:  That was a good algorithm.  You got $6 billion.



LEO:  But he can't cash it in without revealing who he is; right?



STEVE:  There are, I think there are ways.  I mean, you could certainly buy stuff.  You could transfer coins.



LEO:  Coinbase is not going to give you $6 billion.



STEVE:  No.  Coinbase doesn't have $6 billion.



LEO:  Who could you cash that in with?



STEVE:  Yeah.



LEO:  You could buy a couple of pizzas, maybe.  I don't know.



STEVE:  Very expensive pizzas.  All the toppings you could ever have.



LEO:  I mean, seriously, it's really interesting. 



STEVE:  Yeah.



LEO:  That's, of course, how we have always known somebody could identify themselves as Satoshi is if they said, well, here's the key.



STEVE:  Yes, yes.  If you demonstrated ownership of those first early coins, then it would be like, oh, there's only one person who has that, or one group, or one entity.  So, yeah.



LEO:  And either he's fabulously wealthy and really privacy focused, or dead, which some people think.



STEVE:  Oh, that's interesting.  Sad.  But, boy.



LEO:  That six billion could be lost forever; right?



STEVE:  Well conceived, yeah.



LEO:  There's no way to recover that if you don't have the key.



STEVE:  No, no.  It's like my 50 that are wandering around here somewhere.



LEO:  And my seven.  Because I can't - I put the wallet on Dropbox or somewhere, and I keep loading it in different things, and it always says zero.  I don't know what I've done wrong.



STEVE:  So I've talked about TunnelBear a couple times, and they continue to impress me.  Their most recent blog post I just wanted to share, just to give a sense for this is the way you want your VPN.  This is a philosophy you want them to be bragging about.  Their most recent blog posting was how we avoid collecting your IP address on our website.



And just the first two short paragraphs read:  "It might sound odd, but we pride ourselves on knowing very few things about our customers.  In fact, the less we know about you, the happier we are.  As we grow and learn more about the information we need to optimize our service, we find new ways to do things like anonymize site statistics in a privacy-compliant way.  You see, we want to make sure that, even before you decide you need a Bear in your life, we're helping you maintain your privacy.  Here are a few of the things we do on our website to limit the collection of your IP address."



And I'll stop reading at this point, but I dug into it, and I was impressed.  They go on to describe a feature that I didn't realize Google Analytics offered, which is an aid to help anonymize visitors who are otherwise being tracked by Google Analytics, where the last byte of the four-byte 32-bit visitor's IP address is deliberately recorded as zero.  So every IP address ends in dot zero rather than what it really is, regardless of its actual IP.



So as we know, in practice this preserves the geographic data that might be useful and valuable, but hides an individual ISP's customer among or amid up to 255 others since that last byte could be any value from zero to 255, so any of a total of 256 possible values.  But by zeroing it, you're never recording which one of you on that, what is it, 16 million different subnets.  So you're one of 256 of that three-byte subnet.  There is no recording of who you are.



So again, there have been - we've talked about TunnelBear a couple times just because a lot of our listeners have asked what do I think.  I've checked them out and looked over their protocol and everything that they've published, and it looks like they're doing everything right.  And, boy, I've got to say I sure like their attitude.  It's the attitude that you want from someone whose services you are purchasing for privacy.  It's like, yes, that's what we want, too.



A bit of errata from a Khyron, K-H-Y-R-O-N, who tweeted, he said:  "Really enjoying Security Now!.  Last week you mentioned bitcoin mining via JS."  He said:  "Most JS miners are using Monero, not bitcoin."  And I keep making that mistake.  So thank you for correcting me.  Yes, I just - in my head I have it incorrectly.  I should be saying cryptocurrency mining or Monero, not bitcoin.  So thank you.  For the record, that's what I meant.  And Leo...



LEO:  They're mining for bitcoin.  They're using Monero to mine for bitcoin?



STEVE:  No, Monero is its own cryptocurrency; right.



LEO:  How confusing is that.



STEVE:  Yeah, I know.  And there are a bunch of them, as we know, that have sprung up.  But this is the one that is being mined by the JavaScript miners. 



LEO:  So the JavaScript miners don't mine bitcoin.  Oh, interesting.



STEVE:  Correct.  And so thank you, Khyron, for correcting that.  And Leo, are you guys finished with Season 2?



LEO:  Not finished.  We're about four episodes in.  But I'm loving it.



STEVE:  I am, too.



LEO:  Isn't it good?  We're talking about "Stranger Things."



STEVE:  It is good.  We are talking about the second season of "Stranger Things."  There's one dud episode.  You haven't encountered it yet if you're four in.  But other than that, I thought it was really good.  And I have a girlfriend who likes to binge on "Stranger Things."



LEO:  I would marry that woman.



STEVE:  Quite happy.



LEO:  Nothing like Netflix and chill, especially if it's "Stranger Things."



STEVE:  I couldn't believe it.  She kept saying, "Oh, let's do one more, one more."  I said, "You're kidding; really?  Okay." 



LEO:  A woman after my own heart.  Lisa's like that, too.  We'll end up, you know, I think Saturday night we ended up in bed at, like, 1:00.



STEVE:  Yup, I know it.  I know what you mean.



LEO:  Very good stuff.  Very good.



STEVE:  So I got a nice note from Rusty Burke, who actually had a question for us about Security Now!.  But en route to that he said:  "Before I get to my question, which is not actually sales related" - because he actually sent email through our sales email, which Sue forwarded to me.  He said:  "I just want to express my thanks to SpinRite."



He said:  "It recovered a Sony Vaio laptop which was, with increasing frequency, inexplicably dying on me.  So I ran SpinRite at Level 4; and, after rebooting, I haven't had a lick of trouble.  Runs like new," he wrote.  So Rusty, thank you.  And I just wanted to remind people that it's not always just a crash that can demonstrate that the hard drive is really beyond trouble at that point.  It's if something, you know, if it starts being flaky, if it starts going slower, that's a big indication.  But also just hanging and flaky and stuff, just run SpinRite over it, and you may find those problems disappear.  We know that lots of people have.



And then he asked:  "My question is whether Steve ever did a series on networking, which he suggested might be coming while doing the How Computers Work series on Security Now!."  He said:  "I've combed through several sources for an answer to this question, but can't seem to find one.  Thanks for a great product and great information.  Rusty Burke."



And you know, Leo, I know that we spent a lot of time in the early days really nailing down how the Internet works, going from packets to routing and autonomous routing and then building, talking about the UDP and TCP and the various protocols on top of those.  So I know that's all out there in our past.



LEO:  The problem is you can't search for "networking" because every show has the word "network" in it, and so that's not going to...



STEVE:  I will try to find some time to go back through and just scan the topics and pull together the list of podcasts.  I kind of think we did them in clumps, where we got onto a...



LEO:  I feel like we did, too, yeah.



STEVE:  We got onto a routine where it was, okay, this week, you know, last week we talked about this aspect, this week we're going to talk about this aspect, and next week we'll talk about that one.  But anyway, I'll see if I can find some numbers.



LEO:  Everything's here at TWiT.tv/securitynow.



STEVE:  It's all there.



LEO:  Or on your site, too.



STEVE:  Yeah.



LEO:  But the problem is to look at all the episodes, I just clicked that button, wait for a while because it's going to take a while.  And then I think we still paginate it because there's, well, 600-some episodes.  So, yeah, there's 27 pages.  Although you could start at 27 and go backwards.  Because it would have been early days, I would think.



STEVE:  It would have been the early days.



LEO:  Yeah, yeah.



STEVE:  Yes, those quaint early days when we were actually able to talk about something other than what happened this week.



LEO:  Unfortunately, the earliest shows really don't have very good show notes.



STEVE:  That's true.  Yeah, well, in fact I wasn't doing the show notes.  They have transcripts, but not the show notes.



LEO:  Yeah.  Yeah, I don't think I - oh, here.  Well, here Steve explains how VPNs can protect you.  He also announces the WPA password generator.  So, yeah, I guess you could go one by one.  VPNs Part 2, so that's kind of a networking subject; right?



STEVE:  Yeah.



LEO:  So I don't know.  Yeah, you can go through them one by one.  PPTP and IPSEC.  The early ones really did have a lot of...



STEVE:  Yes, of networking stuff. 



LEO:  ...information about how networks work.  OpenVPN, I remember you did a big, big thing on OpenVPN.  So, yeah, go through it.  You'll find quite a bit.  It's all there somewhere.



STEVE:  So in closing the loop, Steven, whose Twitter handle is @Ozzyla, he said:  "@SGgrc Just heard on your podcast about running SpinRite on an SD card.  How the hell do you do that?  Tutorial anywhere?"  And actually it doesn't need a tutorial.  I'll just explain that you need an SD card to be seen by the BIOS for Version 6 of SpinRite.  At some point release beyond 6, I don't think it'll be 6.1, actually I'm pretty sure it won't be 6.1 or 6.2.  But when I get to the native USB hardware support, that'll change.



But for now, if you use any adapter to allow an SD card to be connected to your computer, some PCs have an SD card slot, or laptops do, or you can use an SD-to-USB adapter.  Plug it in when you power up the machine, or when you reboot the machine.  That will allow the BIOS to see the card at boot time, and it will assign it a drive designation, a BIOS drive designation.



Then, when you subsequently run SpinRite, it will see the drive.  And you just can run - remember you want to use Level 2, not Level 4, because you want to just do a read test on an SD card, especially on an SD card because they are the least robust.  You know, SSDs are far more - they've got much more overcommitted and available spare space for dealing with troubles, and much more sophisticated hardware controllers on SSDs than SDs.  So just a Level 2, but it'll smooth the card out and probably fix whatever might be going on wrong with it.



LEO:  I don't know if this is what our correspondent was looking for, but Episode 25, "How the Internet Works, Part 1."



STEVE:  Ooh, that looks like a good place to start, yeah.



LEO:  Be a good place to start.  And you can hear what Steve and I sounded like back in 2006.



STEVE:  Back when we were youthful.



LEO:  In our youth.  Yes.



STEVE:  Robert D. Wilson asks - this is off-topic, but sort of ties into the show because everyone knows I'm a sci-fi person.  He just asked:  "Is 'Dark Matter' on Netflix any good?"  And I would have to say yes.  It's not top-of-the-line amazing fabulous Peter Hamilton sci-fi or anything.  It's not as good as "The Expanse," which is amazing.  But it's one of those made for the Syfy channel, and it's compelling.  The characterization is good.  It's kind of like a lot of it happens inside of a starship in dark setting.  But I liked it.  I've got a bunch saved up that I haven't gotten to.  I just don't have time.  There's just too much stuff to watch.  But I would say you'll know in a couple episodes if it's for you or not.  So I wouldn't universally recommend it.  But it's above the normal, you know, it blows "Sharknado" away completely.



LEO:  That's a pretty low bar.



STEVE:  That's a very low bar, yeah.  I liked it.  So I would say it's worth considering.  I wouldn't subscribe to Netflix for it.  But if you've got Netflix, check it out, "Dark Matter."  I think you'll find - and it's like in its third or fourth season now.  So there's enough content to keep you happy if you end up liking it a lot.



LEO:  And we like "Black Mirror."  I'm sure you've seen that.



STEVE:  Yes, "Black Mirror," yes.



LEO:  It's on Netflix.  But that's kind of not science-y sci-fi.  It's more like dystopian future sci-fi.



STEVE:  And there's also - Amazon has one I haven't gotten to, based on Philip K. Dick short stories.



LEO:  Oh, "Man in the High Castle."



STEVE:  There's that one.  No, I'm thinking of - it's also an anthology series.



LEO:  I don't know that.  I love Philip K. Dick.  But I've, with the exception of "Blade Runner," not been thrilled by a lot of the, you know.



STEVE:  Yeah.  And it's Philip K. Dick's "Electric Dreams" on Amazon Prime, "Electric Dreams."



LEO:  Of course that was the short story that "Blade Runner" is based on.



STEVE:  Yes, exactly.  And "Blade Runner 2" was good, too.  I liked it.  It's a little more cerebral.  I don't think it did that well in the box office, but I thought it was very good.



LEO:  I'm waiting to see...



STEVE:  What is it, "Blade Runner 2049," was that the...



LEO:  Yeah.  It was too dark.  I couldn't really - don't see it in 3D, if you can avoid it.  It's way too dark.



STEVE:  Yeah, I don't.  I didn't.  



LEO:  I'm going to watch it on my big screen at home when it comes out.



STEVE:  So two people.  Jan Rademan asked:  "Is there a test to see if a router is a Reaper zombie?  Could ShieldsUP! be used to test for specific port?"  And Dr. Brian of London asked:  "Is there a way to see if my own ISP-provided D-Link router is part of the new mega botnet?"  The answer is probably yes, if you were to get into a command prompt for the flavor of Linux and enumerate the processes, and really revved up your propeller hat and figured out what everything was.  We'll be talking about it here in a second in more detail.  But the best solution, first of all, everybody should know it does not survive a reboot.  It does not make any permanent changes to the file system.



So one of the problems Reaper is having is that any rebooting of the IoT devices flushes it out of RAM.  It is only RAM-resident.  So rebooting your router guarantees you that it then is gone, and what you really want to do then is just make sure that you're running the latest firmware for your router.  That's really - that's the best advice, the best generic advice is that the router manufacturers are definitely responding.



But remember that most of these existing exploits, there are a couple new ones that Reaper - Reaper is continuing to evolve, which we'll be talking about here in a second.  But most of these exploits that it's using have been patched, in some cases as many as four years ago.  So it's just that the routers aren't being kept up to date.  So if yours is, and there's always a reboot following a firmware update, so just update your firmware to the latest, reboot your router, and you'll know that Reaper is no longer sitting around with the ability to look inside your network, in addition to, well, actually not doing anything, which we'll be talking about in a second.



WarrenKC asked @GibsonResearch:  "In your opinion, should a user disable Windows Defender on Windows 10 if said user is careful on the Internet?  Thank you."  And I'll just say, well, I'm very careful, and I'm happy to have it running.  Like you, Leo, I have never used in modern times any third-party AV.  I just - I'm careful.  In the case of Windows Defender, which is built into the OS, supported by Microsoft, being updated constantly, sort of has the sanction of the OS in terms of its ability to see what it needs to see, rather than needing to be reverse-engineered into the kernel as now all other AVs have to.  I would recommend turning Windows Defender on and using it proudly.  I do in my various Windows 10 machines.  So I would say use it, and it doesn't get in the way because it's part of the OS.



And then, finally, two questions regarding DNSSEC and certs.  Tim Chase says:  "On SN-632 on DNSSEC said cert info could be kept in DNS.  This would only be DV certs, not EV; right?"  And then also related to that, Grant Taylor asked, actually in a bit of a Twitter dialogue, he said:  "Why couldn't an EV cert be stored in DNS via TLSA" - which is the protocol DANE, the system for delivering certificates over DNS?  He says:  "Or are you implying DANE only implies DV?"



So there's some interesting issues, DV versus EV.  EV, Extended Validation, really only makes sense in the context of a certificate authority and their assertion.  So the whole point of Extended Validation is that you are not serving the certificate yourself over your own DNS; and, by having that channel be secured, thus assuring that the browser obtains the certificate you want it to, rather with non-DNS in the classic hierarchical PKI model, the certificate authority is using their signing of the certificate to provide an assertion of its authenticity.  In the case of a DV cert, they're simply saying yes, you have proven ownership of this domain.  That's all we're saying.



In the case of an extended validation cert, you're saying, oh, we had a conversation with you.  We said hi.  We called your phone number.  We checked your Dun & Bradstreet rating.  We did all this stuff.  And so we are really sure you are the organization and everything you represent yourself to be when someone visits you.  You are this company.  This is your domain.  That's a much bigger assertion.  And so a third party, this certificate authority, has done all that research and is maintaining its currency in order to make that and to sign that assertion on the certificate.



So the notion of gradation in assertion only makes sense, only has meaning in the context of a certificate authority's signing and what that signature asserts.  It doesn't have any meaning in the context of a certificate being delivered over DNSSEC using DANE where you're using secured DNS to acquire the certificate from the server of the domain to then use that to assert the identity and to provide encryption for your connections.  So really two entirely separate delivery mechanisms, one where quality of certificate means something, the other where it doesn't.



And, finally, Reaper.  In the week that has transpired since we discussed this somewhat breathlessly - the whole industry was last week - a lot more has come to light.  Pretty much all of the interested and interesting security companies have had a chance now to assess this, to look at it, to do their own analysis.  The one of all of them that I liked the best was the take that F5 Labs had, which they first posted last Thursday, so two days after, well, remember this all happened on Monday, so we were happy that it happened Monday because we had the podcast on Tuesday.



Two days later, F5 Labs came out with their look, which was much less frantic.  And then today, this morning, on Halloween, October 31st, they revisited their last Thursday position, so five days later, to sort of further substantiate their position.  I got the sense that they were being attacked a little bit by downplaying this.  But I really think they got it right.  First of all, the title of their posting last week I liked a lot.  And in fact I tweeted the link to this, I guess it was last night.  Actually it had a typo in it.  Instead of "botnet" I said "bonnet," which led to many interesting funny responses on Twitter.



LEO:  The Reaper Bonnet, my favorite.



STEVE:  The Reaper Bonnet, exactly.



LEO:  That's my Halloween costume.



STEVE:  So they titled - so their posting they title "Reaper:  The Professional Bot Herder's 'Thingbot.'"  And first of all, I love the term "thingbot."  We're going to adopt that, "thingbot," because that's pretty much what we're going to have from now on is Internet of Things bots.  And so these are going to be Thingbots.



So last Thursday they started by saying, "This isn't your mama's botnet.  This is a proper botnet.  If you were the world's best IoT botnet builder, and you wanted to show the world how well-crafted an IoT botnet could be, Reaper is what you'd build."  And this is nothing like what we heard during the first couple days.  They wrote:  "It hasn't been seen attacking anyone yet, and that," they write, "is part of its charm.  But what is it doing?  We've got some ideas."



And then they interrupted themselves for their morning update this morning, where they said:  "The intentions of Reaper are as unclear today as they were a week ago.  We hold to our position that the interesting aspect of Reaper is not its current size, but its engineering, and therefore its potential.  From a pure research perspective, we're interested in how Reaper is spreading.  Instead of targeting weak auth like a common thingbot, Reaper weaponizes nine, and counting, different IoT vulnerabilities.  We think the current media focus on the numbers instead of the method is a tad myopic."  Then they said:  "See the next update section below for our clarification."  And we will.



So in their original posting they said of size and position, they wrote:  "Krebs puts the current size of Reaper at over one million IoT devices.  We have data that suggests it could include over 3.5 million devices and could be capable of growing by nearly 85,000 devices per day.  The reason Reaper has gotten so big and, honestly, the reason we're so impressed with its construction is that, unlike its predecessors, Mirai and" - and I don't know how to pronounce this.  Persirai?  P-E-R-S-I-R-A-I.  It ends in R-A-I.  Persirai?  I don't know how to pronounce it.



LEO:  Persirai.  I say Persirai.



STEVE:  Anyway, whatever that is.  Good.  "Reaper uses multiple attack vectors."  Again, remember, Mirai was just guessing usernames and passwords.  "Reaper uses multiple attack vectors.  Mirai used default passwords," they write.  "Persirai used the blank username and password combo, which," they write, "frankly is such a dufus security error on the part of the manufacturer that we feel it barely deserves to have a CVE.  Reaper," they write, "is almost showing off by not even bothering with password cracking, and instead just exploiting different vulnerabilities."



LEO:  This is a nation-state.  It's got to be.



STEVE:  Yes, yes, "remote code executions, web shells, et cetera, in nine different IoT vendor devices."  And then they interrupt themselves again with their morning update from this morning.  "Reports on the 'size' of Reaper vary.  We've scanned," they write, "750,000 unique devices that match the nine vulnerabilities currently exploited by Reaper.  We regularly scan 85,000 new [they used the term] 'Reaper-compatible' devices per day."  Meaning F5 Labs has found three quarters of a million devices that match the nine reported vulnerabilities currently incorporated by Reaper, and they're discovering an additional 85,000 new ones per day, which Reaper could also be doing.



They say:  "We don't know which of them are actually infected, but there's no reason that Reaper itself couldn't infect them, unless its authors didn't want it to.  Nine vulnerabilities," they write, "currently used by Reaper are fairly rudimentary as vulnerabilities go.  If the thingbot authors were to include a few dozen existing vulnerabilities that fit Reaper's device-targeting profile, we think they could grow the thingbot by an additional 2.75 million nodes, if they wanted to.  Adding that 2.75 million to the 750,000 that are currently Reaper-compatible gives us the number 3.5 million."



And they say:  "Note:  We will not be disclosing the additional CVEs as that would simply expedite the authors' exploits."  And they have a nice little chart that I dropped into the show notes which, Leo, you just showed, showing us 750,000 plus 2.5 million brings us to 3.5 million.  They said:  "The actual size of Reaper is probably limited to whatever size its authors want it to be."



LEO:  Exactly.  They don't want command-and-control of too many servers.



STEVE:  Yes.  And this feels developmental.  This feels like, yes, that's the other thing.



LEO:  Oh ho.



STEVE:  They're not using it to attack.  They're using it to hone their system.



LEO:  Interesting.



STEVE:  They said:  "The actual size of Reaper is probably limited to whatever size its authors want it to be.  Right now," they write, "it feels like its authors are experimenting, building and testing.  Maybe Reaper is pure research.  We don't know, and that's kind of why we respect it."  And then they say:  "Reaper has better IoT security.  Unlike many of the devices it infects, Reaper has an update mechanism."



LEO:  Of course it does.  I'm telling you.  And the Lua, and the commands.  This is definitely a nation-state.



STEVE:  Yes, yes.  And they said:  "How impressive is that?  If it weren't malicious, it might qualify to meet the standards of the new Internet of Things Cybersecurity Improvement Act of 2017 federal requirements."  They wrote:  "Heck, the authors could even make a distribution out of it, and it could become the default remote management platform for IoT."  Because it does it better than anything else.



And they ask the question:  "Is it malicious?  So far Reaper has  not been seen attacking anyone with massive volumetric DDoS attacks.  Yes, that's a good thing.  At least one of us" - because it had three authors - "thinks it might never be seen attacking anyone.  If Reaper were to start being used as the ultimate Death Star weapon, that would cheapen its value.  It would also result in an active takedown campaign.



"Remember how at least two strike-back bots were created to combat Mirai after it attacked Krebs, OVH, and Dyn?  Brickerbot actively wiped the file systems of infected IoT devices, in many cases turning them into little more than bricks.  Hajime was more polite and merely blocked ports and left a cute little note informing the device owner that their device was participating in attacks and please patch.  If Reaper starts attacking people with DDoS, it will turn from a marvel of thingbot infrastructure engineering into - yawn - another volumetric attack tool.  The bot herders would be hunted down by law enforcement, and the bot would be disassembled."



So they say:  "What is it doing?  Right now Reaper is an object lesson for IoT manufacturers and security researchers.  It's like a giant blinking red light in our faces every day, warning us that we'd better figure out how to fix IoT security soon."  And they conclude with:  "Is there a lesson yet?"  They said:  "As predicted, we will continue to see more thingbots arise as we expect 'things' to be the attacker infrastructure of the future.  Just because Reaper is the latest doesn't mean it will be the last.  We've added Reaper to the list of botnets that we're monitoring.  We suspect that entire existing botnets will get folded into it, whether they want to or not."  So it's going to be subsuming other botnets, taking them over and acquiring them.



They write:  "If Reaper doesn't attack anyone or give away its intentions, it may enter the same mythical space occupied by the Conficker worm of the late 2000s.  At its peak, Conficker infected over 10 million Windows computers and caused great concern because it could have done an insane amount of damage.  But it was never activated, and it remains a study in bot construction.  The obvious lesson is that the state of IoT security is still incredibly poor, and we need to do a better job of threat modeling the Internet of Things."  So a beautiful piece of work by F5 Labs.  Thanks, guys.



LEO:  And who knows?



STEVE:  And I think they got it exactly right.



LEO:  Maybe John McAfee wrote it, and he's going to release it as a way to update IoT security.



STEVE:  That's definitely John's modus, yes.



LEO:  The ultimate IoT update tool. 



STEVE:  It is terrifying.



LEO:  I love it that it has better security than the devices it's acting on.  



STEVE:  Exactly.



LEO:  It just, I mean, am I wrong, but it feels like a nation-state.  This is too well done.



STEVE:  It's beautifully well done, yes.  The fact that they've got a network of interacting command-and-control, it's deliberately just sort of percolating along without being in a hurry.  I think these guys have characterized it exactly right.  So we got something fun to keep an eye on, too.



LEO:  We're going to keep watching.



STEVE:  Yup.



LEO:  See what happens.  Hope it's our side.  We don't know, though, will we, till it takes off.  Steve Gibson, you see, this is why you listen to the show, everybody, because this is where you learn the good stuff.  If you want to hear it live as it happens, tune in on Tuesdays, right after MacBreak Weekly, about 1:30 Pacific, 4:30 Eastern.  Next week it'll be 21:30 UTC, 21:30 UTC because...



STEVE:  Let's see.  We're falling back.



LEO:  We're falling back.



STEVE:  So we get an extra hour, yay.



LEO:  So an hour later.  Oh, it's confusing.  UTC doesn't change, obviously.  We do.  And the reason I give UTC is so that you can calculate your UTC offset next week and know what time it really is.  Just 21:30, and then add or subtract what you need to.  I'm going to subtract eight because that's how we do it here in the TWiT Brick House or Eastside Studios.  If you want to watch live, got to TWiT.tv/live.  If you do that, please join us in the chatroom.  Great bunch of people at irc.twit.tv.  You can use an IRC client, but you can also just use the website.  We have a web-based IRC client.



And you can also ask questions of Steve.  He's on the Twitter, @SGgrc, or on his website, GRC.com/feedback.  While you're at the website you might note you can download on-demand copies of the audio of the show and read the fine transcripts that Elaine Farris does for Steve.  He pays for them and makes them available.  That makes it also searchable, which is really nice.  You can search for terms and so forth and find the podcast you want.  GRC.com.  And let's do Steve a favor.  Everybody buy a copy of SpinRite.  If you have a hard drive, SSD or spinning, you need SpinRite, the world's best hard drive maintenance and recovery utility.  There's plenty of free stuff there, as well,  including ShieldsUP! and his SQRL project.



STEVE:  Everything else is free.



LEO:  And it's all free.  GRC.com.  We have copies of the show, audio and video, if you want to see Steve's smiling face, at our website, TWiT.tv/sn.  That's where you can go back in time, too, find earlier episodes, all 635 of them.  Steve has all 635, too.  And also a subscription button there because really the best way to get this is to build your own collection of Security Now! shows.  Just subscribe.  That way you'll download them automatically when the show's over and we put it out.  Takes a couple hours to edit it and get it out the door.  But later on a Wednesday, or a Tuesday, I should say, or early Wednesday, depending on where you are, you'll get a copy of it, and you'll keep it on your phone or on your tablet, and you'll always have it, and that's a good thing.  So subscribe.



I think that concludes this episode of Security Now!.  I can't think of anything else.



STEVE:  We've got it covered again for another week, my friend.  And we'll see what happens in the intervening seven days.



LEO:  All right.  And you've seen the whole "Stranger Things."



STEVE:  Yes, yes, yes.



LEO:  Well, by next week I'll have seen it all, too.



STEVE:  Yes.  It's good.  I agree with you.  It's good.  There's just one episode, actually Lorrie noted that, I think, that it wasn't written by the same guys, was it the something brothers with a "D."



LEO:  The Duffer Brothers.



STEVE:  The Duffer Brothers.  And I think she spotted that the dumb one, it was number seven, it was like, okay, why, what's going on here?  Anyway, but overall...



LEO:  Seems like even the best, one of the best TV shows of all time, "Breaking Bad," had one dopey episode, the fly episode, where the entire episode is them chasing a fly in the meth factory.  Like just dopey.



STEVE:  Yeah.



LEO:  Sometimes you just have to, you know, you have to take it out of gear and coast for a day.



STEVE:  Anyway, it was definitely fun and worth watching.



LEO:  Steve writes every episode of Security Now!.  That's why there's no dead spots.  Thank you, Steve.



STEVE:  No flies.



LEO:  See you next time.



STEVE:  No flies on me.



LEO:  No flies.



STEVE:  Bye, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#636

DATE:		November 7, 2017

TITLE:		ROCA Pain

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-636.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the inevitable dilution in the value of code signing; a new worrisome cross-site privacy leakage; is Unix embedded in all our motherboards?; the ongoing application spoofing problem; a critical IP address leakage vulnerability in TOR and the pending major v3 upgrade to TOR; a Signal app for ALL our desktops; an embarrassing and revealing glitch in Google Docs; bad behavior by an audio driver installer; a pending RFC for IoT updating; two reactions to Win10 Controlled Folder Access; a bit of miscellany; some closing the loop with our listeners; and, three weeks after the initial ROCA disclosure, I'm reminded of two lines from the movie "Serenity."  Assassin:  "It's worse than you know."  Mal:  "It usually is."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  ROCA, of course, the public key crypto error that is causing some pain all over the place is the topic du jour.  But there's a whole lot more, including antiviruses that just aren't doing their job.  You'll find out why next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 636, recorded Tuesday, November 7th, 2017:  ROCA Pain.



It's time for Security Now!, the show where we cover the latest security news and what to do about it.  It's both prescriptive and informative.  Mr. Steven "Tiberius" Gibson, our host.



STEVE GIBSON:  Yo, Leo.



LEO:  Good to see you, our moustached man.



STEVE:  Re-moustached.



LEO:  Glad it's back, yeah.



STEVE:  Everybody is.



LEO:  All's right with the world.



STEVE:  Interesting experiment.  It's like, okay.  Fortunately, it will grow back.



LEO:  You know, it's funny.  For me, it's the opposite.  I grow facial hair, I keep trying to do it, and then I give up after a week because it's itchy, and I don't like it.  And so I never get there.



STEVE:  Yeah.  There is definitely an itchy phase you go throw where the hair's kind of poking back in at you.  It's like, okay, let's not do that.  So Episode 636.  And I struggled for a name.  There is some additional news, not surprisingly, about ROCA, which we covered three weeks ago after its initial announcement.  And it's funny because I was put in mind, I have no idea why this came to me, but it was a line that I liked from one of our favorite movies, of course science fiction, "Serenity."



LEO:  Oh, yeah.



STEVE:  Where the Assassin says to Mal, he says, "It's worse than you know."  And Mal says, "It usually is."  And such is the case with ROCA.  But before that we're going to talk about the inevitable dilution in the value of code signing.  A new worrisome cross-site privacy leakage.  The press went wild in the last couple days over the concern that we all had an embedded Unix in our motherboards.



LEO:  Shocking.  Actually, it's MINIX.  It's not even Unix, it's MINIX.



STEVE:  Exactly.  It's mini-Unix.



LEO:  It cracks me up, yeah.



STEVE:  And so what does that mean?  But what's also cool is for somebody who wants to be on the bleeding edge, and there may be blood involved, someone has put together a how-to on shutting that down.



LEO:  This is the management, Intel Management Engine.



STEVE:  Yes.  And it involves solder.



LEO:  Oh, dear.



STEVE:  So anyway, but it's very cool.  Then we're going to talk about...



LEO:  Solder is the new tinfoil hat, by the way.



STEVE:  That's right.  Solder and a Raspberry Pi 3.



LEO:  Oh, geez, Louise.



STEVE:  It's a perfect thing for Father Robert to do on Know How.



LEO:  Oh, good, yeah.



STEVE:  When you have a motherboard you don't mind never having operate again.



LEO:  Right.



STEVE:  Because the risks are high.  But the rewards, it's just sort of a really fun hacking adventure.  The ongoing application spoofing problem, which recently hit Google's Play Store with more than a million downloads of a bogus WhatsApp app.  A critical IP address leakage vulnerability in Tor, which of course is the whole reason you use Tor is not to have your IP address leak, so whoopsie.  Then we also have the pending major v3 upgrade to Tor, which has been four years in the making, and it's now in alpha.  So we'll talk about that.



And I'm very excited that there's now a Signal app for all of our desktops, which puts some interesting pressure on iMessage because I'm chafing at the fact that, as a Windows user, I don't have any access to iMessage on the platform I'm sitting in front of.  Mac people do, of course.  But Signal now cross-desktop.  There's also, and I heard you talking about it, it must have been with Jeff and Stacey last week, this embarrassing glitch in Google Docs, which surprised a lot of people about how much Google was looking at what they're doing.



LEO:  Yeah, no kidding.



STEVE:  Yes.  Also some bad behavior from an audio driver installer.  A pending RFC for IoT updating, I mean, if there was anything on our Christmas wish list, that's what it would be.  Also two different reactions to Windows 10 Controlled Folder Access that we talked about last week.  A bit of miscellany.  If we have any time left, we will have some feedback from our users before we get to, yes, it's worse than you know, and it usually is, talking about ROCA.



LEO:  I love it.



STEVE:  So I think another great podcast.



LEO:  Well, I can't wait, as they say.  All right.



STEVE:  So our Picture of the Week, which we are not going to skip, as I forgot to last week...



LEO:  Well, we ended up showing it.



STEVE:  Yes, thank you for reminding me.  It's like, Steve, did you forget something?  And this is interesting in the context of everything we've been talking about relative to the vulnerability of the devices which are facing the Internet.  And someone sent this to me, and I thought it was just perfect.  It's a notice sent from Charter Communications noting that TWC is now Spectrum.  And the title is "Important Information Regarding Your Internet/Voice Modem."



"Dear Valued Customer," it reads.  "As part of our continued effort to give you the best service experience possible, we will be performing an upgrade to your modem.  We will begin updates starting November 6, 2017.  Updates will occur between the hours of midnight and 6:00 a.m., Monday to Friday, during a four- to six-week period."  So that sounds like they're going to be pushing patches, firmware, to the entire customer base during the early morning hours so that it's less intrusive.



They say:  "During this upgrade, your modem will go through a series of steps and will reboot.  As a result of the reboot, your modem will be offline for two to four minutes.  During this time you will not be able to access the Internet, receive incoming calls, or be able to dial 911 and 611, and your local and long-distance residential voice service will be temporarily disabled.  Your modem should not be powered off or disconnected during this upgrade as this could cause your equipment upgrade to not occur."  Yeah, no kidding.  Sorry, we're not able to install new firmware if you've unplugged it.  "Once complete, your device will automatically reboot.  We hope you will enjoy your improved features, security, and more with this upgrade." 



So of course they're not saying what this is.  But so it's a good sign that they're being proactive.  And presumably there are, I mean, to do something like this, which you don't see that often, there is something that they feel they need to fix.  So props for them doing it, and looks like it's a pretty big campaign for them.



Okay.  So the inevitable dilution of the value of code signing.  A report came out in the last week from some researchers, I think it was three, oh, yeah, three University of Maryland at College Park researchers who took a look at the prevalence of signed malware, which unfortunately is no longer an oxymoron.



LEO:  I was going to say, that's a paradox.  Signed malware?



STEVE:  Exactly.  So, okay.  So we know about signatures.  You take some blob of anything, and you hash it to create a fingerprint of that blob.  And the power of the hash function is that you change even, you know, any one bit change in the source that you're feeding in on average inverts 50 percent of the hash's bits.  And there's no way to design an augmentation or a change to the source that doesn't dramatically change the hash.  So essentially the hash is a fingerprint.  You then sign the fingerprint in a way so that the validity of the signature can later be verified.  So, and we've talked about of course doing this in all different kinds of contexts.



Well, Authenticode is what Microsoft calls their digital signing system.  And over time the requirement for that's been growing.  For example, it was controversial when Microsoft first introduced the idea that drivers would need to be signed.  And of course that's sort of, you could argue, the most critical portion of the system because the driver by its nature is running with full privileges down in the kernel, able to do anything it wants.  So you would hope that signing something brings more integrity to it, more trustworthiness.  GRC's apps, my stuff, ever since this sort of became important, has been signed.  So, for example, Never10 has a digital signature.  All of the SQRL betas have digital signatures.  I've been signing stuff just because, for example, especially for new software, AV, antivirus software, will use that, whether or not software has a signature, as another signal to it.



So, for example, if Gibson Research Corporation achieves a positive reputation with an AV vendor, and something new comes along that itself doesn't have a reputation, but it's digitally signed by a company that does have a reputation, then by inference you can say, oh, well, we trust them, so we trust what they have signed.  So there's sort of the same so of implied trustworthiness downstream from signing.



Now, the problem, though, is that there's a much lower bar for qualification for a code-signing certificate.  Basically you raise your hand and you say, "Hi there, CA, Certificate Authority.  I'm breathing.  And I want to sign code."  And they go, "Okay, pay us."  And so you pay them some money, and they give you...



LEO:  Are you sure you have to be breathing?  I mean...



STEVE:  Actually, bots could probably do it, too.



LEO:  Bots can do it, too, yeah.



STEVE:  Yeah.  So unlike the traditional relationship that we've talked about often here with certificates, where it's a DV where you need to prove domain control, or an OV where you need to approve organization control, or an EV, which is the highest level of enhanced verification.  This is basically just, "Hi, I want to sign stuff," and you buy a certificate, and you can.  So naturally, as soon as AV software, antiviral software began, well, began using signatures as a signal, an additional signal for it to use, malware authors began saying, oh, we should be signing our malware.



So these three guys at the University of Maryland at College Park took a look at 325 signed malware samples.  Of those 325, 189, which was 58.2 percent, carried valid digital signatures.  Which is to say that somebody issued, apparently the malware authors, or maybe a front for them, a valid signature, I mean, a valid certificate that they could then use to sign their code.  Just like I do.  I mean, I have a certificate, of course, from DigiCert, and I'm signing - I signed Never10.  I sign all the version of SQRL that we're working on.  So the problem is it's not difficult to get one.



And so, but what's really interesting and a little disturbing is in their research they found that, as I said, 189 out of 325, which is just shy of 60 percent, carried valid digital signatures, whereas the balance of those signed malware samples, which is to say 136 of them, carried malformed signatures.  That is, anyone actually verifying the signature would find it wasn't any good.  It was some blob, some signature taken from other software that had simply been stuck onto the malware.  And what they found was that many very prominent antiviral software don't take the time to check the validity of the signature.



LEO:  Aw, geez.



STEVE:  I know.



LEO:  What a surprise.  These guys are nitwits.  Oh, my.



STEVE:  They're just saying, is there a signature there?  Okay.  We're in a hurry.  Fine.  And they also found that there tended to be a lot of reuse and that, even in known instances where malware, malicious software had been signed by digital signatures, and in the case of 111 which were used to sign those 189 samples, only 27 had been revoked, while 84 remained valid.  And of course we've talked a lot about how crippled the certificate revocation system is.  It just, unfortunately, it's not revoked by default.  It's revoked if you take the time to check.



Well, if they're not checking to see whether the signature is valid in the first place, they're certainly not going to go do an Internet query to pull up the CRL, the Certificate Revocation List, if the CA even bothers to manage revocation, which in this case many of them don't because, as we know, our operating systems are, like, trust everybody.  Trust all of these CAs.  Any of them are able to produce signatures or to hand out certificates which malware authors can then use to sign their code.



So unfortunately, what's happened is what started out to be sort of a good idea, let's require signatures in order to attest to and assert a higher level of integrity of the software, it's just become useless.  They found that a large fraction, in this case 88.8%, of malware families rely on a single certificate, which suggests that abusive certificates are mostly controlled by the malware authors, rather than third parties.  That is, no one bothers to go steal somebody else's cert because you can just ask someone, some CA - and they're not all reputable, but they still exist.  Ask them for a cert, and they'll sell you one, and then you can just sign your malware.



And so then, until it's determined to be illegitimate, AV, no matter whether they test or not, I mean, if they do test the validity of the certificate, they'll say, oh, this is fine.  It passes all of our checks.  And what we now know is many of them don't even do that.  They found that at least 34 current antivirus products fail to check the certificate's validity.



LEO:  I didn't even know there were 34.  I'm close to all of them, I would guess.



STEVE:  So three prominent antivirus products - nProtect, Tencent, and Paloalto - detected the original unsigned ransomware samples as malware, but considered eight out of 10 crafted samples, that is, these researchers took unsigned malware.  Those three products all said, okay, yes, we see them as malware.  These guys, these researchers, then stuck on some random signature from somewhere else that was invalid for that code, and eight out of 10 of the samples they tested were then passed as benign.  Kaspersky Labs, Microsoft, Trend Micro, Symantec, and Comodo also failed to detect some of the known malicious samples when they had invalid and expired certificates attached to their code.  They didn't even check the date on the certificate.



So basically it looks like these companies are in such a hurry to not slow down their scan that they hoped nobody would notice that, if software is carrying a signature, even if it's expired, even if it's like that they got from something else and stuck it on, I mean, where the signature doesn't even work because it signed a different hash than the code that it was stuck onto.  Lots of products don't care.  They go, oh, it's signed.  Well, okay.  Or rather it has a signature because signing implies that the hash matches, which this doesn't.



So anyway, I'm super glad for the exposure that these guys created.  And what this allows people to do is perform similar tests.  In fact, they created a site to back up their research paper, SignedMalware.org, just all one word, SignedMalware.org, where they run through all of the malware that they found, the certificates that are stuck on them, who's issuing these certs.  And, I mean, it's not the issuer's fault.  I mean, many of them are VeriSign and legitimate certificate authorities.  The problem is anybody can get one.  And now what we're learning is that getting one sends a signal to the malware that don't look any further.  Don't scan this code.  Don't.  Just like, oh, look, we have a signature.  Fine, keep moving.  Wow.



LEO:  It's amazing.  Just amazing.



STEVE:  Yeah.



LEO:  Easily fixed.  I presume all antiviruses will start paying attention.



STEVE:  Yes.  It will, yes, when the light is shined on them, they don't want to be in the doghouse.  So they'll have to check the signature.  They'll check the date.  And they'll have to do a much better job of checking.



LEO:  Did they say which ones passed?  Like there were some that did it right?



STEVE:  I didn't dig into the actual paper.  There is a full-length research paper that they've put out.  And, I mean, they are naming names.  So I wouldn't be surprised if they...



LEO:  When they say Microsoft, is that Security Essentials?  Is that the Defender that's installed with Windows 10?  Or is that some sort of separate - because that would be disturbing, that Windows...



STEVE:  That would be disturbing, yes.



LEO:  We're telling people to rely on the Windows antivirus as sufficient.



STEVE:  Well, and I wouldn't know - yes.  I wouldn't change my advice because it looks like all the competitors are doing the same thing.



LEO:  Nobody else is better, yeah, yeah.



STEVE:  So it's like, ah, yeah, okay.  So Robin Linus has a GitHub page and posted a nice piece of research.  Well, a little bit disturbing.  It turns out that, as a consequence of some clever design leveraging, it is possible for a site you visit to determine whether your browser is currently logged into any other of many possible sites.



So let me explain how this works because it's not too complicated.  We've often talked about how the same-origin policy, the so-called SOP, same-origin policy strongly restricts which web content JavaScript which has been received from a website, restricts what web content JavaScript received from a website is able to access.  Specifically, JavaScript is then, once it's running in your browser page, can only fetch more things or do stuff with the site it came from, thank god.  Otherwise it could be, you know, you'd be doing - you would be loading code into your browser that itself could go do anything else it wanted to on the 'Net, and havoc would reign.



So that's rigorously enforced except for images.  Images, being benign and useful, are specifically excluded from same-origin policy, which you're able to use for many purposes.  In fact, I even use it with SQRL in the spoof-proofing system that SQRL uses when you come onto a page where you can log on with SQRL.  The JavaScript on the page queries to see whether you've got SQRL running in your system, and it does so by asking for an image, which is cross-origin because that code is from the site you're logging into, and we're checking on your own system.



So, I mean, it's a very common practice and can be very useful to allow images to be cross-origin.  In this case, though, it creates a problem.  The login mechanisms of most major sites - and Leo, you should go to, oh, I have the link here at the top of the story, yeah, "socialmedia-leak."  If you click that, it'll bring up an immediate display of all the sites you are currently logged into.



LEO:  Oh, my god.



STEVE:  Uh-huh.  So this is like a social media privacy leak.



LEO:  Facebook, Twitter, Gmail, Reddit, YouTube, Blogger, Disqus.



STEVE:  Yes. 



LEO:  Wow.



STEVE:  So this GitHub.io domain, this page on that domain has just been able to determine all the sites that your browser currently has session cookies for.



LEO:  And this would be worse if I weren't using Linux.  Some of these other things would also be logged into, obviously.



STEVE:  Right.



LEO:  Like Dropbox.  Wow.



STEVE:  Yes.



LEO:  Let me do it on my - I'm going to do it on my...



STEVE:  Yeah, uh-huh.  So what happens is that the login mechanisms of most major sites check every incoming request for the presence of a logged-in session cookie because, as we know, when your browser carries cookies for a given domain, every query it makes, it hands those cookies back.  And that's how we maintain session state and logged-in-ness with sites.  So if you go to a site where you're not logged in, you typically, if you try to go to any page there, you get an intercept, and you are instead taken to the login page to log yourself in.



And so that means that the behavior of the site is different.  If you are logged in, and you for example ask for an image, it just gives you the image because that's what your browser would be doing if it were asking for a bunch of images to populate the page.  If you're not logged in, and you ask for an image, it says, "Oh, we don't know who you are," and bounces you over to the referral page.  So if you combine JavaScript's ability by design, not a bug, by design to be cross-origin for images, just those two features allows you to do what this page is doing, which is JavaScript running on the page asks for an image almost all sites have.  And that's the favicon.ico.



LEO:  Yeah, my site has it, sure.



STEVE:  Well, everybody does because that's where you get your logos in the tabs and on the browser title and everywhere.  So everybody wants their site's logo to show up on browser tabs.  And so favicon.ico is a file in the root page of all servers.  So this script asks for that favicon.ico.  If it succeeds, JavaScript will trigger an on-success result, which means you're logged into that other site.  And if it fails, it's because you got a redirect because you're not logged in, and JavaScript goes, okay, we didn't get a successful image load.  Which, now, okay.  So that's annoying; right?  But ads are allowed to run JavaScript in order to do their ad rotations and what, I mean, we often talk about what a problem it is that third-party ads hosted on benign sites can get up to so much mischief.



Well, now an ad that is being hosted on a site you're visiting is able to probe your browser to determine what suite of other sites you're logged into.  So this is the kind of thing which sites could prevent by looking for this behavior, but no one's bothering.  And this guy is hoping that his shining a light on this is going to bring it to people's attention and begin to get this fixed, which would certainly be nice because it's a little shocking when you go to that page, and it's like, oh, wow, this third-party site knows all the places I'm logged into currently.  I mean, it doesn't tell them who you are.  But it's a form of fingerprint.



LEO:  That's really, yeah, it's a favicon.  Of course, what else?  That's the one thing everybody has. 



STEVE:  Yup, yup, yup.  That's very clever.  So, okay.  As we were saying at the top of the show, the press went wild over the last couple days over this MINIX 3, which is the latest edition of MINIX, which is a minimal Unix which has been around for quite a while.  It's meant to be very robust and sort of self-healing.  I love it, there's actually something known as the "resurrection process" running in it, where if something crashes that shouldn't, it resurrects it and brings it back.  It's got a very small kernel.  Most of the stuff runs out in user space.  So it's also a sort of a - it's a microkernel architecture which hasn't been messed up by commercial interests.



LEO:  People who are familiar with Linux will of course recognize it as the inspiration for Linux because Linus - it was originally written as a teaching kernel, teaching OS by Andrew Tanenbaum.



STEVE:  Right.  It was Tanenbaum, right, yes.



LEO:  And it was what Linus, who was a com-sci Ph.D. student, was forced to use.  And he one long winter Finnish night decided, I'm going to write a better MINIX.  And so Linux really stands for Linus's MINIX, of all things.



STEVE:  Yes.



LEO:  So that's kind of interesting.  Now, it's come a long way from the educational platform, I gather.  It's really - now it's a commercial OS, sort of.



STEVE:  Yes, it is.  It is a very solid, mature, usable - and because it's so robust, it's used in embedded applications.  So not surprisingly, when Intel was wanting to increase the power and capabilities of their embedded IME, the Intel Management Engine, they decided to put a processor running MINIX in the so-called PCH chip, the Platform Controller Hub.  And in fact in the next page of the show notes I show a picture of the Platform Controller Hub.  The press all got this wrong, of course.  They were saying that it was the Intel processor that had this extra OS embedded in it.  And it's like, no, it's not.  It's part of the chipset support system that has this in it.  And so, I mean, you're stuck with it.  It's there.  Well, you're kind of stuck with it.  We'll talk about killing it off in two stories from now.



But I just sort of wanted to clarify, for anyone who caught this and was wondering what was going on, that in fact it's the OS, which we do have no access to.  It is very mature.  It's very small.  The microkernel is like 600K.  And so it would make sense that they would want something toned down.  But when you hear things like the IME has a full networking stack, it's like, what?  In hardware?  Well, no, it's actually MINIX running a driver for the NIC, the Network Interface Controller on the motherboard.



LEO:  Here's a weird coincidence.  Just as you were starting to talk about this, I got an update for my Intel Management Engine 11.8 firmware on my Windows machine.  Wow.  And it says, by the way, we're going to reboot you, so save your work.  Ugh.  This isn't related to this, I'm sure.



STEVE:  Well, Intel's not happy...



LEO:  Oh, maybe it is.



STEVE:  ...that people are now beginning to disable it because there is, in the classic - and I love this - Internet collaboration that we have as a consequence of open source stuff, a guy named Nicola Corna has on GitHub something he calls the "ME Cleaner," the Management Engine Cleaner.  And it's a beautiful piece of work.  If you give this open source code that he's been working on and honing for some time, you give it a firmware image of the firmware from the BIOS, you give it your motherboard's BIOS firmware image, it will go in and remove the management engine.  



LEO:  That doesn't sound good necessarily.  What are the consequences of this?



STEVE:  Yes.  So there are two modules that have to remain, and one of them is the, not surprisingly, the boot module that gets things going.  The other one is like the power-on management that gets clocks set and so forth so that anything is able to happen.  But then there's all this other junk in there, which is the stuff that, like, grabs your network interface adapter and, as we've been discussing, has a lot of people worried from a security standpoint because there have been bugs in it.  Remember there was a huge scramble because a serious security vulnerability was found four or five months ago.



LEO:  Now I'm really worried.  Okay.  So I'm looking at this firmware update.  There's, okay, it says "Before continuing please ensure the following.  Make sure the AC adapter is firmly connected to system outlet.  Make sure that a charge battery pack" - there's either a typo, or this person doesn't speak English very well.  This is supposed to come from Intel.  Now what am I supposed to do?



STEVE:  Yeah, yeah.  And, boy, if that were malicious...



LEO:  Oh, Mother McCray.  I'm just glad you're here.  I'm going to cancel.  I don't think I want this.



STEVE:  Yeah.  Well, I mean, what's it going to do for you?  Probably, I mean, maybe you could do some research, verify that...



LEO:  There were bugs earlier; right?  I mean...



STEVE:  Correct, yes, correct.  Okay.  So as we know, nearly all systems allow the system's BIOS to be updated using software only.  Many times when you get a motherboard it's like, oh, check to make sure you're running the latest BIOS.  The common wisdom had been, and we talked about this at the time, don't mess with your BIOS unless you need to.  But when there's something that you know a BIOS update fixes that's a problem for you, then, yes, apply it.



Okay.  So the software-only fixing is regarded or called "internal flashing."  But on most PCs, only an unprotected area of the flash file system, which excludes the management engine area, can be overwritten by software.  Makes sense.  They would not want you to be flashing their proprietary stuff.  So working off of Nicola Corna's work on ME Cleaner, another individual has come up with a complete "Disabling the Intel Management Engine" guide, step by step.  And Leo, you really should click this link and scroll through it because, I mean, if you thought you were, I mean, it really is a hardware hack.  So what has to be done in order to do so-called "external flashing," in order to overwrite the Management Engine, is to literally reprogram the flash chip on your motherboard.  And so he sets up a Raspberry Pi 3, Model B, as an in-system flash programmer.  It reads the original firmware from the little...



LEO:  Oh, my god.



STEVE:  It's a little tiny eight-pin chip sitting on the motherboard.



LEO:  I am not doing this.



STEVE:  No, no.  This is not for the faint of heart.  It reads the firmware, extracting it from the chip into a file.  You then give it to Nicola's ME Cleaner.  Oh, and if you look, if you scroll down further, you could see the ME Cleaner running.  Or I think you have to go to the ME Cleaner page, GitHub.com/corna/me_cleaner.  I've got that link below that first one in the show notes.  And you can see what ME Cleaner is doing as you run it, where it's recognizing, it's interpreting the firmware that you've given it, and it's unscrambling it and figuring things out and then blasting crap out of the firmware, saying, okay, we don't need that, and we don't need that, and we don't need that, and we don't need that.  And we've got to keep this and this and this.  Oh, and it also flips - remember that bit that the NSA required for their motherboards?  There's a special bit that can be flipped which allows the BIOS to turn off IME.  It also flips that, just so then you get a new BIOS setting to turn it off just for sort of belt-and-suspenders reasons.



But anyway, so this how-to extract - and there's the chip on the screen.  You're showing it right now, a little surface-mount eight-pin chip.  You get a clip, which you snap over its back in order to connect to the wires.  And then if you scroll down a little bit further, you'll see the diagram of the chip showing power and ground and where the signals in and out are.  And again, you need to be a hardware guy or be comfortable with hardware stuff in order to do this.



LEO:  Or buy the Purism Librem laptop, which has this done already to it.



STEVE:  Yes.  Well, and so my thinking is where this would be practical - first of all, if you were just a hacker, and you had an older motherboard that you wanted to screw around with, this would be fun.  Or if you were an enterprise, where you had a thousand identical systems because you bought them all at once - yup, you've got the Pomona clip there, which is used for hooking to the chip.



LEO:  I don't know why Burke has this, but we got it, yeah, yeah, yeah.



STEVE:  So if you had like a thousand of these identical systems and could spare, like risk destroying one because you might destroy it, but the benefit was, if you really cared about shutting down this unneeded Management Engine, if in your organization in fact it was not needed, if you weren't using it for its purpose, then you could try this on one motherboard, verify that it works, and then apply it to all of the rest of the many hundreds or however many motherboards your corporation has.  So my point is that by amortizing the risk across the benefit of hundreds of motherboards, maybe it would make sense.



So anyway, what this does is it hardware reads the firmware into a file.  You then hand it over to Nicola's ME Cleaner, which flips some bits and overwrites and removes a bunch of the junk you don't want.  And then you, using the Raspberry Pi, rewrite the new firmware back onto the motherboard chip.  Then you plug the power in and hold your breath.  And if it boots, you no longer have IME running on that motherboard.  Which is pretty cool, I think.  Oh, and if it doesn't, then presumably you can reflash the original file and at least restore this thing to normal operation, and it wouldn't be any the worse for wear.  But don't try this at home, kids.



LEO:  Yeah, yeah.



STEVE:  Definitely for a high-end hacker, but also cool.



LEO:  I doubt many will do this.  You'd have to be extremely paranoid.



STEVE:  Yeah.  So...



LEO:  Continuing on.



STEVE:  So spoofing we've been talking about a lot lately.  It's the unsolvable problem because it's probably the ultimate abuse of user trust of the world, or I don't want to put too much responsibility on the user and say "user inattention."  I mean, we're all human.  We're all inattentive.  Sometimes we're in a hurry.  Sometimes we don't remember to go check the URL and make sure of the domain we're looking at, or we don't hover the mouse over the link.  The point is that too much responsibility falls to us, and that gets abused.



So there was again a lot of coverage over the last week about an extremely convincing WhatsApp fake which was downloaded more than a million times from the Google Play Store.  And what's disturbing is that it was such a simple thing to do.  Whoever this was who perpetrated it simply added a unicode space character to the end of WhatsApp, Inc.  So that didn't show, of course, on the screen.  And users said, oh, this is from WhatsApp, Inc.  It says so right here.  And a million people downloaded this thing.



Now, the good news is it wasn't, didn't do horrible things to them.  It was simply - it required minimal permissions, and it was a whole bundle of ads for other apps.  So people quickly learned that, okay, this isn't what I thought it was going to be.



LEO:  How did this get by Google?  That's my question.



STEVE:  I know.



LEO:  They'd only have to run it to realize that; right?



STEVE:  Exactly.  Exactly.



LEO:  Terrible.



STEVE:  And so that also suggests that there's minimal curation going on.  And when you consider the size of the store, how much stuff there is, it's almost understandable.  Again, what they'll have to do is add some code to prevent trailing spaces or leading spaces or embedded unicode, you know, put some filters on that in order to try to reduce the spoofing.  But this is just, I mean, this wasn't a huge security issue.  But it could have been.  And it just demonstrates that most users are, even if you checked to make sure that it was legitimate, as far as you could tell, that it was WhatsApp, Inc., which is what you'd expect.  It's like, oh, okay, good, it's from WhatsApp.  But, whoops, it's not.



LEO:  Very disappointing.



STEVE:  It is, yes.  An Italian researcher discovered that the macOS and Linux flavors of the Firefox-descended privacy-enhancing browser used by Tor was leaking their users' IP address.  Whoops.  Because of course the reason you use Tor is to bounce your traffic among onion routers and not reveal your true source, your true IP address.  He named it TorMoil and released it privately to the Tor Project so that they were able to fix it before it became public.  He's the CEO of a security firm named We Are Segment.  And 12 days ago, on Thursday, October 26th, he let the Tor developers know, and they rolled out an emergency update version 7.0.8.  Windows users were never affected, so only people on macOS and Linux.



And then the following day they posted, that is, the Tor Project posted:  "The fix we deployed is just a workaround stopping the leak."  Because they were horrified.  So basically they just shut down the class of URL which was the file://.  As people may know, we're used to http:// or mail://.  It turns out file:// can be used to retrieve a local file on your system.  And a small glitch in the Tor privacy-enhanced browser for Mac and Linux allowed there to be a way to get the user's IP from using a file:// URL.



So what they wrote was, "As a result of that," that is, of the quick fix, "navigating to file://URLs in the browser might not work as expected anymore.  In particular, entering file:// URLs in the URL bar and clicking on resulting links is broken.  Opening those in a new tab or new window does not work, either.  A workaround for those issues is dragging the link into the URL bar or on a tab instead."  So I'm just guessing, but that sounds like something about the referrer because, if you were to click on a URL on a page that came from a file, maybe the recipient of that query in the referrer there was the IP; whereas dragging the link into the URL probably looks like a fresh launch rather than a referred URL.  So that would make sense.



It sounds like what they did was they just quickly shut it down completely, just like broke it in order to produce, in the future, roll out a better fix.  They just wanted to immediately nip that in the bud.  And it's not a feature that would probably upset or be missed by many people.  You'd have to be clicking on a link to a malicious site that knew to collect that information which was contained on a file on your system, so a rather circuitous means to get someone to do that.  But again, they want to keep their Tor browser as watertight as possible and not allow that kind of leakage.  So anyway, I thought that was interesting.



And at the same time we are about to have a substantial upgrade to Tor after four years of work.  They're calling it the "Next Gen Onion Services," also known as Tor v3.  It's now in alpha, has been for a couple weeks, and they're getting it ready for primetime.  Within the Tor community this has been known as Proposal 224, and it does a bunch of things.  It offers better crypto.  They are replacing the use of SHA-1 and Diffie-Hellman and RSA-1024 with our favorite stuff - SHA-3, and they're replacing Diffie-Hellman with the Edwards 25519 Curve which SQRL of course also uses, and they're replacing RSA-1024 with Curve25519, both of which I had chosen a couple years ago when the whole concept of SQRL occurred to me.



So they're updating Tor's crypto to what's state of the art.  They've improved the directory protocol, which as a consequence will leak much less information to directory servers than Tor has been, and we've talked about that modest leakage sometime in the past.  Also another improvement will create a much smaller surface for targeted attacks on the directory protocol.



They've got a better onion address security to protect against impersonation, and in fact they've made it much longer.  I have in the show notes an example of an earlier, the original onion address, which is what, [counts to 10], looks like maybe about 30 characters of address, or maybe 21 or something.  The new one is 56.  So it's much longer.  In fact, that's the quick way for anyone to determine whether they're looking at a v3 onion protocol or a pre-v3, as all pre is a short link, and the new onion links are conspicuously longer, which is in order to create additional security and privacy.  They've also developed a more extensive introduction/rendezvous protocol and really done a big revamp of the code.  It's a much cleaner and more modular code base.



So this is basically four years of work and a significant bump to the Tor protocol, which is now, as I said, in alpha, and which we should be seeing coming out of beta before long, which will be great.  And I forgot to mute some of my iDevices around me, so we're getting some background noise.



Oh, and as I mentioned at the top of the show, Signal.org, the Signal people behind the Signal super-secure messaging platform, have announced standalone Signal clients for our desktops.  If you go to Signal.org/download, you will see that page now boasting Signal for Android, for iPhone, for macOS, for Windows, and for Debian-based Linux.  And they are formally deprecating the Signal for the Chrome browser.  So that's been removed.  The new desktop version of Signal runs independently of any browser, so that Firefox and Safari users will no longer need to use Chrome just to send and receive Signal messages, like using Chrome only as a client platform for the Signal client.  And Chrome users will no longer need to have Chrome open just to be able to use Signal.  64-bit versions of Windows only are supported, from Windows 7, 8, 8.1, and 10; macOS from 10.9 and above; and Linux distributions supporting APT like Ubuntu and Debian.



So this is great.  And it'll be interesting to see what, if any, pressure this puts on Apple to extend the reach of iMessage.  Maybe they just won't care.  I mean, in the same way they don't care about Flash, they'll just say no, we're not doing that.  But as a Windows user, as I mentioned, I chafe a bit that I cannot have iMessage on my Windows desktop because I use it to glue a lot of my dialogues with my friends together.



And being able to link my friends together across all our desktops using Signal, which is as we know really, really, really well done - we talked about the Ratchet protocol some months ago.  And I remember when I took a serious look at it, and we did a podcast on it, my first impression, as I was first looking at the beginning of the whitepaper, was that it was ridiculously over-engineered.  I was reading, you know, this, it does this and this and this and this.  And I was thinking, what the hell?  Why?  But by the time I got to the end, and I saw why they had done what they had done, it all pulled together, and I was very impressed and told our podcast listeners at the time.  It's like, okay, these guys really did this right.



So we already know Signal on our mobile devices is arguably the strongest messaging platform available.  Now we have it running and available on our desktops.  And I would say, if you want to have a private ongoing dialogue from desktop to desktop somewhere, don't use Skype.  Don't use anything other than Signal because it's open source, it's open protocol, it's been vetted like crazy, it was designed by very mature crypto people.  It's the one to choose.  And now you can use it on our OS desktops.  So bravo to those guys for adding that to it.



And Leo, you were talking, after this came to light, about this Google Docs glitch which surprised some people by locking them out of their own private documents, which occurred last Tuesday.  So it must have been you were talking with Stacey and Jeff last Wednesday, and I must have had the podcast playing while I was working on SQRL in the background because when I'm not writing English I'm able to listen to English in the background.  So what happened was that some Google Docs users received an alert saying:  "This item has been flagged as inappropriate and can no longer be shared."  Others saw:  "You cannot access this item because it is in violation of our terms of service."  When, like, private people were just wanting to access their own docs.



This occurred, because it was on Tuesday, that was of course Halloween last week.  Rachel Bale, for example, who is a high-profile individual, she's a reporter for National Geographic's Wildlife Watch, and she reports on wildlife crime.  She tweeted the question:  "Has anyone had Google Docs lock you out of a doc before?  My draft of a story about wildlife crime was just frozen for violating their ToS," their terms of service.  And unfortunately Google was a little - was deflecting a bit.  They said:  "Google's automated systems periodically scan certain file types in Google Drive that are shared with other users to detect abuse and protect users."



LEO:  You could see why they would do that.  It's almost, I mean, it's malware protection.



STEVE:  Yes, yes.



LEO:  And it's only when it's been shared.  It's not just your stuff sitting on your drive.  They have to share it.  You have to share it.



STEVE:  Okay.  So probably she, for example...



LEO:  She shared a link for collaboration.



STEVE:  Or maybe she shared the folder.  And so, for example, you and I share the Security Now! folder so you can always just grab whatever that happens to be in there.



LEO:  It kind of makes sense.  I mean, on the one hand, of course, it's disturbing.  But if somebody is sending out share links, there's the potential that those share links could contain malware.



STEVE:  Yup.



LEO:  So I can understand why they might want to scan those.



STEVE:  And so what was the glitch?  How did this, like, I mean, because it was a [crosstalk] mistake.



LEO:  The glitch was that they - yeah.  So what Google said is we scan - antivirus scanning, malware, phishing detection.  But she just said something, I don't know what happened.  It may have been, well, the glitch might have been that it was a false positive; right? 



STEVE:  So, yeah.  So it may have been - so maybe their heuristics were updated.  I heard something about something, a code push that caused the problem.



LEO:  Right.  So that's probably what it was.  I mean...



STEVE:  And so this has always been going on, but people weren't, because it wasn't false positiving, people were unaware that that could happen to them.



LEO:  Yeah.  So, I mean, we were concerned about it.  Obviously you don't want to know that what you have presumed your Google Docs to be private, you don't want them not to be private.  But this is, upon further digging, it's only when it's being shared.



STEVE:  Right.



LEO:  And so that kind of makes sense.  That's to protect people from sharing malicious docs.



STEVE:  Right.  So under the topic of why we can't have nice things, or in this case secure things, US-CERT has added a vulnerability and some mitigation after discovering that, believe it or not, an audio driver manufacturer - there's a company Savitech, S-A-V-I-T-E-C-H, which provides USB audio drivers for a number of specialized audio products.  When you install their driver into your system, some versions of this driver package silently install their own SaviAudio root CA certificate into your Windows trusted root certificate store.  So it's like, you know, okay, wait.  What that says is they were unwilling to buy a certificate from a CA that's already trusted. 



LEO:  Oh, geez.



STEVE:  So instead they basically did a self-signed cert.  They said, no, we don't want to - we're not going to buy a certificate from a CA.  We're going to take advantage of the fact that the user will click Okay no matter what we show them, and up comes UAC.  Do you want this driver to install stuff into your machine?  What are you going to say?  You say yeah, I need this audio driver.  Well, along for the ride is their own root CA, which now means that anything they choose to sign will henceforth, forever probably, be trusted by your computer, just as if it was a malicious CA.  And to make matters worse, it turns out this was done for Windows XP, which is no longer necessary, but they never bothered to remove it from the installer on later operating systems.



LEO:  I guarantee you somebody got a raise for thinking of this.  Oh, boss, here's an idea.



STEVE:  Yup.  We can save 100 bucks.



LEO:  Yeah, we'll save money.



STEVE:  Wow.  Wow.



LEO:  That's so funny.



STEVE:  So, okay.  As I said at the top of the show, my number one wish list item for this Christmas would be that we solve the problem of updating IoT things, the Internet of Things devices.  What just happened, I think it was October 20th, or I'm sorry, October 30th, I had that date in my head.  And so just two weeks ago, or actually Monday before last, was a draft from a team of three at ARM, which is where you want it to be, the ARM guys know what they're doing, a lot of these IoT things are ARM-based.  It's a proposal, it's a draft, Internet RFC working draft, to propose a set of standards for IoT updating mechanisms.  I read the whole thing.  It is well thought through.  And I'm just, I mean, all my fingers and toes are crossed.  It would be an incredibly welcome addition to this entirely vacant need that we have at the moment.



I'm not going to go through it in detail because at this point it's premature. But if it ever happens, if it gets assigned an RFC and becomes any sort of standard, we will absolutely be talking about it.  But I'll just read just a few words here from the introduction.  They said:  "When developing IoT devices, one of the most difficult problems to solve is how to update the firmware on the device.  Once the device is deployed, firmware updates play a critical part in its lifetime, particularly when devices have a long lifetime, are deployed in remote or inaccessible areas, or where manual intervention is cost prohibitive or otherwise difficult."



So they say fixes to bugs in software that can be applied to the device with a firmware update are needed.  New functionality can be added to the device with a firmware update.  And whatever this updating process turns out to be, it must ensure that the firmware is authenticated, attempts to flash a malicious firmware are prevented, and that the firmware can be confidentially protected, that is, the firmware's confidentiality can be protected so that attempts by an adversary to recover the plaintext binary can be prevented.



So they go through in this proposal the things that would be needed.  For example, there needs to be side-by-side firmware so that there's the current boot firmware and a second sort of sidecar receptacle where an entire next iteration can be downloaded and installed, I mean, like be completely ready.  That way if there's a power outage during the firmware update, basically, Leo, the things your laptop was just warning you you'd better - it's got to be plugged in.  You've got to have a battery, blah blah blah.



These guys are saying, okay, we don't want those to be limitations.  They may be practical for a laptop.  They're not practical for stuff that may be autonomously updating itself when you pull the plug because you want to move it to a different room.  You don't want that to brick your device.  So the idea would be that a shadow firmware region would exist that would receive updated firmware.  Its signature would be verified.  It would be completely confirmed.  And then hardware would reboot the device, and a watchdog would verify that the device came up under the new firmware.  And if it didn't, the device would restart again with falling back to the old.



So, I mean, my point is they nailed this.  They sat down, they said, okay, how do we create a robust architecture which is still inexpensive, so that it's actually going to be adopted, but which incorporates everything that we need in order to set this as a standard.  So I'm holding my breath because it would be great if we solved this problem.  And then, if it were a standard, it would be something that consumers could look for in the same way that the Wi-Fi Alliance has their logo and Bluetooth has their logo, give this thing some sort of a logo so that people can verify that this has self-updating firmware technology built in.  And, boy, that would be great.



LEO:  Little Stevie G, we call him.



STEVE:  Not what we're calling him.  So two contrary opinions about Windows 10 Controlled Folder Access that we discussed last week as a feature coming to the Fall Creators Update of Win10.



LEO:  Oh, interesting.



STEVE:  First of all, Jakob Engblom, he wrote and sent me a tweet saying:  "Windows 10 CFA is off by default for a good reason.  It breaks a bit too much stuff."  And then he sent his analysis, a link to his analysis.



LEO:  It's common with a lot of these things like ASLR and stuff like that.  It's just stuff breaks.



STEVE:  Yes.  And so I think that's why it makes sense, as we said last week.  This is what Microsoft generally does is they'll put it in there.  They'll have it off by default so that the user takes some responsibility for turning it on and hopefully then knows what it is they did when stuff doesn't work anymore.  Whereas - I liked his handle.  His name is probably Barry Wallis, so his Twitter name is Cranbarry Wallis.  Anyway, he said:  "@SGgrc, I turned on Controlled Folder Access.  There were two old Nikon photo editing programs I had to whitelist, but that was it.  It works great."



LEO:  Interesting.



STEVE:  So my takeaway is...



LEO:  Try it.



STEVE:  If you're concerned - yes, try it.  If you're concerned about cryptomalware getting into your machine and wreaking havoc, boy, I think it makes sense to have your stuff that's most vulnerable protected by a whitelisting system.



LEO:  Of course it's what Apple's done with iOS since day one.



STEVE:  Yes, yes.



LEO:  Google tried to add it to Android.  It broke a bunch of stuff.  They backed off a little.



STEVE:  Because it's difficult to retrofit.  That's the problem is adding it later.



LEO:  Yeah, yeah.



STEVE:  Yes.  And so a couple little bits of miscellany.  There's a great blogger and tweeter who tweets from @SwiftOnSecurity.  And I just love this.  Someone forwarded it to me.  So his tweet is:  "We were the victim of a very simple attack.  It was through management's lack of focus on security that this happened," dot dot dot, said no company ever.



LEO:  You must follow @SwiftOnSecurity; right?  She's great.



STEVE:  Of course.



LEO:  And by the way, it's not a he, it's Taylor Swift, the famous rock star who has a sideline doing security.  Many people don't know that.



STEVE:  Yup.  I just got a kick out of that.  "We were the victim of a very simple attack.  It was through management's lack of focus on security that this happened."



LEO:  You'd have to be nuts to [crosstalk].



STEVE:  Said no company ever.  No.



LEO:  It's true, though, almost every time.  But okay.



STEVE:  Yes, exactly.  Yes, exactly, that's what we keep seeing.  Chris Duncan, who is a listener, made an entry, he tweeted to me, on Twit's wiki site with a curricula of Security Now! episodes in our various series.



LEO:  This is so nice.



STEVE:  It is so cool.



LEO:  Now, it's in the Talk section, unfortunately.



STEVE:  Correct.



LEO:  And I guess that's - maybe he doesn't have editing permissions or something?  I don't know.



STEVE:  Probably.  Maybe we could link to it in the notes for this...



LEO:  Well, I will move it over into Security Now!, is what I'll do.  It's fabulous.



STEVE:  So what he's done is he's broken down - oh, it is, it's just great.  He's broken down the podcast history into topic groups.  He's got VPNs, that is, all of the podcasts where that was the major topic.  Internet and LAN Technologies is another.  Cryptography is another.  Virtual Machines and Sandboxes he's separated out.  Web Code Injection as a topic.  Designing a Computer, which is the one that we had talked about before.  And then I think four on SQRL.  So thank you, Chris.  That's certainly useful, and I wanted to share it with all of our listeners, people who have joined us later thinking, well, okay, I know there's a lot there, but we're on Episode 636, so could you give me a pointer?  Well, now we've got pointers.



LEO:  So go to wiki.twit.tv.  This is a media wiki we've maintained practically since the very beginning.  And right on the front page we break it down into shows.  Of course Security Now! is there.  But Chris listened to our conversation from last week, and I think this is exactly right.  I don't think it's on the front page.  No, you're going to have to go to the Talk section.  But you can easily do that.  If you go to the page and click Discussion, you'll get the discussion and then scroll down.  And I don't know, maybe he proposed it for discussion.  I mean, I don't want to move it over if we can't keep it up to date.  So basically the wikis are community run.



STEVE:  Ah, right.



LEO:  But this is really, really great.  So wiki.twit.tv.



STEVE:  Okay.  And Leo, the best CYA bit of jargon I think maybe I've ever heard is what we now call "differential aging."



LEO:  I know.  Oh, man.



STEVE:  Oh, my lord, isn't that wonderful?



LEO:  That's Google's circumlocution for OLED burn-in.



STEVE:  Yeah.  So we all know that once upon a time, well, many of us know, the old-timers, those whose hair is not quite as dark as it used to be, that phosphor on CRTs would age.  And in the old days, the original reason for a screen saver, the reason it was called a "screen saver," was it was saving your screen from having a static image burned into it.  And what would often happen is, if it was a kiosk or an ATM or just if you generally had things that never moved on your screen, so that the same thing was always being there, the phosphor would age, and over time as it aged it became duller.  So, that is, dimmer.



So then, if you put up something white or something different, you could see a ghost of what was normally there because our eyes are so sensitive to that.  Against a white background you could see windows that you used to have or the scroll ticker along the bottom of the screen or whatever.  So that's what screen savers were saving was they were literally saving your screen because after 10 minutes, rather than letting it just sit there burning, your computer would switch it to something moving around in order to also give you some privacy, but mostly to prevent your screen from burning.



So what now we're seeing, now that we have a new technology which is not an optical shutter, which an LCD is, now we have something which is aging, which it turns out, if you don't take proactive measures, will dim over time, that is, OLEDs will do that.  We have a new screen burn-in problem that's come back.  And technically, yes, it's differential aging, meaning that pixels that are more illuminated age differentially to those that don't.  But anyway, I got the biggest kick out of hearing you guys talk about it, I think it was over the weekend, and then Allen Butler...



LEO:  Crazy.



STEVE:  ...said, "I've seen the screen differential aging on my original Google Pixel, and a friend has it on his original Pixel XL, as well."



LEO:  Yeah.  All these OLED screens have it as a potential problem.  Google's pushing out an update right now that will, they say, fix this in a number of different ways.  So we'll see.



STEVE:  And as I understand it, it just does a one-pixel shift?  It just sort of blurs it a little bit?



LEO:  Well, that's what I was told Samsung does with their phones.  Pixel's going to do other things, including dimming the dock when it's not in use.  So, I mean, that will help.  So most people still don't have their Pixel 2 XLs.  I don't have mine yet.  The first batch went out.  But I think this is...



STEVE:  And what do we know about the X, the iPhone X?



LEO:  Apple says on their help page that it is, you know, all OLEDs are susceptible to burning, so they recommend you set the screen off time to the lowest possible, 30 seconds.



STEVE:  Interesting.



LEO:  They recommend you - screen off time.  What was the other thing they suggested?



STEVE:  Not leave sitting on the home page where it's always going to be the same stuff?



LEO:  Yeah.  What you want to do is really turn off the screen whenever possible.



STEVE:  Yup.



LEO:  But that's for battery life, too.



STEVE:  Well, yeah, for battery life, yes.



LEO:  There was one other thing they suggested.  Oh, yeah, don't turn up the brightness all the way.



STEVE:  Ah, right.



LEO:  For sure, most importantly, make sure you've turned on auto dimming, the ambient dimming.



STEVE:  Yes.



LEO:  The problem with OLED is the brighter it is, the better it looks.  You really - it pops.



STEVE:  It's gorgeous, isn't it.



LEO:  Yeah.  So I, you know, I hesitate to turn it down too much.  We'll see.  Yeah, I don't think it's going to have the problem.  I mean, I've never had burn-in on - noticed, anyway.  See, this is the point.  Having it versus noticing it.  I've never noticed it on a Samsung phone.  I've had OLED screens forever.



STEVE:  No, Leo, Apple invented them.



LEO:  Well, it's funny.  This is a Super AMOLED screen.  This is a Samsung screen.  Same kind of screen, although to Apple's specs, that Samsung uses on its S8 and Note 8.  I don't know.  I don't know.  My suspicion is you won't have a problem.  I have an OLED TV.  I've never gotten any burn-in.  Or at least not, again, not that I noticed.



STEVE:  Yes, yes.



LEO:  And part of the problem with this is people are really looking for it.  Yeah, we'll see.  I'm getting a Pixel 2 XL in about a week, so I'll let you know.



STEVE:  Cool.  So I got a nice note from a Yann Fitzmorris.  Actually, it was forwarded to me by Sue, my sales gal.  He sent it to her under the subject "Another success story:  SpinRite data recovery."  This one is kind of heartwarming.  He said:  "Dear Steve and GRC team, I purchased SpinRite a few years ago and have been using it to keep my drives in good health.  I personally have never had to use it for data recovery."  And actually we'll find out in a minute because apparently he has a dedicated SpinRite computer that he runs his drives on.



He said:  "I personally have never had to use it for data recovery.  However, a friend asked for my help this week because her laptop would no longer boot to the login screen.  Her laptop contained the only copy of pictures and videos of the first two years of her daughter's life.  It wasn't looking good for the patient," he wrote.  "When I plugged the drive into an external dock, no OS would recognize the drive.  I used my dedicated PC for SpinRite" - I guess that's what he means, dedicated PC for SpinRite - "plugged in the drive and ran Level 2.  Success.  We plugged the drive into the dock, and we were able to recover all pictures and videos, over 70GB.  Needless to say, my friend will now seriously consider a backup solution, and I'm hoping she will buy her own copy of SpinRite to show her gratitude for this amazing product.  Thanks again for all your hard work and research.  Love the Security Now! podcast, as well.  I've been a listener since 2015.  Regards, Yann Fitzmorris."  And, Yann, thank you for sharing your success.  Always appreciated.



Okay.  ROCA Pain.  I guess we should have expected this.  And of course I'm put in mind of something that - this is a phrase I hope Bruce invented.  I love it.  We credit him with it always, and he deserves it.  This is Bruce Schneier, of course, where he said:  "Attacks only ever get better.  They never get worse."  And when I was writing this, I was thinking of Gibson's Corollary, which is something I've told people over the years:  If the car you're driving ever starts making a different sound, it's probably not an improvement.  That is, cars don't get better, they generally get worse.  And so is the case for attacks on crypto.



So three weeks ago the knowledge of this ROCA attack went public.  And what we know is that the researchers who forensically examined a whole bunch of public keys that were being generated by this Infineon crypto library discovered to their credit that these were not robust private keys, that it was possible to factor the private key - I'm sorry.  These were not robust public keys; that it was possible to factor the public key down to its primes, which revealed the private key.  Which is exactly what this technology, the RSA technology, was designed to prevent.



So it turns out there are other skilled cryptographers on the planet.  Their original disclosure three weeks ago estimated, as we discussed at the time, that it would cost an attacker who was renting time on a commercial cloud service an average of, for the smaller keys, the 1024-bit keys, $38 and 25 minutes.  Much harder for a state-of-the-art 2048-bit key, about nine days of compute time costing about $20,000.  So at the time we said, okay, that's really not good.  Remember that it's supposed to be way, way, way, way more than the age of the universe squared.  So this is far short.  Nine days?  That's a lot quicker than the expected age of the universe.



So we also know that many organizations who are known to be using these keys vulnerable to ROCA have largely downplayed the severity of the weakness, claiming that it was complicated and not inexpensive.  And also even the guys in Estonia said that large-scale vote fraud is not conceivable due to the considerable cost and computing power necessary for generating the private key, cracking the public key down back into its components again.



But turns out, as I said, there are other gifted cryptographers, and particularly Dan Bernstein and the woman he often works with, Tanja Lange.  They came up with a much more potent attack.  Dan, remember, is the originator of the Curve25519, so definitely someone who knows his crypto.  They reported that they developed an attack that was 25% more efficient than the one created by the original ROCA researchers.  Okay.  And their new attack was solely the result of Bernstein and Lange reading the original research paper, which at the time omitted specifics of the factorization attack in an attempt to increase the time that hackers would need to carry out real-world attacks.  So they said, yes, we figured how to do this, but we're not going to tell you how.



Well, they didn't have to tell Bernstein and Lange how to do this.  These guys figured it out themselves.  So after creating their more efficient attack, Bernstein and Lange submitted it to the original researcher, saying, "Guys, we've got a better way to do this."  And then upon receiving that, the original researchers since privately disclosed their own revised attack that's as much as four times more efficient than what they originally published in their paper.



So that suddenly drops the 2048-bit key from nine days to, what, two and a half, and from $20,000 to $5K, making, again, I mean, it was already within reach for anybody who cared to crack one.  Now it's four times easier.  And what this suggests is maybe we still haven't seen the result because the more you look at some of these problems, the easier it becomes to overcome them.  So as a consequence of all this, on Friday Estonia's Police and Border Guard suspended the entire set.



LEO:  Oh, that's too bad.



STEVE:  760,000 ID cards.



LEO:  Can they fix it?



STEVE:  Well, no.  They're now...



LEO:  Because it's in a chip; right?  It's kind of built-in.



STEVE:  Yes, exactly.  It is in the hardware embedded in the chip.  So they are now issuing cards which use elliptic curve cryptography instead of the vulnerable RSA keys. 



LEO:  Well, I'm glad I didn't get my Estonian ID card.



STEVE:  And remember that we've talked about how forward-looking Estonia is.



LEO:  Oh, I was going to get one.  I was in Estonia last year, and I meant to get one.  I just didn't have time.



STEVE:  Yeah.  They use it for voting, for border crossing, as their ID card.  All kinds of things are tied into this.  And they've had to say, whoops, we are just going to remove them all from service, three quarters, more than three quarters of a million cards.  But to their credit, they're going to reissue them using elliptic curve crypto, which isn't vulnerable to any sort of a factorization attack because it doesn't use prime factors as the hard problem that needs to be solved in order to protect the key.  So, yup, we could have anticipated that something like this would happen because, as Bruce said, "Attacks only ever get better; they never get worse."  And this podcast also only ever gets better. 



LEO:  That's a nice coda for the show.  "It only ever gets better."  Well, I am very happy to have, even though we started late, to have gotten everything in.  It's not even 4:07 yet.  We could have gone for another few hours.  You sure you want to stop?



STEVE:  We're good.



LEO:  All right.  We are good, very good.  Steve Gibson is at GRC.com.  If you go there, you'll find this podcast, of course, audio plus transcripts, very nice transcripts that he pays for.  Thank you, Steve.  But you'll also find many, many other wonderful things.  Of course SpinRite, the world's best hard drive maintenance and recovery utility.  Got to have that if you have a hard drive.  You've got to have SpinRite.  Even an SSD you've got to have SpinRite.  



STEVE:  Yup, it works there, too.



LEO:  But SQRL's there, his perfect, what is it, better sleep formula?  All the stuff that Steve does.



STEVE:  Yup, the Healthy Sleep Formula.



LEO:  Healthy Sleep Formula, and Perfect Paper Passwords, Shoot the Messenger, DCOMbobulator, ShieldsUP! and on and on and on.  Steve just has loaded that site with goodness.  There's a feedback form there if you want to leave a message for Steve, GRC.com/feedback.  But probably the best way now is using his Twitter account because he accepts DMs from strangers, bold man that he is.



STEVE:  Well, and there's a lot of nice dialogue that goes on in public, too.



LEO:  No, I think it's great.  @SGgrc.  @SG, that's his initials; and GRC is the name of the company, GRC.com.  @SGgrc.  Tweet him there.  Ask him questions.  DM him if you've got a tip.  And those questions make it in, as you can see, into the show at some point.  We also have, of course, copies of the show at our site, TWiT.tv/sn for Security Now!.  And you can always subscribe to your favorite podcatch client, and that will get every episode, and that way you can start building your collection.  Collect all 636.



STEVE:  If you dare.



LEO:  Each is a unique snowflake.  If you want to watch us do it live, you're more than welcome to.  There's a couple ways you can do that.  You can join us in the studio, and we had some nice visitors in the studio today, and we appreciate that.  Just email tickets@twit.tv so we can put a chair out for you and so that our security guard doesn't shoot you on sight.  Always good, you know, that's always a good thing.  No, he won't do that.  He's very nice.  But do email us, tickets@twit.tv.  The studio is not always open and accessible.  You can always watch live on the stream.  That's always safe.  TWiT.tv/live.  We're a little jumpy around here after all the gunplay that's been breaking out all over the country.  We try to keep this place safe.  I don't want anything to happen to my employees.



STEVE:  Well, and I remember that the TWiT Studios in South San, I mean, security is always present when I've seen TV studios.



LEO:  Yeah.  Yeah, TV studios have to do it.  I thought we didn't have to as a podcast, and then somebody swatted us, and I thought, yeah, I'm going to have - I don't want to tell you the story on the air, but it was nasty.  So ever since, it's been more than a couple years now we've had - Moe is a great guy.  Marine, former Marine.  Very good with weapons, small arms.



STEVE:  Don't mess with Moe.



LEO:  Don't mess with Moe, that's all I'm saying.  I don't want to scare people.  We do more than welcome you; right?  But, you know, I just want you to know, I do want to scare bad guys.  Let's put it that way.  Those people I want to scare.



STEVE:  If you're a bad guy, don't bother.



LEO:  It's a shame, isn't it, we have to do that.  But we do.  I just - it's not that I feel fear for myself.  I just don't want my employees to feel unsafe.  So you can watch without coming to the studio.  The stream is TWiT.tv/live.  If you do that, join us in the chatroom, irc.twit.tv.  To date, no shots have been fired there.  You can also - what else?  I guess that's it.  I told you how to download it.  There's nothing else you can also do.  Are we doing - have you thought about our holiday show, what we're going to do for that?  We can do a best-of.  Should we?  You don't want to work the week after Christmas.



STEVE:  I don't.  But I do have some videos of our earlier...



LEO:  In the past you've done some fun stuff, so that's why I wanted to give you the option.



STEVE:  Yeah.  I'll try and find something fun.



LEO:  We're putting together best-ofs of the other shows at TWiT.tv/bestof.  But this show, I don't think we've ever done a best-of.



STEVE:  We haven't, no.  There's always been - yeah.



LEO:  You shared a lecture last year that you gave, a speech?



STEVE:  Yup.  That was...



LEO:  And of course there's the Portable Dog Killer.



STEVE:  Yup, we've done that several times.  I have some early TWiTs from South San Francisco, and that might be fun, just sort of as a blast from the past.



LEO:  Really.



STEVE:  Oh, yeah.



LEO:  Ah.



STEVE:  That was my deal was I would always get tapes from - literally VCR, you know, VHS tapes.



LEO:  From The Screen Savers.



STEVE:  From The Screen Savers.



LEO:  I think we could show those without getting in trouble with NBC Comcast Universal.  Why not?  It's always better to ask permission later; right?



STEVE:  Oh, and apologize.  We're sorry.



LEO:  It's better to apologize than ask permission.  That's what it is.



STEVE:  That's right. 



LEO:  Steve, always a pleasure.  Have a great week.  We'll see you next time on Security Now!.



STEVE:  Okay, my friend.  Thanks.  Bye.  



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#637

DATE:		November 14, 2017

TITLE:		Schneier on Equifax

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-637.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss why Steve won't be relying upon Face ID for security, a clever new hack of longstanding NTFS and Windows behavior, the Vault 8 WikiLeaks news, the predictable resurgence of the consumer device encryption battle, a new and clever data exfiltration technique, new antimalware features coming to Chrome, an unbelievable discovery about access to the IME in Skylake and subsequent Intel chipsets, a look at who's doing the unauthorized crypto mining, WebAssembly is ready for primetime, a bit of miscellany, some closing-the-loop feedback with our listeners - and then we share Bruce Schneier's congressional testimony about the Equifax breach.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about, including a handy-dandy way to hack any Intel computer using the USB port and a look at Bruce Schneier's testimony in front of Congress about the Equifax breach.  It's all coming up next, plus a great Image of the Week, on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 637, recorded Tuesday, November 14th, 2017:  Schneier on Equifax.



It's time for Security Now!, the show where we secure you.  We fasten your seatbelts, we put on your belt and your suspenders, and the tinfoil hat is optional.  This is the guy, Steve Gibson of GRC.com, who helps us each and every week.  Everybody loves Steve, loves this show.  It's true, it's true.  Fastest growing show on the network, partly because of security.  But I think, you know, there was an article, Steve, I read about a basketball podcast called "Dunc'd On."  And it's just two guys talking about basketball.  And Bloomberg was baffled by its success and the fact that it has 50,000 downloads a week, and it's two hours long, and they talk obsessively about the details of stuff that nobody cares about in basketball.  And I thought, well, god, if you'd listened to this show for the last 13 years, you'd know.  That's what people want in a podcast.



STEVE GIBSON:  Yes, yes.



LEO:  Hello, Steve.



STEVE:  And interestingly, the topic of security never leaves us wanting.



LEO:  No.



STEVE:  And we've got a fabulous Picture of the Week.  The main topic for today's podcast, and the title, is Schneier on Equifax.  Bruce testified in front of Congress last week.



LEO:  Oh, good.



STEVE:  And what he had to say was really interesting.  He presented 11 points, and I'll go over about the first half of them at the end of the podcast, then kind of summarize the rest.  But he made some really, I think, valid points.  And I'm so glad that he had the opportunity to explain what he feels is going on.  And there's a lot of stuff we already know.  But Bruce is in the middle of this and spends a lot of time thinking about this.  And so he made, I think, some very salient and clear points.  So we're going to get to that at the end.



But first we're going to talk about why I won't be relying upon Face ID in the new iPhone X for its security.  A clever new hack  of longstanding NTFS and Windows behavior, which many AV companies are now scrambling to fix, and I think six or eight have.  WikiLeaks has moved from Vault 7 to Vault 8 with an interesting disclosure of CIA source code which reveals the structure of, if we are to believe this, the CIA's command-and-control system for their publicly implanted, I think "spyware" is the right thing to call it.  Everyone calls it "malware."  But if it's the CIA, is it malware?  It's spyware, probably.



Anyway, so that's really - and we actually see the architecture of that, the "Hive," as it's called.  We also have the predictable resurgence of the consumer device encryption battle after Sunday before last's Texas church shootings, and some interesting reaction from Apple and the FBI, and some commentary that's worth sharing.  A new and clever - you're not going to believe this one - data exfiltration technique.



New antimalware, three new antimalware features coming to Chrome in the next couple months.  An unbelievable discovery about a way to access the much-maligned, and deservedly so in some cases, Intel Management Engine in Skylake and all subsequent Intel chipsets.  A look at who's doing the unauthorized crypto mining across the web.  A security researcher looked carefully at the crypto mining that was going on, which by the way has exploded.  It's four shy of 2,500 sites are currently doing it.  But it's not who you would think.



Also, a technology we discussed 2.5 years ago, WebAssembly, which is a next-generation replacement for JavaScript, is ready for primetime as a consequence of the final addition of the last two browsers, which was Safari, which iOS 11 brought to us, and the Fall Creators update of Edge in Windows 10 now offers it.  And we'll talk about that.  And what's interesting is that you couldn't do cryptocurrency without it because browsers wouldn't be fast enough.  But thanks to WebAssembly, they are.  But we all have gained benefits of other sorts.  We'll talk about that.



A little bit of miscellany, some closing-the-loop feedback with our listeners, and I want to talk about Schneier's congressional testimony about the Equifax breach.  Oh, and a really fun Picture of the Week that a number of our listeners sent me, thinking that we would get a kick out of it, and indeed.  Our Picture of the Week will mean a lot more to old-timers because I don't know how many hours I have spent staring at the screen of my computer while it defragged my hard drive.  There's something about it.  It's just mesmerizing.  It's like watching your car go through the car wash tunnel, where you just, if the car wash tunnel has windows, there's something fascinating about watching it just sort of go through without you and coming out clean the other end.



Well, defragging, of course, is the process of pulling all of the various bits of a file together in one place, which back in the day when hard drives were a bottleneck and when our file systems sort of cared more than they do today, or before we had SSDs where physically putting the pieces of a file contiguously on the physical hard drive demonstrably, maybe, increased its performance.  If nothing else, it did decrease the wear and tear on the drive because the head didn't have to jump all over the place grabbing all the little pieces of a file, and files tended to fragment.



Anyway, the point is, those among us who remember the days of defragmenting our drives, you could still get defragmenters, but they're way less used these days than they used to be.  Anyway, the picture is just very clever.  The caption is "I defragged my zebra."  Yes.  And it shows the result of what you would get if you defragged a zebra.  Normally they're striped and zigzag-y and all kind of complex, I guess for a form of camouflage out in the wild.  Not after the zebra has been defragged, in this case.  The front is black and the back is white because all the bits have been aligned with each other.  So anyway, a bunch of our listeners found that and thought we'd get a kick out of it.  And indeed, I wanted to share it for that reason.  Very clever.



So, Leo, predictably, as the world has had access to their iPhone X with its Face ID facial recognition, we're beginning to have the claims that Apple initially disclosed tested, which is that it is, what, whereas there was a one in 50,000 chance, they said, of somebody, of a stranger unlocking your Touch ID fingerprint recognition, they were claiming a million to one.  Now, maybe if you took, I mean, first of all, I don't know how you make that claim without testing it, and it would have been difficult for them to have tested it.  So they may believe that's the case.



But what we're immediately seeing are coincidental, close-enough faces, for example, twins or lookalike family members, in some cases the son of a mother who looks enough like his mother to unlock her phone.  And then there are the deliberate security researchers messing around with the technology, essentially since Apple hasn't told us in detail.  They've given us lots of gee whiz-y stuff about each phone having its own unique sequence in which it throws out the IR array to scan the images and so no fixed system can be the same.  It's like, wow, wow, that sounds like a lot of high tech there.  But with some work, this technology has been spoofed.



And, I mean, I liked a couple of things that you were saying, well, that the group on MacBreak Weekly was saying beforehand.  And a point that I had intended to make, Andy touched on, which is there is a difference between faces and fingerprints, which is that faces are generally far more public than fingerprints.  And, for example, we know that there's a technology available to take multiple angles of a person's face and reconstruct a 3D model.  So we also know that attacks never get weaker, they only get better.  And we credit Bruce Schneier, who's the topic of this week's podcast, his congressional testimony, to saying that.  And it's something we often repeat here because we see it happening in the world.



So what I believe is that Apple has made a huge concession, and an unfortunate concession, a concession giving up security, true security, in favor of convenience.  So I'm not judging that.  I mean, I recognize, I've listened to you talk, ever since you started to use your iPhone X, about just how incredibly convenient it is.  And I get that.  I could not possibly use it because, for me, I have LastPass, which authenticates with an authentication app, and my authenticator app there.  And essentially, somebody who got into my phone, because it's the portal to things that are really security sensitive, they could just take down my life.  I'm statically logged in in Safari sessions.  The authenticator is there, access to which would defeat its purpose, and so forth.



So I was encouraged - I don't have mine yet.  I was encouraged when Rene said there is granularity that can be applied to what things it can be used for.  And this immediately made me think, okay, well, I need an authenticator app which is independently lockable, and LastPass needs to be independently lockable because of course it contains the keys to the kingdom.  I don't want to not have those things on my phone because that's the whole point.  So what this makes me feel like is that maybe what we'll see is an evolution.  As more stories come out, and they're going to, we're going to see the security of Face ID crumble.  That's clearly going to happen.  It's been about 10 days now, and it's already starting to happen.



Again, for convenience and for people who are like, oh, yeah, I don't really have anything on my phone that I care about, I get it that it offers that tradeoff.  I would be reluctant to use it because, first of all, it's a brand new, unproven technology.  It'd be crazy to immediately jump in only on Apple's assurances, which already seem to have been overblown.  This to me seems substantially weaker than Touch ID because none of those instances of unlocking, opportunistic unlocking which we've seen would have occurred with Touch ID.  There may be some fingerprint collision, but it just wouldn't be as obvious as this.  So my feeling is...



LEO:  But you're basing this not on the most recent attack, because that's unlikely.  You're basing it on a presumption it's going to get easier.



STEVE:  No.  If somebody had - there are pictures of me all over the 'Net.  And so you could take those photos and reconstruct...



LEO:  No, no, they didn't use photos, remember.  They did a face scan.  They needed a 3D scan, which they did, took five minutes with a scanning device.  You can't use a 2D - you can't do this.  This is an example.  It's scary, but it wouldn't work.



STEVE:  But you didn't let me finish.



LEO:  Okay.



STEVE:  Multiple 2D pictures can be used to create a 3D model.  That's what I'm saying.



LEO:  Okay.



STEVE:  If you have a picture of me from this angle and one from this angle, and there's all kinds of pictures, from that you can create a 3D model.  That we know about.  I mean, that's done all the time.  So photos are inherently - they provide a huge amount of information.  And I'll bet you that what we will see is somebody taking a bunch of 2D photos to reconstruct a 3D model and then using it to spoof this.



LEO:  You consider the fingerprint better?  Because you've been using that.



STEVE:  Yeah, I have been.



LEO:  Much easier to steal your fingerprint.



STEVE:  Okay.



LEO:  Isn't it?  You disagree?



STEVE:  My fingerprints are not all over the Internet, not all over the public Internet.  So somebody could present me with a 3D model of my head, having never encountered me before.



LEO:  Right.



STEVE:  Because of...



LEO:  [Crosstalk] your phone, so it's not like they could do it remotely over the Internet.



STEVE:  Correct.



LEO:  They're going to come to you at some point.  So they could just follow you into Starbucks, get your fingerprint from a Starbucks mug, and then do it.  They're going to be proximate to you eventually.



STEVE:  Yes, true.  Yeah, and so I think my greater point is that my phone is, I mean, look at all of...



LEO:  Well, I understand that, I mean, yes.



STEVE:  Look at all of the attention Apple has given to the security of their device.



LEO:  Sure.  Get my phone, you've got me.  I agree with you on that.



STEVE:  Yes.  They have thrown that out the window in the name of convenience, which is unfortunate.  I may, I mean, if it were possible to use Face ID to sort of get some surface-level access to the phone, and then need to authenticate with a substantial password, then maybe that would make sense.  I may just not use it.  It may not be a feature that I use.  I could turn it off and not use it until we know more.  To me, Apple's claims are not standing up.  And it would be crazy to rely upon an untested, brand new feature like this until we know more.



LEO:  That makes sense.  I think that's reasonable, yeah. 



STEVE:  Yeah.  So it'll be interesting.  And I can't wait to get mine because it does, I mean, in terms of processing power and the better screen and just, you know, I'm several generations behind.  I'm back on the 6s.  I didn't go to the Plus, nor to the 7 or the 8.  So I've been holding back because you know me, I'm still using Windows XP.



LEO:  Is part of this because you presume yourself to be a target?  Or would any average Joe on the street have this concern?



STEVE:  Our listeners would have this concern if they have LastPass and Authenticator.  I mean, the problem is, at this point, if someone gets into my phone, they could take down my life.  They could take GRC off the 'Net.  I mean, they could, if they had access to my authenticator and my email, because email comes into the phone, email of course is everyone's last resort account recovery.  So they could point GRC.com to some other IP, to other name servers, and I would have a hard time getting it back.



LEO:  I would submit the real risk is that our phones now contain so much stuff.  And I would also say that I don't think Touch ID is necessarily any better than Face ID.  It's more mature, but I don't know if it's any better than Face ID.  In fact, I think it probably isn't as good as Face ID.  It's a lot easier to make a fake fingerprint than it is a fake face.  So, I mean, if you've been trusting Touch ID, you know, somebody has your phone, they probably have your fingerprint.



STEVE:  Yeah.



LEO:  And we know you can easily spoof Touch ID with a fake fingerprint.  So, I mean, I would agree with you on two points.  One is that your phone contains your life, and that needs to be secured better.  But the other, that this is a new technology, and we don't know how bad it is yet.  We knew that as soon - I mean, I knew anyway, as soon as the iPhone X came out, that we'd start to see these articles, as happened with Touch ID.



STEVE:  Yeah.  Yeah.  I guess it's the ease of it happening in instances with people's children or people's lookalike...



LEO:  No one looks like you, Steve.  You're safe.



STEVE:  Thank goodness for that.



LEO:  Yeah.



STEVE:  Okay.  So a security researcher came up with a clever use for NTFS file system links.  NTFS, very much like Unix, has long had this concept of a symbolic link.  There's something called "directory junctions."  There's hard links, there's soft links, and so forth.



What this researcher recognized was that, when AV systems detect malware and quarantine the malware in another directory, and when the user has the operating system privileges required to say, "Oh, no, that's not malware, that's a false positive, bring this out of seclusion," just this combination of features, along with something we've talked about a long time ago which is the DLL search path that exists in Windows, by cleverly combining these things, it turns out that this researcher was able to induce most of the industry's AV manufacturers to immediately revise their products.  Trend Micro, Emsisoft, Kaspersky Lab, Malwarebytes, IKARUS, and ZoneAlarm have all been contacted and have released fixes.  Some others who have also been contacted have not yet released fixes.



So here's the ID is when an AV product quarantines something that it considers malicious, it moves it into a quarantine directory for safety, to get it out of the normal operating system location, the download directory of wherever the user may have had access to it in order to sequester it.  However, by the use of this NTFS file system feature known as "directory junctions," it is possible to place the quarantined directory into the Windows system DLL search path.  The search path is a series of directories through which Windows will look, will automatically look in order to find a DLL which an application is requesting.  By default, the system looks in the same directory as the application itself.



And I should mention, I'm sure most of our listeners know that a DLL stands for Dynamic Link Library.  It was intended in the original design of Windows to be a way for applications to share libraries of functions without having to include them all in themselves.  It turns out to have really pretty much collapsed because of conflicts among DLL versions.  Microsoft has almost completely given up on the idea.



So now there's this new WinSXS, Windows Side-by-Side, technology which attempts to basically give up on this notion of sharing code completely because it ended up causing more trouble than it was worth.  No longer do we really need to worry about hard drive space and economizing on the duplicate presence of common libraries and so forth.  So once upon a time it worked.  Now, not so much.  However, all of the technology for backwards compatibility's sake still is there.



So there have been in the past exploits which relied upon Windows search order to get a malicious DLL upstream in the search sequence of where the valid one was actually residing in the file system so that Windows would encounter a same-named DLL and instantiate it, execute it, essentially.  And in a DLL there is essentially an on-load function which is always called when the DLL is loaded into memory.  So if you combine an AV program's decision to sequester something with the ability to essentially map the sequestered directory into the DLL search path, and if the bad guy then names the sequestered file something that any other program may ask to have loaded into its own process space, all of those things together actually do create a reliably and robustly exploitable hole which a huge number of AV is currently exposed to.



It's being fixed immediately.  It's known as AVGator, is what this guy named it, #AVGator.  And it's significant that it can only be exploited if the user's account is allowed to restore previously quarantined files.  So one of the things that is probably a mitigation in enterprise environments is that normal users would be blocked from restoring threats that their AV had decided to quarantine.  Hopefully that's the default policy, as it should be, in which case this would not be a problem, assuming that that's the case, that regular users cannot deal with this themselves and pull things out of quarantine.



The only real resolution for this, because this essentially isn't something easily turned off, is to make sure that you keep your AV up to date because we don't have full disclosure yet from those companies which have not patched, but any that are exposed have probably been notified and are in the process of fixing this very clever discovery about how to take advantage of a set of longstanding features in Windows.



We've talked a lot about Vault 7, which is the continuous dump of CIA material, programs, tools and so forth, from WikiLeaks.  They've now started what they're calling "Vault 8," which is the release of the source code for a system known as Hive, which appears to be the CIA, the U.S. Central Intelligence Agency's malware or spyware control system.  So they're leaking this under the name of Vault 8, and we're learning a lot about how this works.  Okay.  So this Hive, as it's known, presents itself as a network of publicly available web servers.  So, for example, you would have a VPS, a Virtual Private Server, hosted on some random hosting site.  The domain would be registered and, from all external appearances, looks just like a website.



If you go to that domain with your browser, you see some website, which is a front, essentially, as a control point for any of a number of implants which exist out in the world on machines that have access to the public Internet.  There is a technology known as "optional authentication."  Back in the old days, and I'm sure you remember this, Leo, you could go to a website that would suddenly hit you with a pop-up that was requesting your username and password.  That's always been built into HTTP as mandatory authentication.  That is, going to this page, you would be forced to provide a username and password in order to proceed.



Well, there is optional authentication where you're not stopped, I mean, you're not prompted at all, you're not stopped, but you can provide authentication as part of the query headers to the site.  In which case the server will see that you are authenticated and be able to treat you differently; and, if you're not, you'll still have access.  So this is a clever means, essentially, of hiding access in plain sight.  And there are any number of these servers can be out in the world, just looking like a normal, legitimate website.  People go there.  You see text and pictures.  They could be blogging sites.  I mean, they could be anything.  But only if an implant knows how to add its own authentication to its queries will it then go to a different place.



So the front is these VPSes, these Virtual Private Servers.  They then establish a VPN connection to one or more proxy VPNs back somewhere else on the Internet which then, depending upon whether the query has this optional authentication or not, if it doesn't, routes that request to a so-called "cover server" to just provide cover to make it look like, oh, this is just a regular website.  Otherwise, it goes to something known as the "honeycomb server," which then provides essentially the command and control to these remote, publicly located implants.



So certainly we've talked often about command-and-control systems for botnets and what this next generation of leaks, this Vault 8, which is leaking the source code for this technology, and which security researchers are now crawling all over, reveals the structure that, if we are to believe this, that our U.S. Central Intelligence Agency has designed in order to allow things that are out in the wild to essentially phone home in a way that isn't obvious.



The other characteristic of this is anyone looking at the traffic would just think, oh, look, you know, this service is for some reason accessing a website.  And if you saw it doing so, and you typed that domain name into your own browser because you were curious, you would get a regular-looking domain.  So it's clever.  Not high tech, uses longstanding technologies, and there's no reason to believe this isn't what they're doing.  It certainly would make sense for them to be doing exactly that.



LEO:  Given the evidence, not conclusive, but the evidence that WikiLeaks is a mouthpiece for the Russian government, I wonder how credible it is in leaks like this, of this sort?  I mean, obviously somebody made this.  This isn't something somebody at WikiLeaks, you know, Julian Assange didn't write this in his spare time.



STEVE:  Right, yeah, right.  There's a serious body of source code here.



LEO:  But who knows what its real provenance is; right?



STEVE:  Yeah.



LEO:  I mean, can you definitely say this is from the CIA?



STEVE:  No.



LEO:  Not the FSB, for instance?



STEVE:  I guess the only way, I mean, if someone were really interested, you'd have to find one of these.  You'd have to look at the credentials and then track down the VPN and, like, do some forensics.  But, yeah, I mean, as we know, attribution is always the problem with these things.  So unless the CIA said, "Yeah, you got us," you know...



LEO:  Unlikely.



STEVE:  Yeah, exactly.



LEO:  How about Vault 7?  I mean, I think that's accepted now that that was fully the NSA's stuff; right?



STEVE:  Yeah.



LEO:  Yeah.  So that doesn't mean, I guess, that the information WikiLeaks has is false.  You might question their motivations, I guess.



STEVE:  Yeah.  And if nothing else, it's interesting, and it certainly seems feasible.  But again, you're right, Leo, we don't know one way or the other.  I mean, this is leakage that wasn't intended to be leaked.  So, I mean, well, leakage that could have any source.



LEO:  Right.



STEVE:  So we had a mass shooting Sunday before last at a church in Texas.  And naturally, immediately afterwards, much as happened with San Bernardino, the FBI wants to understand everything they can about the background of this person who we pretty much know did the shooting as a consequence of the evidence trail.  And what reporting, and this is from Reuters reporting, Apple claims that it immediately contacted the FBI to offer their assistance in unlocking this Texas shooter's iPhone.  The FBI is arguing that this is yet another case of them not having critically required access to encrypted data.



The problem in this case is that there was an immediate clock that was ticking.  We know who the suspected shooter was.  And if the shooter had been using their fingerprint to unlock their phone, and if even post mortem this fingerprint had been applied to the phone, presumably a thumb, within 48 hours - after which we know that Touch ID stops working, and you have to be using your passcode - then there was this window of opportunity.  And so this has caused some controversy because the FBI did not take advantage, the reporting alleges, of Apple's immediate offer to provide any assistance that they could.  We don't know what the FBI did with the phone, whether they in fact got access to it or didn't.



Tim Cushing, writing for Techdirt, looking at all of the coverage, wrote that:  "If everything alleged by the Reuters report is true, the FBI's struggles are of its own making."  But then again, there is still a lot of information that is unknown.



LEO:  Now, I actually thought this was weird.  Apparently Apple said, "Well, you know, you should have called us because you could have just put the dead guy's finger on the phone and unlocked it."  What?



STEVE:  Right.



LEO:  Do you think Apple really said that to them?  They said that the - remember the San Bernardino thing, they said:  "You should have called us because we could have, you know, there was a chance he was synced with iCloud, and we could have given it to you then."



STEVE:  Exactly, yes.



LEO:  That I believe.



STEVE:  Yes, yes.



LEO:  But can you put the dead person's finger on the phone and unlock it?



STEVE:  Heck, yes.



LEO:  But doesn't it have some sort of infrared to detect heat?  I thought it did.  Maybe I'm...



STEVE:  There's not IR because it's capacitive.



LEO:  I thought that you had to use like a sausage - oh, capacitive would still work with a dead finger, I guess.



STEVE:  Yes.



LEO:  But you've got to do it right away because 48 hours afterwards [crosstalk].



STEVE:  The gummi bear fingers also work.



LEO:  That works, too, okay, all right.



STEVE:  Yeah.  So it has to have ridges in order to function capacitively.  And of course then Dianne Feinstein immediately dusted off her legislation which had died for lack of support near the end of the previous U.S. administration, when nobody was jumping to move on this.  So we're still in the middle of this struggle.  And so here's yet another event where the FBI is using this instance to complain about their inability, in a very high-profile event, their inability to get access to a criminal's encrypted data; Apple trying to defuse this saying, "Hey, we offered.  We stepped up immediately."  They probably did, from a PR standpoint, just saying look, we'll provide any help that you need of any kind because, as you said, Leo, arguably there were some mistakes made immediately in the San Bernardino case which may have, if things had been handled differently, may have allowed the data to be recovered.



And I'll just remind everyone that, as we've said on this podcast, there is a solution that Apple doesn't want to be forced to provide, which isn't a back door or a front door or a golden key or a side door or an unlocked door, the idea being that Apple is maintaining a connection to every one of their iDevices.  I mean, that's part of what they're doing.  These things are connected, and they're getting updates, and they're doing iClouds, and all of this is going on.



Ultimately, in every single one of these devices is a master 256-bit symmetric key which is the thing which unlocks the stored data.  And we know that it is super well guarded by multiple layers of security, by hardware, by enclave processors and all this, and that Apple doesn't have it currently.  We believe them that the device, when it initializes the first time, it uses available entropy to invent this key for its own use, and it never leaves the device.  That one thing is all that would need to change.  That is, when the device invents this key, comes up with its super-secret, super-guarded, 256-bit symmetric key, that key could be encrypted with a very long, very safe, master transport key, that is, that's in the device.



This would be a public key using public key technology, which could be used to encrypt this 256-bit per-device symmetric key for transport to Apple, so that Apple then has this unique per-device key in their database.  And if called upon on an individual per-device basis, Apple could decrypt that using the highly guarded at Apple matching private key, which would allow per-device access to the contents of the phone.  They don't want to do it.  I get that.  I respect that.



But there are solutions, is my point, to providing under law single-device access which Apple could at their discretion with this technology choose to make available if legislation is enacted to cause that to happen, without anybody getting a master, any third party, any law enforcement getting any sort of carte blanche access to all of the iPhones in the world.



LEO:  The problem with that, of course, is that they're afraid, not so much about the U.S. government, but other governments asking for that kind of access.



STEVE:  Ah, right.



LEO:  Which they would have to provide.



STEVE:  If it exists, then, right.



LEO:  So acknowledging that that exists would be a problem.



STEVE:  Yeah.  So yet another way - we keep coming up with and discussing bizarre ways of exfiltrating data.  We've had hard drive noise.  We've had ultrasonic sound coming from the speaker.  We've got flashing lights on our routers.  Some of them have been debunked, some not.  I mean, even the classic RF emanations from CRTs, and then LCDs, and reading contents of screens through the wall just by picking up leakage of information.  Side channel attacks, figuring out what key is being decrypted on the processor when you share it with a VM, I mean, there's just all kinds of stuff.



And there's a new one in town.  Imagine some malware in a mobile device which, during a video conference, puts what looks like just sort of a band of static on the bottom of your screen.  And you kind of look at that, and it's like, what?  This is weird.  It looks like in the old days when you didn't have your vertical alignment adjusted on your TV, and you were seeing noise off to the side.



And so the point is, it turns out that, although a little bit frustrated by video compression, it certainly is possible - and it has been done now, there's been a proof of concept of this -  where some malware added some noise, some static to the bottom of a video transmission which, at the recipient end, allowed that what looks just like static is actually fast-moving data in order to exfiltrate data across a video link, where file transfer had been explicitly denied.



And so anyway, just yet one more means of exfiltrating data from a system.  Pretty much, I mean, you couldn't use traditional steganometry because compression would completely destroy the 24-bit color.  And as we know, it makes the images blurry.  It reduces the color depth.  So it is a challenge to do this.  But it certainly is possible from a technology standpoint to pull it off.  And a group at PenTestPartners.com, they made it happen and demonstrated more than proof of concept.  You actually could do it just by putting something that most users would think, oh, that's weird, some static on the screen.  I don't know what that's about, but who cares?



So Google is moving forward with Chrome security and will in the next couple months be adding three new features to Chrome to fight malvertising.



LEO:  Oh, boy.



STEVE:  And these are good.  In the first case I'm glad that Google is willing to do this because it's deliberately breaking some proper behavior, but necessarily.  So, okay.  So with release 64 of Chrome, which is slated for January of next year, January 2018, Chrome will be blocking script-driven iframe redirects.  This is a consequence of the history of abuse by embedded malvertising.  Chrome will no longer accept URL redirections triggered by JavaScript code residing within iframes.



So, I mean, iframes have been sort of a controversial problem on the web for a long time.  They are a means for one web page to embed another web page within itself, literally on the real estate of the hosting web page.  In the HTML you declare a frame, a horizontal by vertical frame of a certain size or percentage of the page.  And in this declaration you give it the URL of the web page to fill the frame with.  And it doesn't have to be the same domain.



So this is inherently a cross-domain thing.  This is a means of embedding any other web page.  And in fact it's been used controversially by some less reputable sites to essentially embed good sites, like steal site content from some other web server.  It's like, oh, look at our stuff, when in fact all they have is an iframe pointing somewhere else, but people get to that somewhere else by going to the primary site.  So it's a convenience.  It was the sort of thing that someone said, "Hey, wouldn't this be cool," back in the beginning, the dawn of the web.  And everyone said, "Yeah, okay, fine, we'll put that in."



So the point is that this day and age it's typically the mechanism used for advertising.  So you have a web page, and you want to host third-party ads on your page.  So you give them an agreed-upon rectangular area on your page, and you point that to the web server's URL.  The web server, when queried, they're going to see from the referrer header where the query is coming from.  Oh, it's from these guys who are hosting our ad.  And of course they also get cookies that belong to their domain, so they know who you are, looking at the site that is hosting the third-party service.  So they get all this information; and they return, hopefully, an ad which you care about and which the site receives some remuneration for.



The problem is this very powerful facility can also run, I mean, it's a full web page, which means it can run JavaScript.  It can run code.  And this has been used by malware that is injected into legitimate advertising services in order to get users to click on something, and they click on something, and that installs something into their computer that they don't want, and it's all bad.  So the point is this is all standards-based.  And so I take my hat off to Chrome for deciding we're going to break something which is being abused because, even though it's legitimate, that is, it's not something that needs to be fixed, it's always been possible.  They're going to say, eh, we're going to stop making that possible.  We're willing to take the hit for script running in an iframe that attempts to redirect to somewhere else.  We're going to have Chrome look at that behavior and not abide it, not follow that redirect.



So I'm glad that Google is willing to do this.  They sort of have to be the first people to be the icebreakers, after which other browsers could follow, presuming that this doesn't cause too big a problem.  And they must have analyzed this in order to decide that, yup, the benefit for the user outweighs the tiny loss of functionality that this could incur.  I mean, and especially since Google is largely advertising supported, and this is something that maybe a legitimate advertiser could use.  They're just going to have to stop doing that because, after January and release 64, Chrome won't follow script-driven redirections from an iframe.



Then two months later, in March of '18, Chrome will begin blocking what's known as "tab under" behavior.  "Tab under" is the term for a web page, sort of a clever scheme by which a web page can bypass Chrome's built-in pop-up blocker.  What the page does is opening links in new tabs and then redirecting the original tab to a new URL.  So starting with Chrome 65, obviously the one after 64, which we were just talking about, with the next iteration of Chrome targeted for, I think it was early March 2018, that redirection attempt for the original tab will be blocked, and Chrome instead will display a notice at the bottom of the page, just letting people know that some JavaScript's attempt to redirect this page to a different URL has been blocked.



And finally, there are pages which abuse the user's expectation of what will happen when they click on something.  This is important because there is a huge difference in the handling of things that JavaScript can automate without user interaction.  That's why malvertising wants you to get to click on something.  It's why clicking on an ad can be dangerous, because browsers handle passive and active differently.  Well, it turns out there are suspicious and malicious sites which will, for example, put up a video that the user doesn't want; but the Pause button, which we would tend to click on in order to say "Stop this playing, this is loud and obnoxious," the Pause button is actually an href click that does something other than pause.



LEO:  Ooh, that's sneaky.



STEVE:  Yes.  It's also possible to put up a transparent overlay on the page so the user, in clicking on something, is actually clicking on a transparent surface which receives the click, instead of what is visually going to receive the click.  That gives scripting or the browser much higher level of permission.  In fact, we use this characteristic in SQRL.  When you click on the QR code because you want to do same-machine logon, that initiates an href jump to the SQRL client running on the system as localhost.  The scripting can't do that.  That would be cross-site.  That would be going from the site you're logging into over to the localhost domain or IP on your own machine.  Script can't do it, but a user-initiated click can.



So that's a perfect example of how permissive it is.  And of course this has to happen because that's the way the web is linked together, right, is URLs linking you to other domains.  So if the user clicks on something, the browser goes, even if it's a jump offsite, somewhere else.  And so what's happening is the problem is this is also - maybe it's benign.  Many times it could be a funny-looking button saying, hey, if you want to go to the Disney site, click here.  So that's legitimate.  So there isn't technology that Google can deploy to block that because there may be very valid reasons for having a click take you somewhere else.



So the way Google is going to handle this is they have something called the "Abusive Experiences Report."  And it's going to take the form of a blacklist of sites known to be using misleading UI elements, misleading user interface elements such as Play or Pause buttons or whatever.  And so the idea is that, as I understand this, website owners can register their site with Google to receive early warnings if Google believes their site is using these types of misleading UI elements.



So this is the way Google's decided to handle it.  They don't want to just blacklist sites without warning, but they're going to blacklist sites that perpetrate this kind of abuse.  So they've decided what they'll do is they'll create a mechanism - and there's no, you know, not every site in the world has to do this.  But they're saying, if you're worried you might get on this blacklist, you can register yourself with us in advance, and you'll be able to check to see if you are being blacklisted in this fashion and then work with Google or change the behavior of your site in order to get pulled off of this pending blacklist.



So I can't think of - oh, and these sites will then end up getting blocked by Google's existing built-in pop-up blocker.  They will become blacklisted, and those UI functions which are what causes Chrome to worry about this in the first place, those will end up just not functioning.  So things that Google believed were misleading will end up being turned off.  So this is pretty aggressive; and, I mean, props to Google for being willing to do this because it's going to break some things, but there's no - I can't see a technological way to solve this problem because the issue is its misleading UI elements.



Well, you can't define that in code, so it's got to be Google learns that there's a certain site which is doing this, and so they blacklist that functionality on that site.  Doesn't kill the site.  You can still get there.  But when you click something that people were complaining about, it just won't work any longer until the site fixes it.  So again, aggressive, but I'm afraid that's what we've come down because the abuse is such that you have to take this kind of action.



Okay, Leo?



LEO:  Yes?  You rang?



STEVE:  You're not going to believe this one.



LEO:  Uh-oh.



STEVE:  I know you have heard the term JTAG, the JTAG port or a JTAG connection.  It's been around since 1985.  JTAG itself stands for the Joint Test Action Group.  But it's most known by anybody who's been playing with embedded stuff, like hard drives have a JTAG interface.  In fact, Leo, you've been changing firmware on some of your devices using JTAG.



LEO:  Yeah?  I didn't know that.



STEVE:  That's what it is.  It's a low wire count, sometimes two-wire, sometimes three or four, which is a universal standard.  So any embedded processor has a JTAG interface.  All the ARM chips and the TI chips, all of, I mean, the PIC chips, all these embedded processors, they have a JTAG interface.  And it is incredibly powerful.



With a JTAG interface, which typically has a clock and a mode and a data in and a data out, it is possible to halt the processor, to read out the contents of its registers, to change the contents of its registers, including the program counter, allowing you to cause it to start executing from somewhere else.  You can single-step it so you can say execute one instruction.  Then you can again read out all the contents of its registers.  In other words, it is a very powerful debugging interface.



You can attach this JTAG programmer, as they're called, clip it on the back of the chip, or find these JTAG programming pins on a device and essentially completely take it over remotely.  When people have hacked hard drives or sucked the firmware out of a hard drive, it's the JTAG interface that allowed them to do that.  And you're able to change the contents.  You're able to read and write RAM, read and write ROM, suck out the firmware.



Now, it's possible to blow a fuse, as it's termed, after using this JTAG interface to do programming.  And after you verify it's what you want, you're then able to irreversibly blow a JTAG programming fuse that turns off either some or all of the functionality in order to prevent, for example, your competition from reading out the contents of your firmware in your proprietary device.  So that can be done.  But believe it or not, a group known as Positive Technologies will be providing full information next month, but they have determined and revealed that, starting with Skylake and all subsequent chips, so Intel's Skylake chipset and subsequently, Intel - it's hard for me to even say this - has added an external JTAG interface to the USB ports.



LEO:  Well, it's convenience.



STEVE:  Oh, my lord.



LEO:  Wow.  That's kind of amazing.



STEVE:  Oh, it's just - it's breathtaking.  They call it the "god-mode hack" because it is.  Now, it's not remote.  So, I mean, thank goodness.  But it means that somebody having physical proximate access to a USB port on a motherboard from Skylake onward is able to essentially do anything they want.  This is complete access to the Intel Management Engine through the motherboard's USB ports.  Intel has an acronym.  They call it DCI, Direct Connect Interface.  How convenient.



LEO:  I take it they didn't blow the fuse on it.



STEVE:  Apparently not because these guys are going to demonstrate next month that it is possible to run unsigned code on the platform controller hub.



LEO:  Just plug in a USB.



STEVE:  Any, yes, any given motherboard from Skylake and after.  And essentially...



LEO:  Is there not a fuse?  Maybe, I mean, can I retroactively blow the fuse?



STEVE:  The problem is that would disable what Intel apparently thinks is for some reason useful.  So it's just beyond me.  Hopefully we will hear a statement from Intel about how they explain this.  But, I mean, the JTAG port, it is god.  I mean, it is a full debugging interface.  And they said, oh, well.  Remember last week we were talking about what was required.  You had to go find the little BIOS chip, eight pins on the motherboard, and get out your soldering iron and your microscope and attach stuff.  No.  Turns out just plug in a sufficiently configured USB device, and you're good to go.



LEO:  Wow.



STEVE:  Making the world much easier for the rest of us.



LEO:  I'm going to pour some epoxy in my USB ports.



STEVE:  Wow.  Yes.



LEO:  I'm thinking, I'm just thinking some of the usefulness would be in manufacture.  If you update your BIOS on your motherboard, and you fry it, that would be one way they could get in and replace the firmware and the motherboard, I would guess; right?



STEVE:  Yeah.  I just, I mean, you could have a dedicated JTAG port.  Why export it over USB?



LEO:  Yeah, that makes it pretty easy and convenient.  It's easy to update.



STEVE:  Now, who are they saying "yes, sir" to is what comes to mind.  I mean, yes, not a remote hack.  But, boy, it does mean that drive-by system, deep system compromise becomes possible.



LEO:  You've got a little device, you plug it in the USB port, press a couple of buttons, bzzz bzzz bzzz, walk away, and you now own that system; right?



STEVE:  Yup.  Yup.  You've just changed the core, as it's known, Ring -3.  You've got...



LEO:  It's that low.  It's so low that you couldn't - and of course antivirus wouldn't, I mean, talk about rootkit, antivirus wouldn't see it because it's in the firmware.



STEVE:  Yes.  That is, it is the processor which is executed before the main processor starts.  And we've already seen that it's possible for that code to be injected into the address space of the higher level Intel processor.  So, yeah, it's a complete sub-rootkit capability.



LEO:  Wow.  



STEVE:  Amazing.



LEO:  I wonder what Intel's response is to this.



STEVE:  That's going to be interesting.



LEO:  They need to take this Management Engine out of all future chips.  That's ridiculous.



STEVE:  Yeah, it's really, I mean, and I'm sure we've talked about how Google is scrambling around trying to get it out of their servers in all their server farms because it just...



LEO:  Oh, it's in everything.



STEVE:  ...terrifies them, yeah.



LEO:  Is there a physical hardware way to disable it?  There's no trace.  It's in the die; right?



STEVE:  But the problem is, if this is a JTAG interface, this overrides.  There is a bit, that bit that the NSA uses because the NSA doesn't want this in their hardware, either.  



LEO:  No.



STEVE:  So there is a bit...



LEO:  They want it in our hardware, not their hardware.



STEVE:  Exactly.  There's a bit that they can set which causes only those two critical modules, those boot config modules, to just - because you need this thing to set the clock frequencies and set the wait states on the DRAM.  It's that our motherboards are so, the BIOSes are so capable now, where you're able to change, like, wait states and clock speeds and everything, it's just amazing.



Well, all of that flexibility is this IME.  That's the processor running this MINIX OS which allows all that to happen.  Because that has to happen underneath the main Intel CPU in order for it to even get up and going.  So what this bit does that the NSA can set is it causes only that to happen.  And then all those other modules, for example that module that was causing me to, well, I was going to say "pull out my hair," but no, apparently I did that already.



LEO:  Because you had a vPRO Ethernet card.



STEVE:  Because I had, yeah, actually it was an older motherboard that was having some sort of ARP storm caused entirely by the fact that this Intel Management Engine was doing its own thing on the port I was using.  All I had to do was switch to the secondary port.



LEO:  Because that port was the vPRO.



STEVE:  Which the IME is - yeah.



LEO:  That was the managed Ethernet port.  Now, previous hacks of the IME have required that you have it be enabled and it be a vPRO machine.  But it sounds like this IME is in every chip Intel makes; right?



STEVE:  Yes.  It is the thing...



LEO:  There's just no interface to it on non-vPRO systems.



STEVE:  Right.



LEO:  But there's still a JTAG interface.  Or no?



STEVE:  Well, it's the same chipset.  It's the chipset.



LEO:  It must be; right.



STEVE:  Yes.  Wow.  And, I mean, I'm sure everywhere is like, at the moment we get more information, this makes the hack we talked about last week obsolete immediately.



LEO:  Right, right, right.



STEVE:  No.



LEO:  I don't need that thing.



STEVE:  You don't even need to open your box.



LEO:  I don't need to clip on.



STEVE:  Exactly.  There will be how-to's, like how to turn off your IME forever.  Plug this in, and it's going to be shut down.



LEO:  Well, that's what somebody should make is a USB key you plug in that disables IME.



STEVE:  That absolutely will happen.



LEO:  Oh, I hope so.



STEVE:  We can predict that now.



LEO:  They could sell that for a hundred bucks.  I would buy it right away.



STEVE:  Yup.  Okay.  So we've talked - there is a new jargon, a new term of art, "cryptojacking," which I think is wonderful.  That's, of course, hijacking your power and CPU for crypto mining for which you gain no benefit:  cryptojacking.  An analysis by a researcher, Willem de Groot, he took a look across the Internet, found 2,496 sites, so four fewer than 2,500 sites, doing crypto mining whenever users visited them.  And this of course was - they were all using the popular Coinhive Monero cryptocurrency mining JavaScript.



But it turns out that the beneficiaries of that had a very interesting distribution.  In the script, there for anyone to see who does a "view source," is the account, the Coinhive account to which the mining effort is credited.  85% of these just four shy of 2,500 sites, 85% of them were benefiting just two mining accounts.  Which, whoa, is not what you expect.  If crypto mining was for the benefit of the hosting site, then you would expect 2,500 different Coinhive accounts, one per site.  No.  85% of them were linked to just two mining accounts; while the remaining 15% were spread out over unique Coinhive accounts, although those 15% had ID tags which, while different, all had the same format and pattern involving the domain of the site that was hosting them, meaning that a third group was using a different approach.  But one group accounts for the remaining 15%.



In other words, of the nearly 2,500 sites that Willem de Groot examined, there were three, a total of just three individuals or groups behind all of this cryptojacking, and in every case he found other malware that was present on the servers.  So the conclusion here is that these sites are compromised and compromisable, that they were already hosting and probably known to be vulnerable to various sorts of website compromise.  And so the bad guys thought, well, we've already got some of our malware on these sites.  We might as well do a little mining for ourselves while people are visiting these sites.



So essentially three different groups are responsible for what appears to be an explosion in cryptojacking.  It's not actually all these individual domains hoping to make some money from their visitors.  It's bad guys, three of them in total, who have used compromised features of those websites to insert cryptocurrency mining.  Now, what this says to me is that Coinhive, which is the single focal point of all this, should clearly be shut down.  I mean, should shut down itself.  These guys should say, okay, we had an idea, it works, but it's being abused.



The problem is that it's sort of a Hobbesian bargain here because remember, when we first talked about it, I was shocked by the high percentage that Coinhive retained of the earnings.  So they're making a lot of money on the fact that, I mean, they're essentially complicit in this.  They must know what's going on.  They know who's hosting their script because those queries are coming to their servers to load their script into innocent users' browsers.  The problem is they're making a lot of money.  Well, some money.  It's not clear that any of this actually makes a lot of money because I don't think these sites are very popular.  But they're obtaining a substantial portion of the proceeds.  So they've essentially become agents of these malicious hackers and, I would argue, probably willful agents.



In terms of what to do, as we've talked about before, things like uBlock Origin are already blocking this.  We know that adblockers are blocking it.  But I did want to mention another old trick of the trade, and that is that the hosts files is  100% effective in blocking this.  There are two domains from which all of this is hosted:  Coinhive.com and Coin-hive.com,  coin dash H-I-V-E dot com.  If you were to just go to your browser, as we did back when we first talked about this, and put in Coinhive.com, up would pop their site.



Well, you simply use the hosts file to redirect Coin-hive.com and Coinhive.com to the localhost, 127.0.0.1.  And the hosts file is still the first place that Windows goes before it performs a DNS query.  And that will prevent any site you go to from providing script which would cause your browser to look up the IP for Coinhive.com.  It'll do that.  It'll get 127.0.0.1, where almost certainly you have no web server running; or, even if you do, it's not Coinhive.com, and you will not get any scripting running.  So I like that as a tip for anyone, our more technical users who like the idea of just preventing their system or any browser on their computer, whether it's got uBlock Origin or an adblocker or not, from inadvertently running anything from Coinhive.com.



And when I first heard that JavaScript was mining bitcoin, my first reaction was, whoa, how can that be fast enough?  Because maybe 10 years ago; but, boy, how could it be feasible now?  Well, it turns out that it is 100% the fact or fault of a technology we first covered 2.5 years ago, in April of 2015, known as WebAssembly.  We mentioned this back then, that work on something known as WebAssembly had started, and that browser makers were going to join forces to create a binary bytecode universal, essentially, assembly language, like machine language, a virtual machine for web browser-based applications.



And as of last month, October, when we got iOS 11 and thus Safari and the Fall Creators update for Windows 10, where Microsoft's Edge HTML 16 occurred, now all major browsers support the WebAssembly standard.  Firefox and Chrome were first this past summer.  And then the other Chromium-based browsers, like Opera and Vivaldi, they inherited this feature as soon as it went into the Chromium stable version.  And now we've got Safari and Edge.  And I don't know about IE11.  I don't know whether that got it or not.



So as it is today, JavaScript, well, without WebAssembly, as we know, JavaScript source code is provided by a web server in ASCII, loaded into the browser, and must then be essentially interpreted.  And typically it's compiled down into JavaScript bytecode by a JIT, a so-called Just In Time compiler, which compiles it sort of as it goes in order not to have a huge compilation burden upfront.  People want things to start immediately.  So the idea is that the first time it encounters not-yet-compiled JavaScript, it compiles it incrementally.  And so, if there are areas that aren't being used of JavaScript, it just doesn't bother with those.  But this takes time.  It wastes resources because, after all, every time you go back, you're getting the same script again and having to recompile it again.



So what we've done is the industry - and the reason I'm bringing it up is not to complain that it's being used by Coinhive, but to mention that this demonstrates the incredible performance.  The browser-based gaming guys were all over this immediately because it offers stunning performance.  It's about a factor of 20 times faster than JavaScript for parsing the language because what it does is it means that this is precompiled.



So the person who is authoring this for a site compiles the JavaScript or the C, C++, or Rust, because there are multiple compilers now, into this common bytecode.  That then is offered for download to the browser.  It comes in, and it can be executed instantaneously.  No need to interpret it.  It is far smaller because it is not ASCII, it is binary.  It's a binary blob of bytecode, which the browser can immediately start executing and execute faster.  All the optimization can be done statically once, and then everyone who downloads it gets the benefit of it.  You don't need to use JavaScript.



Just for example, people who want to write in C or C++ have had to do something known as "transpiling," essentially transcompiling, from another language over into JavaScript in order to be able to offer it to browsers.  Now you can have different frontend compilers that will compile into this common WebAssembly in order for everyone to have access to it.  So we're actually there.  And people are a little surprised that this happened without endless committees arguing about this or that feature.  I mean, it is established.  It is version proof.  It's been, you know, it just sort of happened without us really seeing it happen.



If anyone is interested in more, WebAssembly.org is the site, W-E-B-A-S-S-E-M-B-L-Y dot org.  And it's, as they say, it's in there.  It's already in our browsers.  So, very cool.  I think that, as we begin to see it adopted, certainly companies like Google will jump on this immediately.  And if people are still using pre-WebAssembly browsers, that would be visible in the user-agent field of the query for the page.  So a server could say, whoops, this guy's using a browser that is not WebAssembly-capable; so, yes, we'll feed him the JavaScript equivalent.  But that will happen less and less as those browsers die off and stop being used.



And eventually, oh, there's one more thing I forgot.  This is another beautiful part of it.  The problem was, wait a minute, I'm downloading a big binary blob for which I have no visibility.  Because it's very nice to be able to look at the JavaScript which your browser has pulled down.  It turns out they thought of that, too.  There is a very clean text version of the binary.  So you'll be able to do a view source on a page containing a WebAssembly invocation, a WebAssembly binary blob, and view it in textual form in a meaningful way.  So, I mean, it's not going to be - probably it's going to be like assembly language.  But still, there is a viewer for this as part of the offering and as part of the support that the browsers offer.  So, very cool.



This is just a quickie.  And Leo, you'll get a kick out of this.  The site is Gcc.godbolt.org.



LEO:  Love the name.



STEVE:  Gcc.godbolt.org.  It is very cool.  It is an interactive, side-by-side display of...



LEO:  Oh, look at that.



STEVE:  ...GCC compiler input and output.  And what's neat is you can, over on the right-hand side, you can change which compiler version and which target chip, like x86, x64, ARM and so forth.  And in every case you see the output of the compiler, given the input.  And over on the left, if you highlight different lines, you'll see that the lines are - the source input is in different colors.  And so it shows which line expands into which instruction.



LEO:  That is really neat.  That is so neat.



STEVE:  Isn't that cool?



LEO:  Yeah.  This would be a good way to learn assembly.



STEVE:  Yes, actually, it would be because you're able to see.  And if you put something more fancy in for the source code, the example they have is just a simple square of a number.  But it's a good starter.  But you put something else in, like a little bit of a program, and you can actually see the code that the compiler generates for all different chipset families.



LEO:  Yeah.  This is the ARM code.  Wow.



STEVE:  Yeah.



LEO:  This is so cool.  That is so cool.  I really like this.  



STEVE:  Yup, Gcc.godbolt.org.



LEO:  MIPS?



STEVE:  Yup, exactly, the MIPS, yeah.



LEO:  PowerPC, wow.  That is neat.



STEVE:  And just very, very cool to compare.



LEO:  So it's an interactive C compiler.  Oh, you could try different languages, too.  You can write something in Swift or Haskell or Rust.



STEVE:  Ah, nice.



LEO:  And then see how the Rust compiles or assembles out.



STEVE:  Yes.



LEO:  Nice.  What a nice little hack.  Godbolt.



STEVE:  Yeah.  Godbolt.  Gcc.godbolt.org.  And I did want to mention we've talked about the KRACK vulnerability and updating routers.  I was looking at mine the other day, and there was a little yellow exclamation point flashing.  And I thought, oh, what's that?  And it's like, oh, there's an update for your firmware.  In this case it was an Asus RT-AC68U router.  And I was of course very curious to know what the firmware was doing.



And sure enough, first thing on the list, security fixed:  Fixed KRACK vulnerability.  Then it fixed a series of CVE numbers.  Let's see:  three DNS, two or three DHCP, predictable session tokens, logged user IP validation, a GUI authorization vulnerability, a cross-site scripting vulnerability in their own web interface.  Added some features:  HDD hibernation if you have a hard drive hooked onto it, a URL filter black-and-white list, bandwidth limiter on the guest network.



Anyway, so this is the kind of - and this is from Asus.  This is the kind of support we are now recognizing we have to have for our connected devices, especially those that are facing the public Internet, as most routers are doing for us.  So I just wanted to just - this was my real-world experience.  It didn't do it for me, and that's the takeaway here.  Yes, all this happy goodness was there.  But it was waiting for me to log into the admin interface and notice that there was a little yellow exclamation point flashing in the upper...



LEO:  Yeah, that's the only drawback.  That's why you want pushed updates out; right?



STEVE:  Yup.  And I think that - oh, and a lot of people mentioned the side-by-side firmware.  We were talking about the emerging standard for updating IoT devices.  And I should have mentioned, yes, that for example this has been done historically for Internet-connected devices.  I mean, back from the Cisco Internet Operating System, their IOS, there have always been multiple firmware images.  And so you're able to update one, and then you switch over to it next time you boot.  And I've got a bunch of Internet equipment that runs GRC, which is all multiple firmware, flashable firmware images, and then you choose which one you want to boot from.  So that's well existing technology.  But it's certainly, as far as we know, nothing like that exists for light bulbs.  So the whole point is let's push this time-proven approach down into very inexpensive devices.



And I just wanted to mention also, I recently had to have a conversation with someone at Hover.  And I was very impressed.  Well, I had to do something that required a conversation.



LEO:  We should mention they're a sponsor.



STEVE:  Oh, they are?



LEO:  Oh, you didn't know Hover was a sponsor?  Even better.



STEVE:  They're my domain name, yes.



LEO:  Even better.  Yeah, well, we had a Hover ad in iOS Today, and I used your name in vain because I know you're very happy.



STEVE:  Good.  Yes, I am.



LEO:  Yeah, they're a sponsor.



STEVE:  Okay.  So, good, they're a sponsor.  And, I mean, I fell in love with them before.  As everyone knows, I was looking for a solution, an alternative to Network Solutions, who I had been with forever, have been with forever.  In order to get what I needed done, I first received a PIN through email to my registered email account.  I then had to have a telephone conversation where I had to read them back the PIN and feed them the current, because I have two-factor authentication set up on my Hover account, had to read them the instantaneously correct current six-digit two-factor authentication, which the guy I was talking to, Shane, immediately confirmed and said, "Okay, what do you need?"  So that's what I want.  



LEO:  Nice.



STEVE:  I want that kind of frontline security for the people who are maintaining my domain name registration.  So, yay.  Also, a question that frequently comes up I wanted to touch on briefly.  I saw this, Dave L. in San Rafael, so in your neck of the woods, Leo.  The subject was "SpinRite on Hybrid Hard Drives," send to us on the 5th of November.  He said:  "My new laptop has a hybrid hard drive on which I have been afraid to run SpinRite in fear that the solid-state portion would be degraded by heavy reads and writes.  Can you advise listeners on the best way to run SpinRite on such a drive?"



And the answer is yes, of course, and there's nothing you need to do.  From the very first days of SpinRite, SpinRite takes advantage of some features which have always existed from the beginning of the IDE world to disable all caching on drives.  SpinRite has never wanted to test the RAM cache on drives which do caching.  And so the hybrid drive is an extension of that.  Basically it's a non-volatile cache where the things that the drive determines are read most often are kept in order to minimize read delays because it's possible to read from a big SSD cache, as we know, way more quickly than it is to move the heads and wait for the data to rotate around and get read if it doesn't exist in the much, much smaller RAM cache.



So essentially there's a hierarchy of caches:  RAM, and then essentially non-volatile read cache, and then the big physical magnetic read/write memory.  What SpinRite does is to disable all caching, and the hybrid caching is part of that.  So that gets turned off in order that SpinRite has direct physical access to the magnetic media, which is what you really want to perform recovery on and testing on.  So users need do nothing.  SpinRite, thanks to the fact that caching exists and the hybrid sort of interstitial cache between RAM and magnetic surface is also a cache.  That gets turned off, and SpinRite has direct access.  So works just fine on hybrid drives.



Some quick closing-the-loop pieces.  Christopher Ursich said:  "The installer I downloaded today on November 11th from LastPass was signed on October 3rd, but with a cert that expired on October 29th.  Is it good practice to trust or not?"  Okay.  So this is interesting.  Code-signing certificates are handled differently than web-signing certificates, than certificates that websites use.  There, because the nature is real-time downloading, like on a website, you are establishing a connection right now where you want to verify the identity right now of this website.



There the certificate the site is offering in real-time must have its dates be valid.  We'd often talk about a site inadvertently offering an expired certificate.  But the model for protection is different for code signing.  With code signing, what you really want is to know that, at the time of the signing, the company doing the signing had a valid certificate.  So what you want to prevent is somebody gets an old - some bad guy gets an old expired certificate and then uses that to sign their malware, which would then be trusted.



So the way this is handled is there's something known as a time server.  And the code-signing system is able as part of what it does to, on the fly, query a time server.  The URL for that can be provided by the user, or it's often contained in the certificate, that is, the certificate itself says where to query for time.  So the signing process, when performing the signing, pings a time server, gets the current time, verifies that the certificate is valid, and binds all of this together so that what you get is you get the code with a valid certificate and the timestamp on which the code was signed and the assertion that, at the time it was signed, that time fell within the validity period of the certificate.



And when you think about it, that is exactly what you want.  It makes sense because code that is going to be downloaded could sit on the shelf for years.  And so 10 years from now you may download it.  Well, the certificate is certainly going to have expired.  So rather than, like, forcing all code everywhere to always be resigned, which is not practical, with a now-valid certificate, the idea is this use of a time server and a timestamp sidesteps that.  It's what we really want.  It verifies when it was signed it was valid.  And then, of course, you check the signature to verify that it hasn't been changed since then.  So great question, Christopher, and I'm glad you asked for clarification.



I love this.  David Lemire says:  "In the discussion of disabling Intel's ME" - the management engine that we've just been talking about - "in last week's episode," he says, "it created a terminology question."  He says:  "If killing a phone is 'bricking it,' is killing a motherboard 'planking'?"  Which I thought was kind of fun and clever.  Of course we call it "bricking the phone" because then you're just sort of holding a brick in your hand.  In the case of a motherboard, well, what would a dead motherboard be?  I guess maybe a plank.



Oh, and Marc Boorshtein said:  "I'm not sure favicon.ico is a good login test."  He said:  "Usually anonymous since it's displayed on login pages."  Okay.  So Marc, I think what you're missing here is that the reason favicon.ico is a good test is that it is almost certainly known to be an image on the root of any server.  So without needing to customize a query per site, where you would just go get some image on the site, and you could certainly do that, you could know that Google has this image on their logo or something, and you could query that image.  But most sites are going to have a favicon.ico image.



The point is, images are allowed to be pulled by scripts across domain, meaning that you can go get some other site's .ico image.  And all we're wanting from that is to obtain the cookie for that site and that user.  And cookies are always offered with any asset from a site.  So it doesn't matter if it's anonymous or not.  All it has to be is any image.  And so favicon.ico is a good one.



And this was sent to me and to you, Leo.  Kind of a cool project that a Luis Zepeda performed for us.  He said:  "Hey, Leo.  I remember you asked @SGgrc if he believed that security was getting better across all these years of Security Now!.  Well, I ran some natural language processing and sentiment analysis tools" - and here there's a graph, you should put it up on the screen - "sentiment analysis tools across all the transcripts, and his analysis shows a slight downward trend.  It's getting slightly worse."



LEO:  Downward is bad.



STEVE:  Yeah.  So from Episode 0 out through, looks like he went all the way through, like Episode 636.



LEO:  Look at this outlier up here.  We were in a really good mood that day.



STEVE:  That must have been a happy day, yes.  Woohoo.



LEO:  So these are, like, negative words and thoughts.



STEVE:  Yes, correct.



LEO:  Wow.



STEVE:  And so basically some things are positive, and some things are negative.  And maybe it actually reflects my amount of caffeination.  I'm not sure.



LEO:  Well, that's pretty - it's a pretty random distribution.



STEVE:  It is.  It is, very.  It's only a very slightly downward trend.



LEO:  Very slight, yeah.  Wow.



STEVE:  Very cool.  But thank you, Luis.



LEO:  That's brilliant.  I know we have some geeky listeners.  Wow.  That's amazing.



STEVE:  And Ronald A. Suchland said:  "I cannot find anything to stop Leak Test."  What?



LEO:  Huh?



STEVE:  "Win7 64-bit.  Lavasoft locks up the computer."  Okay.  So Leak Test is something I wrote many moons ago to demonstrate the concept of outbound blocking.  That is, the idea - and this was because I liked the idea that something in your computer could be caught leaking behind your back.  And in fact it was because I was an early tester of ZoneAlarm that ZoneAlarm warned me of the Aureate adware which was connecting without my knowledge or permission behind my back.  And it was like, what?  What?  What is this DLL that I didn't give permission to doing talking to somebody I didn't say it could talk to?



So I created Leak Test as a simple app that anyone could use to see whether their system would detect outbound connections.  So it's really sort of obsolete.  The by-default things in your computer are allowed to create web connections to the outside world.  So, yeah.  If you had a firewall which was in the machine, where you had set it to "Deny all unless I explicitly accept," then Leak Test would fail to make its connection, and maybe you would get warned.  But that would be very burdensome.  We did that once upon a time with ZoneAlarm.  And now we've sort of loosened things up.



The firewall that is in Win7 64-bit will do that.  If you told it to, you could lock it down tight and tell it to deny every connection from anything you didn't ask, explicitly permit.  But you'd pull your hair out in no time.  I mean, if you've ever done a netstat on your computer, netstat -an, oh, my goodness, it shows what's going on.  And we've just lost control of all this now.  So it's like trying to run NoScript these days on a browser.  You just can't.  Everything is scripting, and everything is phoning home.  And we just have to trust that nothing evil is being done behind our back.



And finally, let's see.  Oh, this is Chris in Germany, a "Comment on Steve's comment last episode."  He said:  "Hello, Leo and Steve.  Steve, you commented in the last episode that users should never download anything, even from trusted sources."



LEO:  I don't think we said that.



STEVE:  No, I didn't.  But I want to clarify.  "Does that include the show notes you and Leo upload to your show?"



LEO:  No, never download anything.



STEVE:  "To be fair, I don't think this advice is feasible for the average user and sounds Stallman-esque.  Could you elaborate on that further?  How am I supposed to use the Internet without downloading anything?"  And he sounds German, doesn't he.  "Thanks a lot.  I appreciate the podcast and your experience."



Chris, let me explain.  What I said was do not ever download anything offered to you that you didn't go looking for.  That's the distinction, and I would argue probably the single most valuable piece of advice available today.  That's what sites do when they say, "Oh, you need to download this new font because you want to see this page properly."  Or, "Oh, we want to show you this amazing person making pizza, but you need to update your version of Adobe Flash.  Click here."  Those are things being pushed on you, pushed to you.  And to that you want to say no.  It is almost guaranteed to be malware.



So if you believe you need a new version of Adobe Flash, go to Adobe and get it, after thinking five times about whether or not even that is a good idea.  The point is never accept candy from a stranger, or a download link from some random site, even one you trust.  Yes, download our show notes.  Download SQRL as soon as it's available from GRC.  But go get it.  Don't go to a site that says, "Oh, you need SQRL.  Click here, please."  Come to GRC and ask for it.  It's free.  But never download something that some site is offering to you.  That's the point I'm trying to make because that's a form of social engineering.  Many sites try to take advantage of the naivet of users who are, like, "Oh, I didn't realize that was obsolete.  Okay, I'll click here to update."  I mean, the problem is it's going to be successful most of the time.



Okay.  So last week our often-quoted guru and security expert and crypto person whose books I have behind me on my shelf, Bruce Schneier, was asked to testify before the House Energy and Commerce Committee on his feelings about the Equifax hack.  There is on his site - his current blog is www.schneier.com, S-C-H-N-E-I-E-R dot com - there's a link to the video of his testimony, if you want to watch it.  But I want to share some of the points he made because they were good.



And he starts out a little bit with his CV:  "Mr. Chairman and Members of the Committee, thank you for the opportunity to testify today concerning the security of credit data.  My name is Bruce Schneier, and I am a security technologist.  For over 30 years I have studied the technologies of security and privacy.  I have authored 13 books on these subjects, including 'Data and Goliath:  The Hidden Battles to Collect Your Data and Control Your World,' published in 2015 by Norton."



He says:  "My popular newsletter Crypto-Gram and my blog Schneier on Security are read by over a quarter million people.  Additionally," he says, "I am a Fellow and Lecturer at the Harvard Kennedy School of Government, where I teach Internet security policy, and a Fellow at the Berkman Klein Center for Internet and Society at Harvard Law.  I am a board member of the Electronic Frontier Foundation, Access Now, and the Tor Project; and an advisory board member of Electronic Privacy Information Center and VerifiedVoting.org.  I am also a special advisor to IBM Security and the Chief Technology Officer of IBM Resilient."



So obviously he's got a beautiful CV that demonstrates to these guys who have no clue which end is up that this is a guy whose opinion is informed.  He says:  "I am here representing none of those organizations and speak only for myself based on my own expertise and experience.  I have eleven main points."  And I'm going to skip them toward the end, but I want to share the main salient ones.



He says:  "One, the Equifax breach was a serious security breach that puts millions of Americans at risk."  We all know that, but he wants to establish some ground.  "Equifax reported that 145.5 million U.S. customers, about 44% of the population, were impacted by the breach.  That's the original 143 million plus the additional 2.5 million disclosed a month later.  The attackers got access to full names, Social Security numbers, birth dates, addresses, and driver's license numbers.



"This is exactly the sort of information," Bruce says, "criminals can use to impersonate victims to banks, credit card companies, insurance companies, cell phone companies, and other businesses vulnerable to fraud. As a result, all 143 million US victims are at greater risk of identity theft, and will remain at risk for years to come.  And those who suffer identify theft will have problems for months, if not years, as they work to clean up their name and credit rating.



"Two, Equifax was solely at fault."  He says:  "This was not a sophisticated attack.  The security breach was a result of a vulnerability in the software for their websites, a program called Apache Struts.  The particular vulnerability was fixed by Apache in a security patch that was made available on March 6, 2017 and was not a minor vulnerability.  The computer press at the time called it 'critical.'  Within days it was being used by attackers to break into servers.  Equifax was notified by Apache, US-CERT, and the Department of Homeland Security about the vulnerability and was provided instructions to make the fix.



"Two months later, Equifax had still failed to patch its systems.  It eventually got around to it on July 29.  The attackers used the vulnerability to access the company's databases and steal consumer information on May 13, over two months after Equifax should have patched the vulnerability.  The company's incident response after the breach was similarly damaging.  It waited nearly six weeks before informing victims that their personal information had been stolen, and that they were at increased risk of identity theft.  Equifax opened a website to help aid customers, but the poor security around that - the site was a domain separate from the Equifax domain - invited fraudulent imitators and even more damage to victims.  At one point, the official Equifax communications even directed people to that fraudulent site."



He says, finishing point two:  "This is not the first time Equifax failed to take computer security seriously.  It confessed to another data leak in January 2017.  In May 2016, one of its websites was hacked, resulting in 430,000 people having their personal information stolen.  Also in 2016, a security researcher found and reported a basic security vulnerability in its main website.  And in 2014, the company reported yet another security breach of consumer information. There are more.



"Three," he says, "there are thousands of data brokers with similarly intimate information, similarly at risk.  Equifax," he says, "is more than a credit reporting agency.  It's a data broker.  It collects information about all of us, analyzes it all, and then sells those insights.  It might be one of the biggest, but there are 2,500 to 4,000 other data brokers that are collecting, storing, and selling information about us, almost all of them companies you've never heard of and have no business relationship with.



"The breadth and depth of the information the data brokers have is astonishing.  Data brokers collect and store billions of data elements covering nearly every U.S. consumer.  Just one of the data brokers studied holds information on more than 1.4 billion consumer transactions and 700 billion data elements, and another adds more than 3 billion new data points to its database each month.  These brokers collect demographic information:  names, addresses, telephone numbers, email addresses, gender, age, marital status, presence and ages of children in household, education level, profession, income level, political affiliation, cars driven, and information about homes and other property.  They collect lists of things we've purchased, when we purchased them, and how we paid for them.  They keep track of deaths, divorces, and diseases in our families.  They collect everything about what we do on the Internet."



He says:  "Number four, these data brokers deliberately hide their actions and make it difficult for consumers to learn about or control their data."  He writes:  "If there were a dozen people who stood behind us and took notes of everything we purchased, everything we read, searched for, or said, we would be alarmed at the privacy invasion.  But because these companies operate in secret, inside our browsers and financial transactions, we don't see them, and we don't know they're there.



"Regarding Equifax, few consumers have any idea what the company knows about them, who they sell personal data to, or why.  If anyone knows about them at all, it's about their business as a credit bureau, not their business as a data broker.  Their website lists 57 different offerings for business - products for industries like automotive, education, healthcare, insurance, and restaurants.  In general, options to 'opt-out' don't work with data brokers.  It's a confusing process and doesn't result in your data being deleted.  Data brokers will still collect data about consumers who opt out.  We will still be in those companies' databases and will still be vulnerable.  It just won't be included individually when they sell data to their customers.



"Five," he says.  "The existing regulatory structure is inadequate.  Right now there is no way for consumers to protect themselves.  Their data has been harvested and analyzed by these companies without their knowledge or consent.  They cannot improve the security of their personal data and have no control over how vulnerable it is.  They only learn about data breaches when the companies announce them, which can be months after the breaches occur, and at that point the onus is on them to obtain credit monitoring services or credit freezes.  And even those only protect consumers from some of the harm, and only those suffered after Equifax admitted to the breach.



"Right now, the press is reporting dozens of lawsuits against Equifax from shareholders, consumers, and banks.  Massachusetts has sued Equifax for violating state consumer protection and privacy laws.  Other states may follow suit.  If any of these plaintiffs win in the court, it will be a rare victory for victims of privacy breaches against the companies that have our personal information.  Current law is too narrowly focused on people who have suffered financial losses directly traceable to a specific breach.  Proving this is difficult.  If you are the victim of identity theft in the next month, is it because of Equifax, or does the blame belong to another of the thousands of companies who have our personal data?  As long as one can't prove it one way or the other, data brokers remain blameless and liability free.



"Additionally, much of this market in our consumer data falls outside the protections of the Fair Credit Reporting Act.  And in order for the FTC (Federal Trade Commission) to levy a fine against Equifax, it needs to have a consent order and then a subsequent violation.  Any fines will be limited to credit information, which is a small portion of the enormous amount of information these companies know about us.  In reality, this is not an effective enforcement regime.  Although the FTC is investigating Equifax, it's unclear if it has a viable case."



And so anyway, I won't go on.  "Number six," he says, "the market cannot fix this because we are not the customers of the data brokers."  As we know, we are the products which the data brokers sell.  So this has perverse incentives.  The data brokers are selling to companies that want the information.  So this doesn't, in this system, traditional market forces don't work to apply pressure.  The customers want the information, want it to be easy to get, don't want us to be able to block it from their access.  So as a consequence, it has been made hard for us to do this.  And in fact he makes the point that financial markets reward bad security.



He writes:  "Given the choice between increasing their cybersecurity budget by 5% or saving that money and taking the chance, a rational CEO chooses to save the money.  Wall Street rewards those whose balance sheets look good, not those who are secure.  And if senior management gets unlucky and a public breach happens, they end up okay.  Equifax's CEO did not get his $5.2 million severance pay, but he did keep his $18.4 million pension.  Any company that spends more on security than absolutely necessary is immediately penalized by shareholders when its profits decrease."



And he finishes:  "Even the negative PR that Equifax is currently suffering will fade.  Unless we expect data brokers to put public interest ahead of profits, the security of this industry will never improve without government regulation."  Anyway, so "Number seven, we need effective regulation of data brokers.  Number eight, resist the complaints from the industry that this is too hard."  He notes that credit bureaus and data brokers and their lobbyists and their trade association representatives will claim that these measures are too hard.



He says:  "They are not telling you the truth."  He says:  "Take one example, credit freezes.  This is an effective security measure that protects consumers.  But the process of getting one and of temporarily unfreezing credit is made deliberately onerous by the credit bureaus.  Why isn't there a smartphone app that alerts me when someone wants to access my credit rating and lets me freeze and unfreeze my credit at the touch of the screen?  Too hard?  Hardly.  Today you can have an app on your phone that does something similar if you try to log into a computer network, or if someone tries to use your credit card at a physical location different from where you are."



He says:  "Moreover, any credit bureau or data broker operating in Europe is already obligated to follow the much more rigorous EU privacy laws.  The EU General Data Protection Regulation will come into force, requiring even more security and privacy controls for companies collecting and storing the personal data of EU citizens.  Those companies have already demonstrated that they can comply with those more stringent regulations."



Anyway, so really, really good testimony from Bruce.  He finishes with number 11, saying:  "We need to do something about it.  Yes, this breach is a huge black eye and a temporary stock dip for Equifax - this month.  Soon, another company will have suffered a massive data breach, and few will remember Equifax's problem.  Does anyone remember last year when Yahoo admitted that it exposed personal information of a billion users in 2013 and another half billion in 2014?"  He says:  "Unless Congress acts to protect consumer information in the digital age, these breaches will continue."



Finally:  "Thank you for the opportunity to testify today.  I will be pleased to answer your questions."  And Bruce then did that.  So bravo for having someone who understands the problem, who understands security, and who understands that we could easily, if they chose to, give us the technology, at least in these cases, to manage the availability of our credit far more usefully than we have today.



LEO:  And yet nothing will ever happen.



STEVE:  Nope.



LEO:  I'm impressed that Congress, or staffers probably, had the good sense to invite Bruce, though.  That's good.



STEVE:  Yup, yup.  It certainly won't happen without it.  And so maybe something will come of it.  We can hope.



LEO:  I love the idea.  And of course it would be a simple thing to have an app that says, "Hey, you want to unfreeze your credit?  Somebody wants a credit report."  Say yes, boom, done.



STEVE:  Yeah, well, it would be like so-and-so who you're renting an apartment from wants to verify your credit.



LEO:  Or Google or Microsoft want - you just tried to log in.  You approve it?  Yes.  



STEVE:  Yup, exactly.



LEO:  It's an easy thing to do, but of course it impedes their ability to profit on selling your credit report.



STEVE:  Creates a little bit of friction, yes.



LEO:  And they don't, well, it stops it cold.  If you have a credit freeze on your account, they can't make any money on you.



STEVE:  Right.



LEO:  Ah, well.  Such is life.  Steve has done it again, my friends.  In a mere two hours and a few odd minutes, he has totally - well, less than that, actually.  About an hour 50.  He's totally changed our perception of the world around us.  And now we put him back in his box for another week, and he will continue to gather information for next week's episode.



STEVE:  I'll do it.



LEO:  You can find this and every episode, how many have we done here, 637, at GRC.com.  He's got transcripts there, too.  That's where that sentiment analysis came from.  Those are freely available, downloadable, written not by a machine, but an actual human person named Elaine.  She does a great job, so take advantage of those.  He also has 64Kb audio files there.  And when you're there, make a little yabba dabba doo happen in Steve's heart by buying a copy of SpinRite, the world's finest disk hard drive recovery and maintenance utility.  That's his bread and butter.



STEVE:  It is that.



LEO:  Without that, he would have no coffee.  So please...



STEVE:  Keep me caffeinated.



LEO:  Keep him caffeinated.  God only knows what would happen if he weren't.  At this point I think the caffeine's the only thing holding his veins open.  You've got to help it out.  Help a kid out.  You could buy a copy of SpinRite, or you could turn the page.  Don't turn the page.  You could also find audio and video.  Oh, you really don't want to see the video of this.  You can get video of this fine show, really riveting video.  The only thing I can say about the video that's good about it is, because very little is moving in the shot, basically Steve's lips and those blinky lights behind him, the compression is excellent.



STEVE:  Fabulous-looking, yeah.



LEO:  Fabulous and highly compressed video available at TWiT.tv/sn.  You can also subscribe and watch at your heart's content on your favorite device.  TWiT.tv/sn.  And if you want to watch live, do it every, well, best to come by around 1:30.  We usually get to Steve a little bit later, but around 1:30 p.m. Pacific on Tuesday afternoons.  There's another one.  Did you hear that?



STEVE:  That was a yabba dabba doo.  Somebody's...



LEO:  Somebody yabba dabba doo'd you.



STEVE:  Thank you very much, listener.  Thank you.



LEO:  That's somebody listening live right now and said, "I'm going to do that.  That's a good idea."



STEVE:  Thanks.



LEO:  Let's make more yabba dabba doos happen.  Sometime between 1:30 and, oh, 4:10 Pacific, that's 4:30 Eastern time.  That's 21:30 UTC.  You can come by, watch the show, be in the chatroom at irc.twit.tv.  Yeah, if you shut off the blinky lights, the file size will go down another 20%.  Bill's pointing that out.  I think he's got a good point.  That's really - that's killing the key frame there.  Killing it.  We will see you next time, Mr. Steve Gibson, on Security Now!.



STEVE:  Thank you, my friend.  Next week.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#638

DATE:		November 21, 2017

TITLE:		Quad Nine

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-638.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss Windows having a birthday, Net Neutrality about to succumb to big business despite a valiant battle, Intel's response to the horrifying JTAG over USB discovery, another surprising AWS public bucket discovery, Android phones caught sending position data when all permissions are denied, many websites found to be watching their visitors' actions, more Infineon ID card upset, the return of BlueBorne, a new arrival to our "Well, THAT didn't take long" department, speedy news for Firefox 57, some miscellany, listener feedback, and a look at the very appealing and speedy new "Quad 9" alternative DNS service.



SHOW TEASE:  It's time for Security Now!.  Yay!  Steve Gibson is here, our Turkey Day edition, and there are a few turkeys to talk about, including the Uber breach.  We'll talk about a new DNS service that Steve's going to switch to right away.  And of course our Picture of the Week, exactly why you have to be careful if you're an IT professional where you go to work.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 638, recorded Tuesday, November 21st, 2017:  Quad 9.



It's time for Security Now!, the show where we get together with Steve Gibson, our security officer - CSO, Chief Security Officer - to learn about what's going on on the worldwide - what the heck is it?  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  Good to see you.



STEVE:  Great to be with you again for our 638th episode.



LEO:  Wow.



STEVE:  Had we begun numbering from zero, of course, 638 would be the 639th episode.  But we're unable to go back and change time.



LEO:  Is that the first thing you'll do if you get a time machine? 



STEVE:  Yeah, we really should have renumbered this.  We should have.  Negative one, that'd be a fun start.



LEO:  Oh, yeah.



STEVE:  Especially since it was about the Honey Monkeys.  So that one could have been the negative one episode.  But no.  So this is the week of a Windows birthday, which we're going to talk about.  Not a big birthday, but it was yesterday, so I thought, oh, that's kind of fun.  The RTM of Windows 1.0 happened on November 20th.  That was yesterday since this is the 21st that we're recording this.  So there's that.



And I heard you talking on MacBreak Weekly about Net Neutrality, so we just - we'll spend much less time on the topic than you guys all did.  But I just wanted to bring up this disturbing announcement that will come to pass about a little over three weeks from now, on I think it was December - was it the 21st?  No, I don't remember.  Anyway - oh, the 14th, I think.  Anyway, that's happening.



We've got Intel's response to the horrifying JTAG over USB discovery that we talked about last week.  Another surprising AWS, that's Amazon Web Services, public bucket discovery.  Android phones caught sending positioning data, even when all permissions to do so are denied.  A surprising number of websites, sort of a disturbing number, found to be watching their visitors' actions, even without submitting any information to the website.  More fallout from the Infineon ID card bad public key synthesis problem.  The return of BlueBorne that we discussed in early September.  A new arrival to our, "Well, that didn't take long" department.  And unfortunately this is regarding the Amazon key service that we just talked about.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  That didn't take long.  We've got speedy news for Firefox 57; a bit of miscellany; some listener feedback.  And then, because this episode is titled "Quad 9," we're going to talk about a very appealing and very speedy privacy and security-enhancing DNS service known as Quad 9, and you can guess the IP because...



LEO:  Quad 9.



STEVE:  It's in the name, yeah.  So I think an interesting podcast for all of our listeners.  Not surprisingly, because the industry keeps giving us good things to talk about.



LEO:  There's plenty of juice in this lemon, yes, Steve.



STEVE:  Yeah, we're never running out.



LEO:  Yeah.  Okay, Steve.  I've got a Picture of the Week right in front of me, right now.



STEVE:  Yes, we do.  And I love it.  It's someone sitting in front of the Human Resource guy's desk, and he is speaking.  And the caption says - this is the HR guy:  "We couldn't hire the cybersecurity candidate you sent us.  He was saying too many scary things about our computers."  It's like, yeah, right.



LEO:  That's like - who was it that they're going after, they're threatening to go after the guy who revealed that they were revealing information about their - oh, DJI, the Phantom...



STEVE:  Yes, the Chinese Phantom manufacturer.



LEO:  It's like, yeah, oh, yeah, go after that guy, the guy who told you what you're doing wrong.  Good idea.



STEVE:  Yes.  After he said, "Does penetrating your servers qualify under your bug bounty," and they said yeah.



LEO:  Oh.  They even gave him a go-ahead.  Wow, wow.



STEVE:  Yes, exactly.  And then they decided, oh, we're not happy with you anymore.  So, yeah.  I mean, it is a dicey relationship.  Anyway, I love that we wanted to hire the cybersecurity guy, but he really scared us about our computers, so we said no, no.



LEO:  Just put your head in the sand, yeah.



STEVE:  We want someone who just - just bring ice cream, please.  We don't want anything scary.  That's right.



LEO:  No troubling news, no.



STEVE:  That's right.  So anyway, yesterday was the 32nd birthday of Windows.  The v1.0 of Microsoft Windows was - it went RTM, release to manufacturing, as we used to call it back then.  Now it's kind of rolling disclosure.  Back once upon a time when they actually said, okay, we're done, that was RTM.  And that was 1985.



LEO:  Wow.



STEVE:  So 32 years ago and a day, 32 years yesterday, Windows turned 32.  Then, okay, so that was November 20th, '85.  December 9th, '87, so a little more than two years later, two years and kind of half a month, was Windows 2.0.  So that's December 9th, '87.  Then May 22nd, 1990, that was the big one.  That was Windows 3 where they finally, like, okay, they kind of - they were beginning to get this right.  Remember?  And everyone remembers Windows 3.11.  That was like, everybody was happy.  We had memory management.  We were able to access more than 640MB.  You could do more than one thing at a time.  It was like, okay, this is kind of usable.  I mean, our screens were still freezing, and it was crashing constantly.  You had to always be saving all your work, otherwise it would all be lost.  But those were the fun pioneering days.



LEO:  Yeah, yeah.



STEVE:  Yeah.



LEO:  Very much remember that.  We've come a long way.



STEVE:  We really have, yes.  It kind of works now, pretty much.



LEO:  Amazingly.



STEVE:  After 32 years, yeah.  So I did want to mention just briefly - you guys covered it extensively, and I'm sure you'll be talking about it with Jeff and Stacey tomorrow on This Week in Google.  Today, this morning, FCC's Chairman Ajit Pai, who has made the reversal of the previous administration's Net Neutrality protections one of his top priorities, unveiled the FCC's plan to give Internet providers broad powers to determine what websites and online services their customers can see and use, and at what cost.  So in other words, the end of federal government-enforced Net Neutrality.



So that's the announcement.  The final decision, which will be put to a vote next month on Thursday, December 14th, just over three weeks from now, is expected to pass if the votes fall along political party lines, as they're expected to, since Ajit Pai's Republican Party holds three of the Commission's five seats, with two Democrats in the other two.  So if the vote falls 3-2, that passes, and then the FCC has done what they said they were going to do in the run-up.



And again, I don't want to beat this to a pulp, but this essentially allows the ISPs to not be treated like a common carrier with a hands-off policy, but, if they choose to, to throttle traffic to enforce different sorts of plans, depending upon what it is you're downloading.  In other words, arguably at the moment we're paying our ISP for our connection to the Internet; and the presumption is, and what has always been the case, you have access to the entire world equally.  And that is a luxury that may be changing, sadly.



LEO:  Yeah.  Terrible.



STEVE:  And, you know, it's a problem, too, because, I mean, if you think, okay, well, what if in another two and a half years, or maybe three and a half years, oh, I guess about three years, we switch back to a Democratic executive.  And if there are lots of seats moved back in that direction in the House and the Senate, and all this changes again.  I mean, the problem is we're seeing a lot of this changing of policy based on who's in Washington, which is very expensive for businesses who need to have some sort of planning horizon that allow them to do what they want to do, regardless of which side they fall down on this.  So I don't know.  And then you guys were talking, and I've not followed what happens in localities.  That is, for example, California is tending to be rather activist on the Democratic side.  Well, can California enforce some change?



LEO:  No.  That was among the other things they're proposing with this is that the states - this supersedes all state and local laws.  Which is appalling, frankly.  These are the big "states rights" people.  Oh, but only when we care.



STEVE:  I was just going to say that, unfortunately, the Republicans have been all into this federation and not having all the power concentrated in Washington.



LEO:  Yeah.  No, that's not true.



STEVE:  Except where they want it to be.



LEO:  Only where, yes, only it comes with a few social issues.



STEVE:  Okay.  Well, so that we don't descend into politics, let's talk about - and this is - the good news is there's some takeaways for our listeners, and I love it when listeners can do something.  This is Intel's response to what we discussed last week, which is this horrific JTAG, I mean, it's just hard to even say the phrase "JTAG over USB."



We explained last week that what JTAG is, it's this universally supported, universally recognized, essentially serial debugging interface which, for embedded systems and Intel's IME, their management engine, is an embedded system by definition.  What is going to be disclosed in more detail next month by the guys at Positive Technologies, the research firm who found this, is in more detail what this is.



And what's really interesting to me is was it a bug that allowed JTAG over USB?  I don't understand how that can be.  But maybe it was somehow the USB interface is abused to get access to JTAG.  But it sounds more like a feature.  Like, oh, yeah, we intended to export the JTAG serial debugging interface over the serial USB interface to make it easier, so you didn't have to open the box in order to access pins on the chip.  It's like, okay.



But anyway, what has been revealed - and to their credit Intel scrambled in order to respond, although we also got a bunch of corporate speak.  So Intel in their announcement of this, or sort of acknowledging announcement, says:  "In response to issues identified by external researchers, Intel" - they, Intel - "has performed an in-depth comprehensive security review of our Intel Management Engine, Intel Server Platform Services, and Intel Trusted Execution Engine, with the objective of enhancing firmware resilience."  And of course I'm thinking, wouldn't it have been nice if they'd done that in the beginning, rather than, like, now.



"As a result," they write, "Intel has identified" - Intel, right - "Intel has identified security vulnerabilities that could potentially place impacted platforms at risk."  Uh-huh.  So it turns out in their disclosure we're talking nine years of chipsets.  That is, going back nine years the earlier chips are affected.  So this is the ME firmware versions from 11.0, .5, .6, .7, .10, .20; SPS firmware 4.0; and TXE - that's the Trusted Execution Engine - 3.0 are impacted.



And then they said:  "Based on the items identified through the comprehensive security review..." - which the disclosure, the public disclosure of these forced them to apparently do - they write:  "...an attacker could gain unauthorized access to platform, Intel ME feature, and third-party secrets protected by the Intel Management Engine, Server Platform Service, and Trusted Execution Engine.  This includes scenarios where a successful attacker could impersonate the ME/SPS/TXE, thereby impacting local security feature attestation validity; load and execute arbitrary code outside the visibility of the user and operating system" - in other words, as we know, down in the Ring -3 zone where nothing exists except this environment, which is then able, but which despite the fact that it's so low has access to everything going on above - "cause a system crash or system instability."  And then they say:  "For more information, please see this Intel Support article."



Now, they have in response to this fixed the problems.  Unfortunately, you don't get Intel firmware from Intel.  Your motherboard manufacturer, or laptop or desktop, whatever, manufacturer, they have the relationship with Intel.  We as the customers have the relationship with them.  So at this point only Lenovo is listed on Intel's documents and has very quickly provided comprehensive coverage.  And Leo, there's a link in the show notes, and to our listeners, there's a link in the show notes that is very extensive, support.lenovo.com, which a firmware update listing by model number for laptops, for desktops, I mean, for everything.  And for example, my Carbon X1 Lenovo is there.



But the other cool thing, and this is, as I was mentioning, there's a takeaway for our listeners, is that Intel has published a detection tool.  Yay.  The link is in the show notes.  I can say it because it's not too gnarly, so it's downloadcenter.intel.com/download/27150.  So once again, https://downloadcenter.intel.com/download/27150.  It is available both for Windows and Linux.  And there's a command line version and a graphical user interface version.  Grab the one you want.



And this is very nice.  This will allow you to determine whether the system you're running it on has these problems and therefore needs its firmware updated.  And so understand these are serious motherboard firmware, both remote over the network and local, so this is fixing many things, only one of which - I think it's 5705 was the number.  That's the CSV assigned.  Only one is the USB JTAG bug.  I can't wait to find out next month like what's the nature of this.  Did they discover JTAG on USB?  Did they abuse something in order to get it?  Did Intel mean to publish JTAG out of the USB ports?  I just - this is really interesting to me because it's so crazy to do that.  Or is it fine, do they think, as long as there are no bugs?  I just don't know.



So there are still a bunch of questions.  But in the meantime, certainly for enterprises who have a large number of identical systems, as we were talking about last week, there is now a detection tool.  So thank you, Intel, for that.  And unfortunately it will be then up to the producers of the retail versions of these systems, like Lenovo that's using Intel chipsets, like Dell, well, like everybody essentially, to arrange to provide the similar patches as Lenovo has that people can run on their machines in order to fix them.



LEO:  It's interesting because I got an update to the firmware, the ME Firmware 11.8 on November 1st.  But I just got an update to that, 11.8 with a more recent number, 11.8.50.3425.  I'm guessing that this is the fix.  But I'm going to run the install.  I'm showing it right now on the screen.  And then I'll run the Intel.  The Intel program will tell us whether we've been patched or just whether...



STEVE:  Correct.



LEO:  Yeah, okay.



STEVE:  Yes.  It will tell you whether you are running a version of firmware which they know to be vulnerable.



LEO:  Okay.  Okay.



STEVE:  And so it'll say yes, you're current, and you're not susceptible.  So is that on a Lenovo?



LEO:  That's the X1 Yoga, second generation.



STEVE:  Nice, yes.



LEO:  Which was marked as vulnerable. 



STEVE:  Yup.  Good.  Let us know.



LEO:  That's one of the reasons I buy ThinkPads.  I know you love your X1 Carbon.



STEVE:  I do.



LEO:  I really love these laptops.



STEVE:  Yup.  And I agree with you, too, that the keyboard really feels nice.



LEO:  No, it's a good laptop, yeah.



STEVE:  They've not sacrificed it the way Apple unfortunately has in order to make it incredibly thin.  It's like, okay, look.



LEO:  I like the X1 Carbon, but I wanted the two-in-one screen, and I wanted the OLED screen.  I really splurged on a new OLED screen.  But, boy, it sure looks good.  I use it for photography so it's okay.



STEVE:  So we have another case of an exposed Amazon Web Services bucket.  Well, okay, three buckets, actually.  What?



LEO:  Well, just, you know, just another case, yeah.



STEVE:  I know.  I know.



LEO:  The last one was a government bucket, wasn't it?



STEVE:  Well, this one is.  So it's Chris Vickery of UpGuard.  He's the guy who has recently been discovering these open and publicly accessible AWS shares.  He's found three buckets belonging to the, get this, the U.S. Department of Defense, the DOD.



LEO:  Yeah, nice, mm-hmm.  



STEVE:  Yeah.  Both CENTCOM and PACOM, which are intelligence-gathering operations.  The three S3 buckets were configured to allow anyone with an Amazon Web Services account, which of course anyone can get, to access them, and were labeled "centcom-backup" - okay, you don't have to guess much what that is.  Lord - "centcom-archive," and "pacom-archive."  In response to this, CENTCOM responded in their typical bureau speak, saying, "We determined that the data was accessed via unauthorized means by employing methods to circumvent security protocols..."



LEO:  Yeah, the browser.



STEVE:  Exactly, yeah, "...said Major Josh Jacques, a spokesperson for U.S. Central Command.  Once alerted to the unauthorized access, CENTCOM implemented additional security measures to prevent unauthorized access."  Translation:  We decided not to leave the password field blank.  So, my lord,  yeah.  So UpGuard posted.  They said:  "The data exposed in one of the three buckets is estimated" - and get this - "to contain at least 1.8 billion posts of scraped Internet content over the past eight years, including content captured from news sites, comment sections, web forums, and social media sites such as Facebook, featuring multiple languages and originating from countries around the world.  Among those are many apparently benign public Internet and social media posts by Americans, collected in an apparent Pentagon intelligence-gathering operation, which raises questions of privacy and civil liberties."



LEO:  I think it just found their reddit porn folder and...



STEVE:  "While a cursory," UpGuard writes, "while a cursory examination of the data reveals loose correlations of some of the scraped data to regional U.S. security concerns, such as with posts concerning Iraqi and Pakistani politics, the apparently benign nature of the vast number of captured global posts, as well as the origination of many of them from within the U.S., raises serious concerns about the extent and legality of known Pentagon surveillance against U.S. citizens.  In addition, it remains unclear why and for what reasons the data was accumulated, presenting the overwhelming likelihood that the majority of posts captured originate from law-abiding citizens across the world."



So, whoops.  Caught with their pants down because they decided not to put a password on their 1.8 billion archive named "centcom-archive," thus not even a disguised name.  Wow.



LEO:  Obviously they collected it, and they just left it there, and they forgot about it.  



STEVE:  Yeah.  Yeah, although you have to wonder, like maybe it got collected, and then they transferred it to a bucket to make it available to other people, to sort of - because you would hope that they're not collecting it in an Amazon cloud bucket.  It's like, okay, what?  That's where I keep this podcast backed up.  But I don't - you don't want the USDOD to be archiving its Internet scrapings there.



LEO:  But do we care that they're scraping in Internet posts?



STEVE:  Nah, not really.



LEO:  I guess they're public posts.



STEVE:  It's public, yeah, exactly.  Okay.  So Android was discovered to be, all of this year, starting in early January, to be sending tracking information back to Google without the user's permission.  This was from some research that Quartz did.  Throughout all of 2017, Android devices, with all location tracking permissions disabled and even without a carrier SIM card installed - so you'd kind of think, okay, I'm off the grid - they've been sending, it turns out, the IDs of all cell towers within range back to Google for some kind of analysis.  Which Google acknowledged, after the guys at Quartz observed the behavior and said, "What's going on with this, Google?" when all location tracking has been disabled.



So in their reporting, Quartz wrote:  "The cell tower addresses have been included in information sent to the system Google uses to manage push notifications and messages on Android phones for the past 11 months, which was acknowledged by a Google spokesperson."  Google said they were never used or stored, and the company is now taking steps to end the practice after being contacted by Quartz.  By the end of November, this November, this month, so like a week and a half, the company said, Google said:  "Android phones will no longer send cell tower location data to Google, at least as part of this particular service, which consumers cannot disable."  



Google explained to Quartz by email:  "In January of this year we [Google] began looking into using Cell ID codes as an additional signal to further improve the speed and performance of message delivery.  However, we never incorporated Cell ID into our network sync system, so that data was immediately discarded, and we updated it to no longer request Cell ID."



So I guess my feeling is, okay, not a big deal.  There was some panting and huffing and puffing on the Internet about this.  It's like, eh.  I think there's a worthwhile debate about what truly is anyone's reasonable expectation of privacy when you're walking around with one of these deeply Internet-connected computers keeping your pocket warm.  My own feeling is that anyone carrying a cell phone should have a very low expectation of privacy.  By its very nature, it's a heavily Internet-connected computer that its user, its own user, did not design and build.  So it could be doing anything.  And we keep finding that indeed it is doing anything.



So if you don't wish to be tracked and monitored, leave the phone at home.  It's just not realistic, I think, to imagine that it's possible to have it both ways.  You've got a machine that you want all kinds of incredible features from.  And take the battery out.  That would probably do it, like the way they do in the movies.



LEO:  I don't think Google has yet figured out how to keep it going with the battery out.



STEVE:  No.



LEO:  Pretty sure they don't have that.



STEVE:  And unfortunately, we can't take the batteries out, actually.



LEO:  Oh, then never mind.  Oh.  Now the conspiracy theories are really going crazy.



STEVE:  And the question is, when you shut it completely off, is it really off?



LEO:  Uh-huh.  Now the new Apple Macintosh apparently has an ARM chip that will continue to run even if the Intel chip is powered down.



STEVE:  Yeah, I just think, you know, we old fogies actually care about such things, but, yeah.



LEO:  Nobody else does, yeah.



STEVE:  Nobody else does.



LEO:  Just the people who listen to this show.



STEVE:  And speaking of old fogies caring about stuff...



LEO:  The name of this - that's the subtext, subtitle of this show.



STEVE:  That's a great, yes, that is old fogies...



LEO:  Two old fogies caring about stuff.



STEVE:  ...still caring, still giving a you know what, yes.  We've talked about this before, but there's some interesting new research out of Princeton.  Websites are logging keystrokes and mouse movements in real-time, whether or not their users ever click Submit.  This has been named by Princeton, the researchers there, as "Session Replay Scripts."  So researchers at Princeton's Center for Information Technology Policy have a cool site.  We've talked about it from time to time through the years.  Freedom to Tinker is their - it's freedom-to-tinker, with the words separated by hyphens, so freedom-to-tinker.com, where they blog about stuff that they've found.  They call this their "no boundaries exfiltration of personal data by session replay scripts."



Last week they posted the result of their analysis of websites that are using JavaScript to monitor their visitors' actions and activities while people are present on the site, meaning that you just go to a website, and stuff starts happening before you press any buttons.  And we've talked about this sort of behavior in the past, how once upon a time in the quaint old days when Windows was young, which is now 32 years ago, before a website, before contemporary websites could run code, powerful code in the web browsers of everyone who visited, what used to be the case is a static page was delivered, and it just sat there.  And it was only when we went to another page on that site, clicking a link, or when we explicitly and deliberately filled out a form and then submitted it to the site, that it saw anything further from us.



But today's websites are not going to be content with that.  They want to know what is going on at the other end to every degree possible.  So we've talked about how even the position of the user's mouse can be monitored.  We talked about it in the context of the latest version of Google's CAPTCHA, the I'm Not a Robot, where all you have to do is just click on the box, making that declaration.  Yeah, I'm not a robot.  And the point is that they use a number of signals, one being watching your mouse as it moves, as it kind of zeroes in on the checkbox, deciding if you're a computer calculating an arc, or if you seem human based on the way you moved the mouse.



The point is so there's sort of a, you could argue, kind of a benign use case for constant mouse position monitoring.  And in fact over on the keystroke side I'm using a web page's ability to dynamically watch your keystrokes on GRC's Password Haystacks page because, if you think about it, when you go to the Password Haystacks, as you're entering test, as you're playing with it, putting in passwords, with every single keystroke I'm recomputing and showing you the size of the alphabet, the how long it would take to brute force the password.  That's all being done with client-side script, which is being awakened every time the user hits a key to capture now the password in that field and then recompute everything.



So again, a nice, benign, friendly, user-facing benefit to being able to do this.  But we now also have a technology known as WebSockets, which allows code running on a web page to silently initiate a connection back to the mothership, or in this case to the third-party provider of the JavaScript which is running on the page, and to dynamically send to stream in real-time everything that the page's user is doing without them having any awareness of it, having given permission or anything.  I mean, the presumption is you go to a site, you're sort of turning yourself over to the site more and more as we move through the years with ads and videos playing.  By the way, Ars Technica has started to do this a lot, and it's really annoying me.



LEO:  I pay for a subscription, and I think that that helps.



STEVE:  Okay.  Because, I mean, I'm looking at 47MB if I scroll into - it's like, holy crap, really?  To show me a video that I don't want?  Just really...  



LEO:  Really, that's way too prevalent now.



STEVE:  Yeah.  So, okay.  So the researchers at Princeton have named this practice "session reply" - or I'm sorry, "replay scripts," I have a typo in my show notes - "since the stream can be replayed at the hosting end to recreate the user's actions on the page."  For example, did they scroll?  So like the scroll position of the page is being streamed back.  Where is the mouse?  How far down did they maybe read?  I mean, you can see how these could be useful analytics.



The researchers wrote:  "The stated purpose of this data collection includes gathering insights into how users interact with websites and discovering broken or confusing pages.  However, the extent of data collected by these services" - now, that's the other thing.  This is not just someone like Ars, to pick on them again.  I love Ars, so I don't mean to be picking on them.  But, for example, these are typically not their own website coders doing this.  Naturally there are third-party services where it just, oh, yeah, just like Coin Hive.  Drop this little line of code on your page, and we'll send you all this really cool analytics stuff.  So there are third-party services which are collecting data which far exceeds probably the typical user's expectations.



Text typed into forms, which is being collected before the user submits the form so that, for example, if the user decides, whoa, wait a minute, I didn't know I was going to be asked all this, and this is too intrusive, doesn't matter.  What they've typed in has already left the browser.  It's already been streamed back as they're entering keystrokes.  The mouse movements are saved, which seems a little less invasive, but without any visual indication to the user.  And some companies allow publishers to explicitly link the recordings to a user's real identity, so they're within this ecosystem.  All of this is being deanonymized.



They write:  "For this study they analyzed seven of the top session replay companies" - there are seven companies that are the top ones, and I saw an eighth one mentioned in the comments to the story - "based on their relative popularity in their measurements.  The services studied are Yandex, FullStory, Hotjar, UserReplay, Smartlook, Clicktale, and SessionCam."  So these are services available to websites.  They wrote:  "We found these services in use on 482 of the Alexa top 50,000."  And these are not obscure.  In the show notes under webtransparency.cs.princeton.edu is their raw data.



From the top of the list down we have WordPress, Microsoft, Adobe, GoDaddy, Outbrain, Spotify, Skype, and RT.  So some very well-known, non-obscure sites are employing these third-party services to provide dynamic user monitoring metrics about how their pages are used.  Again, not super invasive, although I wanted to mention it because I know that our users care about these kinds of things.  And this is not behavior which you would expect or is obvious when you go to a site.  And of course all brought to us now by JavaScript, which is you just can't use the 'Net without it any longer because so many sites are using it to enhance their functionality.  So as everyone knows, I gave up on NoScript, and I switched to uBlock Origin to give us some control.



Oh, and speaking of which, after this reporting came out, the EasyList service was updated to include all these domains; and Gorhill's uBlock Origin, which is already pulling from EasyList, will automatically protect anyone using uBlock Origin from these third-party analytics systems.  So anyone using EasyList - and I think Adblock Plus also uses EasyList.  But uBlock Origin I know does, I checked as I was pulling all this together, already blocks this.  So again, another reason to consider maybe using a third-party manager to get some additional privacy enhancement.



So we've many times discussed the fallout, the really amazing fallout from the Infineon crypto library, which was found - which is used in embedded devices and found to be producing factorable private keys, which should be an oxymoron.  That's, I mean, the whole point is that you cannot factor a private key, which is the way this cool technology hides the public key within the private key, by multiplying these two primes, and you don't know how to pull them apart again.



So Estonia, which was very forward looking, we talked about a couple weeks ago, had "only" issued 760,000 Infineon-based identity cards.  Turns out to be one of the smaller players.  Get this:  Spain has issued 60 million of these cards, 60 million now completely useless and vulnerable identity smartcards.  And remember that here's the problem is that they've issued them, and they weren't very popular.  I think I saw 0.02% of them were getting any use.  But the problem with something like this is that, once they cannot be trusted, no one can use them.  Even though they're in very low common usage, now that any of them can be attacked, because that's what this means, the private key generated by the cards can be factored.



And the price to do that, as we have also since discussed, keeps coming down.  Now it's about two grand and a few hours in order to perform this factorization.  That means that, if any card can be attacked, then none of the security assertions made by any card can be trusted.  Therefore all 60 million are useless for their intended purpose, which in any given instance is that this card is producing an assertion that cannot be spoofed, cannot be later either created or broken.  And we now know that's no longer the case.  So, yes, 60 million for Spain.



And, boy, I don't know what kind of pain Infineon is in, what the contracts look like and so forth.  But this has been a big disaster.  And if nothing else we need better oversight.  The problem that we have is that, for example, Spain purchased 60 million cards without requiring a third-party security review of the technology.  Infineon would have balked, but so Spain could say, you want the contract or not?  You're asking us to pay you a chunk of money.  We're happy to do that.  We want your product.  But we're not taking your word for it.  We need you to put some academic researcher under NDA and examine your technology and verify independently that we should trust you.  That's the kind of thing that isn't happening yet because companies are still allowed to say, "Trust us, we're Infineon."  And now this is what happens.



About 10 weeks ago, 10 episodes ago, in early September, we covered the BlueBorne attacks.  Our listeners with a good memory will remember that there were a series of eight newly revealed zero-day vulnerabilities which affected virtually all Bluetooth-equipped devices, regardless of OS, both Windows and Linux and IoT devices, embedded OSes.  Android was affected.  It was big.  Smartphones, laptops, TVs, IoT devices, you name it.



And these attacks which were enabled by these eight new zero-day vulnerabilities were extra potent, more so than usual, because no user interaction was required.  No Bluetooth association was required.  All that was required is that a malicious device leveraging the BlueBorne attacks would get within radio range, which is typically 10 meters, about 30 feet, of a device with a Bluetooth radio on.  And that would allow, without any involvement from the target device's user, remote compromise.



So there was responsible disclosure.  Lots of people scrambled around and updated their Bluetooth stacks in order to fix these eight newly discovered problems.  Well, it turns out that another very popular pair of devices were also vulnerable.  And that's Amazon's Echo and Google's Home, apparently numbering about 20 million, 15 for Amazon, 5 million for Google.  The IoT security firm Armis, who was the original discoverer of the BlueBorne attacks, has disclosed that an additional estimated 20 million Amazon Echo and Google Home devices are also vulnerable to the attacks.  They responsibly disclosed their discovery that the Echo and the Home were also vulnerable.  Amazon and Google have both implemented and issued patches so that this news is only coming out after the fact.



And the good news is these devices are updating themselves.  So it's unlike having an old Android phone where you're out in the dark at this point.  There were two different CVE vulnerabilities affecting the Amazon, and one affecting the Google device.  And there was also a denial-of-service attack.  Basically it could just crash your Google Home, not that that would do the attacker much good.  But in these devices the Bluetooth radios cannot be disabled.  That was our advice 10 weeks ago.  Until you were sure that you had an update, just turn off Bluetooth if you were likely to be targeted.



I mean, it wasn't - because it required physical proximity, some bad guy had to be within 30 feet of you, which seemed unlikely, or much less than an Internet-based attack that could be executed from anywhere else on the globe.  They did responsibly notify, and the patches are out.  So the problem has been dodged.  But this does further reinforce one of the lessons we keep coming back to here, which is the absolute necessity of all interconnected devices, whether they be thousand-dollar smartphones or $7 light bulbs, to be remotely updatable.



We talked a couple weeks ago that there is a forthcoming, hopefully to be adopted, RFC describing a means for doing this, basically moving old-school big-iron Internet technology, the idea of having multiple firmware images from which you can boot either, to allow a device to be updated and then rebooted from the other image in order to bootstrap itself into improved code.  But, boy, if anything is smart enough to have firmware, it needs to be smart enough to have that firmware updated.  I think that's probably the perfect way of saying it.  If anything is smart enough to contain firmware, that firmware needs to be smart enough to update itself, to arrange to keep itself current because, if it's smart and has firmware, what we're seeing is it can be abused.



And from the, as I said at the top of the show, "Well, that didn't take long" department, boy.  We discussed with some skepticism on my part, I will say, a week or two ago, the release of Amazon's great new feature, Amazon Key, where as our listeners will remember a special Amazon camera would be positioned to look at the foyer, the entryway of a person's home, where there would be the back - it would be seeing the inside of the front door and the inside area.  You would also have an Internet-enabled smart lock on the door that Amazon is able to control.  So the idea would be this solves the problem which, Leo, you were explaining.  I don't have the problem, but I guess it is a big problem in some areas, of Amazon wanting to deliver packages, and it just being unsafe from a security standpoint.  Packages are disappearing, apparently, from people's front porch.



LEO:  It's a big problem around here, yeah.



STEVE:  Okay.



LEO:  And, by the way, another problem, which frankly doesn't encourage me, remember we said it was only in areas where Amazon Logistics did the delivery.



STEVE:  Ah, right.



LEO:  Well, I've learned that Amazon Logistics often comes in unmarked cars by a person - they basically do it like Uber.  So now you're letting somebody just who drives up your driveway in an unmarked car, walk in your house.  



STEVE:  Yup.  Yup.  And in fact I got such a delivery last night.  They couldn't - this is a person who didn't know where I was.  And I got a phone call from area code 209.  I thought, what the heck?  You know?  And I just ignored it the first time.  Then it called again, and I thought, okay, well, that's unusual.  So I picked it up, and it was a person saying, "Where's your house?"  I said, "Who are you?"



LEO:  Why do you want to know, buddy?



STEVE:  Yeah.  Anyway, so I said, "Okay, I'll come out."  And so I came out, there was just a big white van and a person who maybe was doing this to earn some extra cash.



LEO:  They do it like Uber.  They post them, and you take up delivery jobs just like Uber.



STEVE:  Wow.



LEO:  And so that worries me quite a bit.  And those are billed as Amazon Logistics.  I don't know if that means that's who's going to have access to your house or not.



STEVE:  Well, so get this.  So just to finish the scenario, you've got the camera looking at the inside of your front door.  With this system, when you permit this to happen, the delivery person is able to push a button on their little handy keypad thing, or maybe on their own smartphone, probably just uses their own smartphone.  The camera starts streaming.  Your front door is then unlocked, allowing them under video monitoring to open the door, slide the packages in, wave hi to the camera, and then close the door.  Then they press a button saying, okay, delivery completed, and the door relocks.  And the idea being then that the video that was captured of this process is then sent to the owner of the home, I think a few seconds afterwards, along with a message saying your packages are safely inside your locked front door.



So the "Well, that didn't take long" is Rhino Labs discovered that a courier equipped with a simple program could use their laptop, and it'll be turned into an app before you know it, to fake a command from the house's WiFi router to cause the Amazon Cloud Cam to be disconnected from the WiFi network, which causes the camera to stop functioning and freezes the image at the last frame.  So it's, I mean, you couldn't make this up.  This is like a movie.  It's literally the camera stops refreshing the image, so it shows no motion, no door opening, no one sneaking in the house.



So the point is that at this point the courier could reenter the house, and I guess they've already done this once.  I don't know why it says "reenter."  But the courier reenters, does whatever they wish with no monitoring, with no surveillance, then exits and reactivates the camera and locks the door as usual.  "This reentry," they write, "would be undetectable by the resident and would appear as a normal delivery in Amazon's data."



So apparently this bug that they found, this glitch, essentially allows the first delivery to occur, and apparently all goes well, but then to immediately reopen the door with the frozen camera and sneak around.  And so while, yes, there would be some connection between some mischief and the delivery, if they did something like ran upstairs and stole some jewelry out of a jewelry case or something and then got back out of the house quickly...



LEO:  You'd never know.



STEVE:  Exactly.  You would never know.  If it was a few days later that you discovered, or a month or a week or something, it's like, "Wait a minute.  Where are those earrings?"  You have no way of associating those events.  So Amazon said, their response to this was:  "We currently notify customers if the camera is offline for an extended period."  They said:  "Later this week" - meaning this week - "we will deploy an update to more quickly provide notifications if the camera goes offline during delivery."  So that's good.



However, the guys at Rhino Lab explaining, who know this in somewhat more detail, said while this could help Amazon Key - which is the name of the service - customers know when something is amiss, it doesn't prevent the event from happening.  Ben Caudill, who's Rhino Labs founder, told Wired Magazine for their reporting of this that the only way to fully close the loophole would be to cache video locally, even when the camera is disconnected from the network.  However, the Cloud Cam doesn't currently cache video locally, and doing so would require significantly more local storage than for the pure streaming which is all it does.  So to robustly fix this, you would need a hardware upgrade, like maybe if it hasn't been 30 days considering returning the Cloud Cam.  Do you remember how expensive they are?



LEO:  It was several hundred dollars for the setup because you need the Cloud Cam and the special slick lock.



STEVE:  Right, right, right.



LEO:  You need the whole setup.  And I think it was $399 or $299.  It was not cheap.



STEVE:  Ooh, boy.



LEO:  To me, that was enough deterrent.



STEVE:  Yeah, just the cost.



LEO:  Is the cost.  Get a Ring Video Doorbell, they'll know.



STEVE:  This is you, Leo, we're talking about.



LEO:  Yeah.  I'm not letting these guys in.



STEVE:  No.



LEO:  Plain, unmarked car?  Come on.



STEVE:  Oh, yeah.  What could possible go wrong?



LEO:  Geez.



STEVE:  So I don't have any personal knowledge of this yet because Firefox 57 doesn't look like it runs on XP.  And I'm still sitting in front of XP, although I've got Windows 7, and I've got 10.  I mean, I have other Windows machines all over the place.  I only have one XP machine.  But it's going to be so much down time for me to switch that I just haven't wanted to yet.  I haven't needed to.  But the news is Firefox 57, which is the so-called "quantum" release, has apparently been clocking in at two times the speed that it used to be.  



LEO:  Yup.  I'm using it now.  I love it.



STEVE:  Oh, nice.  The Hacker News guy said it is time to give Firefox another chance.



LEO:  That's exactly what I was thinking.  I did.



STEVE:  Wow.



LEO:  Yeah, I'm a big Chrome user.  And I know you love Firefox.



STEVE:  I do.



LEO:  But this new Quantum is great.  It's beautiful.



STEVE:  Yeah.  So the Hacker News wrote:  "The Mozilla Foundation today announced the release of its much-awaited Firefox 57, a.k.a. Quantum web browser for Windows, Mac, and Linux, which claims to defeat Google's Chrome."  They wrote:  "It is fast.  Really fast.  Firefox 57 is based on an entirely revamped design and overhauled core that includes a brand new next-generation CSS engine written in Mozilla's Rust programming language, called Stylo," that is, the CSS engine is called Stylo.



"Firefox 57 Quantum is the first web browser to utilize the full power of multicore processors and offers 2X times faster browsing experience while consuming 30% less memory than Chrome.  Besides fast performance, Firefox Quantum, which Mozilla calls 'by far the biggest update since Firefox 1.0 in 2004' [so 13 years ago], also brings massive performance improvements with tab prioritization and significant visual changes."  Now I'm super excited to try this, Leo.



LEO:  It looks like Edge, which is weird.  But it's a very clean UI.



STEVE:  Nice, nice.  I will be using it tonight because I have Win7 on my Carbon X1.  "With a completely redesigned," they write, "UI called Photon.  This new version also adds in support for AMD's VP9 hardware video decoding" - as we know, VP9 is kind of moving toward becoming the standard codec for the 'Net - "during playback to reduce power consumption, and thus helping to prevent systems from draining their batteries.  Firefox 57 also includes built-in screenshot functionality, improved tracker blocking, and support for WebVR to enable websites to take full advantage of VR headsets.



"Firefox has plans to speed things even further by leveraging modern GPUs in the near future.  Firefox Quantum for the desktop version is available for download now on Firefox's official website, and all existing Firefox users should be able to upgrade to the new version automatically.  The Android version is rolling out on Google Play shortly, in coming days, and its iOS version should eventually arrive on Apple's official App Store."  So yow, and yay.  Cool.



LEO:  Yeah, I've been using it for a while on Windows, and I really like it, yeah.



STEVE:  Nice.  So a tiny bit of miscellany.  My Twitter account is creeping up towards 60,000 followers.  I was put in mind of it because I think it was Simon Zerafa who caused me to take a look at ShieldsUP!.  We are just shy of the 100 million mark of ShieldsUP! users.  We're getting, I checked, about 4,500 new ShieldsUP! uses per day.  So we are about 45,000 shy of 100 million.  So in about 10 days from now, probably just shy of next week's podcast, we'll be crossing the "100 million served" mark.



And these are not multiply counted.  If a user goes there and does several ShieldsUP! uses during one session, I only count them as one use.  I maintain the most recent 4,096 IPs from which ShieldsUP! is being used in a most recently used, an MRU list putting anyone who's not found at the bottom and knocking the oldest off the top.  So that's really all - I did that from day one, so that's a really solid, separate 100 million use mark.  So very cool.  And of course ShieldsUP! is what got me into the security business.  It was when I discovered that people's Windows hard drives were mapped as c: on the public Internet.  I thought, okay, somebody's got to bring this to the world's attention.  So I did.



And I love this tweet.  This was a retweet from Simon Zerafa, who was retweeting Stephen Cole, who had just phrased things in a fun way.  He said:  "The U.S. dollar plummets to a new low of 0.00013 bitcoins."  And in fact I checked yesterday when I put my first post of this into my own notes for the show, and then again this morning, $8,200 is where...



LEO:  We've got to unlock our wallets.



STEVE:  I've got to go find mine.



LEO:  You're a lot richer than I am.  You have 50 of them.



STEVE:  Wow.  I do.  Yikes.



LEO:  You're a wealthy man.



STEVE:  Indeed.  And so two notes about SpinRite from a Benjamin Rose.  He tweeted:  "Steve.  Could hard drive ECC" - that's the error correction - "be disabled on a hard disk?  If so,  could that be in the next SpinRite release?"  He says:  "I'd rather let DynaStat do the work instead of the controller's ECC.  SpinRite does a better job."  And so to Benjamin, he should know that's always been there.  SpinRite always disables error correction as one of the things that it does.  It does the recovery in multiple phases.  But, for example, you would never want to be doing surface analysis and checking the surface for defects if error correction was in place because it would hide the defect.



And so SpinRite has always been capable of dynamically bypassing error correction when it wants to get to the truth about the disk.  And then, however, if it is during the data recovery phase,  if it is unable to recover the data to get a perfect read, then it will reenable error correction in order to deliberately bring the error correction back online in order to use its leverage, the error correction leverage, to get just one final last fully corrected read before it then frees the drive up to map that out as a bad and uncorrectable sector.  Then it maps a new one in and replaces the recovered data back in place.  So all this happens behind the scenes with the user just saying, wow, look at that thing spin.  So this is what's going on with SpinRite.  So a lot of technology that isn't normally seen.



And then a second tweet, Naruto Uzumaki, I hope I said that right, he said:  "While more is fun, on a clearly failing 1GB Sony Microvault flash drive, a Level 4 scan using SpinRite is able to temporarily fix it."  And, okay.  So I just did want to mention that this tweet put me in mind of just saying to people that there's a limit to how much SpinRite can pull - how far back from the grave SpinRite can pull data.  And so if someone just wants to play with SpinRite by watching it fix a drive that is really trying to die, it can keep doing so, but there's a limit.  So I do want to encourage people not to confuse SpinRite's ability to in some cases perform miracles of data recovery with that also being a signal that it's time to swap the drive out.  Like use SpinRite to make the drive readable, make an image, make a backup.



LEO:  Don't keep using it.  [Crosstalk].



STEVE:  But, yeah, just don't - yes, exactly.  After your 50th recovery it's like, oh, yeah, it won't keep doing it forever.  Sooner or later the drive is going to win this battle.  So use SpinRite to pull it from the gray zone; but, once it gets too far into the black, you're really in trouble.



LEO:  Yeah, yeah.



STEVE:  So don't push it too far.



LEO:  So what is this, what is it, Quad 9?  What is that?



STEVE:  Okay.  So Quad 9 is a new DNS service.



LEO:  Like OpenDNS, kind of?



STEVE:  Exactly.  Very, very interesting.  It was founded by a consortium of three groups:  IBM Security; a group called the Packet Clearing House (PCH), they're providing the global  Internet infrastructure; and then there's a group known as the Global Cyber Alliance, which is the third piece of this.  So it's called Quad 9 because the DNS is 9.9.9.9.



LEO:  Ah.  Okay.



STEVE:  And the 9-dot network is IBM's Class A, their what is it, 24 million IPs that IBM has.  So they decided they wanted to support this.  They have a security group, the IBM X-Force.  So curious about this, I fired up GRC's DNS Benchmark, and I've got a screenshot of it here in the show notes.  I added 9.9.9.9 to the list of DNS servers that were already built in.  The DNS benchmark uses the ones that my system has, which naturally it's Cox since I'm a Cox cable subscriber.  And then a bunch of others that it knows about.  And then I added 9.9.9.9 myself.  It's possible people don't know this, but if you right-click on the little icon, the little starburst icon in the upper left, there's a big dropdown menu list, and you can do things like add servers.  You can create an INI file to make those choices sticky and so forth.



Anyway, ran the Benchmark, and it is using - comparing 9.9.9.9 to all the, like, 50 other DNS servers.  Not surprisingly, the ones for Cox were the fastest because they're absolutely the closest to me.  They're at the other end of my cable.  But the 9.9.9.9 was not noticeably slower.  That is, it was at the very top of the list, despite just being a universally usable DNS server.



Now, they're doing this by using anycast routing, which automatically finds and uses the nearest DNS server to the user.  So at the time of this launch, which is now, there are 70 points of presence, that is, 70 physical servers located in 40 countries, but that number is growing to 160 through 2018.  So this service is launching with 70 and will be more than doubling to 160 servers.  So people will be getting them even closer to them.



So what is it?  So the focus is a privacy and security enhancing DNS service which is completely free to use.  So it's recursive anycast DNS that provides end-users with robust security protections, high performance, and privacy.  From their own description, under security they said:  "Quad 9 blocks against known malicious domains, preventing," they wrote, "your computers and IoT devices from connecting to malware or phishing sites.  Whenever a Quad 9 user clicks on a website link or types an address into a web browser," they write, "Quad 9 will check the site against the IBM X-Force threat intelligence database of over 40 billion analyzed web pages and images.  Quad 9 also taps feeds from 18 additional threat intelligence partners to block a large portion of the threats that present risk to end users and businesses alike."



They said under Performance, and I verified this myself just from here:  "Quad 9 systems are distributed worldwide in more than 70 locations at launch, with more than 160 locations in total on schedule for 2018.  These servers are located primarily at Internet exchange points, meaning that the distance and time required to get answers is lower than almost any other solution."  For example, OpenDNS is in the benchmark and didn't show up.  I mean, they're way slower than, right now, than Quad 9 is.  "These systems are distributed worldwide, not just in high-population areas, meaning users in less well-served areas can see significant improvements in speed on DNS lookups.  The systems are anycast, meaning that queries will automatically be routed to the closest operational system."



Under Privacy:  "No personally identifiable information is collected by the system.  IP addresses of end-users are not stored on disk or distributed outside of the equipment answering the query in the local data center.  Quad 9 is a nonprofit organization dedicated only to the operation of DNS services.  There are no other secondary revenue streams for personally identifiable data; and the core charter of the organization is to provide secure, fast, private DNS."



And of course it's easy to use, and I'll be switching to it when we hang up the podcast.  I'm switching completely to it.  Administrators can easily configure endpoint devices to point to the Quad 9 DNS server at address 9.9.9.9.  Oh, and I did some more digging about the technology because they mention also having a whitelist.  They have a blacklist.  They also maintain a whitelist of the top one million sites on the Internet to prevent inadvertent spoofing or blacklisting.  So the known safe list of major sites like all of our good ones that are known, overrides the known bad list.



There's also one additional trick.  So this 9.9.9.9, or 9.9.9.9, includes the block list.  They also support DNSSEC.  So their servers will use DNSSEC on origin DNS, which offers signed records, to prevent any spoofing between the authoritative server and the Quad 9 recursive DNS and additional security features.  They don't yet support DNSCurve.  I think I saw something about DNS over TLS, but I haven't had a chance to track it down.  And I also saw a posting where someone from Quad 9 was saying they were in the process of supporting, I think it was DNSCrypt is on the way.  So they will be shortly offering a means for us to encrypt our connections to them so that even our own ISP is not able to snoop on what we're doing.



But for me, especially for IoT devices, the idea would be we would configure our household router not to pick up DNS from DHCP from our ISP, but to override it to 9.9.9.9.  In which case, all of the systems in our home, all of our PCs, our smartphones and IoT devices, would be using Quad 9 to resolve the IP address for the domain.  And if there's anything phishy, okay, P-H-I-S-H-Y, or flaky, we'll just get back a "not found."  There's no redirect page.  They send back the NX domain error code which is "This domain does not exist."



So, and the other advantage is it's not just for the site you're visiting, as we know.  But if your web browser, if the site you're visiting is trusted but has been compromised and is causing your browser to attempt to pull content from a sketchy site, you just won't be able to get the IP through Quad 9.



Oh, what I was also going to mention is that 9.9.9.10 is a variant, just sort of for testing or for people who want to experiment.  No block list, no DNSSEC, and some other security features are disabled.  I'm not clear on why they're doing it, maybe just in case somebody wants the speed but not the security.  Because, I mean, again, these guys are fast.



You can also do a traceroute from your location, if you're curious.  I did because I was.  It hopped out of Cox, and then a couple other hops, and then got to Quad 9's server.  So like about eight hops, and I had a 10-millisecond roundtrip, which is very respectable.  And again, using my own benchmark, none of the well-known alternative DNS services came close.  I should have looked to see where OpenDNS was.  It's easy enough to run a benchmark.  Just add 9.9.9.9 to GRC's DNS Benchmark and run it and see where it rates.  So I'm very impressed.



Oh, and in their FAQ they ask themselves the question:  Does Quad 9 share the DNS data that is generated with marketers?  They said:  "Quad 9 does not and never will share any of its data with marketers, nor will it use this data for demographic analysis.  Our purpose is fighting cybercrime on the Internet and to enable individuals and entities to be more secure.  We do this by increasing visibility into the threat landscape by providing generic telemetry to our security industry partners who contribute data for threat blocking."



In other words, and I know this from having read more deeply elsewhere, in return for these other 18 additional threat intelligence partners providing that data, they receive anonymized information if a Quad 9 user trips on one of the sources of information that the intelligence partner provides.  So the threat intelligence partners get back essentially a ping when someone did attempt to access one of the records that they provided.  So it's useful, but it's completely anonymized at the local server where the query is answered, with completely benign data going back to the partners.



So I just don't see a downside here.  These guys seem to have the right motivation.  They're providing a useful service.  They are basically preventing our computers from getting malware because no known malware will be able to have a sketchy domain resolved.  It'll quickly be, essentially, unless we were the absolute very first to encounter it and get infected before anybody else, before it came to the attention of any of these 19 organizations that would immediately blacklist it, we're safe.  So, which is not anything that any of the existing generic ISP DNS is doing for us.  And there's, for me at least, zero performance penalty.  So I'll give it a shot.



LEO:  Nice, nice.



STEVE:  And I'll let anybody know if I change my mind.  Yes, 9.9.9.9.



LEO:  Very, very nice.  And I like the group of people behind it.  If IBM says that, I'm going to trust them.



STEVE:  Yup.  And they are.



LEO:  And the others are going to keep them honest; right?  That's the other good thing.  It's not any one person, yeah.



STEVE:  Yeah.  And again, it's - and I've got links in the show notes for anybody who wants to dig deeper or just put in Q-U-A-D and then numeral 9 because this has been making a splash, and I wanted to give it some good coverage and the advantage of taking a deeper look and also verifying that it is absolutely as fast as your own ISP's local DNS.  I'll look forward over the next week, I'm sure I'll hear some tweets from people who have run GRC's DNS Benchmark against it, compared to their own DNS servers.  And so I'll know more next week.  But it sure looks good.



LEO:  A breaking story that we will undoubtedly be covering next week on a breach at Uber, fairly significant breach at Uber, that Uber paid hackers $100,000 to cover up, apparently.



STEVE:  [Gasp] Boy, these guys just do not know how to behave, do they.  My lord.



LEO:  It's a good way to put it.  It's just coming in, so I  hope I'm not misstating that.  But that's what the headline reads.  And it is a fairly large amount of data.  Uber has fired their Chief Security Officer as a result.  It says, they just announced this today, that October 2016 hackers accessed seven million driver's licenses and names, emails, phone numbers of 50 million riders.  And then paid hackers $100,000 - I'm sorry.  Did I say 50?  57 million.



STEVE:  Wait, wait.  They have the licenses of the riders?



LEO:  Probably not of riders.  I would guess that's from drivers.



STEVE:  Okay, yeah.



LEO:  But they do have some information from riders, and they paid hackers, even worse, $100,000 to keep it hushed up.  Chief Security Officer Joe Sullivan and another executive have been fired summarily as a result.  Personal information of seven million drivers, that would include driver's license numbers, about 600,000 U.S. driver's license numbers, and of 50 million Uber riders.  But, good news, no Social Security numbers, credit card information, tip location details, or other data were taken.  But names, email addresses, and phone numbers.  So if you've used Uber, somebody's got your phone number.  Big deal; right?



STEVE:  But these clowns are Uber and not in a good way.  



LEO:  Yeah.  Yeah, Goobers.  We're going to call it Goober from now on.



STEVE:  Goober, yes. 



LEO:  At the time they were negotiating with U.S. regulators investigating other privacy violations, which is probably the reason they tried to cover it up at the time.  They now say they had a legal obligation to report to regulators and drivers.  Instead, they paid hackers to delete data and keep the breach quiet.  Uber said it believes the information has not been used, but will not disclose the identities of the attackers.  I'm not sure if they know it.



"None of this should have happened," said the new CEO, Dara Khosrowshahi, "and I will not make excuses for it.  We are changing the way we do business."  This is before, of course, he became the new CEO, back under Travis Kalanick.  After the disclosure today, New York Attorney General Eric Schneiderman launched an investigation.  I'm sure there'll be much more to talk about next week.



STEVE:  Wow.  Good early info, my friend.



LEO:  Yeah.  I thought you'd want to hear that.  Of course Steve, one of the things we love about Steve, he likes to do his homework.  He doesn't talk off the cuff about anything.  And so anything that breaks during the show or right before the show you'll have to just listen to his thoughts next week.  And of course that means there'll be a lot more information next week about this, I am sure.  Just another breach.  Just another day, another breach.



STEVE:  Ho hum.



LEO:  Ho hum.  You'll find this show and all of Steve's good works at his website, GRC.com.  That's where you'll find of course SpinRite, the world's best hard drive maintenance and recovery utility, but also stuff like SQRL, his current project; SpinRite, Shoot the Messenger, DCOMbobulator, ShieldsUP! - now, what, 100 million strong users.



STEVE:  Yay.



LEO:  That's amazing.  I've used it many times, of course.  Really good way to test your router when you first install it, make sure everything's copacetic.  And the podcast, audio versions of the podcast as well as transcripts, which are very valuable for searching, as well as a lot of people like to read while they listen, at GRC.com.



We also have audio and video of the show at our website, TWiT.tv/sn.  And you'll find it everywhere you subscribe.  Just look for, I mean, this is a show you want to not only listen to every show, but keep previous shows.  Lots of people relisten.  Sometimes people listen at high speed the first time and then slow it down if they need to.  It's good to have a copy.  Subscribe on Pocket Casts or Stitcher, iTunes or your podcast app on your phone, whatever you prefer.



We will be back here, as we are most Tuesdays, at about 1:30 Pacific, right after MacBreak Weekly, whenever that ends.  It was 2:00 o'clock today.  That's 4:30 Eastern time, 21:30 UTC if you want to watch live.  If you do watch live, join us in the chatroom.  They're great people in there.  And it's a lot of fun, the behind-the-scenes conversation.  And people who watch this show are smart, so there's always great people in there, lots of good question-and-answers going on during the background.  And jokes, too.  I usually steal all my material from the chatroom, irc.twit.tv.



Don't forget TWiT.tv/store.  The holidays are coming, and now would be a good time for you to pick up that Steve Gibson moustache cup, TWiT.tv/store.  This is a new vendor.  We were using for a long time a vendor that had some weird countdown thing, like we've got to wait till we get a thousand ordered and all that stuff.  Now you can just go and buy directly.  $19 for a Security Now! mug.  Love that mug.  



STEVE:  Smug in the mug.



LEO:  It's the smug mug of Steve Gibson on the coffee mug that says Security Now!.  But we've got T-shirts.  We also have a Security Now! T-shirt, the baseball tee.  Show with pride your affiliation.  Plus of course lots of other great TWiT and other shows.  Oh, there's a hoodie.  You know, when you're going to DEFCON next month, maybe you ought to wear the Security Now! hoodie.  Mr. Robot would appreciate that.  TWiT.tv/store.



Steve, always a thrill, always a pleasure.  A little bit of a short show today.  I hope we haven't disappointed our vast listening audience.



STEVE:  Lots of good news.  I think everyone's going to be putting 9.9.9.9 into their browser.  Yup.



LEO:  I like it.  Somebody said, well, they should also get the IPv6 version.  Then it would be, what, nine nines?



STEVE:  They do support IPv6 already, so they're ready to go.  Okay, buddy.



LEO:  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#639

DATE:		November 28, 2017

TITLE:		News and Feedback

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-639.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss a new bad bug found in the majority of SMTP mailing agents.  Fifty-four high-end HP printers are found to be remotely exploitable.  More than three-fourths of 433,000 websites are using vulnerable JavaScript libraries.  We discuss some horrible free security software, additional welcome Firefox news, a bit of errata, some fun miscellany, and a bunch of feedback from our listeners - including reactions to last week's Quad 9 recommendation.



FATHER ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode 639, recorded Tuesday, November 28th, 2017:  News and Feedback.



It's time for Security Now!.  This is the show with Steve Gibson who reminds all Mac users that we are root.  I'm Father Robert Ballecer, the Digital Jesuit, in for Leo Laporte.  Steve of course is the big brain behind Gibson Research, ShieldsUP!, SpinRite, and our coming non-passworded overlord.  Steve, my friend, it is so good to see you.



STEVE GIBSON:  Well, it's a pleasant surprise.  When I heard Leo saying that they were off to the airport a few hours from now, I thought, oh.  And I happened to be ready a few hours, I mean, like ready immediately at 1:00.  So I sent messages to Leo and Lisa saying, "Hey, I can go now if that would be easier."  But then you sat down in front of the camera and began setting up.  And I said, oh, it's a Father Robert day.  So welcome.  Great to have you.



PADRE:  It's like a magical wonderland here, and things just happen.  There are some very smart people who do scheduling way, way better than I ever could.  And they just tell us where to show up, and things tend to work out.



STEVE:  That's the advantage of having people who are on the job, yup.



PADRE:  Indeed.  Uh-oh.  Steve, of course we're just going to mention it because it just happened a couple of minutes ago, towards the end of MacBreak Weekly, actually, there was a - I'm not even going to call it an exploit.  I'm just going to call it someone fat-fingered something when they were releasing the new version of OS X that allows someone to put "root" as the username, no password, and just enter that in a couple of times and then gain access to root for Mac OS X.  That's bad.  It's going to be patched, I'm betting within the hour or so.  But I don't want to cover it because I'm more interested in figuring out in the post exactly what allowed that to happen.



STEVE:  Well, okay.



PADRE:  Have you heard of this at all?



STEVE:  So what little we know, because I was learning of it, I was finishing putting the show together when the news hit.  So I don't have any additional detail.  But there's an important takeaway from our listeners which is that they need to make sure no one has access to their Mac OS X machines in the interim. This is a complete security bypass of all logon authentication for this latest version of Mac OS X.  And as you said, you put "root" in as a user, leave the password blank, try it a few times, and I guess it fails initially, and then it succeeds and logs you in with absolute root privilege on any one of these most recently updated Mac OS X's.



So yes, Apple's going to fix it.  But what this means - and it's not a remote exploit.  But it means that right now there are people, mischievous people learning about this who know they have a small window of opportunity before this gets fixed, during which time they have unrestricted access to any Mac machine that they can find.  So any machines that you have physical control over, our listeners who are listening to this, be careful.  And you might try it yourself, but don't do anything to hurt yourself if this works.



But anyway, so this is just breaking news.  And certainly by the time we know more about it next week, this will have been patched.  But right now, certainly this is going like wildfire through the globe.  And what it means is, very embarrassingly, at this moment, all of the most recently updated Mac OS X systems are vulnerable to a local attack allowing anyone to get root on the machine and have at the machine.  So a complete bypass of logon security.



PADRE:  Right.  And this is going to work with the High Sierra distro of OS X, so check to see if that's what you've got.  And yes, as you mentioned, it's going to be patched almost immediately.  I'm sure that there's a lot of egg over at the campus right now on faces.  But what has me about this is the fact that it takes a couple of times to take. 



STEVE:  So you agree.  It's a weird...



PADRE:  That's super strange.



STEVE:  It's a weird, weird bug.



PADRE:  Yeah.  This is not a hard-coded password.  This sounds like there's a call somewhere that ends up calling to the wrong thing, if you air the authentication [crosstalk].



STEVE:  If you exceed - exactly.  So, for example, we know there are normal lockouts to prevent you from guessing.  And/or sometimes various systems will slow down and get more deliberately laggy to prevent you from continually guessing incorrectly.  So it makes sense that there could be a multiple failure branch which is engaged if you fail several times.  Unfortunately, it then fails open rather than failing closed. 



PADRE:  Right.  Which this is a bit.  That's a bit that flipped.  Yeah, no biggie.  [Crosstalk] in the chatroom points it out.  He said, "Look, it's the lockout call."  So when it supposed to call for a lockout, instead it calls for this broken authentication, which is awesome.



STEVE:  A lock in.  It's funny, too, because at the end of MacBreak they were talking about both in this context and also about iOS 11.  And I disagree with Rene a little bit.  He took the position that, oh, well, all major iOS updates, major version changes, have had bugs.  I have to take exception.  This has been a catastrophe.  You can't call iOS 11 anything but an absolute disaster.  I mean, I've seen it doing things - it's getting better.  And I remember when I saw this thinking to myself, okay, we are going to see the highest rate of updates that we've seen yet.  And sure enough, every couple days, in some cases several a day, they're fixing, they're giving us new versions of iOS 11.  It's a huge embarrassment.  It's just been a catastrophe.  I don't want to sugarcoat it.



I'm sorry because I love iOS.  I'm an iOS person, all my pads, my phones and so forth.  But, boy.  And it's rendered my iPhone 6s completely useless.  It's just, I mean, I'm managing to use it, but nothing is smooth.  Nothing is quick.  Nothing works the way it used to.  Even just swiping between screens it waits for a while, rearranging icons.  It's just broken.  So it does make me wish for Jobs being back because this would never have happened under Steve's watch.  This is what he prevented from happening.  It's why he kept the iPad off the market for multiple attempts, saying, no, it's not ready yet.  This is not good enough.  And it's just so disappointing.



PADRE:  Well, Apple is not supposed to be the "good enough" company.  Apple is supposed to be it's polished; it's refined.  I mean, yes, you may update every one in a while for new features, but you're not supposed to be updating for simple use.  That's weird.



STEVE:  These devices have always felt like appliances.  Yes, there was a computer in there, but we didn't really know about it.  We didn't think about it.  It was an appliance.  It was, you know, we used it for a function.  Now they feel like broken computers.  They have become broken computers with all of the pain that comes along with something not working the way it should.  It's just really sad.



Anyway, this is Episode 639, last podcast of November, for the 28th of November.  And the good news is we've been accumulating feedback from our listeners that we've just not had a chance to get to.  And for whatever reason, Thanksgiving, maybe the hackers took the week off, I don't know, there just was kind of blessedly not a lot of the typically horrific news that we're dealing with.  Well, of course this OS X is pretty horrible.  But we essentially have, where we normally have 15 main security topics, we have five today.  So I thought, yippee.  And so there was feedback from our listeners.  And I went back and got the feedback from last week that we haven't had a chance to get to and put it all together.  So this one is just called news and feedback.



We have a little bit of news.  We're going to talk about a new bad bug found.  We've already talked about Mac OS x.  Another one found in the majority of SMTP, that's the Simple Mail Transfer protocol.  Mailing agents, the one that has I think it's 54% of the Internet uses Exim, E-X-I-M, and a bad problem was found in those, which is like, as in immediately update.  Fifty-four, I couldn't believe there were 54, of HP's high-end printers were found to be remotely exploitable by some clever hackers.  We'll talk about their work.



More than three quarters of an analysis of 433,000 websites are currently using vulnerable JavaScript libraries.  There's some horrible free security software which I actually got clued into from Leo's retweet of an EFF tweet which put me onto that.  There's also some additional welcome Firefox news.  We talked last week about I think it's 57, which is their Quantum release, which is just running like - it's just screamingly fast.  Leo loves it.  I've been using it.  I'm a fan now.  I can't use it on XP, but it's running over on my Win7 machines just fine.



Then we have a little bit of errata, some fun miscellany, and we're going to fill the rest of the show with a bunch of feedback and discussion between us from our listeners, including the first week's reactions to last week's topic, which was the Quad 9 DNS service.  I've got a lot of feedback from our listeners after using it for a week.  So I think another great podcast to end November with.



PADRE:  And actually I'm really interested in the way that we're going to start this because the Picture of the Week is something that was sent to me, as well.  Actually, I think the same person copied it to both of us.  And actually let's jump into that because it is kind of interesting.  This is one of these carbon copy responses that Comcast has been sending out, especially with Net Neutrality in the crosshairs this coming week.  They're saying, quote, "We do not and will not block, throttle, or discriminate against lawful content.  We will continue to make sure that our policies are clear and transparent for consumers, and we will not change our commitment to these principles," unquote.  Now, I have received at least four or five copies of this, Steve.  How many did you get from people on Twitter?



STEVE:  Yeah, same.  And so that's Comcast's official tweet that you just read.  "We do not and will not block, throttle, discriminate," blah blah blah.  And so there is a great response, a tweet from someone whose handle is Lore.  And so in that spirit he says:  "We never will, but it's very important that we be able to.  But we won't.  So let us do it.  Because we won't do it.  Which is why we're spending so much money to make sure we can.  But we won't.  But let us."



PADRE:  And actually that sums it up.  And we already know, we know that both Comcast and Verizon have, because it's leaked out of corporate, are preparing to do tiered packages.  Once Net Neutrality goes away, they're going to start small.  I mean, it's not going to be doom and gloom.  They don't want to prove their critics right.  So it's going to be simple things, like hey, we noticed that your VPN is going really, really slow.  Would you like to buy VPN service from us?  They're not going to say we've throttled your VPN service.  They're just going to say, if you use our VPN, it will be much faster.  It's going to be these little bits and pieces.  It's the death of a thousand cuts is essentially what we're going to be heading to, unfortunately.



And the thing is, I don't know if you've gotten into these Twitter battles over the last couple of weeks, Steve, but I've got people who were actually saying, "Well, no, Comcast, they were running ads saying that they support Net Neutrality."  I'm like, "Really?  So you're going to believe an ad over what they've been doing the last 10 years."  That actually kind of disturbs me.



STEVE:  Yeah.



PADRE:  I know about that.  All right, Steve.  Before we get onto, well, the fun stuff, let's look at the funner stuff.  You did find something that I may need to add to my Christmas list.  



STEVE:  Oh, yeah.  I ran across it in my Twitter feed as I was catching up on the previous week, as I do every Monday as I'm preparing for the podcast.  And it just caught me by surprise.  It's just so clever.  It's a placemat with a grid pattern which is offered by the Museum of Modern Art, which gives the illusion of it being like a rubber sheet with a grid on it where the knife and fork and the plate are depressing this flexible surface.  Or it could also be seen as a gravity well created by the plate and so forth.



Anyway, it's just - I tweeted it.  I got a lot of positive feedback from the people following me who saw the stream and said - because initially I only had a Japanese site.  Somehow there's a Japanese version of the MoMA.org site.  And so I didn't have a U.S. link.  And several people said, "Okay, you just can't tweet that picture without telling us what the URL is."  So a lot of interest.  And as a consequence I do have in the show notes the link for this and the picture.  For what it's worth, I just think it's the best dinner placemat, or I guess breakfast, although it might be a little much for breakfast, that has ever been conceived.  So I just loved it and wanted to share it with the podcast listeners as a completely off-topic but very fun thing for - and as you said, maybe perfect for the holiday season.



PADRE:  You know what I love about that?  The fact that you could put these down at a table, bring over your friends and family, and just look around.  And you'll immediately know which ones of your friends and family are geeks, like real geeks, because they'll go, oh, time-space displacement.  And the others will just go, oh, it's kind of a funny mat.



STEVE:  Yeah.  So I mentioned before that a very worrisome remote code execution and DDoS, actually two different exploits, two different vulnerabilities, were discovered in the Internet's majority Mail Transfer Agent, MTA, which is the official term for these things.  These are SMTP servers, Simple Mail Transfer Protocol, which forward mail around the Internet.  Exim, E-X-I-M - I don't know if that's how you pronounce it.  Is it xim or ex - I've always just said Exim.



PADRE:  I've always said Exim.



STEVE:  Yeah.  So it is 54% of the MTAs, the Mail Transfer Agents on the 'Net, are using Exim.  It was designed for Unix-like operating systems, and like Send Mail is the even older venerable sort of original Unix implementation for mail.  Exim is easier to use.  It's available on most Unix-like systems.  It's been transported, or it's been ported, to Microsoft Windows using Cygwin.  And it's the default MTA on Debian and GNU Linux systems.  So it's around.  It's very popular in the U.K., where ISPs and universities are using it.  And also it's widely used with GNU's Mailman mailing list manager and also the widely used cPanel.  Oh, it's 56% rather than 54.  I remembered the number wrong.



Anyway, so against this backdrop of this thing being widely used, over the Thanksgiving holiday a security researcher discovered and publicly disclosed two critical vulnerabilities in this majority platform which is by its nature public.  I mean, this is public mail routing.  So they're all publicly accessible.  There are two vulnerabilities.  The first one is a class of vulnerability we've discussed in the past, a use-after-free bug where some memory is allocated.  Then it is released back to the operating system, yet the code still has a pointer to the released memory.  And there are ways then that it's possible to leverage the use of that after it's been freed in order to get code to be executed from that freed memory.



And since this can be supplied remotely and as a consequence of this vulnerability, it can be loaded with code that the remote attacker provides.  It creates the ability to remotely inject malicious code into the server by - in this case it's a special sequence of BDAT commands, which is a class of command that the MTA uses.  So that's the first one, basically publicly vulnerable, remotely injectable, remote code execution in a vast number of machines.  A Shodan scan revealed that more than 400,000 servers are currently vulnerable.



So the takeaway for every one of our listeners is, if you know of or are responsible for an Exim server, you want to update this immediately.  The updated version 4.9 is now available on GitHub.  So it's there.  4.88 and 4.89 are the vulnerable versions.  The second vulnerability of the two is a DoS, a Denial of Service flaw which essentially prevents the server from accepting and processing SMTP mail connections.  So all it does is it just locks up that server and prevents it from performing its function, thus denying the service that it was designed to provide.  But probably, given that it is scannable, Shodan provides them.



There are 400,000 of them at this point.  Until they are updated, anybody within range of this audio absolutely wants to update as quickly as they can.  Oh, and a Python-based proof-of-concept code has been released, so there's even a template for how to do this.  The code itself is not malicious.  It's a proof of concept.  It just puts the server into a five-second loop in order to demonstrate that remotely injectable code is successfully running in the system.



But everybody wants to - anyone who has access to or responsibility for one of these wants to update to 4.9 immediately because we can find them, they're public, proof of concept makes it easy to implement, and in no time we're going to see exploits of this, essentially people taking these servers over and who knows what, botnets or command-and-control waypoints, who knows.  So not good.  And the link is GitHub.com/exim/exim in order to get the latest update.  I've got the link in the show notes.



PADRE:  Now, that second one, the second CVE, that's limited usability because essentially it's a DoS attack.  And I thought maybe, if there was a way to cut all connections except the connections that a remote attacker was using, it could be usable.  It'd be nice because you could cut off all other activity.  But it would cut even the connection that the remote attacker is using.  So that's essentially a nuisance attack.



STEVE:  Right.



PADRE:  But that first one, that is kind of scary because that's built in.  Now, run me through this.  So the BDAT, it's running within SMTP.  I make it generate an error.  And when it generates an error, actually it carves out an allocation of memory, and that's the memory I use to try to run a remote code execution?



STEVE:  I didn't dig into it because it's already been fixed, and the bad guys are going to be cleaning this up or taking advantage of this.  And it has been fixed.  There was some comment about ending a command with a period, but I think that was the second, the DDoS flaw.  And so from the CVE it says that you can remotely execute arbitrary code in the SMTP server by crafting a sequence of BDAT commands.  And again, there was no point at this point, for me anyway, digging any deeper.  The bad guys are certainly going to want to.  And all the information is there, especially in the Python-based proof of concept, for how to take this thing all the way to the mat.



PADRE:  Oh, okay.  Actually, I think I remember reading about this.  The issue is that it's assuming that, if you're putting any code into a memory allocation, that you've compiled it with Pi because Pi will keep you from doing anything malicious with it.



STEVE:  Correct.  I did see that, yes.



PADRE:  But an attacker wouldn't be doing that because an attacker wants to do something malicious.



STEVE:  Exactly.



PADRE:  So it makes the allocation, and then they can just shuttle in their code.



STEVE:  And put anything they want, exactly.  And then release it, and it gets used.



PADRE:  Well, that sounds fun.



STEVE:  Yikes.  Yeah.  Yeah.



PADRE:  Now, I could see someone doing something like doing the first exploit and then doing the second exploit to cover their tracks.  They'd hang the machine.  Someone would just figure that it's gone into some weird state, and they do a restart.  And now everything is running the way that the attacker wants it to run.



STEVE:  Well, and what we see, especially for something like this, is if there are 400,000 MTA machines, half of them are virtually abandoned administratively.  They're in closets.  They're dusty.  No one has looked in on them for years.  They're not going to get fixed.  So there are certainly, you know, the ones that ISPs are running, hopefully ISPs are on the ball.  Well, universities are probably going to be the hosts of some of those that are in the closet that have been long since forgotten.  These are just workhorse servers that nobody thinks about because there's nothing sexy about shuttling mail around.  And especially if it's just a server that accepts mail and forwards it.  It doesn't even have like a body of accounts that clients are connecting to, necessarily.  It's just a forwarding agent.



So these things tend to not be visited often.  And I think it's entirely foreseeable that, as a consequence of this, we now have a new inventory which, for the foreseeable future, bad guys are going to be able to leverage into obtaining presence in as a consequence of this flaw.



PADRE:  Wow.  We'll see.  And fortunately, yes, there is a patch.  And most of those machines will never be patched.  in fact, I would leverage a guess here that the majority of those machines haven't been patched since installation.



STEVE:  Precisely, yes.



PADRE:  Well, that's some good news.



STEVE:  Well, actually, if the problem was introduced in 488 and 489, then older ones may not be vulnerable.  This may be a relatively recent update which introduced the problem at 489.  And so maybe, as a happy coincidence of this, it's the ones that have been maintained that now need just a little bit more maintenance.



PADRE:  Isn't that a kick in the pants.



STEVE:  Yeah.



PADRE:  So that's your punishment for updating your servers.



STEVE:  That's right.  



PADRE:  Well done.



STEVE:  So I didn't realize that - and I guess this must be history also that HP has 54 models of enterprise printers.  I mean, that even exist, 54.  Anyway, it turns out that HP in some of their marketing material was annoyingly boastful, talking about how bad printer security is of everybody else, but they're wonderful.



And in fact on the site, I didn't bother to dig into this video because it wasn't germane.  But on the site of the security guys who decided to take a closer look at HP printers, they have this video which, from their description, which I did read, it just sounds obnoxious.  It sounds like HP marketing getting a little carried away with themselves, thumbing their nose at the poor security of everybody else's printers except HP.  So these guys said, "Ah, okay.  Let's take a closer look at HP printers."  So you're going to get a kick out of this, Father Robert.



So first of all, years ago, I remember us talking on the podcast about how enterprise-class printers contained hard drives.  So my first reaction even then was, what?  What?  A printer has a hard drive?  So and back then the worry was that decommissioned printers that were sent off, being recycled or being thrown away, essentially, had hard drives on which were records of maybe every print job it had ever done.  I mean, so obviously a huge privacy and security concern because these hard drives were caching print jobs and leaving them there, just because the drives were not being wiped.  Nobody was aware that there was gigs of nonvolatile storage that had been saving the print jobs in a printer.  Why would a printer have a hard drive?



Well, turns out they do.  And these HP enterprise-class printers also have hard drives.  Now, the first thing - this is so fun.  The first thing these guys noted is that, wow, okay, HP apparently wants people to not poke around in the operating system and firmware of these devices.  So the printers use FIPS standard, F-I-P-S standard-encrypted, hardware-encrypted drives.  So, and we've talked about this technology.  When the drive boots, a password is given to it through its interface to unlock a private key through which a symmetric cipher decrypts the contents of the drive on the fly as it comes through the electrical interface so that the system gets decrypted hard drive content, but the drive content is encrypted.  And unless you have the key, you're not able to see the hard drive.



So they got a printer.  They purchased two for this purpose.  They take the drive out and mount it, and it's encrypted.  So, okay, they go, whoops, okay, it's encrypted.  What to do?  So they're clever.  They got a non-encrypted, that is, non-encryptable drive, some other drive, I think it was a Toshiba that didn't offer hardware encryption, stuck it into the printer.  Now the printer says, "Oh, I have a problem.  I don't have my OS and firmware.  I can't read it.  We must have had a hard drive problem."  So they use a thumb drive to reload the OS and firmware into the printer.  Which is now not encrypted on the hard drive because the hard drive doesn't support encryption.  Now they take the drive out and mount it and have access to the file system.



PADRE:  Oh, so HP was using the built-in hardware encryption inside of FIPS-enabled drives.  So you just put in a non-FIPS-enabled drive, and it just says, okay, I guess this is all right?



STEVE:  Okay, yeah.  



PADRE:  That sounds like a bad design, Steve.



STEVE:  So, yes, they download the OS and firmware in the "restore your broken printer" mode, onto a drive that can't be encrypted.  And now it's not.



PADRE:  It sounds like that's something you would check for.  Actually, it does check for it, and then it just says, eh, okay.



STEVE:  Yeah.  We gave the drive an unlock password that it ignored, and everything's fine.  So then they discover that - and this is where it starts getting, like, really?  It's running Windows, of course, because of course.  It's running Windows CE.  In their link - and I'm not seeing the link here.  Oh, yeah, there it is.  You might want to bring it up.  It's FoxGloveSecurity.com.  The link is an amazing walkthrough of what they did.  But they show the Windows directory with all the familiar Windows files.  DLLs, there's ATAPI. dll and all the file system files, I mean, it's very familiar to anyone who's ever poked around in Windows.  So yes, indeed, it's running Windows like all of our favorite kiosks around the world.



So they then proceed, now that they have the OS in firmware, to reverse engineer it.  And I won't go into the painful details.  But essentially what they determined was that both HP solutions and firmware updates, HP has this thing called HP Solutions which allow an extension of the functionality of the printer with a proprietary SDK which is not available, but is made available to major HP partners to allow them to create their own updates, their own additional features packs, which are then signed by HP in order to authenticate them.



So they write:  "Both HP Solutions and firmware updates consist of a single file with the .BDL, short for bundle, extension.  This is a proprietary binary format with no publicly available documentation."  They write:  "We decided that reverse engineering this file format would be beneficial as it would allow us to gain insight into exactly what firmware updates and software solutions are composed of."



So what I'm not going to go through in detail - it's all there on their site for anyone who's interested - is the blow-by-blow of their reversing of the file format.  They find CRC32.  They make some changes.  It doesn't work.  They dig deeper.  They find other things.  They end up completely reverse-engineering the structure of the file, so much so that at GitHub.com/foxglovesec/hpwn, which is to say H-P-W-N, they offer a Python script called BDL Patcher.  Okay, this is the proprietary format, signed cryptographically, but not so much, signed bundle.



And the description for BDL Patcher says:  "This Python script" - freely downloadable - "can be used to create modified, but still valid, HP software solution bundles.  Usage instructions are built into" - how convenient - "built into the tool.  All you need to do is open mod.zip, replace or add any files that you would like, and then run the following command:  Python hp_solution_patcher.py [blah blah blah].  This will generate a new, modified BDL file called patched.bdl."  Which the HP printers will then happily ingest, they will accept, and allow you to run your own code on the printer.



PADRE:  This is so weird.



STEVE:  I know.  It's just like, oh, my god.



PADRE:  Wait.  But Steve, how can you make a modified and yet still valid BDL?  That makes no sense to me, what kind of checking it's doing, because obviously it's not doing any sort of checksum.  It's just the certificate?  Just a signature?



STEVE:  Apparently HP, unfortunately, rolled their own format.  They said, oh, let's put some checksums here and some here and some here.  And this will be really hard, and nobody will be able to figure it out.  And besides,  we're encrypting our firmware on this hard drive so nobody will ever get to see it anyway.  So, whoops.



So they say:  "There exist a number of methods for updating the firmware of HP printers.  Most administrators would be aware that firmware updates can be installed through the printer's web interface or through the Web Jet Admin client."  They write:  "Firmware can also be installed at boot time through BOOTP/TFTP [the Trivial File Transfer Protocol] options. Additionally, the Security settings page on the HP printers implies that firmware can be installed through a print job..."



PADRE:  Oh, good, yeah.



STEVE:  Here, take this, HP, "...over port 9100."  Yup, the standard HP printer port.  So they write:  "Thus there are a number of avenues available for remote network injection of malicious firmware into printers."  So there is a list of the printers, linked to from their report and also here in the show notes, of the 54 printers which will accept, basically, any code that's - it's Windows, Windows CE.  So add your malware, wrap it in this .BDL format, send that printer a job it will never forget.  And it will write it onto the hard drive, and you can take the printer over.



PADRE:  And the list is pretty extensive.  It is the most popular HP enterprise printers.  It's the M651, the M680, the M631, and the X556.  Those are their top sellers.



STEVE:  It's the good ones, yes.



PADRE:  Those are the good ones.  And the crazy thing about this is, well, two things.  First, it really sounds like HP was putting all their security eggs in the encryption basket.  They just figured, if no one can see this, then they'll never be able to backwards engineer it.  So this was a clear case of security through obscurity.  They kind of threw a nod by doing some checksums, but they were clearly just hoping that no one would ever figure out a way to actually get the files.



And the second thing is, because you can replace the firmware, and because this is essentially a Windows computer, this becomes a pivot point.  I compromise one printer in an enterprise.  That printer is trusted by the entire network because everyone needs to print to it.  And now I can probe that network as if I had a Windows machine inside the network because that's what I have.  That's scary.



STEVE:  Actually, we have in the past encountered instances of APTs, Advanced Persistent Threats, living in compromised printers.  I mean, that's why printer security is a thing HP is bragging about, denigrating everybody else's security, saying how fabulous theirs is.  Which in fact is what induced these guys to take a closer look.  And when they did, oopsie.



So HP has responded.  There is an update to the firmware for all these printers.  And once again, anyone in an administrative capacity who has access to or has responsibility for any of these printers should seriously consider updating the firmware.  Unfortunately, normally printers aren't doing that themselves.  You've got to log into the web interface, check for updated version.  The printer will say, oh, what do you know?  I've got new firmware.  Then you've got to install it and restart the printer and so forth.  And as you said, Father, what they should have done, but didn't, was to verify that the ATAPI feature set being offered by the drive includes encryption, and to refuse to put that drive online in any way if the drive it encounters does not support encryption.



PADRE:  Well, that makes sense.  If that was going to be your security deterrent, then you need to make sure that it's not as easy to bypass as, oh, I'll just install a different drive.



STEVE:  And I'm sure they're doing that now with this update.  They're like, oopsie.



PADRE:  That sounds like a flag that should be set:  If no encryption, just don't boot.



STEVE:  Well, yes.  And really there is no excuse for not using a cryptographic digital signature around this bundle package, so that the only thing that exists in hardware is the public key, which is used to validate the private key, or the signature made by a private key that only HP has.  They didn't do that.  They just said, oh, let's do some CRC 32s.  It's like, what?



PADRE:  Yeah.  I'm trying to research right now because it may have changed, but when I was still doing active administration, the HP printers that we purchased had two firmwares.  And it was this whole idea, if one gets corrupt, it will just copy over from the backup firmware.  If they still do that, this could be a persistent threat.  Unless you actually overwrite both sets of firmware, then the compromised firmware can actually still exist in memory.  I'll have to look that up.  Maybe they don't do it anymore.



STEVE:  Normally you do want to have a fallback in case an update crashes.



PADRE:  Right, exactly.  That's what it was designed for.  So if you're pushing an update over the network, and it's incomplete, it doesn't just brick the printer.  It says, okay, I had a bad installation.  I'm going to copy over the original.  So, well, now I've got homework, Steve.  Great.  I think we have one of these at the school.  I could take it offline for a few hours.



STEVE:  Yeah.  It would be a good thing because, again, it's not clear whether these ports are publicly exposed, that is, on the public Internet.  This is probably largely an Intranet problem.  But we've seen many problems with configuration where it was like, wait, your printer's port 9100 is mapped through to the public Internet so that somebody can, oh, yeah, well, we want our telecommuters to be able to access it, blah blah blah.



PADRE:  Precisely.



STEVE:  So you would just want to make sure that it's, well, I mean, you certainly don't want even Intranet because, if something gets in, then this allows it to establish a foothold in a place that no one would think to look.



PADRE:  Yeah.  I mean, if I were trying to actively exploit this, I would just be running nmap sweeps right now against port 9100, just looking for 9100.  And anywhere I find 9100, I'd enumerate it.  If it's an HP printer, I've just scored.



STEVE:  Yup.



PADRE:  Well, that's okay.  That's good news, then.  So well done, HP.



STEVE:  Send it a print job it'll never forget.



PADRE:  The last print you'll ever need.



STEVE:  So a recent study of open source JavaScript libraries in use on websites produced some disturbing results.  They looked at 433,000 sites and discovered that just over three quarters of them, 77% of the 433,000 sites checked, were currently using vulnerable JavaScript libraries, that is, libraries known to have at least one vulnerability.  And they also found that where sites had one, they more often than not had more than one.  So not surprisingly, jQuery was far and away the most often seen library, with 82.4% presence in the 433,000 sites.  So jQuery at 82.4, jQuery UI at 19.9, Modernizer at 15.1%, Bootstrap at 13.7, and then downwards with popular ones.  I've enumerated them here in the show notes.



So that's not vulnerability, that's the adoption percentage.  So jQuery far and away, at 82.4, the leading JavaScript library that is present on these 433,000 sites.  The number of times it was found vulnerable was 92.5%.



PADRE:  Wow.



STEVE:  Yes.  The numbers are high:  jQuery UI, 89.7; Moment.js, 73.0.  Even though Moment was way far down the list, it only appeared in 3.4% of the sites, it was still three quarters of the time vulnerable.  So high level of jQuery adoption.  High-level of jQuery vulnerability, which is surprising.  Then we look at, okay, wait a minute.  What's going on?  The oldest version of jQuery with no known vulnerabilities is 3.0.0, released in June of 2016.  So what we're seeing is that, once sites go production live, they stop staying current.  They stop updating the libraries that they're using, and they just stick with what they've got.



PADRE:  I mean, that's human nature.  Once it's working they'd just as soon leave it alone because you might break it.



STEVE:  If it's not broke, don't fix it.  Unfortunately, and what isn't clear, I mean, I don't mean to sound like the sky is falling here.  When we say "vulnerability," they could very well be minor things.  I mean, it's not - if the sky were falling on 90-plus percent of websites, it would have been fixed.  The sky would have been patched and fixed.  In this case they're known problems.  But what's interesting is that, recently, everything is fine.  That is, the current libraries are running known vulnerability free.  And so if these sites were current as of the current version of these packages, they'd be fine.  It's just that they are at least about a year and a half out of date.  And those older packages were vulnerable, and they've not been fixed since.



So they write in their coverage:  "Each of the front-end libraries most commonly found to be vulnerable has been free of known vulnerabilities for anywhere from one to five years."  Which is to say that this massive install base of open source - largely jQuery, 82.4% - 92.5% of those are older than a year and a half and have vulnerabilities that have long since been fixed, and those sites have not been updated.



PADRE:  Right.  I'm looking through the list of the vulnerabilities.  And you're right, most of them are minor.  They can either cause a system lockup, or they'll just cause a slightly unexpected return.  But these little vulnerabilities, when you pile them up, and especially when they're this old, this is how you develop an exploit.



STEVE:  Right.



PADRE:  You combine a couple of these, and suddenly you've got remote code execution.  So that's - and the fact that they've been out there for so long, and you can essentially count on the fact that most of these sites will never be patched, that's, again, then we get into sort of ho-hum worrying territory.



STEVE:  Yes, yes.



PADRE:  Hmm.  Now, it's interesting because, if you look at the list, Google Maps is on this.  Wait, what?



STEVE:  Boy.



PADRE:  The Google Maps library?



STEVE:  Yeah.



PADRE:  Okay.  Let's get back to the action.  So we've got vulnerabilities everywhere that are piling up, and people aren't going to patch.  But I'm more interested in finding what the Treasury Department is doing in terms of fraud investigations because this one's interesting, Steve.



STEVE:  Yeah.  So three years ago the EFF posted a detailed takedown of a really disturbing piece of software known as ComputerCOP, Computer C-O-P.  And for years it was being offered by a company somewhere in the Northeast, I can't remember where, like New Jersey or New York somewhere.  It was being purchased in bulk by law enforcement agencies around the U.S. and then freely distributed by them as a public service and as a public relations freebie to their communities.  So, for example, in one case a Sheriff's Department bought a copy for every family in its county.  And the idea was...



PADRE:  [Crosstalk].



STEVE:  Yeah, well, yeah, huh.  So, and at the time of their  reporting, the EFF found that this ComputerCOP software was being distributed by 245 different agencies in more than 35 states in the U.S.  Okay.  So what's the problem with ComputerCOP?  Well, among its several features, first of all, it's just - it's not great software.  It's the kind of thing that Mom and Dad install on their computer that proposes to allow them to find out what images their children are using, except it turns out that it only knows where the images of IE and Safari are located, so it doesn't find any images if they're using any other browser.  And it does no filtering of them, showing all of the icons and favs and all the little Like buttons, I mean, it's a very, very poor piece of software - poorly written, poorly designed, basically just a scam.



So, I mean, it sort of does what it purports to, but it's not anything anyone would want to use, so it's worth what you pay for it, or what Mom and Dad pay for it, which is nothing because it's just being given out by law enforcement, being called the first line of defense against the evil, the dark web, the problems on the Internet.



Well, among its features is a keystroke logger, which records the keyboard activity for all of the computer's users; stores the entire keystroke archive, unencrypted, on the systems hard drive, meaning credit card numbers entered, passwords entered for all the sites everyone on the computer visits.  Account names, everything.  Every keystroke.  When it encounters a keyword which might be questionable, a trigger word, it emails the unencrypted - unencrypted - stored archive to a central server, which then emails it back to the household's parents.



PADRE:  That's such a good idea.



STEVE:  All in the clear.  All in the clear.



PADRE:  Okay.  So let's break this down.  This is essentially - it's not quite a RAT, it's not a Remote Access Tool, but it's pretty dang close.



STEVE:  It's an auto RAT.



PADRE:  Yeah, it's an auto RAT.



STEVE:  It's an auto RAT.



PADRE:  So it sits there, and it keylogs everything.



STEVE:  Yes.



PADRE:  And then upon receiving...



STEVE:  Because you never know, you never know what nasty words someone might type.



PADRE:  Precisely, precisely.  And then upon the receipt of some sort of trigger word which says that I want a list of everything, it doesn't just provide it to you.  It mails it, unprotected, unencrypted, in cleartext...



STEVE:  To a third party.



PADRE:  ...to a third party which then mails it back to me.



STEVE:  Yes.



PADRE:  In unencrypted, unprotected format.



STEVE:  Yes, yes.  There's email containing...



PADRE:  That's a good system.



STEVE:  ...all the usernames and passwords and credit card numbers and everything that's been entered into the computer's keyboard.



PADRE:  So essentially anyone listening to the line would just receive a cleartext stream of everything that you've typed in your computer.



STEVE:  Exactly.  Anybody doing a packet capture, anyone watching the Internet is going to see, in cleartext, all of the private keyboard activity on that machine.  So to increase the tastiness of this for law enforcement, it was at the time that the EFF brought it to the world's attention being marketed with invalid endorsements, purportedly from the U.S. Department of Treasury, which has since issued a fraud alert over the document, which the publisher of ComputerCOP doctored.  ComputerCOP's publisher claims an apparently nonexistent endorsement by the American Civil Liberties Union, the ACLU, and an expired endorsement from the National Center for Missing and Exploited Children.  Even if those endorsements were legitimate, they're not backed by any responsible investigation into the design and operation of the product. 



That was three years ago.  Today, the EFF is able to report that the U.S. Treasury Department, speedy folks that they are, have concluded their fraud investigation into this ComputerCOP, quote, "Internet safety" software.  The Treasury Department did find that the company had forged documents which it had been using as marketing material for its products, and that those forged documents were instrumental in the sales of the software to U.S. law enforcement agencies across the country.  



PADRE:  That's where you get in trouble, yeah.



STEVE:  However, the three-year statute of limitations had since expired.  And the forged documents were no longer present on the company's website or marketing materials.  So there's no penalty that can be brought against these cretins who are publishing this.  Even today, the software continues to be sold and offered free of charge to the public where the software is promoted to the law enforcement agencies as "perfect election and fundraising tools."



PADRE:  Well, there's different levels of meaning for that because, if you want to spy on people, yeah, that's a great tool.  The amazing thing about this, well, a few things.  One is that they would use the ACLU as one of their fraudulent endorsements.  I mean, the ACLU is basically a pile of lawyers.  Why would you ever do that?  That's just stupid for self-preservation.



The second thing is, and this bothers me, Steve, yes, the statute of limitations is over.  But they were going to get in trouble, not for the fact that they created horrible, horrible software, but for the fact that they marketed it wrong.



STEVE:  Correct.  Yes, I know.  In the EFF's follow-up just a day ago, they wrote:  "In 2017" - that is, this year - "the Lake County Sheriff's Office in Florida purchased 1,000 copies for $5,975" - by the way, this is using public funds.  This is funds from evidence recoup slush fund or something.  So nearly $6,000 was spent for a thousand copies of this crapware, according to SmartProcure, which is an oversight database.  And McGruff the Crime Dog was handing out copies...



PADRE:  Oh, not McGruff.



STEVE:  McGruff has been subverted, "...handing out copies" - in his hot little dog suit - "this summer at a community screening of the film 'Elf' in Islip, New York."



PADRE:  They dragged McGruff into this, Steve.  Now, that's fighting words.



STEVE:  You can't make this up, Padre.  You can't make this up.



PADRE:  I actually kind of want a copy of it now.  I just want to see [crosstalk].



STEVE:  Ooh, I'm sure you can find one.  I bet you can find one.  The takeaway for our listeners, of course, and for anyone they know, is to stay as far away from this ComputerCOP spyware as possible.  What a catastrophe.  So thank you to the EFF for, you know.  And no thanks to the Treasury Department for not moving quickly enough to slap these people with a fine that would knock them out of existence forever because this crap should not be provided anywhere.  



PADRE:  There's something larger here, Steve, and I actually saw it over the break because I went to be with my family, my parents, in Las Vegas.  And I basically had to nuke every machine in the house, aside from the Chromebooks that I had given to my Mom and my Dad.  And it's because my father kept downloading software.  And he's like, "But I paid for this.  I paid for it."  I'm like, "Dad, just because you pay 10 bucks for a piece of software doesn't mean that it's good."  And there was every kind of malware installed on these things.  And this is not a singular thing.  This is a particularly bad example.  But there's software out there that, even if it isn't straight up malware, is just so poorly coded that it doesn't belong on a computer of anyone that you like.



STEVE:  Yeah.



PADRE:  Well, I think it's back to the "I'm locking down your computer so you can't install anything without me."



STEVE:  Yeah.



PADRE:  Oh.  I don't want to do that.



STEVE:  Okay.  So...



PADRE:  Give me some good news, Steve.



STEVE:  One happy piece of good news.



PADRE:  There we go.



STEVE:  Then we get on with quicker bits.  And that is something interesting.  Firefox is planning to flag sites that have been hacked in the past, ever.  Working with Troy Hunt, who we often speak of - he has the HaveIBeenPwned site and database, which currently has a list of 254 websites that have had their users' password databases maliciously exfiltrated in the past.  A new feature planned for Firefox will check any sites Firefox's users visit and preemptively warn them when they're visiting any site that has in the past suffered a breach of its users' secret data.  So, for example, you go to Yahoo.com.



PADRE:  [Buzzer sound]



STEVE:  There'll be some sort of a warning, just telling you, oh, by the way, this website has had its data lost.  And naturally breached websites won't like this.  But users will.  And the news of this has been greeted with some happiness on the part of users.  And of course we could hope that the specter of this kind of long-term persistent shaming may serve to keep companies more focused upon not being added to that list in the future.  It's not clear whether there will be a date or a number of - like what additional information will be made available, what sort of metadata might be included.  But Troy has confirmed that he's working with Mozilla, and they're in discussions about how Firefox can pull from his database to keep itself current and just let people know if they visit a site who at some point in the past has not been sufficiently secure.



So it's interesting.  We're seeing, as web browsers become more centric to everyone's experience, I mean, they're now the application platform that many people use with Office 365 and everything being - and all of Google's stuff being cloud-based.  We're seeing browsers being increasingly proactive now in coming up with useful security and privacy features for their users, which I think is just all for the best.



PADRE:  The thing about this, though, is I have seen a complacency among users when they get the warning window, be it whatever browser it is, that this site is not secure, it's not using HTTPS.  And they've gotten desensitized to it, so they just click through, click through, click through.



STEVE:  Yup.



PADRE:  They need to do something different for this because this is not a warning in that sense.  It's not saying, hey.



STEVE:  And it does not stop you from having access.  It's just  an advisory.  Probably a little shim up at the top across the top of the page that we sometimes see browsers offering, just saying, oh, by the way, you should know.



PADRE:  I think that needs to be something that, if you're a security professional today, and you design products, you need to start looking at UI a bit more because you can give all the great information to the user, but if they don't actually understand what you're trying to do, they see it as just a nag screen.



STEVE:  Right. 



PADRE:  And they'll click through it just like they do when they click through EULAs upon installing software.  But there has to be a little bit of thought into, okay, here's a behavior that I want to encourage among my users.  What's the best way to encourage that?  And it's not just throw something up onscreen that I can click through.



STEVE:  Right.



PADRE:  If someone goes to the Equifax website, there needs to be something that's very quick and to the point that says "You may not want to trust this entity."  Because you mentioned that maybe it's time for these companies to feel a little bit of shame.  But I think you also have to do the opposite.  There has to be sort of a carrot-and-stick approach.  You have to be able to hold up the companies that do it right.



STEVE:  Yeah.



PADRE:  Well, okay.  You know what, that is a little bit of good news, Steve, so thank you.  I do appreciate this.



STEVE:  Yes, yes.  So we've got some errata.  I wanted to thank all of our listeners.  So many people reported that I had, on the Security Now! page at GRC.com, when I started posting the November updates, so November, what was it, 7, 14, 21, and now 28, I forgot to change October to November.  So for everybody who said, "Steve, you got the month wrong for the last three weeks," thank you.  The month is now correct.



PADRE:  You know what, it is always the same month in Gibson world.



STEVE:  That's right.  Things move slowly here, but we try to keep things clean or corrected.  Speaking of corrected, I made a mistake last week that I wanted to correct, thanks to someone posting as CHY, just sent me a short note saying:  "@SGgrc Correction about the role of USCENTCOM, Steve."  He said:  "It's not inherently an 'intel gathering op,'" as I had said last week.  He wrote:  "It's a theater-level combatant command under which there are many, many sections."  So just to correct the record, thank you for that.



PADRE:  Yeah, it's always nice to know the internal organization of that group.



STEVE:  Yeah.  In the past we've talked about, and they've been very popular, Humble Bundles, which are these pay what you want for collections of eBooks of various sorts.  The most recent one was a security suite which had among them some outstanding security texts.  I know many of our listeners jumped on that.



There are currently six days remaining in a Java language-centric Humble Bundle which looks very good.  It's basically all of O'Reilly's Java books.  And, I mean, just it's a phenomenal - if Java is a language which impacts you, and in fact I'm surprised at how successful Java is, especially in the enterprise, it's because it's based on a VM that is then able to run on cross-platform.  It's a write once, run anywhere.



I mean, it's taken a bunch of hits from us on this podcast in the past because it was a source of many - the Java Virtual Machine was a catastrophe from a security standpoint, and Java itself should have never been given a web-facing presence.  But as an implementation language for non-web browser solutions, I think it's the leading language for enterprise implementation.  Anyway, this Humble Bundle is very inexpensive.  It's just a broad sweep of O'Reilly books.  I've got the link in the show notes.  Six days remaining as of today.



The second is a science fiction Humble Bundle that has eight days remaining.  And again, a huge number of eBooks available.  I saw one author who I'm a fan of, Reynolds, who writes sort of dark, creepy sci-fi; but I've read a couple of his, and they're just kind of interesting, but good.  Alistair Reynolds is the guy I'm thinking of.  Anyway, so for what it's worth, if you just like - I think you've got it pulled up there, Padre.  So it's like, what, for $15 or something, it's just a big collection of sci-fi.  If someone just wants a big input of new eBooks, I wanted to bring it to our listeners' attention.



PADRE:  No, I love the Humble Bundles because I tend to stay to the same sort of themes of sci-fi.  Like right now I've been big into the Monoverse.



STEVE:  Okay.



PADRE:  And making a jump to the next and the next and the next is not always comfortable for me.  In fact, I'm a little weird in that I will read and listen to the same book over and over and over and over again.



STEVE:  Well, in that case join me because I'm currently rereading my favorite series, which is the Frontiers Saga, which is now the first set was five, and the sixth book is now published in the second set of - I'm sorry.  The first set was 15.  And now we're six books into the second series.  And so I'm rereading - I immediately started rereading them all.  They are so good.  So, yeah, I'm with you.  I reread books I really enjoy just because I love the experience of suspending my disbelief and getting completely absorbed.



PADRE:  I do the Whispersync for the Amazon ecosystem.  And I've got to - I love it.  I did that with the Expanse series.  And I've probably listened to the entire Expanse series at least five times, maybe even six times.  I just keep going through.



STEVE:  Yup.  And of course I read all the books, and they are fun.  And the series on Syfy, in fact we do have a question coming up in our closing the loop is about that.  But I found just - we've been talking recently about bitcoins.  And I have got a good friend, Mark Thompson, who is a miner of bitcoin and who is in fact considering moving to a different state because he's found a huge warehouse and is planning to invest a million dollars in mining hardware in order to mine.  That's something he's doing.



But what's happening with this phenomenon is interesting.  There's a U.K. site, PowerCompare.co.uk/bitcoin, that has pulled some statistics together.  For example, bitcoin mining in aggregate is now consuming more electricity than 159 countries, including Ireland and most countries in Africa.  So, yes, admittedly, they're not massive countries.  But still, I mean, bitcoin mining power consumption is a thing.  And just to give people some other little bullet points, in the past month alone bitcoin mining electricity consumption is estimated to have increased by 30%.  In the past month it has grown by 30%.



PADRE:  That's unsustainable.



STEVE:  Well, yes.  And if it keeps increasing at this rate, bitcoin mining will consume all of the world's electricity by 



February of 2020.



PADRE:  Oh, good, yeah.



STEVE:  Which is when we become a black hole and gravity exceeds our ability to pull away.



PADRE:  Who knows?  I mean, maybe this will spark a new gold rush for power generation.  Someone will finally come up with some very cheap and very plentiful power so they can mine.



STEVE:  Well, the estimated annualized global mining revenue is now at $7.2 billion U.S.  Whereas estimated global mining costs is at 1.5 billion.  Which means there's some profit in that.



PADRE:  Yeah, there's still money to be made.



STEVE:  There's money to be made if you're the person driving the coin up the curve.



PADRE:  But the money to be made, though, is not in the mining.  It really isn't, not anymore.



STEVE:  Right.



PADRE:  The money to be made is just having the bitcoin and watching the price go up.



STEVE:  Yes, well, yes.  Just incredible.  Incredible.  



PADRE:  We've touched on this before, though, Steve.  Because, I mean, there's actually a cost to running the blockchain that is not associated with mining.  Mining is a huge part of it.  But it actually does cost energy for every transaction that you do in the blockchain.



STEVE:  Yes, yes.



PADRE:  And you could say the same thing about the Internet.  A lot of people will look at their phone, and they'll say, "Oh, this is a very low-power device."  Yeah, but everything that it's connected to to make it useful, that uses up a lot of power for all they'd capture.  



STEVE:  Right, right.



PADRE:  I don't know.



STEVE:  So I didn't know I was going to have you here when I had a tweet from a Chad Emory, who tweeted to both you and me.  He said "@padresj @SGgrc," he wrote, and this was - I just ran across this in my Twitter stream in the last, well, yesterday when I was pulling things together for the podcast.  He says:  "I keep hearing of people using SpinRite on Android phones."  Of course you famously did, Padre.



PADRE:  Right.



STEVE:  And he said:  "Standard build and MS boot build will not see phone on USB.  Is there a trick on how to get it to work with Samsung phones?"  And I didn't know I was going to have the benefit of your experience, which you've shared with us, about using SpinRite to successfully recover a phone.  But I did want to tell Chad that the trick is that until - and I'm not sure which point release it'll be.  Probably .2 or .3, that is, 6.2 or 6.3, because the first .1 release will solve the - it will remove SpinRite's use of the BIOS for AHCI controllers.  So allow it to operate at the hardware level, but probably not, almost certainly not USB at that point release.  But probably .2 or .3 it will then understand the USB hardware controllers, as well.  So then it won't be necessary to access USB through the BIOS interface.  Today it is.



So for that to work, because the BIOS doesn't understand plug and play, that is, you're not able to hot plug a USB as you will be once SpinRite is enumerating the USB bus on the fly, as it will be under .2 or .3, the secret is to have the phone powered up and connected to the computer when the hardware boots, not just the OS, but the BIOS itself because the BIOS comes up and looks at the USB ports and assigns them, technically it's INT 13 presences or appearances, which then SpinRite is able to see in order to run on the USB device.  So it has to be a smartphone which is able to appear as a drive, but it also has to be present when you power on the PC in order to start SpinRite up.  And if you achieve those two things, you can run SpinRite on a smartphone. 



PADRE:  There are a couple of other things that you can do to help along.  Turn off all encryption.  So if you've encrypted the storage, which you should, turn that off because I know at least on the OnePlus phones and the Samsung phones, when you do that, it requires an additional driver layer because that's what it's going to unencrypt.  That's what allows your PC to act as a dumb USB device.



STEVE:  Ah, right.



PADRE:  So turn that off.  Secondly, it worked on my OnePlus One, but that was way back when my OnePlus One still had three versions of software ago.  That allowed it to be seen as a plain stupid USB device.  It was cheap storage.  They took that out.  So if you updated the software, they don't do that anymore.  And I haven't tested trying to go back to a previous version of the software.  I would assume it's probably not going to work.  So right now it's kind of hit and miss.  But as you mentioned, when you update SpinRite, that hopefully will help.  Because it really did help.  That OnePlus One was on its last leg.  I couldn't get anything to work properly.  It took forever to start applications.  But after I did a Level 2, which is basically trash collection, it sped it up for about three or four months before it started slowing down again.  And I just realized the memory on this is just worn out.



STEVE:  Is just getting worn out, yeah.



PADRE:  Right, Steve.  Let's close this loop.  You know what, anytime we've had an episode together, we've never been able to do this because we always run really, really long.  So I'm kind of happy we can finally do this.



STEVE:  So it was nice that we had a low security catastrophe week, relatively.  So last week I turned our listeners on to the recently out of beta Quad 9 DNS service.  It looks really good.  Based on the available points of presence, I think it's 70 current points of presence of the DNS server using anycast, moving to - their plan is to have 180 by the end of 2018.  For many people, it is very fast.  But the main benefit is they are being very proactive about security.  They return an NXDOMAIN error for any sketchy malware domain that could hurt you.



And so the beauty is, if you configure your router to give all of the machines in your LAN 9.9.9.9 as its DNS server, you're prevented from having your browser look up the IP of a sketchy site; or, even if you went to a trusted site that had been compromised, that had some content it was trying to pull from the darkness of the Internet, again, that DNS lookup would be blocked.  So results have been mixed in the first week of feedback, though largely positive.



So, for example, Jason Egan said:  "With respect to Quad 9, do you suggest we utilize any secondary DNS, for example Google's 8.8.8.8, or use 9.9.9.9 solely?"  He says:  "I'm giving Quad 9 a go on my network now and was curious.  Thanks."



And I would say yes.  I heard one report of some sketchy service of Quad 9, but no details.  As we know, nothing works without DNS, essentially, especially now that we're moving to HTTPS.  You're getting a certificate whose domain name must match.  So you can no longer substitute IPs for the domain name because the browser will say I'm trying to connect to this site by IP, which will cause a certificate mismatch; and browsers are really getting huffy about that, in order to protect their users.



So the problem is that, if there's ever a problem looking up Quad 9, systems are generally sticky.  When the primary DNS fails, if the secondary one succeeds, they adaptively learn and will continue using the secondary, assuming that the primary is just not functional.  So if you're relying on Quad 9 for security, giving it a nonsecure backup can cause your security to fail.  On the other hand, maybe that's better than complete lack of access to the Internet.



I guess my advice, if you really want the security, would be not to provide a secondary and see how it goes.  Be alert to the possibility that there might be a failure, and know what that means if it happens.  But, I mean, this is a solid global network, and I only heard one report that I haven't tracked down of some sketchiness.  And it may have been months ago.  It may have been back before it was brought out of beta, and it's just no longer a problem.  My advice, and it's what I have done, is just rely on Quad 9 exclusively and see how that goes.



PADRE:  I was just wondering, I always put a backup.  I'm typically - I'll use Google and OpenDNS.  Those have been my favorite for years.



STEVE:  Right.



PADRE:  So 8.8.8.8 and 208.67.222.222 or 220.220.  And I know that there's a couple others that OpenDNS has.



STEVE:  Right.



PADRE:  Can you think of - is there a way in OS X and in Windows, to determine which one it's using?  Because I can't think of any command line that I have.



STEVE:  No.  The only thing you could see would be if you were looking at the packets, you would initially see attempts to use the primary.  When that didn't respond, then it sends out a bunch.  Actually, when the primary fails, all of the other ones are simultaneously sent to, and then it sticky remembers the first one that responds, until you reboot.  It doesn't make any record of it; but it just, in the software stack, it sets that then as its new preference.  And subsequently you'll only see it asking the secondary or tertiary or whatever DNS did respond.



PADRE:  Yeah.  And actually "PSchops" in the chatroom, he points out netstat might actually show you because it will show the active connections.  So if you pinged something...



STEVE:  It's not a connection, though.  And netstat won't show you UDP.  It'll only show you TCP connections.



PADRE:  So the way I could do it on this laptop right now is I would open up Wireshark, and I would just watch all my line traffic, send out a DNS query and just look at the address, see where it goes.



STEVE:  And that's why I think, until we have any reason to think that the service might be flaky, if you want the security, I'd just put in a primary DNS and see how that goes.



So Dan Kutka sent, he said:  "Switched over to Quad 9 DNS and definitely see better performance than OpenDNS and SafeDNS I was using previously.  A nice addition to my toolkit."



Kevin Sanders tweeted:  "I'm taking it to the 9s.  Even in rural Utah, Quad 9 is kicking it in gear."  And he attached a screenshot of GRC's DNS Benchmark, which I'm sure our listeners know has pretty much taken over the industry.  It took me quite a while to write it.  Let's see, it's got a copyright of 2010, so seven years ago.  But it is arguably the benchmark for DNS on the Internet.  And this definitely shows that, I mean, it's amazing.  He must be sitting on top of one of the Quad 9 points of presence.



PADRE:  Yeah, that looks like an anomaly.  I don't understand how that works.



STEVE:  Yeah, it's freaky because it's in number one with a much faster response.  And then OpenDNS is in second place with one of the ones you mentioned, Padre, 208.67.220.222, then 8.8.4.4 and 8.8.8.8, both of the Googles are in third and fourth place.  And then a whole bunch of OpenDNSes, and then several of the Level 3s.  I think that was NTT, the 129.250.35.251 and .250.  And then below it there was UltraDNS and some Level 3s.  But if this, I mean, if what he gets is replicable, then that's a win.



But not everybody found that.  Tim Grissom, he said:  "In Orlando, 9.9.9.9. is slower by a factor of two than OpenDNS."  And Chris Erice wrote:  "Unfortunately, Quad 9 routes Seattle users to Palo Alto, California," he said, "verified by traceroute.  Sticking with OpenDNS for now."  So I wanted to just say that it certainly matters where you are.  Apparently, if you're in Utah, next door to one of these points of presences, it's the fastest thing ever.  And I heard from several people in Seattle who are listeners who all experienced the same thing that Chris did, which is that there wasn't a local point of presence for 9.9.9.9. in Seattle, so it didn't make sense yet.  And this is why they're saying through 2018 they're going to be expanding to 280 GlobalPOPs, which hopefully will be more useful.



So I would just say use GRC's DNS Benchmark, see how it performs for you, add 9.9.9.9 to the list of DNS servers - you can do that by right-clicking in the context menu in the upper left of the benchmark, and you'll find a surprisingly long list of options which are there for power users that allow you to add servers, save an INI file which is then used by default when it restarts, and compare.  See whether you are lucky enough to be close enough that it's at least at parity with other DNS options.  And if not, you may have to wait a while.  But you can check from time to time.



PADRE:  Right.  And that's why that benchmarking tool is fantastic because it does matter where you live and what provider you're connected to.  Although I would guess that any of these, probably any of the top half of these results would be better than the response you're going to get from Comcast or Verizon because they've notoriously oversubscribed their DNS. 



STEVE:  They're not even on the map.  I mean, literally, they're not even on the list.  You have to dig down in order to find them.



PADRE:  Take a four-hopper over using Comcast DNS.



STEVE:  Right, right.  Justin Alcorn responded by my confusion last week about what 9.9.9.10 was for because I commented that it was there.  In my coverage of this I had picked up that it was unfiltered, that is, 9.9.9.9 would return NXDOMAIN.  Anyway, Justin tweeted that .10 is for research.  So you know if the NXDOMAIN returned by .9 is for blacklist reasons.  So I guess that makes sense.  That is, they're providing you with a differential query, one that is filtered and one that is not, so that you could, if you were a researcher, you could pull from both.  And if you get a different response, then you know something definitive.  And lastly...



PADRE:  Now, I've got this map here, JammerB, if you can show this.  This is old.  This is way outdated, and this was before the IPv4 address exhaustion.  So a lot of this is gone because they've given it back.  But, yeah, there are whole domains here.  There are whole ranges that were reserved in the early days of the Internet that are, quote/unquote, for "research."  In fact, where's 45?  Forty-five used to be all of Interop.  We owned the entire thing.  We had a Class A that was just given to us for our network.  And it had so much space.



STEVE:  And 5, that was the Hamachi Class A that the Hamachi guy used because nobody else was using it.



PADRE:  Yup, yup.  And of course, if you're a good Internet citizen you've returned those so that they can be allocated, although they're now all gone.  All gone gone.



STEVE:  And 9, does 9 show as IBM?



PADRE:  Oh, yeah.



STEVE:  Does that show who has?  Because 9 was always IBM's network.  In fact, that's where we got 9.9.9.9 was they gave up that IP.



PADRE:  Yeah.  So you still had HP's old - DEC's old address range right here.



STEVE:  HP was 14 and 15; right?



PADRE:  HP, yeah, 14, well, part of 14.  All of 15.  Ford had 19.  MIT had 18.  Apple had 17.  Bell Labs had, what is this, looks like 12.



STEVE:  Wow.



PADRE:  Their number is kind of weird on this graphic.



STEVE:  Wow.



PADRE:  Yeah, just it's kind of fun.  Look this up.  This is the old xkcd comic that they made.  Again, doesn't work anymore.  Actually, IBM is 9.  There's IBM.  There's UK Mod, Cable TV - Cable TV?  Hmm.  And then the entire multicast range, which is fun.  You know, I wonder, if I were to scan this again, who is where?



STEVE:  Yeah, it'd be fun to have an update.  



PADRE:  Yeah.  Well, I mean, they can't make a map of IPv6 because that would be huge.



STEVE:  Yeah.  So Nerds On Site are friends of the show.  They were early sponsors back in the day.  And I did want to note a tweet I saw from David Redekop, who said:  "Great Quad 9 coverage.  Just in case you're wondering, yes, of course we have DNSthingy support for it already."  And DNSthingy is a cool solution that those guys up in Canada, the Nerds On Site folks, have created, DNSthingy.com.  And sure enough, they've got a dropdown list that shows OpenDNS Family Shield, OpenDNS Home/Umbrella, and Quad 9.  So using Quad 9 is as simple as selecting it from a dropdown list if you are a DNSthingy user.  And DNSthingy offers all kinds of other benefits, as well, that we've talked about in the past.



David Hay said:  "Hey, Steve.  Have you yet had a chance to test Firefox Quantum and LastPass?  If so, what's your verdict thus far?"  So I've heard some controversy.  I am a fan of Firefox Quantum on my Win7 machines.  It is really fast.  Lower memory consumption, and I like the way it looks, and I'm definitely using LastPass because I can't function without it.  I'm not running across any problems.



I know that there was something about some incompatibilities.  It may be some feature of Last Pass that I haven't encountered.  But for what it's worth, it's working fine for me, and I'm not seeing any problems.  And I imagine, if there was some startup glitch, that they will be resolving it.  I do also remember saying or seeing that LastPass will be supporting the previous plugin technology for quite a while moving forward, even though they're moving now to the standard plugin paradigm that's being adopted cross browser, which I think is all good, all for the best.  There will be overlap, so older versions should still be supported.  But for what it's worth, I've not been having any trouble with it.  And I can't operate without it.



PADRE:  Steve, just a quick addition here.  I just got a note from a TWiT Army, Simon Zerafa, who has learned that the Apple root account bug actually works remotely, as well.



STEVE:  [Gasp]



PADRE:  So it works over VNC and Apple Remote Desktop.  So, yeah, that suddenly went up in priority.



STEVE:  Boy.  Boy.  So you would - oh, my god.  That would mean that, if you were exposing remote login, you can no longer rely on the machine's credentials to make that safe.



PADRE:  Yup.



STEVE:  Boy.



PADRE:  Yeah.  So how about this?  Until you get it patched, maybe turn off Remote Desktop and disable VNC.



STEVE:  Boy, yeah.  So not only physical local access, but even VNC and Remote Desktop.  Boy.



PADRE:  Ouch. 



STEVE:  Yeah.



PADRE:  Now I really want to go try that.



STEVE:  Yeah.



PADRE:  I have an account for my sister's Mac.  Hmm.



STEVE:  So Andrew, who tweets from @ISpaceCab, said:  "@SGgrc On security, could you make a comparison between Telegram and Signal messaging apps?"  Yes.  Telegram is closed source, using a very bizarre, homegrown, untested, and unknown encryption protocol.  Signal is open source, using well-tested industry-standard encryption, with completely documented and very clear additional features added.  I would never use Telegram ever.  Nobody who actually cares about security should.  We've in the past covered well-known cryptographers saying, oh, my god, I mean, of Telegram.  So I know people like it, its UI.  It's candy.  It looks nice.  Fine.  It's good enough because, I mean, if you're using a phone, you don't really have security anyway because phones can have other stuff in them that are intercepting keystrokes and messages and so forth.



So to some degree this is all an illusion, that is, messaging security.  I've said to Leo, if you actually want security, you and who you want to have a conversation with need to strip naked, walk into the middle of Central Park, throw a black blanket over yourselves so that no one has any electronics, and no one can read your lips, and then whisper into each other's ears.



PADRE:  Steve, that's a different kind of messaging that you're talking about.



STEVE:  Yes.  That's where iMessage stands for "intimate messaging."



PADRE:  Also I've heard that Telegram, the way that they encrypt it is they have a hard drive with built-in hardware encryption, and you put that into the program, and it works fine until you replace it with an unencrypted hard drive and [crosstalk] everything.



STEVE:  That's right.  There's nothing worse than homegrown encryption.  We cover it all the time.  It's like, oh, Billy came up with this new cipher; and, oh, it scrambles the bits.  Yeah, and it's super secure because it uses infinite bit cheese or some nonsense.



PADRE:  We just can't show it to you because it's super secret.



STEVE:  Yes.  And Telegram offered a large bounty, I think it was a million-dollar bounty for someone who cracks their encryption.  No one has bothered because it doesn't matter.  Signal, the guys at Signal know what they're doing, Marlinspike and company.  I've covered it on the podcast.  I did a couple podcasts about the Ratchet protocol and the cool things they did.  It's the one you want to use, if security actually matters.  If it doesn't, fine, use Telegram.



PADRE:  That's a great endorsement there, Steve.  Yeah, use Telegram if you don't care about security.



STEVE:  If you don't care about - yeah, yeah.



PADRE:  It's fine.  It's perfect.



STEVE:  Yeah, yeah.  I mean, it's good enough.  It's going to scramble the bits.  Someone is not going to be able to see what you're doing.  I mean, no one has attacked its protocol because what they invented was just bizarro.  They've just got arrows pointing around in circles around in the boxes.  And it's like, okay, this really scrambles stuff up a lot.  So it must be hard to unscramble.  Okay, fine.  But it's not based on any theory.  There's no reason to trust it except it scrambles a lot.  So, fine.



PADRE:  So, Steve, this next one actually I am very interested in hearing your thoughts because the JTAG, the Intel JTAG vulnerability is pretty huge.



STEVE:  So this is Simon Zerafa, who tweeted again.  He said:  "Any thoughts on the Intel JTAG bug being deliberate?  How useful would such a working attack be on any Intel platform to a three-letter agency?"



Now, I got a tweet from an Intel person who said that, in fact, JTAG over USB was a thing.  That is, and I did some digging into this, like digging deeper.  And the argument is that doing a separate JTAG interface is expensive because you need to have dedicated pins on the system on a chip on the SOC.  You need to set things aside.  You need connectors.  You need traces.  You need to open the box, blah blah blah.



So the idea is there's a rationale for JTAG over USB.  I found a company that a couple years ago got a patent on some way of sharing a USB, an existing USB port which was still functional as USB, and simultaneously having it give JTAG access over USB so that you didn't need weird voltages or weird hardware.  You were actually using the USB bus.  I didn't dig into that any further.  We're just going to have to wait, I think, until we learn at this forthcoming Black Hat, I think it's next month, it's in December, exactly what those guys at - I can't remember the name, it's like Precision Security or something like that.  Those guys, they're going to tell us what they found.



Maybe Intel - the only thing that I can imagine is that Intel was being responsible.  They recognized the security implications of exporting the JTAG interface, which is incredibly powerful, I mean, you can stop the processor.  You can single-step it.  You can suck the registers and memory out through this.  I mean, it's unbelievably powerful.  So they must have somehow implemented security like you have to send magic incantations using signed requests and date stamps and replay attack proof.  And hopefully they did a really good job and just made a mistake.  That is, there's just a bug, so that if you give it a packet that's too big, or you flip it upside down and send it in backwards, who knows.  But they must have - hopefully they found an actual mistake, rather than JTAG just being exposed over USB.  



PADRE:  I'm actually afraid of that.  I'm thinking they just figured no one is ever going to be able to figure this part out.  I mean, who would be even looking for JTAG access over USB?



STEVE:  The ultimate obscurity reliance, which turns out to be insufficiently obscure.



PADRE:  No, I'm with you.  I'm hoping that they said, yeah, it was a very complicated process, and we needed to malform certain commands in order to get access for this.



STEVE:  Right, right.



PADRE:  Yeah, I don't want it to be, yeah, they just assumed that no one would ever try to do this.



STEVE:  And we hooked up a JTAG debugger and, "Oh, look, we're in."  Yikes.



PADRE:  That would be a horrible presentation.



STEVE:  So Adam van Kuik said:  "What was the 19-book series you were talking about earlier this year?  You mentioned it a few times on Security Now!, and I believe you said you read it twice."  For the record, it is called the Frontiers Saga.  It is now - the plan is 75 books from the author, with whom I have established a dialogue, Ryk, R-Y-K, Brown.  And it's at the top of my list of my absolute favorite science fiction of all time.  And I've got a long list.  Peter Hamilton is there.  I created a PDF, and I'm going to have to update and put this first.  The first arc is 15 books.  He's now working on his second 15-book arc.  He's six books into the second one, so what is that, 21 books total.  And I am reading them a second time, and I'm now in the second book of the second arc.  And soon I will be reading books I have never read before as I catch up to where he is for my reread.  But I'm just loving it.  So Frontiers Saga, the Frontiers Saga.



PADRE:  Now, are they staying with the same group of characters over the 19 books?



STEVE:  Yes.



PADRE:  So it's not just a universe, it's the actual same characters.



STEVE:  Yes.  It is.  It's future - and that's why this guy's so good.  It's future history, a really interesting theme with good physics, interesting protagonists, people you really care about, really good characterization.  He's created a fabulous world.  And it's not the crazy world with people doing implants and really bizarro stuff.  Just humans, humans set in the future who we care about.  I just love it.



So also, and this comes to something that you were referring to earlier, Padre, Ove Karlsson sent:  "Thank you, @SGgrc.  Got bored on Friday, went scrolling through Netflix, and 'The Expanse' was on one of the cards."  He said:  "Remembering you raving about it on SN, so I gave it a go.  Now a two-day and two-season binge later, I can't wait for Season 3."  So "The Expanse," when I knew it was coming, I read the books, loved the books, and have loved it on Syfy.  It is a topnotch production, which is very unusual for the Syfy channel, which is normally just, oh, my god, how many Sharknados must we tolerate in order to have an "Expanse"?



PADRE:  You know what, if they give me - well, we are getting a Season 3.  But if they give me a Season 4 of "The Expanse," they can make as many Sharknados as they want, and all is forgiven.  We actually had the main author - there's co-authors - of "The Expanse" series, Daniel Abraham.  He was here for "Triangulation."  



STEVE:  Yay.



PADRE:  I interviewed him.  And one of the interesting things that comes out is Ty, the co-creator of - originally "The Expanse" was a board game.  He was building a board game, and he had done all this research to make a board game.  



STEVE:  Wow.



PADRE:  And Daniel said, with all this research, you've got enough here to write a book.  And so they started turning it into a book, an incredible book series.  Now, here's the fun thing about "The Expanse," Steve.  There's a lot of times where I have a beloved piece of work, especially in Syfy, that when they turn it into a series, it's okay.  I mean, it's not great.  It's not what I pictured.  With this, they've cut so many things from the books, sort of combined different elements from the books, but it still feels right.  And that's what you get when the authors are fully part of the project.



STEVE:  A classic one is "Under the Dome."



PADRE:  Yes.



STEVE:  When that was coming out, I thought, oh, cool.  So I read it - Stephen King.  Read the book.  Loved the book.  The series was godawful.  Oh, my lord.



PADRE:  It was horrible.  It was so bad.



STEVE:  Oh, it was just atrocious.



PADRE:  No, but, I mean, if you read "The Expanse" series and then watch the series, they actually build on each other. 



STEVE:  Yes, yes.



PADRE:  And the casting was so perfect.  Every major cast member, and even most of the minor parts, have been fantastically done.  So kudos to them.  If you're not watching "The Expanse," you're missing undoubtedly the best sci-fi currently on.  I haven't been excited about a sci-fi project like this since the end of "Battlestar Galactica:  The Reimagined" series.



STEVE:  I agree.  Okay.  This one's going to blow your mind, Padre.  John Arundel said:  "Mind.  Blown.  Pingfs is a filesystem that stores data in the Internet itself, as ICMP packets going to remote servers and back."



PADRE:  This is great.  Everything's in transit.  It's always in transit.



STEVE:  Yes, yes, yes.  So it's on GitHub, if anyone is interested.  So one of the things we talked about years ago, back in our foundation of computing series, is the difficulty that early computer designers had of storage.  I mean, when you think about it, how do you store a bit?  Now, there's a thing called a flip-flop, which we've discussed, where basically you take two inverters, and you connect the output of one to the input of the other, and the output of the other to the input of the one.  And that's stable.  So that, if the input of the first is high, its output is low, which goes into the second inverter.  So its output is high, which folds around and keeps the input of the first one high.  So it's stable.  If you force it into its other configuration, that is, invert the lines, then that's stable, too.  So that's two inverters cross-coupled, as it's called.  And you could do that with a tube, and they did.



So they had tubes.  A pair of tubes could store one bit or a relay.  You could have a relay that would latch itself, or two relays that were inverters that would also be a flip-flop.  But those are large.  They consume a lot of energy.  And they store one bit.  So storing bits was a huge problem early on.  One of the solutions that was actually developed was a mercury delay line, where it was a large tube - technically a column, but it's on its side - a tube of mercury with an ultrasonic transducer on one side and a microphone on the other.  And the speed of sound through the mercury was used to hold data.  That is, this was like a recirculating acoustic delay line, and the output was fed back into the input.  So there were enough bits in there to be practical.



And so that's what I was put in mind of when I read about this Ping File System.  You know, a ping isn't just an event.  It actually can be a full-size UDP packet or IP packet.  So it can store, what, 1,500 bytes, or is it bits?  1,500 bits.  



PADRE:  It's mostly empty, but it doesn't have to be empty.



STEVE:  Doesn't have to be.  Typically - exactly.  Typically it's just nothing.  But you could store bits.  And you send it off, and the goal of the recipient is to return it to you.  So you send it as a pin query or an echo request, and it sends you an echo reply.  And so you could send data off that you then forget, and you rely on the remote end sending it back to remind you of what that data is.  Essentially, if you were crazy, you could build a file system.  And you would need redundancy and error correction because packets can be lost.



PADRE:  Right, right.  You can lose a packet.  You'd have to send multiple copies of the same packet in different directions.



STEVE:  Off in different directions.  And you keep them going back out again.  And then they'd be recirculating, and so then if you wanted to look for some data, you'd have to wait for the packet with that address to come back to you, get the data out of it, send it back out again.  Maybe you'd do a read-modify-write cycle, which is what we had to do in the core memory days.  So when that comes in, you would get the data, see if you wanted to change it; if so, change it and then send it back out again.  And in theory you could do a file system on the Internet.  Anyway, I knew you'd get a kick out of that.



PADRE:  For those geeks out there who think that this sounds familiar, that's because you've probably remembered this from I think it was Season 6, Episode 4, the 130th episode of Star Trek:  The Next Generation called "Relics," when Scotty was discovered crashed on the outside of a Dyson sphere.



STEVE:  Yes.



PADRE:  He and his co-engineer had holed themselves up...



STEVE:  In the pattern buffer.



PADRE:  ...in the transporter buffer, right.  But because the pattern buffer will eventually lose its pattern integrity, they did a cycle between the dematerialization and the rematerialization cycles back and forth, back and forth.  So I'm not sure why that came into my brain.



STEVE:  That's good.  That's good.



PADRE:  But you're right.  Mind blown.  Thank you.



STEVE:  So JMWhitty said:  "When you talk about Apple versus FBI regarding golden keys, you often do not mention the impact to the non-Apples of the world."  And I wanted to say yes, that's true because among the things Apple has done is to provide, typically annually, these amazing security whitepapers where, while they don't go into code level, they really give a very useful overview of the architecture of their security, enough that we've been able to do podcasts on it several times.  The other solutions are relatively closed.



I talk about Apple because I know enough about the architecture of their security to put the impact of encryption in context that Apple has provided.  But we just don't have - the other manufacturers are just black boxes.  It's like, oh, yeah, we have security.  Trust us.  And so if there were other manufacturers issuing the same level of detail, I'd love to know, first of all, to know more about it, to share it with our listeners, and then be able to create some context.  But we just don't have the information.



Mike Synan says:  "Any suggestion for network firewall hardware not containing IME?  Or how to build an uncompromised network firewall without the hardware hack discussed two episodes ago?"  And of course the answer is a Raspberry Pi, is to do non-Intel.  You want to stay away from Intel.  And you can certainly do a Raspberry Pi on a non-Intel platform.  So Padre, your suggestion?



PADRE:  Well, I would go with the - so SonicWALL uses the Cavium processor, which again is - it's not Intel, doesn't have any Intel architecture.  It's ridiculously fast.  It's used in all of the SonicWALL SOHO, all the way up to their enterprise class.  They just add more processors to get more speed.  They just entered into an agreement with Cray, that's right.  Cray is going to be using Cavium processors in the new supercomputer blades.



STEVE:  My lord. 



PADRE:  Yeah.  So they've done a little bit of development over the years.  You can find lower end Cavium processors in bargain bins now, from the old Dell hardware.  And it's relatively easy to develop.  You can actually...



STEVE:  Are they ARM architecture?



PADRE:  It's an ARM offshoot?



STEVE:  So probably an ARM license.



PADRE:  Right.



STEVE:  And then they did their own thing.  Wow.



PADRE:  Right.  But it's ridiculously fast.  Their silicon is fantastic.  So if you wanted to make a high performance one, that would be my suggestion.



STEVE:  Cool, thank you.  So Kyle from Craig Consulting said:  "Hey, Steve.  If you think watching disk defragmentation is mesmerizing, you should try watching a 3D printer at work.  Very soothing."



PADRE:  Yes.



STEVE:  I think it was last week or the week before, our Picture of the Week had the caption "I Defragged My Zebra."  And you know how a zebra is all covered with black and white.  This showed the front half black and the second, the hind end, all white because of course it pushed all of the color to either side.  And so we had fun with that.  Anyway, so Kyle is suggesting 3D printing.  And, yes, and I'm sure you know, Padre, it is certainly fascinating to watch a 3D printer at work.



PADRE:  This is actually the 3D printer I just tested at home.  This is the Monoprice Maker Select Plus.  And it is, especially if you watch it in time lapse.  I love how this looks because you see it build up layer after layer after layer.  I mean, what kind of a geek wouldn't love just to see that all the time?



STEVE:  Yup.  Very cool.  And finally, Will Springer said:  "On Security Now! you and Leo were discussing the trend and importance of pushing updates to IoT devices to patch firmware vulnerabilities.  Would that functionality create a new vector for a party to push malicious firmware to these devices?  Thank you."  So it creates the issue, but we have the solution.  And coincidentally, we were just discussing a failure in implementation in those 54 HP enterprise-class printers.  It is trivial to do this correctly, and that is, any incoming firmware has its signature carefully tested before it's deployed.



For example, I do this with SQRL.  The SQRL client has a mature, finally mature, automatic update capability.  When it is informed that there's a new version, it downloads the version to a staging area, and it performs an Authenticode signature verification on the newly downloaded code, before that code ever has a chance to run.  By definition, you cannot have software perform an Authenticode test on itself because it could be subverted, in which case it would say, "Oh, I'm fine."



So you have to have a third party look over at the code you're considering deploying and verify it before it ever runs.  The SQRL client does that; an IoT device could do that.  And there's nothing reverse-engineerable about it if it's done right because the IoT devices would only contain the public key which could be used to verify the signature that could only be created by the matching private key that, assuming that things are done correctly, never leave the control of the entity signing the firmware updates.



PADRE:  Right.



STEVE:  So everything, all the technology, all the infrastructure, all the capability is in place for doing this securely.  It's got to be implemented correctly.  HP didn't.  But a light bulb might be able to.



PADRE:  Yeah.  What we find is, as you mentioned, we have the technology.  We know how to make this process secure.  But you have to start using it in the design stage.



STEVE:  Yes, early, early.



PADRE:  The problem is security is often added after everything's been created.  And then you kind of fudge the security in order to make it work properly.  Well, if you know that you need a couple of things - you need a signature, a bulletproof signature.  You also need a way to change the signature in case it gets compromised, so that's something that a lot of these manufacturers don't take into account.  If they lose the magic key, what's going to happen?  So do you have a way to fix that and repair damage that's been done?  And then you just have to make sure that there are no shortcuts to that.  It's relatively simple when you put it that way.  It's just making a manufacturer actually stay to security best practices that tends to be the problem.



STEVE:  Yeah.



PADRE:  Well, you know what, Steve, that's actually a positive note.  I don't know the last time I've ended a Security Now! feeling better about security.



STEVE:  Yay.  Yes, there is hope.  There is a future.



PADRE:  There is always hope.



STEVE:  And we can get there, yes.



PADRE:  We can get there.  Folks, of course Steve Gibson is the genius behind Gibson Research.  He provides us such products as ShieldsUP!, of course SpinRite, and soon coming SQRL.  I know that you've toned down the SQRL talk over the last couple weeks, but I'm still very excited to see what you come up with.



STEVE:  We are getting very close.  It's funny you mentioned ShieldsUP! - 99,993,060 shields tested.  So we are, what, less than 7,000 tests away, so that's about two days at our current testing rate from crossing the hundred million mark.



PADRE:  You could have one of those McDonald's signs.  I can remember when I was kid, I still remember...



STEVE:  Hundred million served, yup.



PADRE:  I saw, like, 10 million served.



STEVE:  Yup.



PADRE:  And then it was 100 million served, and a billion.  Now they don't even do it anymore.



STEVE:  No.



PADRE:  But Steve, can you tell the folks at home where they can find you?  I mean, of course they need to go to Gibson Research.  But where are all of the places they can see the wonderful work that you do?



STEVE:  GRC.com.  That's where everything lives.  We've got a menu at the top, and you can browse around there to your heart's content.  And I can't predict when SQRL will be ready.  It's being tested now by a small group in the SQRL newsgroup.  I'm going to stage it to the entire community that hangs out at the new server, which is a much bigger group, before - we have about, I don't know, we have hundreds of thousands of listeners to this podcast, and I want to make sure it's nailed down before it goes that big.  But I'm working on it full-time, so I'm very excited about it.



And of course SpinRite is, as Leo always reminds everybody, is what pays my bills and gives me the freedom to do this.  So I certainly appreciate everybody helping themselves with SpinRite.  And as soon as I get SQRL off, I'm back to working on the first point release of SpinRite 6, which I can't wait to get back to.



PADRE:  Someone once told me that you're the George R. R. Martin of the security world, where people are just hoping that you sit in a room somewhere and just keep writing, just write and write and write, so we can have what we want.



STEVE:  I love to do it.



PADRE:  Steve, it's been an absolute pleasure.  I know this was a bit of a surprise.  I guess no one told you that we were going to be together.



STEVE:  Worked out great.



PADRE:  Anytime Leo goes on vacation, I am so happy to jump in the booth because this is one of my favorite sets of hours that I do here on the TWiT TV Network.  It's been an absolute pleasure to be here and talk about all the important things that our digital lives require.



STEVE:  Great to be with you, Padre.  And until next time.



PADRE:  That does it for this episode of Security Now!.  Don't forget that we're live here on the TWiT TV Network every Tuesday, 13:30 Pacific time.  Now, Steve will be here to inject you with some healthy paranoia to help you understand the wonderful world of security.  That's for those of us who don't have his genius IQ.



You can find all of our shows here at TWiT.tv/sn, as well as iTunes, Stitcher, and wherever fine podcasts are aggregated.  You can also find high-quality audio downloads at GRC.com, which is also where you'll find everything that GRC offers, including SpinRite; ShieldsUP!; and, coming soon, SQRL.  He's sort of the Szechuan sauce of the security world.  I'm Father Robert Ballecer, the Digital Jesuit, saying that if you want to keep your data into the future, you need to start thinking Security Now!.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#640

DATE:		December 5, 2017

TITLE:		More News and Feedback

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-640.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the long-awaited end of StartCom & StartSSL, inside last week's macOS passwordless root account access and problems with Apple's patches, the question of Apple allowing 3D facial data access to apps, Facebook's new and controversial use of camera images, in-the-wild exploitation of one of last month's patched Windows vulnerabilities, an annoying evolution in browser-based cryptocurrency mining, exploitation of Unicode in email headers, Google's advancing protection for Android users, a terrific list of authentication dongle-supporting sites and services, Mirai finds another 100,000 exposed ZyXEL routers, Google moves to reduce system crashes, a bit of miscellany including another security-related Humble Bundle offering, and some closing-the-loop feedback from our terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about including the demise, long-awaited demise of StartCom; another 100,000 exposed routers to the Mirai botnet; and, already, an in-the-wild exploitation of one of the Windows vulnerabilities patched last month.  Plus the latest on cryptocurrency mining.  Ooh, boy.  It's all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 640, recorded Tuesday, December 5th, 2017:  More News and Feedback.



It's time for Security Now!, the show where we cover the latest security news, help you protect yourself and your loved ones online, and sometimes laugh, laugh ha ha, at the futility.



STEVE GIBSON:  Of even thinking there's hope.



LEO:  Mr. Steve Gibson is here from GRC.com.  First, thanks to Robert Ballecer.  Father Robert filled in last week for me.  I ran out of the studio so we could go to New York for a couple of days; and he did, as always, a great job.  But I'm back in the saddle.



STEVE:  Glad to have you, yup. 



LEO:  Yeah.  And if I had a saddle, I'd be back in it.



STEVE:  Well, and we don't have any big topic, as we didn't also last week.  Nothing really stood out.  There were a couple themes.  I am going to talk about, and I had gotten ready to talk about, the ElcomSoft, what I would have to call sort of a takedown of what they feel are some tradeoffs that Apple has made in the interest of convenience that has really hurt iOS 11 security. 



LEO:  Oh, I'm so glad to hear you say that because I was waiting for your analysis of this.  When we talked about it on Sunday on TWiT, the panel seemed to think, well, ElcomSoft makes money protecting people, and so it's in their interest to kind of spin it negatively.  But Rene Ritchie today on MacBreak Weekly said, no, they're right.  So I want to hear what you have to say.



STEVE:  Yes.  And we don't do that today, unfortunately, because you know me, I want to absolutely have all my ducks in a row.



LEO:  You bet.



STEVE:  And a few of them wandered off.  So I will get them back lined up, and I'm sure we'll talk about it next week.



LEO:  Well, that's one of the reasons we value you, Steve, is you always make sure that you spend the energy to get it right the first time out.



STEVE:  Well, and it's not as if we don't have enough to talk about.  We're going to talk about the long-awaited end of StartCom and their StartSSL certs.



LEO:  Oh, boy.



STEVE:  And also, as I'm sure you did in the previous hour, take a look inside last week's macOS passwordless root account access bug.



LEO:  Another one I wanted to ask you about, yeah.  



STEVE:  And the subsequent, yes, and the subsequent problems with Apple's patches of that, which, I mean, it's like, oh, boy, did they sort of stumble over that, yeah.  Also an interesting question.  I'm not as alarmed, and I know you won't be, as I've seen some of the coverage of the announcement of Apple's intention to allow apps to access the 3D facial data in the iPhone X, which is interesting.  Also we have another example of what I heard you talking about already on the TWiT network, I'm not sure which podcast it was, of Facebook's new and controversial use of camera images.  You guys were talking about Facebook asking people to upload nudes of themselves.



LEO:  Yeah, yeah.



STEVE:  And there's even more.  So we'll talk about that.  Also, in-the-wild exploitation of last month's Patch Tuesday, one of the 60-some vulnerabilities - or, no, I think it was 53.  It was 60-some over on the Adobe side that were patched.  An annoying evolution in browser-based cryptocurrency mining.  Exploitation of Unicode in email headers, who could have not seen that coming?  Google's advancing protection, which is welcome, of Android users.  A terrific list of authentication dongle-supporting sites and services.  Mirai finds another 100,000 newly attackable routers.  Google moving to reduce system crashes that are a consequence of what's happening with Chrome.  Some miscellany, including another security-related Humble Bundle offering that will be certainly of interest to our listeners, and some feedback from those same listeners.  So as you can see, I don't think we're going to run out of things to talk about.



LEO:  We're full of good stuff today.  And of course don't forget our Picture of the Week.



STEVE:  Oh, yes.



LEO:  It's coming up.  All right.



STEVE:  So our Picture of the Week follows on from one that we had three weeks ago that you'll remember, where the zebra was defragged, and so all of the zebra stripes were pushed to one end, and the zebra was now 50/50 black and white.  So this following on from that, we have your standard-looking zebra who's busily chewing some grass, who is being confronted by a zebra with a QR code instead of stripes.  And the zebra with the QR code is saying to the one with the stripes, "Upgrade, man."  Meaning of course that the normal zebra is a barcode, which is old school, and new zebra has QR code.  So thanks to one of our listeners for sending this to me.



LEO:  Love it.



STEVE:  Just kind of fun.  Geeky fun.



LEO:  Upgrade, man.  Yeah.



STEVE:  So StartCom is this Chinese-based certificate authority, a CA based in Beijing.  And we've sort of been following their rocky history for a while.  To remind people, they're the people who were behind the StartSSL certificates.  They opened branches over time in China, Hong Kong, the U.K., and Spain.  Then they began having problems because, as we've often said, there is some serious responsibility that comes with a certificate authority's inherent ability to basically print money, which is what happens when you hand out bits and people pay you for them because they're special bits.



But with those special bits comes some serious obligation.  And if you mess that up, because our entire public key ecosystem is based on trust, if that trust is broken, then your ability to charge people for your bits goes away.  So we've seen multiple failures on the company's end over time.  All StartCom certificates were removed from Firefox in October of 2016.  Then Google's Chrome browser followed suit March of this year, March of 2017.  It was then discovered, due to some website shenanigans which we discussed at the time, that StartCom had been secretly acquired by WoSign in Shenzhen, China, through multiple companies.  I mean, and it seemed really suspicious.  And remember that someone discovered that the WoSign, I guess it was the StartCom site was secretly using WoSign on the backend, and then it was figured out that WoSign had acquired StartCom.  So there was no disclosure of this.  Not disclosing that is a violation of the CAB guidelines, the CA Browser guidelines.  So this was just really making everybody feel uncomfortable.  There were, of course, misissuance problems.



Finally Mozilla and Apple sanctioned the company.  StartCom announced it would be restructured during 2016 by the WoSign parent, which was the Qihoo 360 Group.  They detached StartCom from WoSign that was having its own problems to make it its own subsidiary.  Anyway, the last time we checked in, I remember talking about how weird StartCom's website was.  And I remember, Leo, you sort of agreeing with me on this because it looked like a fire sale.  And it's even up there now.  If you go to StartCom.org, it's "UNLIMITED number," UNLIMITED all caps, "UNLIMITED number of 2-year EV SSL Certificates for FREE - up to 99 domains," which pretty much ought to cover anybody's needs.



LEO:  But don't do it; right?  I mean, well, you'll see why in a second.



STEVE:  Oh, yeah, they're worthless.



LEO:  Yeah.



STEVE:  And also an "UNLIMITED number of 2-year OV SSL Certificates for Free, with multidomain and wildcard."  And an "UNLIMITED number of 3-year OV Client Certificates for FREE," or a "3-year Code Signing Certificate, including kernel signing."  Sounds great.  And actually those offers were there last time we looked, quite some time ago.  So finally, last week, or two weeks ago, on November 16th, StartCom announced its termination of business, which comes as no surprise to anybody except, I guess, somebody - I don't know if you've even been able to pay them for anything recently because they seem to be giving away everything for free.



So they said:  "StartCom has played a critical role" - it in fact gives "critical" a new meaning - "as a Certification Authority in data security and electronic commerce by providing an independent 'trusted third party' guarantee all these years.  Around a year ago," they write - and a whole bunch of our listeners forwarded me the email that they sent out, which is what I'm reading.  "Around a year ago, the majority of the browser makers decided to distrust StartCom..."



LEO:  Out of nowhere.  We couldn't understand why.  What happened?



STEVE:  Yeah, we don't know what happened - "...remove the StartCom root certificates from their root stores and not accept newly end entity certificates" - which is their term for server certificates, the end of the certificate chain - "end entity certificates issued by StartCom.  Despite the efforts made during this time by StartCom, up to now there has not been any clear indication" - quite the opposite, in fact - "from the browsers that StartCom would be able to regain the trust.  Therefore" - yes, the trust has been lost - "the owners of StartCom have decided to terminate StartCom as a Certification Authority (CA)."



LEO:  Although I notice on this very same page they are still offering certs.



STEVE:  Yeah, well, you know, what have you got now, a week?



LEO:  We'll take your money.



STEVE:  Two weeks, maybe?



LEO:  Yeah, till January 1st, yeah.



STEVE:  "From January 1st, 2018, StartCom will not issue any new end entity certificate" - because really, now, what's the point? - "and will only provide validation services through its OCSP and CRL" - that's the two types of certificate revocation.  So it's nice that they're maintaining those for two years starting January 1st, 2018.  And then starting 2020, all remaining valid certificates will be revoked.  Of course, they won't be trusted anyway, so it doesn't really matter whether they're revoked or not because no one is going to care.  StartCom, however, they conclude, "wants to thank all our customers and their partners during these years for their support."  So it ends.  And we've been covering...



LEO:  Steve, do you think they were up to no good, or just inept?



STEVE:  I don't think this was malicious.  



LEO:  They were just inept.



STEVE:  Yeah.  And, I mean, even VeriSign, I mean Symantec, who is much more prominent and a major CA, as we know, they're selling their assets to DigiCert because they, too, arguably, are responsible for the behavior of their subsidiaries, and they allowed a subsidiary to make mistakes.  And so it doesn't matter whether you're some obscure CA in Beijing or you're Symantec, who purchased VeriSign's business.  You really do have to behave yourself.



And, I mean, the good news is that, as we know, there's been an explosion of certificate authorities in all of our root stores, meaning that everybody trusts everybody.  So it's only the threat of this happening.  I mean, and this is bad.  I mean, Symantec's not happy to be selling off their CA business, but they don't have a choice because they're not going to be trusted any longer.  And similarly, StartCom, they were printing money, but they screwed up.



LEO:  Who is their root certificate holder?  Was it the Hong Kong Post Office?  Do we know?



STEVE:  I don't know who...



LEO:  Because don't they have to have...



STEVE:  No, they don't.  They were a root.



LEO:  Oh, they were a root.  Oh. 



STEVE:  Yes, yeah.  So they did not have anybody signing for them.  The only reason you need to have someone signing for you is, for example, when Google was bootstrapping itself, I think they had someone like Global Cert or one of the old...



LEO:  Somebody that's already in the browser certificate list, yeah.



STEVE:  Right, right.  And Let's Encrypt did the same thing.  Their certificates were cross-signed.  They signed them, and they had somebody else sign them.  And then once their own roots propagate out into all of the roots, then they no longer need to be cross-signed.



LEO:  That makes sense because their root could have yanked it.  But since they don't have a root, the browsers have to yank their certification.  Okay.



STEVE:  Right, right, right.  So anyway, so it goes.  And I think so it should go because this, I mean, all the other CAs are watching this and thinking, oh, there but by the grace of our IT staff go us.  So they don't want to go there either because - and this is what browsers have to do.  Otherwise it's nothing.  The system falls apart.



LEO:  It's proof that there's integrity in the system.  "Strengths" in our chatroom says:  "Any sufficiently advanced incompetence is indistinguishable from malice."



STEVE:  Yes.



LEO:  Nice paraphrase there.  I like it.



STEVE:  Yes.  Okay.  So last week, right as we were starting to record this podcast, the news of this seemingly horrific - well, and more than seemingly, but we wanted to be careful - this horrific flaw in macOS High Sierra came out.  And it was like, what?  Our listeners who remember last week will remember that the news was you could simply declare yourself root as your username and leave the password field empty; try it not once, but twice; and be able to log into any High Sierra system with full privileges.  And, yes, it even worked remotely.  So it was like, holy crap.  Apple jumped on this, of course, and attempted to fix it.  I say "attempted" because, as we'll discuss here in a second, that was a little bit of a fumble, as well.



But Patrick Wardle, who is the Chief Security Researcher of R&D at Synack, who maintains his own site, Objective-See, S-E-E, is just interested in security.  He's a macOS guy.  He describes himself in his site.  He says:  "As Macs become more prevalent, so does macOS malware.  Unfortunately, current Mac security and antivirus software is fairly trivial to generically bypass.  Objective-See [his site] was created to provide simple, yet effective OS X [and now macOS] security tools.  Always free of charge, no strings attached.  I created Objective-See to publish the personal tools that I've created to secure my own Mac computer."



So he plowed into this and figured out, I mean, by reverse-engineering the OS, what exactly it was that happened.  It turns out that there was a function, "od_verify_crypt_password."  So od_verify_crypt_password, which is the function you call to have the system stored authenticated password hash compared with a newly created hash of the password the user has just provided.  So it compares the hashes.



Well, it turns out that that function returns its success on its ability to compare the hashes and, separately, whether they actually compared.  So the original code in 10.13 and 10.13.1, it reported, yes, I was able to determine whether the hashes compare.  And the code was using its declaration of its ability to successfully compare them, rather than whether or not they compared.



LEO:  Well, that makes sense.  That would be an easy mistake to make, I think, to say, oh, I got success, and not understand that the success wasn't what you thought it was.



STEVE:  Correct.  So in a different...



LEO:  That kind of makes sense now.  I get it, yeah.



STEVE:  Yes.  In a different register, it was returning the validity of the comparison.  So its own return, the function's own return said the other register's statement of comparison is valid.  And so the original code forgot to check the register which contained the validity.  Instead, it was checking whether that register value was valid.  Which it always was, whether or not the password hashes matched or not.



So what was happening was, if the account was disabled - that is, the root account is not normally enabled at all.  It's offline.  It's disabled.  But as a convenience, if you attempt to use it, and it's disabled, it will enable it with the provided password.  That's why it took two times.  The first time woke it up.  It woke up the root account and gave it a null password.  So then it was ready, but it didn't work the first time.  So then you just did it again.  And now the root account has been wakened up, and it's been given the hash of a null password.  So when you give it another null password, or anything else, it doesn't even matter if the passwords match because this function says, yes, I have successfully determined whether they match.



LEO:  Right.  But it would normally do...



STEVE:  But not that they matched.



LEO:  It had done in previous versions, before it would then allow you to log in, it would say, all right, well, but now you have to provide a password.  And it just skipped that step.



STEVE:  Correct, correct.  So then the little fumble here is that, as you no doubt know, Leo, patches were issued for both of the two current OSes 10.13, many of which were still out in the wild, which had not been yet updated to 10.13.1.  So that fixed everything.  The problem was that what was then discovered, first of all, was that the patch for 10.13.1 broke remote filesharing for some users.  So that turned out to need a little bit of a fix.  And there was a support article, and it didn't require too much.  You had to use a terminal command and issue some magic incantations, but then that was okay.  What was worse was that, if a system, if a 10.13 system was then upgraded to 10.13.1, the patch to that had not been applied to 10.13.1, so it regressed you back to the original vulnerability so that your post-patched updated 10.13.1 was now again vulnerable.  Unless you reapplied the patch, this null root login patch for 10.13.1.  And even if...



LEO:  And rebooted.



STEVE:  Yes.  Yes, exactly.



LEO:  Yeah, the one little thing, yeah.



STEVE:  And even if you did, it didn't tell you that you needed to restart the system, and it didn't make you do that.



LEO:  Whoops, yeah.



STEVE:  So you were still vulnerable.  Yikes.  So, boy.  They got it.  They figured that out, and the word got out, and they stumbled a bit more than we wanted.  But that's what was going on is that there was a function which said "I successfully determined whether you entered the proper password."  But separately, there was whether or not you entered the proper password that the code wasn't checking.



LEO:  Success can mean different things to different people.



STEVE:  Indeed it can.



LEO:  That's the moral of the story.



STEVE:  So the Washington Post got all freaked out over the fact that Apple has announced, and apparently it's already available because I saw a sample app that's doing this.  In fact, Leo, there's a cool-looking app called MeasureKit, which is available for iOS 11.  If you look at MeasureKit.com, you can see, I think on his site he shows you a bunch of samples of this thing.  It's a cool augmented reality app that allows you to point at the wall at pictures and determine whether they're level or not.  You can use it to walk around and do really cool augmented reality measuring stuff.  It's free with, I guess, some things not enabled, or three bucks if you want all the features enabled.  But measure door frames just by moving this little thing around.  It's really cool-looking.



One of the things that the author of this has said he will be supporting is this availability of the Face ID wireframe, the whole 3D modeling of the face.  So this is what's got the Washington Post guy - and the author of the Washington Post article managed to find a whole bunch of other people who are running around in circles screaming that this was the end of life as we know it on Earth.  It's like, eh, okay.  All I'm wanting to do with our listeners is to note that this is a thing.  That is, that an app can say they want access to your camera, and that's all apparently they have to do.  They then get access to the camera.



What this now means in an iPhone X world is that all of the 3D facial mapping, this 30,000 points of 3D grid will then be available to apps.  So they can watch you raise your eyebrows, make a frowny face if you don't like something that the app just showed you, I mean, who knows.  Maybe it's going to create some cool features.  It's worth knowing that, if someone is concerned about privacy, maybe this is an issue.  I think we sort of have to see how that goes.  If it becomes a problem, I imagine Apple will make it more clear, maybe make apps more explicitly say that this is what they want to do, or explain why they want it, or maybe make that expire or time out or who knows.



But anyway, I read through this huffing and puffing thinking, you know, okay, maybe.  But I guess if we were to compare it to the - it's different than Touch ID was because Apple never exported any aspect of Touch ID.  Apps could get a go-no-go, and apps can certainly get that with Face ID.  But now this is certainly much more than that.  So this is Apple exposing the 3D mesh in real-time to the apps of the people who are looking at the phone as they're doing so.  So I wouldn't be surprised if it ends up being used for a bunch of interesting things.  And so we'll have to see if this evolves as a privacy issue.



LEO:  It doesn't release the Face ID data points that are stored in the Secure Enclave.  It only gives a developer access to the routines in the camera.  So it's not that much different, it seems to me, than just giving a developer access to the camera.



STEVE:  Well, for example, someone's already stated that they will turn this into 3D printing data so you can print a mini-me of yourself.



LEO:  But we were doing that before.  You just had to move the camera around.  So now they have that built-in face-mapping camera.



STEVE:  Right.  Yeah, and the argument was that, because this is a big, like a unique distinguishing feature that Apple has, maybe Apple is promoting it and saying, hey, we have this grid, this mesh, this 3D mesh data that we're going to let you have as an app running on the phone.  So, yeah.  It's not clear...



LEO:  You'd want to make that available to developers.  Although it's interesting because Apple didn't make NFC available to developers for the longest time, and that's a minor, compared to the face-mapping, that's a minor...



STEVE:  Well, and even Touch ID.  Remember that was initially - we had to wait for that to get out beyond Apple's own use.



LEO:  But, yeah, you could make that same point.  I mean, my bank app allows me to identify, as does LastPass, with my fingerprint, and now my Face ID.  But that...



STEVE:  Well, but that's...



LEO:  But all they're getting from Apple is a yes; right? 



STEVE:  Yes, correct, right.  A go-no-go.



LEO:  Right.



STEVE:  So anyway, we'll see if it ends up being useful.  It'd be fun to see what developers do with it.  Okay, so...



LEO:  [Crosstalk] security flaw.



STEVE:  And on the topic of faces, Facebook is reportedly testing a new CAPTCHA system that requires users to upload a fresh clear photo of their face to prove the account is not fake and belongs to a real person.  So as we understand it at this point, Facebook is being a little quiet about this because they don't want to provide too much detail to allow this to be bypassed.  But at various points in Facebook's authentication flow, Facebook may require and is experimenting with requiring its users to aim their smartphone's camera at their face to provide a unique and never before seen image of the user.



These new tests were first spotted last week by someone in the U.S. who said that - oh, and our friends at Bleeping Computer picked this up, saying that "Facebook is now locking users out of account features" - this person was a little grumpy - "then requiring that those users verify their account to get back in by scanning an image of their face."  So the text that is presented says:  "Please upload a photo of yourself that clearly shows your face.  We'll check it and then permanently delete it from our servers."  So the new system is designed to catch and block the creation of fraudulent and automated accounts.



And when asked about this, Facebook confirmed that the new CAPTCHA system, well, confirmed the existence of the new CAPTCHA system, saying that its role is to catch suspicious activity at various points of interaction on the site.  So it's automated.  It uses facial recognition and requires the use of unique photos to prevent bots from taking existing photos of random people off the Internet or from other Facebook accounts to authenticate.  So for this to work, Facebook must feel that it has acquired a database literally from scraping the Internet and Facebook of all current photos.  And so you show it something that you're saying isn't online, and Facebook's system says, yup, never seen that, never encountered that picture anywhere, so we think you're you.  It's like, okay.  So be interesting to see how this works.



And then you reported, Leo, on I don't remember which podcast, it might have been the podcast after - it might have been with Jeff and Steph that Facebook had been asking potential victims of revenge porn - I mean Stacey, yeah - of revenge porn to upload nude photos of themselves in anticipation of the possibility that those photos might get loose so that Facebook could proactively block them from being posted by anybody else.  And the company said it would create a hash of the nude photo and use that hash to ban other people from uploading a bit-identical, because of course it has to be bit-identical for the hash to collide and have any value.  And then Facebook initially said that the system was fully automatic, then later admitted that a Facebook employee would be manually reviewing all nude photo uploads.



LEO:  Oh, lord.  And by the way, I want to give credit to Jeff Jarvis because on the fly he said:  "Why don't they just give people the app that generates the hash and let them upload the hash?  Then no one has to see the nude photo."  And I thought Jeff deserves an A+ in computer science for coming up with a better system than Facebook came up with.  I hate - I'm starting to really hate Facebook.  These guys are a problem, I'm starting to feel.



STEVE:  Yeah.



LEO:  This is just terrible.



STEVE:  Yikes.  So we'll see how that goes.  We'll see how many people say, hey, here's me.



LEO:  Here's my nude photos.



STEVE:  That I'm planning to release into the wild.



LEO:  I'm worried, yeah.



STEVE:  So please proactively block it.  How does this make any sense at all?



LEO:  Well, especially since there's a better way that's completely private.



STEVE:  Yes.  And Jeff, I listened to that, and I was...



LEO:  Wasn't it impressive that he thought of that?



STEVE:  It was, yes.  Jeff, you know, you are turning him into a techie, Leo, kicking and screaming all the way.



LEO:  Yeah.  Isn't that awesome?  I was so impressed with that.



STEVE:  So we did have news that hackers have recently exploited a bug that we found out about officially on Patch Tuesday.  And of course this is the problem that we're seeing now is that, because of the fact that many systems aren't being patched, the patches can be reverse-engineered in order to determine what it is they're patching, and then those vulnerabilities can be exploited until they're fixed.  In this case, this was a juicy one.  This was a problem in Microsoft Office which, going back to, well, going back 17 years - 17 years, like before there was the Internet almost - there was DDE, Dynamic Data Exchange, which was an early Windows system which allows applications to share data back and forth.  It was sort of behind the scenes of drag and drop.



When you drag and drop something, you're actually sending a message to another app.  And there's this DDE, Dynamic Data Exchange, which is a messaging system that allows one app to package up some memory and hand it off to another app.  So because it was ever used, it is still present in Windows 10 Creators Update.  I mean, like all Windows ever have had this, and all Office has had this in order to support drag and drop.  That's one of the means for doing that.  So there was always, has always been a bug, which came to light relatively recently and was just patched two weeks ago in the Patch Tuesday, November's Patch Tuesday, last month, where 53 vulnerabilities were patched.  What's happened is that it was immediately jumped on by miscreants who figured out how to create a malicious RTF document having a .dot extension.



LEO:  Damn those miscreants.  I hate them.



STEVE:  Don't you hate them?  They're just...



LEO:  I hate miscreants.



STEVE:  Those pesky miscreants who used DDE, this still-present-everywhere Dynamic Data Exchange, to launch a PowerShell script which in turn runs a component of a pen-testing library known as Cobalt Strike to install malware into a user's machine after they open the malicious document.  They don't have to click on anything or do anything.  Opening the document is enough.  And we've talked about those problems before.  Here is another one.  So it's not wormable, meaning that some user interaction is required.  It can't just be received in email when we've run across those that can.  This one can't be.  But merely opening and observing a document attachment is enough.



So our takeaway is, so first of all, this has been patched.  So you definitely want to make sure that you applied the November Patch Tuesday patches across systems, especially that would have access to incoming email, phishing schemes and so forth.  But also this forms yet another example of why everyone needs to always be cautious and suspicious when opening documents sent via email.  That is, don't, unless you absolutely have no choice but to do so.  So try to avoid doing that, if there's any way you can.  This is fixed.



But the problem is, as we know, there's this window of opportunity between - especially when people, you know, right now we're seeing more people being annoyed at Windows trying to update them right in the middle of giving a presentation.  When I was in Utah attending the DigiCert conference, one of the people opened their laptop, and I could see the screen, which was sitting - it was a Windows 10.  It took, like, and I'm not exaggerating, half the day for him to have access to his machine.  It was just sitting there spinning with the little rollercoaster dots running around.



LEO:  He's very patient.



STEVE:  Oh, my lord.  And I remember saying, oh, look, it's finally back.  Yikes.  Anyway, the point is people are tending to defer updating because it's so annoying, and it can take so much time, and they have other things to do like use their computer.  In this case, fine, but don't read any email or open any docs until you know that your system is current.  And in general the big takeaway is this problem has been there for 17  years.  It allows bad stuff to get into your system and run if you just open a document that you receive in email.  So now this problem is fixed.  We know probably there are others.  So the takeaway is don't allow this to have the opportunity of happening.



LEO:  Wow.  On we go with the newly caffeinated Steven "Tiberius" Gibson.



STEVE:  Yes, recaffeinated.



LEO:  Recaffeinated.



STEVE:  So...



LEO:  Did you lose your place?  What are you thinking?



STEVE:  No, I just can't believe this.



LEO:  Oh, okay.



STEVE:  I'm just thinking, oh, you're kidding me.  Believe it or not, cryptocurrency mining, browser-based, which we've been talking about...



LEO:  Coinhive and all that.



STEVE:  ...has figured out to keep running after you stop your browser.



LEO:  That's really bad.  Now they're using your CPU cycles all the time. 



STEVE:  Oh, lord.  Okay.  So then, as we know, the current most popular browser on the Internet is Chrome.  So it's the biggest target.  We also have talked about how Chrome has designed their architecture so that they have a separate process per tab, or in some cases per window, which gives them some advantages.  The downside is it tends to be significantly more resource intensive.  It burns up memory because you're creating whole processes per tab instance.  But the flipside is you get better isolation and, in some cases, superior performance.



So the recent rise in bitcoin price, and as we know it's dancing around $12,000 now - it briefly touched it a couple days ago, it's 11.7 at the moment - it's created increased pressure to steal mining cycles from, well, pretty much anybody they can.  So of course people visit websites.  And as we discussed, Coinhive has been until very recently - we're seeing some now almost predictable expansion of that.  But Coinhive was the early JavaScript-based crypto mining source.  But an analysis that we discussed a couple weeks ago of Coinhive use demonstrated that it didn't look like it was individual sites, with a few exceptions, that were deliberately hosting Coinhive instances and using visitors' processing power to mine bitcoins or Monero.



LEO:  Actually, it's Monero, yeah, they weren't doing - yeah.



STEVE:  Yes, exactly.  Instead it looked like all of the instances across the 'Net, thousands of sites were all accruing to the benefit of one or two entities.  So it looked like these were JavaScript injection attacks of various sorts.  For example, if a library could be commandeered and mining injected into the library, then all the sites everywhere that were causing their visitors' browsers to download that library would inadvertently be also inducing mining on those visitors' machines.



So anyway, so naturally, as bitcoin or Monero, whatever - I think this may be bitcoin that is being mined in this new instance because it's being hosted now on an AWS, on CloudFront, from Amazon.  And what's happened is the guys at Malwarebytes found websites spawning where visitors went with Chrome because this requires Chrome because of its independent process model.  They're spawning a tiny pop-under Chrome window which hides the mining code under the date of the taskbar on Windows.  So it's literally just barely not offscreen.  And if your Windows UI has transparency set on the taskbar, you can see this little mining window.



And so what happens is you go to one of these sites.  Without you being any the wiser, another instance of Chrome is started under your Windows taskbar.  And so you don't know it.  And you do whatever you're doing.  Maybe you think, gee, my system seems a little sluggish.  Wonder what's going on?  But whatever.  You then close Chrome.  Well, you didn't close the pop-under, which is running in its own process.  And it's deliberately not using 100% of your system because it doesn't want to give itself away.  It keeps itself to between 50 and 60% of your CPU.



So you're not getting, I mean, you are having your processing cycles stolen.  This thing establishes a connection to the cloud, and somebody is gaining some benefit from tying up half and more than half of your system persistently in the background.  There will be the Google Chrome icon on the taskbar for users who know to look for that and understand that it means here's a running instance, not just a click here if I want to launch Chrome.  And of course savvy users who know how to run Task Manager could bring Task Manager up, and you will see chrome.exe with 50+% of the system of your CPU constantly being sucked up.  So the user visits a website, silently loads the crypto mining code.  The CPU rises, but is not maxed out.  User leaves the site, closes Chrome completely, yet your CPU continues running at a little more than half of its use as it continues crypto mining in the background.



So I just, as I'm looking at this, I'm thinking - I sort of stand back, and I just think, what a world that we're living in now.  We have virtual currency, which is a thing, and we're on the Internet.  We're using powerful computers.  It's possible now to visit a site which, through no fault of our own, no insecurities, no vulnerabilities, everything patched up to the hilt, and a chunk of the system that we're using gets commandeered by either a malicious site that we make the mistake of going to, or a good site that's own security may not be up to snuff, or in fact it is an ad network which is spreading this.



So a site is being paid by an advertiser to give it a presence on its page, and that presence allows it to run script to cause it to launch another instance of the browser, which hides itself where it won't be seen and then statically burns half of our processing power in the background until we realize, wait, what's that thing running on my tray; or we reboot, because the system does seem a little slower than usual, in order to cleanse ourselves of this.  And meanwhile, somebody somewhere that we have no relationship with is making some small, I mean, not even a lot because our systems are not optimized for crypto currency mining in this way.  But it's more than nothing.  And that's the world we're in as we enter 2018.  Incredible.



Also, we keep seeing problems when we retrofit things that were not designed from the start into newer technology that we now want.  And of course Unicode keeps giving us problems.  We've had lots of coverage of Unicode being used in URLs to spoof websites, where it's possible for a domain name to contain a cleverly designed set of Unicode so that it looks like PayPal.com, but it's not PayPal.com.  It's another domain with Unicode in its name which is intended to be deliberate because we want to allow foreign language characters, not just the original seven-bit ASCII which is just the normal printable characters on our keyboard, upper and lowercase and numeric and so forth.  We want to allow accented characters and so forth.  So in abusing that capability, miscreants - they're back Leo, the miscreants are back - they're able to register domains with Unicode that look like PayPal.



So, okay.  Turns out that a researcher, Sabri Haddouche is a developer and pen tester - penetration tester - bug hunter and privacy advocate.  In his day job he's part of the security team at Wire.  In his free time he dabbles with his own security projects such as this one.  He figured out how it was possible to similarly abuse Unicode in email headers such that all of our antispam/antispoofing measures would still function, yet a user could receive email that looked authentic - and we might as well keep picking on PayPal because probably - from PayPal.  And no matter how closely you inspect it, and even if you then, like we've talked about add-ons that would dynamically show that the DMARC, which is the state of the art in anti-email spoofing, which uses DKIM and SPF, all of that cryptographic signature stuff, you could have a big green checkmark saying, yes, this is authentic from PayPal.com email, and it not be.



So it turns out way back in 1992 RFC 1342 was titled "Representation of Non-ASCII Text in Internet Message Headers."  The subtitle:  "What could possibly go wrong?"  And in fact, yes, 33 different email products and offerings were vulnerable when Sabri responsibly informed everybody.  And I've got a link here, Leo, in the show notes to a Google Docs spreadsheet which he's maintaining, showing all of the products which are vulnerable.



And unfortunately, iOS, actually all of Apple products, both iOS and macOS, are vulnerable to this display of spoofed Unicode.  Not everybody is.  But it's rather sweeping in terms of the exposure.  If you keep scrolling down, you'll find a link to Google Docs in his coverage there.  So he responsibly announced; and, as of today, because this happened this morning, everybody got at least three months' prior notice.  Eight products have fixed this, eight of the 33, so about a quarter of them.



LEO:  It's pretty much everything I've ever used.



STEVE:  I know.  I mean, it is widespread.



LEO:  Oh, Claws is safe.  I really love Claws Mail.  Okay, good.



STEVE:  Whew.  Especially for this time of year because we wouldn't want Santa to have a problem with his email.



LEO:  No.  But this would be - Unicode would even affect this if you didn't use HTML mail; right?  Just text mail would work.



STEVE:  Yes.  For example, iOS's own email client has a problem because you're able to embed a null into this Unicode string, and Apple stops showing text after the null.



LEO:  Gmail on the web is safe.  Good news.



STEVE:  Yay, good.



LEO:  I would bet that's the most used email.



STEVE:  I think you're right, yes.  And do you know if they were, or did they fix it?



LEO:  I just see no, no, no, all green, across the board.



STEVE:  Oh, wait, wait.  It's listed there because it was vulnerable.



LEO:  Oh, okay.  So they fixed it.



STEVE:  So yay to them.  Yes, they did fix it.  Good.



LEO:  Okay.  Doesn't have a date of fixing, so I don't know.



STEVE:  No.  And...



LEO:  That's good news.  And FastMail, which I use on the web, is also safe.



STEVE:  And Proton Mail also.  They jumped on it and got that fixed quickly.



LEO:  But, I mean, on the web says it is still vulnerable.



STEVE:  Oh, oh, oh.  I thought I...



LEO:  Yeah, see, this is unclear.  So he says "Is affected by Mailsploit?  Yes.  Spoofing, yes."  But then it says "Fixed as of 1 September."  It's unclear.  I guess "no" means never vulnerable.  "Yes" means once vulnerable.  And then you've got to check the date.  This is not very clear.



STEVE:  No.  He's good at finding exploits, but not so good at explaining what it is that is going on afterwards.



LEO:  Not so much.  Yeah, okay.



STEVE:  But interestingly, two vendors, Mozilla and Opera, both said they won't fix the bug.



LEO:  What?



STEVE:  They consider it to be a server-side problem.



LEO:  Well, they might be right.  Like Gmail doesn't have the problem; right?  On any browser.



STEVE:  Right, right.  And another one, which is Mailbird, closed the ticket on this guy without responding.  So anyway, I'm glad to have the research.  And it's interesting, I mean, I guess not that surprising that here again Unicode has bitten us in a way that wasn't - oh, I forgot to also say there are some sites which he also discovered a cross-site scripting hack that can be leveraged in the same way.  So that can be even worse.  And on that page that I have a link to, that he links to from his page and that I have a link to in the show notes, he does also have a column for whether they are vulnerable to various cross-site scripting attacks, which a number of them can be vulnerable to.



Okay.  Google has announced from their blog posting just this month, I think it was on the first, from their safe browsing team, that they're going to start expanding their enforcement of something we covered about a month ago which they called the "unwanted software policy."  They initially sort of announced it, but they didn't explain exactly what was going to happen, the idea being - and we'll remember that they were saying that they were going to start getting proactive about websites whose behavior they didn't like, and that developers could register ahead of time to be informed if Google was unhappy with them in order to resolve this before the hammer fell.  We now are getting more information about what this means.



In the show notes here I have a big picture of what they intend to display, which would pretty much drain the blood out of anybody.  It's this big red screen, big X in a stop sign, Deceptive Site Ahead, saying that attackers on whatever site you're about to visit may trick you into doing something dangerous like installing software or revealing your personal information, for example, passwords, phone numbers, or credit cards.  And the default is to go back to safety, meaning don't proceed.  So you're able to click something if you want more details.  But this is pretty much going to stop anybody who doesn't really know - I'm not even sure if you can bypass it.  It doesn't look like there's an, "Okay, fine, I want to proceed."



So their intention is that, as this big scary banner says:  "Apps handling personal user data or device data will be required to prompt users and to provide their own privacy policy in the app.  Additionally, if an app collects and transmits personal data unrelated to the functionality of the app, then, prior to collection and transmission, the app must prominently highlight how the user data will be used and have the user provide affirmative consent for such use."



So, I mean, these are reasonable things.  Basically, as it has been, apps have been able to just, as we all know, when you install it, it says, well, we need all of the following permissions.  And it's like, okay.  So lots of apps get overly broad permissions from their users at install time and are never accountable afterwards.  Google has decided, okay, enough of that.  We're going to, if an app is doing something beyond its clearly intended scope and function, it's going to have to provide a pop-up and say we want to do these things for these reasons, and the user provide affirmative consent.  So yay to Google for this.  I mean, this is all for the better.



So what's going to happen is that, starting in 60 days, two months from now, this expanded enforcement of Google's unwanted software policy will likely be resulting in warnings shown on  user devices via Google Play Protect, or on web pages that lead to these apps.  Webmasters whose sites show warnings due to distribution of these apps should refer to the Search Console, that is, Google's Search Console, for guidance on how to deal with remediation and resolution of the warnings.  Developers whose apps show warnings should refer to guidance in the Unwanted Software Help Center.  And then developers can also request an app review by Google using the article that Google posted on app verification and appeals.



So basically Google's providing a bunch of tools to keep this from being a big deal, but to begin to enforce apps being more responsible.  And then apps published in Google Play will have specific criteria to meet Google Play's enforcement of this unwanted software policy, which they announced, and we covered at the time, back in August of 2017.  So this is great.  This is Google being proactive, holding apps accountable and just tightening up security for everybody who are using Android and Google Play Store stuff.  And I think this will, if Google is able to do this, probably serve to essentially fence off a lot of the sketchy things that have been going on without any control.



So Leo, you'll remember, because I remember you pulled this up and you poked around at it, when we talked about twofactorauth.org, T-W-O-F-A-C-T-O-R-A-U-T-H dot org, where they had this really very nice, by website type, breakdown of which sites and services offer two-factor authentication.  So that was the first step.  Now we have USB-Dongle Authentication, a similar site showing which sites and services offer the use of hardware dongles for...



LEO:  Oh, I love this.



STEVE:  Yes, yes, yes, yes.  Very cool.



LEO:  I'm all in in the YubiKey.  I just use it everywhere now.  Facebook does, which is great.



STEVE:  Yup.  And so it's www...



LEO:  I can be secure when they're stealing my social graph.  It's great.



STEVE:  Yes, exactly, www.dongleauth.info is the site, D-O-N-G-L-E-A-U-T-H dot I-N-F-O, dongleauth.info, which is a list of websites and whether or not they support one-time passwords or universal second factor, U2F.  And they do the same thing.  In order to help people quickly go somewhere and answer a question, they break it down by category:  backup and sync, cloud computing, communication services, cryptocurrencies, developer sites, domains - I guess that means like domain registrars - education, email, entertainment, finance, gaming, health, hosting and virtual private servers, identity management, investing, payments, remote access, retail, security, and social.  So a complete breakdown there.  And then also they have a separate index by devices.  So you can click on "dongles," and it will show you all of the current one-time password and U2F dongles, and which ones support which of those or both protocols.  So nice piece of reference information for people who are interested in hardware-enforced second-factor support.



LEO:  That's great.



STEVE:  Very neat, yeah.



LEO:  Yeah, that's really great.  Do you do that?  Do you use a YubiKey?



STEVE:  I haven't.  I was...



LEO:  You're going to be doing SQRL soon enough.



STEVE:  Exactly.  And so we'll see if that - hopefully that will get some traction.  I know that Yubico themselves have said that they will be supporting SQRL in their hardware as soon as they're able to.  So we'll see what goes on.  But I do, I'm a big user, as we know, of the time-based tokens.  I think that's very strong security.



LEO:  Authenticator, using Authenticator, yeah.



STEVE:  Yes, exactly.



LEO:  Do you think that's as good as a hardware key?



STEVE:  The argument you could make is that, well, if somebody got into your phone, and they got into the authenticator data, then they could get all of your private keys.  So the authenticator works by having private keys in your phone, which are technically vulnerable.  The advantage of the hardware dongle is the hardware keys exist in it.  And so the idea is it can't be hacked, we hope.  So, yes.  And so I guess the point would be, if you absolutely really desperately needed security, then it would make sense to do that.  But my sense is you're potentially inconveniencing yourself a lot for maybe only a little bit greater security.



LEO:  Yeah.  I mean, your phone becomes your dongle, basically.



STEVE:  Right, right.



LEO:  So that's fine.  I don't have a problem with that, yeah.  And it is inconvenient.  I mean, I keep my dongle, my YubiKey on my keys.  But I'm always reaching for my keys.  And maybe that's a problem because, if you found my keys, you'd have my YubiKey, although I don't know if you'd know what to do with it.  You'd have to figure out who I am, figure out what my logins are, get my passwords, and then you'd have the second factor.



STEVE:  Right.  And now every listener to the podcast knows...



LEO:  Yeah, shoot.  Oh, dang it.



STEVE:  Oh.  Yeah.  Now fake them out, Leo.  Put a dummy YubiKey on your keychain and keep it in your shoe or something.



LEO:  I should do that.  I keep two on there.  I keep two of them on there.  You get to figure out which one.



STEVE:  So we have - and this is going to be, unfortunately, an ongoing topic.  We have the persistent danger presented by insecure Internet-facing routers.  And this is back in the news because last week researchers from Netlab 360 spotted a new publicly available Mirai variant.  You know about the Mirai worm, which was causing huge havoc earlier this year.  So an update in the Mirai malware has allowed malware to spread to another hundred thousand networking devices made by ZyXEL.  Is that how you pronounce it, ZyXEL?



LEO:  I say ZyXEL [zy-cel].



STEVE:  ZyXEL, that's probably it, ZyXEL.



LEO:  Z-Y-X-E-L, yeah.  They've been around forever.



STEVE:  Thanks for making a pronounceable name.  Oh, yeah, they have, and I have liked their stuff.  I think I have a couple of their dumb switches.



LEO:  Remember they used to have modems, really the best, the best.



STEVE:  Early, yes.  



LEO:  In the USRobotics phase.



STEVE:  So on the end of October, actually on Halloween, October 31st, a new exploit was posted that allowed remote access, unauthenticated remote access to a class of these modems.  Over the course of just 60 hours, starting on November 22nd, when this was deployed into Mirai, nearly 100,000 new devices, which almost without exception had IP addresses in Argentina, suggesting that they were all provided by an ISP, like a single ISP would say, here, take this fabulous router in order to use our services, were commandeered by Mirai.  It's a well-known  CVE.  It's 2016-10401, which explains that affected devices all share the same fixed superuser password, allowing, not surprisingly, remote attackers to obtain root access when a non-root account password is known.  And that's also defaulted.



So this appeared on October 31st.  A couple weeks later Mirai gets it.  And now 100,000 new devices are under the control of Mirai.  Now, as it happens, the two domains that the attackers were using to control these newly infected devices were seized and quickly sink-holed, which had the short-term effect of stopping the infection from spreading further and preventing the attackers from using the hijacked devices to cause Internet outages.  Remember that's what Mirai did was it was a massive DDoS bandwidth attack that was aimed at various targets.



But those 100,000 Internet-connected devices remain insecure.  So they are still susceptible to takeover by any other Internet worm that may wish to take up residence in them.  And unfortunately, this is where we are today.  We have a large inventory, and probably a growing inventory, of insecure and persistently insecure, well-connected routers which are going to be hosts for worms and service attack platforms.  I don't know how we get out of that situation.



So various classes of software have been injecting their own code into Chrome in order to provide features like accessibility features, but also AV software has been doing this prevalently.  The problem is that, in literally sinking their hooks into web browsers through a process known as code injection, they tend to destabilize the browser.



Google's telemetry has observed that over 15% of Chrome users running code-injecting third-party applications on their Windows machines are experiencing crashes of Chrome, which is causing them a great deal of trouble.  They don't know that it's because they've got some flaky AV that hooked itself into Chrome in order to protect them from stuff that they're downloading.  They just know that their system crashed when they were using Chrome, and they're not happy.  And of course they blame Chrome, where in fact it's actually a third party that is running in-process stuff.  So it induces significant instability, and Google has pretty much had enough of it.



So last Thursday in the Chromium blog they announced their plan to roll back and eventually block third-party software from injecting its own code into Chrome.  And they're going to, of course, as Google always does, they're going to do this in a staged - announce it and try to keep anybody from being inconvenienced, but yet the hammer's going to fall.  So in April of next year, 2018, with the release of Chrome 66, Google will begin informing users if code injection causes their browsers to crash.  In other words, Google's going to first create some accountability here so it just doesn't, like, oh, Chrome crashed, darn it, and the responsible party is not identified.  So they're going to alert them with the name of the responsible application and a guide to update or remove it, which they can do because they've been collecting telemetry on these apps.  They know now who the culprits are.



So then, a few months later, in July of 2018, with Chrome 68, they're going to start blocking third-party software from injecting code into Chrome processes.  But if this blocking prevents Chrome from starting, the browser will restart and allow the injection.  So again, turns out that there are some situations where the third-party add-on software can have hooked things in a way that Chrome becomes dependent upon it so that, if Chrome then says we're not allowing you to do an injection here, Chrome will have a problem.  So Google will watch that.  If it can't start, then it will allow the injection but display a warning guiding users to remove that particular software.  So Google's really going to push back.



And then finally, one year from now essentially, January of 2019, with no exception, starting with Chrome 72, Google will completely block code injection from any third-party software.  So basically any AV, any accessibility solutions or anything else which is using code process injection in order to change, enhance, protect, whatever the reason, in Chrome, that's going to be impossible.  Chrome's simply going to not allow that a year from now.



However, in their announcement, Google notes that there are safe means for achieving the same things using the native messaging API that Chrome offers, or just having the application create a standard Chrome extension to add functionality to the browser.  The user would then need to install the extension or the installation of the feature, or the app, the AV, whatever, would have to take them through doing that.  So it makes it apparent.  But what this does is it then creates a sanctioned means for apps to add functionality to the browser, rather than the third party just autonomously reverse-engineering hooks which they then insert into Chrome and create instability in the process.



So again, I think this is the right way to do it.  It's unfortunate that this wasn't blocked from the beginning because then applications would have always had to do it the right way.  Chrome is saying, okay, we're going to force this now because this is causing too many problems; 15% of Chrome users are having this happen.  That's big.  That's a large number.  So Google is just saying, nope, we know who you are.  We're going to warn people in April.  We're going to get stronger in July.  And we're going to shut it down completely a year from now in January of 2019, which I think seems like the right thing to do.



I haven't talked a lot about sharing a real fun success story about SpinRite for a while, but I ran across one in the mailbag when I was pulling the show together.  Rana Omar, who is in Canberra, hope I pronounced that right, Canberra, Australia?



LEO:  Yeah, Canberra. 



STEVE:  Canberra, with the subject "SpinRite yet again saves the day."  He said:  "Hi, Steve.  Hope you and Leo are well.  Big fan of the Security Now! show.  Love the work both of you do."  He said:  "I thought I'd finally share my story of how SpinRite saved my day."  He said:  "You've probably had enough of these stories by now."  No.  No, no, no, no, no.  No, no, no.  Love, love, love them.  Keep them coming, please.  They always make me feel good, and I love - and then they're all different in various ways.  And this one is, too.  We haven't had one like this.



He said:  "After listening to you mentioning other SpinRite users' feedback all these times, I thought I knew the ins and outs of the whole process; but, boy, I was wrong.  I've had a single 3TB NAS drive which I had been using for years with all sorts of data on it."  So that's Network Attached Storage, NAS.  "One day the drive suddenly died, and I thought to myself that me being the SpinRite expert," he says, "after listening to this podcast, this will be walk in the park for me to restore.



"In Windows OS, the drive would show as MBR/RAW" - which is not good.  That means Windows doesn't recognize it.  And he says:  "And SpinRite wasn't completely happy with it, either."  So that sounds like there was some spooky stuff on the drive.  He said:  "I can't remember the exact message.  I decided to pick up and scan the drive anyway.  SpinRite encountered trouble that caused it to stop before it was completed.  So I came to terms," he said, "with the failure to complete the scan message, and thought the drive is never going to be brought back.  I let it go for few months and bought a new NAS [Network Attached Storage] with multiple drive bays.



"I realized after a few weeks ago that you had mentioned a similar scenario.  Suddenly it dawned on me.  I sprinted towards my PC to plug in the drive and, to my surprise, found out the drive was working and allowed me to back up almost all the data off it."  He said, parens:  "(There were few folders which I couldn't back up, but they were not important anyway.)  All this time, I didn't think to plug in the drive on my PC to check.  I'm so glad that I didn't throw it away.



"The lesson here is that, if SpinRite shows you that it has failed to complete the scan, in reality it has done enough magic to bring it back to life.  As always, thank you very much for the awesome product.  Can't wait for the new version.  Keep up the good work.  Thank you."  Well, and Rana, thank you very much for sharing the story.  And that is the case.  We know that the problem is that drives are so autonomous now.  SpinRite works with them to fix things.



But first of all, there's a limit to how much software can do.  As we've been saying, ultimately in this battle for the drive to die, it will succeed.  So don't push it too far.  We know that, had he been running this on SpinRite periodically, this probably wouldn't have happened.  But the message I wanted to share and what I'm so glad he shared was that, if you run SpinRite on it, even if it doesn't seem to have worked perfectly, it very often can have done enough to be very useful to you.  So definitely give it a try afterwards and see if you got enough back.



So we've talked about Quad 9, this DNS provider.  I think it was two weeks ago, Leo, that we first talked about it.  Last week with Father Robert there were many people super excited, some people unhappy that it was too slow for them.  They liked the idea of all the security benefits from using Quad 9, but they just, like, okay, it wasn't as fast as my own testing showed it to be.  I wanted to tell people that there is now a points-of-presence map for the service.  It's at pch.net/about/pops, P-O-P-S, which is short for Points of Presences.  So www.pch.net/about/pops, which people can look at and also follow over time because these guys will be more than doubling their points of presences over 2018, which means for people who are not currently near a Quad 9 DNS server, there's a good hope that'll change in the future.  Also...



LEO:  Yeah, they're all over the place.  That's great, yeah.



STEVE:  Yeah, yeah.  They've got 70-something at release a couple weeks ago, and they're intending to go to 180, so many, many more.



There is a Network Security and Certification books bundle at Humble Bundle that I wanted to tell our listeners about.  It's got a lot of time left, 13 days.  And the books are, as the title, it's network-security-certification-books, all hyphenated.  I've got the links in the show notes.  They're MCSA Windows Server 2012 R2, various study guides for different exams, for the MCSA exams, and also for System Center and Hyper-V.  A dollar gets you five of those.  Eight dollars gets you the CompTIA Network+ Study Guide Exam, Third Edition; Cisco Networking Essentials, Second Edition; Microsoft Windows Networking Essentials; and Data Storage Networking:  Real World Skills for the CompTIA Storage+ Certification and Beyond.  That's all of the first five plus those four for eight bucks.



And then for a total of $15 you get another four:  The CompTIA Security+ Study Guide, Sixth Edition; Network Attacks and Exploitation, a Framework; Network Security Bible, Second Edition; and Phishing Dark Waters:  The Offensive and Defensive Sides of Malicious Emails.  Ooh.  So anyway, I knew that would be of interest.  And those of course eBooks that you're able to purchase.  These are all published by Wiley.  This is a Wiley bundle, whereas the previous was - there was a sci-fi set, and then there was also another, oh, it was the Java.  And those were all O'Reilly offerings.



And, finally, some great feedback from our listeners.  @theNickNiti, Nick is his name, said:  "iOS 11.2 is almost usable on my iPhone 7.  Looks like most glitches were fixed.  Maybe Apple actually QC'd this version."  And of course 11.2 just came out, what, a day or two ago, I guess.  No, I think it was Friday, wasn't it.  And so I've updated all my stuff.  And I caught your talking on MacBreak about the subtle little new things.



LEO:  Like a second bug, yeah.



STEVE:  I hadn't noticed that little gray underline.



LEO:  Oh, that bar, yeah, that was weird.



STEVE:  Yeah.  And the weird spinning...



LEO:  Yeah, the fidget spinner underneath the icons, yeah.



STEVE:  Yeah, crazy.



LEO:  I don't know what that is all about.



STEVE:  Yeah, yeah.  Also the black, @theblack - I don't -  @theblack - I don't know what this - @theblacktandog, I guess is his name.  He said:  "Wow, Quad 9 and Firefox 57."  Which of course everyone's liking the speed improvements of Firefox 57; and so he's saying put those two together, and you've really got something.  Also, big news, thanks to a number of our listeners who sent me this.  I noted here the first person who I encountered, and that was Dan Kutka, who said:  "Quad 9 does have a secondary secure DNS address."



Remember that 9.9.9.9 - and that's the only one I knew about.  9.9.9.10 exists, but it's nonfiltered.  So there is a secondary secure, that is to say filtered, DNS.  It is 149.112.112.112.  So again, 149.112.112.112, that's what you want to use as your secondary DNS behind 9.9.9.9.  And I saw somebody somewhere say that there were some DNS resolvers which always queried all registered DNS.  I had said last week or the week before, my own observation of Windows, and I have studied what Windows does with DNS because of course I wrote the Benchmark, it always issues the primary.  And then, if that fails, it issues all.  And then it remembers who responded first.



I did see some comment somewhere that there were some systems that always issued all and then used whoever came back first.  So it suggests you do want to, if you can, if you want to use Quad 9, use the filtered IPs.  And so for the secondary you want the 149.112.112.112.  And so thanks to everyone for making sure that I knew about that.



Tracy Lipp tweeted:  "I don't think iOS is to blame with instability issues in your phone."  Okay, well, first of all, they're not instability, but okay.  He says:  "I have the same phone.  In Finland it works perfect," he tweeted.  "Just got to L.A., am using T-Mobile, and it's terrible.  Phone is occasionally unresponsive and unstable.  It didn't do this at all in Finland."  So data point, I guess.  That's interesting.  For me it seems to be entirely performance.  Well, okay.  There were cosmetic bugs.  And I heard you refer to them exactly that way in the last podcast also, Leo.



LEO:  So that's what you were experiencing?  Because I wasn't sure based on what you...



STEVE:  Yeah.



LEO:  So it's not showstoppers, it's just ugly.



STEVE:  Yeah.  Although...



LEO:  Weird stuff, though, like [crosstalk].



STEVE:  Yes,  like I've had iMessage balloons like stick on the screen when other stuff scrolls behind them.  And like, okay, whoops.



LEO:  Yeah, yeah, yeah.  Updating issues, yeah.



STEVE:  Yeah.  But definitely seems to be performance.  It's like, for example, and I just went back and checked it after iOS 11.2, if I swipe sideways to scroll through the apps, there's always a pause.  Whereas the phone used to just follow my finger right from the get-go, now if I do a swipe it thinks about it for a while, then it goes, oh, he's swiping, and then it does it.  It's like, okay.



LEO:  Have you tried turning off - going into Accessibility and turning off animations and...



STEVE:  I always turn it off.



LEO:  Okay.



STEVE:  I don't like it at all.



LEO:  There's a lot of cosmetic stuff that would absolutely slow things down.



STEVE:  Yeah.  All that zooming and swooping and all that, yeah.



LEO:  Yeah, yeah, I don't like it.  So you turn that off, and you're still getting these hesitations.



STEVE:  Yeah.  I just think it's old, although I don't, I mean, I'm loving my iPhone X. 



LEO:  Me, too.  Isn't it nice?



STEVE:  It really is a nice experience.  And I like that it's smaller, yet has the same physical presence.  And I didn't think I was going to like the rabbit ears, but I don't care at all.  They don't - I don't even see them.



LEO:  No.  I don't see them anymore.  Well, because a lot of times they're not visible at all.



STEVE:  Yeah, yeah.



LEO:  Screens work around them.



STEVE:  And a last question from Yann Saint-Laurent.  He said:  "On the last episode of Security Now!, you talked about an HP printer that the drive was encrypted" - and I'm just going to quote from him directly - "it gets swapped to a non-encryptable, then back to a FIPS encrypted drive.  If the drive is providing the encryption, how could it now be readable?  Is the private key still in the printer?"



Okay.  So to clarify, this was a very clever hack by the guys that took a deep poke at HP last week.  And Leo, you and I talked about how disconcerting it was years ago when it was discovered that enterprise-class printers that had been taken out of service had had hard drives kind of secretly in them, had all of their print jobs sitting on the hard drives, which is representing a huge privacy concern for the companies that had decommissioned these printers and didn't know there was a hard drive that had financial statements and who knows, corporate future planning stuff, potentially, on those drives.



Well, it turns out that HP uses encrypting drives where the drive itself supports its own native encryption.  And so when the printer boots, the board that the printer is docked onto, the printer's motherboard provides the key to the drive to unlock it.  So these clever researchers realized, huh.  If we pull the drive out in order to suck the firmware out, it's encrypted.  So that gets in our way.  So what they did was they just stuck into the printer a drive that did not support native encryption.  Well, of course it was empty.  But then the printer thought, oh, I must have had a failure of my hard drive.  So they reinstalled the firmware through the USB thumb drive stuck into the printer, and said, oh, here, please repair your hard drive.



So the HP printer sucks in the firmware, loads it on the hard drive, tries to give it the key for encrypting itself, but it doesn't support encryption.  So the drive ignores the key, stores all of the firmware unencrypted.  Then they pull the drive out and mount it on their research machine and have access to all of the firmware.  And as we mentioned last week, unfortunately it's using Windows CE, so they have file systems, and they're able to reverse engineer, and they found an incredible number of vulnerabilities in HP enterprise-class printers.



But so to answer Yann's question, the idea was they did not put back in a FIPS standard encrypted drive.  They put back in a drive that did not support encryption, reinitialized the drive using the printer firmware from a thumb drive, and then pulled that out and had access to it.  So very clever hack by those guys, and hats off to them.  And that's our podcast.



LEO:  Wow.  I hardly can believe it.  We are here at the end already?  Wow.  All right.  And next week we'll talk a little more about ElcomSoft's contention... 



STEVE:  I'm going to do a - that's going to be a, yes, a deep dive.



LEO:  ...that Apple has overbalanced for convenience over security on iOS. 



STEVE:  Yes.  That will be our topic.  I'll be fully tuned up for that next week.



LEO:  Yeah, good.  Because you have been singing the praises, of course, for iOS and its security policies.



STEVE:  Why I really want to understand, yes, what they've done.



LEO:  All right, good.  We'll look forward to that.  That's next week.  You can find this podcast and SpinRite, the world's finest hard drive recovery and maintenance utility, and many other freebies at Steve's site, GRC.com.  That's Gibson Research Corporation, GRC.com.  He also has transcripts, and Elaine should have an easy day today.



STEVE:  Yay.



LEO:  It'll get up there, it takes, what, a few days; right?  Four or five days to get the transcript up.  He'll also have audio, MP3 audio of the show.  We have audio plus video, if you want to see Steve's - he looks just like he does on the T-shirt and the mug, actually.  The moustache really grew back nicely, I must say.



STEVE:  A little whiter.  I think it's a little whiter than it was.  But it's punishing me for ever removing it.



LEO:  How dare you.  You'll find that at our site, TWiT.tv/security - or I guess it's "sn" to save typing, TWiT.tv/sn.  If you want to subscribe, I encourage that because you really do want to download these automatically, even if you don't get a chance to listen every week.  Having each episode is a very nice thing to do.  So find your favorite podcast app and type in Security Now!.  You'll find it.  It's one of the longest running shows on Broadway.



STEVE:  Number two.  It's the second show on the TWiT podcast network.



LEO:  That's absolutely right, yeah.  Let's see.  What else can I tell people?  We do have the Steve mugs and T-shirts in stock at TWiT.tv/store.  They make a lovely holiday gift for the security-minded geek in your life.  I'm just saying.  I'm just saying.  And we will be back here as we are, typically, every Tuesday, 1:30 Pacific, 4:30 Eastern time, 21:30 UTC if you want to watch live at TWiT.tv/live.  You could also join us in the studio, Sharif did, from Vallejo.  Hi, Sharif.  All you have to do is email tickets@twit.tv, and we'll put a - well, actually we don't have to put a chair out.  We've got a lovely Barcalounger for you.



STEVE:  Hi, Sharif.



LEO:  He says hi.  You know there's a handle on the side there.  You can put your feet up, if you just want to get comfy.  He's got like a 25-inch laptop.  That's big.  Is it 17?  Yeah, looks like it.  Big old laptop in his lap.  So he's clearly involved, taking notes as we go.  We love having the studio audience.  But do email us so we know you're going to be coming, tickets@twit.tv.  That'll save you the cavity search that Moe, our front door guard, normally would have to do.  We could prepare ahead for you.



If you can't watch live, again, download it.  If you want to watch live, be in the chatroom, irc.twit.tv, some very nice people in there who can talk you through any questions you have.  They're listening intently.  And we, Steve, at what point are we going to do the SQRL show?



STEVE:  We're close.



LEO:  I hope so.



STEVE:  And we're going to do one.



LEO:  Good.



STEVE:  Believe me, yes, we're loading up on nuts right now.



LEO:  We have [groaning].



STEVE:  Ah, careful.



LEO:  We have decided, I think, on our holiday episode because you know this show airs two days after Christmas, and we don't want to make anybody work on December 27th, so we're going to do a special.



STEVE:  I think I'm having my teeth cleaned that day, actually, so...



LEO:  Well, good.  It's about time, yes.  I think I am, too.  I save it for Christmas every year.  It's easy to remember that way.  Well, I won't - we'll just surprise you with the episode.  It should be a lot of fun.



STEVE:  Okay, yes.  We do know what we're going to do.  People will - it's one of our "blast from the past" episodes.  Not one we have...



LEO:  Not the Dog Killer, no.



STEVE:  No, we're not going that.



LEO:  A very timely one.



STEVE:  It's not one we've ever repeated before.  But it's the right one.



LEO:  It's timely, as they say.



STEVE:  Yes.



LEO:  All right, Steve.  Have a great week.



STEVE:  Okay, my friend.



LEO:  I will be here next week.  I'm sorry to have surprised  you last week.  Thanks to Father Robert for filling in.  But I will be back next Tuesday, December 12th, for another thrilling, gripping edition of Security Now!.



STEVE:  I can't wait.  Oh, wait, I'll be here, too.



LEO:  You can wait, and you must wait.



STEVE:  I will.  I must.  Okay.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#641

DATE:		December 12, 2017

TITLE:		The iOS 11 Security Tradeoff 

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-641.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the details behind the "USB/JTAG takeover" of Intel's Management Engine, a rare Project Zero discovery, Microsoft's well-meaning but ill-tested IoT security project, troubles with EV certs, various cryptocurrency woes, a clever DNS spoofing detection system, a terrific guide to setting up the EdgeRouter X for network segmentation, last week's emergency out-of-cycle patch from Microsoft, a mitigated vulnerability in Apple's HomeKit, Valve's ending of Bitcoin for Steam purchases, finally some REALLY GOOD news in the elusive quest for encrypted email, a bit of miscellany, some closing-the-loop feedback with our listeners, and a look at the security sacrifice Apple made in the name of convenience and what it means.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here, the penultimate episode of 2017, and oh so much to talk about.  We've got lots of security news.  There's an out-of-cycle patch update from Microsoft.  That usually means something's going down.  Some iPhone exploits.  And at the end we're going to address this issue of iPhone security.  Is it getting better, or is it getting worse?  All coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 641, recorded Tuesday, December 12th, 2017:  The iOS Security Tradeoff.



It's time for Security Now!, the show where we protect you and your loved ones and their security and teach you a thing or two with Mr. Steven Gibson, our hero of the hour.  He is the man in charge at Gibson Research Corporation, GRC.com, creator of the first spyware, antispyware tool.  He also...



STEVE GIBSON:  Yeah, make sure you say "tool" and not...



LEO:  Yeah, tool, anti, anti, not spyware, antispyware.  In fact, he's revealed much spyware, has exposed it in many cases.  And he is also a great guy to have on your side in a knife fight.  No.



STEVE:  Well, I'm not sure.



LEO:  In a bot fight, for sure.



STEVE:  Maybe yes, maybe in a hacker fight or something, yeah.



LEO:  Hey, Steve.



STEVE:  So as promised, we're going to finally discuss what we just didn't have a chance to dig into, which is this ElcomSoft kind of rant about a change that Apple made in iOS 11 security which their position is just collapses the whole security foundation and structure that Apple has built.  And so we didn't have a chance to talk about that last week.  I promised we would this week, and we will.



But we've also got now, thanks to Black Hat Europe happening, the details on that much-discussed here USB, or JTAG over USB compromise of Intel's Management Engine.  We now know the details of that.  And remember for weeks we've sort of been scratching our heads saying, well, we're just going to have to wait to find out because it can't really be as bad as it sounds.  And it isn't.  I mean, what they've done is true; but, boy, did they have to jump through hoops.  We've got a rare Project Zero discovery from the Google team.  Microsoft has a well-meaning but ill-tested IoT security project that I want to touch on because maybe we'll hear something from it in the future, but I'm not sure.



Some interesting troubles with EV certificates, which we all know I'm a fan of.  But as they become more popular, of course people are going to start screwing around with them.  We've got various cryptocurrency woes, a clever DNS spoofing, like spoofed site registration detection tool.  A terrific guide appeared for setting up our favorite router, our favorite little consumer router, the $49 EdgeRouter X, for network segmentation.  Somebody did a beautiful how-to walkthrough for how to take advantage of all the power in that cute little powerhouse router.  We have to talk about last week's surprise emergency out-of-cycle patch from Microsoft that our Windows listeners may have noted.  It's like, wait, what?  It's not the second Tuesday.  In fact, today is the second Tuesday.



LEO:  I'm installing it right now.  I just noticed it, yes.



STEVE:  Yes.  We've got a problem with Apple's HomeKit, which thanks to the nature of the problem they were able to mitigate.  But it does mean that we're going to go to 11.2.1, probably this week, it's expected.  Valve announced that they're ending support for bitcoin purchases of Steam stuff, which is sort of interesting.  We're going to talk about that briefly.  Finally -  and this is going to be of big interest to you, Leo, because you're a big PGP proponent; but also really good news for all of our listeners that really want email security, even though that phrase itself is an oxymoron.  But we've finally got some really good news in this elusive quest for encrypted email.



Then we've got a little bit of miscellany, some closing-the-loop feedback with our listeners, and then I've read and reread and reread this ElcomSoft blog.  It wasn't well written, but I think I understand what it is this guy's point is, and I can see what Apple did and that it does really create a problem.  So I think 641, today's podcast, is going to be another goodie.



LEO:  I'm just doing a little math here, multiplying, let's see, it's $17,431.20 for a bitcoin, and you have 50.  You have $871,000 in your wallet.  I'm just curious if you've made any progress finding it.



STEVE:  Haven't looked.  I think I know where.



LEO:  C'mon.  C'mon.



STEVE:  I think I know where it is.  But it's almost better to sort of have it and not know that I don't have it because...



LEO:  Well, yeah, because if you didn't have it, that would be really disappointing.



STEVE:  I did do...



LEO:  All you need, I found out, is the wallet.dat file.  That's where the private key is stored.  It's a small file.  And what I was able to do is install the bitcoin core, the current bitcoin core, get it completely synced up with the blockchain 150 gigs later - unbelievable.



STEVE:  I know.



LEO:  And then take the wallet.dat, the generic wallet.dat it created and move that out, and move my old wallet.dat in.  And it recognized all - I have almost eight, what do I have, 7.85 bitcoin.  It recognized it all.  Unfortunately, I had the good sense - I was half smart - to encrypt it.  You know, it has built in, the bitcoin wallet has an encryption capability, I presume strong encryption, I hope not, but I presume, and then promptly forgot the password.  I didn't put it in LastPass, which tells me it's something I use all the time, like my PGP key.  So I'll find it.  I'll find it.  But I have sitting here, right here in this little laptop, that's a mere $100,000.  You've got almost a million dollars in there.  Will you not look for it?



STEVE:  I've got some boxes in the way.  I'd have to move them. 



LEO:  But you know the machine you used to mine it; right?



STEVE:  Yeah.



LEO:  And you haven't erased that machine.  So it's there.



STEVE:  So what happened was, it was on this little i7 cube that I built for the podcast.  And I just sort of had it next to me, and I left it running overnight, and I woke up, oh, what do you know, 50 bitcoins.



LEO:  Fifty.  Five zero, kids.



STEVE:  Five zero.



LEO:  This was in the good old days.



STEVE:  That was when one successful hash gave you 50 bitcoin.  And it wasn't hard.  I mean, this wasn't GPU.  There was no Freon cooling or steam coming off of it.  It was just I left the machine running.  So then I think I remember moving it to a different machine because I thought, well, if 50 is good, 100 would be better.  So I kind of think I know where it is, and that I just sort of forgot about it.  It's like, okay, fine.  And then that machine stopped, that has like a RAID and a bunch of stuff, it sort of - I turned it on once, and it didn't boot.  And I thought, okay, well, you know, and I have other machines.  I haven't worried about it ever since.  Now I'm kind of thinking about it a little bit more lately.  



LEO:  Steve.  I don't know how well off you are.  But I think even Bill Gates, knowing that there's a hard drive with $871,000 on it, would look for it.



STEVE:  But Leo, look what's happened.  I mean, I'm not in a hurry, either, because this shows every sign of, you know, it staggers a little bit.  It hit $20,000.  Then there was some profit-taking that dropped it down to 15.  Now we're back up to 17 something.  I mean, believe me, I've got the bitcoin ticker on every device that I own.  And every so often I go check in.  It's like, oh, okay, that's nice.



LEO:  So that's actually really, that's the first question is what, if you found it, would you sell it immediately for cash?  No, you wouldn't.  You'd hold it.



STEVE:  No, I wouldn't.  No, absolutely.



LEO:  So there's no urgency.



STEVE:  Yes.



LEO:  No urgency at all.



STEVE:  I'll leave those boxes where they are.



LEO:  Yeah, I kind of thought about that, too.  Now, here's the one thing you might want to do.  There are, as you know, two forks.  There's bitcoin gold and bitcoin cash.  And if you take your bitcoin and put it in an exchange that supports the other bitcoins, you'll get the equivalent amount in their bitcoin.  Which I think one of them is $1500.  So you've got $75,000 just free money.  And I, see, I would - that's what I would do if I got access to mine is I'd get the bitcoin gold and cash and get out of it because who knows how long the forks are going to survive, and keep the main bitcoin.



STEVE:  Well, and of course the moment I realize this in a true currency, I owe the U.S. Internal Revenue Service half of this.



LEO:  Well, that's what I thought.  But a number of people have said, "Oh, Leo, don't be foolish."



STEVE:  Yeah.  I really like not being behind bars, Leo.  Doing the podcast and having bars up in front of my face, that would really put a damper on my security recommendations, I think.



LEO:  Although, I have to tell you, if you are in a prison cell, you might want to get a Ring Video Doorbell for it.



STEVE:  Ah.



LEO:  That way, if the jailer comes a knocking, you know it's  him.  This is not what Ring has told us to say about the Ring Video Doorbell.



STEVE:  There's a segue we will never repeat.  You know that every week we have a picture.  And sometimes we go a little - wander off the beaten path.  But for anyone who's an Amazon customer or has been doing ecommerce, this just tickled me.  And so, though this is not security related, it's just too fun.



So for those who are listening, we have a woman sort of in her middle age, kind of frumpy, with her shopping basket in her hand, and she's standing in the market, looking at the display.  The upper shelf says "Customers who bought this item," and then it shows some bananas.  And then down below, "also bought," and I don't know, looks like a jar of mayonnaise, some carrots, a loaf of bread, and some milk.  Anyway, she just sort of looks bemused.  She's like, what?  And of course all of us are used to the nature of our transactions being electronic and the system knowing us.  And I have to say, sometimes I find that information very useful when I'm trying to...



LEO:  If it were my Amazon account, it would just have five more bunches of bananas below it.  They always recommend stuff I've already bought.



STEVE:  Yes.  Yeah, well, in fact, when the Healthy Sleep Formula was happening, people have stopped talking about it now, although apparently still many people are using it.



LEO:  They're all asleep.



STEVE:  I don't know, about 2,000 visitors.  Yeah.  When they would put one of the ingredients into Amazon, it immediately brought up all of the rest of the ingredients.  And so many people sent me snapshots of that.



LEO:  That's cool.  That's cool.



STEVE:  Saying, hey, other people have been here, yeah.  And like sometimes you'll want to know, like, what batteries this thing takes.  So you'll select that thing, and then there's like, oh, people who bought that thing also bought these batteries.  And it's like, okay.  So it's sort of a shortcut for you.  So certainly handy.  Anyway, not about security, but I just got a kick out of this moving from the cyber world into the so-called brick-and-mortar world.



LEO:  Yeah, yeah.



STEVE:  So, okay.  We've been talking many times and for several weeks, ever since the guys at Positive Technologies informed the world that they had managed to get a hold of a JTAG interface over USB to, what was it, the last seven years of Intel motherboards and chipsets.  Which was like, what?  Because we explained the JTAG is the industry standard debugging interface which is a serial interface that allows you to halt the processor, single-step it, examine its registers, examine the state of the system, read and write to memory, I mean, it's everything.  So the question was could Intel have actually been, possibly have been so irresponsible as to have deliberately exported this incredibly powerful debugging interface on their  USB ports.  And the answer is no.



So now, thanks to Black Hat Europe, which has happened, we've got the detailed research paper where they take us through what they had to do.  And it was exhaustive and exhausting.  First, we talked about this a while ago because we've been talking about the worry with the Intel Management Engine, the IME stuff, for some time because bugs are beginning to surface in that, and it is like ring -3.  It's super powerful.  You could do everything with it.



So the first breakthrough was that the firmware was compressed using a simple but powerful compressor known as Huffman coding, where you look at chunks of bytes, and you assign it a variable bit length token which is inversely proportional to how many times that chunk of bytes appears.  So that is to say, the more times a run of bytes occurs, the shorter the token you give it; and the fewer something occurs, the longer the token.



As a consequence, you end up with, first of all, removing the byte boundary.  You turn it into bit encoding, which is going to be inherently more efficient because you're not, like, having to round everything up to the next byte size.  And the things that happen more often are encoded in fewer bits.  This of course is what Morse did when the Morse code was created.  The things most often sent in the English language were given the shortest representations, dot and dot dash and so forth; with the things that occurred less often being more lengthy to send.  Very clever.



LEO:  Etaoin shrdlu.  Right?



STEVE:  What?



LEO:  Etaoin shrdlu.  You would know that.  I bet you would know that; right?  



STEVE:  That's right.  Right, right, right.



LEO:  E-T-A-O-I-N S-H-R-D-L-U.  Those are, in order, the most common letters.  The only reason I know that is it's great for crypto, right, for...







STEVE:  Well, and you just passed your ham license not too long ago, also.



LEO:  Yeah.  I didn't need to know Morse code, though, thank god.



STEVE:  Oh, no kidding.  That's gone?



LEO:  Yeah, they dropped it.  Oh, yeah.



STEVE:  Okay.  So some people managed to reverse-engineer the Huffman tables, which had not been published.  Once that was done, then the firmware could be decoded.  The problem was, until relatively recently, the management engine controller was not x86.  It was known as the ARC family.  ARC is kind of similar to ARM, but different.  It is a RISC family.  It's IP.  It's intellectual property in the same way.  You're able to build basically your own controller.  They manage and groom and evolve the architecture over time, exactly the way ARM does, but this is ARC.  And so that's what Intel had historically been using.  The problem is there are no really mature reverse-engineering tools for that wacky RISC engine, so hackers weren't that interested in digging around in there.



Well, Intel said, hey, why are we licensing this ARC thing when we've got x86 stuff coming out of our pores?  So they switched the IME some time ago to x86.  Well, that made it much easier for security researchers to reverse-engineer because now they had all of the mature code analysis tools that the research and the hacker community have developed over time, automatically breaking out subroutines, following things along, figuring how things are interlinked, giving you beautiful traversal graphs of how the flow of control goes.  So that made it much more easy to do.  So that allowed them to extract the firmware, decompress it, understand what it was being written in, and apply mature code analysis.  They found a stack overflow vulnerability.



But Intel had a stack cookie, which we've talked about in the past.  The idea is that you put little sentinels, they're called "stack cookies," on the stack, normally generated by something.  And in fact there's a hardware random number generator that generates the stack cookies so they don't repeat.  And the idea is that, if any buffer overflow occurs, and the stack is overrun, it will stomp on the cookie.  And so before control is returned to the data on the stack, the cookie is checked.  It's validated to make sure that the stack has not been smashed.  So props to Intel for really having a lot of protection down in a system that they would never have expected anybody to be digging around in because it's just hard, they made it so hard to get to.  But it's there.  So there's stack cookies.



And so then they figured out, okay, how to get around the stack cookie.  But then it turns out the stack is nonexecutable.  So again, props to Intel.  They said, hey, stack is for data.  Code should not be running on the stack.  So mark the stack segment as nonexecutable.  Well, one thing that's missing, and I'm not criticizing Intel because they went to a lot of extent in order to do this, is Address Space Layout Randomization.  There is no ASLR.  What that means is that all of the location of the modules of code are in known fixed locations.  That means that they can avoid the problem of needing to execute from the stack by using so-called ROP, Return-Oriented Programming, where instead you jump to known code that is executable, being code, and get it to do the stuff you need.



So it turns out that there are file system components that are used during the boot-up process, the so-called BUP, B-U-P, that we've talked about before, that can be replaced to allow unsigned code to run.  So they went through all of this in order to arrange to bypass Intel's across-the-board code-signing protection, the idea being that the system absolutely is designed not to let any non-Intel signed code to run.  They figured out a way around that, got their own code to run, then managed to turn on a mechanism in the PCH.



The PCH is the Platform Controller Hub.  That's the chip where this all runs, where the x86 is, I mean the x86 engine that runs all this.  The way they explained it in their paper, the platform controller hub is god of the motherboard.  It has direct access to about 50 different little itty-bitty devices all over the motherboard that are all about just motherboard level stuff, way below where the main motherboard processor is running.  And so, if you can get into the PCH, into the Platform Controller Hub, and run your own code, you can do anything you want.  I mean, it is supremely powerful.



And it turns out that all of those devices all over the place, including that x86 Management Engine Processor itself, support JTAG.  And so by manipulating something known as the PCH Red Unlock, they were able to obtain JTAG debugging access to all of the devices on the platform, including the x86 Management Engine Processor itself, and then were able to export that over one of the USB interfaces to the outside.  But only after all of that other work.



So I give them serious props for having achieved this.  This is the kind of research where the result only makes the platform stronger.  Certainly Intel will immediately move, if they haven't already, to shut this down and resolve the vulnerability that these guys managed to find.  But they certainly had to work for it.  And by no means does it mean, as we were worried, that anybody can walk by any of the Intel-based, Intel chipset-based motherboards made in the last, what is it, 10 years I think, since 2007, and just stick a thumb drive in and take over your machine.  Not even close.  It requires serious deep level surgery in order to make that happen.



So Intel really did a good job of working to prevent this from happening.  They missed a little spot, so I'm sure that will be fixed in firmware updates to prevent it, and for sure moving forward.  So anyway, that closes that chapter and pretty much resolves the worry we had.  These guys did a great piece of forensic reverse-engineering and security work, which is how the system is supposed to work.



We have a couple more stories that are sort of similar to that.  And in fact the next thing I'm going to talk about was that last week Ian Beer, who is one of the Google Project Zero security researchers, tweeted something very tantalizing on December 5th, exactly a week ago.  He tweeted:  "If you're interested in bootstrapping iOS 11 kernel security research" - meaning, okay, if you're a researcher who wants to play with rootkit-y kinds of things.  He said:  "...keep a research-only device on iOS 11.1.2 or below."  And then he said:  "Tfp0 release soon."



Well, tfp0, that's a special nomenclature known to iPhone kernel hackers, stands for Task For PID, P-I-D, Process ID 0, which is the kernel.  And so what he was saying was, again, like he was foretelling something coming soon that was going to be huge for the iOS jailbreaking and kernel research community.  And he was saying do not move to 11.2, which is what happened last week.  So stay at 11.1.2 or below deliberately.



Then yesterday he dropped the other shoe, saying:  "iOS 11.1.2 now with more kernel debugging."  There's a link in his tweet to the bugs.chromium.org, which is where the Project Zero repository is.  And basically all the details for getting a kernel debugger, which he produced a proof-of-concept local kernel debugger, which runs on all the machines he has to test on - an iPhone 7, a 6s, an iPod Touch 6G.  And he says that adding more should be easy.  There is a link to what he calls the "async_wake" exploit attached to this, with full details.  He worked with Apple to responsibly disclose this.  This was closed in 11.2.



So this has been fixed.  But this is significant for people wanting to poke and examine the internals of iOS 11 or, yeah, iOS 11 and earlier, even 10, because Apple, as we know, has been working frantically and successfully to lock down, to increasingly lock down iOS.  The flipside is that researchers have also been locked out, as is necessary.  And so for a while, because it's been increasingly difficult to get jailbreaks on iOS, what that also means is that valid research has been pushed out.  And so there sort of isn't any way to have it both ways.  You can't allow researchers in because there's no way to keep those secrets away from the bad guys.



So Apple, in order to continually lock down iOS more and more, the problem is that it prevents valid research which may discover and responsibly report problems, just as Ian has, and did.  I mean, he found a way on 11.1.2, that was what we had two weeks ago, he found a way in.  And we haven't had that for a long time.  So anyway, in their most recent 11.2 update, Ian was responsible for I think it was - I had it here in my notes - a large portion of the updates.  He is Project Zero's iOS specialist.  Thought I had it here; I don't see it.  Something like five of the last 10 updates were from him.  So anyway, so the research community is excited because they've not been able to look in and poke around for a while.  Maybe they'll find some other things that Apple has missed and responsibly reported that have not been fixed as this was.



Oh, here it is.  I said:  "In the latest security bulletin for iOS 11.2, five of the 15 iOS 11.1.2 vulnerabilities that were patched were discovered and reported to Apple by Ian."  So he's been very useful and valuable to Apple in continuing to find problems and responsibly disclosing them, thus allowing them to continue, essentially, the attempt to lock him out, which they continually seem to be unable to do.  But in the process...



LEO:  It's funny that he's telling them his jailbreaks, and they're fixing them.  He must just enjoy the game.



STEVE:  Yes, exactly.  And, I mean, he really, as a part of Project Zero, the goal of course, Google's goal is not to only fix Google's things, but to fix everybody's everything by responsibly reporting.  And remember, they do start that 90-day clock.  So if you don't jump on this and fix it fast, it goes public after 90 days because they're not screwing around.  If they say, look, here's something bad, the clock is started.  In fact, there have been cases where Tavis found something, remember, after one of his famous showers, reported it to the LastPass folks, and before he was finished toweling off they had this thing fixed.  And so the 90-day clock didn't even get down to 89 before LastPass had something that Tavis Ormandy found fixed.  So, yeah, I mean, hats off to Google for having this project.  It just makes everything a lot more secure.



Now, that's all the doing it right.  Microsoft has a project called Sopris, S-O-P-R-I-S.  That is, that's the Microsoft Research branch.  And the good news is that this project SOPRIS is exploring the very worthwhile idea of making future IoT security better by designing it in from the start, by integrating security into the underlying chip and architecture and everything, thus effectively completely eliminating its cost.  That is, we could argue that the reason it isn't being done with light bulbs is that you can get a cheap processor that doesn't have any security in it that works, and the light bulb's able to phone home or do whatever it needs to do, and security is just not there.  So their correct argument is, if we build it in, if it's just always there, then it'll get used because it's there.



So the bad news is, I would argue, and I do, that they're going about it all wrong.  They proudly came out and created a hacker challenge.  They organized it through HackerOne and called it the Project Sopris Challenge.  And over on HackerOne they announced that the Microsoft Research Project Sopris team invites the security research community to test our most recent experiment, the Sopris Security Kit, by applying to the Project Sopris Challenge.  And then there's a link in this announcement.



And they wrote:  "A primary goal of The Challenge [in caps] is to broadly engage with the security community towards sharing and learning:  150 security experts have been selected to receive and test a Sopris security kit through The Challenge. The application period closed at midnight on April 14th Pacific Daylight Time.  Applicants have been notified if they are accepted to participate in The Challenge by email by April 21st.  Devices were then distributed by early May, with the challenge ending on July 12th.  It is free to participate, and there are no fees for the hardware."  So then I thought, okay, this is interesting.  And so I dug a little bit deeper.



And so Microsoft then, Microsoft Security, boasts on their SOPRIS page:  "Project Sopris Security Challenge.  Thanks to all the skilled hackers who participated.  Over 150 hackers pounding on their Project Sopris boards for 60 days.  Microsoft Research Sopris Challenge completes as a great learning experience, with no verified exploits.  Stay tuned for updates from the Project Sopris team as we continue to work with the security community to explore devices secured from the silicon up."



And so I'm thinking, okay, all that sounds good.  So they released something.  They gave everybody this information, the kits and things, and then said find some problems.  Well, then I found a quote from one of the - it was Wired's coverage - one of the hackers who participated who goes by the handle HexDecimal, and said:  "It's stupidly easy to hack most IoT devices, but this was very different.  The chip was definitely built for security from the ground up.  One of the noteworthy things would be" - wait for it - "the lack of information."  And I thought, what?  He says:  "The board and its web server were very closed off, nothing that would hint at an exploit."  He says:  "I only started to get a foothold after decompiling one of the setup tools that came with it.  But I never managed to find anything, and neither did anyone else in the challenge."



And so I thought, okay, what?  That's the dumbest thing I've ever heard.  Microsoft gave these guys an undocumented black box and said, "Try to hack it."



LEO:  That sounds like fun.



STEVE:  Well, except that - yes, fun, except what you want is something completely documented, completely open, completely known.  And that isn't what Microsoft gave them.  Galen Hunt, who's the managing director for Project Sopris, says, quote:  "The team was actually disappointed that the penetration testers didn't find any flaws."  Okay, so give them documentation.



LEO:  The ultimate security through obscurity.



STEVE:  Exactly.  This exactly was pure obscurity.  It's like, here's something.  See if you can break it.  But we're not going to tell you anything about it.  We're not going to tell you what it does.  We're not going to tell you what it supports.  It's just like, what?  So anyway, I would say give them total documentation, give them source, give them some schematics, give them everything because that's what the world will ultimately have.  It's, I mean, it's exactly like what Intel did not do.  They tried to hide their Intel Management Engine under layers of compression with undocumented Huffman tables and just buried it away.  And so of course researchers managed to reverse-engineer it, as researchers are always going to do.



So it's absolutely ridiculous to give people a black box and say, okay, you have 60 days to crack this, and we're not telling you anything about it.  Give them 60 days with the source code and see how well you do, Microsoft.  Maybe someday, but not now.  They'd rather parade around and say, wow, look, nobody can figure out why what we told them nothing about they were unable to penetrate.  It's like, okay.  Well, fine.



LEO:  Steve "Tiberius" Gibson has been - somebody said:  "Steve doesn't drink caffeinated coffee, does he?"  I said, oh, yes, he does.  Oh ho ho.  That is not a misnomer when we said Steve's off to caffeinate.



STEVE:  I reused some Starbucks cups the other day.  I made a latte here and took it back to my other house.  And Lorrie saw the side of the cup that Starbucks had originally marked up, and it showed - it had a six in the window of shots of espresso.  And it's like, yes, that's the hex, the venti hex latte.



LEO:  That was literally an eye-opener for Lorrie; right?



STEVE:  Indeed.  



LEO:  What?  [Crosstalk] what she's got into.



STEVE:  Yeah, she does.  So extended validation is now the subject of people poking at it, as was almost predictable.  Leo, go to stripe.ian.sh in Safari, if you've got Safari as your normal machine in front of you:  S-T-R-I-P-E dot I-A-N dot S-H.  And what you will see - or you can just click the link in the show notes, if you have them.  What you will see...



LEO:  Unfortunately, I don't have Safari.  This is Chrome.



STEVE:  Ah, okay.  Well, Chrome works, too.  You should see an...



LEO:  I see it says secure, yeah.



STEVE:  It says secure.



LEO:  In green, yeah.



STEVE:  And probably does it say Stripe, Inc. in the U.S.?



LEO:  You know, Google has done a bad thing.  They now hide the certificate.  Whoops.  Clicked too many back.  They hide the certificate from you, which I find very frustrating.



STEVE:  Yeah, we have talked about that, how they took that away, essentially.



LEO:  Yeah.  Let me see if I can get back to the page in search.  All right, stripe.ian.sh.  And I think I remember that I - what do I do?  I click the settings?  I can't even remember anymore.



STEVE:  Yeah.  They have come back a ways to make it a little more visible.  But anyway, under Safari, what someone who goes to stripe.ian.sh would see is a nice, green, centered in the URL, they don't even - Safari doesn't show you the URL, it just shows you what the certificate - and it says Stripe, Inc., and then, parens, (U.S.).  Same thing in Firefox.  I see this happy glowing green Stripe, Inc., as in the United States. 



LEO:  Yeah.  It says secure via HTTPS, valid certificate.  Let's look at the certificate.  Stripe.ian.sh, Stripe, Inc.  But wait a minute, it's "Ian."  It's not "Stripe."



STEVE:  Correct.  Exactly.



LEO:  Now, does it have anything to do with the fact that it's Comodo?



STEVE:  Well, no.  But he did get a Comodo cert.  What this researcher did was he incorporated Stripe in a different state.  And so that cost him $100.  And then he purchased an EV certificate from Comodo for his valid firm, Stripe, Inc., but which has a name collision in a different state of the U.S.  In the United States you can't have two incorporations of the same name and the same state, but you can incorporate the same name of a company in a different state.



So the point is, here is a social engineering hack on extended validation, which is like, oops.  The problem is, if you're Stripe, Inc., you're well known.  You're sort of famous.  Somebody seeing this in Safari or in Firefox would just assume, oh, okay.  And if you clicked on a link, and if you didn't take the trouble to inspect the URL carefully, and you really can't even - you don't even see the URL in Safari.  But you have learned to trust the extended validation greenness.  It says Stripe, Inc.  So you immediately think, oh, I am at the right place.  No, you're not.  And in fact another researcher got an extended validation for the firm Identity Verified.



LEO:  Brilliant.  Brilliant.



STEVE:  Yes.  And it's green.  And it looks wonderful.  And it means nothing, unfortunately, except that you went to a site of a firm who got an extended validation cert, Identity Verified.  And it's all warm and fuzzy.  And unfortunately, it doesn't mean what any user would think it means.  So we're beginning to see some hacks against the underlying extended validation implied meaning, which kind of were inevitable, but it just demonstrates how difficult it is to do these sorts of things.  Yes, we have EV certs.  But yes, there are ways around what the EV is trying to say so that showing you Stripe, Inc. incorporated in the U.S. isn't really a guarantee that you're at the Stripe, Inc. you think you are, or is being asserted by the certificate.



So anyway, this has stirred up some dialogue in the CAB Forum with the guys that are active, our CA Browser guys, trying to figure out what to do about some of these things.  So this is a good conversation to have, but it just demonstrates that just the fact that we're involving more humans in the loop for certificate validation doesn't automatically solve every problem.  And we could also argue there probably is no solution to every problem.



So I ran across, in doing some research for a mistake that Apple made in allowing an app to get into the app store, I ran across one of the premier websites for managing cryptocurrency.  It's called MyEtherWallet.  And Leo, if you go there, and this time it doesn't matter what browser you use, M-Y-E-T-H-E-R-W-A-L-L-E-T dotcom, what you get is this big annoying clickthrough, and with a bunch of apology, saying "Welcome to MyEtherWallet.com.  We know this clickthrough stuff is annoying.  We are sorry."  Then it says:  "Please take some time to understand this is for your own safety.  Your funds will be stolen if you do not heed these warnings."  Which hopefully will get their attention and beg some more patience from them.



And then they say:  "We cannot recover your funds or freeze your account if you visit a phishing site or lose your private key."  And then basically they go through a long series of intercept pages, each of which is annoying to someone who doesn't like these things.  But what I was immediately put in mind of is the problem I'm already understanding I'm going to face, I and the SQRL community, with SQRL because...



LEO:  This is actually good because it does explain what the hell's going on.  And I think a lot of people just launch into this without any idea. 



STEVE:  Yes, exactly, Leo.  So, for example, to give our listeners a sense, they say:  "What is MEW?" which is their acronym for MyEtherWallet.  And they say:  "MyEtherWallet is a free, open source client-side interface.  We allow you to interact directly with the blockchain while remaining in full control of your key and your funds.  You and only you are responsible for your security."  Then they say:  "MyEtherWallet is not a bank."  So they try to draw the distinction.



"When you open an account with a bank or exchange, they create an account for you in their system.  The bank keeps track of your personal information, account passwords, balances, transactions, and ultimately your money.  The bank charges fees to manage your account and provide services like refunding transactions when your card gets stolen.  The bank allows you to write a check or charge your debit card to send money, go online to check your balance, reset your password, get a new debit card if you lose it.  You have an account with the bank or exchange, and they decide how much money you can send, where you can send it, and how long to hold on a suspicious deposit, all for a fee."



And then they differentiate themselves from all of that, saying:  "MyEtherWallet is an interface.  When you create an account" - and blah blah blah.  I won't drag everyone through it.  But "We do not charge a transaction fee.  We never transmit or store your private key," blah blah blah.  So they're working to educate the consumer who's got themselves all revved up over bitcoin and blockchains and so forth without any idea what they are, but they want some.  They're trying to say, look, we want to help you, but we want to help you help yourself.  So there's, like, a limit to what we can do.



And so they're attempting to explain the responsibility that doing this has because they don't want people to lose their money.  But they want to explain that they are not a place where you can go for recourse.  They can't get your money back.  They can't help you if you get spoofed and somehow your private key gets grabbed by scammers.  They can't help you.



And so of course I was reminded of this because of course SQRL is very similar.  SQRL gives the user, requires the user to have responsibility for managing their identity.  There is no one to complain to, no third party to say, oh, I forgot my password.  Please send it to me in my email.  No.  That doesn't exist because, if it did, it could be hacked.  And that's how identities are lost, and that's how Mat, we talked about years ago, lost his Twitter account, and then they got everything else and got all of his stuff.



And so I've been, as our listeners will understand when we do our podcast on SQRL, there's all kinds of - like we've done everything we possibly can to help people not get hurt and to recover from anything they do.  But ultimately, very much as with bitcoin, the ultimate responsibility is the user.  And we're seeing the consequence of that because people aren't used to that.  They're not used to, like with your credit cards, wait, that's not my charge on my card.  And the bank says, oh, okay, and they take it off.  They reverse the charge.  And so these guys, I really liked what they did because right upfront they said, "We're sorry for what we're going to make you read, but please pay attention to this because we don't want you to get hurt.  But if you hurt yourself, we're sorry, but we can't unhurt you." 



LEO:  It's funny you should mention this because this is the wallet that was spoofed on iOS.



STEVE:  Yes, exactly.



LEO:  Completely unrelated, but...



STEVE:  Apple, after all of this care and concern and worry, Apple allowed a knockoff version of one of the world's biggest crypto wallets, this one, the MyEtherWallet app, onto the App Store.  And it went right up near the top of the most popular Ether wallets.  It's an imposter.  MyEtherWallet got onto the App Store.  TechCrunch wrote:  "The app rose to the number three spot" - that is, this fraudulent, this spoofed imposter app - "the number three spot in Finance of the App Store last weekend as part of the bitcoin frenzy that saw the Coinbase exchange top Apple's free download list in the U.S."  So a huge amount of interest.



"However, in this case it's important to note that the app is not official.  So users," TechCrunch wrote, "should avoid downloading it."  The app developer of the spoofed app, who's listed as Nam Le, has three other apps with Apple.  Two are panda fighting games.  And he has no history of crypto or bitcoin services.  So the legitimate creators of MyEtherWallet immediately posted a tweet saying that they have contacted Apple and asked to have it removed, and today it is gone.  But so it was in looking into this story that I went over to the MyEtherWallet and was so impressed with the work, the degree that they went to and the care that they showed to try to help people protect themselves, yet here was a spoofed version of their app that got onto the App Store, and thousands of people downloaded it.



So unfortunately, I don't see how we solve these problems except by being as responsive as possible to them.  And props to the MyEtherWallet people for understanding that they need to explain what this is because they're doubtless feeling the same sort of frenzy over Bitcoin and Blockchain and Ethereum and everything, and recognizing that people are jumping into this without appreciating what this is and that the nature of the anonymity, the nature of the security that this offers also holds responsibility.  I mean, here you and I were talking earlier at the top of the show.  It's like, oh, where are those bitcoins?  Oh, I don't know.  You know?



LEO:  Let's put them in this wallet on iOS.  What could possibly go wrong?  Yeah.



STEVE:  There's no one I can email a complaint to and say, hey, could you please send me my 50 bitcoins?  I'd like to have them now because suddenly they're really valuable.  Whoops.  Nope.  So, I mean, I understood that.  But today other people are not going to.  So we need - that education has to happen, and there's going to be some pain in the process, inevitably, I think.



LEO:  Yeah.  Of course whenever there's a bubble like this you get a lot of people who are just greedy and not particularly savvy.  They get taken advantage of.



STEVE:  Yes.  So a very cool project on GitHub that I know that some of our listeners are going to find interesting is known as - it's called DNS Twist.  And it is GitHub.com/elceef, that's the guy, and his project is /dnstwist.  And he wrote:  "See what sort of trouble users can get in trying to type your domain name.  Find similar-looking domains that adversaries can use to attack you."  His project can detect typosquatters, phishing attacks, fraud, and corporate espionage.  He says:  "Useful as an additional source of targeted threat intelligence."



So get a load of this.  "The idea," he writes, "is quite straightforward.  Dnstwist takes in your domain name as a seed, generates a list of potential phishing domains, and then checks to see if they are registered in the DNS."  He says:  "Additionally, it can test if the mail server from MX record can be used to intercept misdirected corporate emails, and it can generate fuzzy hashes of the web pages to see if they are live phishing sites."



So it's just very cool.  It was developed under Ubuntu Linux, so that's the primary development platform.  So if you're a Linux user, it's easy to run it.  macOS users can use Homebrew in order to host it.  And it's got a Docker container, so you could use that.  But so what this does, and the page shows it running, he has got an animated command screen that you kind of see it going.  You give it, for example, Amazon.com, and it generates an incredibly large number of lookalike Amazon.com -esque or -ish domains and then emits DNS queries to all of them, looking to see if anything that looks like Amazon.com was registered, and tells you, if so.  And of course you put your own domain in and see if anybody has bothered to register things that look like it.  So it's simple, but it's just a very cool idea.  He supports Unicode, so he's doing all kinds of Unicode character lookalike attacks.  And anyway, just a very cool spoof test.  And it's free.  So if that idea...



LEO:  Installing it now.



STEVE:  ...jumps in, well, I think it's a cool - yeah, see what happens with TWiT.tv.



LEO:  You want me to run it on that?  Oh, geez, I know.  I don't even want to know.  I know.  Yeah, let's try it.



STEVE:  So in the meantime, someone sent me a link to a very cool piece of work.  The links are now permanent on GRC's Linkfarm page, so everyone can find it.  All you have to remember is that GRC's Linkfarm has it.  They're also in the show notes here.  It is a beautifully assembled how-to for using the Ubiquiti EdgeRouter X, setting it up for network segmentation, specifically for IoT devices.  I mean, I don't know that this guy is a listener to this podcast.  But if not,  the coincidence is extreme because all of this assumes exactly what we want, that we've talked about here.



The Ubiquiti EdgeRouter X is this cute little $49, incredibly powerful router that our listeners are using, many of our listeners are using.  The problem has been that I've never had time, essentially, to do this.  I would love to have time, but as everyone knows, SQRL first, SpinRite 6.1 next, and so forth.  But that exists now.  So it is a - don't remember how many pages, 60-some I think it was, page PDF.



So you can grab it from his GitHub page or get the link from the Linkfarm or from the show notes here for this Episode 641.  And it takes you through the entire process of how to create separate segments and even separate SSIDs on separate WiFis so that all your light bulbs and your door locks and your Apple and Amazon and all that IoT stuff can be on a segment of your network in its own address space with no ability to touch your computers or your high-value things, and how you can, you know, each port of the five ports of the Ubiquiti can be its own network.



So thank you for the guy who put it together.  And for anyone who's been wondering if they set theirs up right, or maybe got an EdgeRouter X and then just immediately came to a standstill when it's like, ooh, boy, I'm not sure how to do this, well, now there's a perfect guide for answering that question for our listeners.  So I'm delighted for that.



There was, last week, an update across the board for Windows.  It was in response to a CVE, a common vulnerability that Microsoft was made aware of; and it was so bad they had to immediately patch it across the board.  They said:  "A remote code execution vulnerability exists when the Microsoft Malware Protection Engine does not properly scan a specially crafted file, leading to memory corruption.  An attacker who successfully exploited this vulnerability could execute arbitrary code in the security context of the LocalSystem account" - which we know is as bad as it gets.  I mean, LocalSystem account is root, essentially, in Windows - "and take control of the system.  An attacker could then install programs; view, change, delete data; or create new accounts with full rights."



So anyway, this was a very bad vulnerability that was found.  And we've talked before about how antimalware systems can themselves, by their very nature, create an expanded attack surface.  If they're not implemented correctly, since they're looking at everything coming in from the outside, they have to be perfect.  And they are, also by their nature, they're interpreters, and we know how difficult it is to get interpreters exactly right.



So I would argue, and we have been, that once Microsoft has a good enough malware protection facility, it makes more sense to use Microsoft's than a third party's because we have seen third parties having problems discovered in theirs.  Microsoft is able to and does respond immediately to push patches out to their systems.  So I think they did everything they could.  I still feel best using Microsoft's built-in solution for protection.  And hopefully it won't be needed very often.  But they are keeping it updated as quickly as they can.  And they've got a facility for immediately pushing out any changes.



So that's what happened last week, prior to today's big second Tuesday of the month update.  And I'm using a Win10 system for my Skype link, and so one of the things I do when it's the second Tuesday of the month is turn it on very early in the morning and immediately go to check for updates and, oh, look, there is something, what do you know.  Now we know it's because there's going to be a rollup every month.  So I get that done, and I reboot a few times, and I go back and check to see if it thinks there's anything more.  So I make sure the system is settled down well before the time that we have to make a recording so that in no way is this going to be updated.  But one has to do that these days with Windows 10.  And so that's why I was a little surprised to find something last week.  It was Microsoft jumping on a problem.



And speaking of jumping on a problem, Apple was also notified of a problem for which there is no current information.  9to5Mac had the reporting on this, and they said a HomeKit vulnerability in the current version of iOS 11.2, so that's not the one that Project Zero found the problem in last week, where we were at 11.2.1, but rather the update later last week to 11.2.  So a vulnerability was found in 11.2, which we're all using at the moment, and was demonstrated to the guys at 9to5Mac, which allows unauthorized control of accessories, including smart locks and garage door openers.



They wrote:  "Our understanding is Apple has rolled out a server-side fix that now prevents unauthorized access from occurring while limiting some functionality and an update to iOS 11.2 coming next week."  And they wrote this five days ago, so last Thursday.  So an update coming next week, meaning this week, so we're already expecting once again iOS 11.2 to be replaced, probably by iOS 11.2.1 this week, will fix the problem and restore that full functionality, which was reduced.



So they wrote:  "The vulnerability which we won't describe in detail was difficult to reproduce, but allowed unauthorized control of HomeKit-connected accessories, including lights, thermostats, and plugs.  The most serious ramification of this vulnerability prior to the fix," they wrote, "is unauthorized remote control of smart locks and connected garage door openers, the former of which was demonstrated to 9to5Mac.  The issue was not with smart home products themselves individually, but instead with the HomeKit framework itself that connects products from various companies."  So of course we know, as always, bugs are not good.  But in this case the centralized nature of HomeKit's architecture allowed Apple to immediately neuter the undisclosed vulnerability at their end while whey prepare a proper fix, push it out, and then reenable the reduced functionality.



So as a consequence of the architecture that HomeKit uses, Apple again was able to respond quickly, prevent there from being any consequences, and give them a little breathing room to fix this.  And it wasn't clear what functionality was reduced.  But I imagine anybody who had a vulnerable front door lock and garage door would just as soon have the vulnerability prevented, even with some loss of functionality, until Apple gets it fixed, which apparently is going to happen this week.



And finally - well, not finally, but next - I thought this was interesting.  I just ran across Valve announcing that they're ending support for Steam purchases made with bitcoin.  And it's like, okay, what's the problem?  Well, two things.  It turns out that, well, in a post on Steam, Valve wrote:  "In the past few months we've seen an increase in the volatility in the value of Bitcoin" - yeah, no kidding - "and a significant increase in the fees to process transactions on the Bitcoin network."



They said, "For example, transaction fees that are charged to the customer by the Bitcoin network have skyrocketed this year, topping out at close to $20 per transaction last week," they said, "compared to roughly $0.20 when we initially enabled Bitcoin," which was sometime in 2016.  "Unfortunately," they wrote, "Valve has no control over the amount of the fee.  These fees result in unreasonably high costs for purchasing games when paying with Bitcoin.  The high transaction fees cause even greater problems when the value of Bitcoin itself drops dramatically."



Valve also explained why surges like that can have an impact on purchases through Steam.  They wrote:  "When checking out on Steam, a customer will transfer X amount of Bitcoin for the cost of the game, plus Y amount of Bitcoin to cover the transaction fee charged by the network.  The value of Bitcoin is only guaranteed for a certain limited period of time, so if the transaction doesn't complete within that time window, and with Bitcoin value fluctuating wildly, the amount of Bitcoin needed to cover the transaction can change.  The amount it can change has been increasing recently to a point where it can be significantly different."



So anyway, Valve said that those price discrepancies would normally result in a refund or an additional payment from the customer when more traditional, more stable currencies are involved.  But in the case of bitcoin, high transaction fees themselves can make those subsequent resolutions expensive.  So they concluded, saying:  "At this point, it has become untenable to support Bitcoin as a payment option.  We may reevaluate whether Bitcoin makes sense for us and for the Steam community at a later date."



And it's interesting, that made me dig into a little bit like what is this about transaction fees?  Like what's happening?  And so the way the bitcoin bit chain system works is that the bitcoin block is deliberately limited to a megabyte of data.  And a block is completed every 10 minutes.  So only a megabyte of transaction data is allowed to be processed every 10 minutes.  So this creates a transaction rate scarcity within the entire bitcoin ecosystem, and those wishing to push higher value transactions through the system have ended up creating a bidding war to get their transactions into the next block to be computed.



So what's happened is there's now essentially an auction system in getting transactions into the blockchain for validation, which has pushed the price up from what used to be just a minuscule fee to now $20.  And the other reason is that the transaction fee was in a made-up unit of satoshis and some fraction of a bitcoin.  But once upon a time bitcoins weren't very valuable.  Now bitcoins have been floating around $20,000 each.  So what was once a micropayment has become a mega-payment because the bitcoin price has gone up, but the fraction of satoshis has remained fixed and is fixed by the system, so it's become expensive.



So anyway, I just sort of thought - I thought this was an interesting snapshot into some other unintended consequences of both the high value of the bitcoin and the fact that the bitcoin value is volatile and that it's also become so popular that this limit on block size has created a constraint in the rate at which transactions can be processed, which creates rarity there.  And that then further drives up the price, as people bid essentially in an auction in order to get their transaction processed.  So, eh.  I don't know.  I mean, Leo, I guess I know I'm glad to have my, well, somewhat ephemeral bitcoins or the promise of them.  But it does seem like the system is becoming a little bit a victim of its own success.



LEO:  Well, that's why there are those two forks, you know, partly because the blockchain got so big, but also because of the speed of transactions and the transaction cost.



STEVE:  Right.



LEO:  And the bitcoin board really seemed reluctant to make any changes at all to the algorithm.  



STEVE:  Yeah.



LEO:  So, yeah, I wonder; you know?  I think you and I should sell our bitcoins quickly.



STEVE:  Well, it's not clear that they're ever going to go down.  They may just stop going up.  But, I mean, it's like people are, I guess - now [crosstalk] speculate.



LEO:  They can go down, though; right?  I mean, it's not - yeah.  I mean, it's all speculation.  Come on.  The Winklevosses are bitcoin billionaires.



STEVE:  Yeah.  Well...



LEO:  And they're so bullish that that's all imaginary because they're not going to cash them in.



STEVE:  Right.  And in fact one of the things I had in my miscellany, I'll talk about it now because it's on point, is it's a little disturbing to wonder if those cryptocurrency or the cryptomalware people that we talked about for years, who were asking for payment in bitcoin, had they kept them...



LEO:  Oh my god, yeah.



STEVE:  Yes, yes.



LEO:  Could have been a big difference, yeah.



STEVE:  It was 400 bucks for a bitcoin for a long time, or 350-something, I remember.  So people were like, oh, well, I guess it's worth a bitcoin to get my files back.  Well, if those guys kept their ill-gotten funds in bitcoin, well, now they may have a lot of, I mean, they may be nefarious millionaires.



LEO:  For me to exchange my bitcoin for dollars would require that somebody buy them; right?



STEVE:  Yeah, I mean, there are exchanges where this is happening.



LEO:  It's an exchange.  And the price is determined by what people are willing to pay.



STEVE:  People are buying, yeah, yeah.  



LEO:  So it could go to zero if nobody wants them.



STEVE:  Yeah, you should grab - Bitcoin Ticker is my favorite iOS app.



LEO:  I don't want to follow it that closely.



STEVE:  It's just interesting.



LEO:  Wow.



STEVE:  So the good news, the good news for people who want encrypted mail.  Yesterday ProtonMail announced Bridge.  So I finally have truly good news about email encryption.  In their blog posting they said:  "Today we are officially launching ProtonMail Bridge, which brings easy-to-use email encryption to desktop email clients."  And the reason I'm sharing this with our listeners is I'm bullish about this.  These guys got it right.  They wrote:  "Ever since the day that we first got the idea to create ProtonMail, one of the most enduring challenges has been how to do email security right while simultaneously making encrypted email easy enough to use for normal people.  Since our early days working from the CERN cafeteria, we have been working tirelessly to address this specific problem.



"In the years since, we have made many great strides towards creating usable encrypted email, first with ProtonMail's webmail interface and then with our award-winning iOS and Android secure email apps.  However, one of our goals has always been to bring easy-to-use encrypted email to the desktop.  The problem is formidable.  Desktop systems encompass multiple operating systems with dozens of popular email clients with their own adherents, and virtually none of them natively speak PGP, the email encryption standard upon which ProtonMail is built.  Around two years ago, we created a small task force to tackle this challenge.  Today, we are finally ready to present ProtonMail Bridge."



I'll skip a lot of the stuff, and jumping down they said:  "Furthermore, after the technical documentation of the ProtonMail Bridge code is done, we will be releasing the source code of the Bridge, so that you can even compile it yourself instead of getting the binaries from us, so there is even less need to trust us.  This is an important step in our work to eliminate ProtonMail itself as a threat vector.  Currently, the officially supported email clients are Thunderbird, Apple Mail, and Outlook, on both Windows and macOS.  Linux, they write, "is coming in spring of 2018."  So, what, a few months from now.  "However, in theory, any IMAP email client can work with the Bridge; and, in our beta testing, many were shown to work. If you are a ProtonMail user, you can immediately get started."



Okay.  What is it?  Well, it is simple and clever.  It is a local mail server.  So you run this app on your Windows or your Mac machine.  It opens an email port on your local system.  So rather than aiming your email client at some remote SMTP server, for example ProtonMail or Cox.net or whatever, you point your email client, whatever email client you're using - they're officially saying Thunderbird, Apple Mail and Outlook, but it could be anything because what they've done is they created a local IMAP and SMTP server running on your local machine, that is, on localhost.



So your non-secured, non-encrypted email client talks to the local server running on your desk.  And then that server, which is currently what they haven't yet published the source for, but they're going to, it uses the Proton Mail API to connect securely to the ProtonMail mothership and exchange all your mail over PGP securely, encrypted and with authentication and all the benefits of PGP.  But the point is, from before it leaves your desktop, before it leaves your system, it is wrapped in PGP protection and on its way to ProtonMail for store and forward to whoever you want to have secure communications with.



So this is beautiful because it solves the problem of complex configuration, of email clients not natively supporting or easily supporting PGP.  The problem is that people keep - I see this problem.  I know that there's an interest among our listeners because I'm constantly getting email from people talking about this or that email system that is secure.  And it's like, yeah, except that the whole problem hasn't been solved.  These guys have a nice solution.  By setting up a local server on your machine, you're talking essentially within the system on your own network stack.  Your client connects to the ProtonMail server running locally, establishes a remote connection, and you're secure all the way through.



So yay to the guys at ProtonMail.  And I know that we've got a lot of users of ProtonMail who are arranging to use it through whatever means predates this.  I wanted to make sure everyone knew, and people who are interested in this who could become ProtonMail users, that you can now use your native system email client and get all the benefits.



LEO:  I'll stick with GPG.  It makes absolutely no sense to me.  I don't understand it at all.  GPG works fine, and it doesn't need to set up a server on your system, and you don't need to go through some third party.  You just use GPG.



STEVE:  Okay.  Cool.



LEO:  Yeah, I don't get it.  I frankly don't get it at all.  But okay.  Why do we need to do this?  I don't even understand why we need to do this.  Why are we setting up a mail server and then routing it through ProtonMail?  I guess if you're a Proton Mail user, maybe.



STEVE:  Right.  And so it allows you to use any client that you want to.



LEO:  But I can use, well, I can use all the clients that they currently support with GPG, so I'm not sure what I gain.  Anyway, I'm sure we'll find out.



STEVE:  Well, actually, I already talked about the annoying idea that ransomware miscreants could have seen a huge value inflation in their ill-gotten gains.  I did want to mention that I had a breath-holding moment as I moved my one most critical domain, but then the second most critical one, from Network Solutions, where it has been since 1991.  December 18 of 1991 I registered GRC.com, and Network Solutions has been its registrar.  It is now at Hover because, of course, December 17, its annual expiration was coming up.  And I said, okay, I refuse to remain at Network Solutions any longer.  I am now at Hover.



I just wanted to say that it went perfectly.  No one even knew it happened.  No outage, no glitch.  I was a little nervous about it because it's of course the most important domain I have.  And it went perfectly.  So now I've got all of the services that Network Solutions either didn't offer or wanted to charge extra for, which is annoying, like privacy and lock and so forth, all there, all turned on, and GRC.com is happily at Hover, probably happily ever after.



LEO:  I just have to mention, even though I know it has nothing to do with [crosstalk]...



STEVE:  Oh, yes.



LEO:  ...that Hover is a sponsor of some of our shows.



STEVE:  Yes.



LEO:  But that's not why Steve's choosing it.  And this is [crosstalk].



STEVE:  No, in fact I...



LEO:  He's paying for this.



STEVE:  In fact, didn't my raving about Hover precede them being a sponsor?



LEO:  No, actually.  They've been a sponsor for many years.



STEVE:  Oh, okay.  Well...



LEO:  I think, if I'm perfectly fair, there might have been a little bit of a feedback loop because you tweeted what's the best one to use, and I imagine many of your respondents were listeners who already were [crosstalk].



STEVE:  Ah, well, I certainly do agree because I've been raving about it ever since, yes, yes.



LEO:  It is good, yes, right, I agree, yeah.



STEVE:  So in another fun example of success with SpinRite, Al in Portugal sent me a note saying "SpinRite Sound Recovery," dated the 10th of December, so a couple days ago.  He said:  "Hi, Steve.  Am still running Win8 here.  Today the sound stopped playing back."  He said:  "I checked the sound troubleshooting area in the Control Panel, but it said it found nothing wrong with the sound.  Brilliant.



"So," he says, "I dug around the sound dialog from Control Panel to see if I could refresh the playback by disabling and reenabling things, but nothing worked."  He said, "In fact, when I clicked the playback tab, sometimes it would bring up the small spinning blue circle that never stopped spinning."  He said, "I had to terminate the rundll32.exe to close the sound dialog.



"This made me think that perhaps there was something wrong with Windows being unable to read some files somewhere.  So I thought, why don't I give SpinRite a go to see if it fixed things.  I first tried to reboot twice to see if that might help, but it did not."



He said:  "So then I started running SpinRite on Level 2 and let it run for just the first 1.5% of the 500MB disk drive.  I only did such a small amount because I heard that sometimes people had only run SpinRite for a little while, and it had fixed various problems.  I think it fixes some stuff in the filesystem.  Anyway," he says, "I fired up Windows after that, and the sound was working again.  So I think it must have been SpinRite finding and fixing the problem.  Thanks very much, Steve, for SpinRite, and for the Security Now! podcast, too.  Al in Portugal."



And of course we have talked a bit from time to time, in fact recently, about how sometimes, even if SpinRite runs across something it can't apparently fix, when people reboot their system, sure enough, it got the job done anyway.  And so in this case Al just deliberately ran it for a little while, 1.5%, and that was enough to get the job done.  And of course we know that, when you set up Windows, when you install Windows from scratch on a blank drive, it's probably going to be at the front of the drive.  That's where the files will go.  Windows will sort of probably start allocating itself from the beginning.



And so that's one of the reasons that Windows-based problems can get themselves resolved most easily.  And in fact it's also why that tends to be where the damage occurs is that the hard drive head will generally be hovering around those areas most read.  And so if something happens, if you bounce the head off the surface or you trip over the power cord when it's in the middle of writing, it may glitch the data there.  So that generally is where the problems tend to accrue, toward the beginning of the drive.  And so it actually is where SpinRite generally finds the most problems to fix.  So running it just for a while can often solve the problem.



So a couple of closing the loops with our listeners.  Jim Clary said, or asked:  "Any reason for 'I'm not a robot' check on a logon page?"  He said:  " I understand the usefulness for account generation, but not login."  And he said:  "Humana uses it, making it more difficult to use a password manager."  And Jim, I have to say I think you're right.  I can't really see why, I mean, I guess a bot that was doing guessing would be thwarted if it were trying to log on.



LEO:  There are better ways to do that, like rate limiting.



STEVE:  Yeah, yeah, exactly.  And of course there is a little bit of a problem, too, because remember from a privacy standpoint the presence of that means that the supplier of the "I'm Not a Robot" checkbox is seeing that you are logging in every time you are.



LEO:  I see CAPTCHAs more and more often.  It really bothers me.  I signed Abby up for the Affordable Care Act yesterday because the deadline is December 15th, don't forget.  If you're going to use ObamaCare, got to do it now.  And it had a CAPTCHA at the end.  And I think maybe healthcare providers, maybe it was - might have been through the healthcare provider.  I can't remember.



STEVE:  Might be mandated, you mean?



LEO:  Well, yeah, but I wonder why.  I don't understand the security benefit to it.



STEVE:  Interesting.  Yeah, no...



LEO:  You make an excellent point.  It's just routing it right through Google.



STEVE:  Yeah, exactly.



LEO:  I mean, what robots are signing up for healthcare?  No, I'm just kidding.  I'm kidding.  I'm kidding, robots.  Not talking to you.



STEVE:  So all the bots could use some healthcare.



LEO:  Maybe.



STEVE:  We often discuss on this podcast.  Peter Griffiths asked if a branded router...



LEO:  Not his real name, by the way, I think.  He's a character on a TV show. 



STEVE:  Oh, Peter Griffiths?  Okay.



LEO:  Maybe it is his real name.  I don't know.  I shouldn't make fun of him.



STEVE:  He said:  "Is a branded router always best?"  He said:  "Visiting the folks and, given recent router security awareness, wanted to update their firmware.  Linksys router, no firmware update available newer than 2013.  He says:  "Time to buy a new one?"  And, okay.  So that's a good question.  I guess the question would be whether that particular firmware had known problems.  But you would expect, then, if it did, that there would still be newer firmware available unless Linksys was just taking down firmware for older hardware.  But I don't really see a reason to do that, and four years is not that long.



So what you really want is you want a router from a company with a reputation for really maintaining the currency of their firmware.  And frankly, something like, I mean, we have seen Ubiquiti having some problems, but they're immediately putting up firmware patches and responding pretty quickly to them.  And the router's not very expensive, and it's very capable.  So maybe.  I of course like the idea of running something like pfSense on a small platform and having essentially a non-branded router which runs and is reliable.



So I don't know.  I think it's kind of six of one, half a dozen of the other.  But certainly sticking with a company that is going to be maintaining the firmware moving forward makes a lot of sense, I think.  And also it's worth noting you can always put a router inside of a router.  It's sort of the - that was the whole "three dumb router" concept.  But you can certainly just do two.  And that prevents the inside network from exploitation from the outside.  It doesn't prevent malware from running on your router.  But it does at least prevent them from looking into and getting access to your devices directly.  So that's maybe something to consider, as well.



LEO:  And it was Peter Griffin, so not Griffith, that's in "Family Guy."  So I apologize, yes.



STEVE:  Ah.  So Patrick Hogan said:  "Steve, do you think it's annoying that some ISPs don't enable custom DNS servers to be configured in their customers' home routers?"  And he said:  "@SkyUK are dragging their feet with this."  And, yeah, that would be annoying.  I don't use a router that my ISP has control over in the states.  And in fact it was in that context, of course, there are two things you could do.  In most of the DHCP config of our systems, you're able to - the Obtain IP Address Automatically also has a subsidiary of get your DNS servers also from DHCP.  In Windows, at least, you can turn that off.  And I've seen that option over on Macs, and I'm sure it's there on Linux.



So one option is just to not have your systems where you have control of it.  Of course you don't have control with light bulbs and so forth.  But where you do have a desktop UI, you could override the system's DNS so that it's not getting it from the router's DHCP.  And that's another place where you could put a router inside of your router, that is, a router inside your network that you do have configuration control over, even if you may not have configuration control over the one your ISP provided, and then set it up to provide DNS to everybody within your network.  So that's also a possibility.  So with a little more configuration responsibility, you can probably get around it.  But, yeah, it would be annoying if you had to go through all that, rather than just having control over the router that your ISP has provided.



Jonathan Harris said:  "I believe I've found a bug in a brand of networking equipment my company uses.  I have tried contacting them regarding the issue, but receive no response.  How can I verify this problem and how should I proceed?"  Well, okay.  Without any more details, it's impossible to provide any advice.  But I will remind our listeners of security.txt as the new file which is probably too new to actually exist.  But you know how robots.txt has been around since forever, which is a file that well-behaving robots look at in the root of any domain in order to get essentially marching orders to tell them to stay away from certain areas in the domain.  We now have security.txt.  And there is a site, securitytxt.org, where they document the format of the file and are working to publish this.



So one thing you can do, Jonathan, the only thing I can think to do, is to go to the site for that networking equipment and put security.txt, I'm sorry.  Go to the site and go to /security.txt as the URL and see if you get a file.  If so, then that company - and certainly if they're a networking equipment company, the chances are hopefully more likely that they would have a file that they would offer.  There they may tell you who to contact in order to support security problems.  



LEO:  I asked Patrick Delahanty to do it on our server.  I don't know if he has yet.  But, yeah.



STEVE:  Good, good.  That's a brilliant...



LEO:  Everybody should do that, just to reassure security researchers it's safe to report flaws.



STEVE:  Yes, exactly.  It's safe, and we will not...



LEO:  [Crosstalk], yeah.



STEVE:  Exactly.  We will not hurt you.  And here's where you send your report to, yes.



Belwig asks:  "Long-time Security Now! listener.  I don't recall you ever going over the right and wrong ways to handle password resets.  Are the token-based links in emails secure for resets?"  Then he said:  "Waiting patiently for SQRL."  And so, okay, first of all, we all know the wrong way is for you to press a button saying I forgot my password, and then they send it to you.  [Buzzer sound]  Wrong.  You also don't want them to send you a temporary password.  That's wrong, too, the idea being that anybody who is able to, I mean, the big problem with all of these email-based resets is that email is not as secure as you would like it to be.  But unfortunately, email is what we've got.  And so nobody likes the idea of using email to reset passwords.



But certainly the only thing we've got, if email is your lowest common denominator, I mean, and if you've got two-factor authentication enabled, then you want to be able to tell the person, okay, you need to provide your second-factor code.  But otherwise, clicking on a link that you receive in email, that then takes you to the site, that then requests any other information that you may be able to provide like a second factor, if you've got that registered, is the best that we're able to do with email, and arguably sending you your password means that, among other things, they didn't hash it, and so that they're able to provide you your password back in plaintext, which should terrify anybody.



But, yeah.  Unfortunately, unless there is some other means of authenticating you other than email, the lowest common denominator is a link that you click on that takes you to the site, where you're then able to provide a new password.  And that's the best we've got at the moment.  And, yes, I'm also waiting for SQRL.



Yosef Berger asks:  "When storing passwords by hashing and salting, why is it considered bad practice to use the user's name as the salt?  If we want to make it so different users with the same password don't end up with the same hashed password, why is a unique username not as good as" - now, he said an RSG, and I guess he means just a randomly generated token.  And I agree, Yosef.  A hash does not need to be secret.  It's best if it's unique.  But it's really good enough if it's probably unique.



So I don't see a downside for using the user's name as the hash.  You don't want one hash globally because then that allows, I mean, you want hashing at all so that you can't use pre-computed hash tables.  A global hash would allow a computation of precomputed hashes to be made once and then applied to everyone.  So a per-user hash means that the hashing needs to be done for every single user.  But a username as the hash salt is going to be almost user-unique.  Maybe you could have two people with the same name; but, okay, that really doesn't provide someone who wants to brute-force the hash much benefit.



So I agree.  I mean, it's probably better just to use a randomly generated token.  And why is that hard?  It's as easy, essentially, as using the person's name.  But if there were some reason, if someone wanted to make an argument for it, it's like, yeah, I don't really see that being much of a problem.



And, finally, Javier - wow, Javier, I don't know how to pronounce your last name, Matusevich I guess - said:  "You've said it's incredible WINE works at all," referring to the fact that the guys under Linux have managed to get essentially a complete Windows environment running under Linux.  And then he said:  "Running your DNS Benchmark on my Mac.  Flawlessly works."  And it's no coincidence because I want my Windows stuff to have as much reach as possible.  And so when I was nearing the end of the work on the DNS Benchmark, I made sure that it would run under WINE.  And of course WINE is also available on the Mac, so that allows the DNS Benchmark to run on the Mac and on Linux machines.



And I will assure people that a lot of the time and attention has been going into making sure that SQRL also runs under WINE, and it does.  So that will allow, until there are native clients written for Linux and for macOS, will allow GRC's SQRL solution to be run across all those platforms by having it hosted on WINE.  So we're taking the time to make that happen; and we've got a bunch of WINE users, Linux users, over in the SQRL forum who are providing valuable feedback on the clients' operability under WINE.



Okay.  And finally, what we promised:  iOS Security versus Convenience.  What is it that Apple reportedly, well, definitely did with iOS 11?  Now, okay.  This is coming from the ElcomSoft guys who blogged about this last week, calling it the "Horror Story of the Rise and Fall of iOS Security."  And I've got to take that with a little bit of a grain of salt.  First of all, these are the guys who do phone hacking stuff.  So they're actually not unhappy that Apple made the change that they made because it gives their forensics tools much greater reach. 



And so as I'm reading this, and they're complaining about it, it's like, what?  Really?  You're happy.  I mean, they published something called the iOS Forensic Toolkit.  They've got the ElcomSoft Phone Breaker with the Keychain Explorer and the ElcomSoft Phone Viewer, all of which are forensics tools which  have just become more valuable as a consequence of a decision that Apple made to back off on the way backups are protected.  As I said, I read through this thing, this kind of poorly written blog posting that rambles along and gives lots of examples, but never really gets to the point, which was why this was sort of difficult to nail down.



But so to sort of drive the point home, what they said, what this guy wrote in his blog was:  "With the release of iOS 11, Apple developers made too many assumptions" - okay, I don't really agree with that - "breaking the fragile security-convenience balance and shifting it heavily onto convenience side."



They write:  "Once an intruder gains access to the user's iPhone" - so they have to have physical possession of the iPhone - "and knows or recovers the passcode," so those two things, "there is no single extra layer of protection left.  Everything" - and, they write, they mean everything - "is now completely exposed.  Local backups, the keychain, iCloud lock, Apple account password, cloud backups and photos, passwords from the iCloud Keychain, call logs, location data, browsing history, browser tabs and even the user's original Apple ID password are quickly exposed.  The intruder gains control over the user's other Apple devices registered on the same Apple account, having the ability to remotely erase or lock those devices



"Finally, regaining control over the hijacked account is made difficult as even the trusted phone number can be replaced.  Why," he writes, "Apple decided to get rid of the system that used to deliver a seemingly perfect balance between security and convenience is beyond us.  Once someone has your iPhone and your passcode, you are no longer in control of your device or your Apple account."



So, finally, what can you do to protect yourself?  Since the passcode is now the one and only safeguard left, make sure you use at least six digits, or of course switch to the full alphanumeric one, and use one that's robust.  Four-digit PINs are no longer secure.  Other than that, we'll just wait and see if Apple can fix it.



So that's ultimately the takeaway is the passcode is your last line of defense.  So then in trying to understand exactly what it was that happened, it boils down to Apple's change, which they did make in iOS 11, of allowing password recovery for backups because backups are where everything lives.  And until iOS 11 - and we've talked about this in the past.  If you lose your backup password, your only recourse has been to reset your device and start over.  So they write in this blog posting for backup passwords in iOS 8, 9, and 10, in these versions of iOS one could protect their backups by specifying a backup password in iTunes.  One would only need to do it once.



Once a password was set, all future backups made on that computer and any other computer with no exceptions would be protected with that password.  The password would become the property of the iDevice and not the PC or the copy of iTunes that was used to set the password.  You could connect your phone to a different computer and make a local backup with a freshly installed copy of iTunes, and that backup would still be protected with the passwords you set long ago.  Any attempt to change or remove that password must pass through iOS, which would require the provision of the old password first.  Forgot the original password?  There's no going back.  You're stuck with what you have unless you're willing to factory reset the device and lose all data in the process.



He writes:  "If you ask me, this was a perfect and carefully thought-through solution.  Want to protect your data against an attacker?  Set a long and complex backup password and don't store it anywhere.  Forgot that password?  You can still make a cloud backup and restore your phone from that backup.  Even your passwords in the keychain would be restored if you rolled out the cloud backup onto the same device you made the backup from, or used iCloud Keychain if that was to be a different device."



And then he finally concludes with this, saying:  "A perfect system?  Apparently, it was not to everyone's liking.  The users whined.  The police complained.  The FBI complained.  And Apple gave up."  So under iOS 11, he writes:  "Stripping Backup Passwords.  In iOS 11 you can still specify a backup password in iTunes, and you still cannot change or reset it through iTunes if you don't know the original password.  However, this means very little as you can now easily remove that password from iOS settings."  He quotes Apple.  This is what Apple has to say in its Knowledge Base.



Quoting Apple:  "You can't restore an encrypted backup without its password.  With iOS 11 or later, you can make a new encrypted backup of your device by resetting the password.  Here's what to do."  And Apple says:  "One, on your iOS device, go to Settings > General > Reset.  Two, tap Reset All Settings and enter your iOS passcode.  Three, follow the steps to reset your settings.  This won't affect your user data or passwords, but will reset settings like display brightness, home screen layout, and wallpaper.  It also removes your encrypted backup password.  Four, connect your device to iTunes again and create a new encrypted backup."



And then Apple concludes:  "You won't be able to use previous encrypted backups, but you can back up your current data using iTunes and setting a new backup password.  If you have a device with iOS 10 or earlier, you cannot reset the password," writes Apple.



So ElcomSoft says:  "That's it?  That's it.  You have just removed the backup password.  You can now make a new backup or, rather, extract information from the device."  He says:  "Don't rush, and do make sure to specify a temporary password - '123' always works - before you make that backup.  A password-protected backup will allow you to decrypt the user's passwords, credit card data, health data, and other things that would be otherwise inaccessible.  So set a temporary password, make that backup, decrypt it with" - and then here of course they're selling or promoting their ElcomSoft Phone Breaker or just use Keychain Explorer, a tool in the ElcomSoft Phone Breaker, to access that user's passwords, authentication tokens, credit card numbers, and other interesting things.  Oh, and their pictures, too.



So essentially it must have been that there was some pushback, I mean, that's the only rationale I can see is Apple decided with iOS 11 to allow people to sacrifice their previous backups when they've forgotten their password, but to reset the password which was not resettable before and make a new backup.  So what that means is that if someone does acquire your phone and has your passcode and can get your passcode, because that is required in order to perform this reset, then you are allowed to reset your backup password for subsequent backups.



So then the person doing the forensics work can choose a trivial password, produce a backup and then, knowing that password, get decrypted access to the backup using any of the iPhone forensics tools that are available and essentially dump the contents of your phone.  So that's what that was about.  That was the argument that this guy was making about the tradeoff that Apple, they argue, made.  And it does appear like Apple must have deliberately consciously decided they want to allow people to reset their backup password.  You cannot do it prior to 11, and you can now.  



LEO:  Well, there you have it, folks. 



STEVE:  Yup.



LEO:  I mean, so to summarize, is it fair to say that Apple has backed off a little bit on its once extreme security...



STEVE:  I think it has.



LEO:  ...in favor of convenience.



STEVE:  This allows you to get to your keychain, which, I mean, and a lot of other things that you could not otherwise get to.



LEO:  So as poorly - as confusing, and I tried to read it, too, as ElcomSoft's article was, it does make a valid point.



STEVE:  I think it does, yes.



LEO:  That was my sense, too, but I [crosstalk].



STEVE:  Boy, it was a journey to get to it, I know.



LEO:  Yeah.  Good.  All right.  Mr. G., you've come to the end of another fabulous episode of Security Now!.  We do the show every Tuesday, right after MacBreak Weekly.  That usually ends up being around 1:30 p.m. Pacific, 4:30 Eastern, 21:30 UTC.



Now, let's see.  This here is December 12th.  We'll be back on the 19th.  And then we have a couple weeks off; right?  I'm trying to figure out what we're doing here.  On the 19th, and then the next one will be the 26th, that's going to be our special episode.



STEVE:  Right.



LEO:  Kind of a how - can I say what it is?



STEVE:  Well...



LEO:  We'll surprise you.



STEVE:  Yeah.



LEO:  Although I think there'll be a lot of interest in it, let's just put it that way.



STEVE:  I do think it's - yes.  Yes, yes, yes.



LEO:  And then we will be back doing the show for the new year.  So we have one more episode in 2017.



STEVE:  Yay.



LEO:  Before our holiday break.  But you're invited to come by and say hi.  Just tune in TWiT.tv/live about 1:30 p.m. Pacific, or go to YouTube or Ustream or Twitch.  We're on all of them.  You get your choice at YouTube, rather at TWiT.tv/live.  If you do go in live, join us in the chatroom.  Nice bunch of people, always kind of have a good fun attitude about the whole thing at irc.twit.tv.  See, they're all saying thank you.  Some people are suggesting maybe a bitcoin mining option in SQRL to raise the attention there, kind of a marketing thing.  You can also get it on demand.



Now, Steve has a great version of it.  He has not only the audio, 64Kb audio, at his website, GRC.com, but he also has the transcripts of it.  So if you like to read along while you listen, or you just want - it's really most useful for searching for concepts and ideas:  GRC.com.  While you're there, pick up a copy of SpinRite.  That's Steve's bread and butter, the world's best hard drive recovery and maintenance utility.  You can also get lots of other free stuff.  It's like the Old Farmer's Almanac of Tech.  It's just chockful of fun and interesting stuff:  GRC.com.



We have audio and video, believe it or not, for the show at our website, TWiT.tv/sn.  And possibly the easiest thing to do would be get your favorite podcast client on whatever device you carry around in your pocket and subscribe so you don't miss an episode.  You just get it automatically, right after we finish on Tuesdays.  I think that's everything, Steve.  Thank you so much.



STEVE:  My friend, my pleasure.  See you next week for the last episode, the last new episode of the year.  And I think our listeners will enjoy the holiday special.  And then we're back in 2018 to do some more.



LEO:  The holiday special:  Twas the night before SQRL.



STEVE:  Ooh, yeah.



LEO:  That's a good idea, though, huh?



STEVE:  Thanks, buddy.  Bye.



LEO:  Bye. 



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#642

DATE:		December 19, 2017

TITLE:		BGP 

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-642.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we examine how Estonia handled the Infineon crypto bug; two additional consequences of the pressure to maliciously mine cryptocurrency; zero-day exploits in the popular vBulletin forum system; Mozilla in the doghouse over "Mr. Robot"; Win10's insecure password manager mistake; when legacy protocol come back to bite us; how to bulk-steal any Chrome user's entire stored password vault; and we finally know where and why the uber-potent Mirai botnet was created, and by whom.  We also have a bit of errata and some fun miscellany.  Then we're going to take a look at BGP, another creaky yet crucial - and vulnerable - protocol that glues the global Internet together.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about the strange case of the Estonian ID cards.  A weird bug or actually flaw introduced into Microsoft's Windows 10.  We're still not sure exactly who got it and why.  We'll also talk about the case of the Firefox plugin promoting "Mr. Robot," and a whole lot more, all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 642, recorded Tuesday, December 19th, 2017:  BGP.



It's time for Security Now!, the show where we cover your security online with this guy right here, the Explainer in Chief, Steve Gibson.  Happy holidays, Steve.



STEVE GIBSON:  Hey, Leo.  I get to say now that this is the penultimate episode of 2017.



LEO:  It is indeed.



STEVE:  Which we've all learned is not the last one, but it's the second to last one.  So, yeah.  Next week we're going to do a fun "blast from the past" special for people who haven't listened for - I think it was seven years ago that we discussed something.  We'll let it be a little bit of a surprise for anyone who has joined us in the last seven years, missed this particular episode that we're going to repeat.  Not the Portable Dog Killer.  Not any of the other things that we've repeated before.  This is a first-time repetition.  So I think it's going to be good.



But this week, for the penultimate episode, I want to talk about something that we've touched on by necessity through the years in the 12-plus years we've been doing this.  Wait, now, we're in year 12, so 11-plus years.  And that is BGP, the Border Gateway Protocol.  It's in the news because of a recent mistake.  Well, no, actually it wasn't a mistake, and that's what sort of raised - it's the fact that it wasn't a mistake that puts it more on our map.  This is significant because the protocol has been around from the beginning of the Internet.  It's at v4, BGP 4, where it's been for a long time.  It's sort of done.  There have been security efforts made, but none that have gained traction.



So it's surprisingly kind of out there hanging, just waiting to get blasted.  And there have been attacks on it, subversions of it through the years, which is why we've touched on it a little bit.  But I thought, on the occasion of a clear, probably state-based deliberate subversion, we need to just plant a stronger flag in this and talk about it because I think it's very clear that, as we move sort of toward a world of state-sponsored manipulation of the Internet, we're going to be seeing more Border Gateway Protocol-based attacks.  So that's our main topic.



But of course we've got lots of news, as we always do.  We're going to talk about how Estonia handled the Infineon crypto bug, which crippled so many of the smart cards that we've talked about for the last couple months.  They came up with their own approach, which I wish the other - I'm sure the other countries are thinking, ooh, why didn't we think of that, because it's kind of clever.



We've got a couple new consequences of the recent pressure to maliciously mine cryptocurrency as a consequence of the crazy valuation of the Bitcoin and Monero and other cryptos.  Actually two zero-day exploits in the very popular and latest version of the vBulletin forum system, which those guys, the vBulletin maintainers did not respond to.  So the guys who found these problems lost patience and just said, well, here it is.  And they're bad.  And so all vBulletin forum systems are currently vulnerable to these zero-day exploits.



Mozilla, believe it or not, is in the doghouse over "Mr. Robot," which is not a phrase I thought I would ever hear myself saying.  But yes, it's true.  We'll talk about that.  And then Win10 had a weird thing happen where they bundled what turned out to be an insecure password manager by mistake.  That got caught by the guys at Google.  And I also want to talk about when a legacy protocol comes back to bite us, as some more actually Project Zero guys at Google found.  Also a way to bulk steal any Chrome user's entire stored password vault, which is sort of simple.  And Google said, yeah, we're not fixing that.  So I just want to make sure our listeners understand that this is possible.



Also, we finally know where and why the uber-potent Mirai botnet was created, and by whom.  So that we kind of, in time for Christmas, we get to put a little bow on that one.  We also have a bit of errata; some fun miscellaneous stuff; and, as I said, we're going to then finish by taking a look at BGP, which is another critical, yet crucial and vulnerable protocol that glues the entire global Internet together.  So I think as our penultimate podcast of 2017...



LEO:  It's a good one, yeah, yeah.



STEVE:  ...we've got a goodie.  This Picture of the Week I just had in my archive.  Sometimes more than one comes in that's just too fun.  And so I have a backlog, and no particular picture caught my eye this week.  So I thought, okay, I'm going to pull one out of the archive that we've never shown before.  I just got a kick out of it.  It's just very clever.  It shows a CAPTCHA license plate with the explanation:  "To ensure that a human is writing my ticket."  And so the guy is holding up a Texas license plate.  It says "The Lone Star State" down below.  And you could imagine that some image recognition license plate snapping, you know one of those photo ticketing stoplight cameras could be challenged by this.  I mean, we can kind of say, oh, that's BJN1484.  But, ooh, boy, if you're not up to speed, especially if you're not expecting to have a highly obscured license plate, well, yes, that's the CAPTCHA.  So I thought that was very clever.



LEO:  We are not advocating this.  I think it's probably illegal.  But go ahead, yeah.  You can't modify your license plate like that, I'm sure.



STEVE:  Yes, I'm sure you cannot obscure it to make it difficult.  I have GRC space COM, and I did put a big...



LEO:  I put a dot on ours, too.



STEVE:  ...color matching dot right in the middle.  And one guy drove past me and was all incensed and said he was going to report me.  And I thought, really?  Because, I mean, if anything it makes it more memorable, rather than less memorable.  Suddenly it's not just six unrelated letters, it's clearly a domain.  But anyway.  Can't make everybody happy.  Oh.  What?  Hello?



LEO:  Hello.



STEVE:  Oh, I'm sorry.  I wasn't sure if we had lost our connection.



LEO:  I just stopped making noise.



STEVE:  Oh.  Leo.



LEO:  I'm still alive.



STEVE:  Okay, good.  Okay.  So the case of the Estonia ID card.  Estonia, as we've talked about before, is a very crypto technology progressive country, and they just jumped on all kinds of latest technological solutions.  Well, they got bit in this case as a consequence of the very widespread use of the Infineon library-based crypto cards which, as we know, generated a weak private key, I'm sorry, a weak, yes, a weak public key which was factorable down to discover the private key that was hidden within it.  So there was some good coverage of this, and I just wanted to - I'm going to share what was written because it explains Estonia's position.



The coverage wrote:  "The Estonia ID card is used nationwide for both governmental and private sector services.  Several critical processes rely on the operability of the digital identity infrastructure, while some of the systems support the ID card exclusively.  In Estonia, mobile ID is also available for authentication and digital signatures in many, but not all services."  So they explain that shutting down all ID cards, which this problem would have created, would have had a severe impact on the entire country, including the economic impact to businesses, probably resulting in the deployment of substitute measures with lesser security standards, while making several services more costly and time-consuming.  So they bought full into the use of this secure crypto smartcard for all kinds of identity, for signing contracts and so forth.



So they write:  "Replacing all the cards physically would have taken a long time given the necessary steps for creating an entirely new card:  choosing the chip, programming and testing the application, acquiring the necessary certification, and procuring the new cards equipped with the new chips.  Only after those steps would the actual replacement procedure take place, limited by the low amount of available personnel in the border and police service points."



They wrote:  "According to our estimates, the process would have taken at least a year, if not more, to complete.  The alternative was to create a solution that would bypass the vulnerability by updating the existing cards.  There is a requirement that keys" - and this is key for basically the guarantee.  They wrote:  "There is a requirement that keys must be generated on the card and never leave the card.  This is required in order to be able to use the ID card to give legally binding digital signatures.  The vulnerability that we," they wrote, "had to bypass was found in the on-chip RSA key generation procedure.  We had to abandon using RSA altogether.



"Thankfully, the ID card chip also supported elliptic curve cryptography, which was not affected by the vulnerability that had been discovered.  The solution was to update the cards to use elliptic curve crypto instead of RSA.  We analyzed the possibilities to continue with RSA by using alternative key lengths, but ruled them out for several considerations."  They said, for example, there's no capability of generating 3K-bit keys within the chip.  And the problem was that the only way to solve the problem that it was too easy to factor the output would have been to dramatically increase the size of the primes in order to bring the difficulty back up, but these chips couldn't do that.



So they said:  "Moreover, the decisions had to be made before" - I love this - "before the article on the vulnerability was published.  In that situation, migrating to ECC was the most viable decision."  



So in our initial coverage we talked about how it was stakeholders in this library, the major producers of the cards and the countries that were using them, did have an early pre-announce window.  And these guys are so committed to the security that they were okay relying on the known defective signing, the known defective encryption, as long as it wasn't made public.  So their window in which they had to fix this was when this knowledge would go generally public.



So they said:  "Devising the concept for the solution itself was done rather quickly, mainly due to the lack of alternatives."  There wasn't much to choose among.  They said:  "Most of the time was spent on the development and testing of the ID card base software and card application, retuning the remote update system, updating the service provider systems to support elliptic curves."



So they had to update the infrastructure to support this and then basically reprogram the existing cards.  They said the two core functions of the Estonian ID card are authentication and digital signatures.  Legally binding digital signatures in Estonia have always been time-stamped, retaining the authenticity of digitally signed documents even throughout this situation, as it is possible to provide evidence that the signature was given before the information about the vulnerability became available.



So we had only talked about this Infineon library problem being a complete catastrophe and basically shutting down everything.  Because the Estonian government chose to, even though they use the Infineon library, the cards that they used also supported elliptic curve, they were able to move the entire country's infrastructure from a crypto based on the difficulty of prime factorization over to elliptic curve.  They didn't talk about how the keys would have gotten much smaller as a consequence, which we know is the case because the elliptic curve problem is so much more difficult to solve that you need fewer bits, where essentially each bit is stronger than you do to protect against prime factorization.



So I wanted to - I think this is probably the end of our discussing this Infineon problem, but it was nice to see that, as a consequence of the fact that the processor in the smart card could do elliptic curve crypto, that they were able to essentially sidestep the problem.  Certainly, I mean, it was a big deal to switch the entire country's infrastructure over.  But they had many months of foreknowledge before the factorization problem became public; and that meant that, before that happened, everybody was switched over.  And thanks to the fact that signatures are timestamped, they would be able always to know whether an RSA-based signature had happened after the disclosure or before, trusting it before and not after, and the crypto should have been moved to elliptic curve anyway by the time of the disclosure.



LEO:  Very smart of them to have elliptic curve as an alternative device.  I mean, that's really interesting that they put both RSA and elliptic curve.



STEVE:  Yes.  Yeah.  And I didn't include it in my notes here, but in their discussion that went on at some length, they recognized how fortunate they were and suggested that, as a matter of policy, any similar future use of widespread crypto for applications like this should absolutely include the ability to update the technology.  It's just, you know, and that's been the history of this podcast is we keep seeing protocols which were initially believed to be secure are found to have some problems.  We keep moving hashes up from, you know, used to be MD4, then MD5, then SHA-1.  Nobody wants to use any of those, although we all did at one point.



Now it's SHA-256, and we're even looking to future next generations beyond that hashes.  And the same thing with crypto.  Some of our early podcasts talked about all of the problems with the WiFi protocols that we were once using.  It's like, whoops, can't use that anymore.  So I think it's very clear we're learning lessons.  We're learning that IoT devices have to have a means of keeping themselves current and updating themselves because they're computers, even though they look like toasters.  So, similarly, a smart card has to have a way to be able to update, to be updated when problems are discovered.  



LEO:  Did they, I mean, it's more than just an update.  They had the capability in there; right?  I mean...



STEVE:  Yes, yes.



LEO:  So they had to have the foresight to do that.  I'm sure it cost more to do that.  They had to have more memory and more capability.  But they had the foresight to do that, which is really remarkable, really.



STEVE:  And to have chosen a chip that had a processor that was able to do it, yeah.



LEO:  Really impressive.



STEVE:  Yeah.  So I read this, and I thought, what?  Except it was covered by Bleeping Computer.  These guys really know what they're talking about.  Kaspersky Labs has discovered an evolved version of an older, from a couple years ago, 2015, older Android malware family, which at the time was known as Podec, P-O-D-E-C.  The updated strain goes by the name Loapi, I guess, L-O-A-P-I.  It has been found in a large number of apps hosted by third-party, not Google Play, repositories, Android app repositories, masquerading either as adult-themed apps or bogus antiviral utilities.  But the kick here is, okay, so if users don't remove this from their Android devices, when left to its own means, this Loapi malware, which is itself capable of all kinds of different malicious behavior, will eventually get around to downloading a Monero cryptocurrency miner, which it runs, ignoring all - yes, exactly.



And I said at the top of the show there are two stories which are a consequence of the pressure now to mine cryptocurrency.  Web browsers are doing it, and they're just trying to, you know, everybody's trying to sneak this in and steal CPU cycles.  So this thing will overheat and overwork the phone's components, ignoring all power management throttling, which ends up making the battery bulge and deform the phone's cover or worse.  So it's literally destroying, ultimately destroying people's phones.  The original malware, which was just used to bypass the advice of charge, so-called AOC, and CAPTCHAs, which then subscribed their victims to premium rate SMS services, that was a couple years ago.  Today this Loapi malware is far more advanced.  Kaspersky experts called it a "jack of all trades."



First of all, as we just saw, it can mine the Monero cryptocurrency.  It can install a proxy to relay the phone's traffic; inject ads in notification areas.  It can inject ads in other apps; open URLs in browsers also used to show ads; download and install other apps launch DDoS attacks; interact with the Phone's SMS functions; and crawl web pages, apparently to subscribe users to premium SMS services and more.  So the real takeaway here, folks, is really be careful where you get your software.  The fact that this is - I saw a picture of the icons that this malware was behind, and I was sort of wishing they had done a better job of blurring the adult themed icons because they weren't quite blurry enough.



So there are all these apps in the non-Apple Play store, non-curated repositories, and I'm sure there are people who are wanting to get things for their Android devices that you can't get from the Google Play Store.  But getting them really is a mixed blessing.  So I would say, first of all, be very careful with where you obtain apps for your Android devices.  And also, the first moment you notice your phone getting hot, consider what you may have recently installed and get rid of it, if you can, because if it happens to be this thing, it looks like it just very basically melts down your phone, given time.  Which seems a little antithetical to its purpose.  If it destroys the hosting platform, then it's not going to get any more crypto mining there.  But I guess they just want to squeeze it as hard as they can for as long as they can.  And then...  



LEO:  And you'll get a new phone, and then [indiscernible] because you need that wallpaper app.  You've got to have it.



STEVE:  That's right.



LEO:  Although I have to say you can't trust stores anymore at all.  You saw that Apple had two counter - we talked about one last week.  And now there's another counterfeit app.  Who's minding the store?  I don't understand how this happens.  I mean, that's not malware.  It was just ripping people off.



STEVE:  Yes.  Or in this case who's "mining" the store.



LEO:  Now you've got your title, I tell you.



STEVE:  That's right.  And in a similar - I said that there were two instances of consequences of this increased pressure to mine.  There's another campaign which has been named Zealot, Z-E-A-L-O-T, by the guys at F5 networks who found it.  It is also mining Monero, which seems to be the one to go for these days, on Linux and Windows servers.  The guys at F5 Networks found evidence of an aggressive and sophisticated malware campaign currently underway, targeting Linux and Windows servers with an assortment of exploits whose goal is installing Monero cryptocurrency mining malware.



And it's interesting, I mean, this makes sense when you think about it because, first of all, servers have a public Internet-facing presence so you can scan for them and find them.  And they're typically beefier than end-user - certainly they're beefier than light bulb processors.  A server machine has got a lot of RAM and lots of cores and heat sinks and fans.  And it's going to be a much more effective miner.  So again, it sort of makes sense that, if somebody was thinking, scratching their head, huh, where could we find some ready-to-go CPUs to use for cryptocurrency mining, it's like, ah, servers.  Well, if you can get them.



So the F5 guys noted that the attackers are apparently fans of StarCraft since many of the names found in the code and the files of the campaign are taken from the game.  First of all, of course, Zealot.  But then also Observer, Overlord, Raven, and others, all apparently familiar to StarCraft fans, one of which I am not, but I'll take their word for it.  So for Linux servers they are scanning for the now famous Apache Struts flaw, which as we know was the flaw used for the Equifax breach, on the Linux side, and the so-called DotNetNuke ASP.NET Content Management System vulnerability, which exists in unpatched Windows servers.



So they're targeting both Linux and Windows using well-known previously fixed flaws, assuming that there are some machines they can find that are publicly exposed and don't have those things fixed yet.  And essentially, so they're using those problems to get into the system, then using Windows PowerShell to download and install the final stage malware in the course of Windows, or the Apache Struts flaw to use Python scripts, which apparently were lifted from the EmpireProject, which is a post-exploitation framework, very popular, to then install the Monero miner software.  So again, it's sort of clever that they're using scanning for publicly available servers and targeting servers because they're more effective crypto miners than people's light bulbs and toasters and DVRs that typically have much more modest processing power.



I mentioned also at the top of the show something of concern currently, as in right now, for vBulletin.  vBulletin is one of the oldest, longest running, very popular forum software.  It was written, I think, initially in 1999, so 17 or 18 years old.  It's gone through several versions.  And then actually there's been some churn and some controversy as developers came and left.  And Zen 4.0, which is the web-based forum software that I ended up choosing, was written by some guys who left vBulletin in order to do a complete rewrite.  And so there was some litigation as a consequence of that that ended up getting settled a few years ago.



But in this case, web forum systems have been traditionally very difficult to secure.  I mean, they have been, though not a lot recently, historically they have really been a problem because their nature inherently allows remote users to submit posts, which are stored, typically on SQL server backends.  Then those postings are parsed by the server and displayed on everyone else's browsers.



So as we know, the famous example of a user naming themselves Johnny Drop Table, which combines some SQL commands with something that the server would read from the database while it's displaying the forum page and, I mean, actually has in the past caused tables to be deleted from the forum software.  And cross-side server vulnerabilities, I mean, all kinds of problems have surfaced on forum software because the framework being used had vulnerabilities which clever hackers were able to leverage for their own advantage.



LEO:  Also PHP is kind of horrible, and they're almost always in PHP.



STEVE:  Yes, yes.



LEO:  I think vBulletin, was it a successor to PHP My Bulletin?  I bet it was.



STEVE:  Yes, I think that's the case.



LEO:  And, yeah a more modern version of that.



STEVE:  And so they've gotten better.  And for what it's worth, because of this historical problem, I have built another server.  Actually, I did it last summer, ready to be used.



LEO:  Isolated, yeah.



STEVE:  Yes.  And it is on physically separate hardware.  I have a physical firewall between it and the rest of GRC, and the entire network is on its own physically isolated network because I just absolutely, I mean, I'm not writing a bulletin board.  Everyone's glad that I'm not writing my own bulletin board system from scratch.  Yes, I would love to; but, no, that's just not practical.  So what I'm doing is I'm using the best that I could find, and I really like Zen 4.0.  But I can't trust it, and I can't allow anything that happens there to infect the rest of GRC.  So as you said, Leo, it's on a physically separate box with complete network isolation.  So, yes, if something nasty crawls into it, well, at least it'll be contained.



So two different groups found zero-day, well, found existing vulnerabilities in vBulletin, bad ones, remote code execution vulnerabilities that not only allow them to execute code, but to do so with elevated privileges.  In both cases they contacted the vBulletin maintainers and, after more than a month, or I guess not quite a month, actually, got no response.  Never received an acknowledgment, an "Oh, crap, we'll fix this immediately," nothing.  So they went public with them.  And not only with full disclosures, but also proof-of-concept code demonstrating these vulnerabilities.  So they are in the wild.  They are effective against the most recent v5 of vBulletin.  And they've not been patched.



So I don't know, unfortunately, there's no real takeaway.  I don't know, it's not clear whether the servers that are hosting this vBulletin would be subject to compromise.  I mean, certainly they would.  I guess it depends upon what the attacker wants to do.  If the server's compromised, that allows them to get into the network that the server is residing on and then probably spread from there.  So they might just be using vBulletin's flaws as a way into the Intranet that is hosting that server, meaning that users, external public users of that vBulletin forum would not be attacked.  On the other hand, if the vBulletin system has been compromised, it could be used to turn around and maliciously attack visitors to that forum.



So I'll be keeping an eye out for updates.  And of course the problem is that many of these sites are not being updated regularly.  So although it doesn't look, as far as we know, it doesn't look like there are any fixes to that now, which is why these problems were made public.  Even when they are, forum software tends to lag and not to be kept current.  And because of the history of problems and security problems, they are being updated, at least by people who are being responsible.  I can't explain why there was no response from the vBulletin people.  I haven't looked into whether, in general, they are being responsible or not.  If anyone's interested in more details, I have links and the various CVEs that have been issued to cover these vulnerabilities.



But, I mean, and in fact you mentioned, Leo, about PHP, and that is absolutely the case here.  The first vulnerability discovered in vBulletin is a file inclusion issue that enables remote code execution.  A remote attacker is able to include any file from the vBulletin server and execute arbitrary PHP code against any file installed on Windows.  The disclosure includes working proof-of-concept exploit code to show the exploitation of the vulnerability; and in this case a CVE, the Common Vulnerabilities and Exposures number, has not been issued.



The second vulnerability has been assigned a CVE.  It's 2017-17672 and is described as a deserialization issue that an unauthorized attacker can exploit to delete arbitrary files and execute malicious code in some circumstances.  The vulnerability arises from the unsafe usage of PHP's unserialize function on user-supplied input.  So in both cases, as you thought, PHP is the underlying problem because that's the implementation language for this BBS software, this forum software.



LEO:  It always is.  The problem with PHP, it can be written securely, it's just that it's so easy, and it allows anybody to write code and doesn't really, yeah, gives you all sorts of dangerous tools.



STEVE:  Yup.



LEO:  It's not good.



STEVE:  Yup.  Now, okay.  And get a load of this.  When I first saw this I thought, what?  Mozilla has gotten themselves in trouble.



LEO:  Yeah, this pissed me off, actually.



STEVE:  Yeah.  By force installing what appeared, well, what is a promotional add-on relating to, of all things, "Mr. Robot."  I mean, the "Mr. Robot" show.  So Mozilla and Firefox stumbled into the doghouse last week when they used a facility called Firefox Studies which is built into all recent versions of Firefox and enabled by default.  So the takeaway for our listeners may be that you might want to consider disabling this because you can, which would have prevented this from happening.  It didn't affect all users.  But essentially what happened is that - and it's not clear whether it was done as a test, or if they wanted to stick their toe in the water, or what.  Because it didn't affect all users.  But they installed this apparently promotional add-on without their users' knowledge or consent.



So, okay.  So first of all, this Firefox Studies facility is something that allows Firefox's developers to update their users' browsers with additional code for whatever purpose they may have.  In this case, it took the form of an add-on named Looking Glass v1.0.3.  And Looking Glass is a mystery.  It's not clear what that's about.  However, this Firefox Studies can be disabled on Firefox by opening, under the Settings, going to Privacy and Security, and turning off "Allow Firefox to install and run studies."



So this was done under that setting's default enabled state.  So Mozilla's own support page said, which they put up to explain what was going on, they called it "Through the Looking Glass," and they said:  "What's happening?"  They ask rhetorically:  "Are you a fan of Mr. Robot?  Are you trying to solve one of the many puzzles that the Mr. Robot team has built?  You're on the right track.  Firefox and Mr. Robot have collaborated on a shared experience to further your immersion into the Mr. Robot universe" - this is them writing this - "also known as an Alternate Reality Game (ARG).  The effects you're seeing are part of this shared experience.  No changes will be made to Firefox unless you have opted into this Alternate Reality Game."  Okay, except they installed an unwanted add-on, so I would call that a change made to Firefox.



Then they ask themselves:  "How do I opt in or out?  To participate, install the Looking Glass add-on from" - and then they have the add-on Looking Glass link.  I have it in the show notes, if anyone wants it.  This add-on is available only in the U.S. in English.  Then they say:  "If you no longer wish to participate in the shared world experience, enter about:addons into your address bar and remove Looking Glass."



And then they say, kind of confessing here, "Looking Glass was previously delivered as a shield study, so you might see Looking Glass II or Pug [P-U-G] experience in your past studies in about:studies.  It has already been removed as a study and moved to an add-on so you do not need to take any further action."



Anyway, this whole thing is a mess.  And as you said, Leo, it has angered a lot of Firefox users that Mozilla was doing something like involving something that seems to be promotional.  It's like [crosstalk]...



LEO:  Well, it has.  I mean, I don't know if - they say they weren't paid.  But that's just dumb, then.



STEVE:  Yeah.  Yeah, exactly.  Exactly.  Not to have at least gotten something in return.



LEO:  They didn't make it better, just made it dumb.



STEVE:  Yeah.  So, wow.  And the problem is I guess they didn't want to push a question out to all Firefox users.  Are you watching "Mr. Robot"?  Do you want to participate in an alternate reality game?  You know, that would have been annoying, too.



LEO:  If I do, I will install the extension.  Don't put that in your main install.  That's ridiculous.



STEVE:  Yes, yes.  And don't push it in anyway.  Somehow, I mean, I don't think there is a good way to ask a generic browser user who you have no reason to believe has any interest whatsoever in "Mr. Robot," good as it may be, there's just no way to inflict that upon them.  That's not Mozilla's job.  They really do, as you said, they need to stay neutral.  So bad on them.



And this wasn't advertent or deliberate on Windows 10's part.  But it's kind of a problem when Windows 10 has chosen to bundle and install a password manager in all Windows 10 systems which turns out to be vulnerable.  And turns out to have known that it could have been vulnerable.  Our good friend Tavis Ormandy, he wrote in the Chromium bug project, under Project Zero, his title was "Keeper:  Privileged UI injected into pages again."  And he wrote, Tavis wrote:  "I recently created a fresh Windows 10 VM with a pristine image from MSDN" - that's the Microsoft Developer Network.  I'm a member, and Microsoft developers are.  You're able to get - what?



LEO:  Well, I just want to point out that seems to be in the MSDN version.  It's not in any version, I've tried all my Windows machines.  Keeper is not installed on any of my Windows machines.



STEVE:  Oh, no kidding.  So it's...



LEO:  It must be something they put in MSDN for developers as a kind of [crosstalk].



STEVE:  Oh, good.  Oh, okay.



LEO:  I mean, at least I can't find it.  I'll ask tomorrow on Windows Weekly.  Obviously we'll be talking about it.



STEVE:  There is a reddit link I have in the show notes where - and again, I didn't go there to look.  So let's hope that it never went public.  That would be great because...



LEO:  Well, they put stuff in MSDN versions of Windows they don't put in other versions of Windows, I presume, for developers.



STEVE:  Okay, good.  So I don't know either way.  But anyway, Keeper that was in this MSDN version, which, well, it did have a problem that Tavis had previously reported to them.  He did say he's not the only person who's noticed this with their link to reddit.  But it may have been other developers.  I don't know whether it was in the public or not.  So I'm glad you add that caveat, Leo.



He says:  "I assume this is some bundling deal with Microsoft."  He said:  "I've heard of Keeper," he said, "and I remember filing a bug a while ago about how they were injecting privileged UI into web pages."  He says:  "I checked, and they're doing the same thing again with this version.  I think I'm being generous considering this a new issue that qualifies for a 90-day disclosure as the same attack [still] works.  Nevertheless," he said, "this is a complete compromise of Keeper security, allowing any website to steal any password.  Here is a working demo that steals your Twitter password."  And he provides a link to a demonstration of this happening.



So the tech press coverage had headlines like:  "For eight days, Windows bundled a password manager with a critical plugin flaw."  Which may or may not be overblown, depending upon whether this was actually pushed out to end users.  So the good news is the Keeper folks responded instantly to Tavis, fixed the problem by removing some functionality that contained this problem, and had all of the patches pushed out within 24 hours.



So Keeper itself I guess has been around for years, and it's a well-known password keeper extension available on Edge and Chrome and Firefox.  The takeaway for our listeners is you want to make sure, if you're a user of Keeper, that you're at 11.4.4 or newer since they fixed that moving forward.  So anyway, I guess we'll find out whether it ended up being made public, or whether this was just MSDN.  I don't know.  I think that Tavis is suggesting that this was the image that, well, we know he's suggesting or he's saying that it's the one that he got from Microsoft.  Who knows whether it went out public.  And Leo, it's time for me to sip my coffee.



LEO:  Indeed, indeed, indeed.  And I'm just asking the chatroom if any of them saw Keeper installed.  And it looks like - I have Keeper on the Fall Edition, just my luck.  So some people might have it.  That's [chatroom handle].  That's interesting.  I mean, I've used Keeper.  I wonder if that bug in Keeper is similar to the bug - there was a cross-site bug in LastPass, as well, that was fixed; right?  Is it similar to that, I wonder?  Or maybe something else?



STEVE:  I think what upset people, as I understand it, is unlike other Windows 10 bundles where you get a link to install something, this actually installed it.  I mean, it was there installed by Microsoft with this update.  And as we now learn, for a window of eight days, apparently, there was this vulnerability present.



LEO:  Yeah, hmm.  More research will be done, and we will report back.  Yeah, I mean, it may be they put an ad for it, but that still doesn't install it.  You have to...



STEVE:  No, no, no.  It's installed.



LEO:  You say it's installed, yeah.



STEVE:  Yeah.  So, for example, here in reddit:  "I just reinstalled Windows 10 today, and I was uninstalling all the bundled apps like usual, and I noticed that Keeper password manager is pre-installed now.  I've never seen this come installed with Windows before."



LEO:  The Windows 10 I'm using was installed from the Windows 10 Media Creator and does not have Keeper installed, does not even have anything promoting Keeper installed.  So I just don't know.  Yeah.  It's unclear.  But I'll ask Paul Thurrott.  Windows has many versions.  Many versions of Windows exist.



STEVE:  Ah, yes.  Not simple.  The coverage of this talked about Windows 10, but this is really not a Windows 10 issue.  The problem is much bigger than that.  There's a technology which has been around since the very beginning of the early days of the Internet, even before 1996, known as "proxy auto-config."  The problem that this was intended to address is that a browser comes alive, wakes up launched in a machine, and it needs to have a way to get to the public Internet.  Yet there may be no direct connection.  It may need to run through a proxy, but it may need to be configured for that.



And those of us who've been around a long time may remember the days when you could manually configure the proxy, and those dialogs are still around if for whatever reason there isn't an auto-config.  Well, there is this auto-config capability which is a protocol that Netscape, I mean, the Netscape, back in the early Netscape Navigator days, they designed it.  And they decided that the file which would be used, a PAC file, would be JavaScript.  So not XML, not something less capable, but full-on JavaScript.  So probably because that's the language that they had in the browser already, and they said, oh, let's just have the auto-config operate by running a JavaScript script.  So, okay, we have that.



Now the next problem is where do we get this file?  How does the browser know to get the file?  Or maybe how does the Windows OS itself know to configure access for itself to the Internet?  And so essentially all Windows systems have this proxy auto-config and the protocol that carries it, WPAD, Web Proxy Auto Discovery protocol, enabled by default.  It exists, because it's a standard, it is also - it's not just something random that Netscape came up with.  It exists as an IETF draft which, although it expired in 1999, here we are in 2017, and every Windows machine will ask the network when it boots up, essentially, hey, where can I find a JavaScript file to execute?  And it looks, it asks for this through DHCP and then DNS and then the WINS, W-I-N-S, and maybe even some other solutions.



So basically it casts about all over the local network, querying for anyone who will respond with the URL where this JavaScript file can be found and executed.  Even Chrome does this, but it executes the JavaScript in a sandbox to limit any damage that can be done.  The problem with Windows is that, even though there are newer JavaScript interpreters available, it still uses the old original JScript engine which has a whole bunch of known vulnerabilities.



So Google's Project Zero guys took a look at the JScript engine and generated a complete soup-to-nuts execution exploit.  And what's a little more worrisome is that there are a number of ways of actually pulling this off.  For example, if you were a machine on an Intranet where a Windows machine comes up and first sends out a DHCP broadcast saying "I need the WPAD PAC [Proxy Auto-Config] file," if you beat the legitimate DHCP server that may be running on that Intranet, assuming that there is one, if you beat it to the punch, you're able to provide the URL to the Windows machine for its configuration and essentially cause it to run JavaScript of your own design.



And unfortunately, since it runs it on the older JScript engine, probably for compatibility reasons because in fact it was one of the - the lack of features in JScript gave the Project Zero guys some headaches because they couldn't run their normal JavaScript find-bugs-in-JavaScript code because there were too many things that were not supported by the old JScript.  So I'm sure that it's still running JScript, specifically to be able to leverage for compatibility reasons the code that once a long time ago ran and is still being run today.



So the big problem is that there's a list of things that's being done.  If no WPAD PAC file URL is obtained from DHCP, then DNS is queried.  And believe it or not, it's queried publicly.  And there have actually been exploits in the wild where a public WPAD DNS URL was used, was being asked for, and a URL returned that a system would then execute.  And it's .local is the domain, but it is being made publicly to the public DNS.  So it turns out that the Google guys came up with a way of answering the query, which are going out into the public, generating some code which would be executed on any version of Windows.  They talk about Windows 10, but there's no reason that it wouldn't work on any version of Windows for the last 10 years, at least.  And take advantage of flaws in the JScript engine in order to execute their own code.



So they found multiple vulnerabilities; engineered a highly reliable proof-of-concept exploit.  And the reason they just went public with this is that there's nothing necessarily to be fixed.  I mean, this is all using long-established standards through multiple editions of Windows.  And they suggest that the only solution they came up with was for Windows to disable looking for this WPAD PAC, the proxy auto-config file, or to block it at your border so that your own Internet queries don't go out into the public, or make sure you provide an answer locally so that you sort of snub the systems looking for this.



And I don't know if anybody here who's listening to the podcast has sniffed Internet traffic and Intranet traffic.  You see these WPAD queries all over the place.  I mean, Windows systems are actively asking, making these queries, looking for an answer.  And the Project Zero has demonstrated that this represents a significant danger to Windows, for which there's no clear solution at the moment.  So I thought that was a little bit chilling.  But it's when old protocols that are still in use come back to bite us.



We've talked about Mirai extensively, of course, because it generated the largest denial of service attacks the Internet has ever seen.  It brought down the very large DynDNS service.  Brian Krebs of Krebs on Security was under such a sustained high-bandwidth attack that his longstanding service, Akamai, said, "We're sorry, Brian, we'd like to help, but these attacks are larger than we're able to afford and absorb.  So you're going to have to find somebody else to host your DDoS protection for you."  And we've talked about it.  It was, what, some routers and DVRs and basically Internet appliances, Internet-connected appliances that the Mirai botnet was more effective in corralling and commandeering and finding and spreading to than anything we'd seen before.



And we've said on this podcast months ago, why?  What is its purpose?  What is its application?  What is it trying to do?  I mean, because it really frightened the security engineers and folks who track these things, generating multiple terabits of thousands of gigabits, thousands of thousands of megabits of attack traffic.  Essentially, something that nobody could withstand.  Well, the FBI got on the case and tracked down the origin of this.



LEO:  Who was it?  North Korea?  China?  Those Russkies?  Who was it?  Who did that?



STEVE:  Three college kids.  They were running...



LEO:  Not so bright college kids, either, by the way.



STEVE:  They were, yeah, they were running a Minecraft server, and they wanted to attack other Minecraft servers and force them off the 'Net in order to capture some of their Minecraft users.  Apparently, and you know about Minecraft more than I do, Leo, but apparently if you run a popular Minecraft server, you can make money hosting Minecraft.



LEO:  Yeah, sure you can, yeah.



STEVE:  And so these guys - so again, the dollar underpins the motivation.  They wanted more users on their Minecraft server.  So they said, okay, let's go blast some other Minecraft servers.  So they built this botnet which ended up being way more powerful than they needed.  So not only could they blast any Minecraft server that they could find, but it turns out that because DDoSing Minecraft servers is a thing, there are Minecraft DDoS hosts that specialize in hosting and providing bandwidth to Minecraft servers to protect them from DDoS attacks.



Well, the Mirai botnet was so powerful that they were able to bring down the DDoS protectors, the Minecraft DDoS protectors, and essentially take everybody, take all of the Minecraft servers off that were behind the Minecraft DDoS protectors.  I mean, it was just this uber powerful, as we've discussed, just crazy powerful system.  So the lead guy just pleaded - he's a college kid.



LEO:  Rutgers, yeah.  



STEVE:  Yes, at Rutgers.



LEO:  Very proud of him.



STEVE:  Just pleaded guilty in an Alaskan court.  He had two guys that he was working with.  And our listeners will remember when we covered the fact that the Mirai source code had appeared online.



LEO:  Senpai.



STEVE:  Yup, exactly.  And we believed that - it was presumed that the authors of Mirai had let it go public in order to obscure themselves so that other people would pick it up and start using it, in order to sort of, like, spread the blame around, essentially.  Well, before they did that, the FBI was already zeroing in on this little trio.  And so it turns out that they were not the people who blasted the DynDNS and brought all this attention to Mirai.  Their own use of their own original Mirai botnet was more focused on Minecraft.  But by releasing it to the public they turned a lot of other - and here's your favorite word, Leo - "miscreants" lose.



LEO:  You miscreants.  Get off my lawn.



STEVE:  And it was other people who weren't probably themselves sophisticated enough or hadn't done the work of reverse-engineering the various IoT devices.  Remember that Mirai was sort of a living organism.  Shortly after a new exploit was found, it would get incorporated into this botnet, so it was constantly increasing its power and sophistication.  By releasing the source publicly, anybody could get on the Mirai bandwagon.



And in fact it was some other people that were involved in blasting DynDNS off the 'Net and causing a big chunk of the eastern portion of the U.S. to go dark as the caches of existing DNS queries drained, and the resolving DNS servers tried to go back to Dyn in order to get their records refreshed, but found that Dyn was off the 'Net. So anyway, that's the story behind Mirai.  It was three college kids who didn't like competition from other people's Minecraft servers and thought, okay, we're going to blast them off the 'Net, cause their systems to be slow, and thus collect other users onto our server.



LEO:  Were they at some point selling DDoS as a service?



STEVE:  Yes, yes.  There was also - it occurred to them, hey, you know, we could make some money by selling this to other people.



LEO:  Hooligans.



STEVE:  Yes.  What are you going to do?  Unfortunately, I guess they're probably 21 by now, so they're probably being held responsible - or probably 18, rather.  I'm sure they're 18.



LEO:  Oh, yeah, yeah, yeah.



STEVE:  So, yikes.  So, okay.  I'm less overheated about this than several of our listeners who sent this to me.  But Google does classify this, not as a bug, but as a feature.  And that is the fact that anybody can essentially steal all of another Chrome user's saved passwords, form fields, bookmarks, and history, if they have physical access to their browser.  Okay, well, now, the reason this is not that big a deal is that we've already talked about how it's possible, if you sit down in front of someone's computer who's using Chrome, with a few pokes around the UI, you can go to their passwords and tell it to show the passwords, and there they are.



So, okay.  So if you allow anyone physical access to your Chrome browser, or for that matter many other browsers, they'll let you see the passwords, without having to authenticate.  I mean, it's just there.  So someone posting on Medium.com explained how that process can be done for all of Chrome's synchronized saved things.  And it doesn't take any great rocket scientist to do this.  You sit down in front of somebody else's Chrome browser.  Go to settings/manageProfile, and then on that page - so it's chrome://settings/manageProfile.  And then click on Edit Person, or you could go to chrome://settings/people.  Then you sign that person out.  So you sign them out of the browser.  You then click Sign into Chrome and use your, as the attacker's, Gmail account.



So then Chrome helpfully notes that another user - and provides their Gmail account name - was previously using Chrome, and you're then given the choice of choosing between "that wasn't me" or "yes, that was me."  So you say yes, that was me.  And along with the "that was me" is "add my bookmarks, history, passwords, and other settings to" - and then the attacker applied Gmail account.  And then you click Continue.  And essentially that's exactly what happens is all of the, essentially, the profile of that Chrome user who you just signed out from is assumed to be the same person because you just said it was, as this other Gmail user and all of their profile, including all their passwords, are merged into the newly logged-in Gmail account.



So again, the takeaway is just be aware of that, that this browser synchronization will work across Gmail accounts and that, if somebody had access to your Chrome browser, they could sign out, sign in as a new person and say, oh, yeah, that's me, too, and merge all of your stored Chrome profile stuff into their own account and then browse through it, use it, log in as you, assuming there's no second-factor authentication, at their leisure.  So again, not a huge issue because that could be done on a site-by-site basis just through the Chrome UI.  But this does do the whole thing at once, which certainly could be a concern.



I didn't know where to put this.  I thought, okay, we haven't had any errata for a long time.  So I got a tweet from a Drew Green who said:  "@SGgrc Been listening to previous episodes and heard you and Leo discussing Google's Advanced Protection Program.  A Bluetooth two-factor authentication device isn't necessary."  He said:  "I found it works with an NFC YubiKey in place of a Bluetooth device."



LEO:  Yeah.  You can use the NEO, but you can't use it on iOS.



STEVE:  Ah, okay.  And so he said:  "Works with an NFC YubiKey in place of a Bluetooth device, with the other device being a different USB-only YubiKey."  So anyway, I wanted to add that to our knowledge base, that you don't need Bluetooth.  You can use an NFC YubiKey.



LEO:  Unless you have iOS or a phone without NFC.



STEVE:  Right.



LEO:  So YubiKey offers the NEO, which is an NFC-capable YubiKey, as well as the YubiKey 4, which you should use because it's the more modern one.  But you want the Bluetooth just because there's devices that don't use NFC.



STEVE:  Ah, okay, good.



LEO:  I think that's why Google just says it.  You're right, they could qualify it and say, "If you have a device that doesn't support NFC, get the Bluetooth dongle."



STEVE:  Right, right.  And a little bit of miscellany.  There is a "Be a Coder" Humble Bundle which had a whole bunch of No Starch Press books available.



LEO:  Yeah, some of my favorite LISP books in it.



STEVE:  Yes, yes, yes, yes.  And they look like actually a lot of fun titles in there.



LEO:  Oh, they're great, yeah, yeah.



STEVE:  So there's about 13 days remaining as of this podcast on December 19th.  So it's called "Be a Coder."  So you can just go to www.humblebundle.com, and you'll find it there, and four different sets of books.  And I browsed through them.  As you said, there's LISP, and there's...



LEO:  "Learn You a Haskell for Great Good!" is such a good book.



STEVE:  Uh-huh.



LEO:  I have many of these books, actually.  It is such a good book.  I have not read Erlang.  That's a kind of a copy of the classic by Miran.  But, yeah, these are great:  "Clojure for the Brave and True," have that.  "Realm of Racket," have that.  "Land of LISP," have that.  You probably have "The Art of Assembly Language," I don't know.  



STEVE:  I don't, but I know the author.  He's at UC Riverside and has put together a very nice introduction to assembly language.



LEO:  Randall Hyde, yeah.



STEVE:  Yeah.



LEO:  Now, these are just eBooks, though.  You don't get the physical books; right?



STEVE:  Correct, correct.



LEO:  "Ruby Under a Microscope," "Understanding ECMAScript 6" - there's some good stuff in here.  "The Book of R."



STEVE:  Yeah, of course ECMAScript 6 is actually JavaScript, so that is the current scripting language for our browsers.  So the funds are donated.  They offer donating, or they suggest donating to the EFF, which of course is a great cause that we all support.  So take a look and check out this "Be a Coder" bundle.  As you said, Leo, there's a bunch of good stuff there.



LEO:  Good stuff.  Some real classics in here, yeah.



STEVE:  Somebody got a kick out of doing some math with the current bitcoin valuation.  ZeroHedge noted - and thank you, Simon Zerafa, for forwarding his tweet.  Okay.  Back when the FBI confiscated the bitcoins which were in the possession of Silk Road back then, they liquidated their bitcoins and got a tidy sum, $48 million.



LEO:  Oh, not bad, hmm.



STEVE:  But they would currently be worth 2.4 billion.



LEO:  Geez Louise.  I just don't get this.  It's a bubble; right?  It's not - I don't understand.



STEVE:  I like it.  I like it.



LEO:  Yeah, you got some sitting in the corner there.  What would it have to get to before you said, yeah, I really feel I should - you know, I was thinking, you have a bad hard drive.  Well, gee, it's too bad you don't know of any programs that could fix bad hard drives.  Here's the guy who makes SpinRite, and he says, "Oh, it has a bad hard drive.  I don't know if I want to."



STEVE:  Well, actually, as far as I know the RAID is fine.  It was the motherboard that got crazy.  And so I just moved the RAID to a different motherboard.



LEO:  Yeah, that's easy.



STEVE:  It's probably there.  So Tavis...



LEO:  You're just going to sit on it, aren't you.



STEVE:  I'm going to kind of wait and see what happens.



LEO:  What if it got to like 100,000?  Then you'd be worth five million bucks.  What if it got to a million?  Then you'd be worth 50 million bucks.  I'd take it.  But then [crosstalk]...



STEVE:  Yeah, well...



LEO:  Maybe it'll get to five million.



STEVE:  Maybe it's going to pop and crash.



LEO:  It's going down.  It's now at 17.5 again.



STEVE:  Yeah.



LEO:  I don't know what to do.  I don't know what to do.



STEVE:  Okay.  So Tavis tweeted a little something sort of short and pithy, which I just liked because this is what we do on the podcast.  He tweeted:  "Everyone wants there to be simple answers in security.  But sometimes there are no simple answers."  And we've often said that.  I was thinking of that in the context of SQRL, that it's a two-party system.  I want it to be as easy to use as possible.  I want it to be bulletproof.  But as I have said, the user has to bear some responsibility.  That's going to be a sticking point because everybody is used to having an, oh, you know, I don't know what that is.  I forgot my password.  Click this link.  I mean, it's incredibly insecure to do that.  I mean, it's all we've got.  It's possible to have much more secure solutions.  But they're just not there.  As Tavis says, everyone wants simple answers in security, but sometimes there are no simple answers.  It's true.



And I did want to just follow up with the iOS 11.2.1, both for iOS and for tvOS, that the anticipated patch that we discussed last week to fix the problem in HomeKit did happen.  So iOS is now at 11.2.1, which fixed that HomeKit vulnerability and restored that remote access for shared users which they had briefly neutered at the server side in order to fix a couple of those problems that we discussed at greater length last week.



And also, speaking of bitcoin and liquidating, Leo, a bunch of people said to me that any sale would qualify as a capital gain, as long as the asset had been held for more than a year.  I wanted to verify that because it would be a significant effect, and I verified, yes, they are in fact...



LEO:  Yeah.  They're an investment, really; right.



STEVE:  It is the case that I've had them for many, many years.  It would be treated as a capital gain, as a sale of a capital gain held for a long time, or long term.  So in the U.S....



LEO:  If this new tax bill passes, you're a corporation.  It was a corporate creation.  It's a mere 21 percent.



STEVE:  Well, actually, as a capital gain it's 15.



LEO:  Oh, 15.  Oh, well, we're happy now.



STEVE:  Yeah.



LEO:  Or you could just pretend you didn't get anything.  How would they know?  Maybe they listen to the show.



STEVE:  Yeah, well, and Leo, as I said, if I were behind bars, people might look askance at my security advice.



LEO:  Good point.



STEVE:  How good a security expert is he when he's been locked up?



LEO:  Excellent point.



STEVE:  Anyway, and we all know I'm not greedy by nature, so what the hell.  Okay.  Two notes about SpinRite real quick, just tweets.  Nothing makes my day more than #SpinRite saves the day.  Philipp with two Ps, P-H-I-L-I-P-P, tweeted:  "SSD drive was failing.  Had everything backed up" - dot dot dot - "except my private GPG key.  Damn!" in his tweet.  He said:  "A Level 2 on the drive took 26 hours, but the drive was mountable again, and I rescued the key.  Thanks for a great product."



LEO:  Nice.



STEVE:  So once again, we are now, I mean, this is why there will be a SpinRite 7 is because it turns out it fixes solid-state drives, too.  Who knew?  And I'm going to keep the name.  It's not going to be SSDRite or anything, no.  It's got to be called SpinRite, even though it's not spinning anymore.



LEO:  Someday you'll have to do a bit saying, you know, it's called SpinRite because in the old days hard drives moved.



STEVE:  Yeah, once upon a time.



LEO:  You may not remember this.



STEVE:  And also Gregory Paul said:  "My two USB keys booting my FreeNAS failed, both around the same time.  Thanks to @SGgrc for SpinRite, which bring them back online" - okay, I didn't really read this closely, which he's saying "brought them back online in time to migrate from USB keys to an SSD."  So he said:  "#FreeNAS and #SpinRite."  And I should mention that one of the things you want to make sure FreeNAS is not doing is swapping at all.  You want to make sure that your swap drive is in RAM because you absolutely don't want to have an unnecessary swap drive for an instance of Linux or FreeBSD which is actively swapping on solid-state memory because it'll kill it.



So I don't know that that's how these two USB keys died because we know that they do tend to die.  Thank you, Gregory, for sharing the fact that SpinRite was able to bring them back.  And I'm really glad because it's much easier, obviously, just to copy those over to an SSD and then boot that.  But for what it's worth, anybody running any sort of a turnkey system from a solid-state drive, whether it's an SSD or any kind of non-volatile memory, make sure that your swap drive is not on that physical media.  You want it to just be in RAM because, as we know, that media does not like to be written to.



LEO:  Okay.  Border Gateway Protocol, BGP.



STEVE:  Border Gateway Protocol, BGP.



LEO:  Not Bill Graham Productions, Border Gateway Protocol.



STEVE:  Well, that is a blast from the past, yeah.  Okay.  So we've talked a lot in the early days, where we were discussing the way the Internet works, about basically the invention, which is what it was back in the DARPA days pre-Internet, ARPANET, of autonomous routing, the idea being that you would have a federation of routers that are interconnected in just sort of an ad hoc, kind of random way.  You want enough connectivity that, if a link between any two routers went down, there would be like another way to get the data around where it had to go.  So the idea being multiple paths.



The Internet is inherently a multipath system where there are many ways, an incredible number of ways today, for data to get from point A to point B.  Thus we get redundancy and resilience and so forth.  At every router, in every router, there is a table, not surprisingly called a "routing table," which is a list of IP addresses, or more technically or more correctly IP address prefixes, which instruct any incoming packet, with the help of the router, which outbound link to forward that packet to.  I call them "address prefixes" because you couldn't and you wouldn't want every single IP address on the Internet, not just for the time being, restrict us to IPv4 for the sake of ease and explanation.  There's 4.3 billion IPv4 addresses.  So it would be impractical to have every IP address associated with which interface it goes out of.



And it's not necessary because one of the cleverest things, one of the many clever things about the Internet is that networks are groups of adjacent numbered IPs.  So, for example, and we've talked about this, HP once upon a time had 15-dot everything.  In other words, they had the entire - they had every IP address - I think it was 15 and 14, come to think of it.  They had every IP address beginning with 15 or 14, which meant that anywhere else in the world, if a packet was bound for an IP address beginning with 14 or 15, it could be sent toward Palo Alto, back say when HP was only in Palo Alto.



The point being that one entry in a router, in the routing table of every router in the world, was able to just aim, essentially, if it was two IPs, 14 and 15, that would be one 128th of the entire address space, which is two IPs in Class A of the first byte, one 128th, at a single location, dramatically simplifying the challenge of routing all 4.3 billion IPs to their destination.  So this is this notion, this way the IP space is divided is the bits on the left, the most significant bytes and technically bits specify the network number, and then the right-hand bits are the machine within that network.



So what that means is that, for example, a given company may have several blocks of IP addresses that are adjacent, several, for example, Class C networks, a Class C being 256 IPs.  Maybe they have four of them, so that's 1024 IPs.  Well, that means that the last 10 bits don't matter for routing because they just have to get to the company.  Once they get to the company, then internally the 1024 machines on those IPs will respond.



But that means all the routers in the world, as long as the upper - let's see.  We have the lower 10 bits, so the upper 22 bits, that specifies that company.  So again, the routing table is simplified.  And the company is probably buying its IPs from its ISP, and the ISP is selling similar blocks to many other companies, meaning that the ISP has a much larger network of, like, 10 for this company, 10 bits for that company, and so forth.  So even fewer IPs or bits on the left side will get the packets to that ISP.  Then it uses the next chunk of bits to decide which of its customers it routes the packets to.  And then each of those companies routes them to its system.  So just this incredibly brilliant hierarchical system.



And to us it's sort of - we can take it for granted now because we've been living with it for decades.  And you could almost say, yeah, well, that's obvious.  Well, yeah, in retrospect.  But these guys thought of this, invented it from scratch.  So all over the world we have routers containing these tables which need to figure out where to send stuff.  And the challenge is this is not - the Internet is not a static environment.  It is dynamically changing.



And as I mentioned before, links between routers are coming up and down.  If a link between two routers drops, then suddenly the routing table of both routers is wrong because they were wanting to send packets to each other, if each of them was nearer the destination of where those packets were trying to go.  But if that link goes down, suddenly they need to update their routing tables to find the next best route in order to send the packets to, since the better route, the preferred route, is no longer available.



So think about that for a minute.  You have routers all over the globe that have tables that need to be managed continually and dynamically.  You've got ISPs adding customers, removing customers.  You've got, remember also, we've got IPv4 address space depletion so that there are governments that used to have big blocks of IP space where IPv4 is suddenly valuable.  It's a commodity.  So they're selling off chunks of their network to other people who want to buy them.  So suddenly IPs that used to route into that country now go somewhere else entirely.  The point is this cannot be done by hand.  You cannot do - there's no possibility of managing and maintaining the tables of the routers that are global manually.



BGP, the Border Gateway Protocol, is the protocol that automates this management.  It is carried over TCP.  So all routers establish persistent TCP connections to every other router that they're connected to.  And I forgot what port number, it's 179 maybe, something like that.  It's a well-known port in the same way that 25 is for SMTP and 80 is for the web and 443 for HTTPS and so forth.  There is a well-known port which accepts TCP connections and only TCP connections because we need to prevent spoofing.  And routers establish a link to another router and communicate using BGP.



Okay.  So let's step back a bit.  Understanding how routers interact, the necessity of routing tables being maintained dynamically, the need to automate that process, and that BGP is what does it.  The reason this all popped up on my radar is that exactly one week ago - there is a great site, BGPmon.net, B-G-P-M-O-N dot net.  They're just a freely available Open BGP monitoring site.  They were purchased some time ago by OpenDNS; and, as we know, OpenDNS was subsequently purchased by Cisco.  They watch, among other things, but as BGPmon would suggest, they watch what's going on with BGP on the Internet.  A week ago one of their guys posted the following:



"Early this morning (UTC) our systems detected a suspicious event where many prefixes for high-profile destinations were being announced by an unused Russian Autonomous System."  So let's stop back.  We need to define some terms.  "Early this morning our system detected a suspicious event where many prefixes for high-profile destinations were being announced by an unused Russian Autonomous System."



So first of all, "prefixes" is the term for the right-hand side of the IP.  So that's called the "prefix" as opposed to the - sometimes it's also called the "network number" as opposed to the right side, which is the machine within that network.  So those are prefixes.  They use the term "announced" because that's also part of the BGP jargon.  A router "announces" to the world via BGP the networks that it has responsibility for.  So, for example, an ISP's border, it's called an "edge router" because it's on the edge between the ISP and the public Internet.  An ISP's edge router will announce that it's managing the traffic.  Behind it are the following networks.  And so it announces it by announcing prefixes, essentially saying all packets where the left number of bits match this prefix should come to me because the machines for those prefixes are behind this edge router, so send all that stuff to me.



An autonomous system is, again, the official nomenclature, the name.  It's called an "AS number," and they're by number.  It is the numerical designation for an ISP like I was just describing.  So Cox or Level 3 or Google, Apple, any major player on the Internet is an autonomous system.  And I kind of regret that I didn't apply for one myself for GRC many, many moons ago, years ago, because what you get if you're an autonomous system is an AS number and your own allocation of IP space.  The point is that, when I've moved from, as I used to be at XO, to Level 3, and before that I was at Verio, and then Cogent, all of those movements have required me to renumber my IPs for GRC.  It's not painless, and it can be challenging to not have any downtime or to minimize downtime.



But if you are an AS, you have your own IP space, and it's portable, meaning that, had I done that, I could have said to Level 3, okay, I want to now be hosted in your datacenter, and my AS number is as follows.  And so essentially their edge router would have begun announcing the prefix that I own, and the same IPs that were once going to, for example, Verio, would have been rerouted, literally rerouted to Level 3.  No DNS would have changed.  That IP space would have just moved to a different location.  So, I mean, this is how cool this system works.  And frankly, we owe the fact that this is even possible to the cleverness, the automation of this routing system behind the scenes.



But as BGPmon wrote:  "Early this morning our systems detected a suspicious event where many prefixes for high-profile destinations were being announced by an unused Russian Autonomous System."  Now we know what that means.  A Russian-located AS, like a big ISP, was announcing prefixes, announcing that it was the owner of prefixes, that is, big chunks of IP space, for high-profile destinations.



They write:  "Starting at 04:43 (UTC), 80 [eight zero] prefixes [blocks of IPs] normally announced by organizations such as Google, Apple, Facebook, Microsoft, Twitch, NTT Communications, and Riot Games were now detected in the global BGP routing tables with an Origin AS of 39523 [which is DV-LINK] out of Russia."  Meaning that this entity, this Russian entity was claiming that it was Google, Apple, Facebook, Microsoft, Twitch, NTT Communications, and Riot Games, among others.



They write:  "Looking at the timeline, we can see two event windows of about three minutes each."  So they were not long-lived.  "The first one started at 04:43 UTC and ended at around 04:46 UTC.  The second event started at 07:07 UTC and finished at 07:10 UTC.  Even though," they write, "these events were relatively short-lived, they were significant because it was picked up by a large number of peers and because of several new more specific prefixes that are not normally seen on the Internet.  So let's dig a little deeper," they write.



"One of the interesting things about this incident is the prefixes that were affected are all network prefixes for well-known and high-traffic Internet organizations.  The other odd thing is that the origin [which was] AS 39523 has not been seen announcing any prefixes for many years," with one exception which they note later.  "So why does it all of sudden appear and announce prefixes for networks such as Google?"  Okay, I'll stop at this point.



So the idea, when they mention "peers," is that not only was it, this AS, this Russian Autonomous System, announcing that it controlled IP space that it did not, but the nature of BGP is propagation.  That is, when I was speaking earlier, I talked about how routers are all linked between, they're all interlinked inherently.  And they establish links to all the routers that they connect to.  And then they share their routing tables.  So this bogus announcement, which is essentially bogus routing information, bogus routes, were stuck in one router, and it shared them with its peers.  The peers said, "Oh, wow, look, this AS 39523 suddenly is in control of these IPs.  So that means, rather than sending them somewhere else, we need to send them there."



The other thing that I haven't mentioned yet is specificity, that is, the way routing tables work is that among a table with many entries, the entry which is most specific is the one that wins.  Now, this is important.  Take, for example, that government I mentioned.  We have a government that owns a huge block of IPs, say a whole Class A.  Call it, I don't know, just out of the blue, 200-dot everything everything everything.  Then it sells a chunk of its IP space to someone else.  So the point is that it is - so in order for that IP space to be useful to somebody else, they have to obviously be able to receive the traffic bound for that IP space.



So in the routing tables of the world, every single routing table in the globe, there is still the general entry 200-dot anything anything anything goes to whatever continent that government is on.  But also now in the table, having sold a subset of their IPs, 200.20 dot anything dot anything goes to the buyer.  So the point is there are now - the way the routing table works, the incoming packet containing a destination IP is caused to match the most specific rule, the most specific entry in the routing table, in this case while, yes, an incoming packet to 200.20 dot something dot something does match 200 dot anything dot anything dot anything, it more closely, that is, it more specifically matches 200.20 dot anything dot anything.  So that's the entry in the table that is the controlling entry, and that specifies where to send the subset of the 200 dot anything anything anything packets.



So here's the kicker is that, as this posting on BGPmon mentioned, the entries, the bogus entries that appeared in this Russian router were more specific.  They were deliberately more specific in order to capture the routes and the actual traffic that would have otherwise gone to Google and Microsoft and Apple and Facebook and so forth.



Okay.  That was a week ago.  That was seven days ago.  We've discussed these things in the past, and I want to remind us briefly about them.  On Sunday, February 24, 2008, also at BGPmon because these guys have been around for a long time, they said:  "For two hours viewers across" - get this - "most of the world were unable to reach YouTube.  The root cause of this outage was a flaw in the Border Gateway Protocol which governs how routing data propagates across the Internet.



"It began the previous Friday when the Pakistan Telecommunications Authority ordered the nation's ISPs, that is, the Pakistani Internet Service Providers to black out a YouTube video it feared would trigger riots.  Pakistan Telecommunication Company Ltd. (PTCL), Pakistan's largest Internet provider, responded to the order by the government by blocking the entire YouTube site" - in this case YouTube network - "through a quirk in BGP that's readily exploitable by anyone" - listen to that, anyone - "who controls a BGP-enabled router" - which is an edge router or a border router - "of an autonomous system (AS) such as PTCL is.  PTCL connects to the Internet solely through another AS, in this case PCCW Ltd., a Hong Kong telecommunications company.  PTCL technicians" - that is, the Pakistani technicians - "failed to warn PCCW" - that is, their parent provider, the ones that they connect to - "to block the bogus route."



So essentially the Pakistani Company PTCL deliberately put a bogus route for all of YouTube IP space into their own router so that nobody in Pakistan would be able to reach YouTube.  But they did not tell their parent, PCCW, not to propagate that bogus route.  Consequently, that bogus route spread through PCCW and out to the rest of the Internet.  That route got propagated globally, and for two hours most of the world was unable to reach YouTube.  Not from a denial of service attack.  Not through anything like crazy, some active powerful attack.  Just an engineer in Pakistan entered a command into a terminal and forgot to prevent that known bogus BGP route from going global.  And it did, and it created a two-hour global blackout of a major network on the Internet.



LEO:  This seems like a huge vulnerability.



STEVE:  Exactly.



LEO:  You could do it maliciously; couldn't you?



STEVE:  Exactly.  So that was February 24.  The same summer, attackers arranged to cleverly misuse the bitcoin stratum protocol by deliberately hijacking IP addresses of the pool server IP block.  An attacker was able to steal, at the time,  $83,000 U.S. dollars' worth of bitcoin.  This was an advanced attack, not just from the bitcoin perspective, but also from the BGP perspective.  In an attempt to hide the attack, the originator used autonomous system path prepending, which is a way of hacking BGP, with a range of autonomous systems.



And I'll just note that later in that same year, in 2014, it was the same technique of spoofing BGP was used by spammers.  An unknown spammer was able to manipulate BGP to cause a block of IPs to get routed to them.  While it was routed to them, they used it for spamming; and within a couple hours, before the antispam systems could blacklist their IPs, they shut down that block and opened another.  So they were able to walk through a chunk of the Internet's IP space briefly, commandeering it for the purpose of sending out a huge amount of spam which would not be blocked by IP address filters because it was brand new and had never been seen sending spam before.  And then before they could get filtered, or maybe the moment they sensed they were being, they shut that down and grabbed another block of space.



So you got exactly the point I wanted to make, Leo, and the reason I wanted to spend some time talking about this is that we have a fundamental weakness in the Internet.  And mistakes happen, and abuse happens.  Routing errors can happen either deliberately or inadvertently.  And we absolutely rely on this.  There is continuous BGP protocol noise as routes are changing.  On the order of 50 to 200 routing updates per minute are propagating through the Internet, and there are high periods of churn where those who monitor see 9,000 updates of routing information per minute.  So it's not something anybody is able to manage or take advantage of.  And at this point we're vulnerable.



And so I would not be surprised, the reason I wanted to take the instance of this occurring for a couple minutes suspiciously by a Russian AS exactly a week ago is that I'm virtually certain that during the balance of this podcast's life we will be talking about the apparent deliberate abuse of BGP, probably by state actors who certainly have the power to perpetrate these kinds of deliberate Internet misrouting attacks.  Really, really interesting.



LEO:  Fortunately, nobody would want to do that.  That would be such a terrible thing to do.



STEVE:  No, that's just theoretical.



LEO:  They'd have to have some status, though, some standing in the network.  I mean, people don't just blithely accept new BGP rules; right?  I mean...



STEVE:  They do blithely accept new BGP rules.



LEO:  Well, that's terrible.



STEVE:  I know.  It is.  Yeah.  And the problem is you can't - so like there's no vetting system.  If that government sold a block of IP space, all it means is that somebody on an edge router somewhere says we're now responsible for this block of IPs.  Please send them to us.  And it happens.  There's no...



LEO:  It's not the first time we've seen this.  This has happened many times over the past years; right?



STEVE:  Yes.



LEO:  These mistakes, BGP mistakes.



STEVE:  Yes.  And they can be mistakes, or they can be deliberate.  And there is...



LEO:  We've not seen any deliberate attacks using this though, at this point.  Or have we?



STEVE:  Well, yeah.  We, for example, in that case of spammers squatting on stolen - basically they're stealing IP space in order to route spam and then releasing it and then stealing some more.  So it is the case that you need to be an autonomous system.  But the problem is there's no oversight.  There's no clearinghouse.  There's no one that you have to check with before you enter a block of IPs into a router, claiming that it's yours, and it propagates across the globe, and now all of those IPs some to you.  I mean, it is that bad.  It's really, really worrisome.



LEO:  It's pretty amazing.



STEVE:  It is.



LEO:  All right.  Well...



STEVE:  And on that happy note, we end 2017.



LEO:  The midnight ride of Steve Gibson is concluded.  The BGPs are coming, the BGPs are coming.  Yes, and next week a fun one.  We're going to have a lot of fun, and I'm looking forward to that.  But that is a - I don't want to say a rerun, but a return of a classic episode.  And not the whole episode, just the timeless part.



STEVE:  Yes.  And it is an important, I think, something that our listeners will really find very interesting.  And then we'll be back all fresh-eyed and bushytailed or whatever that expression is.



LEO:  Bright-eyed and bushytailed.



STEVE:  Bright-eyed.  Bright-eyed and bushytailed in 2018.



LEO:  January 2nd.  I don't know how bright-eyed or bushytailed I'll be.



STEVE:  Yow.  Ooh, yeah. 



LEO:  But we will be here, 1:30 Pacific, 4:30 Eastern, 21:30 UTC on Tuesday, January 2nd.  But again, next week is going to be a fun one.  And I wasn't here for that episode.  Tom Merritt was filling in.  So it'll also be the return of Tom Merritt to our airwaves.  You must listen.



Now, there are numbers of ways to listen to any given show.  Certainly the first place to go is GRC.com.  That's Steve's website where he has all sorts of good stuff, including, of course, his day job, SpinRite, the world's best hard drive recovery and maintenance utility.  But he's got lots of freebies, and he has not only the audio version of this show, but transcripts, not just of this show, but all of them.  You have all the shows, all 642?



STEVE:  I had Elaine go back and do them all.



LEO:  She went back in time, okay.  Wow.  Which makes it a very searchable archive, as well.  So go to GRC.com and get that.  You can tweet Steve, @SGgrc.  He takes DMs of any length, as well.  And you can also find video as well as audio of the show on our site, TWiT.tv/sn, or subscribe wherever you get your podcast.  That way you'll get every episode, including next week's special holiday episode.



Steve, I hope you have a great Christmas and a happy New Year, and I will see you in 2018.  Wow.



STEVE:  Will do.  And I look forward to hearing about your catamaran adventure.



LEO:  Whoo.  Can't wait.



STEVE:  Don't try to get too much sun.



LEO:  I think it's going to rain the whole time.



STEVE:  Ah.  In that case...



LEO:  So that'll be a problem.  But I will be taking my Vitamin D, thanks to you.



STEVE:  Oh, good.  And on to 2018.



LEO:  On to 2018.  Thanks, Steve.



STEVE:  Okay, bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#644

DATE:		January 2, 2018

TITLE:		NSA Fingerprints

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-644.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss a new clever and disheartening abuse of our browsers' handy-dandy username and password autofill, some recent and frantic scurrying around by many OS kernel developers, a just-released MacOS zero-day allowing full local system compromise, another massively popular router falls to the IoT botnets, even high-quality IoT devices have problems, the evolution of adblocking and countermeasures, an important update for Mozilla's Thunderbird, a bit of miscellany, listener feedback, and an update on the NSA's possible intervention into secure encryption standards.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Our first show of the new year, and there's lot of little tidbits to clean up - including, yeah, a Macintosh zero-day, discovered just a couple of nights ago.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 644 for Tuesday, January 2nd, 2018:  NSA Fingerprints.



It's time for Security Now!, the show where we cover your security and privacy online with the king of the tinfoil hat brigade.  No, I'm not going to say that.  He's our commander in chief, the man who protects us from all that is evil, Mr. Steve Gibson.  Happy New Year, Steve.



STEVE GIBSON:  Leo, great to be with you again, as always, for Episode 644.



LEO:  Holy cow.  Holy cow.



STEVE:  On this January 2nd of 2018.  So, yeah, I think heading into another year of great interesting podcasts.  And also...



LEO:  Of course the good news is security is going to be tight, and we won't have to do much this year.  No?  No?



STEVE:  You know, I was noticing how much shift there has been over to IoT stuff.



LEO:  Yeah, yeah.



STEVE:  I mean, it really is, I mean, it's what everyone wants and is using.  And it's just like sort of this brand new frontier of learning all the same lessons over again.



LEO:  Well, it was an IoT Christmas for a lot of people; right?  They got their Echoes and, you know, I mean, this is the year.



STEVE:  Well, yeah, and the problem is people really want the features.  They want the convenience.  They want to know if they left their garage door open as they're driving away.  They want to be able to, well, I mean, Ring is a sponsor of the show, a major IoT device that allows you to be on the beach in the Bahamas and tell the delivery...



LEO:  Which I was, and did.  It's the funniest thing to be able to know who's at your door while I'm on a sailboat in the middle of the Caribbean.  The doorbell was going off, and I said, hey, somebody's at home.



STEVE:  I can imagine, yes.



LEO:  That's pretty funny, actually.



STEVE:  And in fact I was mentioning that Lorrie and I, my girlfriend and I went up to visit my sister and her family, and they've got a Ring on the front door, and it was going off from time to time as people were approaching.



LEO:  Dong dong dong, yeah.



STEVE:  Yup, exactly, so that familiar sound.



LEO:  So, now, this is the time of year when people come home from explaining to family various issues.  And we were talking on MacBreak Weekly where everybody had explained the battery issue on the iPhone.  And I imagine that you spend - do you spend time when you go home securing family computers and things like that?



STEVE:  No.  My sister's husband is a techie.



LEO:  Oh, good.



STEVE:  And so he's got that well in hand.  And in fact, while my mom was alive, he was happily the one responsible, or that is to say, saddled with, you know, "Do I have WiFi?"



LEO:  Yes, Mom, you do.



STEVE:  "Where is it?"  And you know, it's okay, "Ken, go fix that."  So, yeah, so I'm a safe distance away, although I do have that role down here in Southern California for a number of...



LEO:  Yeah, of course you do, your friends, yeah.



STEVE:  ...friends and family and so forth.  So we've got a lot to talk about, although I have to say, interesting topics, but not a huge number.  Everyone was maybe enjoying the holidays.



LEO:  Even hackers take the week off.



STEVE:  Even hackers, yes.  The title for this week's podcast is "NSA Fingerprints," following an interesting posting by our friend Matthew Green at Johns Hopkins, a cryptographer.  And I noted that his bio now says "Professor."  And I'm pretty sure it used to say "Associate Professor."



LEO:  Yes.  I think he got tenure.  I think you're right.



STEVE:  I think that happened, so congratulations to Matt.  Anyway, we've talked in the past about this interesting oddity in one of RSA Security's random bit generators, a DRBG, Deterministic Random Bit Generator.  And this was the Dual EC, the Dual Elliptic Curve generator, which was slower than the other three, and weird, and no one would choose it, but it was the default.  And so that, you know - and this was years ago we discussed this, when this news came out.



Well, something just happened that surprised the cryptography community, that furthers the impression or provides additional evidence that - because there's another big piece of this that just kind of all came together as this weird side effect of our movement towards the next version of TLS, TLS v1.3, which really pretty much puts the last nail in the question of whether the NSA was actually up to some funny business.  And remember that Reuters also reported back at the time that RSA had been paid $10 million to...



LEO:  I remember that.



STEVE:  ...choose this funky and not clearly secure source of entropy for their crypto library.  But people were kind of thinking, eh, okay, well, so that's not good.  Well, it turns out, yeah, it's actually pretty bad.  So those pieces have come together, which we will end the podcast with.



But in the meantime, or until then, we're going to talk about a new and clever and disheartening abuse of our browsers' handy-dandy username and password autofill, which shouldn't be a surprise to us.  Everybody is clever.  And unfortunately all of this technology that we're using can be abused when people focus themselves on that.  And so we have another instance of that which has been found in the wild being deployed by 1,110 of the top chunk of websites on the Internet.  We'll talk about that.



Also there is something afoot.  And for a while the initial reports were speculative.  Now, in the last couple days, some additional information has come to light.  There's been some frantic scurrying around by many OS kernel developers, changing some things that led the early people who were watching this to believe that there was an undisclosed major problem that was like that there had been nonpublic disclosure to kernel developers about.  And now we think we know more about it, but it actually is going to have an impact on the performance of our systems.  So we'll talk about that.  There is a just-released macOS zero-day which allows full local system compromise.



LEO:  What?  We missed it.



STEVE:  Beautifully, beautifully written up.



LEO:  Oh, boy.



STEVE:  I mean, the guys did a great job.  There's also another massively popular router has fallen to the IoT botnets.  Also I wanted to note that even high-quality IoT devices - Sonos - have problems on the public Internet.  And I don't know if you've noticed yours updating more recently.



LEO:  Lately, yeah, yeah, yeah.  Just got an update.



STEVE:  Uh-huh.  And we know why now.



LEO:  Uh-oh.



STEVE:  So we'll talk about that.  Also there's - I wanted to briefly touch on the evolution of adblocking and countermeasures to it.  We've, of course, that's been a topic of ours because it revolves around privacy, and the technology is of interest and interesting.  Well, this has been evolving.  In a new study that'll be presented at a conference later this February, next month, has updated or matured, I guess, their impression of how much websites are responding.  And it's much more than people knew.  Also there's an important update needed for users of Mozilla's Thunderbird email client, which I just want to shake people up about a little bit.  We have a little bit of miscellany, some listener feedback, and then we're going to talk about what recently came to light that leads us to believe that the NSA is a little more involved...



LEO:  Uh-oh.



STEVE:  Well, it provides additional evidence of, I should say, the involvement that they've been suspected of until now.



LEO:  All right.



STEVE:  So our picture this week you got a big kick out of...



LEO:  Oh, I love it.  I love it.



STEVE:  ...when it first came up on the screen, before we began recording.  And this is just sort of the - I don't know how to describe it.  It's some sort of a - it's something called the "output matrices" of apparently a major TV network.  And it just sort of - it demonstrates the bailing wire and chewing gum approach to keeping something online.  Apparently these things were overheating because they weren't in an adequately cooled environment.  So somebody said, okay, well, we need to have a fan blowing on them.  But of course the fan was too short, so they got a cardboard box and stuck the fan on top.



LEO:  My favorite part is the electrical tape holding the fan to the...



STEVE:  Yeah, it's like duct tape-width black electrical tape  holding the whole thing down.  Oh, I guess it's sitting on another table that is resting on the floor.  And then there's a sign on the front of this Rube Goldberg contraption saying, like explaining what is going on here.  



LEO:  What the heck?



STEVE:  And it says:  "This fan is keeping the output matrices cool."



LEO:  Oh.



STEVE:  Yeah, well, you know about those.  You need your output matrices to be kept cool.



LEO:  Always keep your matrix cool, yeah.



STEVE:  That's what I say.  "If it is moved, all of the UKTV networks will fall off the air."



LEO:  Can you read the - there's an asterisk and fine print, and I can't read it in this image.  Can you?



STEVE:  I can't either.  I would love to know what that says.  But, yeah.  So, yeah, that's not a fan that you want to, like, bring over to your desk when you're too warm.



LEO:  Don't move the fan.



STEVE:  Leave this alone.  This is high-tech, yes, equipment plumbing.  And, boy, if the UKTV networks are relying on this, then let's hope the fan keeps spinning.



Okay.  So betraying our browsers' autofill.  Some researchers, and they've been busy at the Princeton Center for Information Technology Policy.  We've had a couple of stories about their publications and research.  That's at the Freedom-to-Tinker.com, with hyphens, Freedom-to-Tinker.com site, where they blog about work that they're doing in privacy.  They performed an extensive scan of the scripting being deployed by a bunch of the top Internet websites, and they discovered something disturbing.



They discovered that at least two commercial companies, one called Adthink at AudienceInsights.net - and Leo, my computer can't go there anymore because I don't want my computer to go there.  But yours can, and our listeners can.  Go to AudienceInsights, all one word, dot net.  And you're going to find out what your ID on their network is.  Mine is A008bc977d7aa23 blah blah blah.  And everyone apparently has a unique ID because this company has been tracking us for quite some time, AudienceInsights.net.



So when you go there, they say:  "What is it?"  And they say:  "AudienceInsights is a web application offering analysis and statistics based on anonymous data collection."  Then they say:  "How information is collected?  Our partners' websites and applications host our program" - which is to say JavaScript - "that collects information and sends it to our servers where it is stored and analyzed."  Okay.  Then they happily tell me my own ID:  "Your ID on our network is" and then that long string of looks like hex gibberish.



Then they ask themselves the question, "What is collected?"  And they say:  "We collect only anonymous data through anonymous cookies and technologies" - okay, that's the "and technologies" part - "that record, first, events related to your activity on the partner's website, such as the number of pages viewed or your searches made on the partner's website."  And I'll note that anyone who cares who's running a web server can easily determine that for themselves.  So no third party is necessary.  And, second:  "Information provided by trusted partners that may include sociodemographic data such as age range."  Well, that's sort of the least of what they apparently collect because the Princeton researchers discovered, when they looked more closely at this code, that birth date, age, gender, nationality, height, weight, body mass index...



LEO:  Wait a minute.  Where are they getting this from?



STEVE:  Apparently this is the data contributed by their trusted partners.



LEO:  Jesus.



STEVE:  Eye color; net income; education; occupation; raw income; relationship states; seek for gender M, F, transman, transwoman, or couple; pets; location with postal code, town state, and country; loan, loan type, amount, duration, and whether you're over indebted; insurance for car, motorbike, home, pet, health, and life.



LEO:  Wow.



STEVE:  Card risk, including chargeback and fraud attempt.  Whether you have a car and, if so, make, model, type, registration, model year and fuel type.  I guess they want to know if it's electric...



LEO:  How do they know all this?



STEVE:  Also tobacco, alcohol, travel - from, to, departure, and return.  And I love this, your car hire driver age.  Oh, and hotel stars.  Those are the categories which this script is apparently filtering through and sorting.  Then they say:  "We do not collect any personal information.  We do not know who you are.  We do not know your residential address."



LEO:  We know what kind of fuel your car uses.



STEVE:  They know what type of, apparently, what your sexual preference is for people you are seeking.



LEO:  Geez.



STEVE:  And so on.  So what they don't say in all of this happy page - and again, it's go to AudienceInsights.net, and you can learn your ID on their network.  What they don't say is that they deploy - get this, Leo - hidden scripts running on these trusted partners' web pages which deliberately trick the users' browsers auto form fill into populating the user's login username and password for that website.



LEO:  Yeah.



STEVE:  Which then, after being hashed, is sent back to Adthink's server farm for collection.  So first of all, the data is hashed, so they're not obtaining - that is, the script is choosing to hash it.  If the script chose not to hash it, or if because that data is being sent along with a cookie from the browser, they're able to link all this together.  And also, since the problem with hashing is that their - and their whole goal is this is a non-cookie form of tracking.



But what the Princeton researchers found disturbing, aside from the breadth of information which they're hoping to collect from people, is that this is arguably abusing the browser's willingness to populate username and password fields on behalf of its user.  So by hiding this browser autofill, they are getting hashes of the username, which are in many cases the user's email address, which gives them cross-domain tracking because they'll get the same hash, and the user's password, which, if they're not using a separate password for every site they visit, will again create hash collisions, so they'll know that this user has popped up on a different site.



So anyway, there are two domains.  And the reason I said you need to go there and I can't is I immediately put static.audienceinsights.net into my system's hosts file, aiming it at 127.0.0.1, and api.behavioralengine.com.  I have all of this in the show notes, for anyone who's interested.  Those are the two companies that are doing this that the Princeton researchers poking into what current scripting behavior was causing passwords to be populated on invisible form fields.  And I just, I mean, a quick hack is just to route those two localhosts so that our own browser is unable to initiate a connection to them in order for this information to get populated.



So what the researchers wrote was:  "We found two scripts using this technique to extract email addresses from login managers on the websites which embed them.  These addresses are then hashed and sent to one or more third-party servers.  These scripts were present on 1,110 of Alexa's top one million sites."  So, okay, 0.1%, not a high percentage, but a thousand of them, more than a thousand of the top million.



"The process of detecting these scripts is described in our measurement methodology," they wrote, and I've got a link to their full PDF.  Also, at webtransparency.cs.princeton.edu is a complete list of all the sites which are performing this autofill behavior.  And there is over on GitHub and also links here in the show notes a list maintained that looks like it's set up to drive pfSense for blocking any of these sites.  The good news is I did a cursory scroll through, and I didn't see any major sites.  This looks a little more like some of the more fringe sites are doing this.  But unfortunately this is an instance where any of these so-called "trusted partners," I mean, they're not partners we trust, they're basically the other sites that these two ad companies are in cahoots with in order to build a database that they're able to somehow leverage for more profit.



Oh, and there's also a demo page that I have a link to in the show notes where you can go, if you're curious, to actually watch this script approach being leveraged.  And so the takeaway is, first of all, you may want to just blackhole these two domains static.audienceinsights.net and api.behavioralengine.com if you object to this behavior.  But this really begs the question, the obvious solution is for browsers not to be so quick to autofill.  That is, maybe for them to be smart about whether the fields being shown are visible to the user.  They should certainly be able to do that.



And I remember you and I played with this whole autofill problem sort of in a different instance months ago, where you discovered that just the browser's information about us, which it has available to helpfully populate form fields, could be brought up offscreen, like at a negative 1,000 coordinate of the screen, and not be visible.



LEO:  And this is also tied to the fact that we learned that you don't have to press the submit button for these fields to be read; right?



STEVE:  Correct.



LEO:  That these fields are available to the browser and the site without submit.



STEVE:  Exactly.  And in fact there's an on-change event which, for example, I use over in the Password Haystacks, where, think about it, as the user is typing, I'm updating the calculations about alphabet size and time required to brute-force the password on a character-by-character basis.  So there's an on-change event which a field can be given such that, when its contents changes, some script is notified.  So the script would just sit there.  And when the browser populates the field with the information, that fires off the on-change event.  The JavaScript wakes up, responds to the event, hashes the data, and sends it off to the mothership.



And so, yeah.  So as you said, Leo, the first problem is that this is done before you hit Submit.  And of course from a UI standpoint we probably need that to happen.  The user needs to know whether their browser is going to populate the field or not.  But it would be nice if, I mean, the only mitigation anyone has suggested, and there's been a lot of dialogue about this, is to not make it automatic.  Require some user interaction in order to cause the browser to fill the fields, but then of course that breaks the user experience.  It's so handy and convenient to have this happen.



So really what I hope is that this will get enough attention that browser vendors will realize this convenience, and it's really their responsibility to the user to do the right thing with the information the user has entrusted to the browser, is being abused, and to come up with a way to block these scripts from being able to collect username and password data offscreen because note that these guys are hashing this and not taking advantage of it.  But if anybody else gets their script on someone's site that is able to do this, well, they're collecting usernames and passwords.



LEO:  Right.



STEVE:  Which is not a good thing.



LEO:  These are first-party scripts, though; right?  So an adblocker probably wouldn't block them.  



STEVE:  That's probably true.



LEO:  [Crosstalk] asking if Quad 9 would block them.  I think if it's a first-party script they can't.



STEVE:  No.  Now, Quad 9 could blacklist those domains in order to prevent them, in order to prevent your...



LEO:  From sending information back.



STEVE:  Yes, yeah.



LEO:  Okay.  So that wouldn't be - that's, yeah.  So sending it to somebody who is a third party.



STEVE:  Right, right.  So again, as I was saying, I think we're going to have to see if browsers come up with a way of blocking this behavior.  And once again, a feature that was intended to help us and to remain between the site we're visiting and our browser has been compromised by companies saying to those sites, hey, put some of our script on your server so that, as you said, Leo, it becomes first-party script and then has access to everything that that site would normally have access to.  I mean, so this really is, it's the fault of those websites hosting script which is sending their users' usernames and passwords to a third-party service, and presumably through some sort of financial arrangement.  So creepy behavior, but nothing we can do unless our browsers behave themselves.



Okay.  So we've talked about ASLR years ago, Address Space Layout Randomization, where all contemporary OSes today, because this is a problem, are deliberately rearranging where things end up residing in memory.  In 32-bit OSes we have potentially a 4.3GB address space.  In Windows, about half of that is lost to the OS.  But even so, the user space is still two billion bytes.  So the good news is most apps are not that big.  So there's enough empty space that chunks of applications and libraries that they're loading and pieces of the OS that run in user space can all be scrambled around in memory.  That's important because, if a buffer overflow is found, the randomization of where things are makes it significantly more difficult for that overflow to be exploited.



Once upon a time the overflow which often occurs on a dynamically allocated buffer on the stack, the stack itself could be used to contain executable code.  Well, then we fixed that by making the stack nonexecutable with the so-called NX bits on various platforms.  Well, so that meant the hackers had to work harder.  They couldn't bring their own code along with the overflow.  But they very cleverly realized, oh, we can just use existing code which is executable, in so-called Return-Oriented Programming, ROP, where they would jump to some code at the end of a subroutine that got a little bit of the work they wanted to get done, done for them.



Then it would come back, and they would jump again somewhere else and stitch a bunch of those little pieces together into performing some feat that they needed.  And sometimes it's as simple as, like, flipping off a protection bit.  For example, if you could find some code somewhere which, although it probably wasn't intended to, turned off the no-execute bit for the stack, suddenly your stack becomes executable.  So sometimes you just need to flip the right bit somewhere.



So that same technology then moved into the kernel to give us KASLR, which is Kernel Address Space Layout Randomization, to provide the same features because, naturally, people began abusing kernel code if it wasn't also being jumbled around.



Okay.  So we had Address Space Layout Randomization for the user, Kernel Address Space Layout Randomization for the kernel.  A couple months ago, some signs of a breakthrough in hacking Kernel Address Space Layout Randomization began to surface.  And this probably occurred initially very much on the QT in the summer of last year because it was subsequently observed that Windows kernel began getting some patches, and a very short turnaround on the Linux kernel development side was witnessed and caused a lot of people to wonder what was going on.



A new technology was created known as Kernel Page Table Isolation, KPTI, which is a hardening technique, probably first adopted in the Linux kernel to improve security by better isolating the user space and kernel space memory.  We've talked in the past, when we were toward the end of our series on processor architecture about virtual memory and the way that physical memory, which starts at zero and runs up to however much physical memory you have, is seen in the abstract by not only the kernel itself, the operating system, but most specifically by user processes.



So, for example, every user process thinks it's starting with a very similar snapshot of memory.  For example, it might see memory beginning at 40,000 in hex, that is, every process sees sort of the way the OS wants to present memory to the process the same.  But they can't be actually seeing the same physical memory.  First of all, you need the processes to be isolated so they're not able just to poke in each other's memory, but there's a level of indirection.



So the idea is that the memory addresses which processes use is looked up through a series of page tables.  And on the Intel architecture there's like a chain of three of them.  So the page that the memory access that a processor makes with typically 4K bytes granularity is looked up to see where it should, that is, that page is virtual memory.  It's looked up to see where it actually is in a first table; and that table, the contents of that table is looked up by a second table; and the contents of that table is looked up by a third table.  And so it's unbelievably complicated.  It's very powerful and very flexible.



But the only way this can possibly work is if these tables are cached; if they are kept in high-speed, on-chip, locally accessible memory so that all of these lookups can happen fast.  And there's something know as the TLB, the Translation Lookaside Buffer is the term, which also even further caches the past results of this multilevel lookup.  Well, there's been a shortcut that kernel developers, all kernel developers have been taking for years because it was understood that having the kernel and the user space sharing these cached tables was a theoretical problem.



Intel implemented a sort of a mitigation to this problem known as Process Context Identifiers, PCIDs, like 10 years ago.  So they've been available in chips for a long time.  But the problem is it creates a performance hit. If what you really want is code running in user space, any of our processes, to be able to call down to the kernel, whether it's Linux or Windows or macOS, you want it to be able to call down to the kernel as quickly as possible.  I mean, the applications are making API calls, kernel API calls, all the time.



Now, there is a lot of OS now running in user space; but still, for interrupt handling and device driver and I/O things, that's all being done - and memory management - that's all being done by the kernel, where it has to be in order to have the isolation required.  So all of the OSes have for years been sharing the page tables used by the kernel with the page tables being used by applications, purely for efficiency's sake, with some little bit of anxiety that maybe there would be a problem found in this.



Well, what apparently happened a few months ago is a problem was indeed found.  And over the last couple weeks, where there's been a lot more churn among the developer community about what's going on, there's been additional information which has surfaced.  For example, in the Linux kernel where this work has been done, comments have been redacted, and the documentation pages for this work are absent, apparently because they want to get these mitigations in place before this thing gets more public.



Interestingly, it looks like this is purely an Intel architecture problem.  It does not affect AMD processors.  So what this Kernel Page Table Isolation does, KPTI, is it reluctantly separates the kernel page tables from the user space page tables at some significant cost in performance.  It looks like operating systems which implement this are going to take something like a 5% performance hit across the board because what is going to probably emerge soon is that it has never really been safe to do this, and that some clever developers or hackers really have figured out how to break the isolation which sharing page tables with the kernel has, for the sake of performance, has been providing.



A kernel developer by the name of Tom Lendacky said:  "AMD processors are" - he posted to a list recently.  "AMD processors are not subject to the types of attacks that the kernel page table isolation feature protects against."  He wrote:  "The AMD microarchitecture does not allow memory references, including speculative references, that access higher privileged data when running in a lesser privileged mode when that access would result in a page fault."



So that gives us a big clue.  That, if you read that carefully, that's suggesting that something about the Intel architecture's speculative accesses, remember that that's the way Intel has improved performance, is that they're doing what are known as speculative accesses, where the CPU is running ahead of the code in order to prefetch things that the code may need.  Apparently, they're performing speculative references of data that would result in a page fault, that is, that the actual process has no legitimate access to; and that speculative reference is altering the cache, the contents of the caching in the chip, which we know that precise timing analysis can reveal cache hits and misses with sufficient granularity and resolution to cause a security problem.



Anyway, so what Tom wrote in his posting was "Disable page table isolation by default on AMD processors by not setting the" - and then here it is - "X86_BUG_CPU_INSECURE feature."  So there is a new feature flag, X86_BUG_CPU_INSECURE, which controls whether this page table isolation is being used.  And then in some other digging that I did, I found a tweet from @pwnallthethings.  Matt Tait on Twitter said one possible vector is if speculative operations can influence what the processor does with the cache, the results can be observed with cache timing attacks.  If the branch predictor reads the results of a speculative operation, it's quite easy.



So anyway, there's been a lot of people wondering what is happening and what's been going on.  And finally, in a post on Tumblr, a guy posting as Python Sweetness said in his TL;DR at the top of a much longer posting, he said:  "There is presently an embargoed security bug impacting apparently all contemporary CPU architectures that implement virtual memory" - which, yes, as we know, is all of them - he says, "requiring hardware changes to fully resolve.  Urgent development of a software mitigation is being done in the open and recently landed in the Linux kernel, and a similar mitigation began appearing in NT" - and by that he means Windows - "kernels in November.  In the worst case, the software fix causes" - he wrote "huge," I'm not sure that 5% qualifies as huge, but still significant - "slowdowns in typical workloads.  There are hints the attack impacts common virtualization environments including Amazon EC2 and Google's Compute Engine, and additional hints the exact attack may involve a new variant of Rowhammer."



Well, now, this doesn't actually look like Rowhammer based on information that has come to light subsequently.  But it does look like probably in the next week or two, once this gets pushed out and implemented.  It's just landed in - I don't have it in front of me - one of the most recent Linux kernel builds, and it's being moved back then into 4.15.  So it doesn't look like anything is happening yet over in the FreeBSD source tree.  But it does, it looks like something interesting is impacting other kernels.



So we'll stay tuned.  If this is the case, if this is exploitable, it means that our OSes are going to have to isolate user space and kernel space page tables, which will give us some sort of a performance hit.  AMD-based systems don't have to do it.  It looks like it's a glitch.  I mean, a clever something that Intel did to squeeze the last drop of performance out of their execution pipeline speculation, and that somebody has found a clever way of leveraging that into an information disclosure about the contents of the page tables which, if used, would allow a bypass of the kernel ASLR and then access to kernel code again.  So, wow, I mean, this is - we're really down in the weeds with this stuff now.  But we have very complex systems.  And as we keep saying on the podcast, security and complexity are archenemies of each other.



I mentioned a zero-day attack which was just published.  A Swiss hacker who goes by the name Siguza, S-I-G-U-Z-A, and who describes himself as a hobbyist developer and hacker, dropped a tweet on New Year's Eve.  And he said:  "Eff it."  And he didn't say "eff," he spelled it all the way out.  He said:  "Dropping a macOS zero-day.  Happy New Year, everyone."  And then in his tweet was a link to his page on GitHub.  He has his own subdomain at siguza.github.io.  He called this "IOHIDeous," I-O-H-I-D-E-O-U-S.  And HID, of course, is Human Interface Device.  He claims, and it's pretty well substantiated now, that this is a more than 15-year-old bug which has long existed in the Mac operating system family.



He provided a proof of concept which targets all macOS versions, crashes the kernel to prove the existence of a memory corruption.  He has what he calls a "leak," which targets High Sierra, just to prove that no separate KASLR leak is needed.  And the third piece of code is HID, which targets Sierra and High Sierra.  He says:  "Up to 10.13.1," and then points people to the readme, "achieving full kernel read/write and disables SIP" - which is the System Integrity Protection which Apple added not long ago - "to prove that the vulnerability can be exploited by any unprivileged user on all recent versions of macOS."



So this is not a remote execution, but this is a full-kernel read/write compromise from any unprivileged user on all recent versions of macOS.  So that means, if you run some code on your machine which you should not trust as a, well, which you should be able to trust as an unprivileged user, relying on the OS to prevent code you didn't write from abusing the inherent trust.  That code can do pretty much anything it wants to on your system.  So I didn't dig into this in depth, but I did look at it closely enough to be very impressed with this hacker.  I mean, this guy really knows his stuff.  And there is absolutely no question in my mind that this is the real deal, and legitimate.



In digging around more, I found that he had jumped into a dialogue about this over on YCombinator, where he wrote, he said:  "I had actually submitted to the ZDI [Zero Day Initiative], but I had written the exploit and write-up in the first place mainly because," he wrote, "I like hacking rather than for money.  I figured I'd see what offers I'd get anyway.  But once I had spent all the time on the write-up, I mainly wanted people to see that, and the amount offered wasn't enough to convince me otherwise."



He wrote:  "I might've published this earlier even, but my December was kind of busy, first with the v0rtex exploit and then with," he wrote, "34C3," which of course is the Chaos Convention, which has just happened.  He said:  "And an engineer from Apple's security team contacted me a bit after releasing.  They had found the bug a while ago," so this wasn't news to them.  I'm editorializing.  It wasn't news to them.  He wrote:  "...but hadn't verified the subsequent patch which actually did not fix it.  And a while ago I tweeted this," and he has a link to some interaction with them.  "So they do have people on it," "they" meaning Apple.  He said:  "I also told that person to extend my condolences to whoever has to come in and fix that now.  But they basically said that there's nothing to apologize for and that they, the team, really like such write-ups."



And I don't blame them.  I mean, this guy was super thorough so that, I mean, he told them exactly - he told them, Apple, exactly what they need to do to fix this.  He said:  "So I guess I'm not that evil."  And by the way, this was responding to people on YCombinator saying who is this guy dropping a bad Mac zero-day on New Year's Eve?  And he said:  "And I neither want to watch the world burn," again referring to one of the earlier comments, "nor did anyone brush me the wrong way," another comment.



"I didn't publish this out of hate, but out of love for hacking.  If you're concerned about skids hacking you now, they need to get code execution first on your machine.  If you're concerned about people who can do that, then those can also get kernel read/write without me, so nothing really changed for the average user."  And then people were wondering whether this was actually he, and he said:  "Yes, it's really me.  Will add keybase proof if my karma gets above greater than two," which did happen, and then so he did provide some proof.  But anyway, there is a...



LEO:  Plays reddit like a fiddle.  Man.



STEVE:  Yes.  So there is a problem.  Apple knows about it.  They've been working on it.  While they would have probably liked to have this communication in private, they are grateful for the communication.  And if anyone is curious, I mean, it is an amazing piece of beautiful detailed work which will allow Apple actually to get this fixed properly, probably much more quickly.  So look for a macOS update in the very near future because this is bad.  This is a zero-day which has only been known about for a few days.  And so it's zero-day plus three or plus two, and people are going to want to get their macOS patched.



LEO:  I hope this doesn't become a trend, though, of people just not following procedure and just releasing stuff because he sounds kind of like an ass.  Obviously he's a smart kid.



STEVE:  Oh, Leo.  When you see the - if you scroll through this work he did, I mean, it's eye-popping.  I mean, the guy really knows his stuff.



LEO:  But just, you know, tell Apple.  He's so anxious to get the credit that he didn't want to tell Apple ahead of time.



STEVE:  Yup.  Yup.



LEO:  But to reiterate, somebody has to have code execute access to your machine.  That means not necessarily physical access; right?  If there were another hack to allow them to execute code...



STEVE:  Well, no, if they just post a malicious app somewhere.



LEO:  Right.



STEVE:  I mean, so an unprivileged user...



LEO:  Can escalate.



STEVE:  Should be, yes, should be protected by the OS.



LEO:  Right.



STEVE:  That's why you have to have root login - well, now - on the Mac in order to do privileged things.  This allows non-root code that you would run by mistake to do whatever you could as a root...



LEO:  So somebody'd have to put this in a malicious app.  Or it could be a website; right?  I mean, a website could probably do this if it had malicious code on it; right?



STEVE:  Yeah.  They probably, you know, it would be like, oh, here, download this, and you'll be glad you did.



LEO:  Yeah, he's going to do that, yeah, yeah.  He said he spent 250 hours on this.  But again, more interested in the glory than anything else.  And I think that's too bad.



STEVE:  Yet another IoT device falls.  The code used in a zero-day to attack Huawei routers has been made public.  It was used by one of several Mirai botnet variants.  Remember last week or two weeks ago we were talking about how there are unfortunately, because the Mirai source code got posted, lots of Mirai botnet clones have sprung up.  But apparently those clones are also being inventive.  They're not all just following off of Mirai because one known as Satori was found to be compromising Huawei routers.  In fact, it compromised hundreds of thousands of publicly exposed Huawei routers over the past several weeks.  And of course the problem is routers are supposed to be publicly exposed.  They're not supposed to have publicly facing vulnerabilities, but the router by nature is WAN-facing, so it's out there.  And so thus its security is really crucial.



The exploit code for compromising Huawei routers was recently posted to Pastebin and is now public.  So researchers who have been watching all of this now expect that the code will quickly become a commodity and be leveraged in future DDoS attacks via other existing botnets such as Reaper, which is still going strong, or IoTrooper.  And I also saw some reference to Brickerbot in this context.



So two weeks ago Check Point Security identified the vulnerability.  And I have to say we were at the end of 2017, and my eyes sort of popped when I saw the CVE number because the CVE, the common vulnerabilities registration, always starts at zero at the beginning of the year.  So that was actually one theory of why that other guy posted his New Year's Eve macOS zero-day was that he wanted to score CVE-2018-0.  I don't think he did.  But my eyes popped because the Check Point CVE about the zero-day Huawei router is CVE-2017, of course, for last year, hyphen - and get this:  17,215.  So, yes, 17,215 vulnerabilities logged in 2017.  So it'll be interesting to see how 2018 fares by comparison.  And Leo, again, as you said at the top of the show, I have a feeling there will be more in 2018 than fewer than we saw in 2017.



LEO:  You ain't seen nothin' yet, boys and girls.



STEVE:  17,215.  I don't know what the actual last number was of the year, but this one would have been recent.  Anyway, they discovered, in this case it was a Huawei home router Model HG532, that it was being exploited to spread Mirai, that Mirai variant, the Satori variant.  Since then, Huawei issued an updated security notice to customers warning that the flaw allows a remote adversary to send malicious packets to port 37215 to execute remote code on vulnerable routers.  Now, of course, remember, the good news is mostly people just want these routers in order to commandeer the bandwidth of their owners.  But if somebody can hook to your router on port 37215 and execute malicious, like random code on it, then they've got access to your network, which you really don't want.



So if you don't have - if you have a router like this which is compromised and not fixed, it's another real good reason for maybe sticking another router inboard so that that router talks to your router, which then talks to your network.  That way they're not able to get any - if they've compromised your WAN-facing router, they cannot get in past your second router because it's hopefully behaving itself the way it should.  And certainly it's not easy for them to scan it because, well, I guess they could scan it if they really were able to run arbitrary code on the outboard router.



Anyway, I will just say get a good router, one that's being maintained.  And these guys, I mean, Huawei responding as quickly as they did is behavior you would want.  The problem is these are hundreds of thousands of consumer routers.  And consumers are, like, connected to the Internet, and they're happy.  They don't realize that their router has been compromised behind their back.



Anyway, so if you happen to have a Huawei router, do go to Huawei and make sure you are running the latest firmware because hundreds of thousands of these routers are being compromised, and it looks like what's happening is researchers are expecting that with this new exploit going public, essentially, all the other Mirai variants will jump on it and add that to their bag of tricks.



So even high-quality IoT devices can have problems.  The good news is our favorite Internet-connected or network-connected speakers, Leo, our Sonoses...



LEO:  Yes?



STEVE:  ...were not remotely compromisable.  Or maybe I shouldn't state that.  The Trend Micro researchers who poked at both the Sonos and at the Bose networked speakers found a worrisome array of problems in both.  But what was more worrisome is that somewhere between 4,000 and 5,000 Sonoses are publicly exposed to the Internet.  And you just have to wonder how that is happening.  I'm tongue-tied.  I was going to say this is further reason why I think that, if anyone needs to do a New Year's Resolution, it is logically segment your LAN so that IoT devices, where practical, can be on their own network.



The reason I say that is that people probably want their smartphone and their tablets to be on their secure network, except that, if you put a Sonos or other IoT system on a segmented network, well, by definition you cannot access it.  You can't get to it.  So it does create inconvenience if the devices you're trying to have in the maximum security network are unable to communicate with, but in order to use the IoT devices they need to.  I don't have a solution for that except, for example, if it would be feasible, maybe use an older iPhone or older smartphone, Android, to be the controller of your IoT things, if that's feasible.  It lives over in the insecure network, and you remember not to do anything important on it because it's over in the insecure network.  And you keep your other devices more safe, or at least your PCs, where you probably don't need the PC to be talking to IoT devices if your, for example, your smartphones can.  Leave all of them over on the segmented network.



Anyway, the point is we keep running across, and I'm sure this trend is going to continue and accelerate, as you said, Leo, this is the IoT Christmas because so much has happened in the last year, people were giving each other all kinds of Internet-connected things.



So Sonos responded very quickly to the Trend Micro researchers' reports, as quickly as anyone could ask.  They just chose Sonos because it was popular.  They also looked at Bose, although there was like four times, four or five times more Sonos systems that were just available due to Sonos's popularity.  What they found was, believe it or not, an internal web server exposed to the public Internet.  Shodan could be used to search for them and revealed more than 4,000.  And a subsequent search showed 5,000, but they realized there may be some IP overlap between them because IPs on consumer systems don't tend to be rigidly fixed.  But at least, in a single scan, more than 4,000 Sonos exposed web servers.



So our little cute Sonos speakers, which are very popular because the quality is very good, and then the feature set is nice, I really like mine.  And I am glad that I'm running them on a segmented network because experience demonstrates we have to.  The problem is for some reason, and I haven't yet gone to look because I wasn't worried about this from a security standpoint, but now I'm curious from an intellectual standpoint, do these all use UPnP to map themselves for public access?  It would be disappointing if they did that deliberately.  Maybe these are just somehow misconfigured or deliberately configured for that mode of operation, like they for some reason statically mapped the WAN into their Sonos device.



But for whatever reason, there are more than 4,000 Sonos speaker web servers visible on the 'Net.  And I think I had here which port they were using, but I'm not seeing it in my notes.  There is an extensive PDF that I've got links to from the show notes, which also shows a map of a big chunk of Europe, I meant to put it in the show notes and forgot, showing like their own geographic map of Europe just covered with Sonos speaker installations, each of which has an exposed web server.  So through the web server you are able to do a bunch of things.  You are able to look at the device's ARP table to discover the physical MAC addresses of all other devices on the LAN which the Sonos speaker is sharing with the LAN.  The most worrisome feature is you can look at the logon credentials of any music services which you have registered with the Sonos because of course it has to log...



LEO:  Oooh.



STEVE:  Yes, yes, not good.  That's been fixed, so that's no longer flapping in the breeze, but it was until Trend Micro said, uh, you know, we could see our Spotify account information.



LEO:  That's not good.  Password in the clear?



STEVE:  Yeah.



LEO:  Wow.



STEVE:  Yeah.  Because, again, the device needs it.  It needs to send it in the clear in order to log onto Spotify and wherever, Pandora and so forth, in order to access that music content.



LEO:  Yikes.



STEVE:  Yeah.  Also there's a very handy network page which allows you to issue your own network commands, like ping.  And so you're able to do a broadcast ping and get affirmative responses from every pingable device on the LAN.  So the point is, while this isn't a huge security problem, it really is, or was, a significant information disclosure problem.  And of less concern, but still a little annoying, they could peruse your music library, determine what your musical tastes were, determine what song you were listening to at the moment, if any, and basically get full access to what you're doing with Sonos.  In extrapolating the potential nefarious purposes this could be put to, the researchers said, well, you know, for a targeted attack, if somebody wanted to get you, and knew you were a Sonos user, they could get into your Sonos and remotely determine what hours of the day you are using it and when you're not, and to some degree fuzzily infer whether you are probably home at given times or not.



So anyway, it's not a huge issue, but it's an example of how, I mean, even very high-quality devices.  And I've had mine plugged in since the late summer, and I've been - they've been updating themselves a number of times over the last few months.  And the first time I've installed any of those devices, they've done a firmware update.  So the company is being responsible, but they must have assumed that this information, this access to the web server, was going to be restricted to other devices on the LAN, that is, never intending for it to be made public.



And that brings me back to something which is a constant issue with me, and an annoyance.  And that is, I have never seen anybody ever, except me on this podcast, say or observe that all that has to be done by any of these devices where you really do not want remote access, is set the TTL, the Time to Live, of the packets that are emitted from the device to one or two.  I mean, one.  Remember that any router that encounters one of these packets will decrement the TTL and drop it like a hot potato if it goes to zero.  That is a rule that is universally, I mean, it's like in an Internet, a global Internet where very few rules are actually universally followed, that's one that is.  You have to honor that, or we end up with the Internet having packets that never die, that just circulate and spin around on the 'Net forever.



So every router decrements TTL.  When it hits zero, it drops it and probably sends a notification back saying "expired."  And we know that this is, for example, how the traceroute command is able to figure out how packets travel, by deliberately setting short TTLs which cause an expiration ICMP message to be sent back.  My point is that, if devices like the Sonos that should never be accessible publicly simply emitted their packets with a TTL of one or maybe two, just to be safe, then you could never access them remotely.



In order to run a web server you've got to set up a TCP connection.  In order for that to happen, you have to have a three-way handshake, which means that you need to send them a packet, like it a packet, "it" the Sonos, and it needs to get one back to you.  And so all of this is TCP, which if you emit TTLs of one, those packets die at the first router they encounter, either at your border, you know, the little consumer router may not decrement TTL.  I'm not sure whether it does, or whether they all do.  But certainly the first router that you hit at your ISP is going to decrement that and just say, sorry, we don't know what you're doing.  You may be trying to traceroute us.  But we're not forwarding that packet.



If people would do that, people who do not want their local stuff to be publicly available, it just ends it.  I mean, you could have all your ports open.  You could do anything you wanted, if you simply set TTLs to one.  I've never seen it done, and I've never seen it talked about or mentioned.



LEO:  This is not something you could do.  This is something that the device would have to do.



STEVE:  Correct, correct.  It's down in the stack.  So it's the kind of thing you could do with raw sockets.  But it's also something that someone building, I mean, these are all Linux-based.  You just rebuild the kernel so the default TTL - and normally there's a setting because remember we've talked about this problem.  Once upon a time TTLs were, like, 32.  And it's considered the diameter of the Internet, that is, what are the two points on the Internet where there are the greatest number of hops.



And at some point, years ago, OSes were setting their TTL to 32 because, well, that's high enough.  Well, no.  There were so many routers added that the diameter of the Internet became greater than 32, and there were some locations you could not reach with a TTL of 32 because the packet would expire before it got there.  So then we had to bump it up to 64.  In some cases now it's 128.  But it is normally something you can set.  And so if they'd just set the TTL to one, this problem would never occur.  You could connect to these devices locally with no problem at all because it's just device to device.  But the moment it tried to cross a router boundary, that packet would die.



And that company, as a consequence, would have amazing security because it wouldn't matter if they left ports open, or they had insecurities that might end up going to be publicly facing on the Internet.  It just - everything would just die.  So I don't think of it often, but I talk about this whenever it occurs to me because it's a great security measure that we're completely missing, that I've just never seen anyone implement.



LEO:  You've said this before, have you?



STEVE:  Yeah.



LEO:  Wow.  This is a good - seems obvious and simple.



STEVE:  It'd just be - and bulletproof.  And like it cannot be defeated.  It's just bulletproof.



LEO:  Wow.  It just doesn't pop out of your own network.  It can't.



STEVE:  And you wouldn't think I'd need any more caffeine after that, but...  



LEO:  Well, too late.  Go ahead, we're going to watch you drink.  We've done all the ads.  We shall enjoy the sight of Steve consuming beverages from his massive mug.  Dilly dilly.  Just say "dilly dilly."  Go ahead.  You don't know what dilly dilly is.



STEVE:  Leo, you don't need any more caffeine.



LEO:  I've been hanging out with the teenagers.



STEVE:  That's right.  So TechCrunch covered some interesting research which will be presented at the upcoming Network and Distributed Systems Security Symposium at the end of next month, end of February 2018.  This was conducted by the University of Iowa and UC Riverside.  They carefully profiled the behavior of the top 10,000 sites on the 'Net.  And what they discovered was one third of the top 10,000 were showing some active awareness, concern, or in some cases deliberate anti-adblocking effort.  That is, they were adblocking aware.  Their behavior was different if a client did not follow through and pull ads from the site.  And in some cases they were going to anti-adblocking efforts to thwart the efforts of ad blockers to block ads.



And it's interesting is that the top 1,000 sites, so the top 10th of the top 10,000, the top 1,000 sites had an even higher instance.  They were at 38.2% of those top 1,000 sites altering their behavior in some way based upon the blocking behavior of the browser.  So essentially they are seeing evidence of what we anticipated when we first started talking about advertising and adblocking, which is an escalation of this battle.  Sites are beginning to decide that they need to show people ads, even when the people, their visitors have said, "No, I don't want ads here"; or in some cases have responded to a "please accept ads" in some fashion because we're ad...



LEO:  I get that more and more now, yeah.



STEVE:  Yes.



LEO:  I do that.  I think we want - I want to support these sites.



STEVE:  I do, too.  And what I'm wishing is that it was easier to do that.  You know, we're all in a hurry.  We're visiting somewhere quickly.  We're not sure if there's anything here that we need.  But unfortunately it's too difficult in several cases to whitelist a site whose ads you do want.  And so I'm wishing...  



LEO:  In uBlock Origin you just go to the uBlock Origin dropdown and click that button; right?



STEVE:  And press the button.



LEO:  It's permanent; right?  Yeah.



STEVE:  That should be permanent.



LEO:  It's not too bad, yeah.



STEVE:  I think it's whether it's that page or that site that I can't remember.  So, yeah, you're right.  It's not too bad.



LEO:  I do it all the time.  I think it's...



STEVE:  And worth doing.



LEO:  Yeah.



STEVE:  Yes, and boy, Jimmy sure dunned me on wanting money for Wikipedia this last time.



LEO:  Yeah, that's really annoying.



STEVE:  It really is.  I gave him $100 and said go away.  And then I kept getting, I mean, it's like they didn't know who I was.



LEO:  Yeah, I'm thinking they probably don't want to set a cookie.  Maybe they're just being privacy focused.  You see?  There's advantages to cookies.  I give him money every month, and I still get that, you know.



STEVE:  Yeah.  It's just like, okay, I mean, but they were pushing it this last season.



LEO:  Well, they did this last year, too.



STEVE:  Oh, they do.  Some of the information that I was just talking about in this podcast about the behavior of the page table stuff I got by doing a quick read through what Wikipedia had to say about it.  So super useful.



LEO:  Right, right.  It's a very good resource, yeah.



STEVE:  Yeah.  So a quickie for users of Thunderbird, if there are five of you out there using Thunderbird.  I had no idea that the adoption was so skinny.  I found a number of 0.7%.  So I don't know how many people are using it.  But for what it's worth, there was a bunch of significant problems just fixed, which are fixed in 52.5.2, which is the one that you want to make sure you're using.



I mean, again, it's hard to imagine with the adoption level so low that anyone would take issue.  But email typically carries in its headers the identity of the email client.  So somebody would know that you were a Thunderbird client user and could target you if they knew that and you had not patched.  So just a quick heads-up that, if you're a Thunderbird user, make sure you've got the latest.  Mozilla patched these problems quickly.  There was a critical buffer overflow.  It does allow an attacker's code to be run on your system.  So make sure you've got that patched.



Also, I did want to mention, because I've been on this podcast bitching about how unusable my iPhone 6 Plus had become, and saying that, you know...



LEO:  Now we know why.



STEVE:  Exactly.  Exactly.  Apple finally admitted to what we all pretty much knew or presumed, which is that the newer iOS releases were for some reason apparently deliberately underclocking the older phones.  Well, now we know that's absolutely the case.  And this whole thing seems a little fishy to me, Leo, even so.



LEO:  Interesting.  Okay.



STEVE:  Yes.  I really baby my battery.  And it's 100% strong, and it's a 6 Plus.  So I skipped over the 7 and the 8, and I've stayed with the 6 Plus.



LEO:  6s or a 6?



STEVE:  6 Plus.



LEO:  There's 6 Plus or a 6s Plus.  So you're saying it's a 6.  Wow.



STEVE:  Yes.



LEO:  That's pretty old, Steven.



STEVE:  I know.  And, I mean, absolutely the battery, I can have it out all day, and it's at 95%.



LEO:  You can check.  There are battery tools.  In fact, Apple's going to put it in iOS.  It'll tell you how much of your original battery life is still available.  It's obviously thinking, you know, we don't know what Apple [crosstalk].



STEVE:  Well, I will absolutely be taking advantage of the $29 discounted battery replacement for that phone.  I have a 10.  But in truth, I was driven to the 10 because my 6 had become unusable.  And I'm now looking - I use CPU Dasher X, and I'm at about - I'm less than half of the phone's original clock rate on my 6, for no good reason that I can see, when it's plugged in, when it's charged at 100%.  I believe they're looking at age rather than battery capacity and just assuming that, oh, well, if the phone's that old, it just must be hardly able to get off the ground now.



Well, no, not my phone.  My phone's in great shape.  But I don't get any credit for that.  So, I mean, so I'm absolutely going to replace the battery.  For 29 bucks it turns it into a new phone.  Anyway, I'm really very upset.  I've been talking about what a catastrophe iOS 11 has been with all of the crazy bugs.  And it rendered my phone, my perfectly functional iPhone 6 completely unusable.  And it's not like these things are cheap, as has often been noted.  So anyway, I think I'm done ranting about my iPhone experience.  I'm just, you know, wow.



LEO:  I would love you to get one of those battery checkers.



STEVE:  I'll do it, for sure.



LEO:  To just see what percentage, how many charges and what percentage is left.



STEVE:  And so does it pull data from the phone?



LEO:  Apparently.



STEVE:  Do you let it run, and it runs the battery down to see how long it takes?



LEO:  No, no, no, no, it doesn't have to do that.  It can just - there's information offered by the chip, and somehow it gets that information.



STEVE:  Okay, cool.  I will.



LEO:  Yeah, there's a number of those.  And I'd just be curious.  I mean...



STEVE:  Yeah.



LEO:  I would hope Apple's - I know they're not doing it by age because, if you put a new battery in, by all reports it fixes it.



STEVE:  Well, but, see, the battery is a subsystem itself.  So it would know that it was a new battery.  It would know when that battery was installed.  It probably has a serial number and a battery management processor and, you know...



LEO:  Right, I'm sure that's the case.



STEVE:  ...all kinds of crap.



LEO:  Yeah.



STEVE:  I wanted to alert our listeners that we have another Humble Bundle, and this time it's Python books, which I think is probably right in the middle of many people's sweet spot.  You know, Python is just really taking off.  I'm encountering, as I have said over and over, I mean like more and more.  And I had a very respected top-level programmer friend who has been a friend of mine for - how old am I now? - like 40-plus years, who programs in everything, tell me that he has never been more productive than he is in Python.



LEO:  Yeah, it's a great language.



STEVE:  So, and it is my intention to rewrite the UI and the high-level portions of SpinRite 7 in Python.  The idea is that the SpinRite 6.1, .2, .3 stuff, that will be sort of the testing bench for the new low-level drivers.  And then I'm going to scrap it and keep all of that low-level work, but then write a whole new front-end for SpinRite, adding a whole bunch more features for SpinRite 7.



LEO:  I would look at the reviews of these books before you buy them.



STEVE:  Yes.



LEO:  These are not my favorite Python books, by any means.



STEVE:  And I'm glad you said that.  I was going to say that.  I don't know Packt, P-A-C-K-T.



LEO:  They're all from Packt, aren't they.



STEVE:  Yes, they are a Packt book bundle.  And so I think they may be along the cheesy side.  But they're also not very expensive.



LEO:  Yeah, doesn't hurt to have it.  I mean, I do the O'Reilly books pretty much.  "Learning Python" is excellent, "Programming Python," those are classics.  



STEVE:  Yup.



LEO:  But there are some actually very good Python books out there.



STEVE:  Yeah.  And you can't go wrong with O'Reilly.  Tim is not going to be publishing junk.  



LEO:  Yeah, yeah.



STEVE:  And finally, in miscellany, we've had a lot of fun, Leo, over the years, laughing at robots falling down.  There is a recently posted YouTube video that I linked here which is rather phenomenal.  This is our friends at Boston Dynamics who are doing these amazing robots.  This is Atlas, which is their human anthropomorphic robot, and it is doing some incredible things now, including a back flip.  So this thing isn't just able to walk upright, bipedal, but it's able to do some incredible work.  So if anyone is interested, I've got the link to it.  And, yup, there it is.  Look at this thing.



[YouTube clip]



LEO:  And for those who aren't watching but listening, it's doing back flips, moving around...



STEVE:  Oh, Leo.  I don't want to meet this thing on a...



LEO:  Oh, I know.  Look at that back flip.



STEVE:  It's, wow.



LEO:  It's better than a gymnast.



STEVE:  It's incredible.  But, I mean, when you think about what is necessary to do that, it's just stunning.



LEO:  Yeah.  Good balance, yeah.



STEVE:  So, wow.



LEO:  And they have videos of it running out in the field, too.  Those are the ones that scare me, you know, when they go outside.



STEVE:  Yeah.



LEO:  And they start chasing people.  That's what scares me.



STEVE:  That's right.  Skynet is circling above.



LEO:  Can't escape it.



STEVE:  So I got a note which prompts me to explain something from an Adriaan Stander, who's in Le Havre, France.  This was dated two days before the new year, the 29th of December, was asking a question about SpinRite's levels.  He said:  "Dear Steve, I'm a longtime SpinRite owner, which I bought not because I had a problem with a drive, but as a reward for your work on Security Now!."



LEO:  Nice.



STEVE:  Well, thank you, Adriaan.  "I have never had to use it in anger" - or desperation, I suppose.  He says:  "...(i.e., for data recovery), but I do use it from time to time for disk maintenance."  And of course I would argue that's why you've never had to use it in anger is that you're running it occasionally to keep your disks from ever having a problem, which actually has been proven as much as possible to prove such a thing, to work.



He says:  "Can you perhaps sometime in the future" - well, technically this is, but only five days - "on a Security Now! podcast, explain the different levels of SpinRite in a bit more detail than what comes up when you run SpinRite?  In particular, can you please give some practical examples of when levels 1, 3, and 5 may be useful?  Kind regards, Adriaan Stander."



Okay.  So for anyone who knows SpinRite 6, you're aware that the UI promotes the use of 2 and 4, levels 2 and 4.  And that's the ones we talk about.  I'm probably going to whittle those down even further.  In the old days, SpinRite had a bunch of levels where I was using different levels of surface defect analysis.  Level 1 has always been enforced read-only, just because I thought it should have that.  That is, it will only, only, only ever read, and it will notify you if there's a problem.  It will not ever write to the disk to fix it.  So Level 2 is sort of the read-only level where it only reads unless there's a problem, and then it will examine that location to determine whether the problem is soft, and it could be fixed, or whether it is safe for data to remain there in the future.



I don't even remember now what Level 3 is.  It's somewhere between 2 and 4.  Literally, I can't remember what Level 3 is.  I mean, it has some different features, but it's been a while since I was down in the code.  Four goes beyond 2, which is read-only, and writes by default.  So it does a full pattern test, well, as much of a pattern test as contemporary drives need.  What's happened over time is that so much technology has been installed into the drive's read and write channel that it absolutely no longer makes sense to imagine that user-provided data can result in knowable flux transitions on the drive.  Before drives got as smart as they are, you actually could determine, you actually were able to write data that produced specific events of magnetic flux on the disk.  And that was the big breakthrough in SpinRite 3.1 was it reverse-engineered the write channel in order to deliberately find defects.



Now there is so much going on between the data we write and what happens magnetically that there's no way for the user to control that.  So what I've settled on, which is proven to be effective now because SpinRite 6 has been around long enough that we've acquired a lot of history, is it reads what's on the sector.  It inverts it and writes it back, reads it to make sure that it was able to read the inverted data, then inverts it again, writes it, and reads it back.  So essentially it flips all of the data bits into their non-one or zero state and then back again and makes sure that that process succeeds.  Often in the process that shows the drive that there's something wrong with a sector it wasn't aware of, which causes it to then take action, map that data, map that sector out of use, and swap a spare sector in, which then is where SpinRite writes the final data.  So that's part of that process.



And 5 just does that even further.  It performs additional testing of the surface, but not testing that I'm convinced has ever been useful.  I think I just - I was reluctant when I was working on 6 to prune back the options too much.  And the user interface, by the way, does show you what Level 3 is.  If you bring up the normal UI, you're able to step through 1, 2, 3, 4, and 5.  And I describe there in the text what each of the levels does.  And so it's possible to answer that question just by looking at the UI.



But basically 1 absolutely never writes anything, period.  It can't.  Which doesn't mean the drive won't still perform its own autonomous sector swapping, if you were to read a sector which scared the drive.  And anyway, so it's very fast and may induce the drive to make some changes.  Two definitely does.  But it's read-only unless it finds a problem.  So that's the one we recommend for use on solid-state drives where you don't want to read and write to them needlessly.  And 4 is what we recommend for spinning drives, although because it's transferring so much data, remember, it's writing the entire drive, reading the entire drive, writing the entire drive, and reading the entire drive, which just takes time on big drives.



The first thing that will happen with SpinRite 6.1 is that time it takes will be dropping dramatically.  As I have said in the past, I did clock it before, I clocked the prototype driver that I had written at half a terabyte per hour.  So that means it'll be able to do a multi-terabyte disk in a reasonable amount of time.  So that'll be good news for everybody who will be able to get that at no charge, as I have promised.



And one bit of closing-the-loop feedback from a Dallas Haselhorst, who said - oh, yeah.  He said:  "Steve, on Episode 642, in relation to SpinRite, you mentioned you should not use swap with SSD.  I read about this quite extensively some time ago, and I was unable to come up with a solid answer one way or another, so I was surprised to hear you give it a definitive thumbs down.  Does it matter which OS you are using, for example, Windows or FreeBSD?  If the system has plenty of RAM, would adding swap cause any harm?  Thanks."



Okay.  So, yeah.  I'm adamant about it.  I don't know whether Windows is aware, but I sure hope it is because on a system that doesn't have an alternative place to write to put a swap file, I'm worried that the paging file is being written a lot.  What we do see is a lot of page file activity on traditional Windows.  And I do know from an experiment that a good friend of mind, Mark Thompson, performed that a Linux system will burn out a compact disk where there's a swap file on it in very short order.  So I don't - it's impossible to know whether OSes, or I should say I don't know whether OSes are smart enough to recognize that they are swapping on an SSD and to dramatically limit that.  But the problem is at least Windows seems to swap when it has no good reason to do so, just because it can.



So one of the things that I have seen done is for a RAM disk to be established.  And as far as I know, even on late-model Windows systems, you can still set up a RAM disk, although it's expensive because that RAM of course becomes unavailable to your system, but then set the swap drive there.  That's something we used to do in the old days, a long time ago, in order to speed things up.  But I'm just hoping that OSes are being faithful about this because it really does needlessly write to an SSD, even when really you could argue there's nothing, no need to swap when you've got a ton of RAM and you're not at max.



Okay.  Finally, the strange story of "Extended Random."  Or, as I titled this podcast, "The NSA's Fingerprints."  Matthew Green, who is often quoted on this podcast, Johns Hopkins University cryptographer and now professor, made some interesting observations.  He said there is additional circumstantial evidence to strongly, well, actually I'm saying, it's my voice, there's additional circumstantial evidence to strongly suggest that the United States NSA, the National Security Agency, was attempting to compromise encrypted communications.



As I mentioned at the top of the show, for reasons that were never clear, RSA Security, the very well-known inventors of public key cryptography and publishers of a very popular cryptography package known as BSAFE, were found years ago to have set as default an arguably insecure pseudorandom number generator, a so-called DRBG, Deterministic Random Bit Generator, using some elliptic curve functions whose magic numbers were never adequately explained.  The magic numbers came sort of like from whole cloth.  And it's like, okay, who chose those?  And it was never really clear.  But it looked like this came from the NSA via the NIST as part of the standards process.



But whenever you use magic numbers in something, moving forward especially, there has to be evidence provided of where that number came from, why that number was specifically chosen.  We didn't have that with this elliptic curve DRBG.  And moreover, because it uses elliptic curve, which is an expensive algorithm in terms of time, it was the slowest of the four.  There was a symmetric cipher DRBG that would have been knowably secure and faster, which RSA didn't choose to use as their default.  No explanation given.  And as I mentioned, Reuters also apparently, well, Reuters did report that RSA was apparently paid $10 million from the NSA to put that into their BSAFE library.  But it was sort of off in the, I mean, the whole RSA package wasn't clearly in use.  No evidence of its deployment could be found.  And that's what changed a couple weeks ago.



The trouble with the actual exploitation, or the compromise, of the Dual EC DRBG was that sufficient bits generated by it had to be obtained by an attacker in order to determine the internal state of the generator.  So you need to see enough of its output in order to have enough leverage against it.  And the normal TLS connection handshake, which as we've often discussed does contain some nonces, some pseudorandom data that has to not repeat, the amount of entropy in the handshake falls just shy of providing as much nonce information as an attacker would need to compromise the future numbers that the DRBG was going to be spitting out.



So now enter what's known as the Extended Random extension to TLS.  As Matthew writes, he said:  "The Extended Random extension is an IETF draft proposed by an NSA employee named Margaret Salter," he says, parens, "(at some point the head of NSA's Information Assurance Directorate, which worked on 'defensive' crypto for the DoD), along with Eric Rescorla as a contractor."  And I've seen Eric's name on a number of RFCs through the years, so he's well known in the community.  So again, this Extended Random extension to TLS was proposed by an NSA employee.



So this Extended Random extension was never adopted.  But it was a means of asking a TLS endpoint to provide more random data, more nonces, essentially.  Matt wrote:  "It's important to note that Extended Random by itself does not introduce any cryptographic vulnerabilities.  All it does is increase the amount of random data, the nonces, used in a TLS protocol connection."  He writes:  "This shouldn't hurt TLS at all; and, besides, it was largely intended for U.S. government machines."  He says:  "The only thing that's interesting about Extended Random is what happens when that random data is generated using the Dual EC algorithm.  Specifically," he writes, "this extra data acts as 'rocket fuel'" - his words - "significantly increasing the efficiency of exploiting the Dual EC backdoor to decrypt TLS connections."



Okay.  So there's that.  But all of this still remains academic, or did until two weeks ago because what happened was some researchers discovered that older Canon printers were unable to connect with TLS v1.3.  Remember that pretty much the world is now at TLS v1.2.  1.3 is causing some problems because it's hostile to middleboxes.  It's hostile, for example, to corporate enterprise proxies that want to be able to filter HTTPS connections and see into those.  And it's been hostile to apparently large datacenters who want to have a similar proxy on their border and decrypt all traffic within the datacenter, which makes people feel a little bit queasy.  So 1.3 has sort of hit some problems, some roadblocks as a consequence of the fact that its security is so good that it's hostile to some of the insecure things that people are currently doing with TLS.



Well, it turns out that the 1.3 specification was using an extension identifier, I think it was 4.0 if memory serves, which was also used by the proposed, but never adopted, IETF draft which the NSA proposed for Extended Random.  In other words, this Extended Random extension was proposed.  It was going to use a certain extension number to tack itself onto the existing TLS protocol.  Didn't get adopted.  Went nowhere.  People forgot about it.  And as a consequence of the fact that the extension that it was using was unused, it became used as part of the official TLS 1.3 work.



Then some Canon printers were found to be unable to use TLS 1.3.  So some researchers thought, okay, what's going on here?  Why can't these printers connect over 1.3?  Why?  Because those printers were actively using this never-deployed Extended Random extension and the Dual EC DRBG random number generator for their TLS.  So no one was ever aware that the elliptic curve DRBG had actually been deployed, nor that this Extended Random Extension had ever been used.  It turns out they were both in the RSA BSAFE library.  And for whatever reason, and for whatever history, Canon chose to use all of that.  And everything was fine until - that is to say, it went undetected until TLS 1.3 reused the extension of the never adopted proposed extension from the NSA.



So again, we're never going to know for sure what was going on here.  But what we do know now that we did not know before is that the RSA BSAFE library added the Extended Random extension to provide more Dual EC DRBG bits in the TLS handshake, which the NSA asked for, the industry never adopted and ignored, but RSA went ahead anyway and stuck it in their library, which Canon printers then used.  And now we finally got this last bit of information.



So none of this is proof.  But as we know, this is the nature of these things, that there will likely never be absolute proof found.  But one could argue that this is about as much proof as you could ever get that something was going on behind the scenes where the NSA was hoping that what would become standard would end up being put into products where they could connect with them and compromise or watch the connections occur and then compromise the handshake nonces and then be able to encrypt the data.  It doesn't look like that happened, but it does give us - it certainly looks like an NSA fingerprint in this whole process.  Wow.



LEO:  Very interesting.  Busted.



STEVE:  Yup, exactly.



LEO:  Ladies and gentlemen, we've come to the conclusion.  Two liters of coffee and two hours of security has brought us to the end of this fine program.  Thank you, Steve.  We do this show every Tuesday, right after MacBreak Weekly, about 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  If you want to tune in and watch, you can at TWiT.tv/live.  But we also have on-demand audio and video available.  Steve's got it at GRC.com.  You also get transcripts there, a few days after the show.  Steve gets Elaine Farris to write everything down, and that way you can read along or search, which is really a nice feature.



GRC.com is Steve's home on the Internet, short for Gibson Research Corporation.  While you're there, pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility, and check out all the other freebies Steve has there.  You can leave him feedback there at GRC.com/feedback.  But I think probably the easiest way is Twitter.  He's @SGgrc on Twitter, and he takes DMs from anyone, of any length.  We get a lot of questions for the show from there.  You can also follow him, and it's a good way to kind of have a constant radar on security issues because you tweet all week, I know.



And then we have audio and video at our website, TWiT.tv/sn.  Or you can subscribe, you know, if you use a podcast client on your phone or your tablet or your desktop, you can use any podcast client to search for Security Now! and subscribe in that way.  You'll collect all the episodes, the best thing to do.  Now, you'll be starting from now, but you can go back in time at TWiT.tv/sn and get the old shows, all 600 whatever it is, 30 - what did you say, 630 - 644.



STEVE:  Six hundred and forty-four.



LEO:  Jiminy Christmas.



STEVE:  Woohoo.



LEO:  Jiminy Christmas.



STEVE:  And going strong.



LEO:  Let's do our weekly bitcoin check.  Have you checked your bitcoins lately?



STEVE:  Well, they did take a hit over the holidays but are beginning to creep back up.  I saw it at like 14,000 when I looked this morning.  So I'm just kind of keeping an eye on it.



LEO:  Not worrying about the bitcoins in the corner, the 50 bitcoins in the corner.  I still can't find my password, so...



STEVE:  Oh, in fact I heard - I got a nice note from a listener of ours who, back when we discussed bitcoins...



LEO:  By the way, that was our holiday episode; right?



STEVE:  Yes.  And in fact it was...



LEO:  I know what you're going to say because I got that email, too.



STEVE:  Oh, okay, yeah.



LEO:  Because I created the wallet during that show.



STEVE:  Right.  Well, remember that we pointed people at the Bitcoin Faucet, which at the time allowed - it was like a little drip.  It would give you a little drip of bitcoins.  What that drip was out of that faucet is now worth $743.



LEO:  Each drip.



STEVE:  Each drip.  And one of our listeners thanked us for $743 because he cashed in his drip from his wallet that he still had and made some money off of it.  So, yeah.



LEO:  I'm glad you cashed in your drip.  We should all cash in our drips.



STEVE:  Which is not a phrase I expected to be saying.



LEO:  But we all learned something in that bitcoin episode.  Thank you for suggesting that as a holiday.



STEVE:  I got a lot of nice feedback from people who were happy to see that again.  So that was a good thing.  And Leo, speaking of good things, I will be back next week with you.



LEO:  Au revoir, Steve.



STEVE:  And we will catch everybody up on what has happened since today.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#645

DATE:		January 9, 2018

TITLE:		The Speculation Meltdown

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-645.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week, before we focus upon the industry-wide catastrophe enabled by precisely timing the instruction execution of all contemporary high-performance processor architectures, we examine a change in Microsoft's policy regarding non-Microsoft AV systems, Firefox Quantum's performance when tracking protections are enabled, the very worrisome hard-coded backdoors in 10 of Western Digital's My Cloud drives; and, if at first (WEP) and at second (WPA) and at third (WPA2) and at fourth (WPS) you don't succeed, try, try, try, try, try yet again with WPA3, another crucial cryptographic system being developed by a closed members-only committee.



SHOW TEASE:  It's time for Security Now!.  Finally, the long-awaited what the heck is going on with Intel processors edition.  Steve breaks down Meltdown and Spectre - by the way, it's not just for Intel anymore - and talks about what mitigation might involve and what the consequences of fixing this massive flaw could be.  There's lots of other security news, too.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 645, recorded Tuesday, January 9th, 2018:  The Speculation Meltdown.



It's time for Security Now!, the show where we cover security, now, now, with Steve Gibson.  He is our security minder in chief.  Hello, Steve.  Good to see you.



STEVE GIBSON:  Leo, great to be with you again for another week of exciting stuff for the true geek.



LEO:  It was kind of funny last week because you talked about what were later to be dubbed Spectre and Meltdown.  And you were kind of matter-of-fact about it.  I mean, I heard it.  In fact, I even mentioned, gee, that sounds kind of like Rowhammer.  But I don't know.  You must have understood its impact, but I didn't.



STEVE:  I did.  And as you know, I don't like to go, like, overhyping things and running around with my hair on fire.  So I got a little down into the weeds about the performance impact of being forced to flush the virtual memory, the so-called Translation Lookaside Buffer caches, whenever you made a switch from the app to the OS, which is potentially huge.  And I got into, like, explaining what this meant, that it was possible with the Meltdown attack.  And so this was the day before then the full disclosures came out that there was so-called Meltdown and also something called Spectre.



LEO:  Yeah, we didn't know that much on Tuesday because it was just a hint.  We hadn't seen the full report till Wednesday. 



STEVE:  Yes, it was from looking - it was like weird footprints in the Unix source where comments had been redacted that normally wouldn't be, and the explainer page for the changes was missing.  And it was like, and then news that Microsoft had been scurrying around, I mean, there wasn't anything definitive.  Now we know way more.  So I titled today's podcast "The Speculation Meltdown" because, sort of taking more of the 10,000-foot view of this, believe it or not, there was a paper written in 1992 which recognized this problem.



LEO:  What?  What?



STEVE:  Twenty-five years ago.



LEO:  Oh, that's not good. 



STEVE:  And it was then referred to in more detail three years later, in '95, about like how this kind of thing could happen.  And back then it was, oh, but, you know, that would be too hard.  Or no one's going to bother with that.  And so we, the industry, and not just Intel, but as we know now all of the major chip vendors, AMD and ARM, are all subject to these problems.



The reason is that this has been the way, the so-called "speculation" has been the way all high-performance modern processor architectures are able to continue to squeeze phenomenal performance out of the system.  And so this is not just Intel, as we know.  And in fact, I was distressed to see that there were some class-action suits that were being filed immediately in kneejerk reaction to this, which I regard as absolutely unfortunate.  But anyway, we'll get to all that.



I formulated a general law of cross-task information leakage which reads:  "In any setting where short-term performance optimizations have global effect, a sufficiently clever task can infer the recent history of other tasks by observing its own performance."  And I worked on that for a while.  "In any setting where short-term performance optimizations have global effect, a sufficiently clever task can infer the recent history of other tasks by observing its own performance."  Which that's like the meta statement of the problem of how there has always been, and that's what's so interesting about this, there has always been this problem.



And what happened was that sort of, I mean, and this is classic security paradigm that we see over and over and over, is that something that was sitting there for a long time, somebody stubbed their toe on and said, "Ow, wait a minute, what's that doing?"  And suddenly it's a big deal.  So we're going to have fun at the end of the podcast talking about, again, sort of not like which patch to apply or what to do immediately, but where this problem came from.  What's the origin of something which, without overhyping it, is probably not overstating it to call it an "industry-wide catastrophe."



LEO:  Wow.



STEVE:  But there are a few other things to talk about, as well.  We're going to examine an interesting change in, well, I write "Microsoft's policy," but more of a Microsoft implementation regarding non-Microsoft antiviral systems that appeared last Wednesday, on the 3rd.  An interesting comparison of Firefox Quantum's browser performance when tracking protection is enabled or disabled.  Some very worrisome hard-coding backdoors which were discovered in 10 of Western Digital's My Cloud drives, since patched, now public.  But we want to make sure anybody with a My Cloud drive knows about this because they're incredibly bad.  And if at first with WEP and at second with WPA and at third with WPA2 and at fourth with WPS you don't succeed...



LEO:  Try WPA3.



STEVE:  Try, try, try try try yet again with WPA3, exactly.  And here we have another crucial cryptographic system being developed by a closed members-only committee as all the past failures of the Wi-Fi Alliance have been.  It boggles the mind that in this day that can still be the way we're doing things when again, five strikes so far.  And, okay, they're getting warmer.  So we'll see.  So I think a great podcast for us.



LEO:  A nice bunch of stuff.  All right, Steve.  You have a picture?



STEVE:  So our Picture of the Week, yes.



LEO:  Oh, yes.  Wow.



STEVE:  It's more what I was expecting when I said last week that the consequence of being forced to flush the page lookup tables, to me, was mindboggling.  I'm old school, and I don't really believe any of this works.  I mean, it just - it's like, no, that can't get off the ground.  So the idea is, as I was explaining last week, the virtual memory mapper in Intel architectures is this three-level lookup table where the address that you're fetching isn't the actual physical address at all.  It's an index into a table which is an index into a table which is an index into a table.  And you need all of those in order to figure out what the value is.



And you do cache the results, and those tables are cached, and that's what's called the TLB, the Translation Lookaside Buffer, because there's this notion in general in computing that something you've done recently, you are more likely to do again than not.  And the classic example is looping.  If you are looping over some code, that means you go back to the top, and you do it again with different values or for different inputs or something.  But so that says that, oh, look, I'm rerunning instructions that I recently ran.



So this is why the whole concept of caching makes sense.  We have dynamic memory, the main motherboard, whatever it is, 4, 8, 16 gigabytes of main RAM.  Due to the technology, it fell behind a long time ago in keeping up with our processors.  Just no one has been able to make main memory go fast.  Now, we've talked about little interesting ideas.  There's some cross-wire technology that HP is kind of trying to get off the ground.  Intel has it, too:  XPoint.  So maybe we'll see something dramatic happen.  But so far main memory, compared to the way our processors have just leaped ahead in speed, it's the laggard.  So it's necessary to use this notion of reusing something or hoping that you're going to be able to reuse something you recently slowly fetched from main memory again.



So all of these modern chips have on-chip caches.  And you mentioned L1, L2, L3.  They're in a hierarchy.  The ones closest to the chip are the fastest and the smallest.  And then there's a next level further back, and then a third level.  And they each get larger and sometimes slower, but also it's more of a function of access patterns when you've got multiple cores.  Now you've got different processor cores all competing with each other for sharing this common pool of cache, all of which is trying to provide, like just arrange to feed these hungry chips data as quickly as they need it.



So also shared are these virtual memory lookup tables.  And so what I was saying last week was the idea that crossing from the userland, user code to the kernel would require the flushing of that incredibly valuable because it's so slow to get it to load it.  The idea that it would force the flushing of that cache was just like, as I said, I don't really believe any of this works to start with.  I've sort of reluctantly agreed that, okay, WiFi seems to be working somehow at ridiculous speeds.  And when modems started going to 56K, it's like, what?  No.  Modems go at 300 baud, or maybe 12, not 56K.  But, yeah, they somehow did.



So, okay.  So this picture demonstrates what happened on one of three of Epic Games' servers when they installed the Meltdown mitigation.  And this is much more in keeping with what I expected.  So you can't see it, but there are three chart lines.  There's green, orange, and blue.  There's a little index down in the lower left, a little key showing them.  And so what you see is this sine wave, which anyone who's run a big server is familiar with.  That's the diurnal cycle of CPU being busy.  You notice it hits a nadir.  They're all kind of lowest around 4:00 a.m., and I'm not sure if this is UTC or what time zone.  But then it also peaks pretty much on the opposite side of the day.  And so that's just sort of generally people are sleeping, and then they're waking up and doing stuff.



Well, that sudden jump is one of the three processors, the green trace, where Meltdown mitigation was applied.  And it's about two and a half times.  If you look at the low point there of all the traces, just before 4:00 a.m., you could see that it crosses right around 10%.  And now if you look up at the green line, it's more than 25%.  It's like maybe about 27%.  Whereas the other guys are at 10.



So that's, I mean, for their use case, the Meltdown mitigation jumped their processor's utilization, CPU utilization, by about 2.7, which I believe based on the severity of the need to discard this slow-to-acquire cache data every time you switch between the kernel and the user.  So anyway, this is the kind of picture that is what I was expecting, rather than this, you know, we've heard 1%, or point something, or maybe 5.  It's like, okay.  Again, it is completely a function of what you're doing.  We've talked often about...



LEO:  How much kernel mode access you need compared to how much user mode?



STEVE:  Well, actually it's how much crossing.  It's the boundary crossing.



LEO:  Right.  So if you go into the kernel, and you read, read, read, read, read...



STEVE:  And you stay there.



LEO:  ...and then you exit the kernel...



STEVE:  Right.



LEO:  ...Then there's that one swap then.



STEVE:  Right.



LEO:  But if you go back and forth, that's when you make the biggest hit, right, because you don't want user mode to have access to kernel mode information.  So you flush the cache.  You go into user mode.  And then you go back to kernel mode.  Do they flush it twice?  Do they flush it when you go back into kernel mode?  No, you wouldn't need to then.



STEVE:  Correct.  You don't need to, well, you don't need to flush it when you go from the kernel, I mean, when you go from the user to the kernel because we trust the kernel.  There's no malware there.  It's user code looking at the residual from being in the kernel.  And so it's when you switch to the user that the kernel needs to remove its what I described last week as "footprints" because essentially, and this comes back to that statement I made, in any setting - and caching is a perfect example.  It's not Speculation.  We'll talk about that in a second.



But in any setting where short-term performance optimizations have global effect, a sufficiently clever task can infer the recent history of other tasks by observing its own performance.  In other words, so caching is a short-term performance optimization.  And if the cache is global, that is, if everybody gets the benefit of it, then clever tasks can figure out what's in the cache and what isn't by making a fetch and observing how long it takes, observing its own performance.



And so the 20,000-foot view here is that the way we've been cruising for the last several decades is we've been creating faster performance by creating processors that are increasingly clever about optimizing what they expect to have happen next.  But that expectation, whether it's cache, or whether it's specular architecture that we'll be talking about, Speculation, that inherently, inherently allows these kinds of problems.  And so that's why this is, as I said before, this is industry-shaking.  I mean, again, this has always been sitting there, nobody really worried about it, until now everybody's worrying about it.



LEO:  And we don't know who knew about it and when.



STEVE:  That's true.



LEO:  We've learned about it now, but who knows?



STEVE:  That's true.



LEO:  Although, you know, it feels like this is - and this is the other thing I've been speculating on.  And I can't remember if it was Bruce Schneier or Brian Krebs talked about the synchronicity, the fact that two different teams simultaneously discovered this.  I would guess security researchers kind of run down similar avenues.



STEVE:  Four teams, actually.



LEO:  Four teams, wow.



STEVE:  Four teams.  Well, if you broaden the window a little bit, within a couple months.  And that was Bruce talking about this.



LEO:  Bruce, yeah. 



STEVE:  And there is a security community.  There's conferences.  There's papers being published.  Everybody's reading everything.  And so it's not like there's been no cross-pollination at all.  So it may have been, for example, that last summer somebody did something, or like cache timing, that's now a thing.



LEO:  Well, and Rowhammer, maybe, something like that, yeah.



STEVE:  Exactly.  So there is something kind of in the air where individual researchers will kind of think, huh, I'm going to spend some time thinking about this.  And because they're often researching in a silo within their own group or their own team or their own university or their own corporation, and they're wanting to produce results that are proprietary until they're not, until they're publicly disclosed, especially if it's something like this, where they have to whisper what they found to the parties who are affected, they're not going to be talking about it.  So it is interesting, but not surprising, that suddenly this just kind of all happened.  It was probably set up by, again, just sort of what was in the air in the last few years and eventually bound to happen.



LEO:  Yeah, yeah.



STEVE:  So we'll get to that further in a minute.  Now, you're going to have to pay attention to this one, Leo, because you'll be wanting to talk about this.  This is really strange.  Okay, I don't know how to read between the lines.  This is from last Wednesday, January 3rd, 2018 on support.microsoft.com titled "Important:  Windows Security Updates released January 3rd, 2018, and antivirus software."



And Microsoft wrote:  "Microsoft has identified a compatibility issue with a small number of antivirus software products.  The compatibility issue arises when antivirus applications make unsupported calls into Windows kernel memory."  This is something you and I have been talking about now for a while.  That is, the problem with instability being created by AV which is running in the kernel, and in order to get the hooks it needs, they're doing unofficially supported things.



So Microsoft writes:  "The compatibility issue arises when antivirus applications make unsupported calls into Windows kernel memory.  These calls may cause stop errors, also known as blue screen errors, that make the device unable to boot.  To help prevent stop errors that are caused by incompatible antivirus applications, Microsoft is only offering the Windows security updates that were released on January 3, 2018, to devices that are running antivirus software that is from partners who have confirmed that their software is compatible with the January 2018 Windows operating system security update."



So listen to that.  So Microsoft is only offering the Windows security updates that were released last week to devices that are running antivirus software that is from partners who've confirmed that their software is compatible.  Okay.  What are the mechanisms of that?  How does that work?  Okay, get this.  Believe it or not, there's a new registry entry.  And if you don't do this, you stop getting updates.  That is, if you're running Windows without antivirus which has affirmatively set that registry key to say "Yes, I'm compatible, proceed with the updates," you don't get them.



LEO:  Now, if you don't have antivirus at all, that key - will that matter?



STEVE:  Yes.  Get this.  Down on the second page, if you scroll down from what you're showing on the screen, under "Customers without antivirus," I had to read this, like, three times to make sure I, like, what?  "In cases where customers can't install or run [or choose not to] antivirus software, Microsoft recommends manually setting the registry key as described below in order to receive the January 2018 security updates."  In other words, you just don't get them.  If you don't do anything, and if you've decided I don't want antivirus, then nothing sets that key.



LEO:  But I think Defender counts.  Defender changes the key.  So you would have to be proactively disabling Defender, which is installed by default on Windows 8, 8.1, and 10.



STEVE:  Correct.  Correct.



LEO:  But the weirdest situation is there are a number of antiviruses that fixed the kernel calls, but didn't set the key.  Defender does, though.  So you should be using Defender.  And most people are.



STEVE:  Yeah, and that's what you and I have been saying now for some time is I know that there are a lot of people who have been using some, whatever their AV is, for decades, and they're just sticking with it.  But these kinds of problems occur.  So, yeah.  I just...



LEO:  Yeah, because also we've been talking about is don't use a third-party antivirus because they do things like these unauthorized kernel accesses.



STEVE:  Right.



LEO:  It's not good.



STEVE:  Right.



LEO:  I thought that - I remember when Microsoft came out with Windows 7 they were deprecating this and said don't do this because we're going to prevent you from doing this.  And I thought that they did, but I guess they didn't.  They backed down.



STEVE:  Yes.  There's antitrust issues.  Nobody, like, what happened to third-party firewalls?  Those went the way of the whatever.  They're gone.  There's still third-party AV.  Now, their document does say for Windows 7 and Server 2008 R2, which is the equivalent server version of the Windows 7:



"In a default installation of Windows 7 SP1 or Windows Server 2008 R2 SP1, customers will not have an antivirus application installed by default.  In these situations, Microsoft recommends installing a compatible and supported antivirus application such as Microsoft Security Essentials or a third-party AV application.  The antivirus software must set a registry key as described below in order to receive" - that is, in order for the user to receive - "the January 2018 security updates."  And I should also mention, it's not just this month.  Down in their Q&A they ask themselves the question, how long will Microsoft require setting your registry key?  And basically, yeah, from now on.  That is, if this doesn't happen, if something doesn't set that key, you never get them again.  So...



LEO:  Ai yi yi.



STEVE:  I know.



LEO:  Well, part of it, and I'm reading between this lines in this bulletin, is that if you do have the blue screen because you installed an incompatible program, Microsoft can't guarantee that you can fix it.  It could be permanently screwing - you'd have to reinstall Windows.



STEVE:  Yeah.



LEO:  Terrible.  What a mess.



STEVE:  And in Microsoft's defense, we know this is a pain for Microsoft.  That is, third-party AV, anything that is allowed into the kernel that starts poking around and hooking things that it's not supposed to and using undocumented calls, clearly what happened is, with this month's release, Microsoft changed something.  Well, they're allowed to change their own kernel because it's theirs.  But antivirus that wasn't verified against these patches - and how could it be if Microsoft just said, oh, here's a patch - I'm sure that that change collapsed some things, and Microsoft said, oh, crap.  We can't push this out to users who have incompatible AV.  They'll get blue screens.  So they created the registry key to create a trapdoor that would prevent the update from installing itself if this key did not exist or was not set properly.



So, again, this is not Microsoft's fault, but it all does sort of create a little pressure against third-party AV.  I mean, there's probably enough pressure against third-party AV already that this doesn't really, in the grand scheme of things, make much difference in terms of being more pressure.  But again, as we have been saying, it's another reason just to use Security Essentials or Defender or whatever, just let it take care of you.  So anyway, an interesting bit of information that I wanted to make sure our listeners knew about in case somebody had shut down their Microsoft AV in Windows 10 or didn't have Security Essentials or anything else from Microsoft installed in Windows 7 and is kind of scratching their head, hey, where's my update?  Especially this week, when everybody's wanting updates like crazy important.



LEO:  Well, we'll get into another reason why you may not get the update in a little bit, too.



STEVE:  Yeah, that's right.  So AMD.  Anyway, so somebody sent me a tweet from Jason Kint, probably Simon Zerafa.  So thank you, Simon, if it was you.  I didn't make a note of who forwarded it to me, so I'm sorry if it was somebody else not Simon, but thank you anyway.  So Jason tweeted:  "Seriously.  Drop Chrome.  Download latest version of Firefox called Quantum and turn on Tracking Protection in the settings.  Tracking protection will also help mitigate ad fraud.  And," Jason writes, "it's Google's Achilles heel."



So I thought, okay, what?  So I did some digging, and this is a little interesting.  So first of all, that chart you're showing needs to be treated with some skepticism.  It's Mozilla's chart.  However, if it's to be believed, it shows that Chrome average page load time per browser was 7.7 seconds.  Chrome Incognito didn't change, 7.7.  Firefox Quantum normal mode was a little touch faster at 7.3 seconds.  But turn on Private Browsing with Firefox Quantum, and it drops by more than 50%, to less than 50%, to 3.2 seconds.



I also wanted to take a moment to note that I heard Andy in last week's MacBreak Weekly give a shout-out, surprisingly.  He was jumping up and down.  And I think I've heard, I think, other TWiT hosts or people on the network in general, I mean, it's more than just me noting, holy crap, Quantum is really fast.  In fact, you, Leo, I think, had noted that the latest Firefox browser, Quantum, was a so-called "quantum leap" forward in performance.  But I got a kick out of hearing Andy saying, wow, he's running them side by side, and his measure is where does he generally end up opening new tabs or something.  He had sort of a heuristic.



LEO:  But you get even more benefit if you turn on tracking protection.  That's a huge, apparently, difference.



STEVE:  Yes.  And so what Mozilla wrote was that "Most browser performance benchmarks focus on the use of a regular browsing mode.  But what about private browsing?  Given that private browsing use is so common" - and they linked to a DuckDuckGo Private Browsing PDF, which has some interesting stats in it, I mean, it's like for people who - there are a lot of people who've never heard of it, don't know what it is, so obviously never use it.  But there are people who know about it and from time to time - typically the example was when doing a search for something embarrassing, they would turn on private browsing in order to not leave footprints behind.  But anyway, apparently it is like maybe 50% of users use it with some regularity who know that it exists.  And of course there's a big chunk of people who don't know.



Anyway, Mozilla says, "We wanted to see how Firefox's Private Browsing compared with Chrome's Incognito when it came to page load time, that time between a click and a page being fully loaded on the screen."  Now, I want to mention that the contention is that tracking script is slowing things down.  But tracking script may not matter visually.  So it's one thing to say, oh, look at our instrumentation that showed how long it took for everything the browser needed to fetch to finally get fetched.  But that may not represent the user's experience.



So what I want to do is I want to plant the seed in our listener's ear that, well, see how your own mileage is with this.  Try it yourself.  So what their results were across the top 200 news websites tested, the average page load for Firefox Quantum Private Browsing, as I mentioned, was 3.2.  Everybody else was like 7.7.  So like 2.4 times faster than Chrome in Incognito mode.  And so the primary reason for the dramatic difference, they write, is that Mozilla's private browsing mode automatically activates tracking protection; whereas Chrome's Incognito mode does not.



So first of all, okay, that's of interest because you'd sort of think that, if you're in Private Browsing or Incognito, you'd like to have tracking protection off.  Well, Chrome keeps it going, and by default Mozilla's Firefox Quantum turns it off.  The takeaway here, though, is that anyone using Firefox Quantum can change that setting.  It is possible to flip a switch.  You go to Privacy and Security, scroll down to find the Tracking Protection section, and you can change it from Tracking Protection During Private Browsing or Always.



So what they're asserting is that you could probably, with Firefox Quantum, obtain the benefit of that speed.  And again, instrumentation measurement versus how it feels may be two different things.  That is, if scripts continue to load in the background that are only for tracking, but they don't slow down your browsing experience, then okay, it's sort of folded into you looking at the page and figuring out what's going on, and it may not matter.  But I wanted to let our listeners know that tracking protection can be set to always and certainly lower your bandwidth consumption, apparently significantly.  Maybe you'll see an increase in performance.



And Mozilla did note that, when enabling this for always, keep in mind that tracking protection may block social like buttons, commenting tools, and some cross-site video content.  So you may find a bit of a compatibility problem.  And probably, if you're already running with uBlock Origin enabled by default, it's probably already doing a lot of this speedup.  And there you have the ability, as we've discussed, of disabling it on a per-site basis; or if something seems broken on the page, you can try turning it off and maybe things will go better.



But what we have seen is that, unfortunately, the tracking - and think about what that means in terms of speed.  That means that well more than half of a page load time is now blocked by tracking protection, which says this stuff that we don't see, but which is increasingly JavaScript and busy doing stuff, not always for our benefit, is now taking up a substantial percentage of the pages of the top 200 news sites.  And also that's another point.  It might be that that represents top 200 news sites could be a bit of a skew from where we normally go.  Just there maybe more, I mean, news sites are generally trying to keep themselves alive by generating revenue.



LEO:  Any site with an ad, almost every site with an ad, the ads will be - it's kind of effectively an adblocker.  So you should be aware of that, that if you're going to a site you like, you are disabling their ability to monetize you entirely, not just with ads, but with tracking, like Google Analytics.  They can't even tell you're there.  So I understand people's privacy concerns.  But don't think that using it always is anything less than an adblocker because it's effectively an adblocker.  You are telling a site that you visit, whether it's Engadget, the Verge, or TWiT, we don't want you to monetize us.



STEVE:  Actually, it's a little better than that because it uses a curated list.  And if a site does obey the DNT, the Do Not Track, then it's not on that list.



LEO:  Yeah, but if you have ads, you're going to - like we, I mean, we obey Do Not Track.  But we don't personally, but because we have ads, it will block the ads on our site.



STEVE:  Ah, okay.



LEO:  So unless they have first-party ads or somehow they're doing an ad server that doesn't, you know, we even run our own ad server so that people don't, I mean, we do everything we can to protect people's privacy, and you still block the ads on our site.



STEVE:  Right, right.



LEO:  You're basically saying, no, I don't want to give you any money.  So it's something to be aware of.



STEVE:  Well, and as we've discussed, sites are increasingly detecting that and saying, hey, you know, we need to be able to show you ads in order to support ourselves.  Please consider allowing the ads to come through so we can make some money and  stay on the air.



LEO:  Right.  And then maybe people will disable their adblocker.  But if they have tracking protection on, they're not changing it.  I think that's part of the problem I have is you're doing adblocking without knowing it.



STEVE:  Yeah, good point.  So for quite some time, it turns out that Western Digital, and apparently D-Link also, but primarily Western Digital My Cloud drives had a hard-coded - it's hard to even believe this - a hard-coded backdoor.  It got a lot of coverage in the last week.  A researcher, James Bercegay, with GulfTech Research and Development, discovered and reported the flaws to WD, to Western Digital, back last summer, June of 2017.  He then waited patiently for Western Digital to release firmware updates.



He took a close look at several of the My Cloud drives, 10 of them, by the way.  I think there were two that weren't affected.  Ten were.  And he found a bunch of problems.  They allowed unrestricted file upload so that, for example, a PHP file which was found on the WD's My Cloud's built-in web server allowed an attacker to upload files to the device.  He was able to use the flaw to upload web shells to the device, which in turn granted him control over it.  There was, as I mentioned, a hard-coded backdoor which was present in every single one of these My Cloud drives.  And the reason that there's some obviously D-Link connection is that the admin username was mydlinkBRionyg, and the password was abc12345cba.



So this is sitting in all of the drives.  The point of them is to be on the Internet, exposed, so that you're able to access them.  And they all have a hard-coded backdoor which would have allowed anyone who discovered it, and who knows who did, until this was fixed, to have full access to your drive.  And there was a cross-site request forgery bug that could be exploited to execute pranks and rogue commands.  For example, you could change the device's interface language so that, when you brought the website up, it would be in a language you didn't speak and had a hard time even changing back because you couldn't find the button to do that.



So the middle of last month, mid-December, an independent and highly detailed disclosure of the vulnerabilities was publicly posted by somebody else.  And so the good news is that Western Digital had fixed this in November, so the month before.  So the takeaway for our listeners is you want to make sure that you are at firmware 2.30.174 or later.  Or just, if you're a WD My Cloud drive user, definitely update your firmware, your drive's firmware to the latest, which will have all of those bugs, including the hard-coded backdoor, removed because you don't want your WD drive and all of its contents probably globally accessible and able to execute root, you know, bad guys.



Oh, and these are wormable.  So this was a wormable set of bugs that would have allowed, had this gotten loose, it would have been part of a Mirai botnet in a few weeks because they would have added it to the code, and the drives would have been out scanning for each other and becoming compromised.  So not good.  Oh, and a Metasploit module now exists, making this easy to do. I looked at the documentation through Metasploit, and there's  nothing particularly new or newsworthy that I didn't already talk about.  Basically they're just covering the same sort of stuff.



So how not to ever learn the lesson?  Announce yet another WiFi specification developed in secret.  And boy, are these guys proprietary.  So this is - get this.  Wi-Fi Alliance(R), and then we have the "circle R" to remind us that that's a registered mark.



LEO:  Hell, yeah. 



STEVE:  Yup, Wi-Fi Alliance(R) introduces security enhancements.  And their announcement, and this was yesterday at CES, new WiFi, oh, "circle R," that's our trade, we also own that, registered trademark.  Security features available in 2018.  Now, okay.  That's good, except - so dateline Las Vegas, Nevada, January 8, 2018:  "Wi-Fi Alliance(R) introduces enhancements and new features for Wi-Fi Protected Access(R)" - oops, "circle R," we own that phrase also.  We don't have good security; but. boy, we've got our trademark patent attorneys.  They've locked down all of these phrases that you have to give them credit for - "the essential family of Wi-Fi CERTIFIED(TM)" - that's all caps, oh, and that's a TM, trademark on that.



LEO:  Oh, my god.



STEVE:  "...security technologies for more than a decade."  Yes, and we're suffering through it for more than a decade.  "Wi-Fi Alliance is launching configuration, authentication, and encryption enhancements" - because, you know, we haven't got it right yet.  That's me.  They didn't say that - "across its portfolio to ensure Wi-Fi CERTIFIED(TM) devices continue to implement state-of-the-art security protections."  Somehow they didn't trademark that phrase.  "WPA2(TM) provides reliable security used in billions of WiFi(R)" - we own that phrase - "devices every day and will continue to be developed" - anyway, it goes on like that.  I won't bore our listeners.



Anyway, they've trademarked and circle R'd every possible phrase that they want to make sure they get credit for.  Unfortunately, they're still struggling to get the security correct, as I said.  We went through WEP, WPA, WPA2, then we had the single button configuration where it turns out you didn't have to guess all, remember all eight digits at all.  The eighth one was a check digit.  You could guess them in two separate sets of four and then three, totally shredding that security.  So these guys, I mean, they just cannot get it right.  However, there are, you know, they're going to keep trying.



And what I wanted, the useful bit of information here is that there will be a WPA3.  They're being mum about what it will have.  That is to say, no open spec.  No ability for actual industry academics and non-paying members of the Wi-Fi Alliance(R), to see what they're doing.  But we'll have a chance to take a look at it after everybody has already got it deployed; and, with any luck, maybe they will have made fewer mistakes.  So they say four new capabilities for personal and enterprise WiFi networks will emerge in 2018 as part of - wait for it - Wi-Fi CERTIFIED all caps WPA3(TM).



Two of the features will deliver, they are saying, robust protections - we don't know that, of course, until somebody else sees them - even when the users choose passwords that fall short of typical complexity recommendations.  So, oh, maybe they're implementing some sort of password-based key derivation function, which would be handy.  Everybody else has that now.  And, they say, we'll simplify the process of configuring security for devices that have limited or no display interface.  Okay, sounds good.  IoT stuff, light bulbs and so forth.  Another feature, they write, will strengthen user privacy in open networks through individualized data encryption.



Now, that sounds great.  Let's hope they get it right.  Some speculation that I found suggests that maybe what they will be implementing is a Diffie-Hellman key agreement.  We've talked about how that works in the past, where anyone can observe the two endpoints sending stuff back and forth to mutually arrive at a key and, even while seeing all of the back-and-forth traffic, still be unable to know what the key is. 



So the presumption is that maybe there will be some sort of a public key agreement protocol like Diffie-Hellman which will then be used to encrypt the standard WPA encryption, which would allow what's known as "opportunistic encryption," meaning that, if you have WPA3 at each end, and that'll take some length of time for access points to come up to speed, for example, in coffee shops and so forth and restaurants and airports.  But the idea would be that no longer would your non-password-based WiFi connections be subject to simple eavesdropping.  And that's a great step forward.  So again, if they didn't screw it up in implementation and definition, then yay, that would be good.



And finally, they say, a 192-bit security suite.  I guess for whatever reason, probably performance, maybe, they're not wanting to jump all the way to 256 where everybody else is.  But it's better than 128.  So they're saying that that's aligned with the Commercial National Security Algorithm Suite from the Committee on National Security Systems.  And I don't know who any of those people are.  We've never talked about them before, the CNSA?  Oops, it's got NSA in its name.  Well, okay.  Will further protect WiFi networks with, they're saying, with higher security requirements such as government, defense, and industrial.  And maybe this fifth shot will be better than the previous four.  Let's hope.



So anyway, they've got their trademark all over everything.  WPA3 announced yesterday at CES, will happen apparently sometime this year, and sounds like it's got some good new features.  So good for that.  My only wish is that it were being developed the way other open systems are, where you freely invite the community of people who know a lot to take a look at it because, in the past when that community of people who know a lot have looked at every single thing this Wi-Fi Alliance has ever done, they've found serious glaring problems with it.  But again, that's not the way they want to play the game.



Also, and this is my last piece of news before we get to some miscellany stuff and then talk about the Speculation Meltdown, is that the Let's Encrypt gang are celebrating their success in 2017 and have talked about their plans for 2018.  In their "Looking forward to 2018" posting yesterday, or actually it was dated - oh, last month.  They said:  "Let's Encrypt had a great year in 2017.  We more than doubled the number of active unexpired certificates we service to 46 million.  We just about tripled the number of unique domains we service to 61 million.  And we did it all while maintaining a stellar security and compliance track record."  And we'll talk about that in a minute.



"Most," they write, "importantly, though, the web went from 46% encrypted page loads to 67%, according to statistics from Mozilla.  A gain of 21 percentage points in a single year," they write, "incredible."  And I agree.  That's great.  They said:  "We're proud to have contributed to that, and we'd like to thank all the other people and organizations who also worked hard to create a more secure and privacy-respecting web."



Then they finish with a paragraph:  "While we're proud of what we accomplished in 2017, we are spending most of the final quarter of the year" - that is, 2017 - "looking forward rather than back.  As we wrap up our own planning process for 2018, we'd like to share some of our plans with you, including both the things we're excited about and the challenges we face."  



Anyway, so in summary, where they go with this post is that they are going to be giving us a next generation of their ACME protocol, which will support some additional features.  They will, during 2018, be allowing the use of wildcard certificates for the first time.  So you'll be able to get a *.domain.com or whatever.  Which will then allow that single cert to be used on multi-home systems within that domain or on multiple machines behind a network concentrator, whatever, rather than - and each with a different domain name, rather than requiring that each of those machines obtain its own certificate.  And they're going to be adding an elliptic curve, an ECDSA elliptic curve root to their root that will then allow them to be issuing elliptic curve certificates, which have lower computational overhead, higher performance.  And that's good looking forward.



All of what they wrote is true, but it is also true that an inevitable consequence of the automation of DV, that is, domain validation certificates, which is what they're issuing, is fraud.  And while it's true that a greater percentage of the web is encrypted, and that's good, it is also unfortunately true that a phenomenal number - and we've discussed this in the past, a phenomenal number - of Let's Encrypt-issued certificates are fraudulent.  I mean, they are being used for fraudulent purposes.



And again, there's nothing to have prevented that from happening with domain validation certs before Let's Encrypt.  But the automation of this process has really taken the lid off.  And the numbers of fraudulent certs being used essentially to spoof security and get the closed padlock and better treatment in the browser URL to seduce people into believing they are where they're not is phenomenal.  So, while, yes, it's good that more of our connections are encrypted, it was inevitable, I think, that we were going to see a significant jump in the abuse of this kind of automated cert issuance, and indeed we have.



And this is a note I got middle of December, last month on the 19th, that caught my eye.  I mentioned it tangentially last week, I think, or the week before.  The subject was "Thanks for giving me $728!!!!" with four exclamation points.



LEO:  Oh, yeah, you mentioned this, from the drip, the Bitcoin Faucet.



STEVE:  Yes, exactly.  Someone named David L. in Utah wrote to both of us.  He said:  "Steve and Leo, I've been a Security Now! listener since the single digits" - and wow, okay, that's the beginning.  So he says:  "But I did not buy a copy of SpinRite until my computer wouldn't boot because of some error."  Then he wrote:  "Whatever it was, SpinRite fixed it.  Thanks, Steve."  And then the reason that I had brought this up before was he said:  "Also thanks for $728.  After your first episode about Bitcoin in 2009, I decided to download it and fiddled around for a week.  My computer was too weak to mine, but I did go to a website called the Bitcoin Faucet, which gave me five bitcoin cents for free.  I finally found my wallet in my backup drive" - which maybe thanks to SpinRite is still working - "and I just sold them and netted $728."



LEO:  Nice.



STEVE:  But he says:  "By the way, it took my old computer two weeks to process the 157GB block chain, which used to only be 250MB when I got my five cents."



LEO:  Yeah, exactly.



STEVE:  "Anyway, I must say it pays to listen to Security Now!."



LEO:  Yes, it does.  Very nice.  



STEVE:  So very cool.



LEO:  Happy ending.



STEVE:  So I have a couple of closing-the-loop bits, feedback from our listeners.  And then we will take our last break and talk about the Speculation Meltdown.  Mike Gatzke, I guess - sorry, Mike, if I mangled your last name.  He wrote, or tweeted.  He said:  "I've started using Firefox with the Tree Style Tab add-on.  Is there a way for Firefox to remember the tabs I have open?"



So to Mike and anybody else, absolutely.  I use an add-on called Session Manager which is fabulous.  It was written by the Mozilla developers, so they've nailed it.  Because, you know, they have to know how Firefox works.  And it is far more than just session management.  That is, you're able to take sets of tabs and save them.  You can create named sessions.  You can export sessions, like email them to yourself and get them somewhere else.  You can do all kinds of things with it.  So anyway, I highly recommend it.  I've been using it for years.  Session Manager is the add-on.



And then we will talk - actually now we should talk about it, probably, because I'm going to talk more about theory than practice when we talk about the Speculation nightmare.  But this started with, well, of course a lot of people have problems with what began happening with their computers.  The first I saw of it was Richard J. Wilcox, who tweeted to me and to Simon Zerafa, he said:  "I had the same experience as reported in this article."  And he quotes a SecurityAffairs.co posting.  He said:  "The Microsoft update bricked" - not quite the right term in this case - "my AMD Athlon 64 X2 6000+ computer."



And so this was an article, one of many, of AMD users - and you referred to this at the top of the show, Leo - who were having problems with the AMD Athlon.  And also the Semprons were doing the same thing.  Woody Leonard got in on this dialogue in Twitter.  And he tweeted, he says:  "I'm getting reports of a lot of AMD Win7 machines blue screening, plus a handful of others - Intel, Win10, various mixtures.  No common theme as yet except Win7 AMD Athlons really took a hit."  And of course we know since then that Microsoft has officially stopped updating AMD and blamed AMD for this.  I don't know if you saw that, Leo.



LEO:  Oh, I didn't, no.



STEVE:  Yeah, Microsoft has an official statement saying that the documentation which AMD provided to Microsoft has some errors, which is the reason that this bricking was occurring.  And again, I've misused the term.  What actually happens is that the reboot after the update will blue screen.  Then Windows will recognize it had a problem.  It will try it again.  And after, I think it's two, might be three, then it will do a self-rollback to the version that worked.  But apparently that's a continuous loop.



There was actually one funny posting that I got a kick out of.  It was posted at the Microsoft forum.  Someone wrote:  "I have older AMD Athlon 64 X2 6000+, Asus motherboard.  After installation of" - and then he quoted the Knowledge Base number which is the one that we're all getting, the one that is causing problems - "the system doesn't boot.  It only shows the Windows logo, without animation, and nothing more.  After several failed boots, it does a rollback.  Then it shows error" - and he shows the hex error 0x800f0845.



"Unfortunately," he writes, "it seems it's not easy to disable the automatic updates without group policy edit tweaks, so it tries installing and rolling back the update over and over."  He says:  "Sfc/scannow shows no problem.  In-place upgrade also doesn't seem to help."  He says:  "I can try full reinstall, but I doubt it will change anything.  It seems like the update is binary incompatible with my old CPU."  Then here's what I got a kick out of.  He says:  "I understand that making the machine unbootable is the best protection from remote exploitation.  But I would rather have the OS working."



LEO:  It is the best, yeah.



STEVE:  Exactly.  If you can't get that machine to boot, nobody's going to get in there.  So anyway, it's been a mess.  I am sure Microsoft and AMD are working together to figure out what it was that Microsoft didn't understand or that AMD posted incorrectly or provided incorrectly in order to fix this.  And then I don't know what Microsoft will do.  I guess if you roll back maybe, I don't know, I'm sure Microsoft must have some path through un-blue screening these systems that keep trying to reload the same thing over and over.  Maybe it will check to see if the patch has been changed.  If Microsoft changes it, then it'll redownload one that no longer has the problem.  But, yikes, a bit of a nightmare for those people.



Johnny Brian said:  "I have an older Mac Pro as my main computer.  The latest OS it can run is 10.11.6.  It's not clear if Apple will patch this OS for Meltdown."  And then he links to a support.apple.com posting.  "Should I be concerned?  Replacement is not an option.  Should I disconnect it from the Internet?"  And, you know, that's a tough call.



I'm of the opinion that, first of all, the exploitation now may not occur if Meltdown has been mitigated.  It's really only a problem, well, the performance overhead will probably not affect most typical users.  It's largely very active multitasking, multiprocess systems where there's a lot of these ring transitions going on.  I guess I would take a wait-and-see, both with what Apple chooses to do and whether we see some chip support mitigations.  There is the possibility of some chip support mitigation, although it may not, and early snapshots suggest they will not, address Meltdown, but will address the other Speculation problems.



So the only thing I could say, Johnny, is to wait and see.  Certainly many people will be in your situation, and it's hard to imagine that Apple won't offer some sort of mitigation.  So I don't know.



LEO:  I'm sure Apple will mitigate all of its devices.  And then there's always hope with the processor manufacturer; right?  Because Intel and AMD and Qualcomm and everybody say they're going to do their own mitigations.



STEVE:  It's complicated.  And we will be talking about that next.  And it's not looking, I mean, okay, so anyway, it's complicated.  And so there's no easy way to summarize it, so we'll talk about that in a second.



LEO:  Got it.



STEVE:  However, you'll remember our Picture of the Week last week, Leo, that we got such a kick out of, which was the fan blowing on the switching matrix with the duct...



LEO:  But we couldn't read the fine print.



STEVE:  We couldn't read the fine print.  Well, leave it to one of our listeners, who found from TheDailyWTF.com the so-called "Sophisticated Cooling Apparatus."  By going back to the original picture and blowing it up, the fine print has been made legible.  Because there's an asterisk in the warning that all UKTVs will be taken off the air.  And the fine print says:  "*In a matter of minutes, probably."  So that answers the mystery.  It won't take long for this thing to overheat and cause a meltdown.



Ian Beckett, who is a frequent correspondent, says:  "I'm wondering how vulnerable ARM controllers in SSDs are to Spectre, Meltdown, and the like?"  So I would say not at all.  An ARM controller in an SSD isn't going to be running code that you download from the Internet.  I mean, that's really the danger is that somehow malicious code will be running in a process in a multiprocess system.  Mostly cloud systems, I think, are in danger.  But ARM controllers are not.  And also...



LEO:  They're probably not running user mode stuff at all, are they?  I mean...



STEVE:  Correct.  They're probably not doing that.  Also, for example, the Raspberry Pi is ARM based; but it, too, is not vulnerable because it's using an ARM license and ARM architecture that lacks Speculation.



LEO:  Same with the Apple Watch.



STEVE:  Exactly.  And I would imagine the Raspberry Pi chose that because they didn't need that level of crazy performance, and it's a much cheaper license and a much smaller die size, thus much less expensive to fabricate an ARM that uses a much less expensive chip.  So we're fine for embedded applications which are certainly going to be using inexpensive controllers and not wide open the way cloud-based Azure and AWS and the like, Google computing platforms are.



Andres Vidal said:  "I just read that MongoDB noticed a 10 to 15% impact on HVM hypervisors from patches applied to their AWS infrastructure.  That seems really high!"  And as I've noted, that doesn't surprise me at all.  What I think we're going to see is an initial reaction that has a stronger impact.  And actually I mentioned this, I guess on Saturday on The New Screen Savers when I was on with you, Leo, at the beginning of the show, is that I think there will be an initial sort of panic reaction mitigation and that, as we get then chip-level solution, although it's not clear that we're going to get that - well, again, it's complicated.



So I don't want to say something that is wrong.  I want to cover it correctly in a minute.  But again, that level of impact does not surprise me because, if it's necessary to flush the cache, and the application, like a big database application which may be crossing back and forth and doing a lot of low-level or disk-level I/O, that's going to have - that's the type of application that is going to see a larger end performance hit.



Oh, and I just wanted to put this in here.  Trevor Welch asks why Security Now! is not on Spotify, but other TWiT shows are.  And I'm thinking maybe he just didn't see it.  Or it would surprise me if it wasn't.



LEO:  I'll check.  Yeah, I mean, it should be.



STEVE:  Yeah.  Okay.  So anyone who went back and reread our series on how processors work would have sort of encountered some of this because we've talked about the phenomenal lengths to which modern processors now go in order to squeeze every, I mean, truly phenomenal, squeeze every last possible bit of performance out of their design in order to give us the kind of performance that we're used to getting now.  Essentially, every possible stop has been explored and removed.  This notion of Speculation, we've already pretty much talked about the Meltdown problem, that the Meltdown problem is a way that was found of leveraging the caching of virtual memory pages which are inherently shared globally because there really hadn't been a reason not to.



Now, I should mention that there is a feature in Intel's recent processors, PCID.  And I referred to this last week.  PCID is a page context identifier which allows, I mean, it basically solves the Meltdown problem.  If you have, and if your OS is using the PCID, it is able to tag the pages with the identifier for the process, which provides the isolation which we need on a process versus OS basis without requiring the pages to be flushed.  So the PCID support was added, I want to say 4.16 version of the Linux kernel.  It is there.  It has been ignored because nobody saw a particular need for it.



Well, that changed last week.  And the good news is, when an operating system supports this PCID, and when your chipset supports it, that's another part of the key is that I don't remember now which family PCID appeared in, but it's been around for maybe a decade, but not before.  So this is something that Intel added.  Maybe it was prescient, or it's just sort of more thorough.  To be able to tag cached pages with the process ID is a good thing.  So the kernel needs to support it.  It exists in Linux.  It has not been used until now.  It will now be used and will prevent this dramatic performance slowdown that we've seen.



So that's why this is a little bit of a nuanced response.  That huge drop in performance that we showed on this week's Picture of the Week was a system that implemented the cache flushing mitigation for Meltdown, not PCID.  It probably wasn't available.  We don't know what OS they're running on.  So that's something that I'm sure Windows will be getting soon, if it doesn't already have it.  Linux has it.  It hasn't had it enabled.  So I'm sure it will.  And it does require that the chipset you're running on support this PCID capability, and really old ones don't.  But it has existed in Intel chips for some time.  So that's the Meltdown, and this ability to use subtle timing in order to sense whether something is in the shared cache or not.



Now, mentioning timing is useful because all that has to be done is that access to high-resolution timing would solve this problem, that is, limiting access to high-resolution timing.  And I mentioned this also last week when we were first talking about this.  It's not really clear that user mode, userland code, that is, non-OS kernel stuff, in a production environment has to have access to the high-resolution timer.



There is a - it's called the RDTSC instruction on the Intel, Read Time-Stamp Counter.  And, for example, I use it in SQRL as just one more source of entropy, just another piece of data which is unpredictable, and no external attacker, no one remote can have any idea how many cycles at whatever, 3.some gigahertz, my particular instance or any of the SQRL client users' instance of their processor has executed since it was booted.  It'll never be the same twice.  It'll never be the same even if you tried to make it the same, just due to all the other things going on, things like where in its rotation the hard disk was or where the hard disk's read head was as it was seeking.  I mean, like there's so much uncertainty in the system that having clock cycle level granularity is exquisitely fine.



Well, that's useful as one of many sources of entropy.  But it's that level of precision timing, at sub-billionth of a second that the clock is running at, it's having access to that that allows these attacks to occur, that allows software to discern with that exquisite granularity whether some data came from the local immediate access cache or even has to be pulled out of main memory, which is much slower.  And so if the instruction stalls in trying to fetch something, and then immediately after executing that instruction it reads the Time-Stamp Counter again, it's able to discern that it stalled.  It's able to sense that time passed while it was not executing while the processor went out to get the data that was needed by the immediately preceding instruction.



So there's an argument to be made that a solution to this would be to fuzz the resolution of the timestamp.  That is, it's not clear that there's any useful real-world need for it.  Maybe the OS needs it, but it's not even clear to me that it needs it at that level of granularity.  It's there because it's in the chip.  It's been there for a long time, since one of the early Pentiums, I remember, that appeared.  SpinRite has used it.  When I was doing the sector re-interleaving in SpinRite 1, the only reliable source of time that was high granularity was this timestamp that has been there from the dawn of Intel.  So it's still there.  And there are many other so-called "performance counters" which Intel makes available in their silicon to programs that want to execute it.



Now, what's interesting is the Read Time-Stamp Counter can be marked as privileged so that software cannot access it.  On the other hand, unfortunately, there's no way to take it away from software now that presumes its presence.  However, in a VM environment where you have virtualized hardware, the timestamp could be and is typically virtualized, which would allow it to be fuzzed.  It would allow it to be made less accurate.  And all you had to do was just add some uncertainty to it so that software cannot actually determine with the required level of certainty how long an instruction took to execute.  And then this whole class of recently discovered problems disappears because the only way they're able to operate is to sense with absolute precision how long something took.



And again, it's not that the instruction didn't execute.  It's just it took a tiny, tiny bit longer in order for the system to fetch data out of main memory, in the case of Meltdown.  So that's one potentially global fix to all of these problems.  But this PCID is arguably the proper way to solve the problem of one process being able to sense the contents of a cache from another process.  Again, five years ago, 10 years ago, it was like, eh, that wasn't a problem.  Today it's a showstopper which everybody is reacting to.



So I think that the short-term sort of kneejerk reaction of flushing the cache whenever you go back out of the OS to the user, or maybe even cross-user, is going to be reduced once we get access and once operating systems are actively supporting this PCID flag on the caches, which solves that.  But that's only one of the several problems.



The other whole class of problem is Speculation.  As I started to say before, Speculation is the notion of executing when you've got execution capability that is not in use.  So, for example, we've got multiple cores and hyperthreading technology on a state-of-the-art chip.  Whether it's Intel, AMD, ARM doesn't matter.  Say that you have - and I'll just use some simple algebraic equations to give an example.  You have A plus B equals C where imagine that these are registers, and D plus E equals F.  In those two equations, the outcome C of the A plus B equals C is not used by the next computation, D plus E equals F.



And the way processor engineers discuss this, they say there's no dependency of the outcome of the first by the second.  So if those two simple math operations occur next to each other on a modern processor, it will notice that.  It's smart enough, it's been engineered by state-of-the-art processor designers to see that it - and imagine that B, you know, A plus B equals C, imagine that B is off chip at the moment.  That is, it's in main memory.  And so it's going to take a while for it to fetch the value of B in order to compute A plus B equals C.  It looks ahead, and it goes, oh, but look, D plus E equals F, and I've got all of that data in my cache.  And I don't need to wait to find out what C is because nobody cares in this next math operation.



So the processor will do what's known as "out-of-order execution."  It will leap ahead.  It'll start asking for B to be hauled in from main memory.  Meanwhile, it'll go ahead.  And as long as there is no dependency, it will move forward, saying, well, nobody seems to be asking for C.  So it'll just keep on executing as far as it can and as far as the architecture will allow it to get ahead, moving ahead.  So out-of-order execution is one form of this.  The other is branch prediction.  If all code could only run in a straight line, it would be very limited in what it could do.  Not completely limited.  There are many instances where, for example, high performance graphics processing does a whole bunch of stuff all at once that doesn't involve lots of branching.  So you can get work done without branching, but ultimately you need to make some decisions.



So here again we have a situation where somebody, a sufficiently motivated engineer could design, and they have designed, a system where the code is coming along, and a conditional branch is encountered, meaning that the decision whether to take the left fork or the right fork in the code is dependent upon the value of something which is not yet known, like that C.  Imagine that it says, well, if C equals zero, then go left.  And if C is not equal to zero, go right.  But we're still waiting to get that value of B in from main memory to add A to it to find out what C is before we know whether C is going to be zero or not.



So what we could do is what's called a "processor stall."  We could just say, shoot, we don't know C because we haven't got B yet.  But again, in this interest of squeezing with any cost of effort the absolute maximum technology out possible, the engineers say, well, instead of having to decide which path to take, let's take them both.  And it's like, what?  Yes.  If we've got, again, excess computing resource, let's just go down both paths, the idea being...



LEO:  Oh, so it's not like branch prediction.  They get both.



STEVE:  Correct.



LEO:  In branch prediction you pick one, and then there's a penalty if you pick the wrong one.



STEVE:  Yes.  Now, there is branch prediction, too.  I mean, so there's that also.  But there's also, instead of taking - and so that's like taking the road more traveled rather than road less traveled.  But you can also just say let's go down both.  And again...



LEO:  Amazing.



STEVE:  ...the complexity, yes, it's just, like, unbelievable how complicated the processors are today because they have to keep track of everything they're doing down both paths and make a checkpoint of, like, okay, we're going to go down both paths, but we have to make sure we don't screw anything up because we have to be able to unwind any changes which end up being made by the path we don't end up taking, once we finally get the result of the earlier math operation to tell us which branch we should have taken.



And so it's like mindboggling.  But this is what state-of-the-art processors are doing.  And this is all collectively called "Speculation."  You mentioned branch prediction, the idea being - and I referred to this, I guess when we were talking about this on Saturday, Leo, the idea being that when you are - in general caching works because what has been known is that code generally runs in patterns, and data which is in use tends to get used again.  Well, the instructions that are being executed also run in patterns.



And so branch prediction is another form of caching.  It notes that, gee, the code took this branch last time and the time before and the time before that.  And so, if it can't, if it doesn't have enough resources to go down both paths because other aspects are using the available resources, then it will settle for guessing the path likely taken based on what's happened recently.



And so it'll go down the path that has been taken more recently, again still making a checkpoint in case it has guessed wrong because that is eventually going to happen; and it will go as far as it can based on how deep the Speculation resources are in the chip.  And when the result of C is finally known, it checks, and it says, oh, yeah, we're going down there again.  Then it commits, or in processor design terms it retires, it's called "retiring an instruction," meaning finally saying, okay, fine, we're done with you.  It will retire and commit all of those things it was kind of holding its breath about, waiting for the answer from the earlier question.



So what we have is we have, in all of these things, we have caching.  We have short-term memory of which branch, which fork in the road is likely to be taken.  And we have essentially a processor which is maintaining a huge amount of state, a huge amount of information of recent activity.  That is, it doesn't know what happened last week.  It only knows about what happened milliseconds ago as code is zipping around and zooming around in there.  But that's enough to give it a huge performance advantage if it's able to guess right, or if it's able to speculate down multiple paths and hold its breath, hold all of these possible outcomes, sort of like the multiverse, where it's like, oh, if a decision is being made, well, that causes the universe to split down both paths, one where it happened and one where it didn't.



Well, this is happening inside of our chips now.  And where decisions are being made later which essentially affect the future, it's able to hold all outcomes and then commit to one that finally occurred, essentially leapfrog, jump forward, as long as it has sufficient resources.  But the secret here is the problem because doing this leaves footprints.  Doing this alters the state of the processor.  And today those things, those alterations have global effect.  That is, there is no flushing of all of this state when you change processes or change tasks because nobody has really seen the need for it.



Now, there are instructions that have been around for a long time in the Intel instruction set which have not been much used.  There's one called LFENCE which explicitly allows speculation to be blocked.  And there are various reasons why you want to sort of like force the chip into a known state because, well, for multitasking and fine-tuned optimizations, there are places where you need to know what's going on, and so you'll sort of want to rein in this speculation.  So this LFENCE instruction can be used at critical branch points to block speculation and prevent these kinds of speculation-based exploits.



And in all of the reading that I did over the last few days, I ran across somebody who - and I think it was from Intel, so I was a little skeptical because I remembered that - I sensed that they had a little bit of a bias in this.  But the sense was that in the Linux kernel there were only a few places where there was really this problem where this LFENCE instruction could be inserted.  There's been mention of compiler changes which would cause compilers to start inserting these.



Now, you want to do them sparingly because it kills your performance.  That is, we are relying on this level of speculation in order to have the performance our systems have today.  And so if we start blocking speculative execution at every branch, suddenly something we're relying upon is gone, and we'll feel the performance hit.  And we don't need to block it everywhere.  We just need to determine where it matters.  So that's why my feeling is we're at the beginning today, like this week we're at the beginning of a somewhat lengthy process of dealing with the consequences of this revelation, which is that this speculation can be - and, I mean, the proof of concepts have demonstrated that it is possible for code running in a userland to read about 2K bytes per second from the kernel.



So it's not super high-bandwidth, but it has been demonstrated.  And so that's why everybody for the last few months has been running around with their hair on fire, quietly trying to get mitigations in place because the second this became known the hackers were going to try to take advantage of it.



Intel has said that there will be forthcoming updates to the firmware in the chips.  They are going to be introducing some additional instructions to give the low-level coders, the kernel developers, more control over speculation in order to mitigate these.  Yesterday, the Intel CEO said that patches will come to 90% plus, that is, more than 90% of Intel's chips in the next week, and the rest of those by the end of January.



So again, typically, end users are applying OS patches and software patches, but not chipset patches.  That needs to go through the chipset supplier.  So I expect that we'll see Apple patching their stuff at the chip level.  And I imagine people who have laptops and desktops with chipsets which are still being maintained from suppliers who are being responsible and still keeping the BIOS and the chip firmware up to date, I mean, who knows how long it's going to take to get through the pipeline.  Weeks, months, probably.



But ultimately we should see a revamping of the firmware to create new instructions to provide additional granularity to speculation where, as we grow as an industry to understand the consequences of this, this begins to filter out, and we basically roll back some of the freewheeling freedom that we have enjoyed for the last couple decades, not worrying about and not really recognizing the consequence of chips maintaining so much state in order to give us the kind of performance we've gotten accustomed to, and now suddenly recognizing, and it going public, that there is a way to sense that state which was put in place by other tasks running in the same system, and that creating an information leak.  That's what it comes down to.



So basically this is the big coverage of this now.  And I'm sure that in coming weeks and months we'll be discussing the consequences, the actual consequences in terms of performance and mitigations, and where is Android, where is iOS, where is Mac, where is Windows, where is Linux, and where are all the various chipsets scattered through all of our machines.  It is somewhat annoying that we can no longer rely upon the chipsets we have, and that they're going to have to be updated.



Note that the mitigations - and this is what I was trying to explain on Saturday, more than last Tuesday, because this wasn't that clear.  What'll happen is the OS kernels will use a blunt mitigation which will cost us in performance, but protect us,  much like with Meltdown.  But then when something like the PCID, the Process Context ID, is available in the hardware, then the OS will be able to back out of that blunt kneejerk reaction to get back most of the performance by leveraging a feature of the processor that wasn't being leveraged.



Well, that works for the PCID.  What is going to happen with the firmware updates in our chips is new instructions which operating systems will then become aware of, which will allow us to stay secure while recovering performance that we have lost.  So what this means is people who don't or are unable to update their processor's instruction sets with these additional instructions that allow fine-grained mitigation, they will see a performance hit and never be able to get the performance back.  Essentially it's like we were running on borrowed performance at the cost of security without knowing it.



LEO:  They were saying that, for instance, Haswell and older chips are not going to be able to.



STEVE:  I think it's true.



LEO:  But that doesn't even seem that old.



STEVE:  That's not that old, yes.



LEO:  It's a couple of years old; right?



STEVE:  Yes, that is, exactly.  So, okay.  Microsoft has a power shell script which I don't know what the story is there.  I made it the bit.ly link for this week.  And then the zip file from Microsoft disappeared.  Then it came back.  Then it's gone again.  I tried to use it last night when I found it, and it didn't work.  So they've been having all kinds of trouble.



There is a very small and clean utility on GitHub.  It's called SpecuCheck, S-P-E-C-U-C-H-E-C-K.  I've been watching it.  I've been following its development because I just haven't had time, I haven't had time to build it myself.  But I have absolutely no reason to mistrust this person.  If you do SpecuCheck/releases, Leo, add a slash, a forward slash "releases" to that URL.  You will find releases, as of the time of this podcast, 1.0.4, where he has the EXE.  So anybody who is skeptical is welcome to compile their own executable from the source - it's there both in ZIP and TAR format - and the executable for people who can't compile their own.  It's 80K.  That's what I recommend people use.  It looks very clean.  He's improved the documentation on his source.



What this does is probe the chip you're running for all of these specular execution mitigations and the OS that you're on, that is, it is a Windows EXE.  So for Windows users, this is what I think you want to use is SpecuCheck.  It will allow you to verify both before and after a firmware update, a microcode update to your processor, whether or not it supports these features, and whether or not the OS is basically in blunt mode, which is costing you performance, or recognizes that your processor has the required features, if it does, and whether the processor is aware of that and is using them.



So this is the best we have now.  Again, I'm sure this is not the last we'll be talking about this.  I mean, this is, as I said, this is truly a major event for the industry that impacts security and costs us performance because it is because these features give us performance that we've had the performance we have.  But it's the nature of the way we've achieved that performance that inherently - which has been global for the chip, which inherently costs us security.  We are going to immediately get the security back at a short-term cost to performance that we then may be able to recover in the future.  Certainly with any chips moving forward.



LEO:  Whenever I run it, it just crashes out.  It opens a window, and it goes away.  So I'm not...



STEVE:  Oh, no, yes.  You need to, I'm sorry, unfortunately this is not a Windows app.  So you need to open a...



LEO:  Oh, a command line, okay.



STEVE:  ...command line and then manually run it from the command line, and it'll dump out a bunch of texts.  I would love to write one of my little Windows apps for this, but I'm going to keep on SQRL and assume that others are going to write a nice Windows tool, because it's simple to do, in order to show people what's going on with their chip and their system.



LEO:  So I just ran it.  And, yes, it says mitigations for CVE-5754, which you can't really read this, which is rogue data cache load.  Yes for shadowing, but no for user pages marked global.  This is with all the Microsoft patches on here.  And a lot of no's for 5715, which is branch target injection.  In fact, the only...



STEVE:  Yup, in fact that's what we're going to be getting some firmware updates, some microcode updates for our processors.



LEO:  Well, actually no's aren't necessarily bad.



STEVE:  Exactly.  As long as there's coverage elsewhere.



LEO:  Yeah.  Kernel shadowing [crosstalk].



STEVE:  And again, unfortunately this is a bit on the techie side.  I'm hoping that somebody will come up with something that  makes it clear.



LEO:  Yeah, yeah.  But, yeah, at least you can do this.



STEVE:  And if not, I'll steal a day and write one that makes it clear.



LEO:  Yeah, branch protection mitigations enabled, no.  Disabled due to system policy registry, no.  So that's that key you were talking about.



STEVE:  Yup.



LEO:  Disabled to lack of microcode update, yes.  So I guess I'm waiting for a microcode update.



STEVE:  Well, we all are, Leo.  We're all waiting.  That's going to be, you know, again, the problem is Intel having it available doesn't mean that it is applicable, unfortunately, because it's got to come through our platform vendor, Lenovo, for example, or whatever motherboard we're using.



LEO:  Yeah, it's not clear what you want to see on these results.



STEVE:  No.  It's unfortunate that it's not more clear.  It's all we've got right now.  But again, I think a week or two from now it'll be made more clear.  And if it's not, I'll steal a day and write one that is clear.



LEO:  Yeah.  Wow.  Okay.  That's good.  I want one for the Mac now.  I guess I could look at his code.  Looks like it's C code, so it's probably not...



STEVE:  Oh, it is, it is.  It's just pulling information from a couple of the model-specific registers in order to figure out what's going on.  And again, anybody who doesn't trust it can write their own.  And again, I'm sure we'll see several tools out in no time to make this clear.



LEO:  I'm getting exactly the same results as his screenshot, interestingly enough.  So I don't - I hope he explains it.



STEVE:  Yup.



LEO:  Yeah, all right, okay.  Interesting.  I wonder if this would work - does it require the Windows kernel, or is it just calling - I think it's, yeah, I think it's probably Windows only.  Okay, good to know.  Good tool.  As usual, good information.  And what it really does is it's kind of like Apple's battery problem.  It kind of requires everybody with anything Haswell and older to upgrade.



STEVE:  Yes.



LEO:  Geez.



STEVE:  Yes.



LEO:  That's terrible.



STEVE:  Or suffer a probably noticeable performance hit.  End users may not get hit.  That's the good news.  But cloud platforms, hopefully they're on more modern hardware, and/or they have the clout to get the firmware and the microcode update that they need.  So anyway, we'll be riding this one for some time to come, I'm sure.



LEO:  Geez.  So Haswell's from 2013.  So it's fourth-generation.  We're up to eighth.  So, yeah, if you bought a computer in the last, you know, more than four years old, you're kind of out of luck.



STEVE:  Yeah.



LEO:  Holy cow.



STEVE:  You can be secure, but you cannot be fast.



LEO:  Right.  Wow.  Well, there you go.  You've heard it here.  Thank you, Steve, as always.  Steve Gibson, you can find him on the Twitter, and I know there's a lot of traffic on the Twitter about all of this, @SGgrc.  You can also DM him.  He accepts DMs from anybody, unlimited length there, if you have something to say or a question.  You could also go to his website, GRC.com.  Leave questions at GRC.com/feedback.  While you're there, pick up SpinRite.  You must.  You have to.  If you have a hard drive, you need SpinRite, world's best hard drive recovery and maintenance utility even for SSDs.



You can also check out all the work he's doing on SQRL, or the passwords.  I use your passwords all the time.  That 64-character password generator is really great.  On and on and on.  All that stuff is free, including your copy of Security Now!.  He offers 64Kb MP3 audio plus very nice transcripts written by Elaine Farris at GRC.com.  We have audio and video at our website, TWiT.tv/sn.  And you can of course subscribe.  You'll get exactly the same audio if you subscribe on any podcast program, and just make sure you do because you want to get every episode as soon as it's available, which usually is later on, late in the day Pacific time, Tuesday.



We record, if you want to watch it live, Tuesdays at 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  And if you do watch live, by all means join the chatroom.  They are awesome, and it's a great way to get kind of some additional information, some back channel, and some socialization during the show:  irc.twit.tv.  If you want to be in the studio, you can do that, as well.  But please don't surprise us.  We don't like surprises.  Just email tickets@twit.tv, even if it's just an hour ahead of time, just to let us know you're on your way.  We'll make sure we have some space for you:  tickets@twit.tv.



Thank you, Steve Gibson.  Wow.  Wow, wow, wow.  Great show once again.  And we will see you next week on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#646

DATE:		January 16, 2018

TITLE:		The InSpectre

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-646.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss more trouble with Intel's AMT, what Skype's use of Signal really means, the UK's data protection legislation giving researchers a bit of relief, the continuing winding down of HTTP, "progress" on the development of Meltdown attacks, Google successfully tackling the hardest to fix Spectre concern with a Return Trampoline, some closing-the-loop feedback with our terrific listeners, and the evolving landscape of Meltdown and Spectre - including Steve's just completed "InSpectre" test and explanation utility.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  This is a hot episode, I think perhaps will be one of the most listened-to Security Now! episodes of the year, maybe of the decade, I don't know, because we're going to give you, not only the latest on Spectre and Meltdown, but a program you can use on Windows machines that will help you know whether you're safe, and what performance hit you're going to get.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 646, recorded Tuesday, January 16th, 2018:  The InSpectre.



It's time for Security Now!, the show where we come to you via the Internet to protect you from the Internet.  I don't know how that works.  But that's Steve Gibson.  He's figured it all out.  It's like Inception.  Steve is the man in charge at GRC.com and our security hero.  And you're going to really be a hero this week, Steve.



STEVE GIBSON:  Well, yes.  After last week's podcast I went back to work on SQRL that evening, and I worked on it Wednesday.  But Wednesday night I was just thinking, you know, this problem with Meltdown and Spectre is not going to be short-lived.  It's not going to go away.  It's going to be an ongoing issue.  At that time the only solution we had, there was a Linux script, and there was the PowerShell thing.  And I had said on the show, "No, I'm not going to do this; I'm not going to do this; somebody else is going to do this."  And Thursday morning I thought, I've got to do this.



So Thursday, Friday, Saturday, and Sunday - and yesterday, actually - I first had to get fully read in on what was going on because my solution, of course, differs from any of the other ones in several respects.  One is on the back end it deals with the hardware, so it's going down and probing the processor itself, not just relying on some newly instantiated calls from Windows.  And then on the front end, very much as I did with the DNS Benchmark, it interprets the complex interaction of all of these different factors into English and writes a description about the status of your system for the user, for their own particular instance.



The press has just begun to pick up on it.  There have been a couple articles written.  I shot a note out, it was late last night when I finally got this thing done, and had about 20,000 downloads.  And it's now the first three links in Google's search for the term "InSpectre" spelled with the "re" backwards, of course.  So I-N-S-P-E-C-T-R-E is just released - paint is still dripping off the edges of it - freeware which really brings, I think, some clarity to a confusing situation because there's whether or not Windows is doing what, what your processor is doing, whether you need firmware updated or not.



So basically it explains everything and then gives you sort of some bullet points of what you might do.  And whereas Microsoft's script, of course, isn't rough on them, I hold their feet to the fire a little bit because Windows 7 is still being updated for security problems, and Microsoft has shown no indication, at least yet, that they're going to bring the mitigations that they've brought to the Fall Creators Update to 7.  And in fact, even the, what is it, 1703 build, the pre-Fall Creators Update, it doesn't give you safety.  So there's some concern that Microsoft may be tempted to leverage this to push people where they want us all to go.



Anyway, so the title of today's show is "The InSpectre," so we'll talk about that a little bit more.  But also sort of where the industry has gone in the last week.  And I listened to you talking about this at the top of MacBreak Weekly.  I now, well, there's some interesting facts about firmware and microcode for our processors because that ends up being the thing that's left hanging out, which is going to affect many people.  So anyway, lots to talk about.  We're going to continue.  We've got of course news for the week.  We're going to talk about some new trouble with Intel's AMT.  Once again it's in the news, and not in a good way.



What does Skype's use of Signal, which is now in pre-release form, but Skype essentially has announced, Microsoft's Skype will be using the Signal protocol for point-to-point encryption of conversations.  What does that actually mean?  The U.K.'s data protection legislation has given, surprisingly, researchers a bit of relief.  Also some interesting moves in the continuing winding down of HTTP, that is, without the "S" on the end, insecure HTTP, it's really becoming something that no one can practically use, certainly moving forward.  Also there was news of, and I have this in quotes, "progress" on the development of Meltdown attacks.  We knew this was inevitable.  People would start working on these vulnerabilities to turn them into practical attacks.  And there's some early results from that.



Also, Google has successfully tackled one of the hardest to fix Spectre concerns with something known as a "return trampoline."  And then we've got some closing-the-loop feedback with our listeners, and then we're going to sort of do a catch-up on where the world stands with these two critical vulnerabilities.



LEO:  I'm willing to be this is going to be the most listened-to Security Now! in some time.  And, I mean, not only because people will be interested in InSpectre, but also because there's still so much we don't know about what's going on and how it's being fixed.  And one of the things I said on MacBreak Weekly is I wish there was something like InSpectre on the Macintosh.  We really don't know what Apple has done.  We can only take their word for it.  There's no way to validate what's going on.  We don't really understand it.  And iOS, same thing.



STEVE:  So my utility does run on Linux under WINE, and on the Mac under WINE.



LEO:  But that's not going to help because...



STEVE:  Well, but it does because it's able to probe the hardware.



LEO:  Well, if the hardware is mitigated, right.  But we won't know if the OS is mitigated.



STEVE:  No, no.  And in fact, probably, what would be interesting would be to develop actual use tests where, rather than just - so, for example - I'm getting ahead of myself.



LEO:  We'll talk about it in a sec.  All right, Steve.  I am going to, while you're talking right now, I have InSpectre running on my Lenovo, so if you want a screenshot from that I'll show you.



STEVE:  You can get a sense for what it does just by scrolling down, and you can see...



LEO:  I love this.



STEVE:  Yes.



LEO:  Love this.  But I don't want to - I'm not going to steal your thunder.  I'll let you talk about it.  But this is really - this is what needed to be written.  I'm really glad you did this, yeah.



STEVE:  Yes.  Okay.  So security news first.  The press went a little crazy over news of another Intel AMT configuration flaw.  And so I wanted to sort of bring some sanity to it.  I mean, it's not good, but it's sort of a bunch of problems coming together.  It turns out that a lot of corporate laptops which support remote management have AMT, which is, as we've often talked, is this Intel management technology that allows corporate management of machines through the network, which upsets some people.  It's problematical and so forth.



Well, it turns out that it has its own BIOS.  And because it's sort of obscure, and it's not the normal password that the user types in, and it's not normally accessible, even though it has a password, it's almost never changed - you know, you can't make this stuff up - from "admin."  Which is the default password for Intel's AMT BIOS access.



So here's the so-called "attack" which, again, it's not good, but it requires physical access.  During a reboot of the laptop, and I've read all the various scenarios where the owner is distracted, or there's a two-man team who asks one person a question, and it's like, okay, fine.  So movie plots.  But the idea is that, when the laptop is rebooting, Ctrl-P can be pressed at the right moment, or maybe just "P," I don't think you even need Ctrl, just "P" in order to intercept the normal boot sequence and get to the AMT BIOS extension.



It is password protected.  But in all the tests that people have run, nobody ever bothers to change that password because, again, it's sort of like many - there may be corporate IT who don't even know you can get in that way.  So you type "admin" in order to get to the AMT BIOS, where you can change or set the password, enable network access, and change a setting for the user's opt-in to "None," meaning that there needs to be no further physical access to opt into AMT through the keyboard.  It's just all in the background.  And at that point the hacker hits F10 or saves the BIOS, whatever they do to save settings, and from this point on that laptop is now vulnerable to local network access.  You can't do it remotely.  It's got to be on the same network segment.  But that's a low bar to get under because WiFi qualifies and so forth.



So anyway, that's the crux of this is that, unfortunately, this very powerful hands-free kind of remote access, or at least network access, local network access by default has a password which anyone who has brief physical access to the laptop can change or use in order to enable a set of features that then allow hands-off access.  Oh, and I forgot that I sort of buried the lede here.  This bypasses, this use of this bypasses the user's password, BIOS password, the Trusted Platform Module protection, and BitLocker as part of this because you want corporate access to the machine in a way that makes sense to it, that it's able to see.



So all, I mean, all of the interactive protections go away once this is done.  So it's just something to be aware of.  I don't think it's the end of the world.  One would argue, you know, how is it possible that there could be a default password on something that allows you to do this, which is "admin," but that's what it is.  And in Intel's defense, in their best practices documentation they say, of course, change the password.  But nobody does that.



LEO:  Nobody even knows it exists.



STEVE:  Exactly.  You can't change the password for something you don't know exists, and nobody hits "P" when they're booting their laptop.  So the recommendations for end-users would be obviously don't let bad guys distract you while the other member of the bad guy team does this to you.  If you have a laptop that's a corporate laptop, that probably has these features, you might want to say hey to your IT service desk.  Hey, you know, what do I need to do about this?  If you're an individual running your own device, that is, responsible for it yourself, then by all means change the AMT password away from admin to something strong, even if you don't plan using AMT.



And that's the problem is these are systems where AMT is present, but has not been deployed, typically.  So it's just sitting there available for the first person to come along and grab it to do so.  So you should grab it.  If there's an option to disable it, disable it, if you don't need it.  But by all means give it a strong password so that you're not, you know, there's no chance that someone could come along and set that up without you knowing it and then get in behind your back.



And anyway, so that's the story.  It's not what it sounded like from the headlines, of course, that this is the end of the world, remote access, like some new major attack.  But with any luck the major attacks on AMT are behind us now, and now we're just dealing with things that require physical access.



I thought it was interesting that Skype, Microsoft and Skype have announced a partnership with Signal.  We're very bullish about Signal.  We did a podcast on it.  We've talked about it a number of times.  I took a deep dive and completely read the spec.  And our listeners will remember that, as I'm beginning to read the spec, I'm thinking, oh, my lord, has this thing been overdesigned, because I'm always about keeping it simple.  But this thing is just nuts.



But as I continued to read and understood what much bigger than I expected set of problems it was solving, I realized why Moxie and company had done what they had done.  And I just, you know, they really, really thought this thing through well.  They solved very difficult-to-solve problems in a way that, like, problems for which there's no perfect solution, but they did a beautiful set of compromises with no compromise in security.  So I thought, okay, this is interesting that Skype is going to get this.



What we need to remember, however, because there's a lot of hand waving about how Signal is an open source protocol, an open protocol, which we know makes a lot of sense.  It allows someone like me to read the protocol and say, okay, I don't see any problems here.  And in fact I see a lot of good, which can't happen - what is that, the BitTorrent protocol, still undocumented.  No transparency.



LEO:  BitTorrent Sync, yeah.



STEVE:  Yes.  Everyone wants to know what I think.  I don't think anything because, you know, except their name and reputation.  But there's no documentation.  Their press guy was hounding me for a while to talk about it, and I said, I would love to, but I need something to talk about.  We do technology here, not just regurgitate press releases that somebody else wrote.  So I just keep saying, okay, it'd be nice to say something.  I could about Signal because it's fully laid out.



But it's also important to recognize that Skype is not open source, and that given a fully proper implementation of Signal with Skype, what we get is the strength that Signal offers for preventing interception, and some of the extra features that Signal brings, which are those clever bits about, like, how do you handle somebody wanting to send you a message when you're not there, and they need a token, so your client has posted a bunch of available ones on a common server.  I mean, Signal did a lot of things right.



But we don't know, for example, that Skype itself isn't able to respond to a court order.  It couldn't probably through Signal if it's a faithful implementation of the Signal protocol.  But remember that the Signal protocol only protects the data in transit, not at either end after it emerges from Signal or before it goes in for that encryption.  So because it's all packaged into a Skype client, there could be any kind of other means for secreting the communications out.



So it's certainly nice, but I sort of wanted to temper any concept that, oh, this means that Skype itself is absolutely secure, and we can send state secrets back and forth.  No.  It's sort of good that it is, but in my opinion in no way does this even make it more safe because, while it's nice to have Signal connecting the endpoints, as we know, security tends to attack the weakest link.  And the weakest link in the chain is absolutely no longer the end-to-end encryption, but it's everything else.  And there's plenty of everything else to attack in a scenario like this, malware on either endpoint or undocumented features that allow Skype to respond to court orders - which, if not yet part of the law, is frighteningly threatening to become part of the law.



So anyway, it's nice, but I'm not sure that it really actually changes the security profile.  I would be far more inclined if I was really concerned about security to use a fully open source solution, that is, where even the client itself is open source.  And of course, if you really want to go all the way, you use source code that has been fully vetted, and you build it in a clean room environment so you know something about the client and where it came from.  So that's certainly not what Skype is.  So anyway, it's not clear what a lot we're getting from that, but at least end-to-end encryption will be strong, thanks to Signal.



And in a surprising move in the U.K., which has been probably the loudest saber-rattler about their privacy laws and generating lots of, if nothing else at this point, worrisome rhetoric, there was a bit of nice news that I hope portends a trend.  How many times on the podcast have we noted that it was by allowing security researchers to poke at things that significant problems were found, even in situations where the company whose software was poked at was absolutely unhappy that that happened.  Yet we, at this point still, we are not stepping on security researchers imagining that the world is better if nobody challenges the assumption of security.



So what happened in the U.K. is that their rather onerous-looking data protection legislation was amended to a little bit protect security researchers.  There was a section in it where the language was really broad.  It said that it was a crime to deanonymize data, period.  That is, if data had been anonymized, you couldn't deanonymize it.  And a bunch of researchers rightly said, whoa, whoa, whoa, wait a minute, you're saying that it's not possible for us to then verify the anonymization of data?  Because the only way we can verify it, the only way we can test it and poke at it is to try.  And in trying, we would become criminals.  So that clause got backed out to read, quote:  "intentionally or recklessly" re-identifying individuals from anonymized or pseudonymized, there we go, pseudonymized data...



LEO:  Don't know if that's right, either, but...



STEVE:  Pseudonymized, yeah, I think "pseudonymous" is the term.



LEO:  Okay, pseudonymous, all right.



STEVE:  So pseudonymized, yeah.  There's no good way to - it's just like a crazy word.  But anyway, so I'm just - I'm so hoping that we don't end up in a world where researchers fear for becoming criminals.  And if it was going to happen, it would be in the U.K.  So this represents at least some movement in an encouraging direction, which I'm glad for.



Okay.  Winding down of HTTP.  I think it was Simon Zerafa, our friend of the podcast, who sent this to me.  And I was curious because this looked like a big deal.  So I dug into it, and it is.  So we have to understand first this notion of a secure context.  There's a formal definition for the term "secure context" in the world of browser design.



And so formally defined, a secure context is a window or a worker, which is a term for an execution thread, a JavaScript worker, for which there is reasonable confidence that the content has been delivered securely, meaning over HTTPS with TLS, and for which the potential for communication with other contexts that are not secure is limited.  So you obtain this data, this window or worker thread, script, securely; and it's walled off so that it is protected.  Many web APIs and features are currently only accessible in a secure context.  The primary goal of secure context is to prevent man-in-the-middle attackers from accessing powerful APIs that could further compromise the victim of an attack.



So, okay.  That all makes sense.  We talk about this sort of thing all the time.  So in fleshing this out in the Mozilla definition of this, they say:  "Why should some features be restricted?  Some APIs on the web are very powerful."  And as we know, getting more powerful all the time.  We're really seeing the browser turning into an application delivery platform.  Where we use to rely on plugins, those plugin things are now moving themselves into the browser, which in general is a good idea because it means that the developers and users can just count on a unified experience independent of which specific browser they use, and that on the other end services are able to presume that those features are present.  Back in the day you would get, oh, you have to load the Flash plugin in order to proceed.  And we know that's advice that you want to be very wary of for quite some time.



So the problem is that, as these capabilities increase in strength, attackers can leverage them to invade a user's privacy, obtain low-level access to a user's computer, even get access to user credentials.  Although it's not yet widespread, Chrome supports the Bluetooth API.  So a browser will be able to access by radio Bluetooth devices within its reach.  So, yeah, we really don't want bad guys to be able to put ads on sites that then run code to do an inventory of all the Bluetooth devices that you have around you.  So these things are both powerful, but with them comes a real security risk.



So Microsoft's blog posting was sort of putting a toe in the water titled "Secure Contexts Everywhere."  And that's sort of a play on the HTTPS Everywhere idea.  Well, and essentially that's what it means, the idea being that what's being proposed is that, thanks to this world where we have Let's Encrypt and we have reduced the objections to establishing a secure connection between endpoints, where you could argue there is really no strong reason now for a site not being HTTPS because it costs nothing.  And once you set it up, it just manages itself.  And given that attacks are so problematical, and nonencrypted connections are hard to have secure - you can't have a session context cookie over HTTP.  It can be intercepted, and your context can be stolen a la Firesheep years ago.



So what Mozilla is proposing is a more formal statement that anything new moving forward will require a secure context, that is, as new features are introduced, unless there's a clear strong reason where it cannot be instantiated into the specification into the World Wide Web Consortium HTML5 spec, if there isn't a very clear reason not for it to require a secure context, it will.  Which says that, moving forward, any significant new features are going to be able to presume security.  That presumption gives them more flexibility and more power.



Now, it also doesn't necessarily hurt somebody who, for whatever reason, adamantly refuses to, like, their server to be doing HTTPS.  But it does mean that with that refusal comes a significant reduction in features.  So that is to say, if somebody were to set up a website that was not HTTPS for whatever reason, they could certainly serve web pages, but there's all kinds of other things that they very well may want to do which browsers moving forward - and, I mean, which browsers today, there are features which browsers today will already not do unless over a secure context.  And moving forward, all new features, if this proposal is adopted, will require security.



So probably what this has the effect of doing, unless you're running just a very barebones, deliver text, low multimedia, no extra features, you really can't log someone in because they have to have a session cookie in order to maintain their login state.  So I think what this says is another nail in the coffin of HTTP, which thanks to the fact that we now have the ability to easily deliver and administer very low cost, essentially zero cost domain validation certs, and have them dynamically maintained, it makes sense that this ends up happening.



And we're already seeing that servers are being delivered with this capability built in.  That'll be the next, probably the final straw is that all of the major servers on the 'Net will offer that feature.  If you do not want a certificate stronger than domain validation, just set this switch, and your server will be set up with a certificate, and you don't have to worry about it.  At that point, it's hard to imagine, you'd actually have to work, probably, in order to get a non-TLS connection.  And it's certainly foreseeable that probably five, 10 years in the future HTTP will just be gone.  It will no longer be - it'll be like, oh, yeah, remember when those crazy web browsers just sent everything in ASCII text over the wire, and everyone could look at it.



LEO:  Crazy.



STEVE:  Crazy.  Two tweets came to mind that sort of fall into the rest of what we'll be talking about here.  A researcher, Raphael Carvalho, tweeted an expletive, the F-word, comma, "I can barely believe that I was able to read non-cached data from other process efficiently.  Removed iteration and issued flush on secret."  And then he gives somebody thanks for helping him technically, and also Alex Ionescu, I guess that's how you pronounce his name, he was the researcher who did the Linux script early last week and has been very involved in the Spectre and Meltdown stuff.  And then he says thanks to these guys for all the tips.  "Not releasing it, or somebody could definitely set the world on fire with this."  And then he says "#meltdown."



And then Alex tweeted:  "I can finally efficiently" - and he says "(fast)" - "and reliably (no errors) read paged pool/non-L1 data."  And then he had, instead of "Mimikatz," he twisted the terms around.  He called it "MeltiKatz" or "MimiDown" in order to take the first and second halves of "melt" and "down."  He says:  "I'll sit on this a few weeks before setting the world on fire and watching it burn.  Or probably somebody will do it first."  So these are indicative of what we would now expect to see start happening.  That is, this is just too exciting for both researchers and those with malicious intent to go and pursue.  I mean, there's so much data about how this is done, the Spectre and Meltdown fronts.



So far this is all just Meltdown, what these guys are talking about.  Spectre is probably going to take some more work.  And what's interesting, though, is that Spectre, the second variant of Spectre, is the one which is the most difficult to mitigate because it requires firmware updates.  It requires that some features not in the earlier versions of our chips be present in order to mitigate this.  Or it requires that code be recompiled.



Anyway, that leads me into the next and final subject before we move into the next section.  And that is, Google has come up with a very efficient system for dealing with the hardest to fix, that is, the one that in unmodified code requires microcode for existing code.  They call it "Retpoline," a contraction of, or I guess a mangling of, "Return Trampoline."  The return as in the return from an end of a subroutine, and trampoline is the process, as the name sounds like, of bouncing your code through something else.  So it's called Retpoline, as in Return Trampoline, and we're probably going to be hearing about that in the future.  They've released the technology and the technique formally to the public domain and explicitly, at the end of their posting, explaining exactly how to do this, releasing all claim to this technology for the benefit of all.



This was designed by one of the Google engineers who was worried about mitigating the Spectre attack for Google's own processes and code and servers.  This is the one of the two that triggers the speculative execution through branch target injection.  It's the CVE ending in 5715.  Google's gotten a lot of attention and press about this, and deservedly so.  I read through it.  It is very clever.  It does require that code be recompiled.



So this is the kind of thing I heard you guys talking about on MacBreak Weekly before, Leo, where Apple and Microsoft are recompiling the browsers in order to deal with these problems.  And I think what we're going to see, Microsoft also just announced an update to their Visual C compiler to add a /Q Spectre switch to, in that case, that's the other one of these two problems.  It's the one that's easier to fix, where you use an instruction that already exists, an LFENCE instruction, in order to essentially put up a speculation fence at critical parts of the code.  The compiler...



LEO:  See, this confuses me because, okay, they're going to fix the compiler.



STEVE:  It is confusing.



LEO:  But if I'm writing malware, I'm sure as hell not going to set that flag; right?



STEVE:  Right.



LEO:  So it's only - I don't understand what this fixes.  It fixes applications I compile myself or write myself.



STEVE:  From being abused.



LEO:  Ah.  So I can't abuse that userland application.  But if I wrote some malicious software I could still do that; right?



STEVE:  Right.  And so that's what I mean.  I mean, so this...



LEO:  Kind of not much of a mitigation, in other words.



STEVE:  No.  I mean, it...



LEO:  Who compiles their own software?



STEVE:  Well, and that's just it.  So the idea being that, as Windows gets recompiled, then its code will be prevented from being abused.



LEO:  Well, but does it mean a rogue program won't work anymore on Windows?



STEVE:  Well, no.  It means that a rogue program would not be able to steal data from those protected Windows programs.



LEO:  Ah.



STEVE:  Would not be able to get into them.



LEO:  So these are userland - okay.  So that's the thing.  What you want to steal is kernel processes; right?  Or what you get is kernel processes.  Or no?



STEVE:  Or like users' passwords and confidential data.



LEO:  Because they're sitting in cache; right?



STEVE:  Right, because they...



LEO:  From a kernel process.  Or no?



STEVE:  Or maybe even more.  How about like cryptographic keys?



LEO:  Yeah.  But where are those keys being - how are they getting in the cache?  From userland apps or from kernel apps?



STEVE:  Well, yeah.  Well, it depends on where the decryption is happening, but it's happening everywhere, essentially.



LEO:  So I misunderstood, then, the problem with Spectre.  I thought it was I could write a malicious userland app that would spy into cache that had been loaded during the speculative execution. 



STEVE:  That is true.



LEO:  During a context switch to kernel apps.



STEVE:  Correct.



LEO:  The data that I would be seeing would be coming only from cache loaded by the kernel apps?  Or would it be coming from cache loaded by userland apps?



STEVE:  It's actually able to get data out of main memory.  And that's what has everyone so freaked out.



LEO:  Out of RAM, got it.



STEVE:  Yes.



LEO:  So there could be other userland apps running, as often happens in a computer, side by side.  And their data would be in RAM, and this malicious userland app could get it.



STEVE:  Yes.



LEO:  But is it only able to get it during a context switch to kernel?



STEVE:  Well, the way to think of it, it's only able to get it at a low bandwidth, like 2K bytes of data [crosstalk]. 



LEO:  This like Rowhammer.  This is that timing attack.



STEVE:  Yeah.  Although, well, although it's very different.  But the idea is that this branch target thing, what that allows someone to do is to reach over into the OS and obtain a specific byte of data.  And you get it bit at a time, but you're able to essentially probe the RAM of the OS and know what you're probing.  And that's why everyone is so freaked out.  It is a...



LEO:  So if I have a decrypted LastPass Vault in RAM, you could run a bad app that would be able to, not only get it, but it wouldn't be just a dump, it would be something - it would say, "I want to see that LastPass stuff."



STEVE:  Yes.  Now...



LEO:  That's risky, of course.



STEVE:  Oh, and it's why everyone's hair is on fire.



LEO:  So if we recompile LastPass with this Retpoline, then it's not vulnerable to that?



STEVE:  Correct.



LEO:  Got it.  Now I get it.



STEVE:  But LastPass would be protecting itself from attackers.



LEO:  So every developer will want to recompile with these flags turned on.



STEVE:  Exactly.



LEO:  And they'll be protected.  Is there a performance hit from Retpoline?



STEVE:  No.



LEO:  Nice.



STEVE:  That's the other thing.  It is, I mean, they've benchmarked it.  I mean, it's barely detectable effect.  But so, like, no - I don't want to say none because it's like...



LEO:  But 1% or, yeah.



STEVE:  It's like an extra cycle of the processor.



LEO:  Yeah.  Oh, good.



STEVE:  it is negligible overhead.



LEO:  And then, of course, if Microsoft recompiles Windows  using it, that goes even further because now you can't take advantage of operating system processes.



STEVE:  Right, right.



LEO:  Okay.  But again, this isn't every possible vulnerability.  But it is a large class of vulnerabilities.



STEVE:  Yeah.



LEO:  Is it all the Spectre vulnerabilities?



STEVE:  No.



LEO:  There's other ones.



STEVE:  There are two.



LEO:  This is the branching vulnerability.



STEVE:  Yes.  This is the one that, if it weren't for this, would require the microcode update.



LEO:  Got it.



STEVE:  Which is good because it means, if we all did this, then the vulnerability, which is still theoretical, but the vulnerability that would otherwise force us to get a microcode update would be dramatically mitigated.  



LEO:  Got it.  So this is the big one that the microcode update fixes.



STEVE:  Right, right.  And Google found a way of just doing a little bit of trickery at the subroutine return, which is where kind of this vulnerability occurs, so that an attacker cannot control prediction of that event.  And if the attacker cannot control prediction of the return, then the code ends up being safe.  



LEO:  Very nice. 



STEVE:  So, nice piece of work.  And, let's see, I said...



LEO:  Sorry.  I didn't mean to derail you.  But I just - I wanted this clarification.



STEVE:  No, no, no.  I'm sure that was useful for our listeners.  So they wrote:  "Variant" - and this is, you know, the 5715 - "triggers the speculative execution by utilizing branch target injection.  It relies on the presence of a precisely defined instruction sequence in the privileged code, as well as the fact that memory accesses may cause allocation into the microprocessor's data cache even for speculatively executed instructions that never actually commit" and are retired, as we were talking about the last couple weeks.  "As a result, an unprivileged attacker could use this flaw to cross the system call and the guest/host boundaries to read privileged memory by conducting targeted cache side-channel attacks."  I mean, this is all propellerhead stuff.  I mean, it's way out there.



But they say:  "Source can be recompiled with a Spectre-aware compiler" - and, by the way, they've already done this.  There is a GCC has been made Spectre Aware so anybody who is using GCC can immediately do this.  And Google provided this to the GCC guys.  "Source can be recompiled with a Spectre-aware compiler to produce modified subroutine return code whose prediction cannot be externally manipulated."  And that's the key.



And so just by tweaking the instruction sequence at the end of subroutines, which any compiler can be taught how to do, this problem goes away for that particular piece of code, meaning that that code is able to keep its secrets.  So what we need, and I imagine this will happen over time with multi-gig downloads, is for Microsoft to roll up their sleeves and fix Windows in a way that then doesn't require, that at least dramatically reduces the importance of microcode being updated.  Although we're going to talk about microcode updates here in a second.



I got a nice piece of email dated January 15th from a Torkel Hasle who's in - boy, how to pronounce this Norwegian - Sandefjord.



LEO:  Sandefjord.



STEVE:  Sandefjord, thank you.



LEO:  You know, I find it's easier to pronounce words from Scandinavian languages if you just go like this:  Sandefjord, oopie-doopie, it makes it easier.



STEVE:  So in this case SpinRite saved the dentist his daughter works for $60,000, which is a pretty good bargain.  Actually, the dentist saved, yeah, not even less 89 because the dentist didn't have to buy a copy.  He wrote:  "My daughter works for a group of dentists and called me" - so he must be like the local guru - "called me when the old PC that runs the machine did not boot.  The machine operated a huge OPG" - which is a panoramic radiograph, so that sounds like something that does some crazy panorama of your face, you know, dentist stuff - "costing around $60,000 and was installed around 10 years ago.  However, no support for newer versions of the OS.  This expensive machine was dead, period."  Well, they got 10 years out of it.



But anyway, he says:  "I offered to run SpinRite for a few hours."  And of course everybody knows how this story goes.  "It worked for a long time with defective blocks.  But after finishing, the machine booted and did a file check.  All was good, and the $60,000 panoramic radiograph was live again.  Since the machine was so valuable, and they had no backup, I bought a new hard disk" - and he says, parens, (SSD) - "cloned the old disk which SpinRite had made readable with Clonezilla" - which is Linux freeware, he says.  Then he wrote:  "Blew dust away, and booted with the new disk.  All set.  Hopefully the OPG will last another three to five years and postpone another $60,000 investment.  Regards, Torkel Hasle," and you've got to help me here again, Leo.



LEO:  Sandefjord.



STEVE:  Sandefjord, Norway.



LEO:  I don't know if that's right.  I shouldn't do that.



STEVE:  Well, no.  But, you know, thank you.  Better than mine.



LEO:  No, I don't think so.



STEVE:  Well, okay.  Russ Johnson asked:  "Why not just code modern OSes to run the kernel on one core and keep user-mode applications on the others?"  Or, he asks, "Do cores share caches?"  And the answer is, yes, they do.  There's typically L1, L2, and L3; and all of that cache is shared.  Sometimes they're shared between threads on a core at the L1 level.  Then sometimes multiple cores will each have their own L1 cache, but then they'll share an L2 that then goes down to the next level.  And then everybody shares a much bigger L3.  There's, like, there's a lot of juggling about architecture.  And that's sort of why there are about 1,900 different versions of an Intel chip, because it's like, it's very confusing.  It's like, well, do you want more of this or more of that?  And it's like, okay, guys.



So anyway, but the answer is that you could technically split these things except that, then, think about it, one processor would only be - if you had two processors, one for the kernel and one for the user, typically you're bouncing back and forth between user and kernel.  So you'd really be having one processor idle all the time, which would not be very efficient.  And I also heard you talking during MacBreak Weekly, and we'll talk about this a little bit more, this whole issue of the performance hit from the Meltdown mitigations and what that means.  And of course now I understand this because one of the things that InSpectre does, which Microsoft doesn't do, is it tells you if your system will see a significant slowdown from its current configuration, not just is it secure or not, but is it secure at a cost.  And is that cost necessary?



So anyway, Simon Zerafa said - oh, this, I got a kick out of this.  Remember we talked about an Internet-based storage device, Leo, which actually used ping commands in order to, like, you would send your database out on the Internet in pings, and they would get echoed back.  And then you'd send them out again, essentially sort of a very dynamic file system.  Well, not to be the last word on that, it turns out that there's a DNS file system where data is stored in DNS caches.



LEO:  Oh, of course.  Why not?



STEVE:  Exactly.  Just anywhere you can squirrel away some data on the Internet, do it.  So, yes.  Anyway, so Ben Cox, whose Twitter handle is @Benjojo12, he says:  "Introducing DNSFS, true cloud storage.  Store your files in other people's DNS caches."



LEO:  Wow.



STEVE:  Yeah.



LEO:  What a great idea.



STEVE:  Because if it can be done, it's going to be done.



LEO:  Sure.



STEVE:  Oh, and I got this so many times, I just happened to - in fact, I didn't even note who sent this to me because so many people tweeted this.  "Attempted to add Session Manager per your recommendation to Firefox.  It appears not yet to be compatible with Quantum, or I've got the wrong add-on.  Not sure if you were aware."  So first of all, I was delighted that, for whatever reason, my recommendation of Session Manager had so much uptake among our listeners.  And last night I was at my Win7 machine that won't run an old version of Firefox, I'm running Quantum there, and it turns out it was Tab Session Manager that I was using.



LEO:  Oh, yeah, I remember that, yeah.



STEVE:  So there's Tree-Style Tabs, and that still runs under Quantum.  But it's Tab Session Manager.  So many of our listeners were scratching their heads.  I'm sorry.  I wasn't clear.  I'm using Session Manager on my old Firefox.  On new Firefox, on Quantum, it's Tab Session Manager.  So, happy to clarify.



And Philip O'Connell said:  "Hey, Steve.  I remember in a Security Now! episode you mentioned something you used to monitor web pages when you wanted to be notified of any updates to the monitored page.  Do you recall the name of whatever it was you used?  Thanks."  So there's two things I use.  The thing I really tend to use most is an add-on to Firefox called Check4Change, and that's a numeral four.



The way it works is, after you add it to Firefox, you go to a page that you want to keep an eye on.  You mark a block of text, that is, the text that would be indicative of the change you're looking for, like on a movie page, something that you're sure would change when the page was updated to reflect that they're now showing you a different set of movies available at that theater.  Then you right-click on the tab, and there will be on the tab's context, Check4Change is there, and then you are able to select how often you want it to reload the page.  And then it just takes over.  It just sits in the background, and every X minutes it will reload the page in the background and see whether the region you marked has changed.  So it's nice because it's not in the way.  It's there if you infrequently need that.  But sometimes you do, sometimes if you really do want to check to see when something changes.  And so it's out of the way most of the time, but really handy when that's what you want.



And then in Windows, that is, outside of any browser, it's just a little piece of freeware called WebMon, W-E-B-M-O-N.  And it does what you think.  It's free.  It's simple.  You give it a URL.  And it's got other features, too.  You can give it like a range of offsets from the page and stuff that it will then monitor.  And so it just does it independently.  It's a non-browser-based query to a remote site that waits for a page to change and lets you know when it does.  So those are my two web browser update goodies.  And lastly... 



LEO:  One more feedback from the peanut gallery, the chatroom.  Sandefjord Sam says this is how you pronounce Sandefjord.  And of course we have no audio at all.



RECORDING:  Sandefjord, Norway.



LEO:  Oh.  Sandefjord, Norway.



RECORDING:  Sandefjord, Norway.



LEO:  Sandefjord.



STEVE:  Very good job, Leo.



LEO:  I was close.



STEVE:  Yeah.



LEO:  Sandefjord.  That's Emma pronouncing - EmmaSaying.com.  So we're going to trust Emma.



STEVE:  Perfect.  I do.  She sounds very authentic to me.



LEO:  She does.



STEVE:  So several people commented that they were stunned by how much turning tracking protection on in Firefox to "Always" improved their experience.  We talked about it last week.  It made you a little uncomfortable, understandably, because it looked like it was just going to do blanket advertising blocking with no finesse.  And as we know, the 'Net is becoming increasingly dependent on advertising.  But after a number of tweets from people saying wow, Grant Taylor sent me a note saying "Firefox's tracking protection always on even helped with uBlock Origin enabled," meaning it does more.



But here's the point, the reason I'm bringing it up, and it can be disabled per site.  So we get the best of both.  If you have sites that you want to support, that complain if you have adblocking, tracking protection on, you can turn it off, and it's sticky.  So thanks, Grant, for bringing that up.  And of course so that makes it useful for people who do want to have maximum speed.  And the feedback has been strong that it really makes a difference.  Yet as we do, we want to be able to support sites that say, hey, please, we need you to look at our ads.  And so we're able to do so.



LEO:  Nice.  Very good.  Here's, by the way, just for completion, here's how you pronounce Alex Ionescu's name.



RECORDING:  PronounceNames.com.



LEO:  Yeah, thank you.



RECORDING:  Ionescu.



LEO:  Ionescu.



STEVE:  Ionesco.



LEO:  Ionescu.  There's Ionesco, which is Eugene Ionesco, who was a famous playwright - was that his first name? - who wrote "Rhinoceros."  But then there's Ionescu.  But I think they're probably variants of the same Romanian name.  So Ionescu, Alex Ionescu.  I just want to make sure we get his name right.



STEVE:  I'm happy to do that.  This is an audio podcast.  We're not doing sign language, so it does matter.



LEO:  All right.  Steve Gibson, GRC.com, has a new tool for all of us.



STEVE:  So you like it.



LEO:  I love it.  So I have run - it started with - and I did this I think on Windows Weekly last week, the PowerShell script that Microsoft put out, the Speculation Execution Validator.  I've run other tools.  But only yours tells us about vulnerabilities specifically to Meltdown, to Spectre, and what the performance hit is.  And you have a very good explainer in there.  Go ahead.  You want me to show it?  Or do you want to talk about it first?



STEVE:  Yeah, go ahead and show it, yeah.



LEO:  So I ran it on the fully mitigated Lenovo, and that's what the screen looks like.  And I'm happy to say vulnerable to Meltdown, no; vulnerable to Spectre, no; and performance, good.  And then this is what I really think you're doing everybody a service because you're kind of explaining it in the app.  And this is something I've not seen anywhere.  So if you are getting a performance hit, you can disable protection.  You have to reboot.  But the ability to take it out and get full performance if you need it, and then put it back, is huge, I think.



STEVE:  Yeah, yeah.  In fact, the show notes start out with a tweet three hours ago from a Brian Yates, who said:  "Thank you, sir.  My work laptop went to a crawl after latest Win update.  Seen your tweet about the freeware.  It showed my performance slower.  Yeah, no joke.  So disabled Meltdown fix, restarted, back to normal."



LEO:  Now, you might want to reenable it; right?



STEVE:  So, yeah.



LEO:  Is there a way to reenable it?  Does that button change to Enable?



STEVE:  Oh, yeah, yes.



LEO:  Oh, nice, nice.



STEVE:  It toggles back and forth. 



LEO:  Now, I should point out, so I also, as you said, it does run on a Macintosh with WINE.  But I don't know if it's seeing the operating system.  I've done all the patches here, as I'm just showing you the update.  And I know this is small.  It's kind of hard to read.  But it says, "Vulnerable to Meltdown, yes; vulnerable to Spectre, yes; performance, good."  I'm not sure that's accurate.



STEVE:  Okay.  So the question...



LEO:  This is a fully patched Macintosh.



STEVE:  The question would be - and you're right, it's going to have a problem because it's going to be seeing WINE.  But you know how I break everything down.  One of the things it says is, and I don't remember exactly what the language is I used.  There is, you know how I show individual paragraphs to explain what it has found, there is something about the performance, whether the hardware supports high performance mitigation of Meltdown.



LEO:  That part is green.  It says it does support that.



STEVE:  Then that shows your firmware is patched on that Macintosh.



LEO:  Good.  



STEVE:  That's something I -



LEO:  That's why I'm saying good.  But it says the hardware has not been updated with new features required to protect against Spectre.



STEVE:  Correct.



LEO:  And it's the 64-bit of Windows, version of Windows.  Obviously that's wrong because I'm not running Windows, I'm running WINE.  So that would be - it says it's not aware of Spectre or Meltdown.  But of course WINE probably isn't.  But that doesn't say anything about whether macOS is; right?



STEVE:  Correct.



LEO:  So I'm going to ignore that paragraph.  But it does say "System hardware has not been updated with new features allowing the operating system to protect against Spectre."



STEVE:  I'm trying to remember...



LEO:  Same with Meltdown.



STEVE:  Yeah.



LEO:  But that's the operating system; right?  So it's really the hardware mitigations that I'm curious about.  The registry is configured properly, but that's ironic.  I don't even know if there is a registry.



STEVE:  Correct.  And so the Intel processor's CPU ID instruction contains this wealth of information.



LEO:  Yeah.  That's where you're getting this from?



STEVE:  Yes.  And so I'm pulling it from the chip itself, underneath the level of the OS.  And I wanted to do that because all of the other, I mean, I'm sure you saw the one, in fact I think I saw you running it over the weekend, the Ashampoo thing.



LEO:  Yeah, that was dopey.



STEVE:  Well, you know that it makes a network connection.  I didn't realize, but it's connecting back to the mothership for some reason.



LEO:  That might be benign, but I - yeah.



STEVE:  It's like, okay.  Well, no.  And I don't think they're bad people.  It's just that, okay, why do it?  And also it pretends to be scanning for, like, a minute or so.



LEO:  Yeah.  I knew that was bogus.



STEVE:  It was like, what the hell.



LEO:  There's nothing to scan.  You just query the CPU, and it tells you.



STEVE:  Yeah.  But anyway...



LEO:  So I should say, on this machine I'm running, this is an older machine.  This is a late 2014 5K iMac, which means it's running probably a Haswell or earlier chipset.  So I am going to run this on my iMac Pro at home, and I'll send you a screenshot.



STEVE:  I would love to know because I'm trying to remember, there was some data that I thought I could trust from Windows, but I am pulling some from the CPU ID.  But I'm not...



LEO:  And that should still work under WINE; right?



STEVE:  Yeah.  But I'm not pulling both.  I don't remember now if it was Meltdown or Spectre.  One or the other, I think it was Spectre - man.  No.  Anyway, I'll have to look because this thing is...



LEO:  I'm just saying it's just great for a PC.  But if you're running it on Linux or macOS, Steve's probably not getting all the information that would be useful in this case.



STEVE:  Correct.



LEO:  It's being hidden.



STEVE:  Now, here's one of the really interesting things, speaking of Linux, is the way microcode is updated.  When we talk about updating our microcode or needing like a microcode patch or a microcode fix, notice that it always comes in the form of a BIOS update.



LEO:  It's firmware, yeah.



STEVE:  Well, yes.  But it's not written into the processor.  And that's the key is that it is dynamically loaded by the BIOS every time you boot.  So if you look at the second link here at the end of the show notes, Leo, this is Intel has published the Linux processor microcode data file.  Linux has the ability to load updated microcode for, I mean, to me it looks like every processor Intel has ever made.



LEO:  Yeah.  It's a modular kernel.  So I'm sure it's part - it's a kernel mod, probably.



STEVE:  Well, it turns out if it's - I think it was under, god,  it was in the - if you place this file in a certain directory in Linux...



LEO:  Oh, how interesting.



STEVE:  ...it will dynamically...



LEO:  Oh, that's awesome.



STEVE:  Yes.



LEO:  I need this.  Because I'll tell you what, none of my Linux machines are mitigated as far as I can tell.  There's been many patches and updates to Arch Linux and to Ubuntu and to Debian.  But none of them have received firmware updates.  So this is going to be a boon.



STEVE:  Yes.



LEO:  If you figure out what processor you have.



STEVE:  Yes.



LEO:  That's nice.  You just put this file in the right spot, and you're good to go.



STEVE:  Yes.



LEO:  Love that.



STEVE:  And so this solves - so my point is that, first of all, Linux has always had this feature, that you're able to fix microcode problems by just putting a microcode data file in Linux, and it patches the processor.



LEO:  And then you put it in the ETSI firmware directory.



STEVE:  That's it, yes.



LEO:  Wow, that's so cool.  That's so cool.



STEVE:  That's it.



LEO:  All right.



STEVE:  VMware has something.  If you click the link above that one, you'll see that VMware has a CPU microcode update driver.  It's not, like, formally supported.  They produced it.  And it works for Intel and AMD processors.  The problem is that VMware loads on top of the operating system, after Windows has decided whether or not it has updated microcode.  So for Windows users, patching the microcode after Windows is running doesn't help us.  But this does say that, if Microsoft cared to...



LEO:  Oh, they could do it, sure.



STEVE:  Yes.  They could do the same thing Linux...



LEO:  All they'd have to do is check and say, oh, let's load this in.



STEVE:  Yes.  They could do the same thing that Linux has long been doing of loading a microcode update on the fly.  And so when we are flashing our firmware, when we're flashing our BIOS, when we're updating our microcode, as you and I have done recently on Lenovo and various machines, what we're really doing is that microcode is being loaded into the flash ROM.  And then, when the system boots, the BIOS patches the microcode.



LEO:  Wow.



STEVE:  It's literally an on-the-fly patch because, think about it, microcode has to be the fastest thing on the planet.  I mean, it's executing at that sub-gigahertz, at the 3GHz rate.  It's like a processor in the processor, microcode deciding what to do.  So there's no way it could even be flash.  It's got to  be RAM.  It's got to be able to run at full-on lightspeed speed.  So the chip has default microcode; but the BIOS, there is a provision for patching the microcode, for updating microcode at power-up, and that's the responsibility of the BIOS.  So that's why...



LEO:  And that's for Meltdown specifically?



STEVE:  No.  That's for...



LEO:  That's for Spectre.  



STEVE:  That's for Spectre.



LEO:  Okay.



STEVE:  Okay.  So here's the deal with Meltdown.  Meltdown can be mitigated with recent features.  And I have this in my notes.  There was one feature added - shoot, I posted it in the newsgroup and didn't copy it into my show notes.  There are two instructions or features.  There's something called PCID, and then there's Invalidate PCID, INVPCID.  Intel put this into the chipset, like in 2010, that first instruction, PCID.  And what this does is it tags the cache with a - PCID stands for Process Context Identifier.  So it adds a tag to all of the cache entries indicating which process caused this to be loaded into the cache.  And if this was used by operating systems, Meltdown would be prevented.  But no OS bothered.  And it turns out the engineers at Intel didn't really think it through very well.  It turned out to actually do it was really burdensome.



So a few years later they added a second instruction, Invalidate PCID, INVPCID.  If you have both of them, then it is easy for an operating system to solve the Meltdown problem with no overhead, no flushing.  What this does is those two instructions allow the cache to be tagged in a way that prevents any cross-process bleed, at no expense, like no performance overhead.



LEO:  So that's why Intel said Haswell or earlier you're going to have a bigger hit.



STEVE:  Yes.



LEO:  So post-Haswell.  So if you have generation 5, 6, 7, or 8 Intel processors, it has that instruction, probably.



STEVE:  Both instructions.



LEO:  Both instructions probably, yes, yes.



STEVE:  Yeah.  Because they did sort of a half measure a few years before.



LEO:  Did that mean they saw this coming?



STEVE:  Yeah.  I mean, they - yes.  And as we know, it was, as I heard you mention during MacBreak Weekly correctly, and we talked about it on Security Now! last week, there's a 1995 paper cautioning about these sorts of problems.



LEO:  Terrible.



STEVE:  The idea, I mean, sharing the cache is so efficient that they just wanted to.  And so they gave us a mechanism for preventing cache probing attacks.  But because nobody was probing anyone's cache, nobody built it in.  And it was like, yeah, okay, fine.  Well, so that's why in a matter of, like, a few weeks, Microsoft was able to say, oops, we'd better do that.  So if those two instructions exist, then the cache does not need to be flushed.  It can simply be tagged using this Process Context ID in a way that prevents Meltdown completely.  And so no performance hit.  But that's only available on recent chips.



And there isn't - I don't hold Microsoft responsible for not doing this except that - get this, Leo.  If you don't have Fall Creators Update, even on Windows 10, they're not giving you this.  The 1703 build that was the pre-Fall Creators Update, it's not taking advantage of these instructions.  And, I mean, Leo, it's the same code.  There is - I cannot forgive Microsoft for saying, oh, if you're not using Fall Creators Update version of Windows 10, we're going to slow you down in order to give you Meltdown protection.



LEO:  Well, there's a class action lawsuit waiting to happen.  Holy cow.  I mean, let's - maybe, to be fair, I'm thinking maybe there was kind of something in the Fall Creators Update that made this easier to do.



STEVE:  It's the same code base.  You know it is.



LEO:  Yeah.



STEVE:  I mean, look at the dialog boxes.  They still look like XP if you dig down far enough.



LEO:  I know, I know.  And I'm sure they will roll out more patches.  I mean, this is kind of the - I'm wondering, InSpectre itself, you're never going to be done with this because there's going to be more and more all the time to mitigate; right?  Or  no.  If I get green across the board, I'm good.



STEVE:  Yes.



LEO:  On this vulnerability, anyway, or these two CVEs.



STEVE:  I just posted, while you were doing that second sponsor slot, I posted a test release to the newsgroup.  If anyone's curious, well, I won't tell you where it is.  That would be a disaster.



LEO:  No, but I will tell you where the current InSpectre is; right?  GRC.com, and then click the Security, or actually the Freeware tab, Security tab within the Freeware tab, and then you'll see it right at the top, InSpectre.



STEVE:  Right.  And Google has now got it as - it's the first three links.



LEO:  I saw that, yup.



STEVE:  If you just look for "InSpectre."



LEO:  Google "InSpectre," I-N-S-P-E-C-T-R-E.  And, yeah, I saw Woody Leonard gave you a plug in Computer World.



STEVE:  Very nice.



LEO:  gHacks has it.  I'm sure it will be mentioned all over.  It is easily the simplest and smallest and yet most feature-rich InSpectre checker I've tried.  Highly recommended.



STEVE:  So what we just realized last night, and I found out about it this morning, is that it was causing false positives on a bunch of AV.  And somebody who had been testing them all week, or the last five days as I've been iterating this, said, "Steve" - I don't remember what his AV was.  But he said:  "No version of InSpectre until the final one was triggering AV."  Well, the final one is where I added the Enable/Disable buttons.  And so I thought, oh, it's annoyed with the registry key.  It sees my use of that registry key as being malicious and raising a red flag.  So I just posted a version that should bypass AV.  I scrambled the registry key, and I decrypt it on the fly so that no scam will see it.  And I imagine we'll go back then to being quiet as we were before on AV.  So the gang in the newsgroup are testing it right now.



I want to - there's one bit of wording I want to clarify where two of the paragraphs seem to be conflicting with each other.  But then I think I'm done.  I was worried that, if it was possible for us, that is, like this VMware solution to patch microcode, that I was going to have to do that because this is the big remaining problem is that, without the microcode patch, there is really no way to mitigate one of these two Spectre problems without recompilation.  Now, maybe that'll just happen organically throughout the entire industry.  All the compilers will just add - which really seems a shame - add this code.  Or maybe they'll add it by default until all the processors do get fixed, and then we can stop horsing around with this Retpoline solution that Google came up with.



But the real problem is, when you think about it, the reason our IoT devices have had such problems is that light bulbs don't update their own firmware.  And up until this point the security problems have been caused by the OS and thus fixed by OS updates and patches.  This is arguably the first time that we've seen a severe security vulnerability caused by or leveraging a feature of the processor.  And so suddenly our PCs are sort of like our IoT, inasmuch as we're realizing that there isn't an ecosystem in place for responsibly keeping processor firmware up to date in the same way we're keeping our router firmware up to date.  We just really haven't had to until now.



And so I was very glad that my Lenovo Carbon laptop could get updated.  I have a Dell laptop that's the E7440.  And Dell, being a mainline supplier, they were right on the ball.  They had across-the-board firmware for their laptops on the 8th, I  think it was dated.  So they responded immediately to this.  But how many of us are using computers we built, or computers with an unknown heritage?  I mean, they're just never going to get firmware updates.



LEO:  And not to mention a bunch of smartphones, IoT devices.



STEVE:  Oh, yes, yes.



LEO:  This is, I would say, the vast majority of Intel and AMD and ARM processors out there will never be patched.  By the way, does your program work for AMD?



STEVE:  Yes.  It is AMD-aware.



LEO:  Okay.



STEVE:  It has whole separate blocks of text for AMD, explaining that you never were vulnerable.  It disables the left-hand Meltdown pushbutton because there's no way to enable or disable Meltdown on AMD because you're never vulnerable.  No, I mean, it's - you're going to like it, Leo, the more you see it.  It just really explains what's going on with your system.  



LEO:  That's nice.  That's really nice.  We should mention a number of people are reporting that some antiviruses flag this as malware when you download it.  Obviously it's not malware.  That's, I don't know, there's some false positive going on.



STEVE:  Well, yeah.  In fact, that's what I was just talking about is that I already have a test version that will bypass  that malware.



LEO:  Oh, I'm sorry, yes, of course, okay.



STEVE:  Yeah.  And so that's what that is.  You're right.  It is not malware.  It's signed by me.  It's a hundred - I was annoyed.  It's 125K.



LEO:  Why so big, Steve?



STEVE:  Because, if you're going to have Windows 10, you need large high-color icons.



LEO:  Oh.  The icon.



STEVE:  The icon is 93K.  93K for GD icons.



LEO:  Well, it's a nice icon.  Who did that, the little ghost icon?  I like it.



STEVE:  I did.



LEO:  It's very nice.



STEVE:  I took the Spectre icon and then made it unhappy and pissed off-looking and took the stick out of its hand.  Yeah, but 93K of my 125 is icons.



LEO:  So really this is just a 22K [crosstalk].



STEVE:  Yeah, it's the proper size for something like this, yeah.



LEO:  Well, thank you, Steve, for putting this out.  And of course, as always, letting us know.  Somebody's saying I bet you spent as much time on the ghost graphic as the program.  I don't think that's true, but... 



STEVE:  No, no.  I've got good tools.  So relative to disabling, we should talk about that for a second.



LEO:  Oh, yes, good.



STEVE:  Okay.  At this moment we don't know - I should mention there's been a lot of dialogue in the GRC newsgroups about turn it on, turn it off, what should we do?  We don't know of a single instance of exploit of these problems.  I salute the industry for no longer being lazy about this and going, eh, it's not a problem, wait till somebody attacks us.  No.  We've learned our lesson.  Everybody has jumped on this immediately.  Clearly, Meltdown is the most worrisome and the easiest to exploit problem; and, with the addition on older processors, flushing the cache at a cost of performance mitigates that vulnerability on Haswell and later.  And InSpectre will tell you whether you have a newer or later processor.



On the newer ones, it's just you need OS support, so in order not to have the cheap, but easy, yet expensive performance hit.  And this is where, I mean, I'm annoyed that Microsoft hasn't done it in the pre-Fall Creators Update.  They haven't done it for Windows 8.1 and 7, either, and they could.  So I'll be really interested to see whether, like with next month's Quality Rollup, we get this fixed because Windows 7 and 8.1 and even Windows 10 before the Fall Creators Update are - they have Meltdown mitigation, but they've got the performance expensive mitigation, even on a new processor.



So you need both a newer version, actually the newest version of Windows, Windows 10 Fall Creators Update, and a very recent processor for Meltdown to be - for you to be protected against Meltdown.  And that's with no performance hit.  If you have an older processor or an older Windows, any older Windows, then you're protected; but XP isn't because they're not going that far back.  But Windows 7 and on you're protected with a performance hit.



So at this moment today, Meltdown is the easiest to exploit; yet it is, as far as we know, unexploited.  Yet to varying degrees, people are experiencing a performance hit on, if they don't have Fall Creators Update, with recent enough hardware.  So the question is should they?  I don't know.  I don't think I'm going to put that mitigation in place for GRC's servers because nobody runs ever, ever runs suspicious code on GRC's servers.  It just doesn't happen.  I'm not your typical, oh, let's download this piece of software.  I mean, those servers are never touched.  So on the other hand, they're also very lightly loaded.



So it may be that I will end up getting around to do that.  But for the typical user, certainly for the podcast listener, people listening to Security Now!, you will know the instant we find Meltdown being used in the wild.  I do expect it.  I think that there are so many systems which are not going to be fixed, that may not have their OSes updated, that will be victims.  And Meltdown is so simple to, I mean, we're going to be covering Meltdown exploitation just as I did with those couple tweets from the researchers who were saying, oh, my goodness, this wasn't hard.  I mean, so my guess is it's going to get exploited.  And at some point we'll all want to have our Meltdown shields up.



But if you're not running the latest Windows 10, if you don't have recent enough hardware, if you actually feel the slowdown, as that one person who tweeted really did, then I would argue there's a case to be made for turning it off until there's some reason to turn it on.  Again, personal decision.  Individuals can make the decision.  InSpectre, my little free app, makes it easy to flip it back and forth.  And in fact you can turn it off and then reboot, do some things and see how your system feels, flip it back on, reboot, do the same thing.  See if you notice a difference.  If you really don't notice a difference, yeah, then I would say, by all means, run with Meltdown protection enabled.  If it does really affect what you're doing, then today I would say it's safe to turn it off.  There's a known vulnerability, but we know of no exploitation of it yet.



LEO:  Okay.  There you have it.  And there's no reason not to get InSpectre right now.  It's free at GRC.com, or Google "InSpectre."  And while you're there, get SpinRite.  I heard a yabba-dabba-doo during the show.



STEVE:  We had one, yes.



LEO:  I like that.  We want to hear some more yabba-dabba-doos as people pick up the world's best hard drive recovery and maintenance utility.  Steve, it's awesome what Steve does.  And this utility is just a simple little bit of all the many things he does for free, which you'll find at GRC.com.  SpinRite's the one that funds it all.



He also for free offers this show, audio and transcripts at GRC.com.  Thank you for paying for those.  We have audio and video at our site, TWiT.tv/sn.  You can also subscribe to the audio or video feeds.  Any podcast program should know about Security Now! by now.  And I think it's a good idea to subscribe.  That way you get it the minute it's out.  And you can listen at your leisure.  Just search for "Security Now!" in your podcast app.



We do this show every - if you want to watch live, you can.  Get the latest, freshest version of Security Now!, well, that's about 1:30 p.m. Pacific, 4:30 Eastern, 21:30 UTC every Tuesday right after MacBreak Weekly.  And you can watch it at TWiT.tv/live.  And you could also join us in the chatroom.  It's always - this is a good show to be in the chatroom.  Lots of explication, exclamations, and fun and games at irc.twit.tv.  They're loving Steve.  They've loving Steve for InSpectre.  Good job, Steve.  Thank you very much.  Well, that's it, I guess, for Security Now!.



STEVE:  Yup.  We'll be back next week with some more news and updates and so on.



LEO:  I'll let you get back to your newsgroups and SQRL.



STEVE:  Till Monday.



LEO:  Right.  See you, Steve.



STEVE:  Yes, sir.  Thanks.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#647

DATE:		January 23, 2018

TITLE:		The Dark Caracal

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-647.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week's news continues to be dominated by the industry-shaking Meltdown and Spectre vulnerabilities.  We will catch up with what's new there, then discuss the Net Neutrality violation detection apps that are starting to appear; a new app and browser plugin from the search privacy provider DuckDuckGo; a bit of welcome news from Apple's Tim Cook about their planned response to the iPhone battery-life and performance debacle; a bit of errata; and some feedback from our terrific listeners.  Then we take a look into a state-level, state-sponsored, worldwide, decade-long cyberespionage campaign which the EFF and Lookout Security have dubbed "Dark Caracal."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And as always, lots of security news.  We'll be talking about it all, including a security threat pinpointed by Lookout Security and the Electronic Frontier Foundation.  Out of Lebanon, the Dark Caracal.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 647, recorded Tuesday, January 23rd, 2018:  The Dark Caracal.



It's time for Security Now!, the show where we cover your privacy, your security, your deep-rooted insecurities with this guy right here, Steve Gibson.



STEVE GIBSON:  And if you don't have them at the beginning of the podcast...



LEO:  You will.



STEVE:  You will know what you've been missing.



LEO:  Very good point.  Excellent point.  Hello, Steve.



STEVE:  Yo, Leo.  Great to be with you once again for Episode 647 of this, what are we, we're in our 12th year and counting.



LEO:  And this one has the most mysterious title I've ever seen.



STEVE:  The Dark Caracal.



LEO:  What the hell?



STEVE:  C-A-R-A-C-A-L.  Yes.  I was hoping that we could be done this week talking about...



LEO:  Oh, Intel?  Oh.



STEVE:  Talking about Spectre and Meltdown vulnerabilities.  But no.



LEO:  It's worse than you thought.



STEVE:  Yes, it's continuing to generate news.  So we will deal with the updates on Spectre and Meltdown, and also talk about Linus Torvalds' freakout.



LEO:  Oh, he was mad.



STEVE:  Oh, he was really not happy.  So we'll catch up with the news there.  Then I think maybe because the whole industry is so preoccupied, there wasn't a ton of other interesting stuff.  There's an interesting few apps which are appearing which are purporting to be Net Neutrality violation detection apps.  So I want to talk about them a little bit.  DuckDuckGo, the well-known privacy-focused search provider, has some new offerings that I want to cover.  Tim Cook also recently said something that really made me happy on the iPhone performance battery front, that I just want to touch on.



We actually have some errata from a misstatement from last week, not that that should be a big surprise, but we haven't had any for a while, that I want to cover.  Then we'll deal with some feedback from our listeners and then dip into the world of a state-level, state-sponsored, worldwide, decade-long, cyberespionage campaign which the EFF and Outlook Security have dubbed "Dark Caracal."



LEO:  Oh, my.



STEVE:  This is sort of like the exact opposite of the news we've been covering recently where every one of our listeners has, like, comes away with action items.  It's like, oh, crap, what button do I push?  What do I download?  I need to update this and blah blah blah.  Here there's nothing you can do.



LEO:  Okay, good.  At least there's nothing I can do.



STEVE:  There's nothing you can do.  But by sort of putting a point on this and looking at this - it was a 51-page report that I read last night in order to really read into this, with lots of details.  I sort of wanted to synchronize us all with an aspect of what is clearly going on, not just in this case in a building focused in Beirut, Lebanon, but I'm sure every state, every nation-state has something like this - the U.S., China, Russia, and on and on and on.



And so here's an example of a big cyberespionage campaign that a nation has had underway for some time.  And it's just for the, I mean, the title of the podcast is Security Now!.  And it's worth us sort of all appreciating that, although there's nothing we can do about it, except be careful if you don't want to become a target, this is all happening.  And it's sort of sobering.  One line from the report really kind of caught my attention, and we will get to that toward the end of the podcast today.  So I think something interesting for our listeners.



LEO:  I know it's not in your notes, but there was a great article today in BuzzFeed News, "Inside the Fight for the Soul of Kaspersky Lab," that I highly recommend.  Fascinating.  And basically the conclusion was there was a struggle between Kaspersky and some members of management and others over how much influence the Russian government would have.



STEVE:  Should have or would have, yeah.



LEO:  Should have, would have.  Now, you've got to consider that they're relying on a story from a Russian publication called Meduza.  But they also did their own research, and they concluded that the battle was won by the Russian security services.  They even assert that Kaspersky has built into it a way to look at any computer that's got Kaspersky installed upon it, to look into their files.  And, now, of course Eugene Kaspersky denies that heatedly, as does the company.  But that's [crosstalk].



STEVE:  They have to.  I mean...



LEO:  I would if I were them.



STEVE:  Yeah.



LEO:  Oh, yeah, we can do that, yeah.



STEVE:  Oh, is this Russian-backed spyware?  No, no.



LEO:  Yeah.  No, no, no, no, no, of course not.  But I think that was, I mean, if you were on the fence about Kaspersky...



STEVE:  That would tip you over.



LEO:  I think this article would...



STEVE:  And not in the direction of renewing your subscription.



LEO:  Yeah.  Yeah.  Basically this is the quote.  According to a former senior manager who was involved with launching Kaspersky Labs and the product, or I'm sorry, KSN, which is the Kaspersky Security Suite, I think it was called, it was launched in 2012:  "The product was referred to as 'cyberintelligence' inside the company.  The system can be run manually from a remote location."  Oh, I'm sorry, it's Kaspersky Security Network can be run manually from a remote location, he told Meduza, meaning an employee of the Kaspersky Lab can download any file from a computer on which KSN is installed without its owner's knowledge.



STEVE:  And you know, I mean, should that really surprise us that much?



LEO:  No.



STEVE:  I've often commented here on the podcast that I'm amazed in the era of Linux, which is open source and beautifully designed and maintained and architected and functional, that non-U.S. companies are choosing to run Windows. I just...



LEO:  Right.  For the same reason.  Right.



STEVE:  Yes.  It just boggles my mind that they're like, oh, yes, we run Windows.  It's like, what?  Really?



LEO:  But it's interesting because Kaspersky originally would run locally only; right?  And when they launched this KSN in 2012, it was a network-based security solution; right?  And they said moving to a cloud solution lets us analyze and neutralize new viruses and et cetera, et cetera.



STEVE:  Yup, yup.



LEO:  And there are a number of other antiviruses that do this.  But of course an always-on connection has - one of their sources says it's like an awesome kitchen knife that can be used for superbly slicing bread or stabbing people.  Okay.  You decide which use you want.  Anyway, I thought I'd pass it on.  I'll send you the link.  I thought I'd pass that along because we've been talking about this back and forth.  This one seems to put a nail in the...



STEVE:  Yeah, well, and I haven't gone into it because all we have is hearsay.  I mean, you know...



LEO:  Yeah, and that's still probably the case.



STEVE:  I want to talk about bits in machine status registers, which are clear and well defined, and I can say something.



LEO:  You're an engineer.  You like...



STEVE:  I could say something about that.  But, you know, it's like, well, you know, yeah.



LEO:  Our Image of the Week talks about philosophy versus engineering, as well.  I like it.



STEVE:  It does.



LEO:  Yeah.  All right, Steve.  Shall we do the picture?  Yeah.  I love this.



STEVE:  So our picture, it's a four-frame cartoon that a bunch of our listeners sent me because of course it's right on target for us.  The first frame has - each frame has two characters talking.  The first frame has the first guy looking at his laptop, saying, "But you can't do that.  It goes against every basic security practice."  And we can't see the screen.  We're looking at them in the back of the screen, so we don't know what it is that this goes against.  But the other guy says, "Yeah, I know."  And then the second frame he continues, "But in philosophical terms, it makes sense."  And then the first guy says, "You don't get paid for philosophy.  You get paid to code."



And then the second guy in the third frame says, "But if it's just one character off, we should give a little encouragement; right?"  And the first guy says, "No, no, no, no, no, no."  And finally we see the screen that has generated this controversy.  And it's your typical username and login.  And the response to the password is, "The password is almost correct.  You're close.  Keep going."  And then offscreen we see the little dialog box:  "If the password is wrong, we show 'error' and that's it."



LEO:  Yes.



STEVE:  So of course we've even spoken of how, I mean, there are many ways to do this wrong.  And one of the things that we sometimes see is a login that asks you for your username and your password and then says, well, you got the username right, but the password wrong.  And it's like, wait, you know, we just gave away some information that wasn't necessary to give, very much like this, which inherently allows the security to be chipped away at.  Properly designed, if you're going to ask for both at once, you say, "We're sorry, we couldn't find you," or like there's something wrong with your information.  Again, don't give them any information that they don't already have.  Just say [buzzer sound], exactly, just error.  Not, well, you know, except for the fourth character...



LEO:  So close.



STEVE:  ...you were really good.  No.  No.  So, okay, Spectre and Meltdown.  Now, first of all there's a lot of misinformation again, just because I think editors are rushing to get to their pages online.  And so they're talking about Spectre and Meltdown together, when we know on this podcast they are related more in time than they are in nature.  So Meltdown is the problem which is easy to resolve and may cause a performance hit or may not, depending upon how recent your chipset is.  But significantly, its resolution, no event requires a firmware update for your chips from Intel, and AMD never had a problem because their chips were not vulnerable to this.



So the reason I bring this up is that Intel has just reversed its so-called "advice" to all of its OEMs, saying stop, stop, stop, stop patching anybody's firmware.  We made a mistake in last week's firmware.  And I don't remember if we touched on it last week, but there were - it's hard to nail them down.  They're saying something as fuzzy as "May cause reboot issues," or "May make your system reboot more often."  It's like, wait a minute.  More often than never?



LEO:  More often?



STEVE:  More often than what?  In other words, this causes a catastrophic crash of your system.  But they don't want to use those words.  So yesterday the newsroom of Intel sent out, quote:  "As we start the week, I," writes in first person this newsroom persona..."



LEO:  Mr. Anonymous, yeah.



STEVE:  Ah, yeah.  "I want to provide an update on the reboot issues we reported January 11."  Okay, so that was 12 days ago, but 11 days ago yesterday.  "We have now identified the root cause for Broadwell and Haswell platforms and made good progress" - Leo, they're making good progress.



LEO:  Well, well.



STEVE:  "...in developing a solution to address it.  Over the weekend" - oh, and you can just imagine there's a lot of late nights over there at Intel.  "Over the weekend we began rolling out an early version of the updated solution to industry partners for testing."  Oh, so everybody was awake.  "And we will make a final release available once that testing has been completed."  Okay, but not now.  Not yet.  "Based on this, we are updating our guidance for customers and partners.  We recommend that OEMs, cloud service providers, system manufacturers, software vendors, and end users stop deployment..."



LEO:  Oh, my god.



STEVE:  "...of current versions."



LEO:  And those of you who've already installed it, oh, gee, we're sorry.



STEVE:  Oh, and actually that's happening.  Dell has announced in following the guidance that they will be offering previous versions of the firmware immediately because, like, to just - because now Intel has scared everybody that reboots might happen any minute.



LEO:  So this is, though, but it's Haswell and Broadwell systems.  It's not Coffee Lake, Kaby Lake.  It's not the more modern systems.  It's older systems.



STEVE:  Oh, no, it's - no, no.



LEO:  Oh, it is?



STEVE:  They haven't gotten to those yet.



LEO:  Oh, great.



STEVE:  Oh, yeah.  It's everything.  So, okay.  So they're saying stop deployment of current versions as they may introduce - get this, again - "higher than expected reboots."  What?  Well, okay.  Like I'm not expecting any.  So I guess a reboot which is unexpected, well, which you didn't Ctrl-Alt-Delete to make happen, or pull the plug out or something.  Anyway, may reboot.  And, they say, "other unpredictable system behavior."  Now, by its very nature, unpredictability is something you are working to avoid in your processors.  So, yeah.  Then they say:  "For the full list of platforms, see the Intel.com Security Center site."  Which is very busy lately.



Okay.  Meanwhile, the technical side - that was Mr. "I'm giving you an update from the coffee room."  Here we have this title.  This page is titled properly:  "Speculative Execution and Indirect Branch Prediction Side Channel Analysis Method."  So you know you're getting something a little more meaty than, ooh, more than expected reboots.  Okay.  So this was updated yesterday.



"We have now identified the root cause of the reboot issue impacting Broadwell and Haswell platforms and made good progress in developing a solution to address it."  Still they're making progress.  They haven't arrived yet.  "Based on this, we are updating our guidance for customers and partners."  So you can tell that Mr. Press Guy basically pulled his language from the technical guys.  Same recommendations to everybody:  "Stop deployment across the board of current versions on the below platforms," they write, "as they may introduce higher than expected reboots," blah blah blah.



"We also ask that our industry partners focus efforts on testing early versions of the updated solution for Broadwell and Haswell we started rolling out this weekend, so we can accelerate its release."  In other words, we don't really know, apparently we're unable to decide that this is fixed.  So we need everybody to try it and let us know if we're getting warmer.  Did your unexpected reboots slow down?  Whoa.



Then they write:  "We expect to share more details on timing later this week."  That is, this current week.  "For those concerned about system stability" - oh, and I guess there are people who are not concerned.  But "For those who are concerned about system stability, while we finalize the updated solutions, we are also working with our OEM partners on the option to utilize a previous version" - like one of the ones that worked - "of microcode that does not display these issues."  Right, no one ever uses the word "bug" anymore.  We don't have bugs, we have issues and more frequently than expected reboots.  Okay.  



But, they say, it no longer crashes, but it "removes the Variant 2 Spectre mitigations."  In other words, of course, you're vulnerable again; but boy, you're able to stay online, which is handy.  "This would be delivered via a BIOS update," which might be called a "downdate" in this case, "and would not impact mitigations for Variant 1" - which is not a problem, which did not require the firmware update - "and Variant 3 Meltdown," which never had any of these problems.



They say:  "We believe it is important for OEMs and our customers to follow this guidance" - in other words, quickly run away back to a BIOS that worked, earlier firmware - "for all the specified platforms listed below, as they may demonstrate higher" - oh, here we are again - "than expected reboots and unpredictable system behavior.  The progress we have made in identifying a root cause for Haswell and Broadwell" - get this, Leo - "will help us address issues on other platforms."  Oh, we're going to learn why this didn't work for Haswell and Broadwell, and that'll help us fix the other ones.  "Please be assured," they say, "we are working quickly to address these issues."  So here it is.



"The guidance applies to at least some of the processors from Intel's last several generations of chips, with affected models in the Broadwell, Haswell, Coffee Lake, Kaby Lake, Skylake, and Ivy Bridge families."



LEO:  But they said continue to patch the newer ones, which to me...



STEVE:  Yeah.  "Certain lines..."



LEO:  You know, Red Hat may have given them the warning on this because they pulled their update last week, saying we're no longer providing microcode to address Spectre Variant 2 due to instabilities that are introduced, causing customer systems not to boot.



STEVE:  Yeah.  Minor problem.



LEO:  Yeah.



STEVE:  "Certain lines," they write, "are affected more than others."  I mean, this is all so weird and murky.  "For example, only Ivy Bridge datacenter/workstation" - okay.  "Datacenter/workstation processors are included."  But chips from most recent consumer lines also appear to be impacted.  So that sentence doesn't seem self-consistent.



"Intel says that it's identified the issue behind the unexpected reboots on Broadwell and Haswell processors and is working toward releasing an update that addresses the exploits without causing the issue.  But the same problems have been seen on Ivy Bridge, Sandy Bridge, Skylake, and Kaby Lake processors, too.  Intel says it's 'actively working on developing solutions' for those platforms, as well."  So bottom line is nobody should - okay.  So if you have not installed the firmware, don't.



LEO:  God.  I just got a pushed - Apple's pushing out an update right now.



STEVE:  Yeah.



LEO:  But Apple did not get bit by this because they didn't do the microcode patches initially.



STEVE:  Right.  I have my main laptop and two other - two Dells and a Lenovo.  I've updated them all.  I've been using them, and none of them have rebooted unexpectedly.



LEO:  I haven't had any reboots, either.



STEVE:  So I guess the advice would be, I mean, it's better to be secure than not.  Although remember the biggest problem, Meltdown, that is the only one that we've ever seen even a proof of concept for, isn't a problem.  That got solved just with a Windows Update.  No firmware update needed.  Spectre is a known problem, but much more difficult to exploit.  And for none of this have we ever seen an in-the-wild exploit.  And it's not remote anyway.  It's something gets into your computer and is able to peek across process boundaries.  And it's not clear that Windows is good about maintaining those anyway for an individual.  I mean, debuggers function by deliberately reaching into another processor and sticking its hands in there so that it can see what's going on and help a developer to fix broken code.



So the big concern had always been in the cloud, where you inherently had a single executing block of hardware that could have many different users sharing the hardware.  And so it was cross-user boundary was the concern.  But on your own machine, you're your own user.  Unless you get some, I mean, theoretically you could get some malware in there that could get up to some monkey business.  But no one has seen that yet.



So our advice would be, if you have not updated your firmware, don't bother.  Why create problems for yourself?  It's harder to fix your BIOS if you have a nonbooting machine than it is - because nowadays you're able to update your BIOS, typically from within the OS.  You just download the app and say yes; and yes, I'm sure; and yes, I'm really sure.  And then it says, okay, be sure not to kick out the power cord for the next minute while we reflash your BIOS, and you'll be all fixed.



So if you end up with an unbootable system, undoing that requires booting from a thumb drive and adjusting your BIOS to boot if you can and so forth.  Potentially, you could brick yourself, so that's not good.  So if everything's fine, and you have updated your firmware, like I have and you have, Leo, and nothing has rebooted, then we're probably okay.



LEO:  Yeah.  I just checked on my Lenovo T470s, and there's no updates.



STEVE:  Right.



LEO:  So I don't know if Lenovo is rolling back those updates at some point or what they're doing.



STEVE:  Well, this has just happened.  So it was like a long weekend at Intel, and now we have this advice that hit yesterday.  And I just saw announcements.  People are tweeting me that they're getting news from Dell saying go back to a previous BIOS.  So I think what will ultimately happen is we will eventually get new firmware which is solid.  And everyone should either update again or update for the first time at that point.



But given the fact that what we have now is flaky, if you are concerned, you could certainly go back to a previous BIOS.  I know that generally when you go to the firmware and BIOS pages, you're able to say, oh, look, here's the earlier one, and the one before that, and the one before that.  You could choose the one before last week and have a firmware which you know is going to be solid because it comes along with the BIOS.  My feeling is, if there isn't a problem, essentially, if everything's fine, stay where you are.  If systems have become unstable, back out.



And in all cases we'll wait till the dust settles, till everybody agrees that this problem has been solved.  And then the good news is any manufacturer who just gave us a BIOS and firmware update two weeks ago will certainly still be around to fix the one that they gave us two weeks ago with something that is known to be more solid moving forward.  And boy, it'd be fun, again from an engineering standpoint, to get some clarity into what is all going on.  It's just difficult to understand from an engineering standpoint.  What have they done that causes more reboots than usual?  Wow.



Anyway, so that's where we are.  I would say follow the advice of whoever your supplier is.  For example, if Lenovo tells us you absolutely really must undo what we did two weeks ago, well, then I'd be tempted to.  But they may be saying, as they generally do with BIOS updates, if there's no problem, don't mess with it.  If everything's fine, just move along.  So, wow.



Now, against this backdrop, Linus Torvalds has been having a temper tantrum meltdown.



LEO:  Oh, boy, is he pissed.



STEVE:  In public.  I mean, dropping the "F bomb," and calling this a bunch of S-H-I-T, and cannot believe what Intel is doing.  And in pursuing this, I just hours ago, this morning, I found the document I had been looking for but hadn't tracked down yet, which is the ink is still wet on the PDF.  It's titled - it's a 15-page PDF from Intel titled:  "Speculative Execution Side-Channel Mitigations."  This is the technical details of what Intel is proposing for the microcode fixes.



And just because I was putting the podcast together I had no chance to dig into it.  But, boy, I'm going to have fun tonight because I'm really curious, having built the InSpectre to bring some clarity to this.  And having taken the approach I took, now I'm really interested to - but I was never - I was always running from third-party comments.  I found some ESX VMware documentation that I based some of the stuff on.  And I'm so happy to get finally the original root document that is the center of this controversy.



So, I mean, and the problem is Linus isn't, in anything I have read that he has written, being clear.  He's just ranting.  And it'd be nice if he were to catch his breath and calm down and explain what it is that seems to be the problem.  Near as I can tell, he's unhappy that Intel intends to call a "solution" the simple turning off of a couple flavors of branch prediction.  That's what, you know, the whole Spectre thing is that it's possible to leverage this speculative execution of branches and branch prediction in a way that leaks information.



And so Intel's solution for now, and they've said for the time being, is to allow operating systems to turn it off.  So his tantrum is that they're disabling all branch prediction, with the concomitant performance hit that that will bring, as their solution.  And he's wanting them to fix it right.



Now, the reason I think he's wrong, that is, the reason I don't have sympathy for his position, I mean, yes, it would be wonderful if we could have that.  But microcode is not like application code.  By that I mean that microcode is tightly bound to the hardware.  So it's not like there's a whole processor written in software that Intel can just say, oh, well, we've decided we're going to change everything.  No.  It's that what the industry realized decades ago was that implementing an instruction set as complicated as Intel's, in pure hardware, that is, in gates, in just simple logic gates that do all the work, was becoming impossible.



This is the difference with the ARM architecture.  Because the ARM instruction set was so simple, was so clean, they've been able to give us a high-performance system that doesn't rely - I don't know if it relies at all on microcode.  But if it does, not nearly to the degree that Intel's does.  So essentially, Intel's instruction set got so complicated that you could no longer actually do it in hardware.  You had to back off.



And so what they did was they moved some of the complexity into sequential functions which are driven by this microcode.  But the point is you can't - for example, their branch table prediction system doubtless is a hybrid of hardware that does most of the work, and some microcode that just kind of points in different directions and says, oh, now raise this line, or let this data flow through here.  Okay, now stop it, now run a compare, and that kind of thing.  It's not actually doing the work, it's directing the hardware which is purpose-specific to do the work.



I have no doubt that Intel has learned some incredibly important lessons from this and that, downstream, a future generation of Intel processor will solve this problem.  But they can't solve it with a patch to the microcode.  And that's the problem.  So what Linus is asking for is not feasible.  What they've done is the only thing they could do with the existing chip hardware, which is say, "Oh, crap."



LEO:  Doh.



STEVE:  And basically they've given us the "oh, crap" bit.



LEO:  Yeah, we got that part set.



STEVE:  Yeah.  And so here's the "oh, crap" bit.  And if you turn it on, well, sorry, things are going to run slower.  But that's the only thing we can do.  We can let things run fast, and you hold your breath; or you can turn on the security bit, and it's going to go slower.  But this class of exploits can't happen.



So Linus is not happy.  But what he wants, you can't get there from here because microcode isn't like operating system code, where you could say, oh, we're going to change everything.  On the same chip we're going to give you Windows 98.  Now we're going to give you NT and Windows 10.  No.  It's the microcode is just basically the minimum required to simplify the hardware.  But you're stuck with the hardware that you've got, which is doing all of the real work.  Microcode is just like a traffic cop, raising and lowering, allowing data to kind of flow where it's supposed to and simplifying the process.  So in fact I'm reminded of Wozniak's floppy disk controller because that was the genius that Woz designed where he gave us this six-chip floppy controller that was microcoded.



LEO:  Ah, interesting.



STEVE:  He'd used microcode by taking some of the outputs of the ROM through a latch, and then fed them back into the address inputs of the ROM.  And so by strobing this latch, he basically created a little microcode program in ROM that was, like, essentially, a little hardware processor that just did floppiness.  And it was astonishingly elegant and incredibly inexpensive.  And where everybody else had these - remember, the floppy controllers at the time were these monstrous huge cards where you couldn't get any more chips on the thing.  He had three.  Or, I mean, he had six, six little chips, just sort of wandering around lost on this small little floppy controller.  And he's like, yeah, well, that's the way you should do it.  It's like, with incredible modesty.



But anyway, so what I think, where we're going to be is that there's no question that Intel's future architecture was just scrapped.  I mean, their classic back to the drawing board because they can't sacrifice performance for security in the long term.  But it's not going to be - and I don't even know what their design pipeline is.  It's like years long, I think.  They've got multiple big teams running on stuff that we'll see next year and the year after that and the year after that.  A lot of work is probably already committed.  I don't know what this is going to do to their whole lifecycle system.  But it's doubtless impacted it.



LEO:  It doesn't affect Retpoline, right, the Google fix?



STEVE:  Correct.



LEO:  Okay.



STEVE:  And we do have a solution for that from Google, which...



LEO:  They say has no performance hit.



STEVE:  Correct, exactly, because you're able to - it's a very, like a cycle or two at the end of a subroutine which already burned up thousands and thousands of cycles being in the subroutine.  So like not measurable.



LEO:  Hey, here's some good news.  Remember last week I ran InSpectre using WINE on my 2012 iMac, and it wasn't a very good result.  I think it was No, No, No, across the board.  I mentioned that Apple pushed out a fix today.  And look at this.  All of a sudden Yes, Yes, Good.



STEVE:  Oh, cool.



LEO:  Oh, no.  Yes, Yes, is not good.  Yes, Yes is vulnerable to  Spectre.  Never mind.  It's just as bad as it was before.  I got confused by the yes.  Yes is bad.  No is good.



STEVE:  Yes.  And in fact several people have said, "Steve, you know, you need to rethink the indicators because..."



LEO:  [Crosstalk] patched for Meltdown, no.  Vulnerable to Spectre, no.  I mean, patched [crosstalk], something like that.



STEVE:  Yeah.  I need to reverse that because...



LEO:  Yeah, "Yes" sounds good, yeah.



STEVE:  It is confusing, and I'm relying on color.



LEO:  Darn it.



STEVE:  And "Yes" sounds, oh, yes.



LEO:  Darn it.  I was all excited.  So I may never get it fixed, this is such an old machine.  And you did, and we should update this, we talked on Sunday, or Saturday on The New Screen Savers, say that your correspondents have said that WINE does give you a full and useful result on Macintosh.  It doesn't hide.



STEVE:  Inside of a virtualized environment like VMware, you're not really seeing the chip.  So you cannot rely on what InSpectre shows in a true VM.  But WINE is not that.  WINE is just an API layer.  And my InSpectre app is reading the data from the chip's registers in order to determine whether you have the firmware update...



LEO:  And those aren't hidden.



STEVE:  Right, those are not hidden.



LEO:  So that's the microcode patch, which it turns out it's actually maybe "yes" is right because you don't want them [laughter].  So, you know, maybe I'm in good shape after all.



STEVE:  No one wants their system to be rebooting ever, let alone more than usual.  So, yeah.  Woody on Windows, who's got a great column over at Computerworld, put out a - he's been tracking all of this confusion.  And, boy, it is confusing.  One of the things that users of 32-bit Windows have been annoyed by is that Microsoft hasn't been keeping them updated.  That is, their systems seem to be late in getting any of these patches.  So I wanted to note that there is now, for Windows 10 version 1709, which was the Fall Creators Update of Windows 10, for x86-based system, that is, for 32-bit, there is an update, but you've got to go get it.



Woody wrote:  "Cumulative update KB4073291 brings the Meltdown/Spectre Windows patches to 32-bit machines."  And then he says:  "What?  You thought 32-bit machines already had Meltdown/Spectre patches?  Silly mortal.  Microsoft's Security Advisory [blah blah blah] has the dirty details in the fine print, point 7."



And I think we talked about that before, where they asked themselves the question, "I have an x86 architecture, and the PowerShell Verification output indicates that I am not fully protected from these speculative execution side-channel vulnerabilities.  Will Microsoft provide complete protections in the future?"  They answer their own question:  "Addressing a hardware vulnerability with a software update presents significant challenges and mitigations for older operating systems that require extensive architectural changes.  The existing 32-bit update packages listed in this advisory fully address" - and then there's two of the problems - "5753 and 5715, but do not provide protections for the 5754 at this time."



They say:  "Microsoft is continuing to work with affected chip manufacturers and investigate the best way to provide mitigations for x86 customers, which may be provided in a future update."



So then Woody says:  "It appears as if this is the first 32-bit version of Windows" - meaning the Fall Creators Update, that 1709 version - "that has a patch for the Meltdown vulnerability.  Surprise."  And then he says:  "Like most of the patches I talked about yesterday, this one is available only through the Update Catalog.  It won't be pushed to your machine."  So I've got the catalog update link in the show notes.  Don't know if there's an easier way to find it.  Maybe you could Google, what is it, "KB4073291 x86" or 32 or something, in order to find it.



So I thought that would be of interest to our listeners because you're not going to get it automatically, but it's there.  You just have to go get it yourself.  And that adds mitigation for Meltdown, which again, of all of this, that is clearly the more important.  It's the one for which people are developing attacks.  Proof of concepts exist.  They're able to read data from arbitrary locations in other processes and in the kernel.  That's the one you want fixed.  It is, however, if you have an older processor, the one that will also give you the biggest performance impact.  And using InSpectre, you can flip it off and on, back and forth, to see if you notice a difference and then choose whether you want it to be fixed or not.  But you don't have that choice unless you get the update from Microsoft.  So it is available.



LEO:  So you can just go to catalog.update.microsoft.com and put in that KB4073291.



STEVE:  Oh, good.



LEO:  And when you search for it, you'll get the download directly.



STEVE:  Ah, good.  So there are a couple apps which have just surfaced that are of sort of questionable value.  The first piece of news I picked up on this, it was a couple days ago, and there was some controversy because Apple had blocked it in the App Store, and the people said, "Hey, wait, are you trying to prevent us from detecting Net Neutrality violations?"  And Apple said, "What?  No."  Then they had some banter back and forth, and they apologized, and they said, you know, "We're busy."



LEO:  We didn't understand what this was for, yeah.



STEVE:  Exactly.  Exactly.  So in what is arguably a clever URL, one of them, and I think it's a different one, I'm not sure, because there are a couple, TestYourInter.net is the URL.  So I thought that was kind of clever.  TestYourInter.net is the URL.  And that will take you to an app called OONI, O-O-N-I, which is the somewhat awkward acronym for the Open Observatory of Network Interference.  Now, the good news is it's open source.  It's a nonprofit app for tracking Internet censorship.  Their goal is for it to be sort of a crowd source-y sort of thing, where everybody downloads OONI for their iPhone - I'm not sure if it's cross-platform, if it's Android also.  I think I remember...



LEO:  Yeah, they have Android, too.  Yeah, they have both, yeah.



STEVE:  Okay, good.  I remember seeing two links.  Yeah, there they are.  And then this thing pulls data from a number of different sources and checks the bandwidth.  In some of the coverage I saw, it was already generating false positive alerts because they were noting that, oh, I got 25Mb from Apple, and 14Mb from Google, but only 8Mb from YouTube.  And it's like, yes, because streaming is different.  And so I hope that this ends up not being a boondoggle because, I mean, the idea of being able to, like, the idea of having surveillance of ISPs' provider-based bandwidth, that's sound.



But it is the case, for example, that a streaming provider that is already providing an ungodly amount of bandwidth to the world, I mean, the proper operation is to feed out a stream of content as needed so that they're not sending you a huge blob where you look at the first minute or two and then say, no, I'm not interested anymore.  So you end up discarding a lot of bandwidth that you received from them that you yourself used or downloaded, but then never actually consumed.  So it's going to be important to, like, be careful with any of these apps about how we interpret what they show.



The idea I like, the idea of an observatory, which is how they're couching this, where lots of users download it, everybody runs it, and then this creates a distributed probing network that can send information back to the mothership, and they can understand what's going on.  That seems useful.  And I like the idea that ISPs will know that if they do get up to any hanky-panky, they're going to be - it's going to be spotted.  It's not going to get slipped under the rug because we're all wanting to make sure that Net Neutrality, which they say, oh, we want to have the freedom, but never, never, never, never would we think of abusing that freedom.  It's like, okay, well, trust and verify.  Or at least verify.



DuckDuckGo is a popular search provider, and I just wanted to note to our listeners that they have a cross-platform offering, a browser plugin for Firefox, Safari, and Chrome, and also iOS and Android apps, where they're looking to extend their well-known privacy protecting search to also a general web browsing, blocking, tracking protection.  They say smarter encryption.  What they're actually doing there is they're doing something that we've seen before.  Some URLs are not secure, that is, http://.  Their plugin and/or their browser on the mobile platforms will attempt to make a secure connection to the same domain and, if it's available, will use that preferentially over the nonsecure.



So it's a small thing.  Many sites now bounce people over from the nonsecured to the secured automatically.  GRC has done that, for example, for a long time.  But certainly it's nice just to have that as an additional benefit.  So not a huge thing.  But I thought that some of our listeners would find it interesting.  And I know that DuckDuckGo has a following among people who like the idea of being able to search with some additional privacy guarantees.



And lastly, in our light news week, thanks to the fact that Spectre and Meltdown have just been so dominant, I did note an article in Gizmodo saying that Tim Cook was promising to let iPhone users turn off throttling, which to me makes the most sense.  I think I will probably still, on my older iPhone 6 Plus, take Apple up on their battery offer, if they ever get batteries back in stock again.  I guess they're - do you know how far backordered they are, Leo?  Have you been following that?



LEO:  No, I don't, offhand, yeah.  You should do it anyway, I mean, it's so cheap.



STEVE:  Yeah, why not?  Exactly.  Really extend the life of that device.  But the best solution is to give us the option.  And so the good news is - Apple wasn't saying when.  I'll be interested to know if we just got it with this iOS...



LEO:  No, they said the next version.  I think 11.3 will do it.



STEVE:  So not a security - so just today, a couple hours ago, we got 11.2.5, which adds some new features to HomePod and other stuff, and fixes a bunch of stuff, and also has a lot of security content.  So that's a standard security update.  The nice thing is that soon we should have an option to turn off throttling and take responsibility for Apple's concern that this is going to make our devices spontaneously reboot.  I was always a little skeptical of that.  And maybe if your battery is - the only thing I could ever figure is that the battery was so weak that, if the phone had a sudden surge of demand for speed, that is, because we know that it does dynamically change its power demands, it might just, like, hit the battery with a demand it cannot meet, which would cause the voltage to drop below operating threshold and would indeed make the phone reboot.



LEO:  Yeah, that's exactly what Apple said the mechanism was, yeah.



STEVE:  Okay.  And the problem, of course, was that those who had taken good care of their batteries, even though it was old, they just did that without telling us and then denied it for a long time.  It's like, oh, what?  I mean, our listeners got tired of me saying something is broken.  This does not work the way it used to.



LEO:  Yeah, but what do you mean, take good care of your battery?  Did you not charge it?



STEVE:  I never used it.



LEO:  You were never on battery.



STEVE:  Well, yeah.  In fact, I have one of our closing-the-loop questions here is one of our listeners saying, "You use the term 'babying your battery.'  What exactly do you mean by that?"  So I do plan to address that.



LEO:  We'll get to that.  Good, good.



STEVE:  Errata.  I misspoke, and a number of our listeners were confused by it, for which I apologize.  Sean Spratt said:  "@SGgrc On your podcast you said only post-Haswell has the INVPCID" - that's the Invalidate Process-Context ID support instruction.  He says:  "However, I'm reading elsewhere that INVPCID is included in Haswell.  InSpectre says I have high-performance Meltdown protection.  So, yes, I have INVPCID."



Similarly, Richard Tan:  "Hi, hope you are well.  I keep hearing on Security Now! that Haswell and down would not be patched with the performance fix, but Haswell seems to be the first set of processor that has INVPCID.  So should it be that Haswell onwards should get the performance fix for Meltdown?"  Okay, so yes.  I misspoke, and I apologize.  To get the high-performance Meltdown protection you need Haswell or later.  Because we need two instructions.  And I did initially say it correctly, and I must have just off-the-cuff misspoken later, like a week later.  And so I confused everybody.



The first instruction was introduced way back on January 7th in 2010, so 18 years ago.  Wait, no, sorry, eight years ago.  That was in Intel's Westmere architecture.  And that was the PCID instruction.  Intel meant well with that instruction.  Technically it allowed the instructions in the cache to be tagged with the process-context ID that called that data into cache so that it would then be able to be reused, but no other process with a different context ID would be able to see that.  Intel, it turns out, though, hadn't asked OS developers if this would be useful.  They just said, "Here you go."  And so it turns out it was really awkward to actually use.  It just ended up being something that was nice, but nobody used it because we didn't have to, and it was just very difficult to implement.



So Intel learned from their mistake; and 3.5 years later, in the summer of 2013, they fixed it with the Haswell microarchitecture which was also the fourth-generation core successor to Ivy Bridge.  That's when they added the Invalidate PCID instruction, the INVPCID, with Haswell.  And that's, it turns out, the key that allowed all of the OSes, Linux and Windows, to in very short order fix this meltdown problem.  So I apologize for the confusion.  Haswell and later, that is, Haswell and more recent processors will all have this INVPCID instruction.  So thanks for catching it and letting me know, and I'm glad to correct the record.



I got a nice note from someone who's not an English speaker.  Ansiotropic is his name in Twitter.  And he said...



LEO:  Anisotropic.



STEVE:  Oh, Anisotropic.  Oh, thank you.  I always...



LEO:  It's a video gaming term.



STEVE:  Ah, okay.



LEO:  Don't worry about it.  You don't need to know.



STEVE:  And you're right, I was completely - because there's no way that's ansiotropic.  I just never pronounced it [crosstalk].



LEO:  Anisotropic.



STEVE:  Anisotropic.  As opposed to isotropic.



LEO:  It's a substance having a physical property that has a different value when measured in different directions.  Wood is stronger across the grain than it is along the grain.



STEVE:  Or fur.



LEO:  But in gaming it's a kind of, in 3D computer graphics, it's a kind of filtering, anisotropic filtering.  Fur is also, yes, yes, anisotropic.



STEVE:  Thank you for the correction.



LEO:  I think we've all learned something here today.



STEVE:  Well, I have.  And so did this guy.  He sent me an earlier tweet that I found later.  But he said:  "Today's update."  He said:  "SpinRite 6 on Windows 10 64-bit."  He said:  "Okay.  After a grueling 17 hours at Level 3" - which given what it did doesn't sound like it was that bad - "on normal non-problematic hard drive, 1TB, I can safely say this product is awesome."  He said:  "From what I could notice, it fixed all oddities in, but also out of Windows."  And then he said:  "BIOS resolution.  TY."  So I assume that's thank you.  And so sort of decoding that interesting tweet, there was something going on with his BIOS, and I guess some oddities in Windows.  He did a Level 3 scan on that Windows 10 system on his terabyte drive, and everything got fixed.  So Anisotropic, thank you for your tweet.



And some closing-the-loop bits.  S. Wayne Martin said:  "Hey, can you comment on how you determine performance when your InSpectre says performance is good?  Seems that would require a benchmark to determine.  Curious minds."  And so, yes, what I'm doing is simply using the presumed performance hit which we expect from the Meltdown mitigation when your processor doesn't have the latest instructions, that is, Haswell and later, which allow the mitigation to be performed with low performance impact.  So I'm not doing a benchmark.



And in fact, in the text that InSpectre also emits, aside from the one word "good" or "reduced" or "slower" words that it uses, it explains that your mileage may vary.  You're able to flip the protection on and off using the Meltdown pushbutton and then reboot in order to see if you sense a difference in performance.  But all I'm doing there is essentially I'm saying that the performance is good because I'm basing that on the relative low impact of the performance that's been reported for both the mitigated Spectre and the mitigated Meltdown vulnerabilities when you've got the processor support, with or without firmware update.  So just sort of ballpark.



And, frankly, I'm wanting to put a little bit of heat under Microsoft because, as we've been reporting, they're not yet taking advantage of the Invalidate PCID and PCID instructions on their older OSes.  Hopefully they're planning to do that.  They're settling for that performance hit, which they need not do so on newer processors.  And I'd like to just keep that on everybody's radar so that we can make sure we stay aware of the fact that there is no need, there shouldn't be a need to go to Windows 10 Fall Creators Update, which apparently is the only platform where they are offering this high-performance mitigation of Meltdown, which is annoying.



Alessandro Canepa, he says:  "Regarding WebMon," and he gives the URL, "doesn't seem to monitor HTTPS websites.  Do you know of any alternatives?"  And that's referring to the question I answered last week about the utilities I use to monitor web pages for updates.  And it hasn't been updated for I think six  years, during which time, as we know, websites have moved from mostly HTTP to mostly HTTPS.  And unfortunately this WebMon utility, which exists outside of the browser, it's just a freestanding tool, does not appear to have been updated in order to handle secure websites.  So I just wanted to correct that and make a note of that.



The add-on Check4Change plugin for Firefox, and I'm not sure if it's available cross-browser, it doesn't care whether pages are secure or not because it's just using the browser's framework in order to repull the page however often you ask it to and check the marked region for any modifications.  But it looks like this little freestanding Windows app is sort of seeing its end of life because, unless it gets updated to do HTTPS sites, it's not going to be very useful.  And the question that I referred to earlier, Guillermo Garcia asks, from Security Now! 644, so a couple weeks ago, "How do you 'baby' your iPhone's battery?"  And you asked the same question, Leo.



What I mean by that is I am acutely aware that lithium-ion cells, while they definitely do not like to be overcharged, they really dislike being discharged.  We know that, unlike the nickel-cadmium and nickel-metal hydride technologies which preceded lithium-ion, those had a severe memory effect.  And so, for example, for the longest time, in the early days of portable phones, we all learned that it was better to deep cycle those batteries, running them all the way down and then putting them on the charger.  Everybody got trained, that's the way to handle batteries.



It turns out that's completely the worst possible thing you could do with today's lithium-polymer, LiPo, and lithium-ion batteries.  Those chemistries are killed by deep cycle discharging.  And I've had several friends who have said that phones and pads die on them.  And I note that their batteries are always down in the red.  And I've said, "Well, yes, because you're running the battery down."  They go, "What do you mean?"  I go, "That kills lithium-on."  You absolutely never want to do that.



So anyway, so when I say I baby my phone's battery, my lifestyle allows me to only have the battery, or the phone, off of a charger for maybe a couple hours a day, when I'm out getting a meal, or I'm driving from point A to point B.  I even have a cigarette lighter plug with a lightning cord, and I dutifully keep it plugged in.  So I'm very conscious about preserving the life of that battery, which is why I contend that mine's probably in really good health, despite the fact that it is a few years old.  Age itself is also known to upset lithium-polymer, lithium-ion.  Basically the lithium chemistry of our battery.  So even if you really kept it in good shape, it would, after five or six years, just age, there is a shelf-life problem, but it is exacerbated by deep cycling.  So for what it's worth - yeah.



LEO:  Don't chargers do micro cycling, though?  I mean, don't they, like, let it charge up and let it go down, let it charge up and go down?



STEVE:  Yeah.



LEO:  In little increments?  Does that not add up, though, to as much as a full charge and discharge in the long run?



STEVE:  I don't think so.  I can't speak definitively.  I know that I was very impressed that my Lenovo, that the Carbon X1, after I'd had it, and of course it was plugged in, for like a week or two, I remember turning it on, and I got a notice.  And it said:  "We notice that you seem to be having your laptop live on the AC line.  If that's going to be your normal habit, we recommend, for the sake of the battery, running it down to 50 percent.  And we will hold it there."  And I thought, wow, that's the nicest thing I've seen.  Because, I mean, that's really what you want is if there's any way to maintain it.



Again, if I were to, like, grab it and run out the door, I would have limited life use, and down at the scary end because I really don't want to hit bottom with that battery.  So in fact, when I am planning a trip, I flip it off of that mode.  It then brings the battery up to full charge, and off I go.  So it would be nice if we had a battery technology where none of this was the user's responsibility.  And unfortunately, Apple has tried to give us that.  Apple has just, "Oh, don't worry about the battery.  There's nothing you need to worry about it.  We've got complete management."  Well, but unfortunately they really don't because if the user interacts with it a little bit, as I do, for example, with my laptop, then I'm able to get much better long-term battery life.



LEO:  Yeah, yeah.  I mean, there are - I think there are battery monitoring programs on iOS, there certainly are on Android and computers, that can tell you what your battery capacity is compared to its original capacity, things like that.



STEVE:  Yeah.



LEO:  But I hope Apple does the right thing and gives us that information.  That would be much, much more useful.



STEVE:  And in fact, I'm glad you mentioned that, Leo, because they said they're not only going to give us the ability to turn off throttling, but they are going to surface their internal battery health metrics to the UI.  So we'll be able to see what it is that they think the state of our battery is.  Basically, they just sort of wanted to pretend this was not a consumable thing.  It didn't get old.  You never had to worry about it.  Besides, you'd be getting a new iPhone before this became an issue.  Now they're saying, okay, we're going to show you what's going on inside.  So that's very cool.



LEO:  I'll pass along one other note that I just saw that Ursula K. Le Guin has passed away.



STEVE:  Oh, wow, a major sci-fi author.



LEO:  Very, very significant.



STEVE:  I cut my teeth on her in my youth.



LEO:  Yeah.  I'm trying to think of - what was your favorite Le Guin?  I'm trying to think of what my favorite is.  She was 88 years old.



STEVE:  I was just going to ask how old she was.



LEO:  Yeah, 88.  Nebula Award winner.  Kind of a bete noire for people like Cory Doctorow because she was very actively promoting the rights of writers against people like Google Books and was always very angry about what she saw as copyright violation with Google Books.  But I'm sure there'll be some good obituaries we can read about.  She wrote her first published sci-fi story for Astounding Science Fiction when she was 11 years old.



STEVE:  I was just going to say, I was wondering how old she was because, I mean, I was reading her forever.



LEO:  Yeah, a long time, yeah.



STEVE:  Yeah.  She has a website bearing her name.



LEO:  I think a lot of it was fantasy kind of focused, of less interest.  "The Lathe of Heaven," that was a PBS film based...



STEVE:  Yeah, and as we know, I'm much more of a hard sci-fi person.



LEO:  Yeah, it's much more fantasy.  I do remember "The Lathe of Heaven."  I think it's probably the only one I ever read.  Still, a very important person.



STEVE:  Thank you for the heads-up.



LEO:  Sure.



STEVE:  So a listener, Darrel McQuienn, I guess, asks, he says:  "With respect to precision time, it's quite important in a lot of applications.  I do," he writes, "traffic control systems and astronomy, both of which require precision time."  And I got a kick out of that because of course there's precision, and then there's precision.  He's referring to my suggestion that one of the most obvious mitigations for all of this is to fuzz the application's access to what's called the RDTSC instruction, the Read Time Stamp Counter.  Nobody, nobody except crazy entropy harvesters or somebody doing this level, like branch cache miss cache hit resolution timing needs that kind of precision.  That timestamp counter runs at the system clock rate, which is, what, 3GHz often, or 2GHz?  So that's half a trillionth, is it, half a nanosecond, 500 picosecond or better timing that you just - nothing needs that.



So my point was we're not talking about, like, being off by a second or two, or a millisecond or two, or even a microsecond or two.  Even if we reduced or fuzzed timing down to the microsecond, the millionth of a second, that's enough to completely thwart the detection of whether data is cached or not.  That is, it's only by determining whether something is one cycle or 30 cycles at the resolution of the timestamp counter, so 30 nanoseconds.  Or what is that, .03 microseconds, that these decisions are being made.



So it'll be interesting to see.  I mean, I'm not - it would be better for Intel to fix the microarchitecture, I mean, the way I'm sure they will be moving forward, in order to maybe make the branch table buffer be also tagged with a process-context in order to solve the problem.  I mean, I'm sure there will be lots of clever solutions.  But one possibility is just not being quite so sure about what time it is.  But not quite so sure, even giving applications a microsecond would have to still be fine with traffic control and astronomy, but would completely make it impossible to see what was going on closely enough to execute these attacks.  And that's one 2,000th less resolution than they have now.  So it would completely blur it.  It'd be interesting to see if that mitigation alone would solve the problem.



And, finally, Markzip said:  "@SGgrc You say that DV certificates are free and easy, so HTTP should/will die.  But what about those of us on shared servers in hosting providers who are not offering Let's Encrypt?  These large providers have no plans to do so," he writes, "because it cuts in to their charging for certificates."  And I will suggest that the only thing that is still allowing them to do that is inertia.  There are alternative hosting providers, and there will be more, who are using Let's Encrypt and, as a benefit for luring people away from hosting providers that aren't offering free certificates, are offering free certificates.



So again, this is causing a sea change, and it will not be long before those providers who are still charging for certs are looking around at the hosting provider comparison charts, and one of the columns is "Offers free certificates."  And they've got a No in their column, where everybody around them is getting Yeses.  So they'll be moving to free certificates.  And again, further putting another nail in the coffin of HTTP. And, boy, it just - we're at a point where you just can't do HTTP anymore.  It's becoming wrong.  Okay.



LEO:  Time for a kitty cat.



STEVE:  Isn't that a neat photo?



LEO:  I love it.



STEVE:  Yeah.  That was from the EFF's page, I think.



LEO:  That's a caracal.



STEVE:  Dark Caracal, C-A-R-A-C-A-L.  And the authors explained:  "In keeping with traditional APT naming" - that's Advanced Persistent Threat - "we chose the name 'Caracal' because the feline is native to Lebanon, and because this group has remained hidden for so long.  From the Wikipedia entry, Wikipedia says:  'The caracal is highly secretive and difficult to observe and is often confused with other breeds of cat.'"



Okay.  So for anyone who's interested, in the show notes I have a link to the full 51-page, very, very detailed readout on this, which is posted at Lookout.com.  This was jointly announced research conducted by the EFF, our friends at the Electronic Frontier Foundation, and Lookout.  It describes with as much detail as you could ask, basically they captured all of these bits of malware and reverse-engineered them so that they're showing the command vocabulary and the response vocabulary that these different pieces of malware provide.  They track down the specific building in Beirut, the General Directorate of General Security, GDGS, where this is all being managed.  They found three or four personas, pseudonyms under which domains and hosting is provided and email is being routed through.  And then from those pseudonyms they were able to find other instances of those on the web, and basically assembled this spider web of thinuous - thinuous.



LEO:  I like it.  It's a new word.



STEVE:  Thinuous.  That's a thinuous connection - of tenuous connections in order to pull together the whole picture.  And really, I mean, this is the way this kind of research goes, where you just pursue each lead.  You gather the information.  You record it.  And then something else, you realize, ooh, wait a minute, that's using the same port as this.  And then you go over to Shodan, and you do a scan of that port, and oh, that finds a bunch of other machines.  And then you go look at them, and you say, hey, wait a minute, those IPs have the following provider in common, and that then tells you what provider is being used.  And then you go look at the provider.  And from that perspective, looking out, you can see more than you would have known to find before.  And so that's the way this goes.



So this report pulls this all together, all of these disparate leads - no matter how thinuous they were when they were initially found - and builds a picture.  So they find this Dark Caracal network which is, interestingly, reusing the same infrastructure - the command-and-control servers, IP addresses, hosting and database providers, domain registrars because they've got the whole bunch of spoofing domains and waterhole domains designed to trick people into not noticing, oh, wait a minute, where I'm getting my Flash player from is not actually Adobe, but something that looks sort of the same, unless I looked closely.



This infrastructure had been previously seen in another similar campaign which was targeting journalists, lawyers, and dissidents - so a little more on the political side - who were critical of the government of Kazakhstan.  The Dark Caracal effort has been conducting a multiplatform Advanced Persistent Threat.  And that's a term that I think maybe we first used after the Sony revelations, the idea that there was an APT that had gotten into Sony Pictures' network and set up shop and over a long period of time was performing surveillance from inside their network.



So this is an Advanced Persistent Threat surveillance operation, which they have verified now is targeting individuals and institutions across the world, not just focused on political targets.  They have located and rummaged through hundreds of gigabytes of data identified as having been exfiltrated from thousands of victims spanning more than 21 countries, including North America, Europe, the Middle East, and Asia.  And in their report they have a map that sort of gives a sort of a geographic sense of what the spread is.  Lots in the Middle East and Asia.  There's a mobile component of this Advanced Persistent Threat which is one of the first they have ever seen, which was executing global espionage at this scale.



Lookout, which is the partner of EFF here, focuses on mobile malware and mobile attack analysis.  They found that, in looking at the connections and the data, that this Dark Caracal had successfully compromised the devices of military personnel, enterprises, medical professionals, activists, journalists, lawyers, and educational institutions; that it was targeting governments, their militaries, utility infrastructure - utilities, power, gas, electric and so forth - financial institutions, manufacturing companies, and defense contractors.  So it's got its tentacles everywhere.  They found exfiltrated data, including documents, call records, audio recordings, secure messaging client content, contact information, text messages, photos, and account data.  So again, pretty much everything that was of interest to this kind of cyberespionage.



They also were able to piece together what the typical attack chain looked like, that is, from not having infiltrated a target, how does it proceed?  And they found that the attack chain that they typically see is what's underway, relying primarily on social media and phishing.  In some cases physical access is obtained to target systems, devices, and accounts.  But that's far and away the rarity.  Generally they're learning about their target.  They're figuring out who they are, what interests them, where they spend their time, and just doing that from looking at what's available in social media.  And then they began spearphishing, trying to lure them to a watering hole or to some phishing sites in order to begin the process of establishing a beachhead in their hardware.



Some of Dark Caracal's espionage technology appears to have been developed in-house.  And in fact there is a cross-platform Java-based tool that they called CrossRAT, R-A-T as in Remote Access Trojan, CrossRAT as in cross-platform Remote Access Trojan, written in Java.  It's the first time they've encountered that, appears to have been written by this organization.  And then there's other technology which is either borrowed from or purchased from the dark web.  A version of FinFisher that we've talked about in the past has also been used as part of this campaign.



The guys at Lookout first discovered the presence of something they named Pallas, P-A-L-L-A-S, which is their name for an implant used in multiple trojanized Android applications.  They spotted that last summer, in May of 2017.  And they have identified it in 11 different Android applications, including - and here's what's a little bit freaky - Signal, Threema, Primo, WhatsApp, and Plus Messenger, which is the Telegram messenger.  What they do is they, I mean, this group is good.  So they're taking those apps and trojanizing them, essentially installing Pallas into an application.  It remains fully functional as what it does.



So, I mean, if you think you downloaded Signal, but you didn't get it from the Google Play Store, you got it from a spoofed version of that store.  And so it operates, it functions, it's the Signal app in every way, except you're also carrying a trojan, which sets up communication with their command-and-control network, allows it to exfiltrate any data on your phone, enumerate the hardware devices, like how many cameras you've got, and turn on the microphone and surreptitiously record and send the data out of your phone.  And that's the case for Signal, Threema, Primo, WhatsApp, and Telegram.



So, I mean, this is like state-level surveillance, cyberespionage.  It also makes extensive use of a Windows malware known as Bandook, B-A-N-D-O-O-K	RAT, the Bandook RAT, and employs a continually evolving and changing global network infrastructure.  Many of the threads that they pulled on they found led to abandoned IPs or abandoned servers, which were at one point in use, but had been rotated away from.  So this suggests to them that there's some money behind this; that doing that, maintaining a constantly evolving footprint is definitely more secure, but also significantly more expensive.  You have to have a whole separate team that is evolving this entire structure continually.



They found that the command-and-control servers generally preferred the use of Windows and the XAMPP stack, which is the cross-platform stack, rather than the more traditional LAMP stack that is Linux-focused.  And their research produced more than 90 what they called Indicators of Compromise:  11 Android malware IOCs, these Indicators of Compromise; 26 desktop malware IOCs.  And so, for example, things like this Java-based malware for cross-platform compromise that affects Windows, Mac OS X, and Linux platforms; and then also 60 domain names, IP addresses, and WHOIS information that they all dredged up during this research.



They tracked down WiFi networks and SSIDs, the IP addresses these guys were using.  It turns out that they were almost all using one particular hosting provider.  I didn't have it in my notes here, but it is in the documentation, if anyone's curious.  And it's apparently, it looked like it was an Asian provider that gave absolutely no concern whatsoever for what content was being hosted on their servers.  As long as you paid the bills, they were open for business to you.  And very difficult for law enforcement to have any access to.



There are, and they found and have screenshots of, fully mature watering hole servers which are offering these malicious versions of these well-known messenger apps that sort of tended to be their focus, as well as phishing domains which closely mimic Facebook and Twitter.  So you're following a link from a message that you receive.  You think you are going to Facebook or Twitter.  Unless you are really paying attention, that's not where you are.  You then log in, and they essentially are then leveraging your trust in your belief at where you are for everything that you're presented and the links that you then get.  And from that point on, they say, oh, you need to update your Flash player in order to view the latest content that we have.  Click here, and we'll take you to Adobe.  And in fact they take you to an Adobe clone site, where you download, obviously, whatever it is that is malicious and that is now inside your system.



So it relies primarily, that is, as I mentioned before, getting a foothold primarily relies on social engineering via posts in a Facebook group and WhatsApp messages in order to compromise target systems, devices, and accounts.  And they wrote that, at a high level, the attackers have designed three different kinds of phishing messages, the goal of which is to eventually drive victims to a watering hole controlled by Dark Caracal.  Interestingly, neither the desktop nor the mobile malware tooling use zero-day vulnerabilities.  That isn't the way they get in.  They're not leveraging them or assuming them.  They're simply downloaded instead of the intended application, and then they rely on whatever permissions have been granted at the time of the installation in order to give them access to sensitive user data.



So anyway, it is a - oh, the one chilling thing that I said I mentioned, at one point they were enumerating what the Windows client malware was obtaining.  In addition to full transcripts of all the messaging that's being done through the desktop messaging, they get screenshots of the desktop.  And at one point they said this data included full screenshots taken at regular intervals and uploaded to AdobeAir[.]net.  And the authors wrote:  "By observing these images, it is disturbingly simple to watch a victim go about his daily life and follow that individual every step of the way."



They wrote:  "Not only was Dark Caracal able to cast its net wide, it was also able to gain deep insight into each of its victims' lives.  It did this," they wrote, "through a series of multiplatform surveillance campaigns that began with desktop attacks and pivoted to the mobile device.  Stolen data was found to include personal messages and photos, as well as corporate and legal documentation.  In some cases, screenshots from its Windows malware painted a picture of how a particular individual spent his evenings at home."



So anyway, this is obviously, as I said at the top of the show, this is not something that is affecting us individually at the level that Spectre and Meltdown do.  But this is a look inside a mature, nation-state level, well-financed, cyberespionage campaign.  This of course is not an outlier.  We have to assume this is what all significant nation-states, and even insignificant ones, are probably doing to varying degrees and varying levels of success.



Unfortunately, this is the downside to this amazing global network and PCs and mobile devices and the free availability of software that does useful things.  The flipside is it can all be turned against us.  And as individuals, I don't think any of us are probably targets or victims.  But it's worth being aware that, when targeted, and we've talked about this before, it is very, very difficult to never make a single mistake.  All it takes is a single wrong click on something that you have been given every reason to believe is safe.  And you can then have something installed on your system that will remain as an APT, an Advanced Persistent Threat, moving forward, and give somebody you don't intend to have given access to essentially every detail of your life.



LEO:  So it's coming out of Lebanon.  Does that mean that's - who made this, you think?



STEVE:  It's not clear.



LEO:  Lebanese?  I mean, I don't think this is a - is there a Lebanese secret police that has interests around the world?  I mean...



STEVE:  Well, I would argue that any large country does now.  I mean, we know that North Korea has been bragging about their cyberattack people.  We know in the U.S. we have a whole cyber program that is spun up and running.  We know that China is active.  And we know that Russia... 



LEO:  It doesn't necessarily have to be coming from Lebanon to be, I mean, they pinpointed Lebanon, but that doesn't mean that's who's originating it.



STEVE:  Well, they pinpointed the intelligence services building in Beirut.  So, yeah.  I think in this case they were stopping just short of providing - it wouldn't be until you got a formal declaration of, yes, we're doing this, and here's our annual budget for it.



LEO:  Right.  Responsibility, yeah.  But then in that case it sounds like something that was targeted perhaps at Lebanese citizens that just kind of got loose?  Or maybe somebody got the code and is exploiting it? 



STEVE:  No.  They're, I mean, if you read this paper, I mean, it's all there.  They have a global network targeting more than 21 countries where they're actively surveilling individuals across the globe - in military and government, in power utilities.  I mean, this is the nature of the world we're in today.



LEO:  Okay.  It's interesting that GDGS somehow has some interest in what's going on in some distant country.  Just surprises me a little bit.



STEVE:  Yeah.  I just think that's the nature of spooks, national spooks.  They're like, oh, yeah, we have our hooks into Pacific Gas & Electric in California, just in case that's of some use to us.



LEO:  Yeah.  Interesting.  Okay.  Well, there you have it, the Dark Caracal.  Steve Gibson is at GRC.com.  That's where you can get InSpectre, his free utility to check the status of your Meltdown and Spectre mitigations and the impact.  And there's lots of information in the app, and it's free.  In fact, he's got a ton of free stuff.



But while you're there, check out SpinRite, the one thing, the one thing he charges you for.  Might be the most useful.  It's the world's best hard drive maintenance and recovery utility.  You'll find that there.  Also copies of this show, audio and transcriptions, handcrafted by Elaine Farris:  GRC.com.  If you want to leave a question or a thought or a comment for Steve, it's on the Twitter at @SGgrc.  And he accepts DMs from strangers.  I've told him to stop it, but he persists.  Nevertheless, he persisted.  Tips and all sorts of stuff coming that way:  @SGgrc.



We have audio and video of the show if you choose to see our smiling faces at TWiT.tv/sn.  You can watch us do the show live every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  You can be in the studio audience if you email us:  tickets@twit.tv.  We've got a couple of nice fellas here, Chris from New Waverly, Texas, and Mark from Oakland, California.  Welcome.  Good to have you both.



If you can't be here live, if you can't listen live, you can always get on-demand audio and video from your favorite podcatcher.  Just subscribe to Security Now! and complete your set.  You need all 647 episodes to really say - we get emails, I'm sure you do, too, every week from people saying, "I just completed listening to all the back episodes."



STEVE:  Yeah.



LEO:  That's a major effort.



STEVE:  That is a labor of love, they call that.



LEO:  Forty-three days, three hours, 20 minutes, and one second.  And counting, according to [crosstalk].



STEVE:  Of nonstop audio.



LEO:  Nonstop audio.



STEVE:  And counting.  Yay.



LEO:  Thank you, Steve.  It's great to see you again, and we will see you all next week.



STEVE:  Thanks, buddy.



LEO:  On Security Now!.  Now you can say it.



STEVE:  Thanks, buddy.



LEO:  Take care.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#648

DATE:		January 30, 2018

TITLE:		Post Spectre?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-648.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss continuing Spectre updates, how not to treat Tavis Ormandy, a popular dating app where you'd really hope for HTTPS but be surprised to find it missing, the unintended consequences of global posting of fitness tracking data, gearing up (or not) for this year's voting machine hack fest, another record broken by a cryptocurrency exchange heist, bad ads and fake ads, the unclear fate of the BSD operating systems, a caution about Dark Caracal's CrossRAT Trojan, another way to skin the Net Neutrality cat, a bit of errata and miscellany, one of the best SpinRite testimonials in a long time, and some closing-the-loop feedback from our terrific listeners.



SHOW TEASE:  It's time for Security Now!.  A security roundup today, all sorts of stuff to talk about.  The latest on Spectre.  Steve hears from a programmer who's hurt by his thoughts.  He will apologize.  And why Net Neutrality may actually find a savior in the state legislatures.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 648, recorded Tuesday, January 30th, 2018:  Post Spectre?



It's time for Security Now!, the show where we get together with this guy, this guy right here, Steve Gibson, and talk about security and privacy online and bugs and all sorts of stuff.  It's the geekiest show we do, thanks to the geekiest guy I know.  Hey, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for Episode 648 as we wrap up January and head into February in two days.  We're still trying to get the whole Spectre and Meltdown thing behind us.  And I'm really wishing that it wasn't two different words.  I wish there was like a nice combined name for this, just because it's sort of awkward to keep saying Spectre and Meltdown.  But it's really Spectre which has turned out to pose the biggest problem.  Although Meltdown poses the greatest performance hit, it's easier to fix.  So I titled this week's podcast "Post Spectre?" sort of in the hopeful...



LEO:  Question mark?



STEVE:  Hopeful, yes, sense that maybe finally - but no.  We do have some additional news about Spectre that we have to talk about.  But some other interesting stuff, too.  No really massively dominant story, although a bunch of interesting stuff for us to talk about, among them how not to treat Tavis Ormandy when he's trying to help you.



LEO:  Ooh, don't mess with the Tav.



STEVE:  Don't mess - he'll just go take a shower, and you'll regret it.  A popular dating app where you'd really hope for HTTPS encryption, but be surprised to find it missing.  The unintended consequences, and I'm sure you're up to speed on this story, of global posting of fitness tracking data.



LEO:  Yes.



STEVE:  And how that can bite you.  Gearing up or not for this year's voting machine hack fest during the August upcoming DefCon.  It's turning out to be a bit of a problem this year.  We have another record broken by a cryptocurrency exchange heist.  Bad ads and fake ads.  The unclear fate - which this story caught me by surprise, but sort of certainly interesting to our listeners - the unclear fate of the BSD operating systems.



LEO:  What?



STEVE:  Yeah.  They're not doing that well.  We also have a caution about last week's discussion of the Dark Caracal, in this case the CrossRAT trojan.  Another way to skin the Net Neutrality cat.  And in fact, just while we were setting up, another piece of news broke on what states are doing on their own about Net Neutrality.  We also have a bit of errata, some miscellany, one of the best SpinRite testimonials we've encountered in quite some time.  Historically we've had some really fun ones.  Haven't really had somebody who exercised his creative writing abilities for a while, but we got one this week.  And then we'll wrap up with some closing-the-loop feedback from our terrific listeners.  So I think an interesting, fun podcast.



So we've been having a lot of fun in recent weeks with our Pictures of the Week.  And this occasion there were just too many.  We will have an overage of Pictures of the Week for at least several weeks while we catch up.  So it was difficult to choose which one.  One of the things that has happened is that the Meltdown and Spectre vulnerabilities have prompted the creation of a number of interesting twists on the Intel Inside logo.  And of course so we have Meltdown Inside and Spectre Inside for this week's picture.  Next week, unless it's preempted by something even better, we have another take on the older Intel Inside logo.  And I almost used that for this week.  But I thought, well, who knows what's going to be in store for next week.



LEO:  I don't remember the older Intel Inside.



STEVE:  You will when you see next week's Picture of the week.  It brings back some fond memories.  But anyway, so just some more fun about Intel and their logos.



A couple things have happened on that front, on the Spectre and Meltdown front.  Actually, Meltdown, as I mentioned at the top of the show, that got resolved quickly, although at some performance cost.  It was the most expensive in performance for older machines that didn't support the PCID and Invalidate PCID features which were added, as we corrected the record on it was Haswell and subsequent have the Invalidate PCID instruction which works with the PCID feature that was introduced earlier but never used.  So as long as you have that, you are able to mitigate the Meltdown problem without much performance hit.  Earlier machines did take a hit, but that pretty much resolved things.



What happened, though, as we have been discussing the last couple weeks, is that Intel stumbled in - and no doubt they were under tremendous pressure and a great hurry.  Although, as we understood the coverage, there were months going by in the late summer where they knew there was a problem, yet that hadn't come out onto the surface.  Maybe they weren't ready to release yet, or maybe they were testing.  Who knows.  But as a consequence of that, as we were discussing last week, they made mistakes in the release of the firmware which affected apparently a large subset of their processors, maybe all of them.



Unfortunately, the disclosure was sort of deliberately murky, and it was squeezed out through the PR funnel that left us with more questions than it did answers.  Since then, Microsoft has formalized their withdrawal of the support for that firmware.  In other words, last week I was talking about, okay, well, if you're not having any problems, then just probably holding your breath is good enough.  The Spectre liability, the Spectre vulnerability is protected.  It's mitigated.  And if your system is not having what Intel sort of obtusely says "more frequent reboots than usual," whatever that means, then leave well enough alone.



If you are having a problem, as we discussed last week, you could back out to an earlier version of the firmware offered in an earlier version of the BIOS from your manufacturer, which would probably be around since you would have just been able to update recently.  And of course Intel, we discussed last week, formally said to all their partners, don't give anybody else this firmware.  We're working on, like we now understand what's wrong with the firmware in a couple cases.  We're working on fixing all of this.



Well, Microsoft since has gone a step further.  They have issued a Windows update to disable the mitigation against the Spectre Variant 2, which is the focus of all of this.  So their update is literally titled "Update to Disable Mitigation Against Spectre Variant 2."  They wrote in this:  "Intel has reported issues with recently released microcode meant to address Spectre Variant 2.  Specifically, Intel noted that this microcode can cause" - quote, and they're just quoting everybody down some chain - "'higher than expected reboots or other unpredictable system behavior' and then noted [says Microsoft quoting Intel] that situations like this may result in data loss or corruption.



"Our own experience," Microsoft says, "is that system instability can in some circumstances cause data loss or corruption.  On January 22, Intel recommended that customers stop deploying the current microcode version on affected processors while they perform additional testing on the updated solution.  We understand," says Microsoft, "that Intel is continuing to investigate the potential effect of the current microcode version, and we encourage customers to review their guidance on an ongoing basis to inform their decisions.



"While Intel tests, updates, and deploys new microcode, we are making available an out-of-band update today" - and this is their KB4078130 - "that specifically disables only the mitigation against" - and then we have the CVE number.  And I've got these things memorized.  I didn't ever think I was going to be memorizing CVEs.  But there's three of them.  This is the Variant 2 of Spectre ending in 5715.  That's the bugaboo in all of this.  And that's the branch target injection vulnerability.  



Microsoft says:  "In our testing, this update" - that is, this one they've just released a couple days ago out of band - "this update has been found to prevent the described behavior."  Meaning the higher than expected reboots, which is a good thing.  "For the full list of affected devices, see Intel's microcode revision guidance.  This update," Microsoft says - that is, their just released out-of-band yet another update - "covers Windows 7 (SP1)" - that is, the most recent service pack of Windows 7, the only one you'll ever get - "Windows 8.1, and all versions of Windows 10 for client and server.



"If you are running an affected device, this update can be applied by downloading it" - this is not going to be pushed to you, so this is the reason I'm bringing this all up is that our listeners have to go get it if they want it - "can be applied by downloading it from the Microsoft Update Catalog website.  Application of this payload specifically disables only the mitigation against" - and then the CVE ending in 5715, that is, the branch target injection vulnerability.  I have a link to it in the show notes.  I'm sure, Leo, that the search for it will find it.



Now, what's interesting is Microsoft also goes on in this guidance to describe registry changes which will also do the same thing.  And that is exactly what GRC's InSpectre utility does.



LEO:  We should mention you probably shouldn't install this unless you're experiencing these issues, though; right?  Or do you recommend people roll back even if you're not getting reboots?



STEVE:  Okay.  So, I mean, this is why this is all somewhat controversial, that is, it isn't just cut and dried, black and white.  There is still no known exploitation of any of this in the wild, even the Meltdown that was the easy one to do, that is, the easiest one to exploit.  So these all remain theoretical vulnerabilities for which there is no reason to believe anyone is actively in danger from.  So the biggest danger apparently is from turning on this feature with both the faulty Intel microcode and Microsoft's enabling this feature which causes some system instability.  That's the biggest problem that has ever been seen from all of this is the mitigation of this problem.



So my Twitter feed has been full of feedback from people.  I've seen some reports of people believing their system became less stable, who then disabled or backed out, and things seemed to be better.  But it's all kind of like no one has been able to firmly find a test for the problem or invoke the instability and conclusively demonstrate that, oh, now it's gone again.  So I think my advice is what it was last week, although now we have another piece.  Last week was back yourself out of the microcode if it's causing a problem.  Now, since then, Microsoft has said we'll back you out by giving you a link you can click to give you another "go get it if you want it" update which does the same thing as just turning it off.  That is, even if it's in your microcode, if you turn it off, the instability goes away.



Now, that's news.  I did not - I couldn't assert that last week because I always, as we know, I built that in to InSpectre from the beginning.  There's cute little buttons down there at the bottom of the InSpectre app which you can just press, and it'll turn off the Spectre or the Meltdown, you've got a button for each, mitigation.  So now Microsoft has confirmed that turning it off prevents the instability.  So you could either download Microsoft's thing that turns it off, and then I'm not sure how you turn it back on again, which is one problem.  I mean, I guess maybe...



LEO:  Presumably there's another update coming from Microsoft at some point that would fix it.



STEVE:  Where it's like, okay, everything's finally good.



LEO:  Whenever.



STEVE:  Exactly.  Whenever that happens, yeah.  So you could use InSpectre to flip it off, and then you could flip it back on again.  But arguably you would be waiting in any event for newer microcode which doesn't have this more frequent rebooting side effect.  And then Microsoft, once everyone's sure we've got it this time, Microsoft will send us another update out of band or in band or some, you know, who knows what the band will be playing at that point.  And then we could all move forward.



Also InSpectre got a revision because we found a bug in the Creators Update, not the Fall Creators Update, that is, the Fall Creators Update is 1709 of Windows 10.  It turns out that 1703 had a bug.  The 32-bit support in Windows turns out to be completely different than the 64-bit support.  Back once upon a time, the Windows API was 16 bits.  And then of course they went to 32.  Those were very similar.  That is, you were able, in fact, you could load 32-bit code into a 16-bit process and call it.  But you can't do that with 32 and 64.  They are completely different.



Well, it turns out that InSpectre, my code, is a 32-bit app.  And the 32-bit app, as all 32-bit apps do, calls a 32-bit API.  Well, there's a function in Windows, a DLL, ntdll.dll, which exists in both 32- and 64-bit flavors.  And we discovered, actually it was a researcher in the GRC newsgroups, discovered that, because users were reporting who were using the Creators Update 1703 that InSpectre was saying they were not protected.  But the 64-bit version was saying they were protected.  Well, it turns out that there's a bug in Windows 32-bit implementation of ntdll in the Creators Update, which they fixed in the Fall Creators Update, but it persists in the Creators Update.



So I had to scramble around, and I wrote my first 64-bit assembly code last week.  I created a 64-bit probe which is contained within the 32-bit InSpectre so that, if it sees it's on a 64-bit system, it launches this 64-bit probe into the system, which is the only way for it to get a view of reality in light of this buggy implementation of ntdll.dll in the Creators Update.  So we're at Release 6.  Oh, and Leo, while I was there, I also implemented the fix that we talked about last week which is inverting the sense of Yes and No in the presentation.



LEO:  Oh, good.  Nice.  I'll download it right now.



STEVE:  So if you get a green Yes, that's good, instead of getting a green No, which was confusing.  So now if you get Yes/Yes, that's the good news.  And the no's with an exclamation point are red.  So that's been fixed also.  And we're at about 355,000 downloads.  So we're trucking along and doing well and continuing to get good attention and coverage.  And it's helping people.  So now we know that, if you feel that your system is unstable, you don't have to back out of your microcode.  You don't have to update and go out and get the out-of-band patch unless you want to.



And in fact, it may be that that thing only sets the registry.  I don't know what it does.  But they all - because Microsoft also said, "Or you can do this to the registry."  Which InSpectre does for you, and safely, and reversibly.  So lots of options for our users.  And it's confusing.  There's no known exploitation of these vulnerabilities.  The ones that are causing the biggest problem would be the hardest to exploit, so we would expect to see them maybe taking longer to happen, if they even ever do.



The biggest problem, as I mentioned, is the cure is worse than the disease in this case for many people.  All of our systems are running more slowly for apparently no good reason at the moment.  But again, better safe than sorry, I think.  In this case, certainly if you suspect that your system is misbehaving in any fashion, then turning off the Spectre mitigation until we get updated firmware from Intel certainly would make sense.  And Microsoft is sort of saying, you know, data loss or corruption.  Meaning that's kind of more scary.  At least a reboot is like, ooh, whoops, we crashed.  If it actually could cause, and I think that's what Microsoft is saying, data loss or corruption, I don't know if that's with the reboot or silently without it bothering to reboot, but that's of concern, too.  So, yeah, I don't know.



LEO:  Yeah.  I'd heard the same thing.



STEVE:  Yeah.  So anyway, that's an update on the gift that keeps on giving the industry.  And certainly once - and I'm sure that Intel is working hard.  They said they understand what's going on.  They're not sharing it with us, which is really my only source of angst is it would be - we're just sort of dealing in a vacuum, and this podcast likes to have facts to deal with.  And so, like, oh, it's worse than it used to be.  More reboots than usual.  What?  You know, it's like, okay.  Okay.  So speaking...



LEO:  We should mention that we were talking about this on MacBreak Weekly, and I knew you heard this.



STEVE:  Oh, good, what?



LEO:  Apple had wisely avoided using the Intel fix.  They did the calculus that we've kind of done which is, well, since there's nothing out in the wild, let's patch it on WebKit on our browser because that's the only exploitation vector that we really know about, and we'll wait until Intel gets it right or, I mean, it sounds like they kind of sensed there was something wrong with the Intel fix, and they didn't...



STEVE:  Well, and remember that Google was also very quick with their very clever Retpoline solution.



LEO:  Right, right.



STEVE:  Which does require code recompilation, but Apple is able to recompile their code in order to implement that fix.



LEO:  Yeah, so, hmm.



STEVE:  So Tavis, our great buddy at Google, who just everywhere he looks it's like with this amazing fine-tuned precision ability to find vulnerabilities and help the industry get smarter, has decided to take a look at highly popular online gaming, thinking, eh, nobody's looked over there with his level of expertise.  It turns out that last month, last December, he took a look at Blizzard's company-wide, distribution-wide download and install and maintenance facility and found a significant problem.  This affected World of Warcraft, Overwatch, Diablo III, StarCraft II, the entire catalog of extremely popular Blizzard online gaming.



He reached out to them as he does for full responsible disclosure, opened a dialogue, explained the problem that he found, provided them with a proof of concept which allowed - get this - any website to leverage the lack of strong authentication in Blizzard's entire suite of gaming to allow it to download and install and run arbitrary code on any gamer's machine.  Whoops.



Unfortunately - and who knows what the politics is behind this.  But I don't think whoever it was he talked to at Blizzard knew who he was because they stopped communicating with him.  They went silent, cut him off, and then implemented a fix that didn't work, without any further interaction.  They then went public with their great ballyhooing, we care about security, we fix things.  And Tavis said, uh, no, you didn't.  That's not a good fix.



And the bottom line is, after a little bit of embarrassment and backpedaling, somebody more mature or more aware of what's going on in the industry said, oh, shoot, yes, Tavis knows what he's talking about.  Apologies were made.  Dialogue was reestablished.  Blizzard then said that what they put out was something that they had around for a while, and it wasn't really meant to be the permanent fix, but they're doing one now, and they'll have it online soon.  And Tavis said, well, okay, good.  And so they've reestablished communication with him.  And with any luck they will involve him in their proposed solution this time, and he can take a look at it and say, yes, that looks good.  And this is of interest to all other major online gaming companies because Tavis has since decided that he's going to look at the other ones.



So if you're not Blizzard, and you have been watching this happen, make sure Tavis has a way of getting a hold of you, and listen when he tells you that he's found a problem.  Don't be embarrassed.  Don't suddenly pull the rug in over yourself and hide.  Just work with him, fix the problem, and that'll be the path to least resistance.  So those of us in the security industry, and we talk about this all the time, have learned that - certainly, for example, the guys at LastPass respond within minutes of Tavis contacting them and saying, whoops, I found a problem here, as does everybody in the security industry.  So Blizzard probably now understands that Tavis and Google and Project Zero are valuable components of this industry that should be taken advantage of when offered freely, as they are.



Some researchers at a security firm in Tel Aviv named Checkmarx took a look at Tinder's iOS and Android mobile apps and were surprised to find that it lacked HTTPS encryption, if you can believe it or not.  Anyone sharing the same WiFi as an iOS or Android mobile Tinder device or Tinder app user can see your Tinder photos, add their own into the photo stream, and track what you do on Tinder.  They're not using encryption to keep the photos safe from anybody who wants to eavesdrop.  And although apparently they are using encryption for encrypting the actions of a Tinder user, the encryption is spotty.  And it turns out that even looking at the encrypted stream it is possible to ascertain what the user is doing.



So Checkmarx produced a proof-of-concept app called TinderDrift which is demoed on YouTube which reconstructs any Tinder user's online session of what they see and the way they respond to what they see, essentially unmasking any person's behavior who is using Tinder on a shared Ethernet medium like WiFi.  So although, again, the swipes and matches are encrypted, Checkmarx explained, and approved, hackers sharing the same WiFi are able, essentially, to completely unmask everything that is being done.  It turns out that the specific patterns of bytes that represent a left swipe, a right swipe, a super like, whatever that is, and a match, are all clear.



Even though technically they're encrypted, nobody at Tinder really gave the security of their application much thought, apparently, or any real concern over protecting the privacy of their users.  The Verge in their coverage asked Tinder to respond to this.  And so Tinder did respond, saying that the unencrypted photos are profile pictures, and Tinder is a free global platform, so the pictures are "available to anyone swiping on the app anyway."  Of course, what isn't free and global are the individual Tinder users' choices and reactions to those unencrypted, unprotected photos which this research demonstrates was poorly concealed.



So I'm glad, again, this is, I mean, we just keep seeing example after example.  I mean, we just talked about it with Tavis and Blizzard, and here now Checkmarx in Tel Aviv and Tinder, where we have to have oversight.  We have to have an environment where security firms and researchers are able to examine these things.  And I'm thankful that they're willing to volunteer their time and do so.  They bring some beneficial positive press to themselves for doing so, and in the process it's benefiting the users of these applications for which it's clear not enough attention has been given to security.



Which is the perfect segue to, Leo, this was - for a while I kept trying to sort of ignore this story because I thought, really, this is bad?  This is a story?  Finally I was just overwhelmed by it.  It was without a doubt the most tweeted news item this week.  And that is the unintended consequences of globally connecting your fitness tracking devices, and what happens when arguably position-sensitive personnel are tracking their fitness with them, specifically U.S. military personnel.



Turns out that the Pentagon, I mean, military personnel are probably already fitness-oriented, and they're going to be using Fitbits and similar tracking devices.  Turns out that the U.S. Armed Services has even taken to giving personnel these devices to encourage them and their fitness.  And what came to light was that there is a firm that had been collecting all of this data and aggregating it globally and pushing it through mapping software.  What it turns out you're able to do is essentially it creates a heat map of where people are all over the world who are using fitness tracking devices.  And so my first thought was, okay, well, yeah, duh.  So what?



But it turns out there are many places where our military would not want confirmation of their personnel.  And although this, I mean, this doesn't deanonymize these people, you can't use it to prove what is going on somewhere, any bad guy can pinch and zoom and pan around and wonder, okay, wait a minute, what is going on here, and what is going on here, and what is going on here, and what is going on there?  So it certainly does clearly represent a leakage of information of the potential position of people which you would not otherwise have.  You might presume to be maintaining radio silence where unfortunately your fitness tracking device knows, is receiving GPS coordinates, storing it, and then uploading it somewhere where it's being aggregated.



And so this sort of creates an end-around disclosure of the position of these things which may be worn by military personnel, maybe in locations where we would rather not have, or, I mean, anybody would rather not have any disclosure that people are running along a certain trail in a certain location in the world.  And yet now it's all public.



LEO:  I wonder if you can fault Strava for releasing this data.  The other question we had is - because Strava you can turn on private so that you're not sharing.  But it's not clear whether  this data released - it's private because it's not any individual.  But if it's in aggregate, including private information, in other words, if it would be enough for the military to set private and then go for a run, and but still Strava has a heat map.  It sounds like that's what happened.  And that they released it is to me, like, come on, guys.  Nevertheless, the military has changed their procedure, and they will be strongly encouraging people not to do that anymore.



STEVE:  Well, and I'm sure, no doubt, that military minds are themselves pinching and zooming and looking to see what is now in the public domain.  I mean, now...



LEO:  Well, all this satellite data was already out there; right? 



STEVE:  Right. 



LEO:  So they know, I think they know where the bases are.  The one weird use case was if a base was pretending that it was closed, and then you saw people running around...



STEVE:  Like deliberately being dark.



LEO:  Yeah.  But like, if you look at, on the Strava heat map, if you look at the Pentagon, it's actually quite interesting.  We showed it yesterday on TWiT.



STEVE:  Oh, cool.



LEO:  And you could see people running around the Pentagon, but it's completely dark inside the ring.  And at first I thought, well, nobody's getting exercise inside the ring.  But really it's not that.  It's that that's radio hardened.  Nothing's getting out of the ring.  Of course.



STEVE:  So no device that is in there knows where it is, yup.



LEO:  Doesn't matter if you're wearing your Fitbit.  Yeah, of course not.  And I bet you if you looked at the same thing, if you looked at the White House or anyplace they're trying to keep it secure; right?



STEVE:  Yeah.  And at the same time, black zones are information, too.



LEO:  That tells you something, too, yeah.



STEVE:  Yeah.



LEO:  That's interesting, yeah.  Leave it at home, kids.



STEVE:  So anyway, I mean, this is the consequence of a world where everything is connected to everything else, and we're still learning how to handle the power of this kind of information.  Speaking of which, voting machine makers...



LEO:  Oh, this is - 2018 is nine, 10 months away; right?



STEVE:  Yes, are telling - yeah.  Voting machine makers are trying to stifle the sales of decommissioned and resold voting machines by threatening eBay sellers with litigation if they sell these machines, telling them that it is illegal to do so, which it is absolutely not.  So we all remember all the fun we had last year, after last year's DefCon, which was the largest of the Voting Village - DefCon hosts this Voting Village competition where they collect just a completely heterogeneous assortment of voting machines and tell the hackers, okay, have at it, kids.  Let's see what you can do.



And last year's was the biggest by far.  It had been going on quietly at DefCon, but it sort of really got a lot of attention last year.  And what we learned and had fun with on this podcast was the horrifically poor security that the machines actually had.  Well, we had fun with it.  Of course the voting machine manufacturers did not.  They are not happy.  And as I said, they've been actively threatening sellers of used and retired machines with legal action should their machines be resold, even though doing so is completely legal.



So remember last year we talked about this.  I mean, there were machines that all had, I think it was Sequoia machines all had the same hard-coded password across, I mean, like, all of them had it.



LEO:  Well, that's for convenience, you see.



STEVE:  Correct.  You wouldn't have to write all those pesky passwords down.



LEO:  Just put it in the manual.



STEVE:  Yeah, yeah.  Some lack any security, apparently, at all.  Some could be penetrated without physical contact to the device.  In some cases a USB thumb drive had to be inserted, but not all.  Some just offered you the convenience of WiFi.  Many had software that was riddled with well-known vulnerabilities that have been documented for years, if not decades, that had never been patched or updated.  Most machines had never been wiped.  And in some cases there were machines that had - there was one case had a 600,000 voter registration database still present in it that was available for exfiltration from this machine.  I mean, so it was an embarrassing catastrophe for voting machine companies and voting machine security in general.



Well, in preparation for this year's upcoming DefCon, the Voting Village organizers indicated during their own talk at Shmoocon, they told attendees at the Shmoocon hacking conference that they were having a surprisingly difficult time preparing for this year's upcoming DefCon Voting Village event because they were having a hard time acquiring machines after what happened last year.  And what they learned when trying to buy machines was that the, as I have said now, the manufacturers were threatening one of the most popular sources of these machines for resale, which was eBay, eBay resellers receiving notices threatening them with legal action should they sell those machines.



So for what it's worth, if we have any eBay resellers within earshot of this podcast, there is nothing illegal about reselling these machines.  It is perfectly legal, as is their hacking.  And I will say again, this is the only way we're going to get security.  I mean, until recently, these electronic voting machines have been allowed, I mean, even from well-known companies like Diebold have been able to say, you know, we're a security company.  Trust us, these machines are secure.  Turns out absolutely not true.  Remember, there was not a single machine that withstood attack last year.  Not one.  Some of them collapsed within minutes.  Some took maybe an hour or two.  Not a single one survived attack, which is just - should be just beyond embarrassing for these companies.



And so my feeling, of course, is that no machine should be allowed to be used that hasn't had full, well, first of all, they ought to be open source.  They ought to be fully scrutinized.  They ought to have academia looking at them, saying yes, we find no problems with these.  And then that machine should be produced.  So this notion of proprietary software in voting machines for something like U.S. elections, that absolutely has to end.  It is the wrong way for this to have been done.  And these chickens are coming home to roost.



So we have broken a record again on the largest heist in a cryptocurrency exchange.  We all recall when, four years ago, back in 2014, Mt. Gox, which at the time was one of the largest bitcoin exchanges, ended up filing for bankruptcy after admitting that it had lost $450 million worth of bitcoins.  Well, it turns out that now, just a few days ago, Coincheck, which is a Tokyo-based cryptocurrency exchange, has suffered what appears to be the largest hack of cryptocurrency loss in history, totaling what would be $532 million in digital assets.



In a blog posting, they acknowledged or confirmed the cyber heist.  However, they did not explain how the tokens were stolen, and then froze most of the assets and their services, including deposits, withdrawals, and trade, because no doubt they want to figure out what it was that happened.  Interestingly, though, they did not freeze their trading of bitcoin, which did take a 5% hit in its market valuation on this news.  So anyway, I mean, I guess this is the problem with - it's an acknowledged problem, even though this is a huge, more than half a billion dollar loss.  We're seeing people having problems maintaining the security of their cryptocurrency constantly.



We have often said that it is incredibly difficult, if not arguably impossible in a complex enough environment to have true security, and that you only have security to the degree to which nobody is trying to penetrate the barriers that you have erected to protect yourself.  And so money is money, and where there's money there's going to be pressure to penetrate security.  So it looks like, I'm sure, that other exchanges are taking lessons from these massive losses and doing everything they can to protect themselves.  Individuals need to do the same.



Those individuals who have substantial assets in cyber currency need to really, I mean, the only thing you can do, I think, is to store it offline.  And that's a common refrain of mine when, for example, I've talked about the QR codes that I use for registering new devices' authenticators.  I understand it's inconvenient to have them offline, but it's the only way to be safe.  And the same is certainly true of cryptocurrency wallets.  You want to keep them offline when they're not in use because, if it's online, it's vulnerable.  So certainly these guys, Coincheck, have learned that lesson, or presumably learned that lesson.  I hope other exchanges have.



In I guess what would be foreseeable, it came to light from an advertising security company - and I didn't know there was an advertising security company, but I'm glad there is advertising security.  Confiant is the name of this company that discovered the existence of a very large and malicious advertising operation which they named Zirconium, which I thought was kind of a fun name for a fake advertising, I mean, a huge fake advertising effort.



This Zirconium operation created 28 completely fraudulent fake advertising agencies and established them with full-blown apparent credentials, Facebook pages, LinkedIn accounts.  They created fake people to populate them and created fake personas for them, basically erecting an entire 28 separate agencies in order to funnel and mask their advertising.  It was so large that Confiant determined during the peak of their activity the group was purchasing 62% of ad-monetized websites on a weekly basis.  So just a large machine.



Confiant believes that about 2.5 million users who encountered Zirconium's malicious ads were redirected to a malicious site, with about 95% of those 2.5 million users being based in the U.S.  Essentially they purchased over a billion ad views last year in 2017 and used these malicious ads to redirect users to tech support scams and malicious pages peddling malware-laden software updates and software installers.



So unfortunately what was done was, and this is the technology we've talked about often, is that the ads have the ability to run JavaScript.  And they used something that they described as JavaScript code executing a forced redirection to hijack visitors' browsers' queries that were trying to just display an ad on an ad that was on some website.  The browser was redirected in a series of bounces, first to a server, an intermediary domain that would fingerprint and classify the incoming traffic, then redirect the user to another domain operated by Zirconium where they would then be sent to a third domain which was an affiliate traffic jumping-off point which allowed others to buy traffic which had been hijacked from legitimate sites.



So essentially this was, I mean, this was arguably not the fault of the websites hosting the ads.  They thought they were hosting legitimate ads.  But essentially 28 full-blown fake ad agencies were created specifically for the creation of malicious advertising.  And they were often offering fake malware-laced Flash Player updates, which as we know has been historically and is still not completely gone away, a means of getting malicious code into people's machines.



So anyway, I thought this was interesting because this gives us a better sense of the scale to which malicious advertising is being deployed.  And during the same period of time, well, actually not all of 2017, but just in the last couple weeks, YouTube was hit with Coinhive cryptocurrency mining advertising.  It turns out the good news is that antivirus software is now up to speed about Coinhive.  And so what set off alarms is that visitors to YouTube were having their antivirus tools firing off warnings that cryptocurrency mining was going on.  And sure enough, 80% of their processor was being used, leaving just enough unused to play videos.  So essentially, while people were watching YouTube videos, the rest of their system was being commandeered for Monero cryptocurrency mining with Coinhive.  Trend Micro, that of course is one of the major AV monitoring folks, providers and monitors, said that the ads drove a more than threefold spike, that is, the YouTube ads, the ads appearing on YouTube, more than a threefold spike in web miner detections during this period of time.



A Google representative who was asked about the incident by Ars Technica, that was one of the outlets reporting on this, told Ars Technica that mining cryptocurrency - this is Google, the owners of YouTube - "Mining cryptocurrency through ads," they wrote, "is a relatively new form of abuse that violates our policies, and one that we've been monitoring actively."  They wrote:  "We enforce our policies through a multilayer detection system across our platforms which we update as new threats emerge.  In this case," they wrote, "the ads were blocked in less than two hours, and the malicious actors were quickly removed from our platforms."



Well, unfortunately, Ars observed that it wasn't clear what the representative meant when saying the ads were blocked in less than two hours.  Evidence supplied by Trend Micro, and on other social media where there was an outcry about that, demonstrated that the ads containing substantially the same JavaScript ran for as long as a week on YouTube.  And then, when queried about that, the Google representative did not respond to Ars' follow-up questions looking for a timeline of when the abusive ads started and ended.



So again, this is a moving target.  Nobody has found a way to be completely clear of these sorts of attacks.  All you can do is respond to them as quickly as possible.  And as we've talked about here, because it's necessary for users' browsers to go obtain the script from Coinhive, blocking the Coinhive domain, as we covered several weeks ago, or using some add-on such as uBlock Origin or one of the good monitors of these sorts of domain queries, should keep at least your own machine from being commandeered for this.  But of course it's an industry-wide problem.



CSO magazine had an interesting piece that caught my attention, which was brought to me thanks to one of our listeners, posing the question whether the BSD versions of the open source Linuxes are dying.  And apparently some researchers believe...



LEO:  Wait a minute.  BSD versions of open source Linux, those are two different things.



STEVE:  Well, okay.  Well, BSD, like OpenBSD, FreeBSD, and NetBSD.



LEO:  Yeah, those are not Linux, yeah.



STEVE:  Oh, god, I'm sorry, I meant Unix.  Thank you for catching me.



LEO:  You scared me.



STEVE:  Just purely misspoke.



LEO:  No, but I think it is the success of Linux that's starting to make these other versions go away.



STEVE:  Correct.



LEO:  You and I both are BSD fans.



STEVE:  Correct.  And I have been forever.  But ultimately I suspect that the open source ecosystem may not need nor be able to support more than one, like one winning open source solution.  And I think it's very clear that that solution is going to be Linux.



LEO:  In a way that's too bad.  Somebody said, I have a quote in my blog, and I'll go look and see who said that, that Linux is a POSIX-compliant Unix designed by PC users.  Let me see if I can find the quote because it's a great quote.  And BSD is designed by Unix users.  And PCs are kind of secondary to the whole point; right?



STEVE:  Yeah.  So while you look, I'll just provide a little bit of background here.  So we have three of them.



LEO:  There's many, actually.



STEVE:  Yeah.  The top are OpenBSD, FreeBSD, and NetBSD.



LEO:  There's also PCBSD, which is now TrueOS, which is a very popular version, as well.  So there's a lot of them.



STEVE:  Didn't that branch from FreeBSD?



LEO:  TrueOS.  I mean, many of them have been branches.  But at this point...



STEVE:  Yeah.



LEO:  This is Matthew D. Fuller:  "BSD is what you get when a bunch of Unix hackers sit down to try to port a Unix system to the PC.  Linux is what you get when a bunch of PC hackers sit down and try to write a Unix system for the PC."  That's the difference.  It's more pure Unix-y.  Right?



STEVE:  Yes, yes.  And frankly that's been my affection for it.  OpenBSD is regarded to have the highest security because security has been its primary focus from the start.  And in terms of like bugs found in the kernel, it supports that, I mean, it's living up to its promise.  FreeBSD is regarded as the most advanced and feature-rich, though that has come at some cost in security because, if you let your guard down, as we know, bugs are going to creep in.  NetBSD is regarded as being the least secure because its focus has been upon having the widest run-on-anything profile, supporting any device and any platform.



And again, there's going to be a security consequence of just having a lot more legacy driver code so you can continue to run on really old hardware which is not being supported, and nobody is really scrutinizing very much anymore.  And so the argument has been that, as a consequence of just a lack of eyeballs - and that's really what it boils down to, a lack of eyeballs looking at the kernels of these BSD operating systems.  They don't have as many people looking at them.  In some cases vulnerabilities that are reported are not fixed in a timely manner, where they are with Linux.



And again, FreeBSD has been my non-Windows Unix OS of choice.  And I wouldn't be surprised to find that if in time these alternative OSes just sort of - they just slow down and stop being developed because there just isn't enough steam behind them.  And I do wonder if, fundamentally, there is enough interest to support more than one primary, nonproprietary, open source alternative.  And we know what that would be.  That would be Linux.



LEO:  I don't know. 



STEVE:  Kind of probably inevitable.



LEO:  Yeah, I think it is.  Hardware support is a real issue in the PC universe.  That takes a lot of people banging on it.  And then frankly the presentation layer on BSDs are just - it's not as elegant looking.



STEVE:  No.



LEO:  And so I don't - and that's somewhat related to hardware support, I guess.  But anyway...



STEVE:  Yeah, yeah.  And the other thing is that a lot of them are beginning, and we're seeing this, they're beginning to cross-borrow chunks of technology because they can't afford to just redevelop the same thing independently.  So it's like, oh, well, yes, we borrowed this from that.  We like the network stack from this OS, and we're compatible with these graphics drivers because they just can't afford to have them all rewritten from scratch.



LEO:  Yeah.



STEVE:  But again, I just thought that was an interesting little tidbit is what we're seeing is we're seeing them, some of the fervor dying off.  Also you really have to be a serious hardcore developer to be mucking around in the kernel of one of these operating systems and not create more trouble, like more problems than you solve.  So you can have the best of intentions, but it's a little bit like how the U.S. space shuttle system began having a problem at NASA because all the engineers that designed it and knew how it worked had retired. 



And so that knowledge, they took the knowledge of how the shuttle systems functioned with them.  And so you could have really freshly minted, energized young people wanting to come in, but they were looking at this stuff saying, "Uh, okay, why exactly is there bubblegum stuck over here?  Is that important?"  And the old guys would say, "Oh, yeah, yeah, yeah, don't touch that.  That rebalances the kravistat, and we need that."  That was carefully calibrated bubblegum, but the new guys wouldn't know.



So I got a kick out of some of the reporting of this, talking about undetected CrossRAT malware.  And I thought, well, okay.  Or undetectable.  The headlines were saying, and this is, remember, the CrossRAT is the trojan which the Dark Caracal is using, written in Java, cross-platform, runs on Windows, Linux, and macOS.  And some of the coverage said that it was undetectable.  No.  It is currently undetected, but there's nothing about it that is fundamentally undetectable.  At the moment, and this is the useful cautionary about this, only two out of the 55 AV scanners at VirusTotal are detecting the Dark Caracal's CrossRAT trojan, the remote access trojan, as malicious.  Which that is certainly a concern.



On the other hand, this all just became public a week ago, and I'm sure that a week or two from now the detection count will have jumped up to nearly all of the AV scanners.  As we've seen, today's AV uses patterns and heuristics.  And there are certainly features in Dark Caracal, that is, in this CrossRAT trojan, that any AV could lock onto.  For example, there is a very well-known domain which CrossRAT currently uses for its command-and-control server.  All anyone would have to do is scan for the presence of that domain.  The problem is that, as soon as that was unmasked, it was changed.  So what we need is we need a more mature analysis of CrossRAT in order to come up with some signatures that will reliably detect it, even when it changes.



Now that it's on everybody's map, I'm sure that all of the AV tools will be getting samples of it and will be adding detection of it to their suite.  And of course, as our listeners know, the first version of InSpectre set off just about every alarm that there was across the AV industry for no reason except that I had used a registry key that was regarded as "sensitive" because it's the one that I use for flipping the protection on and off.  And it wasn't that I was using that key for that purpose.  It's just that it is a sensitive access key.



And so unfortunately all I had to do was obscure my use of the key.  Once I saw what was happening, I figured out what it was that was probably the problem, and I encrypted the key, and then I decrypted on the fly.  So this sort of demonstrates that it is, I mean, this is the cat-and-mouse game that malware and antimalware go through is that, as soon as a piece of malware is detected, if it's able to determine what it is that it's doing that's setting off the alarms, it can hide itself.  And thus the cat-and-mouse game that is going on constantly.



In this case there's nothing, there's no real concern about this CrossRAT trojan being undetectable.  It's just currently not detected.  What I would remind our listeners, the takeaway is that it's written in Java.  And I understand that there are valid uses, especially within enterprise, for using Java.  The advantage of Java is that it is cross-platform, and there are legitimate uses and applications for Java.  But they're not super mainstream.  So most users do not need Java installed on their systems.  It is not by default installed in systems, so it needs to get added for some reason.  So as long as you do not have Java in your Windows, Mac, or Linux machine, then this Java-dependent trojan cannot run on your system.



So I would say be aware of whether you have the Java Runtime present.  If you do, and you don't need it, remove it.  And be aware of anything that installs it because it itself is not dangerous.  But we are seeing instances, I mean, traditionally when Java was exposed to the browser, that was a source of constant problems.  We solved that problem, and Java now by default finally is not available to the browser, thank goodness.  Not having it around at all is even better.  And if you don't need it, make sure you don't have it because it can be used for mischief.



And in a last bit of news, just as I was - literally, like minutes before we began recording the podcast, I got the news that the California Senate had approved a similar action, actually this one had more teeth in it, than what Montana, that was first, and then two days later, just recently, New York State's governor both signed into orders.  There is state-level pushback on Net Neutrality, that is, on what Ajit Pai has done with essentially overturning the existing legislation, as we've covered and as been covered throughout the TWiT network recently.



What the states are doing is sort of clever.  They're using their purse, the power of the purse, or their purchasing power, in order to effectively enforce Net Neutrality within their states.  These governors are signing executive orders, and we can add California to this list.  It's got to go to the State Assembly for ratification, but it's virtually assured to pass.  Unfortunately, this is ending up being political and partisan.  And so what we're seeing is votes are falling down along Republican and Democratic party lines, but the Assembly in California, something like 58 to 23 Democrat majority.  So given that that same party line vote occurs on this issue, it's virtually certain to pass in California.



So we'll have a number of major states who are saying that no state-funded organization or facility can purchase Internet services from any provider that does not honor neutral handling of bandwidth, that is, Net Neutrality.  So New York is a massive consumer of Internet, that is, New York state services and facilities, and certainly that's the case in California, as well.  The providers, of course, are now asking for legislation to outlaw this kind of state-level action.



So this is all not done yet.  But for now states are exercising their independent rights, saying, okay, fine, if there's no federal policy to prevent this, then we're not going to allow any purchasing of bandwidth from providers at the state level, and we'll take it from there.  And certainly no provider would be willing to give up all of the access to the amount of money that large states are spending on bandwidth in order to play games with their bandwidth.



I didn't know where to put this.  I put this under Errata only because it was kind of harsh.  But I thought, well, it's an opinion I wanted to examine.  Someone named Ryan Dey tweeted:  "I didn't appreciate the flippant nature of your coverage of Intel's Spectre mitigation work," he wrote.  He said:  "Speculative Execution has been a business-as-usual feature for a long time" - no argument there - "and there are a lot of people working very, very hard to reengineer the microcode and change everything on a dime.  Your disrespectful remarks on their less-than-perfect first efforts doesn't help anything, and likely insults many young coders who view you as a role model."



So that kind of caught me by surprise, but I thought, okay.  If there was any confusion, I just wanted to say that, I mean, I understand how difficult this is.  And I'm impressed by the idea that microcode can be flexible enough to surface these sorts of features which were not intended and designed in from the beginning.  So by no means am I intended to demean, or did I intend to demean the efforts or to make light of the difficulty that doing this presents.



What I expressed my dismay over was Intel's presentation of this.  They apparently know exactly what's going on.  This is not a security issue any longer since we've had full disclosure of exactly what the vulnerabilities are with demonstrations and proofs of concept of these problems.  What annoys me is that we're getting a lot of hand-waving from Intel.  They're saying, oh, well, describing this as "more reboots than usual."  Well, tell us what's going on.  Tell us what the problem is.  Tell us what you know, Intel.  And Intel is not doing that.



And so, again, everyone who listened to this podcast knows how well I understand that anybody can make a mistake.  Mistakes get made.  What matters is the way you deal with them.  And dealing with them is about communication as much as it is about engineering in this day and age.  So what I took exception to and still take exception to is the PR spin and the obfuscating that Intel is subjecting us to.  They could tell us, as apparently they know, exactly what is going on.  If we had that, I would have no complaint with Intel.  So it's not the engineering.  It's not the coding.  It's not the work of the people who I'm sure are scrambling around like crazy.  It's that we're not having useful communication about something that I would argue is incredibly important.  This is incredibly important.



As Ryan, you write, speculative execution is business as usual.  And I have never faulted Intel for ending up in this place.  It is the case that this has gone on for decades.  And I have said that I object to the idea of class action suits being filed against any of these companies.  I don't see fault in the fact that there are these defects which can be leveraged, and I never have.  The fault I find is in Intel's communication.  And that's the only complaint that I have.



LEO:  I wonder if Ryan wrote this tweet before Intel pulled back on all of its fixes because it caused spontaneous rebooting. 



STEVE:  Actually, I think not.  And based on his location, I think he is at Intel in their - they have a facility up in the Pacific Northwest, I think, which I think is [crosstalk].



LEO:  I don't think it insulted anybody.



STEVE:  Yeah.  And Leo, Friday.



LEO:  See, I like this book.  You know me and science fiction, televised or filmed.



STEVE:  I know.  And I wanted to give all of our listeners a heads-up, those who have access to Netflix, that it's probably going to be binge time.  Starting on this Friday, February 2nd, is the release of the first 10 episodes, that is, the first season of "Altered Carbon."  "Altered Carbon" is a term sci-fi readers know well.  It's a fabulous title for a book I think written in 2002?  It's been a while.  It was a cyberpunk novel.  And, boy, the preview just looks wonderful.  So just a heads-up.  Again, "Altered Carbon" is released on Friday.  And the good news is I think you and I both have mates, Leo, who are as into this as we are.  So I have no doubt that by this time next week Lorie and I, at least, and I would imagine you and Lisa, will have absorbed the first 10 episodes of this very, very interesting-looking new series.



And as I said at the top of the show, probably one of the most fun SpinRite testimonials we've run across in a long time.  Of course it starts off with a subject that got my attention, "SpinRite Is Great," and then continues with:  "Hi Steve, if that is your real name."  Clete Boyce wrote this.  He said:  "I purchased a copy of SpinRite about a hundred years ago, you know, when dinosaurs ruled the Earth, and I am amazed at how many times SpinRite has saved the day.



"I am," he writes, "a hacker, penetration tester" - so he's probably a listener to the podcast - "penetration tester, software engineer, super genius who enjoys torturing hard drives on the weekends.  I really enjoy doing things like playing around with Linux kernel code; and, since I am a super genius, I can feel free to ignore any and all warnings on the various forums.  However, every once in a while I will be exploring the frontiers of ignorance and write some code that seems to completely destroy the hard drive itself.  I will make one innocent change to the kernel code, reboot the PC, and all of a sudden I hear the PC beeping for mercy, see smoke shooting out the back, and see a tear in the space-time continuum forming in the middle of my living room.  I check to see what the problem is; and, sure enough, my PC can no longer read the hard drive.



"That's okay, though.  Since I am a Level 14 Hacker Elf, probably, and a super genius, I know that I can easily fix the problem.  I reach into my satchel and pull out my Magic SpinRite disk" - oh, and speak of the devil - "pop it into the CD drive, and wait while SpinRite does some stuff that is probably very technical.  I walk away, praying to the computer gods that they may forgive me, and most of all hope that SpinRite can fix the problem yet again.



"I've done these terrible things to poor defenseless hard drives many, many, many times; and, every time, SpinRite manages to fix the problem.  The best part is I can happily continue to torture poor defenseless hard drives on the weekends without fear of completely destroying them because I know that SpinRite can apparently fix anything.  While it might be true that hyperbole is my best friend, the underlying theme of this testimony is true.  Steve, thank you for using your powers for good and not evil.  And SpinRite works pretty well, too.  Clete Boyce."



LEO:  Yay.



STEVE:  Well, Clete, thank you very much for a fun testimonial.  And a couple of closing-the-loop bits of feedback from our listeners.  Neil Gardner said:  "Thank you for Security Now!.  Learn a lot each week.  How does one stop a Win10 machine from updating to Creator Edition?"  And, you know, I meant to get the exact language.  Maybe you can, Leo, if you have a Win10 machine running in front of you.



LEO:  I do, yes.



STEVE:  In the Advanced Settings there are two dropdown list boxes containing numbers of days.  And one of them you're able to ask Windows 10 to defer updating for fixes for some number of days.  And the other one allows you to ask it to defer updating feature improvements for some number of days.  And in fact I was just visiting that because I was setting up a new machine because I had to play with this 1703/1709 question, the Creators Update versus the Fall Creators Update, which is the 1709 newer one.  And I noted that it was immediately getting ready, it was like, there it was, it was updating to Windows 10 1709.



So I thought, agh, and I quickly went over to Advanced Settings and dialed down the feature update to 365 days.  It will let you push it back one year is the maximum on that little dropdown.  And then immediately I was very pleased to see it honored that setting and immediately stopped trying to move me to 1709 because I specifically needed that machine to stay at 1703 because that was the one that had this glitch that I talked about at the top of the podcast where there was a bug in that particular version.  So I was able to do that by using those two settings.  And what exactly is the language?



LEO:  I'm not finding the place you're pointing to.



STEVE:  Ah.  So it was under Updates in Win10.  And there's like Advanced Settings, which takes you to another level, and there's two different things you can change.



LEO:  Yeah.  Neither of these seem to have - remember, this may depend on the version of Windows you're using and so forth.  And you may have other limits on what you can...



STEVE:  I realize that I do have one in front of me.  Maybe I can bring it up.



LEO:  Monthly upload.  I don't know [crosstalk]...



STEVE:  Update and Security.



LEO:  Is it in the main control panel?



STEVE:  So Windows Update.  Advanced Options under Windows Update.  And delivery optimization, is that where it was?



LEO:  Yeah, it's not there.



STEVE:  No, you're right, I'm not seeing that.  I wonder if they took that away.



LEO:  Microsoft giveth, and Microsoft taketh away.



STEVE:  How interesting.  Because I have it in 1703.  And you're right, I'm not - delivery optimization is not it.  That's the thing for getting it from your neighborhood or others.  Weird.



LEO:  I'm in the Advanced System Properties.  Performance, User Profile, Startup and Recovery.



STEVE:  How funny.  I did, I sure saw that and dialed it down and it stopped bothering me.  So, gee, Neil, we'll have the detailed answer for you next week.  Or I'm sure - get help with your updates?  How weird.



LEO:  Tell me, Burke, where is it?  Back here; right?



BURKE:  That one looks different than mine.



LEO:  Yeah, see, that's the problem.  I think that - oh, now, I already have the Fall Creators Update.



BURKE:  Right there on the right, if you go down...



LEO:  Advanced Options?



STEVE:  Hey, you know, Leo, maybe if you're up to date it doesn't give them to you.  Because I'm on 1709 on the machine I'm looking.



LEO:  Maybe that's it.  We're up to date now.



STEVE:  Yeah.  So if there is something to defer, then it lets you defer.  If there's nothing to be deferred, then maybe it just doesn't give you the option.



LEO:  Yeah. 



STEVE:  So we'll go with that.



LEO:  Yeah, because I'm not seeing it.  This is one for - we'll ask Paul and Mary Jo.  Yeah, see, it's not.  Burke keeps saying, no, no, it's there.  But it's not.



STEVE:  No.  And I think maybe it's only there for people who have something to be deferred.



LEO:  Yeah.



STEVE:  And in this case, on the machine - I just was checking on the machine I'm using because I'm talking to you through a Win10 machine, and I'm not seeing it.  But I certainly was on the 1703 machine.



LEO:  Now, somebody on 1709 build 16 says it does have the option.  So who even freaking knows? 



STEVE:  Anyway, Neil, it's there somewhere.  Sorry we couldn't be more specific.



LEO:  You know, I had this problem on the radio show, and I felt like an idiot because I couldn't find the setting.  The problem is Microsoft seems to change stuff randomly from update to update.  And so it really depends not even just what version of Windows you're running, but really what update you have.



STEVE:  Or what mood they were in.



LEO:  What mood they were in.  It's very confusing.



STEVE:  Anyway, Neil, it's there somewhere maybe.  And maybe we'll get - I'm sure, the good news is, this podcast is heard by so many people who know the answer, Leo and I will both be flooded with the answer, and we will have a definitive response.



LEO:  Yeah, so this is interesting because Burke's showing me his screen, which does have this, exactly where mine doesn't.  So I don't - you got me.



STEVE:  That is bizarre.  That is so disturbing.



LEO:  I know.  Imagine how dumb I felt during the radio show, thumpering around with this, saying, "Well, it used to be here.  Where is it?  What is it?  Wah wah wah." 



STEVE:  Yeah, well, add me to that club, Leo, because I just stuck my foot in it, too.



LEO:  Well, I don't know.  Okay. 



STEVE:  We'll have an answer.



LEO:  I am up to date.  Maybe that has something to do with it.  I don't know.



STEVE:  But Burke must be, too; right?  I mean...



LEO:  Well, who knows what Burke's doing?



STEVE:  Okay, don't know if Burke's...



LEO:  It may have something to do with the version of the operating system.  This is Windows Pro, I'm pretty sure.



STEVE:  Yeah, and mine was, too.  And I'm sure this one is.  Oh, wait.  This might be Home because this one came with this little machine.  It was already preinstalled.



LEO:  I'm running, let me see real quickly.  Oh, I'm on Home, too.  You know what, that's probably what it is.



STEVE:  Ah.



LEO:  It's probably - you would think this Lenovo T470s would be Windows Pro.  But maybe I realized I didn't need it.  So maybe that's what it is.  You might have to have Windows Pro.



STEVE:  Yeah, and I haven't wanted to mess with this because this is working perfectly for the podcast.  And what is it...



LEO:  So that's an issue, by the way, because...



STEVE:  Yup, it's Windows 10 Home, Leo, the one that doesn't have it.



LEO:  It's unpredictable what version you'll have.  Now, you could go into GP Edit.  You can download the - I guess you can.  I don't know.  You could download General Policy Editor and change this.  Microsoft's also said, if you accepted the 7 to 10 upgrade, you can only defer for so long.  A year seems long enough.



STEVE:  Yeah, yeah, yeah.  I was happy to see it let me set it to 365.  It wasn't clear that the thing scrolled down into the basement because it came up and it said, like, one to seven or something.  And I thought, really?  But then I, like, set it to seven, then I opened it again, now it went from seven to 14.  And I said oh.  So I just kept - I went all the way to the bottom, and it was 365.  And we know where they got that number.  Okay.  So that answers the mystery is Home does not let you defer. 



LEO:  I'm not going to pay the $100 to upgrade to Pro to find out, either, by the way.



STEVE:  Good luck.  And Pro does, yeah.  So Neil, that's your answer.  If you've got Pro, you can do it.  But maybe you don't, and so that would explain why.



LEO:  I think Home doesn't - that's probably the biggest difference is you can't defer.



STEVE:  Yeah.  Anyway, I had a comment, but I'm just going to bite my tongue because we know how I feel.



LEO:  Yes.



STEVE:  So a person whose name I didn't capture on Twitter asked:  "Will InSpectre" - my little freeware - "work on Macs if run in a Windows VM?"  And that's significant because a VM is different than WINE.  And I wanted just to highlight that for our listeners.  WINE is a sort of a thin Windows emulation layer, so InSpectre will run under WINE and be able to show you the truth about the processor, that is, the firmware.



The problem with a Windows VM, if you mean a full virtual machine, is that it is virtualizing the chip, which means that it's a function of what the VM's chip virtualization is choosing to show its hosted OS.  And that's not clear.  So InSpectre will work inasmuch as it will run.  But you will not be able to, I can't guarantee you, depending upon what the virtual machine is doing, that it will, I mean, it will not, if it's a true virtual machine, it will not be seeing the actual chip.  The VM could be showing its client systems, its hosted OS, the same as what is on the chip, or not, depending.  So you really wouldn't know for sure.  But the good news is WINE is pretty easy to run now on a Mac, so you're able to host InSpectre on WINE and then, yes, you'll be able to see what's really going on.



My comments about battery charging stirred up a lot of debate and question, as they always do.  And I'm going to share two tweets and then try to clarify.  Richard Bailey said, he wrote:  "@SGgrc About battery health, the AccuBattery app recommends only charging your phone to 80% to also preserve its longevity.  It gives me a tone when it gets to the percent I set, and I have to disconnect it from the charger."  Opher Banarie wrote:  "Re babying batteries, what about overcharging?  I've been unplugging the device within 10 seconds of 100%.  Is this no longer necessary?"



Okay.  So to everyone, it is absolutely the case that it's the endpoints, the fully discharged, the fully charged, that are problematical.  That's why I referred to being so impressed with my Lenovo laptop that noted after a few weeks that it was always plugged in and said, hey, how about if I run this down to 50% and hold it there?  That's absolutely ideal, though it does, as I said, require me to plan, if I'm going to be running on battery and want to for a long time, to run her back up to 100 or near 100 before leaving the house.



So to Richard, who has an app that gives him a chime when it hits 80, yes, that would be better than running all the way up to 100.  Also, if you have a battery management system which isn't good about keeping the battery from being overcharged, well, then that's a problem.  And so I guess what I'm trying to say is that this is inherently a tradeoff.  If you don't go to 100%, that's better for your battery.  But obviously then you have less available when you're out running around, and you don't want to go too low because that's bad, too.



So if you're a person for whom it is worth giving a lot of your attention to keeping your battery between, for example, the 80% and the 30% high and low point, then great.  That's going to be better for the battery, requiring more of your attention.  If you don't want to invest that much, for me, I've sort of reached a compromise.  I assume that Apple, the builder of my iOS devices, and I'm sure that, well, I was going to say that Samsung knows how not to overcharge except they're rather famous for having a problem with some of their batteries.



For me it's a compromise.  I'm going to leave my devices plugged in when I'm not using them, and I'm not going to discharge them much because of my own life habits.  When they're off of the charger, they never have a chance to go down much below 70%, and then they get plugged in again.  Consequently, I've never had a battery life noticeably shortened.  On the other hand, I'm not using them very much.  They're mostly tethered to power.



So I hope that clarifies it.  It is really bad to run them to the bottom.  They really don't like that.  It's better not to run them all the way to the top.  If your device allows you to run them to 80% or 90%, that's probably better than running them to 100%.  It's dangerous to overcharge them.  So if you keep them away from the danger zone, that's better, but at the cost of having them not having as much run around time when they're off of the wall.



So that's, I think, the best way to sum this all up is don't let them run down too low.  Try not to cycle them to extremes often.  If your device lets you not charge them all the way up, take advantage of that.  If you want to take responsibility, great.  And I think everyone probably falls somewhere within that spectrum.



And lastly, Peter Kirby noted from our Picture of the Week last week - you remember, Leo, that that was the one we had fun with where sort of the philosophical approach to entering your password was, ooh, ooh, you're close.  You're only off by one character.  And we made fun of it, saying, yes, it's very clear why that's not the proper wisdom for dealing with a password.  Anyway, Peter noted that, he says:  "I was showing someone the Pic of the Week from 647, and it occurred to me, if the backend knows you're only one character off, then they aren't hashing the password like they should be, or they wouldn't be able to know that."  And actually that's a very good point, Peter.



So I just got a kick out of that observation, yes.  Not only are they doing the wrong thing philosophically, but the fact that they're able to do that philosophically means they are also really not doing the right thing technically because they're able to compare the password you gave them versus your in-the-clear password and go, "Oh, cool, he was really close.  Let's let him know.  Try changing the punctuation a little bit, and maybe you'll get there."  So anyway, great feedback from our listeners, thank you.  And that's our podcast, 648, for the week.



LEO:  Nice.  Well done.  A job well done.  We can put this one in the bank.  You can get it now, as soon as we finish editing it and chopping it up, at Steve's site. 



STEVE:  Massaging it a little bit.



LEO:  Massage, just minor.



STEVE:  Oh, and I did want to mention to anyone who looks for these transcripts, Elaine's schedule is a little bit impacted this week and next week.  So she begged me, she said, "Steve, Steve, Steve, would it be a problem if I'm a few days late?"  And I said our listeners love you.  They love having the transcripts.  A little bit late is not a problem.



LEO:  No, yeah.



STEVE:  So I'll put up a notice to that effect for people who don't hear this.  But this week and next.  The good news is she's busy.  And she said, you know, it would really help me if you let me put you off a little bit so I can get another project finished.  And I said absolutely.  She's been so wonderful for 12-plus years, I have no problem with that.



LEO:  Oh, absolutely.  And I think, yeah, the transcript's really as much for archival purposes and search purposes.  Although I know some of you like to read while you listen.  But you'll just have to defer a little bit longer.  You can get the transcripts and the audio versions at GRC.com.  While you're there, check out SpinRite, the world's best hard drive recovery and maintenance utility even today, even for SSDs.



STEVE:  Even if you're an evil super genius, if you still need it then.



LEO:  Even if you're - especially if you're an evil super genius.  You'll also find lots of free stuff.  Steve's very generous.  And including InSpectre to inspect the status of your device.  Now, I did want to - I forgot to get this in while you were talking about it.  WINE recently updated to WINE 3, and for some reason the latest version of InSpectre and WINE 3 on my Mac do not like each other.



STEVE:  Ah.



LEO:  So WINE hangs.  Don't know.  Don't know what it means.  I'll try it on some other Macs.  For what it's worth.



STEVE:  Interesting.  I will end up addressing that because we have been taking pains to get SQRL working under WINE.  So it would be available under Linux.  And apparently it's now running under Android on WINE, WINE for Android.  So that's good.  So I'll have to figure out what's going on, and I will address it.



LEO:  WINE had a big update with WINE 3 this week.



STEVE:  Yes, WINE 3, yup.



LEO:  I was using it under WINE, an earlier version of WINE, before.  Anyway, get InSpectre.  If you have a PC especially.  I think people on Macs with - if you're running High Sierra you probably just can relax.  It's the older operating systems on the Mac that you've got to worry a little bit about because Apple's been slow to get those fixed.



Let's see.  What else?  We have the show as well on our website, TWiT.tv/sn for Security Now!.  Video, as well, if you'd like to see Steve gesticulate.  He's a fine gesticulator. 



STEVE:  Yes.



LEO:  TWiT.tv/sn.  You can also find subscription links there, or just look in your favorite podcast application and subscribe.  That way you'll get every episode as soon as it comes out.  We only keep, you know, the way a podcast works, it's an RSS feed, and we only keep the 10 most recent episodes in there.  If we had all 648 the RSS feed would be hundreds and hundreds of megabytes, which would kill your bandwidth, kill our bandwidth, it'd be terrible.  You'd be downloading that five times a day.  No, you don't want to do that.  So just the last 10 episodes.  However, you can go to the website and get all 648 there, TWiT.tv/sn or GRC.com.



We invite you to take the TWiT survey.  We do this every year, beginning of the year, just try to get a better handle on who's listening.  And there are a lot of new listeners to Security Now!, so it would be very helpful if you go to TWiT.tv/survey.  We are not collecting in any way.  We're not collecting your email.  We're not collecting personal information.  This is to be used in aggregate.



Two different groups:  One is us because we like to know more about how you listen and what you listen to, and the other is advertisers.  Because we don't collect information in any other way about you, this is the one time a year we get to know a little bit more about our audience.  And advertisers would like to know that.  We don't give them any information about you personally, I promise.  And even the in-aggregate is really general broad strokes.  It just makes them feel better.  TWiT.tv/survey if you would like to help us out.



If you want to visit the studio, you can.  No surprises, though.  Let us know ahead of time:  tickets@twit.tv.  Just as you prize your security online, we prize our physical security here in the studio.  So email us first:  tickets@twit.tv.  Thanks for being here.  Thank you, Steve.  Enjoy "Altered Carbon."  We'll compare notes.



STEVE:  Ooh, yes.  



LEO:  Next week on Security Now!.  Bye-bye.



STEVE:  Bye-bye.  



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#649

DATE:		February 6, 2018

TITLE:		Meltdown & Spectre Emerge

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-649.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we observe that the Net Neutrality battle is actually FAR from lost.  Computerworld's Woody Leonard enumerates a crazy January of updates.  EternalBlue is turning out to be far more "eternal" than we'd wish.  Will Flash EVER die?  There's a new zero-day Flash exploit in the wild.  What happens when you combine Shodan with Metasploit?  Firefox 59 takes another privacy-enhancing step forward.  We've got a questionable means of sneaking data between systems; another fun SpinRite report from the field; some closing-the-loop feedback from our listeners; and, finally, a look at the early emergence of Meltdown and Spectre exploits appearing in the wild.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're starting to see the first exploits of Spectre and Meltdown.  Fortunately, they're not potent yet.  But Steve will give us the lowdown on that.  New Firefox capabilities and a whole lot more, it's all coming up, as always, next with our Securer in Chief, Steve Gibson.  Stay tuned.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 649, recorded Tuesday, February 6th, 2018:  Meltdown and Spectre Emerge.



It's time for Security Now!, the show where we cover your security online, your privacy.  We cover all the big exploits, all the little bugs, and we do it all with style, grace, and panache thanks to Captain Steven "Tiberius" Gibson of the good ship What the Hell's Going on Today.  Hello, Steve. 



STEVE GIBSON:  Leo, great to be with you.  Boy, next week is going to be Episode 650.



LEO:  Yikes.



STEVE:  Which of course makes this one 649 since we're incrementing by one.  And once again we've got those words in the title.  This one is "Meltdown and Spectre Emerge."



LEO:  This is like the eighth week in a row or something.



STEVE:  Well, yeah, that's - what would it be?  Well, the fifth week because this is how the year began was with this news.  And now AV is picking up exploits.  So this takes us from, oh, it's only theoretical, nobody needs to really worry about this, blah blah blah, to uh-oh, yeah.  Not as bad as it's going to be.  That's why it's just emerging. But we'll be talking about that this week.



We're also going to talk about the fact that it's very clear that the Net Neutrality battle is actually far from lost, that is, what Ajit Pai has done probably doesn't matter that much.  It would have been nice to leave it the way it was because everything was tied together and unified.  But we do have a system of federation.  And, oh, it's going to come back to bite the FCC.  Also, Computerworld's Woody Leonard enumerated the crazy January we have just suffered through of updates.  And I just got a big kick out of the way he put it, so we'll touch on that briefly.



EternalBlue is turning out to be far more eternal than we would wish.  We pose the question, will Flash ever die, speaking of Eternal Blue that hasn't, will Flash ever die.  There's a new zero-day Flash exploit in the wild.  And actually a couple interesting things that our listeners can do.  In the newer versions of Office there are some ways to mitigate Flash also that I want to touch on.  We'll also pose the question, what happens when you combine Shodan with Metasploit?  It's not good.  Firefox 59 is taking another privacy-enhancing step forward.



There's also a questionable means of sneaking data between systems.  Some security outfit said, oh, we came up with a new way of getting data between two systems.  But it's hard to imagine something kind of less practical in actuality.  So we want to take a look at that.  I have another fun SpinRite report from the field that I read to Lorrie a couple days ago, and she got a kick out of it.  I thought, well, okay, I've got to share that with our listeners.



We've got a little bit of closing-the-loop feedback.  And then we're going to take a look at what has been happening thus far in January as we come into February with actual evidence of Meltdown - we really do need to combine these somehow.



LEO:  Smeltdown.



STEVE:  Smeltdown or Specdown or Meltre or Meltre or I don't know.  Really, it's like annoying.  Anyway, they are in the wild.  And after our first break we'll take a look at this week's picture, which I've had on the back burner.  I thought we were going to, like, not need it because we were going to have closed this chapter.  But no.  Chapter's still open.



LEO:  I got a - I should have tweeted you.  I had noticed a Picture of the Week.  It's a new - I have to find it.  There's a new dating club of people who share the same password.  And I thought you would - match with the person who shares your same password.  I thought you might enjoy that.  I'll see if I can find it.



STEVE:  Imagine what else you may have in common.



LEO:  Just imagine.



STEVE:  Well, unfortunately, Intel doesn't.



LEO:  Uh-oh.



STEVE:  And, well, I mean...



LEO:  We know, doing it right.



STEVE:  Yes, they're scrambling to deal with, as we know, a set of very, very clever exploits of some fundamental architecture of all modern processors.  Last week's Picture of the Week was people have been having fun with the Intel logos.  And I saw this one where someone said, yeah, this sort of reminds me of the original Intel logo that they left behind.  And anyway, it's sort of fun.



LEO:  Oh, this is funny.  I get it now.  I had to look at it for a bit.



STEVE:  Yup.  It shows the kernel memory leak coming from inside - it's that circle where the ends don't meet, originally, deliberately, for the Intel Inside logo.  But now this one has added a little red arrow.



LEO:  They have to close that ring, I think.



STEVE:  That's right.  The ring has not been closed, and it's like, whoops, we have little kernel memory leak.



LEO:  That's hysterical.  I love that.



STEVE:  And maybe not so little, too.  So, yes, we're working through the plethora of imagery that was generated as a consequence of this, which we'll be talking about at the end of the podcast when we talk about what's been going on.  I did want to note some follow-up and a little more detail about what I mentioned last week because Wyoming and New York's governors had both signed some executive orders.  And just as we were starting the podcast, news came that the California Senate had passed, I think it was 21 to 12, some similar legislation, the idea being that states have a lot of power over what happens within their borders.



And while even though the major ISPs were lobbying hard to prevent states from going their own way relative to Net Neutrality, it turns out there are strict limits over what the federal government can do because, for example, states have control over their purse, and they can decide how they want to spend their money.  So what happened was that we had a bill that passed through California's Senate, that is now moving on to the Assembly.  And I just checked my notes, it did pass 21 to 12.  However, this looks like it's just the beginning.  There are also a group of 22 state attorneys general that have gathered together and filed a suit against the FCC.



It turns out it's not possible, much as the FCC would like to simply change their own rules, there are laws that govern what the FCC is able to do.  And it's necessary for the FCC to demonstrate that conditions have changed sufficiently since an original law was put in place for them to use that to justify subsequent change.  And that hasn't happened since the Net Neutrality law was brought up.  It was called the Open Internet Order in 2015.  So it turns out that 22 states have joined together in an action against the FCC to challenge Ajit Pai's much noticed decision just to toss that 2015 decision out.



But more importantly, the EFF, of course, our friends at the Electronic Frontier Foundation, are very much involved in this.  They posted sort of a response to California's action, sounding a little annoyed.  Apparently they were involved in the lawmaking.  And for whatever reason, I don't know anything about what's going on behind the scenes politically, but the legislation which California has passed so far they worry would not stand legal challenge.  Ernesto Falcon, writing for the EFF, said that there were much stronger things that California lawmakers could have done, and he enumerated three which are really interesting, so I wanted to share them with our listeners because this gives us a sense for just how much clout states actually do have over what goes on inside their borders.



First, Ernesto writes:  "California spends hundreds of millions of dollars on ISPs, including AT&T, as part of its California broadband subsidy program."  He says:  "The state could require that recipients of that funding provide a free and open Internet, to ensure that taxpayer funds are used to benefit California residents rather than subsidizing a discriminatory network."  He writes:  "This is one of the strongest means the state has to promote network neutrality," and it's missing from this legislation which just passed which was SB 460.



Secondly, he writes:  "California also has oversight and power over more than four million utility poles that ISPs benefit from accessing to deploy their networks."  He says:  "In fact, California is expressly empowered by federal law to regulate access to the poles, and the state legislature can establish Network Neutrality conditions in exchange for access to the poles."  He says:  "Again, that is not in the current bill passed by the Senate."



And then, third:  "Each city negotiates a franchise with the local cable company, and often the company agrees to a set of conditions in exchange for access to valuable, taxpayer-funded rights of way.  California's legislature," he says, "can directly empower local communities to negotiate with ISPs to require Network Neutrality in exchange for the benefit of accessing taxpayer-funded infrastructure."  He says:  "This is also not included in the California bill."



So I wanted to do some more digging into this because of course this has been a big topic of ours and of huge interest.  And the voting public has demonstrated overwhelming support for the concept of Net Neutrality, which for reasons of the way  Washington works with power brokers and lobbying and money, isn't what our government has been doing.  But it looks like, at the state level, there is very, very strong capability of enforcing this locally.  So as I noted at the top, I think we're far from losing this.



LEO:  Well, I'm glad you're bullish.



STEVE:  What do you think?



LEO:  We had Denise Howell on The New Screen Savers.  And of course the issue always comes down to interstate commerce.  States are forbidden from interfering with interstate commerce, and we imagine the Supreme Court, in the long run, will have the deciding vote on all of this.  And meanwhile, while this is going to be battled, as it will be for weeks and months and maybe even years, we won't have Net Neutrality because that's the current situation.  So it'll be interesting to see what happens.  There's all these lawsuits.  There's 21 lawsuits from 21 states.  And we'll see.



Hey, I just wanted to show you something.  There's a live camera, I don't know if you - did you watch the launch, the Tesla, I'm sorry, the SpaceX launch?



STEVE:  Thankfully, I had your stream on, so I was watching TWiT at that time.



LEO:  Huge success.



STEVE:  Oh, my goodness.



LEO:  The Falcon Heavy, largest rocket ever made, launched successfully today.  Yeah, we just kind of broke into MacBreak Weekly to show it.  And I think we all knew that Elon Musk, who has created SpaceX, had put his original Tesla Roadster, cherry red Tesla Roadster...



STEVE:  Okay, now, will you explain that to me?  Because I  haven't been following along.



LEO:  Well, it's a test, right, it's a test launch.  So they need weight.  So they just, instead of putting concrete bricks in there, he said, well, I'm just going to put the car in there.  And he put a - he didn't need it any more, I guess.  He can get another one, I'm sure.  And he put a spaceman in it, in the driver's seat.  And it's playing a loop of David Bowie's "Space Oddity"...



STAFF:  In a vacuum.



LEO:  Over and over again.  Of course you can't hear it because it's in a vacuum; right?  But, hey, it's not the point.  "I'm a star man waiting in the sky," you know.  No, it's "Ground control to Major Tom," isn't it.  Yeah, yeah, it's "Ground control to...."  Anyway, there's also - I didn't realize this, and John just told me.  The capsule with the car had fairings when it was launching, of course.  Those fall away.  And the car is now out in the open as it rotates the Earth.  And of course Elon being Elon has put a camera on it.  And this is live footage, watch this, from the roadster floating in Earth orbit right now in space.



STEVE:  Oh, you're kidding me.



LEO:  I'm not making this up, but you'd think I was.  This is the planet.  It's coming in over the driver's right-hand shoulder.  It is a stunning view.  So I just wanted to point that out.  The Starman feed is live on YouTube, and actually I've rewound a little bit to show this shot.  But it's YouTube.com, I think /starman.  Anyway, search for "starman live" on YouTube, and you can - is that not a shot?  That's a live shot from the capsule.



STEVE:  This is unbelievable.



LEO:  Because Elon is a madman in a very good way.



STEVE:  Oh, lord.



LEO:  Isn't that great?  Forget sci-fi.  Forget "Altered Carbon."  This is it.  This is real.  Okay.  Enough said.  On with the show.  I just had to - I found that...



STEVE:  We do live in fun times, Leo.



LEO:  Oh, it's exciting.  It's really exciting, yeah.



STEVE:  So Woody on Windows, that's the name of the column that Woody Leonard has been writing for quite a while in Computerworld.  Woody of course, you and I both know him, he's been around forever.



LEO:  Woody Leonard, yeah, great guy.



STEVE:  He's won a whole bunch of awards over the years.



LEO:  Great guy.



STEVE:  And he's pretty much disgusted with January.



LEO:  Good, because it's February now, so that's okay.



STEVE:  Yes, yes.  He called it "The perfect end to a perfect month."  And this, of course, this was on Wednesday the 31st.  He says:  "Yet another Win10 1709 Cumulative Update."  This one was KB...



LEO:  This is live.



STEVE:  Why don't we just stop the podcast, Leo.



LEO:  Let's just watch this.  Watch this.



STEVE:  Sit here and, oh.  So was that the moon that we saw leaving the first time, and now we're...



LEO:  No, I think it was the Earth.  They're in Earth orbit right now.  But he has multiple camera angles, obviously.  And so this is another angle of the planet.



STEVE:  This is ridiculous.  I mean, it's ridiculously perfect.



LEO:  Gives new meaning to that little blue marble, doesn't it.  Wow.



STEVE:  Wow.



LEO:  Sorry, I didn't mean to interrupt you.  Woody, what's Woody saying?



STEVE:  Who?  Who?  Woody who?  Oh, look at that.  That's just ridiculous.



LEO:  That's a live shot.



STEVE:  Okay.  We're teasing our listeners.



LEO:  Well, yeah, you'll be able to go back and see it, if you have the video.  Or even if you don't, you can go to YouTube.  Wow.  Wow, wow, wow.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  So in his column - oh, and by the way, I did fire up my Win10 machine that I'm talking to you over right now.  And sure enough, there was another Windows 10 update.  The good news is I've learned the lesson.  I fire it up hours before the podcast so that it has a chance to revamp itself.  And sure enough, KB4058258 was waiting for me.



LEO:  Now I've got to try, mm-hmm.



STEVE:  Uh-huh.  Another cumulative update.  So he enumerates.  I won't go through it in painful detail.  Woody does in his column.  But in short, Woody notes that we received patches last month on January 3, 4, 8, 9, 11, 12, 17, 19, 22, 23, 24, 26, 29, 30, and 31 - 15 out of the 31 days.  And you might say, okay, January 1st doesn't count because that's New Year's.  But still, half of the days of the month we received updates and patches.



And where did I have it?  Oh, yeah.  He says:  "Today is the 15th day this month" - he's writing on the 31st.  "Today is the 15th day this month that we've seen Windows patches, yanked patches, patches of patches, and re-re-re-patches."  Then he says:  "Welcome to the third cumulative update for Win10 Fall Creators Update this month."  So anyway, I just, yes, we all know this has been going on.  We've been covering them.  As we've been discussing, in some cases it's up to users to go get them because Microsoft, I don't know, just got tired of...



LEO:  And it fails sometimes, right, without warning.  Yeah.



STEVE:  Yes, yes.  And some of them killed AMD.  Then they had the "Fix the AMDs that we killed before."  And then we had the "Oops, we're sorry, the firmware" - okay.



LEO:  There's a message from space that says "Don't Panic."  Did you see that on the screen?



STEVE:  And now we have a camera looking over the right shoulder of Starman.



LEO:  Yeah.  Starman's well covered.  There's the - if you just go back, you can see where it is and so forth.  But, yeah, the screen on the roadster says "Don't Panic," a tribute, of course, to Douglas Adams and "Hitchhiker's Guide to the Galaxy."  I love that.



STEVE:  Yup, the cover of the book.  And for our listeners who didn't see the return to Earth of the two boosters, oh, my god.



LEO:  That was like ballet.  That was synchronized swimming.  I mean, unbelievable.



STEVE:  Again, just go find - you have to go find the return.



LEO:  Well, you won't have to go.  It'll be everywhere.



STEVE:  It's true.  It'll be in your face.  Yeah, that's true.  You're right, it'll be on mainstream news, I guess, this evening.



LEO:  Oh, absolutely, yeah.  You couldn't do a more photogenic, public-pleasing PR.



STEVE:  It's ridiculous.  It's just...



LEO:  Yeah, yeah.



STEVE:  Unfortunately, speaking of ridiculous, cryptocurrency mining malware has now infected over half a million PCs using...



LEO:  This is becoming a plague.



STEVE:  Yes, it really is.  So to remind our listeners, EternalBlue was the name given to the Windows SMBv1 exploit, which was believed to have been developed by the NSA and was leaked by the Shadow Brokers hacking group.  And that was earlier last year.  It was famously leveraged by the widespread WannaCry ransomware, which we covered at the time, devastating the U.K.'s NHS, Health Network System.  I mean, they had to basically take their health network offline because of the ransomware that attacked it.  And of course it was also used in that Petya/NotPetya worm.



Microsoft issued a security update to patch the flaw being leveraged by the EternalBlue exploit, which used the first version of server message blocks, that is, SMB, the Microsoft Windows file and printer sharing, basically, and a lot of other stuff got loaded onto that over time.  They issued a patch last March 14th, okay, so not quite a year ago, but coming up on a year ago.  Middle of next month a year ago.  March 14th, 2017.  And surprised us by even patching the long unsupported Windows XP for the same flaw, essentially at the time acknowledging the severity of this problem.  So at its peak, tens of thousands of machines were infected.



Okay, now, fast-forward to today, as I said, nearly a year later.  EternalBlue exploitation, despite having been patched nearly a year ago, remains alive and well, with several cybersecurity firms reporting the discovery of a new cryptocurrency mining virus that's being spread - so I guess that maybe makes it a worm, although it was described as a virus, and I didn't dig in - being spread using EternalBlue.



Researchers from Proofpoint discovered a massive global botnet which they dubbed Smominru, S-M-O-M-I-N-R-U.  I guess that's Smominru, that's using EternalBlue to spread and infect Windows computers to mine Monero cryptocurrency.  Now, what's interesting is that, since this virus or worm is mining Monero, the money needs to go to the holder of the Monero payment address, which allows its success to be audited.



Okay.  So it's been active at least since May of 2017.  So two months after Microsoft issued the patch, this thing started going.  It's infected over, what, maybe now that makes it nine months, more than 526,000 Windows machines, most of which are believed to be servers still running unpatched versions of Windows, according to these researchers from Proofpoint.  Based on the hash power associated with the Monero payment address for this operation, which is auditable, the mining network is - they've verified it's a massive botnet which has mined approximately 8,900 Monero valued at approximately $3.6 million USD.



LEO:  Whoa.  So this is worth it.



STEVE:  Yes.  Thus the problem we have here.  And the mining botnet is currently running at the rate of roughly 24 Monero per day, generating $8,500.  So it is producing...



LEO:  That's a lot of incentive.



STEVE:  Exactly.  And as we know, security is about incentive.  I mean, everything we learn is that security is to varying degrees porous.  And if nobody really wants that much to get in, then some adequate security keeps them out.  But if somebody is absolutely determined to make something happen, unfortunately, there are always ways to get in.  And so here they've made $3.6 million USD at the current valuation of a Monero.  And they are earning, if you call it that, $8,500 per day by stealing processing power from the machines that have been infected by the exploit which we believe was discovered and in the toolkit of our U.S. National Security Agency for some time.



So you know, Leo, we've been doing the podcast now, we're in year 12.  I wouldn't have believed this story.  I mean, it sounds like science fiction.  It sounds like, what?  And here we are.  Yeah, wow.



LEO:  Thanks, NSA.  Thanks, Microsoft.  



STEVE:  Uh-huh.



LEO:  Thanks, Intel.  Thanks to all of you.



STEVE:  Well, and this also, I mean, further demonstration, we're about to talk about this Adobe Flash zero-day.  But this shows that, while patching is necessary and important, obviously, there is just this bell curve, you know, there's this huge distribution of systems.  Our listeners are following along, trying to get their firmware updated - well, or maybe not now, now waiting until Intel fixes the Spectre firmware.  Then, I mean, people in our orbit here are on the leading edge of being secure.  They're taking action.  They're segregating their networks.  They're using firewalls.  They're using secure browsers.  They're running uBlock Origin and understanding about advertising ware and so forth.



So we're at one end.  On the other end are servers that still have Code Red and Nimda worms scanning the Internet, looking for exploits.  So, and then we have everything in between.  But here's something that's been patched for which there are servers publicly exposed on the Internet that, against all reason, have file and printer sharing ports exposed that allowed them to get taken over by the use of the Eternal Blue exploit and run this Monero miner on their server hardware.  And so it's like, well, it's a crazy world, but it's the one we're in.



LEO:  That explains something because I had a woman call the radio show on Sunday, maybe you heard it.  She had ESET Security Suite.  And she said, "Somebody keeps trying to get into my printer."



STEVE:  Oh.



LEO:  And I said, well, that's Internet Background Radiation, a term you coined, which is there's always script kiddies and other scanning IP addresses looking for vulnerabilities.  I said, merely telling you somebody's trying to get in, that's fine.  That's normal.  If they get in, then you'd want to know about it.  What I didn't realize is they're not trying to get in to use that old exploit, to just print something and go ha-ha.  They're trying to run a bitcoin or a Monero miner on your printer.



STEVE:  Very likely.  I mean, yes.



LEO:  Wow.



STEVE:  Commandeering processing power.  And notice that one of the mitigating effects of cryptocurrency mining now is, as we know, the amount of power consumed.  At least in the case of the bitcoin blockchain, it takes more electricity, especially now that the bitcoin value has dropped so much from its brief high a couple weeks ago, it no longer is profitable in many states, like in California, because our electricity costs more than it does to generate a coin.  There are states where it still makes sense.  But the key here, and the reason essentially this $3.6 million had zero cost, and the 30 whatever it was I said, 3,600, or no, $8,500 per day these guys are making, multiply that by 30 for a month.  I mean, they're still making serious coin.  But it's zero cost because other people are paying for the power.  Wow.



LEO:  Wow.



STEVE:  But so I guess the point I want to make is that, while patching after the fact is important, it is also very clear that proactively pushing updates, I mean, which despite the controversy of Windows 10 and how heavy-handed Microsoft has been, and how upset people have been to have their machine reboot in the middle of the day or when they're trying to use it and so forth - and to give them credit, Microsoft has backed off and now is much more negotiable along those lines.



But the point is, you're going to get patched.  Sooner or later, if your machine is on the network, it's going to find out there's some updates, it's going to get them, and it's going to, sooner or later, it's going to patch itself.  And step by step we've been marching sort of backwards against this.  I remember when updates sort of became - our machines were going to go get them.  A lot of the old timers among us said, what?  I don't want my machine getting updates.  I want to go get them myself.  And so you remember those days, Leo.



LEO:  Oh, yeah.



STEVE:  And so we've sort of been successively moving away, and it's hard to argue when we see all of these servers that are sitting there exposed, it's arguably being abused.  And, I mean, the good news is from their standpoint all that's being done is that their processors are running much hotter that they otherwise would and consuming more power than they otherwise would because people are using them to mine cryptocurrency rather than crawling inside and seeing what's going on in their network and abusing them more deeply.  No, it's just about money.  Which in their case is probably a good thing.  But it's hard then to argue against any company being as deliberate as Microsoft has been about our ecosystem is going to be patched  moving forward, period.  It is going to happen.



LEO:  Well, you can see why.  You can see why.  LawnDog wonders in the chatroom if there's - you could monitor for outbound traffic.  Would there be a signature that would let you know that you've got a Monero miner on one of your systems?



STEVE:  I would say that the easiest indication is look at the processor.  It's going to be pinned.  It's going to be at 100% or 95%.  And hopefully that's not normal.  You know, GRC's runs around half a percent, if that.  I mean, it just sort of ticks along doing nothing most of the time.  But the real key is that these miners are going to be saturating the processor.  So that's the thing to look for.  And certainly there is some communication back to the command-and-control server in order to, you know, that this miner is using.  But anyone running a server should have some sense for how much their processor is in use.  And that's, I mean, it's like a bright light in the dark if your processor is pegged at 100%.  That's like, okay, wow.



So one last comment, and then we'll take a break.  Oldies but goodies.  Adobe Flash is another thing that - as I was putting this story together and noting that Adobe has just today pushed out an update for this newly discovered zero-day, and the fact that we now use the Adobe Flash Player.  Remember it didn't used to be Adobe's Flash Player.  They bought that, right, from Macromedia?  And I just wonder if they are happy with the association of their name, Adobe, with Flash Player.



LEO:  And whose idea was it to buy this?



STEVE:  Exactly.  Here we are, how many years downstream, and they were probably thinking, oh, yeah, we're going to expand our repertoire, and we're going to buy this Flash Player from Macromedia, and that's going to be a really great thing.  And now Adobe Flash has become, I mean, like a horrible thing.  Adobe Flash Player, that's not a good thing.  And so I just wonder, on balance, if they were to do it again, is this an acquisition that they would think made sense over the long term?  Because how did they ever really make any money on it?  It's not clear to me.



LEO:  That's a good point, too.  Well, no, I think they sold tools for people writing Flash plugins and stuff like that.  So I think they probably did make money on it.



STEVE:  And then I think those tools, though, I think that the actual language went mainstream so that there were third parties making compilers and construction kits and so forth.



LEO:  That's an interesting question.  I'd love to - nobody at Adobe will ever admit.  But I'd be very curious to see if Flash, on balance was a good idea.



STEVE:  Yes, because Adobe Flash Player, it's like, ooh, you know, it's got your name right off the bat there.  So what happened was that South Koreans spotted a new Flash Player zero-day being used by North Korean hackers against them when they were researching North Korea.  So far it's been used in targeted attacks.  It looked like it was email or spreadsheets or something.  Somehow the Flash Player was embedded.  And so the idea being I guess probably watering hole attacks.  The Flash Players were stuck in things that South Koreans were obtaining from North Korea, and the North Koreans laced them with an exploit that had never previously been spotted in the wild, thus zero-day.  Adobe issued - Adobe, the owner of Flash Player for better or for worse...



LEO:  Oh, those guys.



STEVE:  Yeah, issued another critical vulnerability security advisory saying that it exists in Adobe Flash Player 28.0.0.137 and earlier, saying that successful exploitation could potentially allow an attacker to take control of the affected system, saying Adobe is aware of a report, blah blah blah.  Anyway, there's a CVE number.  We are now at 28.0.0.161.  You probably know, if you have Flash Player, if you need it for some reason, if there are things that you use.  I do have it in my system, but it is locked down.  I have to, like, explicitly permit three different dialogs because Firefox has it set on probation, uBlock Origin says, uh, what?  And there's like something else, I have a plugin that's also doing something.



I  mean, for me, you have to really, really want to run something in Flash.  And I know that because I went to get.adobe.com/flashplayer/about in order to check which version my system had.  And sure enough, I had an older one.  But in order to do that, that page tries to run the Flash Player in your system, if any, in order to have it report its own version.  So I had to, like, successively click on, yes, allow this time.  Yes, only now.  Yes, I'm sure.  Yes, I'm not currently intoxicated.



LEO:  You don't want to run Flash.  What, are you crazy?



STEVE:  Exactly.  So anyway, at this point, as we know, HTML5 has duplicated the functionality that Flash used to provide.  There are still a few crazy websites that are trying to exist, like Flash-based, where the entire website is Flash.  And I just say to them, well, good luck.



LEO:  Good luck, yeah.



STEVE:  Good luck.  Not much traffic coming their way lately.  But the things that, I mean, once upon a time you had to have Flash in order to do some fancy stuff.  This is now all moved into mainstream web standards.  So it's just - it's over.  Nothing but inertia, like the websites that are Flash-based.  And there are some games, I guess, that are Flash-hosted games.  But, boy, it's just not worth the security risk.



LEO:  No, no, no.



STEVE:  So there is an update available now.  Get it if you know you need it.  But in any event, you definitely want to be running Flash Blocker stuff on your web browser.  IE, Microsoft is now taking responsibility for keeping its Flash updated, and so it'll keep you current.



I did want to also note one other thing because the way these things were coming in was through Office attachments.  In the newer versions of Microsoft Office there's something called Protected View.  Under File > Options > Trust Center there's Trust Center Settings.  And under that is something known as Protected View, where you've got three checkboxes:  Enable Protected View for files originating from the Internet, Enable Protected View for files located in potentially unsafe locations, and Enable Protected View for Outlook attachments.  And you want those things on because that's just general, I mean, we've been complaining about Microsoft running scripts in email as long as this podcast has been going.  For 12 years it's been why does Outlook run scripts?



LEO:  No, please, no.



STEVE:  When did email ever have a valid use for a script?  It just never made any sense.  So if you are a Microsoft Office user, File Options > Trust Center > Trust Center Settings, and enable Protected View.  You're able to then selectively say, oh, yes, I want to run Flash that I've received from North Korea, please.



LEO:  Please let me.



STEVE:  Please let me.  Otherwise, no.  And so that's certainly the way you want your default to be.



So speaking of, okay, now, I don't know if this is a smart guy.  This is borderline irresponsible.  But I guess if you can, then it'll happen.  What do you get when you combine the Shodan global search engine...



LEO:  Yes?



STEVE:  ...with the Metasploit exploitation toolkit?



LEO:  Oh, no.



STEVE:  Yes.  You get...



LEO:  Rosemarie's baby.



STEVE:  ...Autosploit.



LEO:  Autosploit.  Oh.



STEVE:  Autosploit, now available on GitHub.  As its pseudonymous author Vector states on GitHub:  "As the name might suggest, Autosploit attempts to automate the exploitation of remote hosts."



LEO:  Because why work hard if you're going to exploit.



STEVE:  Because, yeah, because script kiddies need something more to do.  "Targets are collected automatically by employing the Shodan.io API.  The program allows the user to enter their platform-specific search query, such as Apache, IIS" - webcam, DVR, whatever - "upon which a list of candidates will be retrieved."



LEO:  Oh, man.  Fully automated.



STEVE:  What do you feel like attacking today?  You want to go after toasters or microwaves?



LEO:  Actually, this is - I have grudging respect, I mean, this is kind of brilliant.



STEVE:  "After this operation has been completed, the exploit component of the program will go about the business of attempting to exploit these collective targets by running a series of Metasploit modules against them.  Which Metasploit modules will be employed in this manner is determined by programmatically comparing the name of the module to the search query.  However," he says, "I have added functionality to run all available modules" - because why not.



LEO:  Why not?  Just check them all.



STEVE:  You just might get lucky.  You know, the microwave exploit might work on the toaster - "...against the targets," he says, "in a Hail Mary type of attack, as well.  The available Metasploit modules have been selected to facilitate remote code execution" - because why not - "and to attempt to gain reverse TCP shells and/or Meterpreter sessions."  We'll discuss that.



LEO:  Meterpreter.



STEVE:  We'll discuss Meterpreter sessions.  He says:  "Workspace, local host and local port for MSF facilitated back connections are configured through the dialog that comes up before the exploit component is started.



"What is Meterpreter, you ask?  Ah.  As Offensive Security" - that's the name of their site and company, Offensive Security - "describes it, Meterpreter is an advanced, dynamically extensible payload that uses in-memory DLL injection stagers and is extended over the network at runtime.  It communicates over the stager socket and provides a comprehensive client-side Ruby API."  Because we could.  "It features command history, tab completion, channels, and more."  And I got a kick out of this because finally...



LEO:  Nice UI job, by the way.  Very nice.



STEVE:  Very, yes.  The GitHub presentation notes under the topic of Operational Security Consideration:  "Receiving back connections to your local machine might not be the best idea from an OPSEC standpoint."  In other words, having the infected targets calling you.  "Instead, consider running this tool from a VPS [Virtual Private Server] that has all the required dependencies available."



So anyway, essentially this takes the notion of finding and exploiting publicly exposed machines to a new level.  And, predictably, the wider security community is unhappy.  A guy named Richard Bejtlich with TaoSecurity replied on Twitter in response to Vector's announcement there:  "There is no need to release this.  The tie to Shodan puts it over the edge.  There is no legitimate reason to put mass exploitation of public systems within the reach of script kiddies.  Just because you can do something doesn't make it wise to do so.  This will end in tears."



LEO:  Oh, what a fuddy-duddy.  Come on.



STEVE:  I don't know.  He's a spoilsport, Leo.



LEO:  Spoilsport.  A metasport.



STEVE:  A metasploitsport, yes.  If you put your toaster on the 'Net, it's because you want somebody else to [crosstalk] bread.



LEO:  Yeah, 'sploit me.  Come on, 'sploit me.



STEVE:  That's right.  And now it's just a pushbutton away.



LEO:  Wow.  "It'll end in tears," I love that.



STEVE:  It'll end in tears.  No good can come of this.



LEO:  Well, that's obvious.



STEVE:  Yes, get Autosploit on GitHub and have fun.  But again, you do want to be careful because you'll have people knocking at your door.  You don't want...



LEO:  It's a federal offense.  Let's not...



STEVE:  Yes, there is that, too.  Just because you can doesn't mean it's legal.  Firefox is continuing to advance its users' privacy, starting with Firefox 59.  Leo, do you remember those quaint old days when we were on Firefox 3?



LEO:  Three, four, yeah.



STEVE:  We had 3, had 3.2, 4.



LEO:  Those were the days; huh?



STEVE:  Fifty-nine and counting.  So starting with 59, private browsing mode will strip the path information after the domain from referer headers.  So a little bit of review.  Remember that the referer header, which is famously misspelled.  I just love that.



LEO:  Yeah. 



STEVE:  It's funny, too, because I'm sure that some copy editors somewhere are correcting them because in various stories that I saw discussing this, they fixed the spelling incorrectly.



LEO:  Yeah, no, it's one "r."  It's one "r."



STEVE:  It's one "r," yeah.  It's R-E-F-E-R-E-R is the actual usage in the HTTP specification, which of course is misspelled.  It should be R-E-F-E-R-R-E-R.  Anyway, this header, when a web page reaches out for any reason, it's going to get other assets like images or, controversially, ads.  Or if you click a link to go from that page to another page, part of the request for that asset or the next URL for the next page, it includes the referer, which as its name suggests, provides the URL of the page making the request.  So that's very handy for servers to know who's pulling images.



Back once upon a time when bandwidth was expensive, people objected to their images being stolen by other sites.  So, for example, people would put up a web page and find some cool image somewhere else.  Rather than lifting the image itself and hosting it on their own server, they would just grab the URL so that someone looking at their page would have their browser go get the image from its original source.  So on one hand you might say, well, that's nice, they didn't steal the actual image file.  On the other hand, they were causing the bandwidth to be stolen, you might argue, because the sourcing server kept having to give the image to every other browser on the planet that asked for it.



So in fact at one point that practice was blocked because servers would start looking to see whether the referer was the site's own page, which was what was intended, or some random page somewhere else.  Now, you could also use it to collect some interesting forensics because you could say, oh, look at all the different places that are pulling images from us.  You might argue that they were bandwidth pirates, but still.



Then of course the other true benefit was that you were able to see what sites had links to your site, that is, if you click on a link on a page, like to jump from one domain to another, it was useful to get the referer data.  You could see where inbound visitors were coming from.  The problem, however, is that you must be careful to use this properly.  There have been problems found.  For example, since the entire URL is normally in the referer header, if that contains sensitive information, that sensitive information can leak.



In one instance, for example, researchers at the EFF, our Electronic Frontier Foundation, found that the referer headers that were being sent from HealthCare.gov were passing on data about the age and zip code of the visitor to HealthCare.gov, along with whether or not they were a smoker, and their income.  So, whoops, a bit of a privacy problem there.  You've got to wonder who designed that site, but that's beside the point.  The problem is it can be a privacy problem.  So what Firefox is doing starting with the next version, is they will be stripping the path information from referer headers when you are in private browsing mode.



So normally the referer contains the entire URL.  But you can think of the URL as being broken into two pieces:  the domain, you know, www.google.com/, and then all of that other data, the page and also the query information.  So what Firefox will be doing is it will be retaining the domain information in case that's useful to know where someone came from, or if advertisers want to use that to glue credit for the ad being pulled by a given site, then that will be possible.  I got distracted by the video that you guys are showing.  You guys are having fun over there.



Anyway, so to prevent this type of data leakage, that will be the default for private browsing mode in Firefox.  However, in the coverage for this, I noted that Firefox does allow users a great deal of flexibility with their referers.  So I wanted to aim our listeners at a wiki page on Mozilla.org.  It's wiki.mozilla.org/security/referrer, in this case spelled correctly, so R-E-F-E-R-R-E-R.  There are some very powerful settings available in about:config.  So if you're a Firefox user, put about:config in your URL.  And as we know, you get a bazillion, that's the actual count, various entries.



Then you need to search for, spelled incorrectly, R-E-F-E-R-E-R.  And that'll bring up a bunch of very interesting settings which have normal defaults, but it's worth considering if you want to change them.  For example, there's network.http.referer.trimmingPolicy, where you're able to control how much referer to send, regardless of the origin.  That is, whether it's the same origin - or that is, how much referer.  The default is send the full URL.  You could set it to one to send the URL without its query string, or two to send only the origin, that is, only the www.domain.com whatever it is.



You also have a setting for, similarly prefixed, .XOrigin.  It's .XOriginTrimmingPolicy.  Where, depending upon whether you're in the same domain or not, the same origin or not, you can have Firefox strip out the query string or send only the origin.  And then there's also a XOriginPolicy where, again, you're able to control how much referer to send.  So anyway, there's a lot of cool stuff buried in Firefox, if you put about:config and then search for "referer," R-E-F-E-R-E-R.  Without doing any of that, the next version of Firefox will start stripping the path and query tail from all referer URLs.  So again, continuing to look after our privacy.



And I did say I wanted to talk about something that I think is really sort of specious, but it's just sort of interesting.  It got coverage in the press, which sort of surprised me.  Researchers at Fidelis Cybersecurity say that they have identified a new technique that attackers can potentially employ for covertly exchanging data using standard security digital certificates.  X.509 is the format of these digital certs.  And they say that the method builds on previous research involving the abuse of text fields in digital certificates to move data across a network.  It takes advantage of the way digital certificates are exchanged during the initial TLS handshake.  Or the mutual authentication process that happens when two systems attempt to establish and resume a secure session with each other.



Jason Reaves is the principal threat research engineer at Fidelis, and he said:  "Most other research involving using X.509 certificates for data transfer" - now, remember, certificates are normally used only to authenticate the identity of the endpoints.  So the idea of using them for data transfer is, first of all, bizarre.  But, he says:  "Most other research involving using X.509 certs for data transfer involves the use of text fields in the certificate such as 'Subject' or some of the other common fields such as 'notbefore' and 'notafter'."  Okay, right, those are the start and end dates of the certificate.



He says:  "Researchers have previously shown how attackers might use these text fields to covertly send and receive data between systems.  Our method," he says, "is embedding data inside of a certificate extension.  This means you can send data between two systems purely from the TLS negotiation."



So I'm thinking about that, and it's like, wait a minute.  All of the certificate body, all of the contents is subject to the signature of the signer.  So the only way that you can be offering valid certificates is if all of these wacky data-carrying certs are signed by a third party that the recipient trusts.  Now, maybe they're suggesting, and this was not clear, they're suggesting that you just manufacture bogus certificates, which are invalid, that don't carry a valid signature, and thrust them upon the other endpoint.  And so I guess if something was watching the other end, it could be extracting this data from invalid certs which are going to be rejected anyway.



I mean, the whole thing just, like, okay.  It just was bizarre.  I was surprised that it got picked up in the security press as, like, something useful in some fashion because I just can't see.  Maybe, as I was like brainstorming, thinking how could you make this work, well, if Let's Encrypt was involved, that is, if you were using their dynamic API in order to make certificates for you on the fly, which can be done, then you could be minting valid certificates and be sticking extension fields into them in order to send data to the other end.  But, okay, why not just let the TLS connection come up, and then you've got encryption.  And if you want to send data.  I don't know.  The whole thing seemed kind of wacky.  But who knows.  Maybe it makes sense.



I did have yet another interesting application of SpinRite, not quite as strange as last week's.  But I got email from Matt Bokan with the subject "DJ Steve - SpinRite Gibson saves the concert."  And he wrote:  "Hi, Steve.  I'm not a music producer, but I've got a friend who's making music on a Korg synthesizer which cost many, many kilo dollars.  These beasts," he writes, "have had SSDs in them for a long time now."  And of course our listeners know where this is headed.



"On the day of the concert there was a sound check, and this synth has all the rhythms, beats, and whatnot of the band stored inside.  But on that day it wasn't working properly.  Everything was having such a lag, or wouldn't load properly, that the concert was in danger and in the process of being canceled.  I had helped this man set up and connect his living room with projector, surround, and PC in my youth.  So I got a call to ask if I could somehow transfer the data to another identical synth if they could find one on short notice."



He says:  "I didn't even know about Korg synths having SSDs.  But when I found out, I ran SpinRite, and half an hour later the synth was singing like a rock star.  The concert was not canceled at the last second, and thousands of concertgoers sadly don't even know that Steve Gibson was the true rock star that day."  Okay, well...



LEO:  Aw.  That's sweet.



STEVE:  Matt, thank you very much for sharing your story.  It's certainly appreciated.



LEO:  Yeah, it's neat.



STEVE:  And yet another application of SpinRite.  Two interesting bits of closing the loop.  Chris Duncan, who tweeted from @cyberdunks, said:  "@SGgrc Is it possible for @intel to encrypt and decrypt the on-chip cache on the fly using its own keys as a mitigation for Spectre?  That way the chip knows what's in the cache for speculative execution and not the OS."  And I thought that was interesting.  And it sort of gives me an opportunity to explain a little bit more why that won't work.  And that is that it's not the access to the data by the other application in the cache which is the problem.  That is, the  caching is transparent.  And the system handles loading the cache and invalidating the cache and all of that on its own.



The trick is the timing.  That's why we keep talking about access and the need for high-resolution time.  The idea is that the contents of the cache is inferred by a malicious bit of code doing something and carefully looking at how long it took to do that something, whatever it was.  If it took no time at all, then the code knows that the system had what was necessary to do that little bit of work, that fetch or that store or whatever, in the cache already.  If it does something, and it takes longer, that is, literally the instruction, that one instruction takes longer to execute, then the malicious program knows, it's able to infer that what it asked the system to do could not be fulfilled from the cache's contents.



And it's because the cache is globally shared among all the processes on the system, because of that global sharing, it's able to infer what other processes had caused to be loaded in the cache from its own timing.  And this is why this is not a bug in the Intel architecture.  This is just - it's hard to call it a feature.  I mean, it is a feature, but it turns out it's dangerous.  It's very clever on the part of researchers to have figured out that this cache, the delay of a single instruction being executed could be leveraged into inferring the contents of the system's cache.  And it's because all the other processes are sharing that cache, they're leaving evidence of what they've done behind, based on just what's still in the cache or not.



So as you can see, whether the system, no process is like seeing encrypted or decrypted contents of the cache, so encrypting it or decrypting it wouldn't change this.  It's whether it's there or not, that go/no go.  Was there a delay while the processor went out to fetch what was requested, or was it still available in the cache?  So by that very delicate timing difference it turns out malware is able to leverage that to create an information disclosure problem.  And that's where we lost January to was all of that.  And as we'll be covering in one second here, that's not the end of it.



I did want to mention, this came back onto my radar from someone tweeting.  Rob Fairhead tweeted:  "Upgraded my network following the excellent Ubiquiti Home Network project at" - and then he gives a GitHub address.  It's github.com/mjp66/Ubiquiti.  Which he said:  "...first seen on @SGgrc #SecurityNow."  He says:  "Now have segregation for IoT devices and guest network.  Longtime itch scratched."  And I was reminded of that.  So I went to take a look at what Mike Potts had done.  He now has, off of that page, a 105-page beautifully detailed, really it's a work of PDF art.  And I noted that he had recently updated this to add support for the Quad 9 DNS provider.



So Mike is clearly, I mean, he is a Security Now! listener.  He's a Ubiquiti router user.  And he put together this beautiful how-to on using the amazing power in that $49 Ubiquiti EdgeRouter X in order to create a highly segmented network where all of your dangerous toasters and microwaves and IoT devices can be on your network with access to the Internet.  But anything nasty that might climb into them cannot get over into your mainstream PCs, where presumably you want to keep anything bad out.  Anyway, I just wanted to point our listeners again to Mike Potts' beautiful work on this now 105-page PDF, which now he recently edited to add support for using the Quad 9 DNS.  Nice work, Mike.



Okay.  What's happening with Meltdown and Spectre?  The good news is there is good news.



LEO:  That's good news.



STEVE:  That's good news, yes.  Because last week we were saying, oh, don't worry about this.  It hasn't appeared in the wild yet.  It's all theoretical.  Yes, it's a problem.  But yes, Intel hasn't yet issued firmware.  And I ought to note that apparently they just did.  I found out about this last night while I was pulling things together, and I was doing that away from my normal office environment.  I have a NUC that Intel - what was called Next Unit of Computing, NUC.  It's a kind of a - it's mini form factor PC.  They have issued Spectre mitigation firmware for their own family of NUCs.  I don't know about anything else yet, but I will certainly keep an eye on that.  So it looks like they are beginning to come out, to come back now with firmware which they feel confident for Haswell and later processor architectures will work without the, quote, "higher incidence of rebooting" unquote problem.  So that's good.



However, an organization known as AV-TEST, which is an independent organization which evaluates and rates antivirus and security suite software for Microsoft Windows and Android platforms, has been monitoring an increase in hits by AV software on the known patterns used by Meltdown and Spectre.  By January 17th of last month, so middle of the month, AV-TEST reported it had seen 77 separate malware samples related to the Meltdown and Spectre CPU vulnerabilities.  By the 23rd, that number, 77, had grown to 119.  And by last Wednesday, which was January 31st, last day of the month of January, they had collected a total of 139 samples from various sources, researchers, testers, and AV companies.  These do not appear to be weaponized, active, like successful data exfiltration, but this is the beginning.



So what we're seeing, what this looks like is this is clearly on the radar of every attacker, every malware author worth their salt.  We were just talking at the beginning of the podcast about the fact that EternalBlue, a now long since nine months ago patched Windows vulnerability, is in active use today.  It seems obvious that these sorts of vulnerabilities, the Meltdown and Spectre vulnerabilities, which have not yet been weaponized, are going to be.  And what's necessary is to somehow arrange to get code to run on a platform.  As we've always said, shared hosting is the biggest problem; but there's one place where code can run easily from a remote source, and that's in our browsers.  Both JavaScript and WebAssembly are two ways that your computer is running code from someone else.



Now, obviously Google is on top of this.  They've got their Retpoline solution.  Remember, that's the Return Trampoline mitigation that we talked about, which is a mitigation against Spectre, which does not require firmware updates.  People are going to feel more comfortable with their firmware updated, especially hosted serving environments.  That's really - the hosted server environment is the biggest target, where you would be running some code from one customer, and you'd want isolation between it and code running on shared platforms from other customers.



However, end users are - there's some danger.  But, boy, if you've got something malicious running in your own local system, our own OSes are so insecure that they don't have to bother with Retpoline or, I mean, they don't have to bother with Meltdown and Spectre in order to get up to mischief in your own system.  The place where we as end users need to be on the lookout is our browsers, which are constantly, as we know, now running code from other sources.



The good news is Google is on this.  I'm sure that Mozilla is on it, and Microsoft is on it.  Hardening our browsers is really what has to happen.  And that's a relatively contained code base.  So basically it means using Google's very clever and publicly released Retpoline technology on the code of the browsers so that no JavaScript or WebAssembly code obtained from a third-party source can be successful.  But the reason this is still of interest to attackers is, as we've said, just as it's been nine months since Microsoft patched for the EternalBlue exploit, and it's now being widely used, there are going to be a huge, huge number of browsers that are not being updated.



So it still makes sense for bad guys to see if there is a way to take these Meltdown and Spectre vulnerabilities and use them to exfiltrate information from users' browsers, like the username and password database that so many browsers now contain in order to fill in forms for users.  And in fact proof-of-concept browser exploitation code has been found in the wild for IE, Chrome, and Firefox.  It now exists.  It has not been proven to be effective, but the bad guys are poking at it and looking at it and developing proof of concepts and trying to use it.



So I would say one thing we want to do, if your firmware is updated, then you have no vulnerabilities.  That is, you're running Windows.  GRC's InSpectre app shows that you're secure for both exploits, Meltdown and Spectre.  You get yes, you're safe, yes, you're safe in both cases.  Then you're okay.  The firmware, as I mentioned, as we know, got released and then retracted, and Intel is apparently now working on beginning to get it out again.  So if you can get your firmware updated, again, you're safe.



If you can't, then as long as your browser is updated for Spectre mitigation - because it's the Spectre, of the two, Windows and presumably any of the other OSes, those immediately mitigated against the Meltdown problem.  As long as your browser is Spectre-proof, then you're okay.  And I'm sure that here before long we will see some browser actual test site stuff to verify whether browsers are safe or not from Spectre.  But so that's where we stand.  We are now seeing emerging, as would have been predicted, people beginning to play around with these things.



The good news is there's enough of a window, the industry responded immediately, we got immediate Meltdown protection, and that was the easiest to exploit and the easiest to fix.  The browsers can fix Spectre.  And I would argue that that's the big source of vulnerability is letting either JavaScript or WebAssembly run in our browser, in a browser that is not - where both we do not have updated firmware and the browser has not been updated to employ its own mitigations.  If either of those is true, if the browser is updated or you've got updated firmware, then you're okay.  So we will of course be keeping an eye on this moving forward and also keep track of where Intel is with firmware updates because that's really what we want.



LEO:  Well, I'm just glad that it's not an effective exploit.  But you're right, that's the first step, isn't it.  Just get something, yeah.



STEVE:  Yup.



LEO:  Well, thank you for keeping us apprised of the ongoing situation in the Spectre/Meltdown, or Speltdown.  Or Mectre.  We haven't yet really - Melctre.  Steve Gibson does this show every Tuesday at 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  If you want to come by and say hi, you can watch at TWiT.tv/live.  Please join us in the chatroom.  A great bunch of people in there at irc.twit.tv.  Kind of a support group for those of us shivering in our boots.



We are now - we have a Flash Briefing, you know, we do of this show.  Sometimes we do an on-air Amazon Echo.  And it is now available, not only in the U.S., but Canada, Australia, the United Kingdom, and India.  So if you live in one of those countries you can get a little bit of TWiT in your Flash Briefings, some technology news from this show and many of our other shows, just by going to your Echo app and searching for TWiT and adding that to the Flash Briefing.  And you know, I think it's like 50,000 people do it now.  The number's going up fast.  So thank you for doing that.



It's also time for our annual survey, another thanks to you for doing that.  We don't really want to pry into your personal affairs.  We're not trying to get your information.  But on the other hand we kind of need to know a little bit about you to help us talk to advertisers, know which advertisers you're interested in and what programming you're interested in.



So once a year, just once a year we do a survey, completely voluntary, you don't have to give us any information, you can check the boxes you care about and not the ones you don't, or all of them, if you want.  It should just take a few minutes.  It's at TWiT.tv/survey.  It is very helpful to us.  So if you get a chance, and you're in front of a browser, TWiT.tv/survey.  I promise we are not going to use any of this information personally about you.  It's all in aggregate, as I explain on the website.  You can see my attempt at explaining what's going on at TWiT.tv/survey.



If you like this show, you can get a copy of it from Steve at his site, GRC.com, Gibson Research Corporation.  In fact, while you're there get SpinRite, the world's best hard drive and macovery rutility, recovery mutility, you know, the best program for recovering your hard drives.  SpinRite is his bread and butter, so you support Steve when you buy a copy of SpinRite.  And of course you keep your drives happy.  That's GRC.com.  While you're there, there's lots of free stuff, too, not only this show but free programs, lots of information.  It's fun to browse around.  If you do want the program, he has not only MP3 audio of it, but he has the transcripts.  Elaine Farris, she's back now?



STEVE:  Yup, yup.  Well, actually this week also we will be a little bit late.



LEO:  A little late, okay.  But you'll get it there.  And what's nice is you can search those, which is a great way to find something in any of the 649 shows.  But you can also - some people like to read.  Some people are more visual, they like to read while they listen, whatever.  It's there for you.  We have audio and video of this show at our website, TWiT.tv/sn.  We also put it on a YouTube channel.



And more importantly, go to your podcast app on your phone, on your tablet, on your computer, and subscribe, whether it's iTunes or Pocket Cast or Overcast.  And that way you'll get an episode each and every week, the minute it's available.  You won't have to wait.  GRC.com is Steve's site.  You can tweet him at @SGgrc.  That's where he gets his questions, his comments, his ideas.  And you will be, I presume, back next week?



STEVE:  Yes, for Episode 650.  Woohoo!



LEO:  Yay.  What's that in hex?  No, pretend I didn't ask.



STEVE:  No.



LEO:  Thank you, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#650

DATE:		February 13, 2018

TITLE:		Cryptocurrency Antics

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-650.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss today's preempted Second Tuesday of the Month, slow progress on the Intel Spectre firmware update front, a worse-than-originally-thought Cisco firewall appliance vulnerability, the unsuspected threat of hovering hacking drones, hacking at the Winter Olympics, Kaspersky's continuing unhappiness, the historic leak of Apple's iOS boot source code, a critical WiFi update for some Lenovo laptop users, a glitch at WordPress, a bit of miscellany (including a passwords rap), some closing-the-loop feedback from our listeners, and then a look at a handful of cryptocurrency antics.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.   We've got lots to talk about, some fascinating facts about cryptocurrency, a little more about Spectre and Meltdown.  And we'll continue to find a name - or look for, anyway, I don't know if we'll find it - for the Spectre and Meltdown flaws in Intel.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 650, recorded Tuesday, February 13th, 2018:  Cryptocurrency Antics.



It's time for Security Now!, the show where we cover the latest security news, help you protect yourself, help you really deeply understand how computers and technology work, and it's all thanks to our Explainer in Chief, Mr. Steve Gibson.



STEVE GIBSON:  Leo, great to be with you again for the - I don't know why this number hits me.  I mean, like 666, that would be a good one, too.  But this one is 650.  I guess just the roundness of it.



LEO:  I know why.  Because Bill Gates famously said no one would ever need more than 640 Security Nows.  Right?



STEVE:  That's 640K, that's right, 640.  No, nobody needs more than that.



LEO:  Nobody would ever need more than that.



STEVE:  As I was pulling things together this week, there were a surprising number of interesting stories that involved cryptocurrency in one way or another.  It's been a topic for us because, as we know, dating from our first description of how the bitcoin block chain works, then sort of cryptocurrency went silent for a long time, and recently it's just, like, everybody's talking about it, and my mother is asking whether I should have her - or once asked.  She's not asking any longer.  But a friend of hers was wondering if she should invest in bitcoin, and Mom called up and said, "Honey, do you know what a bitcoin is?"  I thought, yeah.



LEO:  It's so funny, my dad sent me an email saying, "I'd like to find out more about this block chain thing."



STEVE:  Yeah.  So there's, like, some bizarre fun stories that involve cryptocurrency.  And so as I was putting everything together, I realized, okay, there wasn't any single big news event which we normally put at the end and acquires the title of the podcast.  So this one is Cryptocurrency Antics.  And we have five interesting sort of antics that we'll talk about as we wrap up the podcast, but lots of other interesting news, I think.



We've got what turns out to be sort of a preempted Second Tuesday of the Month because this is it.  This is one of those months where - and we always sort of note them - where the first of the month is Wednesday, which means that the second Tuesday is as late as it can possibly fall - which is the 13th, interesting - in the month.  Which is normally the big patch time.  But as we mentioned last week when we talked about Woody's enumeration of the fact that 15 out of the 30 days or 31 days in January had updates in them.  So anyway, so we've got the preempted Second Tuesday of the Month.



Tracking the slow progress on Intel's Spectre firmware update front.  A worse-than-originally-thought Cisco firewall appliance vulnerability.  I knew about this last week, but it looked like the conditions where it would be a problem were restricted, and it didn't really affect end users.  It was an enterprise problem.  Well, since then it's sort of exploded, so I thought, okay, we've got to talk about that.  Also, MIT Technologies Journal introduced this notion of an unsuspected threat from hovering hacking drones.  And I thought, well, okay, that sounds kind of fun.  We also have hacking at the Winter Olympics to talk about,  Kaspersky's continuing unhappiness, and the historic leak of Apple's iOS boot source code, which...



LEO:  I was hoping you'd talk about that, yeah.



STEVE:  Yeah, got to talk about it.  And what I love about this story is how it's the classic way that this sort of happens where it wasn't malicious, but it was inevitable.  So it's sort of fun.  Also there's a critical WiFi update for some Lenovo laptop users.  A glitch at WordPress, that's a sponsor, and so it's sort of an interesting automatic update thing that I want to make our listeners aware of.  Then we have a little bit of miscellany.  Oh, and I have to have you play, because I think we're going to have time, an interesting rap on passwords.  Don't know if you saw it.  But if you want to scroll down while I'm doing some of this and take a look at it.



LEO:  I'll prepare to play it, yes.



STEVE:  It is SFW, meaning it is safe for work.  And it's really just - it's cute.  It's a few minutes long.  It's posted over on Facebook, and the link is here in the show notes under Miscellany.  Then we'll close the loop with some of our listeners.  Most of the feedback was people suggesting names in response to my musing last week that it was annoying to have to say Meltdown and Spectre and Spectre and Meltdown, and we really need a concatenation somehow.  So we've got every possible variation that you could imagine that our listeners have provided.  And then we're going to take a look at some cryptocurrency antics, including finishing up with one that just makes you do a face plant.  It's just - it's too amazing.  So I think a great podcast for us.



LEO:  And I have an Image of the Week to join your Image of the Week.



STEVE:  Oh, yay.  Cool.



LEO:  Yeah.  I sent it to you, but maybe you didn't see the email.  I know you don't like to look at email.



STEVE:  Actually, I saw it, but I didn't - I don't know, I guess I didn't click on the link, or I didn't find the image.  But I had seen what you were referring to, so I'm glad you'll be able to provide it.



LEO:  Well, we had mentioned it last week.  And the guy who actually created it, or knew the guy, it was created in his office, sent us more information about it.  So I will pull that up, as well.  All right.



STEVE:  So our Picture of the Week ties into the second thing we're going to talk about at the top of the show.  And this is just another fun, I mean, people having a lot of fun with the Intel logos, and we've been having fun with them during the various, well, all year so far, since this whole Meltdown and Spectre nightmare.  And so this is just - it's the Spectre Inside logo that we talked about last week.  This time there's a little circumflex, and it says "still" inside, "Spectre Still Inside."  And that is the case.



In fact, I'll just jump over, and we'll talk about the second Tuesday in a second.  I went looking for the podcast for the latest news from Intel, and it's rather quiet.  The only thing I could find was another press release written in the first person.  I'm always a little jarred by this, when Navin Shenoy at Intel writes in the first person.  But on February 7th he posted:  "Security Issue Update:  Progress Continues on Firmware Updates," which, oh, that's good, they haven't given up.  We don't want them to give up.



Anyway, so Navin said:  "Intel continues to work closely with industry partners to protect customers against the security exploits disclosed by Google Project Zero.  As I shared" - which is always the part that jars me a little bit - "January 22nd, we identified the root cause of the reboot issue affecting the initial" - and remember this is the guy who was saying they were rebooting more often than usual, like what?  Anyway, at all would be bad - "affecting the initial Broadwell and Haswell microcode updates.  Since then, we've been focused on developing and validating updated microcode solutions for those and other impacted platforms."



And then he said:  "Earlier this week, we released production" - now, this is news - "production microcode updates for several Skylake-based platforms to our OEM customers and industry partners, and we expect to do the same for more platforms in coming days.  We also continue to release beta microcode updates so that customers and partners have the opportunity to conduct extensive testing before we move them into production."



So last week Skylake mobile and desktop firmware was released.  So as we remember from this debacle, the first microcode update attempted to add instruction set features to give operating systems control over branch prediction, which is this aspect of the speculative execution and branch prediction which was what Spectre was able to leverage.  Those first firmware updates were releases for Broadwell, Haswell, Skylake, Kaby Lake, and Coffee Lake processors.  But we quickly learned that Broadwell and Haswell systems were having trouble.  And then we later determined that all of them were - Skylake, Kaby Lake, and Coffee Lake also.



So that shut down all of this firmware.  It all got pulled back.  In some cases, new BIOSes were made available to put the old firmware back.  We know that Microsoft ended up producing an official patch which wasn't pushed that their customers could go and get to set the bits in the registry to disable their use of these new firmware instructions.  And as I mentioned last week, it turns out that the same bits that GRC's little InSpectre app sets if you want to deliberately disable the mitigations for these.



So at this point the latest news I have been able to find is that last week Intel did release firmware for Skylake, but for no other major processor families.  So the expectation is that those who have Skylake-based systems may shortly, after the OEMs verify probably more carefully than they did last time that the firmware is not causing trouble, may be able to get firmware updates.



So that's where we are.  This is taking a while.  And again, I'm impressed that Intel can do something this significant with firmware.  I mean, as I explained a couple weeks ago, firmware in a microprocessor is not like software, application-level software where you can write word processors, you can write graphics programs, you can write image compressors, I mean, you could do anything because it's meant to be a general purpose instruction set that the software runs.



Firmware, that microcode firmware is not a general purpose instruction set.  It's a way of simplifying an incredibly complex hardware design by moving some of it essentially into a masked ROM which allows some sequencing to manage the flow of data through pathways in the chip.  And it's easier to do it with some data patterns.  And what Intel provides is the ability to override that built-in ROM with a microcode patch.



But the idea of coming along after the fact and arranging firmware to have control over something you never intended it to control, I mean, this is a major design revision to the processors.  And so I'm impressed that they can do it at all.  I mean, they haven't done it yet, apparently.  So I guess they have on Skylake.  But it is an architecture, an intensely architecture-dependent thing.



It's possible, and I'm not predicting this, and I'm way on the outside, so I have no inside knowledge, but I wouldn't be surprised if they said we can't fix this on this or that processor.  That is, we can't fix it in firmware.  The firmware doesn't have the flexibility.  That wouldn't surprise me.  I would say, okay, well, that's entirely reasonable for that to be true.



So it's going to be interesting to see how this progresses.  The last thing Intel wants to say is a fundamental problem has been discovered in our chips that we can't fix.  But the idea that these chips are even fixable is something we sort of take for granted.  But they may not be.  So it's going to be interesting to see how this plays out over time.  At this point, Skylake, they've released the firmware for that, and we'll be waiting for firmware for the others.



And I mentioned that the Second Tuesday of the Month is today, and that as a consequence of the crazy startup that we've had this year, we got the February security rollup update for our systems last Tuesday, the 6th.  And it wasn't clear to me, because we've had so many of them, I mean, it's been a week.  Which is longer than we went all through January.  But, okay.  I fired up my machine and looked for updates, and nothing came.



So it looks to me like the only thing that we are getting today was what Microsoft did push, which was an update for the Adobe Flash zero-day which we discussed last week.  And so that was made available today.  But it looks like Microsoft is going to sit tight with the major rollup that we received for February last week.  So a week ahead of schedule, ahead of their normal schedule.



I mentioned also Cisco's problem.  Late January, so a couple weeks ago, the news hit that there had been responsible disclosure to Cisco from a Cedric Halbronn with the NCC Group that they had discovered a very worrisome vulnerability in a family of Cisco enterprise class, like kind of big iron firewalls, they're collectively known as "adaptive security appliances."  ASA is the acronym.



And at the time it was initially believed that only the VPN component of those ASA devices - and there's a family of them, but the initial list was like maybe five or six of them of a much greater number were vulnerable.  And that it only affected some configurations.  And I remember thinking, okay, well, this doesn't really impact our listeners directly.  It's not like consumer routers or something.



Well, since then, several things happened.  The number of affected devices has more than doubled, and the list of subsystems, the subsystem components went from one, meaning the VPN, to 11.  So all kinds of things in there are vulnerable.  And it has now become attacked in public.  We are now seeing hackers actively exploiting the vulnerability in these devices.  And there is something on the order of 200,000 of them publicly exposed.  These are inherently Internet-facing firewall appliances from Cisco, so their job is to be out there on the front line, keeping people from getting into your network.



Unfortunately, it itself is vulnerable.  And this has got an exploitation score of 10 out of 10.  So it's critical severity.  And the 10 out of 10, in order to get 10 out of 10 it has to be, essentially, as bad as it can get.  It's easy to exploit so it has a low level of attack code complexity.  It can be exploited remotely and requires no authentication on the device.  So the good news is this was disclosed responsibly.  Cisco made the patches available.



On the other hand, as we know from all the machines, all the Windows machines, for example, servers that are still unpatched, as we were talking about last week, months and months after important vulnerabilities had been made available, anybody running in an enterprise environment that has one of these adaptive security appliances needs to make sure that they're current to their current patch level because it is now not just a known problem, but it is actively being exploited in the wild.



MIT's Technology Review had an interesting story about cyberwarfare taking to the skies aboard drones.  And when I encountered that, I thought, what?  I think I remember sometime ago we were having fun with the idea of a drone that had some sort of a projectile launcher on it, and we were joking that, if the projectile was too heavy, the drone would get pushed backwards from recoil when it tried to launch this.  In this case, these are radio relay drones.  And it turns out they exist in actuality.  It is a new mode of attack, the idea being that there are some instances where people improperly believe that, if they've got unsecure radio, they are safe from attack, that is, just due to the fact that, oh, well, the range isn't that great.



A perfect example was a demo that was given at one of the hacking conferences recently where a drone, one of these hacking drones was flown outside of a building and was able to pick up the radio signal of a wireless mouse.  And although you'd have to watch the signal and do a lot of reverse-engineering in order to figure out what was going on, you could certainly from that get position and clicks.  And potentially that's an information leakage.  It doesn't seem that worrisome to me.  But this is, of course, as we know, this is always a way that these things start.



LEO:  And a wireless keyboard would be more informational.



STEVE:  Exactly.  And we know that, as we had talked about, some of the early ones just used pathetic encryption.  I mean, you can't even call it encryption.



LEO:  ROT13 or something, yeah.



STEVE:  Yeah, well, yeah, in fact it was - they're putting out 8-bit ASCII, and they came up with a random 8-bit mask which just XORed the bits.



LEO:  But they do use the same mask for every keyboard, right, yeah.



STEVE:  Yeah, yeah.  As soon as you put the battery in, it sets this XOR mask.  And it's like, okay, wait.  I mean, in fact, that would be a nice final exam question for CS 101 or an Introduction to Computer Security is here's a stream of text which is from a wireless keyboard with a static 8-bit XOR.  What was being typed?  Because frequency analysis immediately collapses this encoding.  And as soon as you - the interesting thing about this, I mean, how bad that is, is as soon as you identify like what the T or the S or the E is, as soon as you can figure out one character...



LEO:  Right, you're done.



STEVE:  ...then you immediately know what the mask is, and the entire thing collapses.  So Leo, that's...



LEO:  It's a convenient thing about XOR.  It goes both ways.



STEVE:  Yes, exactly.  And so your example, Leo, is perfect.  So what is being done is that there are now experiments being conducted with drones being used to move essentially a spy outpost into an unsuspected location.  So it's easy for people to think, oh, well, we're out in the middle of nowhere.  We wouldn't have to worry about securing our WiFi or using secure phone communications and so forth.  Well, it's just worth planting the seed that lack of proximity to radio is no longer any true security, and that spy drones sucking in radio are becoming a thing.  Yikes.



The Winter Olympics, as we know, are underway as we're doing the podcast, and so is Olympic hacking.  There haven't been any clear stories yet to surface from this except that it is the case that tensions were expected to be lower with North Korea because they're part of the Olympics now, and so there's not the heightened problems with diplomatic relations.  On the other hand, Russian athletes were banned from participation.



And so it was expected and has turned out that Russia's hacking efforts were running full bore, and that there were multiple nations' security teams who were in place for months beforehand and have been fending off and observing attacks against the Olympic network's infrastructure that have been going on.  For months beforehand systems were being probed.  And it's looking more like they're trying to create embarrassment, like of individuals or the actual operating of the system in real time.  So no huge events from that.  We'll have to see.  Maybe a couple weeks from now we'll have more information.  But it is an obvious target for attackers, and they're not missing the opportunity.



Also, yesterday, Kaspersky Lab filed another lawsuit against the U.S.  We talked about Kaspersky's unhappiness with the U.S.'s decision late last year to back away from the use of and in fact moving forward to forbid the use of their antivirus tools over concern that there was some Russian state government influence over what was going on.  We talked about the news that apparently it was as a consequence - which seemed inadvertent and may well have been inadvertent - of Kaspersky's AV picking up some code samples and phoning home with them, that that was a way that there had been some exfiltration of confidential data from systems that were being protected by Kaspersky's AV.



So this lawsuit filed just yesterday is related to this ban on the U.S. use of Kaspersky's products.  That was part of the 2018 National Defense Authorization Act that the Trump administration signed into law at the beginning of October.  And in the court documents filed yesterday in the U.S. District Court of D.C., the District of Columbia, Kaspersky is claiming that the ban is unconstitutional.  Their lawyers are saying that under the U.S. Constitution's Bill of Attainder clause, Congress is forbidden from enacting laws which impose individualized deprivations of life, liberty, and property, and inflict punishment on individuals and corporations without judicial trial.



So I understand, and we've talked about, their unhappiness.  But it's difficult to forgive our agencies from being, I think, reasonably cautious over this idea.  As I have said for quite some time, I'm surprised that Microsoft Windows is as widely deployed globally as it is because it's a closed system.  And apparently Microsoft does make source code selectively available in some instances to major states where they have, like, they have to show what they're doing in order to get the okay.  But we've moved to a mode now where, as we know, Windows 10 systems no longer have any choice about accepting updates.  So Microsoft literally has an institutionalized software implantation system in place today.  We have no reason to believe that Microsoft would ever or has ever abused that; and it would be, I think, the last thing that they would do.  But the facility is there.



So given that we've got increasingly mature alternatives, open source alternatives, it's just difficult for me to see how anything but inertia is responsible for continuing to use these sorts of systems.  So anyway, it'll be interesting to see.  I doubt anything will come of this.  I think that - I'm sure that our governments, our state and local and national governments have a right to choose which software they wish to use.  It is creepy to have antivirus systems installed which maintain a connection back outside of the country, and specifically to Russia, that have the ability to send questionable software back for analysis.  That seems like something you want to think twice about.



And the story, Leo, that you wanted to talk about, and I'm glad you were wanting to go into this because I think this is really interesting, Apple had the source of their iOS boot code posted on GitHub last week, and it's not surprisingly called iBoot, which is the code which first runs as the system powers up in order to form the anchor of the chain of trust which brings the whole system online.  And we've talked about the way secure boot technologies in general work, and Apple's is no different.  That is, you start with some highly trusted code which is probably in ROM or is signed, and its signature is verified in a way that cannot be interfered with, intercepted, contravened, you know, just there's no way to subvert that.



That code comes up, makes sure that it has not been tampered with, and then it goes and loads the next stage in the boot sequence and, similarly, after obtaining it from storage, but before ever executing it, absolutely verifies that it, too, has had its integrity maintained and is then safe to transfer control to.  Control then transfers to it, and it repeats that process, essentially forming a chain of trust where each link is independently verified from the code that's already running, prior to turning control over to it.  The point is that the entire integrity of the system depends upon the anchor, that original code.  And so it is significant that the source for that was posted last week.



Now, Apple has sort of downplayed this a little bit.  In their statement they said:  "Old source code from three years ago appears to have been leaked, but by design the security of our products," writes Apple, "does not depend on the secrecy of our source code."  Well, we know that that's good.  "There are many layers of hardware and software protections built into our products," they write, "and we always encourage customers to update to the newest software releases to benefit from the latest protections."



At the same time we also know that this kind of core code doesn't change very much.  I mean, this was part of iOS9 back in 2016 when this code leaked.  And it's certainly the case that, now we're at 11, there may well have been some changes.  There may be some, and probably are, some new hardware in the later, yeah, some new security hardware in the later iPhone hardware.  So it's probable that the iBoot code was tweaked a bit.



At the same time, we also know that a lot of code doesn't change.  Windows users are seeing dialog boxes, if you drill down underneath the UI very far, back from Windows 2000 that haven't changed.  If you bring up the network adapter details dialog, that's the dialog from Windows 2000.  That core code that is done tends not to change.  So Apple's understandable sensitivity is not only to the fact of the leak, but to the fact that, while their statement suggests correctly that they're not depending upon obscurity, it is also the case that bad guys or phone hackers will be salivating over the idea of having access to this iBoot source because by attacking the source code from the standpoint of trying to find something that was done wrong, you can just get a lot of ideas, a lot of potential leads for where to attack.



So the interesting thing about this from a sort of a reality of keeping things private is that an intern, what was described as a low-level employee in some of the coverage, and I also saw the term "intern" used, a couple years ago had some friends in the phone hacking, in the jailbreaking community.  And he was at Intel, he or she, don't know, I'm sorry, I keep saying Intel, was at Apple.  And the friends were pushing and pushing and pushing to have this person share this iBoot source and a bunch of other tools, as well, that this person had access to.



So this was a small group of trusted individuals.  And I should mention that Motherboard had really good reporting and coverage of this.  They interviewed, under promises of anonymity, a lot of these people who had firsthand knowledge of how this happened.  So there was a very tight-knit small group, friends of this intern, who did receive this iBoot source code two years ago under the promise that it would not be shared outside their little circle.



LEO:  I'll give it to you, man, but you've got to swear, swear you don't give this to anybody else.



STEVE:  Exactly.



LEO:  You gotta swear, pinky swear, man.



STEVE:  And to their credit, I guess, that promise endured for two years.  But the code existed nevertheless on thumb drives or on hard drives outside of Apple.  And some secrets are just too good to keep.  They're just too difficult.  And so you can imagine that when it was new, when it was fresh, when it had just been received, it was, oh, nobody else has this.  This is super cool.



And so the people outside of Apple had their own proprietary interest in keeping it safe.  But those kinds of secrets are hard to keep.  And so they wanted to share it.  One of them - and within this group, they don't today know who leaked it outside of their little group.  But presumably someone did.  Somebody had a friend and of course said, okay, I'm going to share this super cool code with you, but you absolutely have to double pinky swear that it goes no further.  Oh, of course, of course, of course.  And of course, who knows, maybe it was that person.



The point is that some secrets are just too hard to keep.  And so through essentially what amounts to social engineering and insider disclosure, this code got out.  This wasn't a flaw in a highly academically scrutinized cryptographic cipher where, oh, my god, if you number crunch this thing for 12 CPU centuries, then there was a chance that you could find some information leakage.  No, this was an intern who was at Apple who had access to this and did sign an NDA, of course.  There was some mention of that in the Motherboard coverage that the person of course, I mean, he had to sign a confidentiality agreement in order to be there and to have this kind of access.  But that didn't in fact prevent it from escaping.



So it's not clear that this, I mean, this is not probably devastating.  It is true that the other side of this code being as mature and unchanging as it probably is, is that it is probably mature.  But Apple did immediately use a DMCA takedown order to get this yanked from GitHub.  It also was somewhere else, I can't remember, where it was removed from.  So basically Apple immediately moved to clean this up.  And I imagine Apple has a security team that is probably hot on the trail of how this happened, working to remove as much of this from public access as possible.  But it was on GitHub for, like, the fact that it was there at all means it disappeared, I mean, it got loose.  So it is now out in the jailbreaking, the greater jailbreaking community.



Someone whose handle was ZioShiba, Z-I-O-S-H-I-B-A, was the pseudonym of the person who posted this on GitHub.  And among the people who knew of the original leak it was identified as a "copy," whatever that means.  But what we understand it means is that this was a subset of the total leak package.  So there was more that was leaked than ended up being posted.  So maybe the disclosure from the original small group was less than all that was posted, or who knows what path the leak took to get to GitHub.  But it was there, and it was up for some length of time.  It got a lot of attention.



And so I don't have access to it.  I have no interest in it.  But I'm not inside that community.  And you have to imagine now that there are lots of eyeballs scrutinizing the code, looking to see whether, oh, look, you know, here's an unsafe type conversion where, if we're able to get a negative something in here, blah blah blah, who knows.  At the same time, this is not code that probably interacts with the outside world much.  It's not accepting random parameters and things.  The fact that it is as root as it is means that it is probably not subject to external manipulation.  On the other hand, it is knowledge which Apple did not want made available, and it did get out.



LEO:  Yeah, I mean, the interesting - there are a couple interesting conversations that are unknown.  You raised one, which is how much is this code modified?  Apple says it's old code.  But as you point out, why modify iBoot?



STEVE:  Right.



LEO:  I didn't look at the code, so I didn't know how complex it is or how hardware-dependent it is.  This came from iOS, what did they say, it was 2014, so it would have been iOS9, I think.



STEVE:  Yup.



LEO:  But the other one is - and obviously it's security through obscurity.  And of course you don't want to give it away.  But how much damage is done?  Jonathan Levin, the guy who verified the code, said yes, well, I've disassembled the iBoot, and it matches my disassembly.  So what would you get?  You would get symbol names.  You'd get a symbol table that you wouldn't have that with a disassembly.  That's about it; right? 



STEVE:  Yeah.  When you disassemble, you're inherently making lots of guesses.



LEO:  Okay.



STEVE:  And so you're doing a lot of reverse-engineering.  Sometimes stuff happens like with specific hardware registers, where bits are being poked and stuff.



LEO:  Right, don't know, yeah, right.



STEVE:  And so that's like an unknown.  But if you've got the names of those, the actual nomenclature that goes along with that, there's a lot more that you're getting.  So you can really - you can look at it both ways.  I don't think it's the end of the world by any means.  And frankly, I'm surprised this doesn't happen more often.  I mean...



LEO:  Well, it has happened once before, I think; right?



STEVE:  Right, right.  But still, I mean, the idea that a company the size of Apple with as much intellectual property as they have, that this is a major event because it's considered an unprecedented leak.  That alone is impressive, an impressive statement about Apple's security.  I think they're doing a good job overall.



LEO:  Yeah, yeah.  Why that intern had access to the source code is another question.



STEVE:  That's, yeah.  You can imagine there'll be some internal surveilling of their processes.



LEO:  But it may be an indicator that they didn't care that much, you know, like saying, yeah, well, fine, if it gets out, it gets out.



STEVE:  Yeah, it could be.  So last September we covered the news of a big concern in the Broadcom WiFi chipsets.  This affected Apple devices and Android devices, all of which use these Broadcom chipsets.  And our listeners will remember that you didn't have to be associated with an access point.  You didn't have to be connected to WiFi.  It just had to be a specially formed packet that your mobile device could receive that allowed it to be taken over.



So everyone ran around, and it was important to get patched, and we talked about again another example of the necessity of our devices over time being maintained somehow, that the lesson keeps being the nature of this is that these are computers, and they need to have some sort of an ongoing maintenance facility of some kind.



Well, it turns out that there are a subset of Lenovo's ThinkPads also using this Broadcom, it's the BCM4356 is the chip.  The wireless LAN driver for Windows 10 is also and similarly vulnerable.  The vulnerability is rated 10 out of 10 for exactly the reasons I just enumerated.  It's remote, requires no user interaction, and can potentially be used to perform remote code execution attacks.  But it does, as was the case with the previous mobile devices, require you to be within radio reception range proximity.  So it's not like the North Koreans are going to get you when you're in South Dakota.  You're safe.



But it does mean that, if this were to become prevalent, somebody within radio reception range, somebody in the coffee shop where you are, or on the train, or in the airport or wherever, could exploit this driver in a select set of laptops.  And those laptops are the ThinkPad 10, the L460, the P50s, the T60, T60p, T460, T460p, T460s, the T560, the X260, and the ThinkPad Yoga 260.  Those and only those devices have updated WiFi driver available, and owners of those should go to Lenovo, or most of our ThinkPads now have a Lenovo check to make sure you're updated and current and safe option from Lenovo.  So either go get it, or if you have a ThinkPad that I just named off there, they've got many, many more.  For example, our X1 Carbons are not among those.  But I don't know if your Yoga is a 260, Leo.



LEO:  I have a second-generation Yoga, so I'll be checking.  And I have a T470s.  Was that in there? 



STEVE:  No.  Not in there.



LEO:  Yeah, because I don't think anything when I run [crosstalk].



STEVE:  So they have so many models that this is just a small subset.  But it is worth making sure that you've got the latest wireless LAN driver, just to be on the safe side, because it's regarded as a 10 out of 10.  Again, more likely that you would be targeted for attack.  So if your profile is such that you're known to be carrying that laptop, one of those laptops, and you might be a target for attack, definitely want to make sure you're...



LEO:  Actually, when you run the Lenovo Vantage software, which is now what they use to do updating, it says right on the front, "Lenovo WiFi Security, protect yourself from malicious WiFi networks."  Oh, that's not related.  Or is it?  Oh, that's something that you can turn on.  That's something else, though, which I really like.



STEVE:  Ah, okay.



LEO:  That lets you kind of - I don't what they do.  It's not a VPN, but they have some sort of thing that they do.



STEVE:  Extra goodie.



LEO:  That's not - the extra goodie.  Yes, that's not [crosstalk].



STEVE:  Since WordPress is a sponsor, I wanted to let our listeners know that there was a small glitch last week in the release of WordPress v4.9.3 which last week was the latest.  They patched a total of 34 known vulnerabilities.  But unfortunately, in the process of installing the updates, the automatic update mechanism itself got broken.  Which meant that  WordPress would no longer be doing automatic updates.



So I'm not sure what mechanism they have for notifying their users.  They probably - you probably get email if you are a registered WordPress user who's receiving this.  But the problem is that since 4.9.3 broke the update mechanism, it's necessary for WordPress admins whose systems did grab 4.9.3, to go get 4.9.4 manually because the system is no longer able to update itself.



The lead developer Dion Hulse explained, he said that one of these changes aimed to reduce the number of API calls which get made when the auto-update cron task is run.  He said:  "Unfortunately, due to human error, the final commit didn't have the intended effect and instead triggers a fatal error as not all of the dependencies of" - and then there's a function,  find_core_auto_update() - "are met."  He said:  "For whatever reason, the fatal error was not discovered before 4.9.3's release.  It was a few hours after the release when discovered."



So again, I salute WordPress for making this well known, and within a couple of hours they fixed the problem.  But during that little window, those systems that updated to 4.9.3 won't be able to update to 4.9.4 and anything moving forward.  So I would just say to our listeners, check to make sure that you don't have 4.9.3.  If you do, you should now just go to - you can go to the dashboard under Updates and then click Update Now, and your system will then jump to 4.9.4.



LEO:  And as long as we're mentioning that they're a sponsor, just for those of you listening, if you are a customer of WordPress.com, who's our sponsor, they do all of that automatically.  That's been done.  You don't have to worry about it.  This is only for who are on self-hosted WordPress.



STEVE:  Right, right.  Good, good.  I got a note from a frequent tweeter and friend of the show, Simon Zerafa, who saw somebody else's tweet and sent it to me as "Murphy's Law as applied to software design and programming."  Someone named Bruce Dawson tweeted sort of the software programmer's spin:  "Any mistake that is not explicitly prevented will be made.  Any error that can occur, will," which of course is - everything we've learned on this podcast about the way security happens follows that.  And Leo, now is the time for the College Humor video about passwords.



LEO:  Now, I'm glad you mentioned this is safe for work; right? 



STEVE:  Yes, it is safe for work.



LEO:  Okay.



STEVE:  One hundred percent.  And it's just very clever.  I just love the idea that this is being done sort of as a public service.



LEO:  And by the way, I want to - it's on Facebook, and I want to mention that the College Humor had to fire people because they're relying on Facebook for their traffic, and they don't get a whole lot of traffic from Facebook.  Or they don't get to monetize it, I guess.  So here you go, on Facebook, College Humor:  How to keep your passwords safe.



[Begin video clip]



MALE VOICE:  You know what they say.  Mo' money, mo' passwords, to protect that money from hackers.  But there's more of them, too.  Mo' money, mo' potential problems.  But mo' passwords, no  hackers.  Yeah.  MC Safe Search.  Online I'm shopping and I'm feeling fine.  But I've got to use some passwords to keep what's mine.  Passwords are codes that protect your stuff.  You've got to keep them all a secret to say safe enough.



But then how do you remember is the question that I get.  I say, why not use the first name of your favorite pet?  Mine's my dog, Mr. Noodles.  It don't matter if you know because I was tricky and replaced some vowels with [indiscernible].  I mean, not, you know what, never mind.  That was a general example, not specifically mine.  Oh, man, give me two quick seconds.  I mean, I'm not worried, dog, because I'm password protected.



VOICES:  Keep your words, keep your passwords tight.  If you keep your words a secret it'll be all right.  If you want to be safe [crosstalk].



LEO:  That's hysterical.  If you're not seeing the video, it's great.



VOICES:  Gotta go with a phrase that no one knows, don't ever write it down, don't let it show.  Don't you ever let it slip in front of a hacker, got to be extra careful with your secret password.



MALE VOICE:  There we go.  My account's all safe and sound.  What do you think you're doing creeping, peeking around?  Did that video just show what I just typed in?  I just changed my login, I've got to do it again?  Now it's telling me I can't change again that quick.  When I was typing I really hope you did not film it.



Just got an alert that my account got hacked, but they don't have my social number, so I can just relax.  Why would you show this, man?  What a terrible choice.  I said they do not have my number.  Please, listen to my voice.  Oh, cool.  That was another message about my account from my bank saying I just bought a speed boat, so that's cool, thanks.  And a car, and a fur coat, and yet another boat?  These hackers are the worst.  They really get my goat.  Was trying to spread the word about keeping passwords safe, but this whole song has blown up in my face.  Come on.



LEO:  The piano players.



VOICES:  Keep your words, keep your passwords tight.  If you slip up you'll probably get hacked tonight.  Then someone will use your cash to buy a coat, or a boat or a car or another boat. You can't be too careful with your info, don't ever let it slip, don't let nobody know.  You never know who's watching when you show your [indiscernible], stealing someone's stuff is the only way.  [Indiscernible].



MALE VOICE:  Hey, Rodney, where'd you get that coat?  Did I just get my identity stolen by my backup singer?



MALE VOICE:  My man got hacked, ha ha.



MALE VOICE:  Dang, I've known you since the fourth grade, Bobby.  I even shared my pizza bagels with you.



[End clip]



LEO:  Aw.  That's good.  College Humor.  I love it.



STEVE:  So the word is getting around.



LEO:  Yeah, that's good.



STEVE:  How to make your passwords safe.  Now, okay.  This is bizarre, but I thought it was sort of fun.  The tweet came in, and those of us who have been watching or have watched some of the series that we talked about a couple weeks ago, "Altered Carbon," will realize that someone was having fun mixing SpinRite with some of the terminology from "Altered Carbon."



This person tweeted:  "Dear Steve, I have a question regarding SpinRite. I've come across an old stack of my great-great-great-great (possibly more) grandmother.  When trying to spin her up in VR, she loads for a very long time, but eventually fails. There is also no luck when putting her in a new sleeve.  What level would you recommend running SpinRite at to try and recover a failing stack?  Thanks for everything.  Mitch."



LEO:  Love it, love it, love it.



STEVE:  So, yeah, I got a kick out of that.



LEO:  Did you watch any more of "Altered Carbon" or just that first couple episodes?



STEVE:  I didn't.



LEO:  So you missed - did you see the episode where she spins her grandma up, her abuela up into this tattooed Nazi skinhead?



STEVE:  No.  And so now I understand.



LEO:  Oh, that is hysterical.



STEVE:  Now I understand what he's talking about, an old, old, old stack of his grandmother.



LEO:  Apparently ever year on Dia de los Muertos, she spins up Grandma into whatever - they call the body "sleeves" - lying around at the jail where she works.  And so she spins it up.  But the funny thing is this guy's got a role in the show.   Later in the show he turns back into the Nazi skinhead.  So it's very - it's actually quite amazing.



STEVE:  So you are moving through the series?



LEO:  Yeah, you know, I like it.



STEVE:  Yeah, good.



LEO:  Yeah, I enjoy it.  It's very adult, we should mention, very violent, but also sexy.  And I thought it was - and it's beautiful.  I thought it was beautiful.  Did you get your HDR TV yet?



STEVE:  No, I didn't do that yet.



LEO:  Check it out there and do it, yeah.



STEVE:  Okay.  I'll wait until I get amazing vivid color.



LEO:  That's it.  It's so much better.



STEVE:  So Tom Terrific asked:  "If I change my local computer's DNS to 9.9.9.9, and my router has a different DNS lookup address, which has precedence?"  Okay.  So it's a great question.  Normally the way all of our contemporary Internet-connected computers are designed, whether Windows or Mac or Linux, is they use DHCP, Dynamic Host Configuration Protocol, to obtain all of those IP-related things from the network.  So at startup a query is sent out because the computer has no idea what network it's on.  So it's a nonspecific broadcast saying, hey, is there a DHCP server here?



And then your DHCP server, typically your router, will speak up and say, yeah, we've got DHCP services.  What do you need?  And so then the client asks for IP address and DNS, and those things get sent.  And also sometimes time of day and other stuff.  So it's actually sort of a general purpose, DHCP can provide all kinds of information. But that's configurable.  I'm sure that certainly Windows users have seen the "Obtain IP address automatically," and the same is true for DNS.  You're able to obtain the DNS automatically from your DHCP server.  So the point is you are able to override that on your computer to say, no, I don't want it automatic.  I'm going to specify the one or two, typically two DNS servers myself.



So the answer is, if you deliberately override your computer's default setting, which is to obtain it automatically, then it will use what you tell it to, Tom.  It will not use whatever the router's DNS is.  It's offering it, but the computers essentially have to ask for it when they start up.  And if you tell them, nope, I want to provide my own DNS, then that's what gets used.



A question from Skynet asks:  "Does InSpectre" - GRC's little program for monitoring the Meltdown and Spectre vulnerabilities that we talked about a couple of weeks ago - "run in Boot Camp?"  He says:  "Can it see the chip?  Or should I run it in WINE on the Mac side?"  And the answer is, yes, Boot Camp is just an alternative boot for real Windows.  I was making the distinction with a VM.  Boot Camp is not a virtual machine environment.  Neither is WINE.  So either booting Windows on a Mac with Boot Camp or running it under WINE will allow it to see the chip, and you'll be able to determine whether the actual microcode has been updated in your processor and what vulnerabilities it may have.  But only not if you're actually in a VM.



And then we had one, two, three, four, five different people, with a little bit of overlap, suggesting various ways of combining Spectre and Meltdown.  David Lemire, he was suggesting Specdrown since we have an endless stream of news about them, Specdrown.  Kyle at @craigconsulting said he proposes Smectre, S-M-E-C-T-R-E, Smectre?



LEO:  No, no.  That's not - no, no.



STEVE:  Yeah, no, that didn't work.  We have Joe McDaniel and Kevin both had - how about Smelter?  So it's like, okay.  Smelter's not bad.  And then Marko of course said how about just combining Spectre and Meltdown as S&M.  And it's like, ah, okay.  That's not bad, S&M.  I was thinking maybe SAM, S-A-M, Spectre and Meltdown, so just referring to it as SAM.  But unfortunately those are just so different from each other that none of those has really grabbed us so far.  But thank you for your submissions.



I was reminded of that famous quote from bank robber Willie Sutton, who was asked about his chosen career.



LEO:  Willie, Willie, why do you rob banks, Willie?



STEVE:  Exactly.  And he said:  "I rob banks because that's where the money is."  And what we're seeing is so many weird things happening around cryptocurrency because after it happened, and as soon as it goes from being cyber to there being exchanges, which allow you to turn this synthetic made from thin air, or actually made from a lot of power these days, turn them into actual dollars that you can buy food with and use to keep a roof over your head and so forth, suddenly it becomes a thing.



So one thing that we saw in the last week was another worrisome way of getting cryptocurrency mining into unwitting browsers.  We've talked about how malicious ads are running mining software on people's machines.  It turns out that over the weekend approximately 4,300 websites, many high profile, such as, get this, CUNY.edu...



LEO:  Jeff Jarvis's site.



STEVE:  Yes.  USCourts.gov.  CookCountyTreasurer.gov.  All visitors to those sites were mining the Monero cryptocurrency.



LEO:  Uh-oh.



STEVE:  Uh-huh, on behalf of someone.  How did this happen?  Well, all of those sites support a plugin or a browser enhancement, actually it's not a plugin, it's JavaScript called BrowseAloud which is a web screen reading service.  And naturally many governments have a requirement for assistive technology in order to help, for example, visually impaired visitors so that there is a screen reading thing they have built in.  So those sites had on their pages the invocation of this BrowseAloud.com script, ba.js.



Well, what happened was the BrowseAloud server was compromised such that a document.write() was inserted into the BrowseAloud JavaScript, which wrote an invocation for the Coinhive Monero cryptocurrency mining script into the pages.  As a consequence, sort of through this second-level indirection, all of the sites, and there are more than 4,300 of them - in fact, Leo, in the show notes here under publicwww.com it's a search engine that allows you to find instances of scripts.  And it shows, it's how I saw that CUNY.edu was on the list.  It shows 4,300 sites which are known to be hosting this BrowseAloud ba.js script.



And so all of those sites were inadvertently invoking the Coinhive Monero cryptocurrency miner.  And as we know, and as I've said, it's not technically a massive security problem.  It pulls some processing power from your system.  People have objected to it being done behind their backs.  But what it really does is the fact that this could happen highlights a problem that we're sort of not paying enough attention to, and that is - and we've talked about it here before - that blindly importing the contents of third-party scripting libraries is inherently dangerous.  That is, those 4,300 sites intended to only be offering the BrowseAloud service.  Instead, for this period of time until the BrowseAloud folks were notified, and they got their servers cleaned up, they were the inadvertent hosts of cryptocurrency mining.



So of course our web browsers have dealt with the inherent danger representing by third-party scripting libraries by enforcing containment.  And we've talked about how the browsers strongly enforce the so-called same-origin policy, which creates script isolation to limit what a script can do and what it can touch.  But this happens to be sort of an interesting example of a script whose purpose is not to breach that containment, but it's quite happy to purr along running within its own container because its intent is to soak up as much of its hosting machine's spare CPU cycles as possible for annoyingly low yield cryptocurrency mining.



In other words, the thing that's sort of annoying is that JavaScript is just not a useful language for implementing cryptocurrency mining.  But they're hoping to operate on the principle of, well, you know, if it's spread far and wide enough, you end up with enough aggregating hashing power to have it amount to something.  And in fact, in some of the stories that we've been covering recently, we have seen - we talked about one last week where in that case they were actually using EternalBlue to infect Windows servers and mining on them.  Well, that's much more high-yield mining that using JavaScript.  But they were generating thousands of dollars for something.  Was it a day or a week?  I don't remember.  I think it was a week.  So it does add up.



In a somewhat different twist, another interesting bit is a website - and this is sort of what we were talking about probably going to happen in the future.  Salon.com is now formally offering visitors who are using adblockers the option of mining Monero while they're on Salon's website.  And Leo, I went there myself, and sure enough, because I'm using uBlock Origin, I got the blocker.  I got the notice.  If you just go to Salon.com it'll come right up.



LEO:  So they're doing it without telling you?



STEVE:  Well, unfortunately, yes.  You get a blocker.  But if you click on "Tell me more," then while they're telling you - yup, so there you're seeing the page that I saw this morning.  If you click on "Tell me more," then they try to be mining cryptocurrency while you're reading that message.



LEO:  Oh, yeah.  It says "Block ads by allowing Salon to use your unused computing power."



STEVE:  Yeah.



LEO:  Which of course means - and then they have a fairly long thing.



STEVE:  They do.  



LEO:  I don't know if they talk really very much about the Monero part of it.



STEVE:  Yeah, they don't go into any detail.  And it's funny, too, they spin it as, you know, the advertising-based model is beginning to collapse, and we're excited about the block chain, and we want to be participating in the future and blah blah blah.  So it's sort of a way of saying you can either bring your adblocker down, or we're going to run cryptocurrency mining in the background.  Now, it didn't work for me.  I thought, oh, cool, because I wanted to verify...



LEO:  It's darkened my screen.  I don't know if that's uBlock Origin doing it or...



STEVE:  No, that wouldn't be uBlock Origin.  That's probably them saying "We're still not happy with you."



LEO:  Yeah.



STEVE:  Because what happened was the reporting said that they start mining the moment you say "Tell me more," rather than giving them opt-in permission.  So I thought, okay, I want to verify that by seeing my CPU get pinned.  Unfortunately, uBlock Origin also blocks Coinhive, and they're just using Coinhive.  So one of the things, I mean, and as I have said here on the podcast, I don't think this is such a bad idea.  That is, I mean, users need to be informed.  But, for example, I am constantly annoyed by - is it The New York Times?  I think it's The New York Times.  It has a very strong paywall.  Sometimes I'm interested in stories that are there, but I don't want to subscribe.  But on the other hand, I wouldn't mind if they pinned my CPU for the duration of time that I'm looking at a story in The New York Times.  If they want to do that with permission, saying, okay, you're not a subscriber, can we use your processor while you're here, I'd say, yeah, fine.



And so, I mean, it's an interesting sort of micropayment scheme.  It's annoying that JavaScript is so inefficient because it means that the economics won't really work until we come up with browser-integrated cryptocurrency mining.  But I think it's a valid model.  I don't want ads that are annoying.  But if I could pay by using a little bit of processing power while I'm somewhere, and if it's efficient - because JavaScript isn't.  But if our web browsers had true GPU-accelerated mining built in, the economic model might work.  And I think it's kind of an interesting model.  Anyway, Salon is giving it a try.  And so it'll be interesting to see how that works for them.



And again, sort of following the notion that the lure of actual exchangeable-for-real-currency cryptocurrency creates pressure.  We've talked about how security is probably impossible to make perfect.  Certainly Apple security surrounding their source code is going to be incredibly good.  Yet despite that, where there was pressure from this intern's friends who kept bugging him or her to please, please, please, please, please give us the iBoot source, we promise, we promise, we promise we'll never let it go, we'll never blah blah blah, finally the person was convinced to do so.  So it was pressure which created the problem.



Well, cryptocurrency creates pressure.  And so we now have the first known instance of a water purification plant's network, that is, its SCADA - S-C-A-D-A is the acronym for Supervisory Control And Data Acquisition.  It was the SCADA network running the centrifuges in Iran which was breached and allowed them to get messed up.  So these SCADA networks are notoriously insecure.  They're the kinds of things which you really don't want to have on the Internet.  You need them on the Intranet because their nature is to allow you to do process control monitoring and so forth, much as you would have in a big water purification plant.



Unfortunately, it's convenient for them to be on the Internet.  So of course that's where they end up being.  And in this case a company, a cybersecurity company who specializes in this sort of critical infrastructure, a company named Radiflow, announced that they had found for them, and first time it's been in the news, an instance of cryptocurrency mining - again, Monero cryptocurrency - being done on the servers within the network of this company as a consequence of their SCADA network being compromised.  So yes, cryptocurrency also being mined in the water purification plants.



And, finally, and this one pretty much - I wanted to wrap up with this one because it's one for the ages.  Russian nuclear scientists have been arrested for logging into one of Russia's most secure supercomputers to mine bitcoin.  Several scientists working at a nuclear weapons facility in Russia were arrested on a suspicion that they used one of the country's, one of Russia's most powerful supercomputers to which they had access for nuclear science research to mine bitcoins.



This apparently took place at the Federal Nuclear Center in Sarov, which is a top-secret area surrounded in Russia by barbed-wire fences where the Soviet Union's first nuclear bomb was produced during the Cold War.  According to reporting by the BBC and the Interfax news agency, they wrote:  "There has been an unsanctioned attempt to use computer facilities for private purposes including so-called mining.  As far as we're aware, a criminal case has been launched against the believed perpetrators."



So, yes, once again, as Willie Sutton said:  "I rob banks because that's where the money is."  If there's a supercomputer that is not being used completely, well, why not run a little bitcoin mining operation on it and generate some bitcoins while you're at it.  Except that that was unsanctioned.



LEO:  Did you see that China is now shutting down its bitcoin miners?  So they're all moving to Iceland, where because of geothermal and hydroelectric power, it's very cheap.  So Iceland may become the new miner central.



STEVE:  Yeah.  And in fact there was another story last week.  I guess the state of Washington, some county or region in the state of Washington is being turned upside down because their power is $0.03 to $0.04 per something, megawatt or kilowatt or I don't remember what the units were.  But it was, like, way inexpensive.  And so major mining operations are coming in and asking for 200 megawatt connections.



The problem is that requires them to build out a much larger infrastructure, which they would like to do.  I mean, they would like to satisfy demand.  But if mining then collapses, and all of this business dries up, they're stuck with infrastructure that had a 30, 40-year expected amortization life, and no one who needs it and is willing to continue paying for it.  So it's causing all kinds of problems.  It's like, oh, well, cryptocurrency mining.



LEO:  Oh, boy.  Apparently a bunch of warehouses on the waterfront, the requested amount of power I think was 100, maybe it was 1,000 megawatts, exceeds the entire usage of all of Iceland.



STEVE:  Wow.



LEO:  I think Iceland uses 340 megawatt hours, and they're asking for a thousand.



STEVE:  Yeah, you've got to be careful about melting ice up there, too, because that generates a lot of heat.



LEO:  Wow.  That's a lot of heat.  That's amazing.  Steve, as always, so much fun.  So interesting and informative.  Always a pleasure.  I know we were talking before the show, you've got three more things to do on SQRL.



STEVE:  SQRL is - I plan to have an announcement next week.  We're very close.  I thought I had two things left to do, and someone said, hey, I thought you were going to deal with high DPI awareness?  It was like, oh, that's right, I forgot about that because with screens getting higher resolution, my UI normally just is like the standard useful size for standard resolution monitors.  But...



LEO:  It would be this size on a DPI [indiscernible].



STEVE:  It is very small.



LEO:  A postage stamp.



STEVE:  Yeah.  But remember that from the very beginning I built SQRL to be language independent.  Every single - there's no text, no English in the app at all.  It all exists in an external file which - and remember there was a crowd source, no, it was not, it was crowd - I can't remember what it's called.  I think if you go to sqrl.grc.com, I think that was an alias to the service that provides translation.  We were going to crowd source various translations of it.



Anyway, the point is I built that in from the beginning.  And because different languages have different requirements for linear length of their text, I built in a scalable UI facility from the start.  So in theory I should be able, if I see that somebody's got their fonts scaled to 150%, which is the way Windows handles high DPI screens, I ought to be able to just say, oh, and drop that single number in at one point in the code, and it will scale the entire UI.  I've never done that before, but it's built in.  So even if it doesn't work at first, it ought to work pretty quickly.



But anyway, we're down to very few things remaining to be done.  I will then spend a little bit of time on the demo website.  I do need to bring up web forums because all of the people who want me to get back to work on SpinRite don't want me to be spending all of my time redundantly answering questions about SQRL.  So my plan is to bring up some web forums where all of the people who have been working with me over in GRC's kind of semi-private, old-school, text-only NNTP newsgroups, they'll be able to - and are interested in helping people to understand what SQRL is, we've got Android developers, native Mac developers.  It's running for iOS.  Mine runs under Windows and WINE so it can run under Mac and Linux.



And so things are happening.  And so of course my intention is to get it launched.  And then I will turn my attention to GRC's web pages to bring its description up to speed because I haven't looked at it for years, and I need to make that current.  And then back to SpinRite 6, happily.



LEO:  Good.



STEVE:  And not a minute too soon, and happily so.



LEO:  Yay.



STEVE:  And I'm going to come up, and we will do a SQRL announce and presentation in the TWiT studios.



LEO:  Nice.  I look forward to that.



STEVE:  Thanks to your willingness to host.  That'll be great.



LEO:  Yeah, that'll be great.  Wow.  Well, if you listen to the show, and you enjoy it, there's a couple of things I should tell you about.  First of all, you can always watch it live or listen to it live or join us live in studio, which is always nice.  We have our friend from Des Moines visiting.  It's so nice to have - let me get your name right here.  Adam, visiting from Des Moines.  All you have to do is email us and let us know you're coming:  tickets@twit.tv.



You can also listen and watch live on the stream.  We have audio and video streams at TWiT.tv/live.  If you do that, by all means, check out the chatroom:  irc.twit.tv.  People really have a good time during the show in there trying to parse everything we talk about.  It's kind of fun.  You can also get on-demand versions.  Steve has audio at GRC.com.  He also has transcripts.  It's the only place you can get nicely written transcripts.  Steve commissions them from Elaine Farris.  It's very nice.  So you can search.  You can use that to search.  You can also use it to just read along as you listen:  GRC.com.



While you're there, check out SpinRite.  This is Steve's only source of income.  It's his bread and butter.  Make some yabba dabba doos happen in the studio during the show.  Get a copy of the world's best hard drive maintenance and recovery utility.  You can even work it on your sleeve, you know, get your - what do they call that token that's got your personality in it, get that all...



STEVE:  Bring Granny back.  It's called a "stack."



LEO:  Get your stack all - bring Granny back.  And your new slogan...



STEVE:  Get your stack back.



LEO:  Get your stack back.



STEVE:  Get your stack back, yup.



LEO:  GRC.com.  If you want to see video, or you have other interests, you want to see more, you can always go to our website, TWiT.tv.  This show is at TWiT.tv/sn.  We also have links to all of the different podcatchers, so you can subscribe and make sure you get every episode.  Audio or video, you want to keep them all for future reference.  Next week, Steve.  Thanks for being here.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



LEO:  Bye-bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#651

DATE:		February 20, 2018

TITLE:		Russian Meddling Technology

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-651.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we examine and discuss the appearance of new forms of Meltdown and Spectre attacks, the legal response against Intel, the adoption of new cybersecurity responsibility in New York, some more on Salon and authorized crypto mining, more on software cheating auto emissions, a newly revealed instance of highly profitable mal-mining, checking in on Let's Encrypt's steady growth, the first crack of Windows uncrackable UWP system, Apple's wacky Telugu Unicode attacks, a frightening EternalBlue experiment, another aspect of crypto mining annoyance, a note now that Chrome's new advertising controls are in place, and a bit of closing-the-loop with our listeners.



Then we conclude with a look into the technology that was revealed in last week's indictment of election-meddling Russians and, from a practical technology standpoint, the feasibility of anything changing.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got more news about cybersecurity, of course, including, yes, more Spectre and Meltdown news.  Oh, man.  Plus how the Russians hacked Twitter and Facebook, and a bitcoin miner that nearly brought T-Mobile to its knees.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 651, recorded Tuesday, February 20th, 2018:  Russian Meddling Technology.



It's time for Security Now!, the show where we cover your security online with this guy right here, Mr. Steven Gibson from  GRC.com.  Live long and prosper, Steve.  Hello.



STEVE GIBSON:  Yo, Leo.  Great to be with you again, as always.  So we plow in, well, past Episode 650.  I don't know why that number seems so significant to me, but we're on 651 today.  And a piece of information became available last Friday that I thought was interesting.  And also Techdirt's response, Mike over at Techdirt, sort of a little bit of a reality check as a consequence.  What I'm talking about is what we learned from the Mueller investigation about the technology that Russia allegedly used for messing with social media.  And I don't care about the politics.  What was interesting to me was what they did in order to pull this off.



And this is relevant because right now, sort of in the wake of that, there's all this comeuppance, all this furor about the responsibility of Google and Twitter and Facebook and Instagram to have dealt with this or to have known better, and what can we do coming up.  And of course this put me in mind of the FBI's arguing that encryption was bad, and that we needed a golden key.  It's like, well, there are things that technology can do, and there are things that technology can't.



So now that we know a little bit more about how this was done, we have something to talk about from a technology standpoint which I think is sort of interesting about just the nature of the Internet and how little control over it we really have.  But lots of news this week, whereas last week, actually the last couple weeks have been kind of quiet.  So I want to examine and discuss the appearance of, unfortunately, new forms of Meltdown and Spectre attacks.



LEO:  Oh, boy.



STEVE:  Uh-huh.  There's just some beautiful research out of Princeton that revealed something that I guess we should have suspected or expected.  Also the legal response against Intel I want to touch on briefly.  They had to file their annual 10-K report, so we know we've got a little glimpse into what would otherwise be proprietary about what the Meltdown and Spectre vulnerabilities have done in terms of backlash, which I think is unfortunate because I don't think this is Intel's fault, any more than it's - it was the nature of the technology.



Also there's some new cybersecurity legislation which is going into effect, actually last week and also in coming weeks in New York, which I want to cover, and we'll talk about it.  A little bit more on Salon and authorized crypto mining.  We have a little update on software that cheats auto emissions.  Oh, and also maybe a new coined term and a newly revealed instance of highly profitable, what I would call "mal-mining."



LEO:  There's a word I don't want to hear again.



STEVE:  Mal-mining, yes.



LEO:  Mal-mining.  



STEVE:  I want to check in on Let's Encrypt's steady growth, which as you noted is the graph that I chose for the Picture of the Week.  It just keeps on trucking.  We also have the first crack reported of Windows', now in quotes, "uncrackable" UWP system.  I wanted to touch briefly on the iOS and across-the-board updates we got and the crazy Unicode attacks that beset Apple.  A frightening EternalBlue experiment which was contracted for by a company wanting to understand the nature of their vulnerability.  And everybody was a little surprised by what it revealed.  Also we have another new aspect of crypto mining annoyance.



I wanted to make a note now that Chrome's new advertising controls are in place that we've talked about previously, that I know you talked about over the weekend.  A bit of closing the loop with our listeners.  And then, as I said, I want to look at some of the technological cleverness, unfortunately, that apparently Russia got up to in 2016 and whether there's actually anything that we can do about it.  So I think a chock-full podcast.



LEO:  Nice.  Very excited.



STEVE:  From our "attacks never get worse, they only ever get better" department, it was inevitable that there would be additional forward motion on the whole Meltdown and Spectre side.  I loved the proud Princeton professor's tweet which started this.  She tweeted:  "My PhD student @carolinetrippel developed research tools/techniques to synthesize security exploits.  Yes, her tools find #Spectre and #Meltdown.  But they've also discovered two new related-but-distinct vulnerabilities using cache coherence protocols."  So that's from the professor of the PhD student, or candidate, I guess, who generated a beautiful paper.



The research paper, it was done along with Nvidia, is titled "MeltdownPrime and SpectrePrime:  Automatically Synthesized Attacks Exploiting Invalidation-Based Coherence Protocols."  Which is sort of the academic formal way of talking about the idea that you have some situation where caching is valid or invalid, and you need to maintain so-called "cache coherence," that is, you need to make sure that what's in the cache is coherent with what's stored in main memory so that the cache is a faster access scratchpad, but you need to maintain coherence.  And that's an issue when you've got multiple things accessing a shared cache.  You need to make sure that they both have the same view of the data at all times.



Anyway, so these researchers developed a tool using mathematical rigor to formalize essentially the underlying architectural characteristics which enabled the creation of the Meltdown and Spectre side-channel attacks.  So they sort of stepped back and generalized the whole problem, saying that, okay, wait.  So here's a couple, you know, Spectre and Meltdown are specific instances of an attack.  But is there a broader problem here?  And what they answered, you know, they came up with, oops, yes.



And one has to think that, if not before now, then certainly since the summer when this was first revealed to them, revealed to Intel, sorry, revealed to Intel, that Intel would now have similar tools at their disposal, I mean, one would hope; although it's clear that they didn't previously, or they didn't think that this could be leveraged into an attack, you know, it's sort of unclear how much of this came as a surprise because, as we talked about when we originally talked about the Meltdown and Spectre problems, there was some awareness of this decades ago, that this was a potential problem, that is, that there was a way to leverage speculation and caching side channels.



So anyway, this new tool that they built allowed the researchers to apply mathematical rigor to develop new software-based attacks from the description of the CPUs microarchitecture, and then this tool essentially writes the code which is then able to perpetrate the attack.  It is specific to specific microarchitectures.  On the other hand, what we've seen is that, across a large set of microarchitectures, Intel has pretty much done the same thing, which is why they're in trouble across so many years of their successive chips.



So anyway, in their abstract, to give you sort of a sense for the rigor of this, the abstract, just the beginning of it reads:  "The recent Meltdown and Spectre attacks highlight the importance of automated verification techniques for identifying hardware security vulnerabilities.  We," they write, "have developed a tool for automatically synthesizing microarchitecture-specific programs capable of producing any user-specified hardware execution pattern of interest.  Our tool takes two inputs:  a formal description of a microarchitecture in a domain-specific language" - meaning that they invented their own language to describe hardware microarchitectures - "and a formal description of a microarchitectural execution pattern of interest, for example, a threat pattern."  Then they write:  "All programs synthesized by our tool are capable of producing the specified execution pattern on the supplied microarchitecture."



So they succeeded.  And when they applied this tool to the Intel microarchitectures, they found previously unsuspected additional problems, which is the real takeaway from this.  Thus they call their paper "MeltdownPrime and SpectrePrime."  Now, in some of the press's coverage of this, there was the point raised or the question raised whether this meant that Intel had to go back to the drawing boards yet again with firmware updates.  And it's not clear one way or the other whether that's going to be the case, whether the existing firmware fixes will fix this.



But what we do have as a consequence of this very rigorous academic research of architectures is first a nice step forward in the science of the security of our hardware platforms, which as I said would have been nice if we had it quite some while ago, but at least we have it now and will moving forward, which may be the way that we're able to get the best of both worlds.  We are not wanting to sacrifice undue performance that we've been taking for granted and incorporating into everything we do for the sake of security.  But neither do we want architectures that are exploitable.



So I imagine there's a lot of head-scratching going on at Intel and in other chip manufacturers, recognizing that this is, as we said at the beginning of the year and spent weeks of this podcast talking about, this was not a small event in the history of computer science and this industry.  This was a big deal, and the repercussions are still being felt.  And this kind of fundamental research is just - is great to see.  So props to these guys for pulling that together.



And in a kind of a coincidence, because of the timing of Intel's annual report filing, there's a - it's known as the 10-K form that has to be filed with the U.S. Securities & Exchange Commission, our U.S. SEC, which publicly traded companies have to file, where they formally lay out like where they stand, what the known risks to purchases of shares of their corporation are and so forth.  It's a long document.  It's a 201-page PDF.  But coincidentally, page 124 of that PDF has to and did address the consequences of this Spectre and Meltdown on Intel.  We talked about it at the time that the Intel stock suffered as a consequence when people like, oh, my god, this is a big deal.



So as a consequence of this report filing, we learned a few things.  They said - and I'll give a little bit of intro here just to set the stage.  In June, and so this is Intel now in their formal 10-K filing:  "In June 2017, a Google research team notified us and other companies that it had identified security vulnerabilities - now commonly referred to as Spectre and Meltdown - that affect many types of microprocessors, including our products.  As is standard when findings like these are presented, we worked together with other companies in the industry to verify the research and develop and validate software and firmware updates for impacted technologies.



"On January 3, 2018, information on the security vulnerabilities was publicly reported, before software and firmware updates to address the vulnerabilities were made widely available.  Numerous lawsuits," they wrote, "have been filed against Intel and, in certain cases, our executives and directors, in U.S. federal and state courts and in certain courts in other countries relating to the Spectre and Meltdown security vulnerabilities."  So this they have to disclose per the requirements of the 10-K filing with the SEC.



They wrote:  "As of February 15, 2018, 30 customer class action lawsuits and two securities class action lawsuits have been filed.  The customer class action plaintiffs, who purport to represent various classes of end users of our products, generally claim to have been harmed by Intel's actions and/or omissions in connection with the security vulnerabilities and assert a variety of common law and statutory claims seeking monetary damages and equitable relief.  The securities class action plaintiffs, who purport to represent classes of acquirers of Intel stock between July 27, 2017 and January 4, 2018" - so that period after which Intel knew about this and the world found out about it - "generally allege that Intel and certain officers violated securities laws by making statements about Intel's products and internal controls that were revealed to be false or misleading by the disclosure of the security vulnerabilities.  Additional lawsuits and claims may be asserted on behalf of customers and shareholders seeking monetary damages or other related relief.



"We dispute the claims described above and intend to defend the lawsuits vigorously.  Given the procedural posture and nature of these cases, including that the proceedings are in the early stages, that alleged damages have not been specified, that uncertainty exists as to the likelihood of a class or classes being certified or the ultimate size of any class or classes if certified, and that there are significant factual and legal issues to be resolved, we are unable," Intel concludes, "to make a reasonable estimate of the potential loss or range of losses, if any, that might arise from these matters."



So I regard this as unfortunate.  That is, I'm sure this is business as usual.  I'm sure that for a company Intel's size there are always people who are upset.  They buy stock.  It goes down.  They sue people and try to find some relief.  We know that class actions themselves can often be kind of sketchy, where the individual plaintiffs haven't suffered damage, but they're aggrieved or feel aggrieved.  Who knows how this is going to come out?  I'm just sort of sad that something which from a technology standpoint you could argue everybody was doing, and we were all happy, we were all benefiting from the fact that we were using a chip architecture which could be cleverly leveraged to create some information leakage, but in return we were getting performance that we're now being asked to give up to varying degrees.



And as I'm keeping an eye on this, we're still as an industry learning more about what this means.  In some cases - and I'm kind of watching Intel and AMD and ARM.  And in some cases the documentation is being changed in order to make some of the instructions more clear that can be used for flushing, speculation, and synchronizing multiple cores.  In some cases the behavior of the instructions is being subtly changed in order to create more rigor where some was lacking.  But I'm sure Intel is adequately able to defend themselves and will be able to bring out experts that say, yeah, this was unfortunate, but this wasn't negligent.  It's hard to see that that was.



And also it's certainly the case that delay from Intel's learning about this to disclosing was also reasonable because they had a lot of work to do, as I've said in the last few weeks, in order to remediate the consequences of what Google found and reported.  And as we also know, they weren't quite done by the time this all went public, and everybody kind of tripped over themselves when that finally happened.  So anyway, this is a huge, huge event for our industry, and we're still seeing repercussions from it.



Also last week, later in the week, New York State's new cybersecurity laws, which were actually put on the books in March of 2017, but the nature of them were that at some given date certain the following things had to happen by.  That happened, the beginning of that happened last week.  So what has happened is there's essentially an effort to create more accountability about the cybersecurity efforts that major corporations, largely banks, but also insurers and financial service organizations, have put in place in order to protect their customers.  So it affects over 3,000 banks.



And essentially the legislation requires that senior executives at the Chairman of the Board of Directors or a senior officer like a CEO personally certify that their computer networks are protected by a cybersecurity program appropriate to their organization's risk profile.  This is from the New York Department of Financial Services, which are attempting to create some accountability and oversight so that, I mean, and we know that making these sorts of statements doesn't prove anything.  It's different than audits from outside firms and active enforcement.  But it feels like it's a start.



As of last week, these more than 3,000 firms had to have a very top-level executive formally attest to the following six points:  that their firm had adopted a cybersecurity program appropriate to their company's, often a bank's, risk profile; that they had adopted cybersecurity policies designed to protect the bank's information systems and the customer data they hold; appointed a chief information security officer responsible for overseeing and implementing the bank's cybersecurity program and enforcing its cybersecurity policy, so somebody to deal with those first two points; engaged, qualified cybersecurity personnel, either staff or contractors, to work with that company's CISO in managing the company's risk; developed an incident response plan; and taken steps to control privilege access to its IT network.



So those all seem like reasonable things to ask for a large organization which is doing all these things.  But what we have found in past years when specific events have happened, and we've covered it on the podcast, it's like, wait, you don't have a cybersecurity plan?  You don't have any staff who's doing this?  I mean, so what we've found is that things have just been negligently lacking.  So at least this is intended to focus the attention of corporate management on the need for these things.  And they had plenty of time to pull this together.  So this isn't external audits on an ongoing basis, but it's a start.



Then two weeks from now on the beginning of March, on March 1st, is the second implementation deadline for this same legislation, which will then be a year old, and five more much more substantial regulations and provisions come into force.  They have to have implemented either continuous monitoring or periodic penetration testing and vulnerability assessment for their network, so active participation in this; conducted a full-scale risk assessment of their information systems which will be used to inform their cybersecurity program; implement multifactor authentication for remote access, and perhaps more widely used as indicated by what the risk assessment showed; provide regular cybersecurity awareness training for their staff; and begin annual reporting from the CISO to the board of directors.



So again, things that you would hope any large, especially a large banking organization, where this stuff is critical, would already have well in place.  But what we found is no one was really looking until we found instance after instance where this wasn't done.  So New York State has said, okay, we're going to make the management of these major corporations attest to the fact that these things have been done.  So a step forward, if nothing else.



We talked last week about Salon hoping to monetize opt-in cryptocurrency mining.  And I read, as a consequence of the news of that, a number of interviews of Jordan Hoffner, who is the CEO of the Salon Media Group.  And there wasn't a lot of information there, although I did get some that we'll talk about in a second about this notion of authorized mining.  The most curious, to me, and disingenuous thing that I saw from these interviews is this question about CPU usage percentage because in being interviewed the interviewers were saying, you know, in one case I remember one interview said, well, you know, my CPU went to 90% of usage while it was on the FAQ page after I gave it permission to mine.



Oh, and by the way, I meant to say that I reported last week that I had read reporting that stated that the mining started when you acknowledged that first interstitial blocking page, but before you gave explicit permission.  It turns out that was true, but then stopped being true.  So they changed the behavior of their code, which may be why, Leo, when you tried it, it didn't just take off and pin your CPU.  So they cleaned that up a little bit, and that was good.



But the problem is people are saying, whoa, 90% CPU usage.  And then this Jordan, and I don't know how technical he is, but he was saying, "Oh, yes, that's one of the things that we're looking into is there seems to be a great disparity in how much CPU is used.  Sometimes it's 5%, sometimes 3, sometimes 90."  And I'm thinking, okay, well, first of all, it's probably the case that you have something like uBlock Origin in place.  It happens that uBlock Origin blocks any access to Coinhive from where those scripts are generated.  So it's going to prevent essentially any CPU usage of mining.



But as I mentioned before, my real concern, I mean, I'm sort of bullish about this concept, that is, I can sort of see there's something here that I imagine is going to - we're going to see some traction on this.  I think it's going to take hold.  But JavaScript is the wrong way to do this.  It's wrong because it is atrociously low yield.  So as a consequence it doesn't work for anybody.  It means that for value to be generated, a crazy amount of CPU resource, like yes, 90, 95, 100%, needs to be taken.  But as a consequence of the inefficiency of JavaScript mining, which is hugely, as we know, it's crypto-intensive, the amount of dollar value or cryptocurrency generated for this in return for massive CPU use is minuscule.  So it's going to be incredibly wasteful as system resources.



And so what we need for this to succeed is for mining to move into the browser, like to be a native feature of web browsers, if this is what we want to have happen, where for example it's able to access the system's GPU and perform reasonable yield mining with all of the proper controls and metrics and so forth.  So the concern is, my concern is that it feels to me like there's a nugget of something here.  Also, users are going to have to understand that, yes, if they don't want ads, they're going to have to provide value, and the value will be shown as their system generating more heat, their CPU suddenly jumping to just shy of 100.  You know, you still want the system to run, but essentially mining will take up all of the other available resources.  I mean, that's the tradeoff.



So anyway, it's just been sort of interesting, I mean, there's a lot of interest in this.  And I think it makes sense.  The problem is I hope this doesn't stub its toe because we're sort of premature in having a means for it being cost effective, if it turns out it is.  There's just no way that doing this with JavaScript is cost effective.  It immediately needs to get moved natively into the browser.  Or, I mean, as I was imagining this, you could imagine installing a browser-mediated agent on the user's computer that would work, too.  But it just makes more sense if it's native to the browser.  The browser just, you know, which is an app running on the system, just has the ability, with permission, to suck up as much CPU as you choose to give it.  At this point, it's doing that, but horribly insecure.



Now, what's really interesting is that I've also read a lot about these Coinhive guys.  And I've seen my comment on this podcast retweeted a number of times, where I said several months ago when this was beginning that it was impossible to not at least partially hold the Coinhive people responsible, that they had to be complicit in this because, in the early instances, all of the instances of the use of their script seemed malicious, I mean, seemed to be, you know, it was all, to use my new term, mal-mining.  So they've spun off a subsidiary called AuthedMine.com, A-U-T-H-E-D-M-I-N-E dot com.  And it's authorized mining by Coinhive.



And so if you go to AuthedMine.com - and, by the way, I was told that's what Salon is now using - you get the following:  "A Note to Adblock and Antivirus Vendors.  There is no need" - so this is an open letter.  "There is no need to block AuthedMine.com."  Now, you can imagine, they're asking people not to because Coinhive has gotten immediately blacklisted across the industry. "There's no need to block," they write, "AuthedMine.com or any scripts hosted on this domain.  AuthedMine" - god, that's hard to say - AuthedMine.com offers a Monero miner that can be embedded into other websites.  This miner will only ever run after an explicit opt-in from the user.  The miner never starts without this opt-in."



Now, I was initially skeptical, but I kept reading, and I got convinced.  They said:  "We implemented a secure token to enforce this opt-in on our servers.  It is not circumventable by any means, and we pledge that it will stay this way.  The opt-in token is only valid for the current browser session (at max 24 hours) and the current domain.  The user will need to opt-in again in the next session or on a different domain.  The opt-in notice is hosted on our servers and cannot be changed by website owners.  There's no way a website owner can start mining without the user knowing."  And then they say, "Click here to see how the Opt-In looks."  They say, "A detailed and technical explanation of the Opt-In can be found in our documentation.  We believe, they conclude, that browser-based mining can be a viable alternative for intrusive and annoying ads if used honestly and with consent by the user.  We kindly ask adblock and antivirus vendors to support us.  Please help us build a better web.  Cheers."  Signed, Coinhive.



So this is a, from all appearances, a well-designed, properly structured and architected solution that's still script-based, and it'll be interesting to find out how much money, how much value can be mined from JavaScript.  Certainly, it would be great if it's enough to be viable now.  Then we have a solution that could potentially work today.  If not, then we know that we can increase the leverage.



So then over on Coinhive they said of their AuthedMine branch, "A Non-Adblocked Miner."  They wrote:  "Shortly after the launch of Coinhive, several adblockers have begun blocking our miner. This is unfortunate because we intended Coinhive to be an alternative to ads, precisely for users with adblockers."  Eh, okay.  Maybe there's a little bit of history being rewritten there.  Of course we have no idea what they intended.



"However, we have to acknowledge that the decision to block Coinhive was understandable as it was possible to run the miner on a webpage without asking the visitor for consent or even informing them.  Even some antiviruses now consider our JavaScript miner as a threat, which makes it difficult for website owners to use Coinhive at all.  We implemented AuthedMine as a solution to these problems.



"The JavaScript Miner, Simple UI, and CAPTCHA" - so they have three services, JavaScript Miner, Simple UI, and CAPTCHA - "when loaded from AuthedMine.com will never start without asking for consent from the user or" - and they say from for the Simple UI and CAPTCHA - "letting them explicitly start mining through a click.  We realize this opt-in may be clunky and not fit all too well with your use case, but we strongly believe that being honest with the user will ultimately be beneficial for users and website owners alike.  Neither the JavaScript files on AuthedMine.com nor the domain names are currently blocked by any adblockers or antivirus.  We will talk to adblock and antivirus vendors so it will hopefully stay this way."



So I think this is clever and entirely appropriate.  And it does, like right now, give websites that want to experiment with this a useful, valid, non-blocked, I mean, truly opt-in, authenticated way of using browser-side mining in order to generate revenue for themselves.  So I think this is very interesting.  I think Coinhive did the right thing.  It's clear that the Coinhive domain got immediately blacklisted.  They're being very open.  They've got a technology that makes sense.



I understood what they meant when they said we understand it may interfere with the flow of a website.  You can imagine that a website would just like to get - it would itself like to get permission from the user, and then have the mining proceed, rather than require another opt-in from this AuthedMine.com that the website doesn't control.  But there's no way to do that without opening yourself, as Coinhive did, to malicious use; to, well, we'll call it mal-mining.  So anyway, it'll be interesting to see how this proceeds and whether there is uptake in adoption of this.  In return for people's CPUs being pinned, revenue gets generated.  So I think this is exciting.  We'll see what happens.



LEO:  On we go.  Onward.



STEVE:  Yeah.  So following up on my grumbling about the inefficiency of using JavaScript for mining, this is a little out of order in my show notes.  I'm just telling Elaine that since she uses the show notes to do the transcribing.  But of course we were talking last week about BrowseAloud, which was the site whose servers were breached, and the mal-mining Monero Coinhive was injected into, remember, 4,275 websites.  Many of them were governmental.  CUNY was among them, as was USCourts.gov and a bunch of others.



Well, we don't know who the creep was that did this, but as a consequence of the way crypto mining works, we know what success or not that particular crypto mining address had over the course of that campaign.  And in something of a funny quip, I saw someone comment that it was noted that browser-based crypto mining relies upon placing the mining code on sites people actually visit.  So again, a lot of sites got infected.  Many of them may have been low traffic.  But one way or the other, JavaScript-based mining, and this whole campaign infecting 4,275 websites, generated, minted a grand total of $24.



LEO:  Oh.  Oh.



STEVE:  So, yeah.  I don't think anybody won from that proposition.  Either those were not great sites to inject the mining code onto; or, as I said, it's just not generating a lot of money.  And if it's going to be viable, we need to fix that.



So I did want to touch on this question that we first addressed when VW got caught playing games with the emissions control software on I believe it was the diesel VWs.  That caused sort of a spotlight to be cast upon other potential violators.  And it was reported in a German newspaper just this last Sunday that U.S. investigators, in looking at Mercedes, have found that its cars appear to have been playing similar games.



This German newspaper wrote that the documents which they - and these were private documents that the newspaper got a hold of - showed that U.S. investigators had found several software functions that helped the Daimler cars pass emissions tests, including one which switched off emissions cleaning after 26 kilometers of driving.  They found another function in the software which allowed the emissions cleaning system to recognize whether the car was being tested based on speed or acceleration patterns, and they also found emails from the engineers involved, questioning whether these software functions were legal.  So it looks like maybe more than just VW was playing games, and that it's sort of a consequence of how complex our cars have become and the fact that companies just believe they're able to tuck away whatever technology they want to out of sight.



Now, the flipside of the inefficiency of JavaScript mining is the efficiency of mining natively on high-performance machines like big iron Internet servers.  It turns out that the Israel firm Check Point, who is often bringing us interesting bits of news, announced last Friday that they had uncovered the footprint of a large hacking operation targeting something known as "Jenkins servers" which were left connected to the Internet.  These Jenkins servers are Java-based technology which are used for developers staging their software.  It turns out that they're intended to be used internally, but there were on the order of 25,000 of them exposed on the Internet.



Okay.  So as always, there was a well-known patched vulnerability.  This was part of a CVE that was issued last April 26th, so 10 months ago, that allowed unauthorized and unauthenticated remote code execution on these Jenkins servers, using something known as a Java serialization/deserialization flaw.  Java uses a lot of data structures, and so Java objects are very structure heavy.  The serialization in Java refers to a technique for turning one of these structured objects into a byte sequence which then allows it to be transmitted or stored, and deserialization is the reverse process.  The deserializer reads the sequence of bytes and reassembles the original JavaScript object.



Well, it probably won't be any surprise to listeners of this podcast that the deserializer is an interpreter, that is, it is an interpreter of the serialized sequence.  And as we often talk about, interpreters are very difficult to get right.  They are a continuing source of security problems.  And it turns out that there was a high-profile deserialization problem in Java which afflicted these Jenkins servers.  It was known about 10 months ago.  It was patched.  But these servers were not updated.



So despite having been fixed over a year ago, the attackers were able to leverage this known remote code execution vulnerability in more than 25,000 exposed servers.  And much as we're able again not to identify who it was who obtained the money, but we can tell from the block chain technology, the cryptocurrency mining technology, how successful they were.  This had been active for months and allowed them to already, I mean, it's active right now, as of this reporting, which was just Friday, they have already cashed out more than 10,800 Monero, which is over $3.4 million.



So here's an example of a crypto mal-mining operation which is succeeding, is running on these Jenkins servers, as a consequence of the fact that it is running on the bare metal.  It's not JavaScript.  It's actually running natively on probably very powerful machines.  I would imagine that these Jenkins servers are limping along because all of their CPU resource is being sucked up by a crypto mal-mining operation that doesn't care about sparing any CPU cycles for what the machine was supposed to be doing.  And as a consequence, whoever this is has made $3.4 million USD, which is some serious coin.  Thus the strength of the impetus to do cryptocurrency mining.



And it's worth noting that, with remote code execution vulnerability such as these servers have, bad guys could have gotten up to all sorts of much more serious, in terms of security penetration, corporate espionage, and digging around in people's network mischief.  But what do they choose to do?  They choose to combine cryptocurrency with their access.



BleepingComputer, a terrific site that we refer to often, had a terrific summary reporting on this.  In their coverage, where they quoted over 2,500 Jenkins servers left exposed online, they said:  "Attackers aren't the only ones who've noticed the large number of Jenkins servers available online.  In mid-January, security researcher Mikail Tunc published research highlighting that there were over 25,000 Jenkins servers left exposed to Internet connections at the time of his research.  Also on Friday" - that is, last week - "FireEye released new research on other hackers leveraging a different flaw which is being used to infect Oracle WebLogic servers with malware.  This vulnerability has been under active exploitation since early December 2017, and one group has already made more than $226,000."  So there's a different cryptocurrency mining operation running on Oracle WebLogic servers.



Then Bleeping Computer says:  "Besides Jenkins and Oracle WebLogic servers, hackers are also targeting Ruby on Rails, PHP, and IIS [Microsoft] servers, also deploying Monero mining malware.  Trend Micro fears," they write, "that two recently disclosed CouchDB vulnerabilities will also soon be exploited in the same way."  They write:  "Monero mining malware is already this year's biggest malware trend and problem, with numerous malware distribution campaigns spreading such payloads on any unsecured computer and server that crooks can get their hands on."



That's what Bleeping Computer said, and then I was reminded of the famous quote, the Willie Sutton quote, thinking that perhaps we should be referring to all these as Willie Sutton attacks, going where the money is.  So it's interesting that, with the advent of cryptocurrency and exchanges that essentially take the cryptocurrency from being virtual to being real.  As we've been talking about recently, there's now a great deal of pressure to get mining malware running on computers wherever you can find them.  And it's generating dollars, not just random mischief or worms that propagate for their own entertainment, but for no other purpose.  Now, unfortunately, for better or worse, we've given them purpose.



I just wanted to make a quick note, this is from the "some secrets cannot be kept" department, that reports of the first "uncrackable," in quotes, Windows 10 Universal Windows Platform, that's the UWP, based game has reportedly been liberated from its captivity.  Who knows why they chose this one.  The game is Zoo Tycoon Ultimate Animal Collection which has been cracked.  The cracking group CODEX says that in order to do this they needed to successfully penetrate five separate layers of DRM protection.



So doing this was not easy.  But from the beginning of this podcast we've noted that there are some things which cannot be protected.  Our first example was the encryption of DVDs back when there was that effort underway because, if the console in your living room needed to decrypt the DVD to show it to you, well, everything you needed to know to decrypt it was right there.  Similarly, if someone is playing a game on their computer, despite any efforts that anyone could make in order to corral it and control it and prevent it from getting loose, it's running on a machine which is able to penetrate all those five layers.  So it's just a matter of someone patiently doing the reverse engineering in order to pry that apart.



We all got updates, those of us using Apple stuff, iOS for iPhone and iPad, tvOS for Apple TV, watchOS, and even the macOS for a problem that Apple clearly knew about before it went public.  We know that Apple knew about it because the planned next sort of semi-major release of iOS, that's 11.3 that we should be getting soon, that's the one that I'm looking forward to because it removes the speed throttling of older devices, or at least gives a user some control over it and notification, and also apparently clear reports on the health of your battery as the phone sees it.  That's the one that's coming, 11.3.  Apple it turns out had a problem with 11.2.5.  It's known as - is it pronounced "Telugu"?



LEO:  You know, I don't know.  We were debating it.  I think it's Telugu, something like that.  It's an Indian dialect.



STEVE:  Yeah, yeah.  It's the third most spoken language native to South India.  And it turns out that - and also there was another, was it - I don't think it was Belize.  Bulgarian?  There was some other language character that was later discovered.  There's a fabulous posting, if anyone is interested in, like, just for grins, understanding exactly every technical detail of what this crash was about.  I've got a link to a page on GitHub.io, a blog page there that really dissects this problem, courtesy of Simon Zerafa, who sent that to me.



Anyway, the bottom line is that sending a Unicode character sequence using these Telugu characters was able to crash these various devices.  Apple knew about it.  It doesn't, for example, crash the beta of 11.3, so they fixed it there.  But news escaped.  It began getting exploited in the wild, and so Apple was forced to push out a quick update, basically that they'd already fixed and would have disappeared without anyone knowing about it in 11.3.  They had to move us.



LEO:  It's a Bengali dialect.  It's another Indian dialect, yeah, or Indic.



STEVE:  Oh, Bengali, that's right.  Thank you.  I knew it was a "B" word.



LEO:  Yeah, they're Indic, so there you have these strange ligatures tying these characters together.  This is a great blog post.  It kind of explains what's going on.



STEVE:  Yeah, the guy really dissected it, like pulled it apart for us.



LEO:  Yeah, really good.



STEVE:  So, okay.  This one, Leo, you're going to want to look on the next page here.  The page starts with "EternalGlue."  There's a picture of a network of computers.  This is a little chilling.  The NCC Group that we've talked about before, they're a well-known security group.  They were contracted by an unknown or unnamed client.  



LEO:  I don't know, you can't really even read this.  It's just it's a weird...



STEVE:  Right.  But notice that it all starts in the upper left with a single machine.  And what we're seeing is we're seeing an infection branching out within an organization.  So as we know, EternalBlue uses the NSA's leaked SMBv1 exploit.  They call this "EternalGlue," a rebuilt NotPetya.  Remember that Petya and WannaCry were the two different pieces of malware based on EternalBlue.



So they write:  "In June 2017" - so last summer - "we were asked by a client to rebuild NotPetya from scratch.  Instead of the data destruction payload, they asked for telemetry and safeguards.  Why?  Because they wanted to measure what the impact of NotPetya" - that is, essentially, a leveraging of the EternalBlue exploit - "would have been for them."  They said:  "We've completed the first phase of live testing in a secure environment deployed by our client."  They said:  "It has been a marathon, not a sprint.  By the time we emerged from testing the code and the associated safeguards in December 2017, we had already been working with our customer in the lab for a number of months.  This slow and steady approach has ensured everything works as intended, and the quality of telemetry is sufficient to answer the client's questions."



Then they wrote:  "Christmas comes early:  EternalGlue's first outing."  And that's what this picture shows.  They wrote:  "On 7 December, EternalGlue got its first outing on the customer's engineering network," and so they said, i.e., a live network, but not corporate.  Now, okay, if this is their engineering network, this is a big customer.  So we don't know who this is.  It's an unidentified client.  But they said:  "The result?  More data than one could have imagined, and interesting insights as to the propagation in live environments."



So the headlines from Phase I of the experiment were, okay, now get this.  The customer ran this EternalGlue, I don't want to call it malware, experiment ware on one machine in their engineering network with no privileges.  It found three machines unpatched.  It exploited those three machines to obtain kernel-level access.  It infected those three machines.  Within 10 minutes it had gone through the entire engineering network using recovered and stolen credentials.  It then took the domain about two minutes later.  One hundred and seven hosts were owned in roughly 45 minutes before the client initiated the kill and remove switch.



So here was a sandboxed, deliberately safe, carefully engineered leverage of, I mean, today the SMBv1, which has long since been patched by Microsoft, operating successfully within an existing organization.  It found a couple machines which had escaped patching, got into them, used its position there to establish a beachhead, get into the kernel, get credentials, and then move through the network.



So if nothing else, this should be a chilling note that, if you have something as potent as the EternalBlue vulnerability is, you have to be really, really sure that you don't have a single machine that is vulnerable to it because it doesn't just let you get in.  It lets you get down into that machine through that vulnerability, and from there maybe do much more damage, even in a network where other things are patched against it.  So it just used EternalBlue, or in this case they called their system of course EternalGlue, in order to escape.  But once it got kernel level on a couple of the machines it escaped to, in a short time it was in, infected 107 different machines in their engineering network.  So interesting and certainly chilling.  And I would say the client got their money's worth from the commissioning of that test by the NCC Group.



Just in passing, a fun story about a small bitcoin mining operation in someone's residence.  T-Mobile complained that massive radiofrequency interference emanating from a local residence, and I don't know where it was located, at 700 MHz, was significantly interfering with the delivery of the company's cellular services.



LEO:  You know, when they first came out with these high GHz processors, everybody was wondering.  They're not broadcasting in broadcast ranges.  Is this going to be a problem?  It's the first time I've heard of it actually being a problem.



STEVE:  Yeah.  The FCC's enforcement bureau said that they contacted the residence.  I mean, I'm sure they drove a truck past and probably the antennas melted.  They contacted the people and said that continued use of - it was called the Antminer S4, and there was some confusion in the reporting because there's also reference to an Antminer S5, yup, there it is - that continued use of it will presumably... 



LEO:  It's hardware.



STEVE:  Yeah.  Yeah.



LEO:  Wow.



STEVE:  And what you do, of course, is you have...



LEO:  A hundred of them.



STEVE:  Rack, rack, exactly, or thousands of them, rack after rack after rack.



LEO:  This was in Brooklyn, so that's why the FCC - because the FCC has, like, five enforcement vehicles, so it's kind of amazing that they could find it.



STEVE:  Right.  Well, of course T-Mobile was able to say our cell tower's gone down.



LEO:  They knew where it was, yeah.



STEVE:  Yeah.  They had a good idea of where it was.  So they said continued use would be subject to fines, criminal prosecution, or seizure of equipment.  So, yes, kids, you may, depending upon where you're located, may be disrupting cellular service within a perimeter of your mining operation. 



LEO:  So it's the processor?  I mean, 700 MHz, is it the GPU?  It's something - and it has got to be transmitting like crazy.



STEVE:  Well, yeah.  Remember that any time, any time a wire has its charge rapidly changed, that will emit a bit of radio.  So sine wave changes don't, but an edge produces a huge, high, large spectrum of interference.  So basically, I mean, I know you know, Leo, that all of the computer cases that we've seen recently, they've got gasketing, and sometimes, well, even remember the Apple II, the inside of the case was like sprayed with that gray metal stuff.  So you really have to go to some lengths to prevent radiofrequency from escaping from anything digital that is going fast.  And of course mining equipment, the last thing they're thinking about is RF interference.



LEO:  These are made in China.  I doubt very much they have FCC "B" approval or anything like that.



STEVE:  No chance.  No chance.  And now they're, what, at the T9?  So that was the S4.  Now they're up at...



LEO:  Yeah, this is an old model.



STEVE:  Right.



LEO:  You can buy them cheap on eBay.  Pretty funny.



STEVE:  So a little bit under the cloud, or under the hood, rather, about Chrome's ad filtering.  As our listeners may already know, what we talked about some months back that Google had announced Chrome was going to do was the so-called "intrusive adblocking."  And I know you talked about this also over the weekend, Leo.  It went live in Chrome on Thursday.  And so from a technology standpoint I thought it'd be interesting to get some sense for what this is all about.



So Google explained the day before in a blog posting in their Chromium blog, like what was the basis for this, and what were they trying to do.  It started with a survey of 40,000 Internet users throughout North America, which was taken by a group known as the Coalition for Better Ads, wanting basically to show them a bunch of things and rate how annoying different types of advertising was.  The two most annoying were the so-called prestitial page, which covers where you are with a countdown before you're able to get through to it to the site.  You know, I mean, Forbes does this; but I don't really mind because you're able to click past it, so it's like, okay.  And then the second most annoying were the flashing animated ads.



So Google notes in their coverage of this, in their description of what they're doing in Chrome, that while some problematic ads are sourced by the advertising supplier, meaning, as we've talked about this often, a site creates a rectangle, like sets aside a rectangular area, and then the advertiser puts whatever they're going to put in there, meaning that the site doesn't control necessarily the content.  They wrote:  "The majority of problematic experiences, user experiences, are under the control of and at the specification of the site's owner," such as high advertising density and things like the prestitial page covers that is script running on the site that does this.



So Google writes:  "This result led to the approach Chrome takes to protect users from many of the intrusive ad experiences identified by the Better Ads Standards [which is] evaluate how well sites comply with the Better Ads Standards, inform sites of any issues encountered, provide the opportunity for sites to address the identified issues, and remove ads from sites that continue to maintain a problematic ads experience."



I won't go through all the details that I have in the show notes.  If anyone's interested they can look there.  But essentially Google is giving sites, or has been giving sites, notice through the API if behavior that Google is seeing through Chrome is in violation and if Google, starting last Thursday, would be taking action against those sites based on this updated set of policies.  And they conclude the posting saying early results are showing positive progress for users.  Of course Google is couching all this as, look, we're not wanting everyone to run adblocking.  We're hoping we can take the pressure off of users taking their own actions by coming up with some compromise.



So they're saying - in their summary they said:  "While the result of this action is that Chrome users will not see ads on sites that consistently violate the Better Ads Standards, our goal is not to filter any ads at all, but to improve the experience for all web users.  As of February 12" - so that's, what, about two weeks ago - "42% of sites which were failing the Better Ads Standards have resolved their issues and are now passing.  This is the outcome," they write, "we were hoping for, that sites would take steps to fix intrusive ad experiences themselves and benefit all web users."



They say:  "However, if a site continues to maintain noncompliant ad experiences 30 days after being notified of violations, Chrome will begin to block ads on that site.  We're encouraged," they conclude, "by early results showing industry shifts away from intrusive ad experiences, and look forward to continued collaboration with the industry toward a future where Chrome's ad filtering technology will not be needed."



So this strikes me as a good thing, but also it's a little scary.  I mean, it's a web browser choosing to enforce a set of policies which it assumes are what its users want and changing the content of the sites.  Now, it is the case that, when you go to a site where Chrome has made some changes, you will see a notification, typically a bar along the bottom, where you're able to opt out of Chrome's filtering of what it considers to be intrusive ads on that site.  So the user has control.  User has notification if Chrome has done this.



And it would be interesting to see how this goes.  I mean, Google has been using, as we've often talked about on the podcast, their power, the power of being the majority browser on the web now, to make lots of changes in security.  Now we're seeing some clear changes in advertising content, with appropriate controls and maybe with a good outcome, we can hope.  Certainly the annoying ads are annoying to all of us.



LEO:  Well, I notice Forbes doesn't put up that interstitial anymore.  I don't know if that's just a coincidence.  But I just went there.



STEVE:  Interesting.



LEO:  Yeah.



STEVE:  Interesting, yeah.  I just had one quick little tweet about my software, SpinRite, that's something I didn't know.  Scott Napier - I saw this tweet as I was going back through my Twitter feed, pulling the show together.  He said:  "@SGgrc SpinRite has quite literally helped to keep the physical security of the Smithsonian intact, and I have convinced some skeptics of its power. Looking forward to 6.1."



LEO:  Kind of a cryptic tweet there.  What do you think?



STEVE:  So who knows?  It may be that the security system for the Smithsonian uses hard drives that were having trouble, or the system went down, and it was like, ooh, crap, we need our security back up.  Run SpinRite.  And apparently in the process skeptics were convinced.  So thank you, Scott, for sharing that.



So I've decided, after reading an incredible number of suggestions for what we call the combination of Meltdown and Spectre, the single-word attempts, that there probably isn't a good one.  But we do have one that is unique that hasn't been suggested yet.



LEO:  Oh, boy.



STEVE:  So I'll call that the winner.  Several people decided they liked S&M, which was what I finished with last week.  But anyway, so just to remind people about how many different ways there are to put these words or word fragments together, since, I mean, just in going through the notes, my Twitter feed since last week.  Nico de Smidt says:  "How about Speckdown?"  Life Cream Scoop:  "How about Smeltdown?"  Matt Fenton suggests:  "What about Melspec, a twist on the word MIL-SPEC," for of course Meltdown and Spectre.  Eat78, how about, he says:  "How about SpecMelt?"  Carol Saye suggests:  "Meltspec."



But then I got, I finally saw one that was a little different that kind of hooked me.  Again, I just think we're not going to end up with actually using it.  But bcrypt.c - which is kind of an interesting handle, you know, bcrypt is a crypto.  Anyway, he suggests "DownSpec'd," D-O-W-N-S-P-E-C-'-D, which is kind of cute because the consequence of this was that we had to reduce the specifications of our processors.  They were downspec'd.



LEO:  Oh, I like that, yeah.



STEVE:  So Meltdown and Spectre, DownSpec'd.



LEO:  DownSpec'd.



STEVE:  So that's kind of the DownSpec'd attacks.



LEO:  All right.



STEVE:  So anyway, of all of them, I think that one wins.  I thank everybody for their suggestions.  Please, no more.  I think every possible combination of pieces of those words have been put together.  But we did come across a new one.  So thanks, everybody.  Thank you, everybody.  DownSpec'd, I think, of those I've seen.



Doug White, tweeting as @cpuguru, sent two things.  He said:  "Aloha, Steve," so maybe he's in Hawaii somewhere.  "Just an FYI that I upgraded my Comcast Internet service to 1Gb speed last week.  Trying to run the speed test resulted in about 460Mb throughput, even though the technician showed 1Gb at the port."  He says:  "I remembered an earlier podcast about the EdgeRouter X and that the internal throughput would be only about half a gig.  So I purchased an EdgeRouter 4, replaced the EdgeRouter X.  Now I have my 1Gb throughput."  He said:  "Figured I'd mention my experience as I imagine there are other home routers that will be likewise speed limited, even though they advertise 1Gb ports."



LEO:  Interesting.



STEVE:  And that's really - thank you, Doug.  That's really good information because...



LEO:  We had a radio - guy called the radio show, I think he was using Eeros, and he was getting 600Mb.  So maybe that's it, yeah.



STEVE:  Yeah.  So the fact that you've got 1Gb ports means that the wire can carry that traffic.  But it doesn't mean that the complex routing and switching logic which figures out where to send each packet at what is really a very high rate, is able to keep up with the so-called line rate of the actual connections.  And then in Doug's second tweet, which actually came in at a different time, but probably is a part of this work he was doing, he said:  "If you're running a Ubiquiti EdgeRouter, looks like new firmware dropped last week."



So as a listener service announcement because I know that a lot of our listeners are using the Ubiquiti routers as a consequence of our affection for them, which is partly because they are so powerful from a configuration standpoint, you know, they're actual routers where you can give each of the ports of this little box its own network and subnetwork in order to create isolation.  Just wanted to say that, I mean, and all that for, what is it, $49, that new firmware is available.  So everybody may want to go check in with Ubiquiti and update themselves.



Two last tweets.  Chip Steiner said:  "When your ISP DNS has the best performance," and he says, parens, "(using DNSBench)," which of course is GRC's very popular utility for measuring DNS performance.  He says:  "When your ISP DNS has the best performance over Google DNS, Quad9, OpenDNS, but it uses DNS filtering, what's your advice?"  And I don't really have any.



LEO:  That's, well, I mean...



STEVE:  What, what?



LEO:  You're going to opt for performance over security or injected ads; right?



STEVE:  Exactly.  The DNS filtering is troublesome because it means that your ISP is in your business and, as you said, is able to perform various sorts of involvement.  You would like when you enter a domain name that doesn't exist to get a DNS error.  Instead, ISPs often use the opportunity to return a page with their own cert suggestions or their own advertisement of one form or another.  So my sense is that using DNSBench is informative, I mean, it gave Chip a sense for the ranking of his options.  But then try to see what it feels like in the real world because remember the DNS is cached.  It's cached several levels within your own system.  And the benchmark deals with both cached and uncached queries.



So I would say maybe try your ISP's DNS, get a feel for the way surfing the web feels, and then switch over, like to Quad9 or OpenDNS that is performing customer-facing filtering for security, which I think is very valuable, and see if you actually notice a difference.  It's one thing for a benchmark to be able to find a difference.  It's not at all clear what that means in terms of actual experience.  So I guess I would say use the information from the benchmark, but then decide for yourself how you feel about the actual result either way.



Oh, and this is very cool.  Chris Ryan sent me a tweet:  "I saw this link for an IoT checker to see if your devices are on Shodan and thought I'd share."  Okay, so here's what this does.  This is very cool.  You can query Shodan for your own IP to see if there are any entries in the Shodan database for your home network.



LEO:  Oh.



STEVE:  Which would be good to know.



LEO:  Yeah.



STEVE:  But this automates that:  iotscanner.bullguard.com.  So I-O-T-S-C-A-N-N-E-R dot B-U-L-L-G-U-A-R-D dot com.  And there are two levels of tests.  The first is a passive - so when you go to iotscanner.bullguard.com, it has your IP, in the same way that ShieldsUP!, GRC's test does.  So it's then able to submit that IP using the Shodan API to see if Shodan reports any results.  And so you're turning up clear on your network, which we would hope.  That's good.



Then, if you want to, you can go to the next level and use this iotscanner.bullguard.com to actively scan your current public IP to see if it is able to find anything.  And again you came up clean, Leo.  And I did it both on my networks here and over at my residence, and they both came up clear at both levels of scan.  So it's like, yes, good.  But it's absolutely going to be the case that some of our listeners are going to discover something that doesn't make them happy when they do this.  So iotscanner.bullguard.com.  And thank you, Chris, for bringing that to my attention so I could tell everybody.



Okay.  So Russian meddling technology.



LEO:  Sounds like something you need [indiscernible] for.



STEVE:  Yes.  So last Friday we know that Special Counsel Robert Mueller, his investigation into the question of Russian interference with the 2016 U.S. presidential election, released an indictment naming 33 Russian individuals and three Russian companies, accusing them of violating U.S. criminal law.  I remember being sort of nonplussed by that.  It's like, okay, well, so, okay.  So the indictment charged the defendants with conspiracy to defraud the U.S., wire fraud, and identity theft.



And so, as we know, this is a technology, not a politics podcast.  But what interests me, and I think is of interest to us here, is not and should not for this purpose the election politics aspect of the effort, but on the technical side of what was done to pull this off.  And I think this is relevant, not only for technical interest, but because we're hearing now a lot in the news and among the talking heads and commentators that Google, Facebook, Instagram, Twitter, and others were, or at least to some degree to be determined, were responsible, maybe negligent, that they should have known better than to have their services abused in this fashion.



So as I've looked at, you know, into what we learned, in many ways, as I mentioned at the top of the podcast, this struck me as being reminiscent of U.S. law enforcement and the U.S. FBI complaining about the use of encryption and the Internet going dark problem and demanding a golden key for access, my point being that it comes down to technology, and technology is a world that we understand.  So the indictment alleges that Russians purchased servers located in the United States in order to obfuscate their origins, and then created and established hundreds of fake personas on social media which they carefully developed into leaders of public opinion.



So they established essentially IP addresses in the U.S. and then went about investing in the creation of social media personas.  They used virtual private networks to open and operate social media accounts, and in that way behaving exactly as any U.S. citizen might.  A lot of users use VPNs for various reasons in public settings, in hotels, in open WiFi settings, in order to get security and privacy.  And they also found and allege that Russian agents stole U.S. identities to open accounts with PayPal to further substantiate and support these false personas and identities and used those to purchase advertisements on social media sites.



So the bottom line is, leveraging the technology of the Internet, which is about freedom and openness and anonymity to varying degrees, they were able to convincingly pose as U.S. citizens and also, again, just by nature of the way social media works today, they were able to recruit through this technology real paid Americans to also engage in political activities, to promote political campaigns, stage political rallies.  And those Americans that had been recruited had no idea nor any reason to suspect that they were communicating with Russian agents.  I loved your accent verbally at the beginning of this, Leo, because it occurred to me that there's no accent in a tweet.



LEO:  No.



STEVE:  There's no way to know how good, I mean, as long as the written English is convincing, there's an inherent disconnection.  So interestingly, the indictment alleges that in some cases the Russians fomented unrest on both sides of the political divide at the same time.  It said that, quote:  "After the election, the defendants allegedly staged rallies to support the President while simultaneously staging rallies to protest his election.  In one instance the Russian defendants organized one rally to support the President-elect and another rally to oppose him, both in New York on the same day."  So there was just a concerted effort to stir things up.



So here we are.  We in the U.S. invented the Internet.  It was our technology.  And we're collectively profiting mightily from its existence, largely because it's such a wide-open communications platform.  But what happened was that an adversarial country very cleverly used our own technologies and even our own bandwidth, on our soil, against our nation's interest.



Now, what I liked in this was something that Mike Masnick at Techdirt wrote that I think helps to nicely frame sort of like where we go from here.  He did a posting titled "DOJ Russia Indictment Again Highlights Why Internet Companies Can't" - can't, C-A-N-'-T, cannot - "Just Wave a Magic Wand to Make Bad Stuff Go Away."  He says:  "This was not just some run-of-the-mill 'pretend to be Americans.'  This was a hugely involved process to make it very difficult to determine that they were not Americans."



He writes:  "I've seen some people online claiming that this shows why the platforms have to take more responsibility for who is using their platform."  And then he quotes a tweet from a Renee DiResta, who tweets from @noUpside.  On February 16th Renee tweeted:  "While you read the Mueller #Indictment, remember the tech CEO mantra:  'We don't want to be the arbiters of truth.'"  The tweet goes on:  "These platforms were used exactly as they were designed to be used.  Here we are a year later, and still no accountability or governance," ends this tweet.



So Mike continues:  "But my read on it is exactly the opposite.  It shows just how ridiculous such a demand is.  Would any of us," he asks, "be using these various services if we were all forced to go through a detailed background check just to use a social media platform?  That seems," he writes, "excessive and silly.  Part of the reason why these platforms are so useful and powerful in the first place is that they're available for nearly everyone to use with few hurdles in the way.  That obviously has negative consequences in the form of trolling and scams and malicious behavior, but there's also a ton of really good stuff that has come out of it."



He says:  "We should be pretty cautious before we throw away all the value of these platforms just because some people use them for nefarious purposes.  People are always going to be able to hide their true intentions from the various platforms, and the response to that shouldn't be put more blame on the platforms, it should be a recognition of why it's so silly to blame the tools and services for the actions of the users."



He finishes:  "Yes, we should be concerned about foreign attempts to influence our elections, while noting that the U.S. itself has a long history of doing the same damn thing," he writes, "in other countries, so this is a bit of blowback.  But blaming the technology platforms the Russians used seems to be totally missing the point of what happened and risks making the Internet much worse for everyone else."



And I can't find much argument with that position.  Because when, I mean, maybe you can apply sophisticated AI, heuristics, or maybe do a somewhat better job.  But it's not at all clear how somebody who has an agenda, even if it's foreign-inspired, differs from an American who has an opinion.



So anyway, I thought it was interesting that they're using technology, given that this indictment and these allegations are true, using technology in a way that it was designed, essentially residing in the U.S. and acting for their own ends.  And it's not at all clear how it's easy to do more than simply jump up and down and stamp our feet and say we want it to be different.  Yeah.  But how do you pull that off?  So there.



LEO:  You're not drawing me into this one.



STEVE:  No?  Okay.



LEO:  I can see you waiting.



STEVE:  And that's our podcast, Episode 651.



LEO:  I can see you waiting.



STEVE:  For February 20th, 2018.



LEO:  It's a very difficult problem.



STEVE:  It is a problem, and it's one we've got.  It'll be interesting to see what happens moving forward.



LEO:  You don't want to limit free speech.



STEVE:  No.



LEO:  I'd be a little more sanguine about it if Twitter and Facebook had been a little more forthcoming.  They haven't been helpful at all.



STEVE:  Yeah.



LEO:  But you're right, I mean, I don't know exactly how, you know, you don't want a proof of identity.  I don't know.  I don't know.  And obviously the proof of identity, they got around it with identity thieves.



STEVE:  Yes, exactly.



LEO:  Well, Steve, we do this show, and there's plenty - and I'm sure you'll hear from people on both sides of the equation.  But there's plenty more to talk about.  Every Tuesday, 'round about 1:30 Pacific, 4:30 Eastern, 21:30 UTC, we convene and discuss the latest in security news with this guy right here, Steve Gibson.  You'll find him at GRC.com, the Gibson Research Corporation.  While you're there, pick up his bread and butter, SpinRite, the world's best hard drive recovery and maintenance utility.  If it's good enough for the Smithsonian, it's good enough for you:  GRC.com.



STEVE:  And you know that it was on the International Space Station, too.



LEO:  SpinRite was?



STEVE:  Yeah, a copy was sent up some time ago.  They liked it because they were literally counting bytes, and it was so small as a recovery utility that they were able to stick it on like a floppy with a whole bunch of other stuff, like utilities that they needed, rather than it needing its own megabytes of space.  And so, yeah, it was used on the International Space Station for quite some time.



LEO:  Very interesting.  I did not know that.



STEVE:  Speaking of famous locations where SpinRite has been used.



LEO:  Well, you ought to have it in your house.  If you've got hard drives, you need SpinRite.  Also he has the show there, of course, 64Kb MP3s and transcripts.  Elaine Farris writes those all out in whatever order Steve refers to things, even when he messes it around.  You could just find that, read along as you listen, at GRC.com.



We have video as well as audio at our site, TWiT.tv/sn, for Security Now!.  And of course you can always subscribe on your favorite podcast appliance, and that way you'll get every episode the minute it comes out.  Collect all 651.  You need the complete set.  Hey, it's just bits.  Come on.  There's plenty.  You're not in the space station, for crying out loud.



We will be back next Tuesday.  But in the meantime I think you have a little more time to take our TWiT Survey, if you want to.  We don't collect information about our users in any other fashion, but once a year we like to do a little short survey and find out whatever you're willing to tell us about yourself, like you like big giant cups of coffee, perhaps.  I don't know if we ask about that.  What coffee cup size would you prefer?  Maybe we should add that to the next quiz, next survey.



STEVE:  That works for me.  How many grams of caffeine do you consume a day?



LEO:  TWiT.tv/survey.  Don't forget, it's very easy to listen to our shows on your voice-activated device, no matter who makes it - Apple, Google, even Microsoft, and of course Amazon.  Just ask for Security Now!.  Say, hey, you know who, "Hey, Echo, I want to listen to Security Now!," and it will play you the most recent episode.  You can often listen to the live stream as well.  Just say, "I want to listen to TWiT Live."



Oh, and we're also on the Flash Briefing.  Many of our shows we just do little one- or two-minute clips that, if you listen to the Flash Briefing on your Amazon Echo, go to the Echo app on your phone and add TWiT to your Flash Briefing rundown.  I think I've said enough.  I'm going to shut up now and wish you all a wonderful evening.  And we'll see you next time on Security Now!.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#652

DATE:		February 27, 2018

TITLE:		WebAssembly

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-652.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss Intel's Spectre and Meltdown microcode update, this week in cryptojacking, Tavis strikes again, Georgia on my mind (and not in a good way), news from the iPhone hackers at Cellebrite, and Apple moving its Chinese customer data.  ePassports?  Not really.  Firefox 60 loses a feature; the IRS and cryptocurrencies; Android P enhances Privacy; malicious code signing news; a VERY cool CloudFront/Troy Hunt hack; a bit of errata, miscellany, and closing-the-loop feedback from our terrific listeners; and a closer look at WebAssembly. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Tavis Ormandy's found another big flaw, this time in uTorrent, the BitTorrent client that a lot of people use.  You may not want to use it after you hear about this.  News from the iPhone hackers at Cellebrite.  Apple's moving its databases to China.  And Firefox 60 is losing its cookie management feature.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 652, recorded Tuesday, February 27th, 2018:  WebAssembly.



It's time for Security Now!, the show where we cover your security and privacy online with the King, Steven Gibson of GRC.com, the King of Security.  Actually, you...



STEVE GIBSON:  I'm not sure who crowned me, Leo, but...



LEO:  Yeah, you wouldn't want that title, I don't think.



STEVE:  No.  And besides, when you take the crown off, it leaves this line across your forehead.  It's really annoying.



LEO:  I hate it when that happens.



STEVE:  You just don't want that.



LEO:  I hate it.  Last show of February 2018.



STEVE:  It is indeed.  I titled this "WebAssembly," not because I want to do a super extensive deep dive, but because a whole bunch of our listeners have asked me about it since I've been talking about the inefficiency of mining cryptocurrencies with JavaScript.  They're all saying, well, what about WebAssembly?  What about WebAssembly?  So I thought, okay, let's talk about that, sort of put that in the proper context, like where that fits in terms of performance and so forth.  So mostly we're just going to kind of wrap up with a, like, what is WebAssembly - what are the benefits, where does it stand, where does it fit in the performance range and so forth.  So I thought, okay, that's what we'll call this February 27th Issue 652 of the podcast.



But other than that, we had a bunch of interesting news.  We do have an update from Intel on the status of Spectre and Meltdown microcode.  And there's a link in the notes, Leo, to a PDF you're going to want to open at some point, just probably for your own edification.  I think our listeners will, too, because it's the complete processor breakdown - which is probably not the right word in this case - of where Intel stands with every single processor they have across their line in terms of the microcode updates.  And so we're going to talk about that.



Also we have this week in cryptojacking, some news there.  Tavis strikes again.  Georgia on my mind, and not in a good way.  News from the iPhone hackers at Cellebrite, or at least rumors.  Also some concerns over Apple's announcement that it's going to be moving its Chinese customer data to China.  A question about ePassports, which were mandated back in 2007, and something you're not going to believe.  Also Firefox 60, which is the next major release to come out, is losing a feature that I expect some of our listeners are going to be unhappy about.  It kind of annoys me, but progress.  We also have some news about the IRS and cryptocurrencies.  Some good news about the next release of Android, which will be the "P."  And in some ways it looks like it enhances privacy, so maybe "P" is for Privacy, also I guess it has to be, what, Pancakes or something.



Then we also have some updates on malicious code signing, a very cool CloudFront/Troy Hunt combo hack which I think is just very, very clever.  Some errata, a little bit of miscellany, some closing the loop with our terrific listeners, and then we will wrap up by explaining where WebAssembly fits in the spectrum of browser-based code execution because it's not the answer for cryptocurrency mining, but it's definitely an answer to something.  So we'll figure that out.



LEO:  We'll figure out what the question is in a bit.



STEVE:  And, oh, another Picture of the Week, Leo.  This is just - oh, lord.



LEO:  That cracks me up.



STEVE:  Yeah.



LEO:  It's got to be a joke; but, well, maybe not.



STEVE:  I think it also - I really think it has to be.  I mean, it looks completely legitimate.  But, you know, it may be a hoax.  Although I think the one with the pail, remember the generator...



LEO:  Yes, this is a series, an ongoing series.



STEVE:  Yeah.  If nothing else it's good for a hoot.  You know, I've been staring at this Picture of the Week while you were telling us about WordPress.  And I think I understand it better than I did before.



LEO:  Well, I think anybody looking at it's going to get it pretty quickly, at least what the intent is; right?



STEVE:  Yes.  Well, and of course the first one we ran across like this was that big motor generator that was near some railing.  And there was a big green solid copper, but green insulation for ground, wire coming off, wrapped around a pole, a metal pole, that was then stuck into a big pitcher, a bucket of dirt.  And of course we all had a lot of fun with that because some...



LEO:  That one looked like somebody believed it would work.



STEVE:  Yes.  It looked like they actually thought, okay, I'm grounding this generator, you know, into [crosstalk].



LEO:  And we should mention, it might even be easier to understand if you used the British term for it, which is "earth."



STEVE:  Yes.



LEO:  Right?



STEVE:  Yes.



LEO:  We're sending it to earth.



STEVE:  Yes.  And sometimes we use both, an "earth ground."



LEO:  Yeah.



STEVE:  Okay.  So just for our listeners who can't see this, here we have kind of off of the image is some big high-power switchbox thing, and we're just seeing the lower right-hand side of it.  But then as code requires, apparently, coming out from a nice grommeted connector is this yellow-and-green-striped wire that is clearly meant to be ground.  It goes into some other thing that probably some other code requires.  And then out it comes again through an anchoring post and then into a rather small-sized plastic bag...



LEO:  It's a one-quart Ziploc bag with dirt in it.



STEVE:  Yeah, a sandwich would not really fit in this thing.  And it's got some dirt in it.  And then in case somebody wasn't immediately...



LEO:  Now, why is that dirt hanging from the power box?



STEVE:  Exactly.  There's also a round yellow sticker with the famous "ground" symbol on it, the line coming down with three horizontals like in a triangle.



LEO:  This is our ground.  This is our ground, yeah.



STEVE:  So you clearly know.  So at first I was thinking, okay, this looks just like, you know, like, really?  But I realized then that that backing that we're seeing looks like it's metal.  It's all, like, sheet aluminum because we can see a rivet over on the right-hand side.  So, and then if you look at the wire coming out, there's a post in the steel or this aluminum or whatever it is...



LEO:  That's the real ground; right?



STEVE:  Yes.  I think that's the point is this entire back sheet is itself properly grounded.  And so the wire comes out and just attaches to a post, an electrically conductive post.  And then somebody was having some fun.  And so they said, okay.  Or maybe they wanted to make it clear to somebody looking at it that the whole back sheet was a ground, so they came up with this little kind of cartoonish thing just to sort of say, look, yes, you can't tell, but the whole backing sheet is hooked to earth, and so we're just going to hang a little bag of dirt here and connect it up just to drive that point home visually.  Anyway, a fun picture.



LEO:  Yeah.



STEVE:  Okay.  I talked about this PDF.  I've got the link in the show notes.  I think if you were to Google "microcode update guidance," that's from the URL of the newsroom.intel.com, where our friend there is still referring to himself in the first person.  Anyway, I think that will probably take you to the update.



So this was released last week with the latest news of where Intel is in this slog that they're in, this trek to produce new microcode, like across their entire product family.  The announcement that goes along with this PDF states that Intel has now released production microcode updates to their OEM customers and partners for Kaby Lake and Coffee Lake-based platforms, plus additional Skylake-based platforms.  This represents their sixth, seventh, and eighth-generation Intel Core product lines, as well as their latest Intel Core X-series processor family.  It also includes their recently announced Xeon Scalable and Xeon D processors.



Now, the problem is Intel's got just a ton of processors.  And so this PDF does allow someone to go in and figure out what the heck, you know, like is theirs there yet?  Now, this doesn't mean these are available to end users, and I noted in some of the coverage on the 'Net that Intel had not yet updated that Linux package which would allow Linux systems to dynamically patch their microcode.  And I don't remember, Leo, a couple weeks ago I ran across some news that Windows had the ability to do microcode patching.  And I don't remember if I ever mentioned it on the podcast.  There are two DLLs in the Windows System32 directory whose names make it very clear they are for Intel and AMD microcode patching.  And so this suggests that, if Microsoft wished to, they could perhaps do this, although that seems like more responsibility than they're going to want to take.



Anyway, so what I was going to say was that this is the release of this microcode to Intel's OEM customers like Lenovo and Dell and everybody else, not to end users.  Linux end users can potentially get this as soon as Intel updates the Linux package, but the rest of us are going to have to wait.  But at least this, for anyone who's interested or for whom this may be mission critical, this indicates where - this updated PDF indicates where Intel is in the process.  And, what, maybe a third, maybe a quarter of the listings there, there was lots of beta and pre-beta and in-planning, which means not yet in production.  The label "production" in that one column is the only one that says yes, we've pushed this out the door, and we're not going to pull it back again, hopefully.  I'm sure they learned their lesson from the first round.



So anyway, they're moving forward.  And as soon as the OEMs are able to verify that it works and test it, I think what we'll see then is another round of BIOS updates which we as end users will then be able to download and run on our laptops and desktops in order to get this Spectre Variant 2 is the one, is kind of the nastiest one that's been causing all the trouble.  Meltdown, that got fixed pretty easily.  The Variant 1 of Spectre was not a big problem.  This is the really nasty one is Variant 2, which we're now still sort of waiting for final fixes for.



I thought, okay, I'm going to call this This Week in Cryptojacking.  And by the way, I thought I had coined a fun term last week, mal-mining.  That's still not bad.



LEO:  No, I like it, yeah.



STEVE:  Yeah, mal-mining.  Except that cryptojacking, I guess, has already become the official term.  So I was late to the party with mal-mining, although it makes it a little more clear, I think.  I like mine a little bit better.  Anyway, a group called RedLock Cloud Security Intelligence, or RedLock CSI, they discovered that Tesla, our Tesla Corporation, had become the latest victim of cryptojacking.  They started their report by saying:  "We are beginning to witness the evolution of cryptojacking as hackers recognize the massive upside of these attacks and begin to explore new variations to evade detection."



Okay.  So what happened is, in the case of Tesla, this wasn't some massive corporate-wide infiltration.  But there was something misconfigured, as so often is the case.  They discovered that one of Tesla's Kubernetes consoles, believe it or not, was not password protected.  It was just wide open.  Kubernetes, I know I've heard you use the term before, Leo.  For those of our listeners who don't know, it's an open source platform originally developed by Google which has now then been released and is independently maintained and further developed by the Cloud Native Computing Foundation.  And essentially it's an automation system for Linux containers which eliminates a lot of the manual processes involved in deploying and scaling containerized Linux apps, so sort of a meta container management console, and you need to give it a password.  Apparently there was an unsecured Kubernetes "pod," as it's called, that was Tesla's.  By getting into that, and I saw a screenshot of this as I was digging around, there just plain as day were the two Amazon S3, the Simple Storage Service, the ID and the key.



LEO:  Oh, no.  Just like in a picture?



STEVE:  Just right there.



LEO:  Oh.  Oh.



STEVE:  Right on the console welcome screen.



LEO:  Of course that's the secret key.  If you give it out, you have access to those buckets; right?



STEVE:  Yes, yes.  That is, you know, that's what you have to absolutely protect.



LEO:  That's your password, basically.



STEVE:  Yes.  Yes, it is.



LEO:  Oh, criminy.



STEVE:  So there was exposure of sensitive data.  And I saw a reference to telemetry, but it wasn't clear what telemetry, and it really isn't important.  What was interesting was that, in addition to that data exposure, as a consequence of having access to this console, the hackers were performing some, well, they were able to run crypto mining, but of a slightly different nature, or sort of more professional than we've seen before.  They were using a number of advanced evasion techniques in order to keep from being discovered.  It didn't keep them from being discovered ultimately.



But it's interesting that we're seeing this sort of next-generation evolution.  For example, they weren't just running something from Coinhive.com.  They did not use a well-known public mining pool, which is like the easy thing to do.  You just run some mining pool software and point to a public mining pool.  The problem is then you have well-known, well-identified endpoint for your traffic, and it's easy to catch.



Instead, they installed their own mining pool software and then configured the malicious script to connect to an unlisted semipublic endpoint.  They even used an IP address for the mining pool software behind Cloudflare because, whenever you register a CDN, you get an IP.  And this allowed them to use an IP that could be changed at will and was floating around, so not locked down.  They also went to the effort of using a nonstandard port.  So they were off the radar.  And the RedLock people who saw this all running noted that the CPU was not near pinned.



So these guys, whoever the bad guys were who set this up, they went to some lengths to install something which would be able to run probably undetected for a long time.  They basically created their own command-and-control infrastructure so that it would not be easily found from scanning or typical behavior, and then didn't want to give themselves away by going full bore, cranking up the CPU usage, because that could draw attention to them.  So they were taking some percentage of the CPU for mining, but probably hoped to just sit there and mine quietly for the duration of however long they could get.



So I think that the take these guys had, the RedLock folks had, is exactly right, that what we're going to begin seeing, what we're going to be seeing is an evolution in the seriousness of cryptojacking or mal-mining, to use my term, that is serious.  Through the years, viruses never made anyone any money.  They were just sort of curiosities.  They were annoying.  They propagated.  They were sort of in the domain of hackers who you sort of wondered, well, why are they doing this?  What's the point?  It was just sort of, oh, look, you know, I can screw around with the world's computers by creating a virus and seeing how far it goes or how long it lives or something.  But they weren't money-making.



But then, as we saw a couple years ago, when this idea of cryptography-based ransomware occurred, where you could arrange to create a key where each instance's key was unique, and you were able to use public key crypto in order to encrypt someone's files and then hold them at ransom, we saw a change then.  Suddenly there was a way to make money, potentially, from these kinds of, I mean, the same kinds of viruses and games that were being played before.  Suddenly dollars.  And of course now we have real world currency exchangeable cryptocurrencies.  So this is not just virtual money, this is money you can buy things with.



So now what we're seeing is the next evolution of this, is that now it's possible to monetize the theft of idle CPU resources, which arguably exist everywhere.  I mean, you know, everything that's on the 'Net has a computer behind it.  And suddenly, I mean, and sadly, light bulbs are, I mean, all kinds of stuff, all of our IoT stuff may not have much power, but there's strength in numbers.  So I really think we're going to begin to see, well, we are seeing, and I think it's going to continue, I see no sense that it's going to reverse.  I remember when we first saw the tip of the ransomware iceberg years ago.  On this podcast we said, uh-oh, this is not good.  Now you can get paid for infecting and encrypting someone's computer.  That creates incentive.



And I think the same pressure, the same leveraging of our unfortunately porous security is going to create I guess what we would call "intrusion pressure" to get into systems exactly like this, find something, and just begin setting up little mining operations wherever you can and participate in a pool and start generating revenue.  In the same way that the first sign of the ransomware sort of gave us this sense that, uh-oh, that we're going to be seeing more of this, and of course we did, I think we're entering a new world where the ability to monetize idle CPU resources, like other people's - because remember, depending upon where you are and what technology you're using, there's so much competition for mining now that the cost of electric power is oftentimes greater than your ability to mine a given cryptocurrency.



That is, your electric bill goes up more than cryptocurrency comes into a wallet.  Again, depending upon where you are, because there's huge variation, at least in the U.S., in the cost of electricity.  And depending upon the size of your installation, a percentage of that electrical power is not just going to computation, it's going to heat, where there's a lot of heat generated.  So now you have to somehow get the heat out of the environment, so you have to pay for air conditioning, too, you know, classic datacenter operations.



So the point is you don't want to use your own power.  It's not profitable.  You want to steal power from other people, no matter how inefficient it is, because you're not paying for it.  They are.  So anyway, I think this is going to be, like 2018 is going to be the year of the rise of this intrusion pressure, that is, we've talked about how porous security is, that if somebody really wants to get in, they can push and push and scan and probe and phish and use social engineering, I mean, the more you want to get in, the greater the probability is you can.  And the idea that there's a profit, potentially long-term persistent profit - think about, for example, the fact that the Internet is all full of this background radiation that we've talked about, that Code Red and Nimda worms are still out there scanning around.  Well, they could be mining cryptocurrency for the last decade.  Well, it hasn't been around for the last decade.



But the point is there is an opportunity to set up long-term residence, stealing power from somebody's closet.  Who knows where these things are?  Anyway, I think This Week in Cryptojacking - or mal-mining, I just have to settle on a term one of these days - that's going to be an ongoing theme, I think, for the podcast.



Tavis, our illustrious Google Project Zero security researcher, peered into uTorrent, and we can pretty much predict the rest.  As we know, uTorrent - is it pronounced "u-torrent" or "micro-torrent"?



LEO:  I say "micro-torrent."  I think the "u" is supposed to be the micron symbol; right?



STEVE:  Yeah.  That's always sort of been [crosstalk].



LEO:  Yeah.  I always say "micro-torrent."



STEVE:  Okay, good.  And of course that is BitTorrent's official peer-to-peer file torrenting application which is in use, I mean, by many millions of active users every day.  I mean, it's the Internet's No. 1 peer-to-peer torrent application.



After checking into the way uTorrent operated last November, Tavis, on behalf of Google's Project Zero, not surprisingly found some problems, actually some bad problems, and reached out to BitTorrent.  It was another instance maybe of him not knowing who to contact, or them not knowing who he was, or being busy, or who knows what is going on at BitTorrent.  But he didn't get any dialogue with them, despite the fact that what he found was a relatively easily exploitable remote execution vulnerability in uTorrent, which, whoops.  That's not good.



So finally, as the clock's ticking - remember that Google's Project Zero gives companies 90 days to repair from the time they are notified, feeling that that should be enough time for anyone to respond with a fix to a security vulnerability, yet still protect users by not making it two years.  Make it three months.



So last week Tavis sent another tweet to BitTorrent's original creator, Bram Cohen, who's now doing cryptocurrency stuff.  He tweeted:  "I don't think BitTorrent are going to make a 90-day disclosure deadline.  Do you have any direct contacts who could help?  I'm not convinced they understand the severity or urgency."  Well, it turns out that BitTorrent had been listening, just hadn't been corresponding.  A "fix," and I've got that in quotes in my notes because they didn't fix it, for the problem is currently in the beta channel, but not yet in a stable release.



Well, Tavis is not impressed, and he wrote in the Project Zero blog, he said, "Hmm.  It looks like BitTorrent just added a second token to uTorrent web.  That does not solve the DNS rebinding issue."  That's what it was that Tavis found was a vulnerability that could be exploited through DNS rebinding.  He said:  "It just broke my exploit," meaning that unfortunately they prevented the one thing that Tavis found, or the one example Tavis gave them, but didn't fix the underlying problem.  They muted the symptom, essentially, rather than curing the disease.  And in this case the disease appears to be significant.



What Tavis wrote in the blog was, he said:  "By default, uTorrent creates an HTTP RPC [Remote Procedure Call] server on port 10000 [uTorrent Classic] or on port 19575" - and that's not a secret, that's well known - "in uTorrent Web."  He says:  "There are numerous problems with these RPC servers that can be exploited by any website using" - and then he uses the JavaScript function, it's XMLHTTPRequest(), which is a way for code running on a page to generate requests for various assets.  Although, I guess, I think you would need to mess around with DNS because normally you would have some same-origin protections.



But anyway, he says:  "To be clear, visiting any website is enough to compromise these applications."  And he says in his notes:  "As the name suggests, uTorrent Web uses a web interface and is controlled by a browser, as opposed to the desktop application.  By default, uTorrent Web is configured to start up with Windows so will always be running and accessible."  Essentially it's like a server running in the background.  And then, if you want to, you can fire up a web browser and connect to this server running in your machine on the localhost port in order to configure it and drop in requests for it to torrent files.



He says:  "For authentication, a random token is generated and stored in a configuration file which must be passed as a URL parameter with all requests."  Okay, so that sort of sounds good.  At installation it invents a random token, and then that's got to be present in the request URL.  He says:  "When you click the uTorrent tray icon, a browser window is opened with the authentication token populated."  And he gives an example of it, standard localhost URL.



He says:  "While not a particularly strong secret, it at least would make remote attacks non-trivial."  He says:  "Unfortunately, however, the authentication secret is stored inside the web root," meaning you can just fetch the secret by asking for http://127.0.0.1:19575/, which is the root of the server, and it returns it to you.  And actually he said:  "WTF!?!?," and he repeats that a few times.  So he says:  "You can just fetch the secret and gain complete control of the service."



LEO:  That's pretty funny, actually.  So a browser could do that.  I mean, JavaScript in a browser, or a website, obviously, yeah.



STEVE:  Yes, yes, exactly.  Just nuts.  So the takeaway for our listeners is be very careful.



LEO:  Don't run uTorrent I think would be the takeaway.



STEVE:  Yeah, I mean, I think - yeah.  There's no way this is safe because, if you were to go to a malicious website, it could use XML HTTP requests in order to determine the secret token and then issue commands.  And he goes a little bit further because he says:  "Once you have the secret, you can just change the directory which torrents are saved to, then download any file anywhere" which is writable in the system, meaning you can replace, I mean, you can essentially - a remote attacker who uses this vulnerability in uTorrent can cause your uTorrent instance to go get a file and place it anywhere in the system.  So the first one would be something malicious, and the second one would be an autostart invocation in your autorun tree.  I mean, it's just crazy what this could do.



So, yes, not safe.  And they didn't respond.  They didn't actually fix the problem.  They just sort of sidestepped it.  So this certainly turns up the heat.  Certainly anyone using uTorrent, I would say don't leave it running.  As Tavis said, by default it just comes up and runs as a service.



LEO:  That's how you normally do it, yeah. 



STEVE:  Yeah.  So I would say don't leave it running when you're not using it.  Use it carefully and be looking for updates from BitTorrent that convince you that this has been fixed in the right way.  Or maybe - I think I have a link in the notes to what Tavis wrote.  Yeah, I have a link to a 260Blog.com story that talks about this, and I followed that down into Tavis's postings on Google Project Zero.  So there is certainly a way to get that.  And I would say wait, you know, I would wait until Tavis acknowledges in his blog that they finally got this thing fixed because, yikes.



So in a move that is very worrisome, Georgia, the U.S. State of Georgia, is moving - it's already passed the Senate, it's now in the House - to criminalize independent computer security research.  And I don't know if in the last year there have been many podcasts where I have said please, please, please don't let this ever happen.  This will be the worst thing that ever happened.  So this is a bill to be entitled, is the way this bill reads, "An Act to amend" - this is all legalese - "Part 1 of Article 6 of Chapter 9 of Title 16 of the Official Code of Georgia Annotated."  And it says:  "...relating to computer crimes, so as to create the new crime of unauthorized computer access; to provide for penalties; to change provisions relating to venue for computer crimes; to provide for forfeiture; to provide for related matters; to repeal conflicting laws; and for other purposes."



And I found the bill.  On February 12th, a couple weeks ago, the bill passed the Georgia Senate and is now in the House.  The language of the bill says, one, defining unauthorized computer access:  "Any person who accesses a computer or computer network with knowledge that such access is without authority shall be guilty of the crime of unauthorized computer access."  With no caveats, no exceptions, no research, no responsible disclosure, no academics, nothing, just "if you access a computer or computer network with knowledge that such access is without authority."  Which is sort of the definition of checking the security of something.  It's like, knock knock.



LEO:  Mind if I hack your server?



STEVE:  Oh, look, the door just, yeah, the door just swung open by itself.  Then, because they're not totally insane, but this doesn't help much, point two in the bill says:  "This subsection shall not prohibit a parent or legal guardian of an individual who is under the age of 18 from monitoring computer usage, denying computer usage..."



LEO:  Thank god.  Whoa, that's good.  Oh, that's a relief.



STEVE:  I know, "or copying data from such individual's computer."  And then it says:  "All laws and parts of laws in conflict with this Act are repealed."



LEO:  Wow.



STEVE:  There are no exceptions or exemptions for the intent of the unauthorized access, nor for the responsible disclosure of any vulnerabilities which might be found.  So as a consequence, honest researchers risk prosecution for verifying.  Anyway, as you can imagine, the EFF has blown a gasket over this.  They're all over it.  There's an EFF branch in Georgia that is...



LEO:  This is a law?  Or they're considering it?



STEVE:  This is about to get passed, it appears.



LEO:  It's not yet a law, okay.



STEVE:  No.  I saw the voting, it was about two-thirds to one-third in the Senate.



LEO:  Yeah, because these people don't know what the hell they're, you know, they don't know what this means.  No idea.



STEVE:  Exactly.  Exactly.  It's like, oh, god.  Yeah.  So, again, I mean, I don't know if we're going to - I guess this is where having a federal government comes in, where the federal government creates something that...



LEO:  Fortunately, they're so much smarter, this will be no problem at all.



STEVE:  Yeah, yeah, yeah.  Oh, boy.  So, I mean, this is horrible, obviously.  I'll keep an eye on it.  We'll see if the EFF is able to, you know, maybe it'll be challenged.  Who knows?



LEO:  Remember, this is the state where it's illegal to teach evolution.  I mean, I don't...



STEVE:  Correct.



LEO:  You've got to wonder which industry is pushing this.  I wonder if Diebold is in Georgia.  Somebody's in Georgia that doesn't want to be revealed.



STEVE:  Well, and you pronounced it correctly.  I have one of...



LEO:  Yes, I know, I know, I saw that, too.



STEVE:  One of our errata is somebody who's really just blown a gasket, speaking of, over me pronouncing "dye-bold."



LEO:  I think I said it wrong, too.  So I said - that's why I threw it in, just to show you.



STEVE:  Yup, thank you.  So we don't have, as a consequence of the nature of this so-called industry of iPhone cracking, we don't have definitive information from Cellebrite, and we're not going to.  But all of the evidence suggests that Cellebrite may now be able to unlock from iPhone 5 through iPhone X, inclusive, all of them.



There was some good coverage in Forbes.  Thomas Fox Brewster writing in his security column for Forbes, he says:  "In what appears to be a major breakthrough for law enforcement" - and again, with caveats that we don't know absolutely positively for sure, but it's worth just putting it on everyone's radar - "and a possible privacy problem for Apple customers," he writes, "a major U.S. government contractor claims to have found a way to unlock pretty much every iPhone on the market.  Cellebrite, the Israel-based vendor that's become the U.S. government's company of choice when it comes to unlocking mobile devices, is this month telling customers its engineers currently have the ability to get around the security of devices running iOS 11.  That includes the iPhone X, a model that Forbes has learned was successfully raided for data by the Department of Homeland Security back in November 2017," so last November, "most likely with Cellebrite technology."



He writes:  "The Israeli firm, a subsidiary of Japan's Sun Corporation, hasn't made any major public announcement about its new iOS capabilities.  But Forbes was told by sources, who asked to remain anonymous as they weren't authorized to talk on the matter, that in the last few months the company has developed undisclosed techniques to get into iOS 11 and is advertising them to law enforcement and private forensics folk across the globe.  Indeed," he writes, "the company's literature for its 'Advanced Unlocking and Extraction Services' offering now notes the company can break the security of 'Apple iOS devices and operating systems, including iPhone, iPad, iPad Mini, iPad Pro, and iPod Touch, running iOS 5 to iOS 11.'"



So then Thomas's column goes into much greater detail, citing interesting but necessarily hearsay accounts of various models of iPhones being accessed, but as always without absolute definitive detail, but of course that's because Cellebrite protects these secrets as much from Apple as from their own customers.  They could probably put this in some software.  They might be able to package it in something that they sell.  But Apple would be first in line to purchase it through a cutout and figure out what Cellebrite is doing and, assuming that they really wanted to, fix the problem.



So the way this works is phones must be physically sent to Cellebrite for them to work their magic.  And then they either unlock and extract or provide some means subsequently for, after the phone is returned, probably for forensics reasons, to allow the authority that had that device in their possession to then get access to it.  So, again, this is hearsay and rumor.  Seems to be pretty well sourced, however.  So who knows what they're doing, what "in" they may have found.  But it does look like, if this is to be believed, there may be a way in, given the sort of extreme access where you have to have - it has to go to Israel in order to be cracked.



Also on the Apple front, following Microsoft with Azure and their Office 365 services, and Amazon, Apple is complying with a new Chinese law which requires the data for Chinese citizens to reside in China.  So for all of these companies it's a matter of either complying with Chinese law or losing the Chinese market, which none of them are willing to do.  Greater China is Apple's second most important market after the U.S., which generated in its most recent fiscal year $44.76 billion in revenue, which is a fifth of Apple's total revenue for the year.  So China's number two, and Apple's not about to say no just because they're not willing to relocate their data.



What has stirred some controversy is that Apple has indicated explicitly that it also intends to store the encryption keys for its Chinese users in China, whereas neither Microsoft nor Amazon have said one way or the other.  They've declined to comment.  So for privacy advocates, of course, this raises understandable concerns because they worry that China may become heavy-handed once it has the data and the keys under its own control.  In some coverage of this, a Beijing-based attorney who was asked about the decision said that Chinese iPhone users are disappointed by Apple's changes to iCloud data storage because privacy protection in China is weak.  However, he said that the iPhone users still consider that iPhone is better than some other pure Chinese-made phones for privacy and policy protection, which I think is a sane position to have.



So Apple partnered with a provider, and I had no idea how to pronounce this.  It's G-U-I-Z-H-O-U.  But the Internet came to my aid.  It looks like it's "guay-cho" is the way you pronounce it.  And so the company is...



LEO:  "Guay-joe."



STEVE:  "Guay-joe"?  You know that?



LEO:  Z-h-o is "joe," yeah.



STEVE:  Oh, very nice.  Thank you, Leo.  Okay, Guizhou.



LEO:  Or "guiy-joe" is probably how they would say it.



STEVE:  Guizhou, okay, good.  Anyway, so the company's name is Guizhou on the Cloud Big Data Industry Company, or just for short Guizhou-Cloud.  It's overseen by the government of the Guizhou Province.  Apple plans to shift, they said, operational responsibility for all iCloud data for Chinese customers in China tomorrow, that is, by February 28th, tomorrow, although not the data.  Customer data will be migrated to servers there over the course of the next two years.



Apple really does sound like it's being as responsible as it can be under the circumstances.  They've first of all declined to say when the encryption keys would move, and they began notifying iCloud users in China last month that their data would be moved into China.  Updated terms and conditions for China users say that Apple and this Guizhou-Cloud will have access to all data and the right to share, exchange, and disclose all user data, including content to and between each other under applicable law.



Anyway, so the Reporters Without Borders group has urged journalists in China to change their geographic region or close their accounts before tomorrow, worrying that Chinese authorities could gain a backdoor to user data, even if Apple says it won't provide one.  And Apple, for its part, again, I think, continuing to do as much as they can, has said that it has advised Chinese customers that they can opt out of iCloud service to avoid having their data stored in China, and that data for Chinese-based users whose settings are configured for another country or for Hong Kong or Macau won't go to Chinese servers.  And Apple said it won't transfer anyone's data until they have accepted the new Mainland China Terms of Service.  So I think they're doing everything that they can, and this is the nature of a global network and the autonomy that countries have over their own citizens.



LEO:  And we should point out that the Chinese could still go to  Apple in the current situation and ask for the keys through the American courts.  And more importantly, if you're American, this is a clear indication that Apple has the capability of unencrypting your iCloud data and will do so if offered an appropriate subpoena or warrant.  Which people, I think, when I mentioned this on TWiT, people went, what?  I said, yeah, Apple kind of implies that they're secure and encrypted, but no.  Your iCloud stuff, they can see it, just like Dropbox and most cloud storage stuff.  And we've talked in the past about why that is.



STEVE:  Yes.  And I think you're very right to disambiguate iCloud from iPhone because what it is that Apple says they absolutely cannot get into is your iPhone.  But I remember in the case of even back in the San Bernardino case, if that person's phone had been backed up to iCloud, then Apple said, yeah, we can provide the data.



LEO:  Apple literally told the FBI, aw, geez, if you'd just told us ahead of time, we would have told you this.



STEVE:  Right, right.



LEO:  We can't unlock the phone.  Now, I wonder, given that Apple's done everything they can to make sure that they can't get into the phone, why they still persist in keeping the keys for iCloud.  Probably because, functionally, trust-no-one encryption on a cloud storage service eliminates, as we've talked about before, a lot of features; right?  That's probably why.



STEVE:  Yes, yes.  There are lots of things you cannot do that Apple needs to do.  If nothing else, probably like recovery.  They're now backing up, you're able to back up your iOS devices to the cloud.  Then it gets run over by a tractor.  Okay, so now you really need it back.  So you may need Apple's help.



LEO:  Yeah.  Customers want it.  In fact, we talked before...



STEVE:  Yes.



LEO:  ...about how Apple's kind of stepped back a little bit in the amount of security they have on those phones.



STEVE:  Yeah.



LEO:  Because customers are saying, "Well, I can't get my password.  I've lost my data."



STEVE:  Well, and in fact we will soon be talking about SQRL.  And I was teasing people week before last that I would have an announcement to make.  It's not something you can download yet, but it is done.  I'm in the process of adding a visual monitor screen to the secret data block for us in the GRC newsgroup to watch the app's use of secret data so that we can absolutely visually verify that it is being proactively wiped from memory. The only way I know to do that is just to like actually see it and watch it disappear the moment you're finished with an operation.  So during the final wrap-up I need to go through and check the jargon that the app uses, reread the terminology to make sure it's consistent; but, I mean, we're there.



This comes up in this instance, Leo, because the whole point of SQRL is there's no recourse.  There's no one, if you refuse to back up your identity, if you refuse to print out something that we help you print out, I mean, if you refuse, refuse, refuse, refuse, then okay.  The whole point of it is TNO, you know, that you are responsible.  There's ways you can recover your identity, but you really just have to do a couple things in the beginning.  Then you're completely recoverable.  But this is no third party.  And so it remains to be seen whether that is too much responsibility for people to have.  Maybe it is.  I mean, maybe people don't really care about their security that much.  We'll find out.



But what has been built now is a system that at least says, okay, we've built a workable system that is between you and the websites you visit.  You can very securely authenticate your identity in a way that breaches at the server can't expose passwords or email addresses or any - SQRL gives websites no secrets to keep so they don't have anything that they have to worry about protecting from disclosure.  But the flipside is there's no third party to go crying to if you forget your master password, although there's lots of recovery available.



So anyway, yes, Leo.  So I think you're right.  I think that Apple has sliced this carefully and deliberately so that they are able to help people recover the contents of their phones and their photos, like their life's collection of photos, if they need to.



LEO:  Right, right.



STEVE:  And you're always being prompted to increase your iCloud storage.  I'm still within the minimum, but just because I'm kind of persnickety about that.  I imagine a lot of people just say, oh, sure, I'll pay a little more per month.



LEO:  Yeah, Apple pushes you to back up your phone to iCloud.



STEVE:  Yeah.



LEO:  Which means, even if your phone is encrypted, as they pointed out to the FBI in the San Bernardino case, your iCloud's not.



STEVE:  Yup.  Well, it is.  But they are able to access it.



LEO:  Yeah, it's encrypted.  Well, sort of.  



STEVE:  Okay.  Now, you're not going to believe this one.  Introduced and mandated more than a decade ago, and you travel a lot, so I'm sure you have one, back in 2007, so 11 years ago, all newly issued passports must now be ePassports, as they're called.  And I'm sure you've got one, and you've got a little chip in yours; right?



LEO:  I do, yeah, little RFID chip in it.



STEVE:  Yeah.



LEO:  It's in the spine.



STEVE:  Citizens of the 38 countries on the visa waiver list must have an ePassport in order to be admitted to the U.S.  And of course U.S. citizens have to have one when traveling abroad.  These ePassports have an electronic chip containing cryptographic information and machine-readable text, which makes it very easy to verify a passport's authenticity and integrity.  Or at least so goes the theory.  The what is it, CPB, the Customs and Border Protection, U.S. Customs and Border Protection, is the division of the U.S. that is in charge of this.  And the border staff have long since deployed ePassport readers at most ports of entry because it makes reading the data out of the passports and automatically and electronically logging the passport data easier, quicker, and more efficient.



Okay.  Now get this.  Although all ePassport data has been cryptographically digitally signed from the beginning, Customs and Border Protection has never, doesn't today, never has had the software necessary to authenticate the information stored on the ePassport chips.  I kid you not.



LEO:  So they just take whatever you say, they go, okay.



STEVE:  Yeah, yeah.  So it can be forged at will, and it's super encrypted, super secret, cannot be forged.  So they just, I mean, now you're more vulnerable to forgery because this is the unforgeable cryptographically signed ePassport system which doesn't check the signature.  Somehow this...



LEO:  Well, that makes it easier.



STEVE:  Yeah, exactly.  Oh, those pesky signatures.



LEO:  In other words, as [indiscernible] said in the chatroom, fake ePassports are just like fake regular passports.



STEVE:  Exactly.  Well, except worse.



LEO:  Feature parity, yeah.



STEVE:  Because they're impossible to forge; right?



LEO:  Yeah.



STEVE:  So they're more strongly believed...



LEO:  Right.



STEVE:  ...if it's an ePassport because it's all crypto something magical.



LEO:  Yeah.



STEVE:  Except it isn't, and it never has been.  So somehow this fell across the radar of senators Ron Wyden - thank goodness he's there - and Claire McCaskill, who wrote to U.S. Customs and Border Protection's Acting Commissioner Kevin K. McAleenan - geez.  I didn't pronounce this head of time.  McAleenan.  I can't pronounce his name.



LEO:  I'm not going to help you.  I think it's Guizhou.  I think it is.



STEVE:  It's not even Chinese.  



LEO:  I think McAleenan is what I would say.



STEVE:  Oh, thank you.  McAleenan.  I like [crosstalk].  Yes, Kevin K. McAleenan for clarification of this rumor and demanding some attention.  In their letter they said:  "CBP does not have the software necessary to authenticate the information stored on the ePassport chips.  Specifically, CBP cannot verify the digital signatures stored on the ePassport, which means that CBP is unable to determine if the data stored on the smart chips has been tampered with or forged," their letter stated.  And the CBP has been aware of this glaring lapse since at least 2010, when the Government Accountability Office, the GAO, released a report highlighting the gap in technology.  So we know what that means.  What that means is there was somewhere, back in 2007, there was a public and private key pair that was created.  And anyone who is minting one of these passports has super secret private control of this private key.



LEO:  Of course they do.



STEVE:  So the data for the passport is hashed, and the digest, the hash of the data, is then cryptographically signed with the super secret private key, which nobody else has.  And every ePassport.  And, you know, passports can come from different places, there must be a collection of private keys, any one of which can be used to sign.  Okay.



Then, on the border entry side, you would have this device which reads the data, performs the hash of the data, and then uses the matching or one of the matching public keys to decrypt the stored signature and verify that the hashes match.  And none of them do that.  They never have.  So anybody has been able to forge, alter the data in the passports, and it would never be noticed.  So maybe some day.  It's been 11 years.  Maybe we'll get around to doing that.



LEO:  It couldn't be that hard.



STEVE:  Leo, our browsers do it.



LEO:  I know.



STEVE:  I mean, no.  It's trivial.  I mean, that's, you know, every web browser that all of us have has the ability to verify that certificate that we receive from a website.



LEO:  Yeah, can't be that hard.



STEVE:  No, everybody else can do it except Customs and Border Protection.



LEO:  Oh, lord.



STEVE:  Incredible.  I mentioned at the top of the show that Firefox would be losing a feature that some old techie curmudgeons might grumble about.  The next release, we're at 57 point something or other right now, the next major release is thus 60.  And the Firefox Nightly build, which is currently at 60, has made some changes to cookie management.  In other words, it's disappeared.  We've been seeing this trend in Firefox for a while.  It sort of disappeared from the UI where it used to be present.  And then I remember going looking for it, like a year ago.  It was like, where did they hide this?



And you have to, like, you have to change the history.  It's under your history settings for some reason.  And it's like, what?  Okay.  So you say, okay, like custom history settings.  And then out of the UI drops some ability to still see into what cookies you've got.  You can browse them.  You can delete them.  You can basically audit them.  That's disappearing.  Now, maybe, I'm hoping, that for those who this really upsets, there may be like a browser plugin you would be able to add to recover cookie management if you really have to have it.  But I also think this sort of represents a trend in just sort of the general popularizing and user friendly-izing of the 'Net.  I mean, what percentage of - I'm sure they have instrumentation, telemetry on this that says, okay, Steve and four other people in the U.S. last month looked at their cookies.  So, okay, so not enough to have that still in the UI.  So, and we're beginning to see this, where lesser used features are being pruned over time.



So for what it's worth, Firefox moving forward, we lose the ability to audit our cookies.  Maybe if there's a, I mean, I'm a little surprised because Firefox has also sort of been the techie user's friendly browser.  So it'll be interesting to dig into this deeper and see what the story is.  But it has been confirmed that we're losing the ability to manage cookie settings and remove individual cookies in Firefox with the next release.



Under "nothing's certain besides death and taxes," Coinbase has informed 13,000 of its users that their data will be sent to the IRS soon, within 21 days.  They fought an IRS order which was first made back in November of 2016, asking for Coinbase's records of all people who bought bitcoin from 2013 to 2015.  



LEO:  How about selling bitcoin for dollars?  That, too?



STEVE:  Yeah.  And what I - I'm not sure.



LEO:  Bought bitcoin.  You'd think they'd want to know who got dollars as opposed to bitcoin.



STEVE:  Exactly.  So Coinbase informed about 13,000 of its customers who had, now, what the reporting said, completed transactions totaling more than $20,000 through their accounts in a single year between 2013 and 2015.



LEO:  So they're probably more interested in money laundering than tax evasion, it sounds like.



STEVE:  Yeah.  And so they said that they will be complying with an IRS court order compelling it to disclose the details of those previous year transactions.  Of course, that's kind of sticky because these are years for which people have already filed their taxes.  And so, yikes, you know, you want to make sure that you, well, declared those back then.  I don't know what happens if you amend a back return in that way.  It's probably going to upset things.



Anyway, in an email and on its website last Friday, Coinbase noted that it had, quote, "fought this summons in court in an effort to protect its customers and the industry as a whole," they wrote, "from unwarranted intrusions from the government."  But they ended up losing in court.  And so they added that a "...court order requires us to produce information specific to your account."  And somewhere I saw, oh, yeah, including taxpayer IDs, names, dates of birth, addresses, and transaction records from that period.



So you're right, Leo.  The fact that it's a limit of 20,000 suggests maybe what they're looking for is like major egregious large bulk transfers through Bitcoin and Coinbase.  So I guess probably most people would be staying under that limit in terms of transactions.  If and when I ever attempt to liquidate mine, I will certainly declare the taxes without question because I'd have to.



LEO:  Why take a chance.



STEVE:  Yeah, exactly.



LEO:  Coinbase says it's 13,000 users, so that's a small number, I'm sure.



STEVE:  Yeah.  So Android P is the next version of Android.  I guess it's expected at this year's upcoming Google I/O developer conference May 810 in Mountain View.  And I was chuckling because it would be nice if the "P" edition stood for Privacy.



LEO:  No.



STEVE:  No.



LEO:  No.



STEVE:  And at least - and do we know, is it always a dessert?



LEO:  Yeah.



STEVE:  Couldn't be Pancake.



LEO:  No.



STEVE:  What's a "P" dessert?  Has there already been speculation?



LEO:  Oh, yeah.  There's lots of them.



STEVE:  I guess there would be.  Okay.  Anyway, so of course we've talked extensively about app permissions in Android, about the sometimes overly broad permission requests.  In fact, there's a couple people working on Android clients for SQRL, and there was just some recent discussion in our newsgroup about an initial beta of the Android client, or one Android client for SQRL that was asking for more than it looked like it needed just because the framework had those things defaulted.  And so he quickly pared it down to almost nothing, which is nice.  Anyway, overly broad permissions, about permission auditing and management, and also of course about how background applications, which are loaded, but which you haven't been using for some time, can be used potentially to stealthily monitor and spy.



So according to the Android Open Source Project, AOSP, to a recent commit to that, Google is working on two built-in features that will be part of Android "P," meant to protect its users from malicious apps spying on them using the smartphone's camera or microphone.  The XDA developers saw this, and essentially it notices whether - it identifies apps by the UID, the unique ID of the app.  And if the app is running in the background for more than a certain amount of time - and that wasn't specified.  But probably you switch away from it, and after some length of time, a few seconds, because you're no longer using it, and if you don't switch back within a certain length of time, it loses access to the camera.  If the app continues to try to regain access, that will then generate an error, an explicit error from the app.



And as for the microphone, what Google is doing is simply muting the mic.  It just returns zeroes in the audio stream so that no audio - the app continues to believe it's receiving, but it's just gone silent, which sort of makes sense because you put it behind other apps, and you're doing something else.  So anyway, just nice to see that Google is moving forward and responding to its users' privacy concerns, and in a way that doesn't probably interfere with anybody.



I remember, was it Stuxnet?  There was, some time ago, there was some malware that was signed with a valid certificate, and that got us all up in a frenzy at the time.



LEO:  Oh, yeah, from Turkey, yeah, yeah.



STEVE:  Yeah.  And remember like some legitimate manufacturer was broken into in the dead of night, and we believe that the certificate was stolen, and that that was then used to sign some high-profile malware in order to allow it to get used.  Well, those were the quaint old days, Leo.



LEO:  Oh, no.



STEVE:  Yeah.  It turns out that nowadays malicious code signing certificates are no longer stolen.  They are created.  There's a threat intelligence firm, Recorded Future, which dug into the underground certificate supply industry, of which there now is one, and came up with some surprising discoveries.  First of all, for purchase through several organizations on the DL, you can get valid certificates, validly signed, registered in the names of real corporations, which, I mean, simply by paying some dollars.  Most certificates are no longer being stolen from legitimate organizations.  They are using the legitimate credentials of unaware corporate entities, and someone is arranging to mimic the corporation in order to cause a legitimate company - examples were Comodo, Thawte, and Symantec - as certificate authorities that were behind signing these certificates that were legitimate, but spoofed.



So essentially what's happening is someone is arranging to spoof the identities of the requests, and non-stolen legitimate certificates are being issued.  And because the owning, technically the owning or at least the company of record for the certificate doesn't know that it was done behind their back by an authority in good standing that thought it was doing the right thing, these certificates can live for years, for however long the cert's natural life is, typically two or three years.



It turns out that, of course, as we know, the reason this is being done is that legitimately signed code, it turns out, in various studies that have been done, is up to twice as likely to pass unnoticed through antiviral systems.  Antivirus systems, as we know, have become very heuristic in nature.  They're making judgment calls, basically.  And so one of the signals that they judge on is whether the code is signed.  And EV signing, not surprisingly, makes the AV systems even more happy and less prone to give a warning.  EV certificates often suppress the SmartScreen warnings in Windows, for example.  So signing is extremely effective in obfuscating malware from analysis.



Okay.  So what do these things cost?  Three years ago, back in 2015, these non-stolen, created-to-order code signing certs first became widely available to the criminal underground.  The most affordable version of a code signing certificate can be had for $300.  So on the low end is an Authenticode cert for $300.  At the high end is an EV certificate with a smart screen reputation rating, which can be had for $1,600.



LEO:  That's less than I pay for my EV certificates, my real ones.



STEVE:  Yes.



LEO:  My real ones.



STEVE:  Yes.



LEO:  So why buy a real one when you can get a forged one free?  Cheap.  Cheap.



STEVE:  Ah, well, you've got to deal with the dark underbelly of the Internet, of course.  



LEO:  Oh, okay.  But EVs are thousands of dollars.  $1699, that's cheap.



STEVE:  Yeah, yeah.  And it comes with a pre-built-in smart screen reputation rating.



LEO:  Damn straight.



STEVE:  Allow it to just get installed by your customers.  Unfortunately, it wouldn't bear your company's name.  I mean, I would love to know, like, what types of companies.  I wouldn't be surprised if they're legitimate-looking organizations so that, if someone did drill into their cert, although no one does - and that's another aspect of our UI which we've talked about before.  It's become, you know, they're beginning to be like, oh, no one really cares about who signs their certs.  Well, it would be interesting to see what company names are being borne by these fraudulent but legitimately and freshly minted certificates, just to see.



Somehow there would have to be companies where spoofing could be arranged, where all of the verifications which one of, like, a major CA would go about performing could be intercepted or somehow spoofed.  Maybe there are insiders inside some corporations that are getting a piece of the action to facilitate certificates being minted in their companies' names.  To me that seems a little more likely than going to - because remember, any company can ask for a certificate from one of these issuers.  So we have that same sort of "trust everyone" approach to this.  If the certificate is signed by a legitimate entity, and the certificate itself is legitimate, then it passes muster.



So anyway, I just thought it was interesting that it's no longer the case that you need to hack or steal an existing certificate in order to get something signed.  If you've got the money, and a lot of bad guys don't, they want to do this on the cheap, and so maybe hacking is easier, but you can just purchase certificates now on the dark web.



Okay.  Now, in what has got to be one of the cooler hacks to come along, our friend, friend of the podcast, John Graham Cumming.  He tweeted, he said:  "The beauty of @Cloudflare Workers" - which is a technology that they have, so-called Cloudflare Worker, like threads, essentially - "is that it was easy to integrate @troyhunt's new pwned password service to add a header indicating whether a posted password is pwned or not. Then the server can warn the user."



Okay.  So let's back out of that a little bit and understand what this means.  So we've got Cloudflare, that is, of course, a terrific CDN frontend, behind which websites operate.  What Cloudflare is now doing - oh, I should back up and say that Troy Hunt, who runs the HaveIBeenPwned.com site, he added an API which allows something to query to see whether a specific password is in his, what is it, like five point some million password list.  Oh, yeah, 501 million.  So not low millions, 501 million unique passwords.



So now what Cloudflare is doing is, when a form is submitted with an HTTP POST, which is the way you submit forms, that contains a username and password.  Their edge technology in the so-called Cloudflare Worker looks at the form being submitted, sees whether there is a password in the POST field, and, if so, submits a query to Troy's API to see whether the password being submitted is on Troy's list of pwned passwords.  If so, a header, an HTTP request header is added on the fly to that query as it then travels on to the server running behind Cloudflare's front edge.  Meaning that anyone running a website which is hosted on Cloudflare could easily add some technology on their login, or create an account handler to check for the presence of that header, its Cf-Password-Pwned header.  If that's there, then that tells any site who's being hosted on Cloudflare that maybe they should consider notifying their user who is in the process of creating an account or presumably logging in maybe if it already exists, that, whoops, this password may not be safe to use.  So very, very nifty and cool service.



Troy himself, upon learning about this, he tweeted:  "I think this is one of the coolest use cases I've seen for Pwned Passwords yet."  He says:  "It's a @Cloudflare Worker," he says, "(code that runs in their edge nodes) that can automatically check a password on form posts (such as login) and add a response header indicating pwnage."  He says:  "That's awesome."  So hats off to the guys at Cloudflare.  They keep coming up with neat ways to make their service even more useful and terrific.



And Troy, for his part, just launched Pwned Passwords v2 with - and this is where I got the 501 - he says, with half a billion passwords for download.  He actually does offer a torrent, and he asks people to please use the torrent.  It's hosted by Cloudflare.  So if you download this thing, I think I remember it's like 8GB.  So if everybody individually downloads the 8GB file, that's a load on Cloudflare's bandwidth.  So he says:  "Please use the torrent if you can."



Anyway, so last Thursday, in a long and wonderfully detailed posting, and for anyone who's interested, I've got the link in the show notes, but you can just also google the phrase "Troy Hunt:  I've just launched Pwned Passwords v2," and that'll take you to his page.  For anyone who's interested in this topic, I'm not going to go into it in detail, yet he wrote this fantastic, lengthy sort of update on the whole world of this password pwnage and what he's been doing with it, and the service he offers, and the statistics, all kinds of really interesting and cool statistics that he's collected over time.



I grabbed one, just one paragraph from that.  He wrote:  "In total, there were 3,033,858,815 occurrences of those 501,636,842 unique passwords."  So again, a little over 3 billion occurrences of those 500-plus million unique passwords.  He says:  "In other words, on average, each password appeared six times across various data breaches. In some cases, the same password appeared many times in the one incident - often thousands of times - because that's how many people chose the same damn password," he wrote.



So obviously - and hopefully this is just historically because we should all now be using random gibberish passwords.  People don't.  But so what he's got is really interesting statistics.  He's done a cross-correlation of password reuse across breaches.  And as he said, on average, each password appeared six times across various data breaches, and in some cases thousands of times, not because, well, it couldn't be thousands of users because you've only got one user at most per breach.  So lots of password reuse.  Anyway, so cool, cool service from Cloudflare and a neat service from HaveIBeenPwned.



And remember that HaveIBeenPwned.com allows you to safely put your password, put your actual password, yours, into the HaveIBeenPwned web page.  It is SHA-1 salted and hashed by JavaScript running in your browser, so that password is immediately encrypted, essentially hashed, well enough.  And Troy defends his use of SHA-1, which is plenty strong for this application.  So it is salted and hashed, sent to him, where it is then securely compared against this half a billion previously disclosed passwords.  So you can securely check whether your password has leaked in any of these known breaches in the past.  So very, very cool stuff.



As I mentioned at the top, someone tweeting as "Very Stable Genius" tweeted to both, sent to both me and to you, Leo, he said:  "Deeee-bold."



LEO:  "Deeee-bold."



STEVE:  "Deeee-bold."



LEO:  It's spelled D-I-E.  "Deeee-bold."



STEVE:  Yes.



LEO:  Anyway, I was glad to get that corrected.



STEVE:  And it looks like "Die-bold" to me, but not "Deeee-bold."  He says:  "As a former employee who has written you several times over the years..."



LEO:  Whoops.



STEVE:  "...with this same request, please, please, I beg you, please" - this is where we're not sure that it was a good thing for Twitter to expand the length of tweets - "stop making me cringe every time the name is mispronounced."  Smiley face.



LEO:  It's Guizhou.



STEVE  "Love the podcasts."  Right, Guizhou.



LEO:  I'm glad to get the correction because I want to say it right, too.  And I actually, it's like GIF and JIF.  I say it both ways to cover my bases.  So I now know, it's Diebold.



STEVE:  Yeah, so now we know.  And please perk up when you see me stomp on it again.  I'm sure that our Very Stable Genius will.



LEO:  I'll protect you.



STEVE:  But anyway, I will do my best.  Diebold.



LEO:  Steve is a very - Diebold.



STEVE:  You know, why did they spell it Diebold if they wanted people to pronounce it "Deee-bold"?



LEO:  Well, it's probably somebody's name.



STEVE:  I'm sure it's some guy's name, yeah.  Oh, and Michael Horowitz, who writes the Defensive Computing column and has a great site about router configuration, added - this is also in Errata.  He said:  "Steve, what you said on the last Security Now! about DNS servers was not the whole story."  Remember I was answering the question about whether, if you configured your DNS in your router and differently in your computer, who won the war?  And I stated that by default "Obtain IP address automatically" and "Obtain DNS servers automatically" was what computers did.  So if you changed what was being sent in your router, then that's what everybody would use.  But I said you could override that by manually inputting an IP address and/or DNS servers in your computer.



So Michael corrects me.  He says:  "Peplink routers can override all DNS requests from devices connected to their routers.  In this case you WILL use" - you WILL, he has in all caps - "you WILL use the DNS servers in the router, NOT those hard coded in your computer.  From the perspective of the router, it's easy," he says.  "Just look for outgoing port 53 traffic."  And of course he's correct.  It's UDP.  It's unencrypted.  It is instantly easy to find.  It's going to be a port 53 query.  And so you simply rewrite the destination IP in the packet as it's transiting from the LAN to the WAN outbound, and the router wins in that case.  Now, if you got really fancy with something like an OpenDNS, where you were setting up a TCP connection or encrypting your stuff, then that changes the game.  But of course then you're using OpenDNS and not just arbitrarily trying to use a different router.  So thank you, Michael, for the clarification.  And I saw...



LEO:  I'm puzzled by this because isn't there a DNS cache in the computer?  Wouldn't that be sufficient in the hosts file?  Don't those get referred to first?  And if they come up with a response, doesn't the computer just stop?



STEVE:  Well, yes.  Okay.  So the hosts file is static.



LEO:  Yeah.



STEVE:  And if you do put things in there, it absolutely wins.



LEO:  That wins.  And a cache wins.  If it's in the cache, it's going to win; right?



STEVE:  Right.  But the cache is always started from scratch.



LEO:  When you reboot.  



STEVE:  Like when you reboot.  And it also...



LEO:  But I think it's important because...



STEVE:  It also times out.



LEO:  Ah, okay.



STEVE:  Yeah, because all DNS will drain over time.  DNS records have a TTL, a Time To Live, which is like, eh, sometimes it's a day, sometimes it's a week, sometimes it's an hour.  It varies.



LEO:  That's the point is, if your cache gets poisoned, it's going to win every time; right?



STEVE:  Yes, yes, it'll stay poisoned for a while, yes.



LEO:  Okay.  So, yeah.  I mean, I set it in my router, of course.  And I think if you set it in the router, barring that, a hosts file or cache poisoning.  Well, but then, yeah, the computer setting isn't going to make any difference if you have that kind of router. 



STEVE:  Exactly.  And Michael did add another router.  I saw another tweet from him.  But it passed by, and I didn't catch it.  So there are routers, we'll just say...



LEO:  That enforce it.



STEVE:  ...where you are able to override.  And I've not seen this in any typical consumer routers, but the Peplink is a router that offers the feature.  And the point is from a technology standpoint it's not difficult to do.  If the router has the feature, it would be able to intercept anything that was outbound and...



LEO:  Ah, and overwrite it, yeah, got it.



STEVE:  Yes.



LEO:  So if had the feature, it'd overwrite anything, including a DNS cache.



STEVE:  Exactly.



LEO:  Well, wait a minute, though.  There's no cache query.  It's just - no, because the router's not seeing a cache query, it's just seeing 192 dot...



STEVE:  Right.  But the first time the computer...



LEO:  The first time.



STEVE:  Yes, yes.



LEO:  So it isn't - yeah.  So it isn't that straightforward.  The cache wins, and then hosts.



STEVE:  Actually hosts...



LEO:  Hosts, no, I bet cache is first just for speed, Steve.



STEVE:  You know, in every instance when I've made a change to my hosts file...



LEO:  It's immediate?  Okay.



STEVE:  ...it wins instantly.



LEO:  Okay.  Okay.  



STEVE:  So I think it looks there even before it looks for caching.



LEO:  We were talking about Ubiquiti routers, and a couple people asked me about hardware offloading.  And so I wanted to talk about that just because it's something we've never discussed on this podcast.  And I have a link to a really nice page about it on the Ubiquiti site, for anyone who's interested.  And it's significant because it turns out that a long time ago, back in the DDoS era, when people were like doing SYN floods, we talked about how SYN packets, S-Y-N, the synchronized packets that are used to start TCP, they were damaging or difficult to handle because they were so small.



What that meant was that a given amount of bandwidth could carry a very high packet rate.  And so it was oftentimes the per-packet handling capability that was the limiting factor.  So a router that could handle, say, 100MB of legitimate data could be brought to its knees by much less bandwidth of pure little tiny 60-byte SYN packets because they were so small.  That is, there were things that the router did per packet.  And if the packets were tiny, many more of them could be sent per unit of time than legitimate regular data-bearing packets.



Well, this is all, of course, well known to router manufacturers.  So one of the things that network adapters have long done and that routers do, too, is known as hardware offloading.  Because, for example, every single Internet packet has a checksum.  It's a simpleminded checksum.  It's a ones' complement checksum, where you just sum all of the packets and don't worry about overflows, I mean sum all of the bytes of the packet.  And once you've done that, you stick the checksum in the packet.  And the idea is that when any other device receives it, that device sums up all the bytes and verifies the checksum in order to sort of do a simple-minded "were any bits altered in transit."



Well, the point is that takes time.  And that's a dumb use of the main CPU, the CPU in our computers.  So for a long time checksumming of packets is something that the hardware in our NICs, in our Network Interface Cards or Network Interface Controllers, did for us.  It was just like, so the idea was that the driver, the OS driver for the hardware knew that the hardware was capable of doing hardware checksumming.  And so it let the packet go right by without bothering itself to fix, as it's building the packet, without bothering to put the checksum in the packet, knowing that the hardware itself will do that with zero overhead.  It can do it at register speed so that essentially the software is offloaded from that responsibility.



Well, the same thing happens with routers.  The more advanced router hardware has the ability of doing various levels of hardware offloading, in some cases even involving encryption and encryption verification and signature verification of encrypted payloads.  And, interestingly, the Ubiquiti routers, many of them, even the EdgeRouter X, has that capability.  It may not be turned on by default.



So last week we were talking about a router where the throughput that was measured was - I think it was down in the 400Mb for a given router.  And when that listener of ours switched routers to a beefier router, they were able to get right up near the 1Gb that they believed that their connection was able to handle.  I did see in this documentation, and again, if you're running a Ubiquiti EdgeRouter that we've talked about, you might want to check out this link.  Probably you could google "EdgeRouter hardware offloading explained," and I would bet that Google will take you to the Ubiquiti page, which is very comprehensive.  But they have in their firmware the ability to turn on hardware offloading, and everybody will get a substantial benefit if that's done.



LEO:  Why is that not on by default?



STEVE:  Because their page does say that their firmware is a little skittish about it for some reason, and so if there's any compatibility problem created, they want to make sure that users can say, whoops, whatever I did, that just broke something, and so they're able to back out of it.



LEO:  I have to try this because I - oh, a couple things on the EdgeRouter X.  One, somebody in the chatroom says it does support that Peplink thing that we were talking about of enforcing DNS.



STEVE:  Interesting.  I really do like that little router.  It is a kick-butt little router.



LEO:  Thank you, Web9462.  And I think Neo said pfSense will also do that, of course.



STEVE:  Yeah, not surprisingly.



LEO:  If pfSense didn't I'd be shocked.  I actually took my EdgeRouter X out of service because I was noticing periodic very brief dropouts of Internet access.  It was just going away.  And it was really bothering the Roku.  For some reason Roku did not handle it well.



STEVE:  Oh, well, yeah, I mean, you have a streaming device, it doesn't want to lose...



LEO:  Well, there was still enough in the buffer.  Well, I might try this while we're offloading.  That might actually fix the problem.  But taking it out of service and just using a different router fixed it.



STEVE:  Well, and I was going to say that even if you're not saturating your connections, the hardware offloading could substantially reduce latency.  And of course that's the buffer bloat problem, and latency is bad, especially for any kind of real-time streaming stuff.  So by all means...



LEO:  I don't really need it.  So I was just playing with it on your recommendation.  So I'm just going to let the...



STEVE:  So Roku is your choice of streaming...



LEO:  No, it's just one of many.



STEVE:  Oh, that's right.  I did hear you say that on one of the other podcasts.  You're like, oh, I've got them all.



LEO:  I have them all.  We often use Roku.  On my 4K TV I have the Roku Ultra, Chromecast Ultra, and the new Apple TV.  All three do 4K quite well, though the Apple TV is probably the best of the bunch.  But Apple TV doesn't support all the channels Roku does, and vice versa.  So you need to have at least those two.



STEVE:  And Leo, Apple TV's controller is the worst thing.



LEO:  It's awful, isn't it.



STEVE:  Oh, my god.



LEO:  The Roku is nice, and it has a headphone jack if you want kind of private listening.



STEVE:  Isn't that freaky?  It's bizarre that you plug the headphone jack into the remote control.



LEO:  Yeah.



STEVE:  And you get your audio.



LEO:  So I'm very - actually I use the Roku probably more than any of them, yeah.



STEVE:  Yeah.  Apple TV's remote controller is the epitome of the failure of form over function because, oh, my god, is it awful.  I just went back to Amazon, the Fire TV, because I finally got tired of fighting with Apple's controller.  It's like, it's so bad.



LEO:  Yeah.



STEVE:  I got a nice - actually this is a three-note chain.  I don't think I've ever shared one before, but I thought it was kind of fun because it just sort of highlighted a SpinRite customer's interaction with GRC.  I've talked about Sue and Greg from time to time.  They've both been with me for decades now.  A customer named David Stidolph , I hope I - Stidolph?  Stidolph?  Anyway, David, sorry if I mangled your name, Stidolph.



He wrote on the 17th, he said:  "Hello.  My name is David Stidolph.  I have been a LONG [all caps] time user of SpinRite, but I have an immediate need.  I need to recover a laptop drive, and I cannot find my ISO."  He says:  "Any chance I can download it again?"  He says:  "My current email is" - and I redacted it from the show notes, but it's a gmail account.  So then he said:  "My previous email was," and again I took it out.  It was a RoadRunner account which he says is no longer active.  And he says:  "If I cannot download it, please let me know.  Thanks, David."



So Sue got his email, responded:  "Hi David.  Your original receipt with download instructions has just been sent via email.  Your transaction code in your emailed receipt is the key to your download.  It will allow you to obtain replacement copies as well as edit your own contact email should it need to be changed.  Sincerely, GRC Sales Department."



And then he responded in a third mail.  He thanked Sue, and he said:  "Wanted to pass on a SpinRite story.  My first PC was a 4.77" - of course we all know that meant MHz - "clone with a 5MB MFM hard drive."  He says:  "When it powered up, it sounded like a small jet engine."  He said:  " I got a copy of SpinRite, don't recall whether it was 1.0 or 2.0, and ran it on that machine.  Not only did it recover bad sectors, it quieted the drive down quite a bit."  Go figure.  He said:  "Since then I have saved many hard drives over the years.  The software rocks.  Best of luck.  I hope you guys sell lots.  David Stidolph."



LEO:  Nice.



STEVE:  And of course he did receive the ability to download a copy.  That's true of all of our customers.  We have a database.  Sue's able to query the database based on whatever information a customer can provide and can then reenable their ability to download the product.  I have a feeling, once 6.1 starts to become available, Sue's going to be very busy helping people to  regain access so they're able to download their free upgrade to 6.1.



LEO:  Nice.  



STEVE:  I'm not looking forward to that, but our customers are.  So a few closing-the-loop bits.  Oh, so Stefan Korrivo, whose Twitter handle is @NoThumbBowler.  I don't know what that means.



LEO:  He has no thumb.



STEVE:  And he bowls.



LEO:  And he bowls.  So he's a three-finger bowler.



STEVE:  A three-finger - okay.  Do bowling balls have four holes?  I don't know.  Anyway...



LEO:  I believe, yes, they do.  Or three.  Three holes.



STEVE:  So your pinky doesn't go in?



LEO:  They have three holes.



STEVE:  Ah, that's right.  That sort of feels right.



LEO:  No, four.  Three holes.  Your thumb and then your index finger and your middle finger.



STEVE:  Maybe holds the ball with both hands and then rolls it down.



LEO: That's how I do it. 



STEVE:  Yeah.



LEO:  Throw it down [crosstalk].



STEVE:  So he says:  "I'm not looking forward to the day that many domains are using different crypto mining and my browser with multiple tabs is rendered useless since each domain will want 90% of my CPU and degrade my Internet experience."  Well, the good news is I'm sure when that happens noncurrent tabs will not have the miners running still.  So, I mean, this will get managed.  I still, I mean, this could be a passing fad, this idea of permitted mining to replace advertising revenue.  I don't know.  I think it's going to - if it doesn't die before it can be made efficient, I think it might make sense.  We'll see.  But in any event, the browsers will certainly become aware if this becomes a thing, and it will only be the page your eyeballs are viewing at the moment, which has mining running on it.  So when you switch away, that'll be shut down, I'm sure.



Anthony tweeted:  "Bluehost wanted me to provide the last four characters of my password for identification.  Does that mean they have my plain password?"  He says:  "I thought password is hashed in the DB."



Well, that's sort of an interesting hack.  I meant to add a screenshot that Anthony provided, or I guess I went to get it, or maybe both, because it was interesting.  They were in fact asking for the last four characters of his password.  And this could be done securely.  So when he originally provided them with his password, they would have received the whole thing.  They could have hashed the whole password and saved that, and then hashed the last four characters and saved it separately.  So they've got two hashes.  They've got the full password hash, and they have like a little verification hash stored separately.



Access requires the whole password, but identification - and I don't understand quite the use case of that.  I didn't see why it was that they would want identification without full password.  But, I mean, it's sort of legitimate.  Technically it could weaken the whole system because a bad guy could separately attempt to guess just the last four characters.  As we know, there are many fewer possibilities to guess in just four characters.  Maybe that would allow them then to spoof this identification phase, whatever that is.  Maybe that was, no, it wouldn't be password recovery.



Anyway, I don't know why they would want that.  But it could potentially then give an attacker a bit of a leg up because then they could separately verify a piece of the password from the whole and then have those four removed from the whole, although they still wouldn't know what the whole was, then, would then have to guess that.  So an interesting hack.  But it does not mean, to answer Anthony's question, that the whole thing would have to be stored in the clear.  It could have just been set up that way in the beginning, and they saved the last four characters when they were hashing the whole thing.



LEO:  I kind of like that idea actually because it kind of splits the baby.  You get a way of verifying without them knowing the full password.



STEVE:  Yup.  And you know, for what it's worth, SQRL has that built in, too.



LEO:  Oh, really.



STEVE:  Yeah, we call it the Quick Pass.



LEO:  Aren't you smart.



STEVE:  The idea being that the first time you sit down at your computer, you need to enter your whole password in order to decrypt your identity.  But then it immediately reencrypts your identity using just the first four characters.  And so that subsequently you're able to just go bing bing bing bing.  Oh, and you can change four.  You can adjust it up or down to suit your taste.  So as you are reauthenticating yourself with SQRL over the course of the next few hours or the day, like whatever, a session, you're able to just give it the first...



LEO:  Like a PIN.



STEVE:  The beginning of your password.  A PIN, exactly.  And it very quickly says, okay, it's still him.  But if you miss, that is, you can't guess wrong once.  You don't get those first four right, the first thing SQRL does is wipe that hint data out of RAM and then says, oops, you're going to have to re-prove your identity.  But that's just entering your whole password in order to say, oops, sorry, that was just a typo.  But sort of a nice tradeoff of convenience and security, I think, one that we haven't seen before, except these guys are doing it for something, which is kind of cool, too.



Stephen Pickering says:  "Hey, Steve.  I'm in the market for a new Mac, but I hate the idea of buying a new computer that has a patch on its CPU for Meltdown and Spectre, which from my understanding hampers performance.  Is mine a valid reticence?  Thanks, you're the best."  And he also included @leolaporte in the tweet.



So, okay.  So it's probably the case that until a next-generation processor with this updated microcode is shipped from Intel, that all existing processors will be patched.  And the design pipeline is years long.  So it may be two, three, four years before we - or maybe they would be able to update a processor already in manufacturing.  I don't know.  So you could imagine they could retrofit patched firmware for existing, currently manufactured chips so that they start carrying the patch themselves.  But it's not the patch - and this is the key - not the patch that hampers performance.  It's the fact that we've had to give up something we had.  The patch gives software the control that it has not previously had that is necessary for security.  And it's the security change, the security fix which is causing us some performance loss.



So essentially, until recently, and this is why I don't hold Intel responsible, I mean, we've all been - the whole industry has been benefiting from the fact that we were doing something, and "we" meaning everyone, all processors, the entire computing industry, has been doing something that wasn't technically secure and getting a big performance win in trade.  Well, now the bad guys have figured out how to leverage that insecurity, that vulnerability, and so we're having to back off of some performance that was nice to have, but now unfortunately we can't have it anymore.



So future processors will incorporate the patch, and software will use that in order to throttle performance a little bit in the name of security.  But we were sort of getting away with something for a long time that was fine until the academic researchers said, whoopsie.  Actually, it was Google's Project Zero said, uh, Intel, you know, we can read RAM in other processes.  Did you know that?



LEO:  Apple says that benchmarks show almost - or let's see the exact - no measurable performance reduction for their Meltdown mitigations.  They did the Spectre mitigations in Safari.  And on two Safari browser benchmarks, one has no measurable impact; one has less than 2.5%.  That's the JetStream benchmark.



STEVE:  Good.



LEO:  So I don't think you're missing a whole lot on a modern Mac.  And that's the point.  In fact, they aren't even fixing the older Macs.  I think they're just going to let those go because that's where you'd take a big hit.  By the way, it's not just Macs, it's any PC.  I mean, I think you'd have some reticence on any PC, any modern PC; right?



STEVE:  Yes, yes, yes. 



LEO:  But I think Apple's done a good job, and it sounds like at least they don't believe it's much of a hit.



STEVE:  Right.



LEO:  Modern PCs are so fast.  You know, you wouldn't notice.  You wouldn't know the difference.



STEVE:  Oh, I know, Leo.  But we're so spoiled.



LEO:  Make sure you get an SSD.  That makes a bigger difference than anything else.



STEVE:  Yeah, exactly.



LEO:  And lots of RAM.



STEVE:  Yeah.  So a few mining questions, which leads us into our discussion of WebAssembly.  Nick Stoler said:  "Regarding JavaScript mining, wouldn't WebGL allow pretty efficient mining without building it into the browser?"  And I'll respond to that because that's, well, in fact then Michael, who tweets from @Krabby127, said:  "On Security Now! you keep mentioning how browser-based mining are currently terrible because JavaScript is inefficient.  What about WebGL?  That leverages the GPU directly.  Love the show."



Okay.  So both Nick and Michael, WebGL does give browsers access to the graphics processing unit, but not for raw hashing, which is what we want.  The WebGL accelerates, well, WebGL primitives, making them run much faster.  So graphics-y things like blitting areas or fast vector computations or 3D permutation and matrix multiplications, those primitives which high-end graphics processing needs, that's what's accelerated on the GPU.  What mining needs is just raw brute-force hashing.  And so that will be the innovation that I hope we'll see is that there will be some extension of web hash or web crypto or who knows what we'll call it which gives browser code access to GPUs for hashing.  That will be the thing that then gives us really good performance and leverages the GPUs that are sitting around looking for something to do most of the time, relatively speaking.



So finally, two of our listeners, Sebastian, I hope this is Boisvert?  B-O-I-S-V-E-R-T.  Probably maybe the "T" is silent?  I don't know.  Anyway, Sebastien, sorry I mangled your last name.  So Sebastien said:  "Would WebAssembly make web crypto mining more practical?"  Andreas asked, he said:  "Second week in a row you talk about JavaScript being way too inefficient for crypto mining.  What about, for example, WebAssembly?"  And then he gives me the URL, WebAssembly.org.



So I wanted to address this, just to wrap up this week's podcast.  WebAssembly is better than JavaScript, but still worse than native code.  It is a standard.  It is definitely on the way to happening.  The first benchmark is called the MVP, the Minimum Viable...



LEO:  Usually product, sometimes program.



STEVE:  Minimum Viable...



LEO:  Product.  Or Program.



STEVE:  Protocol?  I can't remember what "P" stood for in this case, MVP of WebAssembly.



LEO:  Yeah, Minimum Viable, well, hmm, okay.



STEVE:  Protocol?



LEO:  It's very commonly used in startups, just Minimum Viable Product.



STEVE:  Right.  So what they're doing is they're - okay.  So WebAssembly - and Leo, you'll connect to this because I know you've always been interested in stack machines like Forth is.  WebAssembly is a stack-based virtual machine.  So it is not the - there was the experiment that we talked about a few years ago that Google did where they were using a restricted subset of Intel assembly language, or Intel machine language actually, which they had managed through an amazing amount of work, I mean, just like I was so in awe, where they were sort of able to sandbox native Intel code and arrange to keep it from doing anything dangerous.



Well, that was cool because it was then running on the bare metal.  I mean, it was actually native code.  Well, the problem is it was also Intel chip specific.  And in this day and age, while Intel is the current leader, as we know, ARM is on the rise.  ARM isn't going away.  And so it didn't make sense to standardize on a single manufacturer's native code.  So what WebAssembly is, is it is a binary format,  meaning that it is binary code that is downloaded.  It also has an assembly code-looking text format, and it is executable in web pages.  It's becoming universally supported, is now in the hands of the World Wide Web Consortium with engineers from Mozilla, Microsoft, Google, and Apple.  So it's going to happen.



It is compiled currently from C and C++.  So you start with C and C++ code.  However, it can be targeted at other high-level languages in the future.  So C and C++ compiles to WebAssembly, but it is bytecode, as the term is.  It is interpreted bytecode.  The reason it's interesting is that it is very compact.  It runs fast.  And it's, I mean, the goal is to approach native machine code speed.  It isn't there yet because, again, you need an interpreter.  So there's a layer of interpretation which is emulating a virtual stack-based machine which reads the bytecode and does things with it.  So there's still a layer of translation there.



But for web browsers, the reason it really wins is that it is very small, so it downloads fast.  And it can start running the instant it arrives.  JavaScript is ASCII.  It's script.  It's text.  So we've got all of these JITs, all these just-in-time compilers and JavaScript interpreters.  And to their credit, they're doing an amazing job.  But the code comes down as text and then has to essentially be interpreted/compiled.  And, I mean, what they've done is an amazing amount of work.  But think about it.  Every single time some big Node.js blob is downloaded, or any JavaScript is downloaded anywhere, it is going into a browser, and all of that work is having to be done, redundantly, by every single person that downloads it.



What WebAssembly has done is, I think very cleverly, they've moved that work to the other side of the connection.  They've moved it to the delivery side, or to the server, rather than weighing down the recipient.  So code gets written in a comfortable high-level language like C.  It's compiled into WebAssembly, and it's a binary - it's a precompiled, ready-to-run, binary blob.  Yes, it's not native code.  But the advantage of that is it can run on any processor that has a browser that runs WebAssembly, and they're all going to.



So what we're going to see is I think we will over time see a shift away from JavaScript to WebAssembly because it'll just make sense.  What the user experience will be is snappier applications.  It'll just, the moment the page is there, the code is running.  It's not running at native speed, but it's running faster than JavaScript.  But mostly it's starting up instantly.  So it makes a lot of sense.  So it doesn't - it gets us closer to the hardware.  Still, in order to have chip independence, it's going to be a virtual stack machine, which is now well defined.  It may be moving forward in the future as it evolves, so there will be Version 1, Version 2, and so forth.  But what we really need is we need GPU hashing support in our browsers.  When we get that, either by an extension maybe or native in the browser, that'll give us high-speed crypto mining capability, managed then by the browser.



WebAssembly, nice step forward.  It is happening in the industry, and it's probably a better solution for mining than JavaScript, but not the same as running native code.  So that's the tune-up on WebAssembly.  We haven't really talked about it much.  We talked about it once before, but it's really - it's moving forward.  And it's going to end up becoming a substantial, I mean, I think it's going to largely replace JavaScript in the future because it just makes sense.



LEO:  And it's server-side, not client-side.



STEVE:  Correct.  And so, yeah.  So like right now with JavaScript, everybody who downloads a page with JavaScript has to essentially compile it in order to run it.  It makes much more sense to do that once, like for the web developers to do that once, and then just offer a much smaller precompiled binary which downloads and then just drops in and it's ready to run on the WebAssembly virtual machine, essentially.  Very cool.



LEO:  Hmm.  Interesting idea.  And you can find out more about it.  They have a WebAssembly.org page.



STEVE:  Yup.



LEO:  Where they describe the MVP, Minimal Viable Product is what they've got.



STEVE:  Ah, okay, perfect, thank you.



LEO:  It was the first conceptual version of it.  And that was the Tank game, if you were watching on video, I was playing.  It was written in WebAssembly, and it was pretty snappy, using WebGL.



STEVE:  Oh, very cool.  Neat.



LEO:  Yeah, yeah.  Well, Steve, we've come to the end of this fabulous, gripping edition of Security Now!.  I've been gripped.



STEVE:  Gripped?  Hold onto your whatever you need to grip.



LEO:  I hope you've been gripped, as well.  We do this show every Tuesday, right after MacBreak Weekly, around about 1:30 p.m. Pacific, 4:30 Eastern, 21:30 UTC.  I'd love it if you stop by and say hi.  You could either do that at TWiT.tv/live, that's the live video stream.  You can listen on any of your voice-enabled devices, just ask for TWiT Live.  The syntax depends, and it's changed, apparently.  On Amazon Echo you have to now say "Listen to tune in TWiT Live" for some reason.  But that works.  You can also be in-studio.  Email tickets@twit.tv, we'll make sure there's a chair out for you.



And if you can't watch live or be here live, you can always see the on-demand product.  Steve keeps it at his website, GRC.com.  That's where you can get the 64Kb audio plus the nice transcriptions that Elaine Farris does, so you can read along as you listen, or search.  It's a great way to jump right into the show.  We have audio and video at our site, TWiT.tv/sn, for Security Now!.  And Steve's @SGgrc on Twitter.  I know a lot of people talk to him that way, so that's a good way to reach him, or GRC.com/feedback.



While you're at GRC, please pick up a copy of SpinRite.  You'll get a copy of 6.1 the minute it's done.  You can also read about SQRL, all sorts of stuff, GRC.com.  Everything's free except SpinRite, but that's the one you want to pay for, so there you go.  And Steve will be back next week to discuss the latest in security.  Thank you, Steve.



STEVE:  Yes, sir.  Who knows what's in store for us.



LEO:  What evil lurks in the hearts of hackers.  The Gibson knows.



STEVE:  Catch up next week.  Thanks, buddy.  Bye.  



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#653

DATE:		March 6, 2018

TITLE:		"MemCrashed" DDoS Attacks

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-653.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss some very welcome microcode news from Microsoft; 10 (yes, 10!) new 4G LTE network attacks; the battle over how secure TLS v1.3 will be allowed to be; the incredible Trustico certificate fiasco; the continually falling usage of Adobe Flash; a new and diabolical cryptocurrency-related malware; the best sci-fi news in a LONG time; some feedback from our terrific listeners; and a truly record-smashing (and not in a good way) new family of DDoS attacks.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Great show ahead for you.  We're going to talk about the world's biggest DDoS attack, unbelievable.  TLS 1.3 is coming.  And the little certificate authority that couldn't.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 653, recorded Tuesday, March 6th, 2018:  MemCrashed.



It's time for Security Now!, the show where we cover your security and privacy online with this man right here, Mr. Steven "Tiberius" Gibson of GRC Corporation.  Hello.



STEVE GIBSON:  This is sounding a little bit like the game show host voice.



LEO:  Don Pardo speaking.  So hello, Steve.  I'm excited because we've been talking before the show about something you're going to talk about a little later on.



STEVE:  Oh, yes.  Yes, yes, yes.  In fact, I started by saying I was sorely tempted to name this podcast something, but I won't say, for those who - because it would give it away.  And for the last couple weeks there's been sort of this lull after the whole Spectre and Meltdown specter.  And but not this week.  This one is jam packed with stuff, so much so that for a while I was thinking that I would do a dual-subject, like two main subjects.  Then it was, wait, well, maybe three.  And, okay, finally I said, okay, no, no.  Let's just focus on the biggie because this one is titled Security Now! 653:  MemCrashed DDoS Attacks.



LEO:  Oh, this was wild.  The biggest DDoS in history.  Massive.



STEVE:  Yes.  And in fact the record that was broken four days ago was again broken yesterday.



LEO:  Oh, it's back.



STEVE:  We're now at 1.7 tbps.



LEO:  Is it terabits?  Oh, yeah, I guess it's terabits.  Wouldn't be petabits.



STEVE:  Yeah, terabits.



LEO:  Yeah, that'd be crazy.  We're talking crazy here.



STEVE:  So we're going to talk about this.  It's got, I mean, this one really has the attention of the Internet because this is an ELE-level event, as they call them, an Extinction Level Event for the Internet.  This is, I mean, this is really bad.  And it hasn't even really gotten going yet.



LEO:  Uh-oh.



STEVE:  Anyway, we will be talking about that.  And it uses, I mean, there's never been a service more designed for DDoSing than this.  It's called Memcached, or Memcached - "D" as in daemon.  It's service which many Linux systems run.  There's a MemCached.com where up until a few days ago they were really happy with themselves.  And it's a simple tag-and-value database store that's used for caching.  But unfortunately, like so many of these things, it was never supposed to be exposed to the Internet.  It was only for internal use.  Shodan reports 87,800 and some individual exposed instances of this.  And so far we've only seen maybe five or 6,000 of them in use.  So anyway, we'll be talking about that later, even though I'm having a hard time restraining myself, obviously.



LEO:  It's kind of stunning, I know, I know.



STEVE:  But in the meantime we're going to talk about some very welcome microcode news from Microsoft, which I sort of gave a hint about possibly happening last week.  And the good news is, and it's really good news, we have 10, yes, 10 new 4G LTE network attacks as a consequence of some researchers who took a good close look at the way our cellular network works and found out it kind of barely does.  We have the looming battle over how secure TLS v1.3 will be allowed to be, following up on our original coverage of this some months ago where we talked about that.  And so I'll bring people up to speed on that.



We have the incredible Trustico certificate fiasco of the last week; the continually falling usage of Adobe Flash; a new and diabolical cryptocurrency-related malware; the best, and thus the challenge for the title of the show, sci-fi news in a long time; some feedback from our terrific listeners; and then we're going to get into, roll up our sleeves and get into exactly what it is that has really got the Internet operators worried as a consequence of a so-easy-to-perpetrate, catastrophically large DDoS attack capability.



LEO:  Fascinating.  I had never heard that acronym, the ELE.  That's interesting.



STEVE:  Yes, Extinction Level Event.



LEO:  Extinction Level Event, yikes.



STEVE:  That's like the large asteroid heading towards us.



LEO:  Right.



STEVE:  It's like, oh, well.



LEO:  It's just extinction we're flirting with.



STEVE:  That's an ELE, yup.



LEO:  No big deal.  Nothing to worry about.



STEVE:  So, okay, I've got really good news for everybody.  Really good news.  Microsoft has decided to step up and take responsibility for the Intel processor microcode themselves.  I mentioned last week that a couple...



LEO:  This is instead of Intel?



STEVE:  Well, okay.  So Intel's doing the microcode.  But all of the advice had been necessarily to go get the update, the BIOS update, from your system supplier.  That's not going to be necessary.



LEO:  Hallelujah.



STEVE:  Yes.  I mentioned last week that there were two DLLs in the Windows System32 directory, which were Intel and AMD microcode patch files; that Windows had the capability, much as Linux does, to perform an on-the-fly boot time update of Intel and AMD processor microcode and has in the past.  The question was would they do that here.  And it's difficult to imagine they would not because of the nature of our supply chain being as fragmented and unreliable as it is with BIOSes.  They need to get microcode into their systems.



So they have released the first of a growing family of updates.  It's not being pushed out, at least for the time being, through the regular Windows Update channel, and it's not clear whether it ever will be.  Maybe they just want to have it be on-demand for the time being. If you google, and here's the knowledge base number, you want to google "KB4090007."  That's I think the easiest way to recognize that as 4090007.



LEO:  Warning.  Warning.  Danger.  Danger.  What is that sound?



STEVE:  4090007.  That will take you to Microsoft's announcement of what they currently support.  At this moment they're supporting only a couple of their Skylake processors, probably because that's the strongest, most secure firmware that's known to be rock solid.  They're both sixth-generation Intel core processors.  And only Windows 10 version 1709, meaning the very latest Fall Creators Update version of Windows 10, both on x86 and AMD platforms, so both 32 and 64, but only a small handful.  It's like basically two different processors that have a couple different names.  No word on Windows 7 or 8.1, although I did read some coverage of this suggesting that Windows 7 was going to still be supported within the window that would cause this to be expected to be supported.  So there's hope that there will be updates for Windows 7 which will also be able to do this.



What I'm going to do, because this is tied to hard-to-determine processor specifics, you know, like I have a Skylake I think, but I don't know which one, and there's like a bunch of them, I'm going to give InSpectre, my little tool, a rev in the next couple days, it won't take long, to show the processor ID, the internal processor code which Microsoft uses and Intel uses, but which is not otherwise readily available, as far as I know, anywhere in the user interface.  And that will allow people to see which one they've got and then check this table as it evolves to see whether they've got support.



So what this means is that right now, for people with the supported Skylake processors, who also are on the latest Fall Creators Update of Windows 10, whether you've got Intel or AMD, or I'm sorry, whether you've got 32 or 64.  I keep thinking AMD, it technically is the architecture of the 64.  So Skylake processors from Intel, either 32- or 64-bit versions of Windows 10, you can get this and add it to your Windows 10.  And even if you do not have an updated BIOS from your system manufacturer, this will patch the one most problematical Spectre problem, which is that variant 2 of the Spectre vulnerability.  And then you'll be good to go.



And what we expect is, as Intel is able to mature and confirm other microcode releases, and it goes through whatever vetting Microsoft gives it, then this list will expand, both in terms of supported platforms and also because, for example, they did support the Meltdown under Windows 7 almost immediately.  So they're intending to mitigate these vulnerabilities probably from 7, 8.1, and 10, but then also for other processor architectures as they become available.



And as I said, I'll rev InSpectre just to add a little bit of information about the availability of this for people who show that their system is still vulnerable to the Spectre problem, and then also to reveal the processor ID, the CPU ID, which Intel and Microsoft are referencing in these documents to see whether they qualify yet.  And then maybe, we don't know, maybe Microsoft will end up pushing this out.  But if anyone wants to get it beforehand, I'm sure after everything we've gone through so far this year Microsoft is not making this available until they're sure that they've got it nailed.  So again, you can just google KB4090007, and that'll take you to the knowledge base article that's got links.



LEO:  So I had a caller on Sunday on the radio show.  I felt really bad for her.  She had an Asus gaming machine she had built, and there was a four-year-old Asus motherboard.  And they're probably never going to patch that.



STEVE:  No.



LEO:  This would be in lieu of that; right?  This would...



STEVE:  Correct.



LEO:  Well, it won't be for her because this is obviously not going to be a Skylake machine.  But if they do this for other processors, that would be enough?



STEVE:  Yes, yes, yes.



LEO:  You don't need a BIOS update, in other words.  You just need this microcode update that Microsoft's going to actually provide.



STEVE:  Right.



LEO:  Are they providing Intel's fix?



STEVE:  Yes.  They're getting this from Intel.  So Intel provides it to them.  And so the way you can think about this is that the processor comes with microcode which, when it powers up, it copies microcode in an internal ROM into RAM for lightning speed access.  So there's like an internal microcode RAM store.  So that can be overridden.  Microcode can be patched by something that has the privilege to do so when the system starts up.  That's what BIOSes do when they've got updates to the microcode, so when like Lenovo patched various systems.  Dell had a patch for some laptops.



So the BIOS can contain that patch, and then the BIOS is able to say to the processor, oh, I've got some updates for your microcode.  And so every time it boots, the processor initially puts the microcode in that it has, but then the BIOS is able to say, oh, we've got some fixes.  And so it rewrites a little bit of the microcode in the chip, which then the chip uses for the rest of the duration of it being powered on.



But that doesn't have to only be done by the BIOS.  The OS can do it, too.  And Linux has the ability to do that.  And in fact earlier Intel published a file that Linux users could use to patch themselves immediately.  They then withdrew it when they realized they had problems with the patches.  And I don't know if it's been reissued yet by Intel for Linux.  But Microsoft can do it, too.  And so this is the beginning of that.



So with additional processors going forward, what this essentially means is it will absolutely not be necessary if you have Windows 10 Fall Creators Update.  We don't know for sure how far back they're going to go.  What I've read, because I was trying to get some clear sense for that, and Microsoft isn't saying, but those who seem to know believe ultimately all of the supported OSes will get coverage.  And really there's no reason for them not to.  It's a simple thing to do, if you're doing it for one OS.  It's just, basically, it's a couple DLLs.  The patch, they're less than a meg.  They're 773K or something because it's not a lot of - it's micro, after all.



So anyway, that's great news.  And so for this caller who you spoke to, for example, if she's got an OS that Microsoft is supporting, then either by going and manually getting it, maybe automatically having it pushed, it's not clear yet, but at least right now they are making it available for some users of Skylake.



LEO:  That's good.  That'll help a lot of people.  Probably not her, but it'll help a lot of people, yeah.



STEVE:  There was, about a year and a half ago, a group of researchers, two guys at the State University of New York in Binghamton and another at UC Riverside, developed a side-channel attack on branch target buffer collisions.  It's closely related to everything we've been talking about with this Spectre problem, that is, that this branch target buffer is a cache that saves on computation and speed.  So that was a year and a half ago.  I was reminded of it.



And the thing that's significant to me, and I knew when I saw this graph, I thought, oh, Leo's going to love this one.  On the next page of the show notes, Leo, is a chart showing the timing of using this speculative execution branch prediction when they get a hit versus when they get misses.  And it's just - it's beautiful because what they're doing with this attack 18 months ago is they're essentially managing to remove address space layout randomization.



So you've got that on the screen now.  You can see along the bottom they are trying different groups of high-order bits for where something is located.  And you can see the one that they score on is - so there's sort of a statistical distribution.  It's noisy all over.  But it ranges from, like, about 45 cycles to maybe 65.  But when they hit it, it's suddenly a little over 90.  And just one, just sitting out there all by itself, which says, okay, bang, we had the collision that we were looking for.



Anyway, there's really no other news here, but I just love this graphic that was in their PDF because it demonstrates the sort of thing that we see when we get one of these statistical timing-based results is the amount of time that something takes changes dramatically.  And in this case, with this research, they were able to, in 60 milliseconds, completely strip all address space layout randomization, either from another user process or from the kernel.  So these sorts of attacks have been around for a while.  And as we know, well, of course it's been -  most of the news of the year so far has been about this.  But I just really liked that chart because we hadn't seen something as clear in the previous coverage of that.



So three researchers from Purdue University and one at the University of Iowa took a good hard long look at our current 4G LTE network protocol.  They developed a tool they call the LTEInspector to essentially more closely examine the protocol we're all using.  All of our cellular 4G LTE network is based on this.  In the abstract for their paper they wrote:  "In this paper we investigate the security and privacy of the three critical procedures of the 4G LTE protocol (attach, detach, and paging)..." which are internal names for specific actions in the protocol.



They said:  "...and in the process uncover potential design flaws of the protocol and unsafe practices employed by the stakeholders.  For exposing vulnerabilities, we propose a model-based testing approach."  And then they introduce "LTEInspector, which combines a symbolic model checker and a cryptographic protocol verifier [in what they call] the symbolic attacker model."  So basically, they built, they modeled the protocol in this LTEInspector and then ran it through its paces looking for problems.



And they say:  "Using LTEInspector, we have uncovered 10 new attacks, along with nine prior attacks, categorized into three abstract classes - security, user privacy, and disruption of service - in the three procedures" - that is, these three, attach, detach, and paging - "of the 4G LTE protocol.  Notable among our findings is the authentication relay attack that enables an adversary to spoof the location of a legitimate user to the core network without possessing appropriate credentials.  To ensure that the exposed attacks pose real threats and are indeed realizable in practice, we have validated eight of the 10 new attacks and their accompanying adversarial assumptions through experimentation in a real test bed."



So anyway, what we have here is the kind of vetting that it's easy to wish afterwards that this had happened beforehand.  But these protocols evolved sort of organically.  Things get tacked on.  Somebody, a committee says, oh, let's add this feature or that feature.  And at first blush it looks fine.  Unfortunately, it turns out when you really, really look closely, they've become so complex that it is difficult to make any true security assertions, and it takes careful modeling of the protocol and then a deliberate attack on that model in order to find problems.  So they stopped short, they deliberately stopped short of publishing any proof-of-concept code because they don't want to make it that easy.  They argue that it's difficult to see how this can be fixed through further patching.



I loved their language somewhere.  They said, oh, they said:  "We deliberately do not discuss defenses for the observed attacks as retrospectively adding security to an existing protocol without breaking backward compatibility often yields Band-Aid-like solutions which do not hold up under scrutiny," meaning it may be difficult, if not impossible, to fix this.



So what we have is a better sense than we had before, you know, we've talked about problems with our cellular communications, the whole problem with the signaling system which is old now and never had authentication built into it.  Well, we've been trying to move the system forward and essentially go back and shore it up a little bit with, as this demonstrates, some mixed results.  So what this probably means is that we need to more carefully recognize that the security of the transfer protocol, the security of the transit protocol cannot be relied on.



And so things like Signal and Telegram and Messenger, encrypted protocols that run on top of this questionable transport protocol is what we need to rely on going forward, which would argue for having stronger encryption and security for the protocols that we use on top of this 4G LTE and future cellular protocols.  They also note, for example, that faking things like emergency alerts of the kind which caused the hysteria in Hawaii earlier this year, although of course we know that one was a human error, some guy pushed the button next to the one he meant to push.  But still...



LEO:  No.  They found out he was...



STEVE:  Oh.



LEO:  He just - he misunderstood the...



STEVE:  He deliberately pushed the wrong one?



LEO:  He received an alert that said, "This is a drill.  This is a drill."  And he wasn't paying attention, and he pushed the alert button.



STEVE:  Oh, oh, oh.  So he just didn't... 



LEO:  He ignored...



STEVE:  ...hear the "this is a drill" part.



LEO:  He didn't understand the "this is a drill" part.  He no longer works there.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  And you always wonder, you know, in the movies where they say "This is not a drill." 



LEO:  Right.



STEVE:  Well, okay, well...



LEO:  Sure sounds like a drill.



STEVE:  If it is a drill, and they don't say that, then you know it's a drill; right?  So that never made any sense to me.  It's like, here's an alert, and we're serious this time.  Until now, eh, we haven't been that serious.  But this time really...



LEO:  This time we mean it.



STEVE:  Please pay attention.  This is not a drill.



LEO:  Please, it's not a drill.



STEVE:  On the other hand, if it was a drill, and you wanted to actually drill people, that's what you'd say, too.  So I don't know, it just never seemed to make much sense to me.



LEO:  Yeah, yeah.



STEVE:  So, okay, speaking of what's not making much sense.  Well, although I guess I can understand this, we've seen over and over and over how difficult it is to move security forward, and especially to retire things that have long since had better solutions.  We talked some months ago about the next version of TLS, the Transport Layer Security.  TLS is the new name for what we used to call SSL - SSL 1 and 2 and 3 and lots of sub versions.  And now, you know, 3.0 of SSL was sort of TLS v1.0.  There was sort of a - it was a transition that we made.  Well, 1.2 was ratified in 2008, so eight years ago.  And we're just now to the point where we're beginning to get ready to say, okay, we're absolutely going to depend upon that.



But, as is always the case, we're wanting to create, we're wanting to have the next version ready so that support can begin to get created for it.  And that will be v1.3.  The 1.3 is many good things, and we'll go over it in detail.  I'm sure that there will be a podcast whose title is TLS v1.3 when it happens, when it finally gets ratified, because it hasn't yet.  And so they're still arguing over the details and over the features.  Overall, it's really a good thing.  I'm excited about it because it follows the KISS principle, you know, keep it simple.  And also a protocol should be as simple as possible, but not simpler.



So what 1.3 does is deliberately abandon a bunch of old junk which has been hung on and tagging along and supported, just because it once seemed like a good idea.  We long since learned otherwise, but it's still kind of there.  So it's like, okay, no.  No more of this old baggage.  This is the baggage removal version of TLS, which is all for the good.  But the big controversy, and this is what we discussed before, is that it provides, for the first time, and it's a little surprising to people that we didn't already have this, but it provides for the first time what's known as "forward secrecy," sometimes "perfect forward secrecy," PFS.



What that means is that conversations which are encrypted, as they are under TLS, cannot be later decrypted if in the future something is discovered.  Believe it or not, we don't have that today.  Even with TLS v1.2, as we've discussed in the past, part of the secret of the key which is negotiated on the fly is the server's private key.  That's felt to be safe because it's private, and servers have a huge stake in keeping it private.



So what happens is, right now, up until 1.3, if the NSA, just to pick on somebody - and of course they do have a massive datacenter in Utah with its own power and cooling lakes and massive, god knows how much storage.  If all of the traffic everywhere were just siphoned into this monster facility and stored, even though they can't see it because it's encrypted, right, if the handshake of TLS up to v1.2 is recorded, and if at any time in the future, even after the server's private key expires, if at any time in the future the private key of the server which was in use at the time is disclosed, is found, is discovered - or maybe quantumness happens in the future and so you can get it somehow.  Who knows?  It's possible to take the private key and the handshake observed and obtain the session key, which is what is used to encrypt the conversation.  So that's no forward secrecy.



TLS v1.3 is bringing us forward secrecy, which is a huge improvement.  I mean, that's good news.  The problem is, and this is what we discussed about this before, it's facing some backlash because there have been some ways that datacenters have used the lack of that forward secrecy, for example, to terminate TLS version up to 1.2 connections at their border so that inside the datacenter the traffic is not encrypted.  That is, so they've got something on their edge which clients connect to as a proxy, essentially, a proxy for the servers.  And then inside the datacenter, everything's in the clear.  So that's been a convenience for network guys who are then able to monitor traffic, do intrusion detection easily, see what's going on.  They have complete visibility into the traffic.  Essentially, in their datacenter, nothing is encrypted, which makes people nervous, but makes the network guys happy because they're able just to see everything, which is convenient.



Turns out that coming late to this is what's known as BITS, B-I-T-S, which is the technology policy decision of the Financial Services Roundtable which is the industry group for financial services in the U.S.  All the banks and other high-end financial groups have this roundtable group that they're members of.  And these guys are, like, suddenly woke up to the fact that, oh, my god, this might happen.  And we don't want to spend any money on our datacenters essentially is what this comes down to.  They're fighting it just because, like, they'll have to do something.  They'll have to invest in more security, essentially, in order to work around what this means.



So there's been some - and this is where it's really coming down to some push for a mode for TLS v1.3 that would not be forward secret.  And the people who want forward secrecy, like everybody else, the people who care about secrecy and security, are saying no.  If we make this be a bit you can flip, or if you can have a downgrade to pre-1.3 forward secrecy, then that's what people are going to do, or use.



And so, for example, what that would mean is that the banking industry, where you might argue it would be important to have forward secrecy, in order to save themselves the trouble of changing anything in support of TLS v1.3, they will refuse that mode during negotiation, which means all of the public traffic with those organizations will lack forward secrecy, which arguably is probably where you need a lot of it.



So anyway, this battle is looming.  I just sort of wanted to touch on this because the final ratification meeting with the IETF is supposed to occur later this month.  The purists are absolutely refusing any compromise, saying as I said, that if there's a way to not do it, those who don't want to invest in what it takes on their side will set that bit.  I mean, and it's not like right now anyone has to use it.  I mean, it's not even born yet.  And so we'll be with 1.2 for a long time.



At some point, for example, 1.0 and 1.1 is now old enough, since 1.2 has been around for eight years, that they're on the way toward being deprecated.  I received a note, for example, from my back end that I do ecommerce with, the financial clearinghouse that my own ecommerce system interacts with, just a day or two ago, an alert that this summer they are going to stop supporting TLS v1.0 and 1.1.  And so make sure that your infrastructure supports 1.2.  Mine does.



And so this is an example of what's happening.  Someday 1.2 will be phased out, once 1.3 has been well enough established.  Probably not for eight years.  So again, if it follows the same time course, as it might - because of course 1.2 is very good.  We fixed a lot of the problems in 1.0 and 1.1, so we're pretty happy at 1.2 for now.  But ultimately we'll be going to 1.3.  What these guys want is for 1.3 to be useful, to be the right thing to move to, and not to have pre-crippled it because some people who were saying, oh, we're going to have to buy some more wires, are being too cheap to do that.  So I'm expecting this is going to happen without the weakening provision.



Interestingly, Matthew Green was one of the people who was trying to find a compromise and was rebuffed by the committee, saying, you know, thanks, but we're really going to try to stick with this and get this thing to pass through the IETF as an all-or-nothing, and in this case as an "all" for encryption so that it isn't weakened ahead of time.



LEO:  Okay.



STEVE:  So...



LEO:  You know, a lot of times you heave that sigh on the show.  And I know, I know something's coming.  Something not good.



STEVE:  Yeah, yeah.  So there's a U.K. - or maybe was, I'm not sure if this company's going to survive this - a U.K.-based reseller of Symantec certificates, so sort of purporting to be a certificate authority.  They didn't have their own CA infrastructure because of course that's a big deal.  To run a CA you've got to have a really good network.  You have to really be serious about security.  There's all kinds of ways you can screw up, so you have to not do that.  But you need to have, for example, run certificate revocation services and have those up and available and reliable.  And, I mean, there's a lot to it.



So it's easier for a company to say, as Trustico did some time ago to Symantec, "Hey, we'd like to be a reseller of your certificates."  So you go to Trustico, and you say, "Hey there, folks, I'm in the U.K., too, and so are you, so I want to get a certificate from you."  And they say, "We'd be happy to sell you a certificate."  What they're actually doing is selling them a Symantec certificate with Trustico sort of as the frontend.  So it's much easier to resell than it is to actually be a CA.



So the problem, of course, is that as we know, Symantec screwed up, actually with some of their partners, kind of like Trustico.  And so Symantec's certificates are going to be declined in the future because we can't really be sure what's what with them.  As a consequence, that impacted Symantec and, as we've covered on the podcast, Symantec looked around for somebody reliable and trusted that they could sell their certificate customer base to, essentially.  And that turned out to be my favorite certificate company, DigiCert.



So maybe as part of this, in some sort of an awkward way, Trustico decided - and some of this is not clear.  I read a bunch of coverage about this, trying to really put the timeline together.  But Trustico decided they didn't want to stay with Symantec/DigiCert.  They wanted to go to another shining gem, Comodo.  So they sent a letter to DigiCert, Trustico did, saying we want you to revoke 50,000 certificates.



LEO:  And we know how well certificate revocation works.



STEVE:  Ah, yes, thank you for that, Leo.  Exactly.  We've  spent - I dragged our listeners through my fury over certificate revocation some years ago, which of course is completely broken, especially for Chrome, which doesn't really even try.  So DigiCert says, what?  First of all, we're not even sure you as not even quite a CA yourself can ask us to revoke certificates on behalf of your customers.  They're your customers.  It's up to them if there's a problem with the certificate to say, hey, whoops, we think we lost control of our private key.  We need you to revoke the certificate, please.  So DigiCert says, what?  No.  Like the only possible way we could revoke certificates is if there was a clear breach of their private key.  Whereupon some jamoke at Trustico zips and emails DigiCert some 23,000 private keys.



LEO:  No.  No.  No.



STEVE:  It's unbelievable.  So wait a minute.  Wait a minute.  So not only did they zip them up and email them...



LEO:  I hope they put a password on the zip.



STEVE:  No.  They were non-password-protected private keys, as a matter of fact.



LEO:  Oh, my god.



STEVE:  Not only that...



LEO:  That's one way to get them revoked.



STEVE:  But they shouldn't have ever had them.  A CA should never have your private key.



LEO:  Your private key, yeah.



STEVE:  Yes.  The private key.  For example, when I have DigiCert sign a key, they're signing my public key.  So the server, a GRC server makes a public key pair, consisting of a private key which never leaves its control and the public key.  The public key is what is going to be sent out with every TLS connection.  That's the thing that you want the CA to sign so that when the client, the browser receives the public key, it looks at the signature and goes, oh, this is signed by DigiCert.  I like them.  And therefore I'm going to trust the public key.  The point is DigiCert never gets my private key.  They don't want it.  I mean, nobody wants the responsibility.  Somehow Trustico had the private keys of its customers.



LEO:  Just in case.  It was a convenience thing.



STEVE:  Well, get a load of this.  It turns out that, for convenience, they had a web page where, if their customers were for whatever reason unable to generate a key pair themselves, Trustico, handy-dandy Trustico would do it for them.  And yes, Leo, did it in JavaScript, with JavaScript from five or six different sources, including ads on the same page.  So nothing has ever been less secure.  I mean, you can't make this up.  This is just, as I said...



LEO:  Actually your Picture of the Week might have something to do with it.  Perhaps there is a less secure way to store private keys.  What do you think?



STEVE:  Yes.  The Picture of the Week is just so perfect.  This was somebody's response to this debacle which is just wonderful.  It shows like some cheesy liquor cabinet.  I mean, there's a screw top bottle of red wine on the left and some Band-Aids, just in case you've had too much to drink and you slip and cut  yourself.  And some dry gin and a half-consumed flask of whiskey, I mean, okay.  That's on the lower shelf.  On the upper shelf there's this beaten-up box that says #Trustico, and then in big capital letters, "PRIVATE KEYS.  PLEASE DO NOT OPEN."  Yes, we've stored all of our customers' private keys in this box whose lid is sort of damaged, too.  I mean, oh, this is just amazing.



So of course - so no one can believe this.  This is just incredible that they would, first of all, have private keys.  Then, when challenged by DigiCert correctly quoting the terms of the CAB, the Certificate Authority group, the CAB Forum, saying, what do you mean, revoke 50,000 certificates?  And remember, these were not certificates that DigiCert ever issued.  Symantec issued them, and they sort of inherited them as a consequence of the Symantec acquisition.  So it's like, what?  So they say, here, here's a bunch of private keys.  They're disclosed now, sucker.  So it's like, oh.  Now, DigiCert isn't quite sure what to do about the other 27,000 that have not yet been received in email, so there's that problem.  So as part of this, DigiCert, doing the right thing, posted to a common forum that they all use about what was going on.  This is the Mozilla security mailing list.



Scott Helme, who is an information security consultant and an expert in the CA domain, was quoted in some of the coverage saying:  "To arrive at the conclusion that Trustico have been anything other than grossly negligent here is rather difficult.  Generation, storage, and the apparent ease and willingness to further compromise the keys are all outrageously inappropriate.  They could have trivially proven ownership of those keys without the need to zip 24K-plus of them and send them via email.  If these actions were motivated by business/politics, as some suggest, it'd be ironic if their actions resulted in their removal as a reseller."



And indeed it's difficult to imagine how even Comodo could say, yeah, at this point we'd love to have your reseller business.  I mean, yikes.  DigiCert reported the revocation incident on the Mozilla security mailing list, which we've quoted from from time to time.  It's often used to discuss affairs of the CA Browser Forum.  And not surprisingly, members of the mailing list had concerns regarding Trustico's access to the private keys.  Eric Mill, who's the senior advisor at the U.S. General Services Administration's Technology Transformation Service, posted:  "Trustico doesn't seem to provide any hosting or CDN services that would make use of the private key, nor do they appear to explicitly inform users about the storage of this private key, thus suggesting that there was no apparent reason for Trustico to keep copies of the private keys."



Eric also wrote:  "The storage of private keys appears to be done without the user's knowledge or consent.  Given everything that's known, then regardless of who emailed whose customers when and why, I think it's clear that Trustico compromised those keys and has been routinely compromising customer keys for years.  Given that there's no evidence that Trustico indicated any intent to change their business practices, then I believe it's appropriate for all CAs to immediately suspend or terminate their relationship with Trustico," Eric added.



So anyway, DigiCert, of course, was in the middle of this because they were the inheritor, essentially, of the certificates which were originally issued by Symantec via Trustico.  So they said Trustico - this is DigiCert said:  "Today DigiCert issued the following statement regarding Trustico certificate revocation.  Trustico requested revocation of their Symantec, GeoTrust, Thawte, and RapidSSL certificates" - okay, so I forgot to cover that.  They were getting more of them than just from Symantec - "claiming the certificates were compromised.  When we asked for proof of the 'compromise,' Trustico did not provide details on why they were requesting the immediate revocation."  And remember, this is 50,000.



"Trustico's CEO indicated that Trustico held the private keys for those certificates, and then emailed us approximately 20,000 certificate private keys.  When he sent us those keys, his action gave us no choice but to act in accordance with the CA Browser Forum Baseline Requirements, which mandate that we revoke a compromised certificate within 24 hours.  As a CA, we had no choice but to follow the Baseline Requirements.  Following our standard revocation process, we gave notice via email to each certificate holder whose private keys had been exposed to us by Trustico, so they could have time to get a replacement certificate."  I imagine there's some scrambling going on at the moment.



DigiCert's announcement continues:  "In communications today, Trustico has suggested that this revocation is due to the upcoming Google Chrome distrust of Symantec roots.  That is incorrect," DigiCert writes.  "We want to make it clear that the certificates needed to be revoked because Trustico sent us the private keys.  This has nothing to do with future potential distrust dates."  And then they conclude:  "The upcoming Chrome distrust situation is entirely separate.  We are working closely to help customers with certificates affected by the browser distrust, and we are offering free replacement certificates through their existing customer portals.  That process is well underway."



So, wow.  As I said, yes, Leo, a heavy sigh at the beginning of this one.  This is just, you know, bizarre.



LEO:  Really.



STEVE:  And I don't know anything about Trustico.



LEO:  What's the vetting process of setting up a CA?  I mean, presumably McAfee had the responsibility to vet these guys, or Symantec.  But doesn't ICANN put something in place?  I mean, these guys were clowns, obviously.



STEVE:  They were clowns.  And, I mean, to be minting public key pairs...



LEO:  In JavaScript on a public page.



STEVE:  ...in JavaScript on a public page with JavaScript hosted from five or six other domains including ads, is unconscionable.  I mean, it just - it boggles the mind.  Yet they were doing it until, I mean, the thought I had, and this is following up on your question, is you would imagine that there would be some obligation that Symantec would have had to vet the ongoing conduct of their reseller.  But on the other hand, it is due to the conduct of a different reseller that Symantec lost their whole CA business in the first place.



LEO:  Yeah, they obviously don't care.



STEVE:  So let this be a lesson to other CAs who are essentially extending their trust into third parties by allowing those third parties to resell their certificates.  This is what that looks like when it goes sideways, first with Symantec and then with this other reseller.  Again, this makes Symantec look even worse than they already did because this was the conduct of another one of their resellers.  Wow.  So with any luck, Trustico is gone because this will get enough coverage, and enough people will realize, you know, I don't care how cheap these certs are, I've got to run around in circles and worry about being compromised.  And now I've got to come up with replacement certificates within 24 hours because - not that revocation is going to have any effect.



Oh, I forgot to mention that DigiCert will be providing the keys to the members of the browser community so that, for example, browsers like Chrome, if it chooses to, and/or Firefox and others, could blacklist those certs.  And there's been some discussion of proactively blacklisting them, which is a pain in the butt because this is no small number...



LEO:  It's done by hand; right?



STEVE:  Yeah.  Yeah, it's no small number of certs to have to deal with.  What a mess.  Incredible mess.



A little bit of good news.  Flash usage, Adobe Flash usage has declined from 80% in 2014 to under 8% today.  There was a keynote speech in San Diego last week given by Google's Director of Engineering at the Network and Distributed System Security Symposium.  Her keynote, among other things, showed a slide that I've got here on the next page of the show notes in case you want to put it up on the screen, Leo, showing Flash usage decline from around 80% in mid-2014 down to about 8% today.  And it's - I'd have to call it "precipitous."  Unfortunately, it looks a little bit like Bitcoin valuation, too.  So anyway.



What they did was they looked at the percentage of daily Chrome users who've loaded at least one page containing Flash content per day.  So that's the metric.  Full-time Chrome users probably who encounter Flash at least once per day has dropped from around 80% four years ago to 8% today.  And of course, as we've previously covered, Adobe has announced they're formally planning to give up on Flash.  It's funny, I've seen by the end of 2020.  But I also, yeah, by December 2020 is when they're saying, okay, no more.  So only a few more years, and it's really, really gone.



On the other hand, its usage has gone from, like in the various browsers, Chrome, Firefox, and Edge, from what you might call "enabled" or "enabled and pray" to "click if you dare."  So it is now - it no longer runs by default.  You have to say, oh, yes, I want to do this whatever it is that Flash is asking.  So, I mean, we are seeing updates.  I was watching a system update just yesterday, I think it was a Win10 machine, and Adobe Flash updates, like oh, okay, well, fine.  At least it's not going to just launch. It's going to say, hey, wait, whoops, hold on, do you want to do this?  So that certainly mitigates a lot of the advertising-based and drive-by Adobe Flash attacks that we've seen in the past.



And of course HTML 5 can now play video.  And most advertising networks and video streaming portals have already moved away from Flash plugins to pure browser-based HTML5.  So it gives an improved user experience.  So anyway, someday it will just be a memory, and a long lesson in what can go wrong with the security of a plugin, back from when it was necessary.



Okay.  And this crazy, but I'm sure we ran across a story like this in the past.  There's something that Palo Alto Networks, a piece of malware that they've named ComboJack.  They named it ComboJack because it hijacks the system clipboard, watching for a valid cryptocurrency address to be pasted into the clipboard, and then replaces that address with one of its own on the fly.  So that's the "jack" part, the hijack part.  The "combo" part is that, believe it or not, it can differentiate and detect the individual address formats for Bitcoin, Litecoin, Ethereum, Monero, Qiwi, Yandex Money, and two versions of WebMoney, both denominated in U.S. dollars and in rubles.



So if you're infected by this thing, and you are wanting to send somebody money; right?  They email you their cryptocurrency address, or there's a "donate to" cryptocurrency address on a web page, and you're feeling beneficent so you say, okay, fine.  So of course you're not going to transcribe it by hand, so you mark it with your cursor.  You highlight it, and you hit Ctrl-C to copy it, or whatever you do, right-click and get a context menu and say "copy."  Well, that puts it on the clipboard.



This thing, ComboJack, is watching.  It sees the clipboard suddenly containing a cryptocurrency address which it recognizes.  It parses it and recognizes it because it turns out they're all a little bit different, enough different that they can be disambiguated.  And it has its own accounts in all of those cryptocurrencies.  So it immediately replaces it with its own.  Now you go to your whatever it is, wallet or wherever you go to send some of your cryptocurrency to that cryptocurrency, never really paying attention to the fact that - because, I mean, everyone glosses over those crypto-looking things.  You're not memorizing this thing that you just copied off the web page.  So then you paste it into your cryptocurrency payment app and say, here, send them some money.



Well, apparently this scheme is working.  It isn't easy to get it into your system, or at least it's a long and involved exploit chain.  But it's an exploit chain that we've actually seen before.  You receive an email containing a PDF.  You open the PDF.  Oh, and this particular PDF says it's the scan of a lost passport.  So that seems like a...



LEO:  Oh, I've been looking for that.



STEVE:  Yeah.



LEO:  Send me that email, yeah.



STEVE:  Seems like a bit of a stretch.  It's like, wait a minute, I didn't lose a passport.  I'm just going to delete the email.  But it's more like the scan of the traffic ticket that you received last week.



LEO:  That's more recently, yeah, yeah.



STEVE:  The e-ticket or something.  That seems more likely to get somebody.



LEO:  Your tickets are here.  Your flight to Paris...



STEVE:  Right, that's right.



LEO:  Yeah, there you go.



STEVE:  Yeah.



LEO:  I'm opening that one.



STEVE:  So you open the PDF.  It opens an RTF file that it contains, a Rich Text Format file, which in turn contains an embedded HTA object which attempts to exploit a known DirectX vulnerability which then executes a series of power shell commands to download and execute a password-protected, self-extracting SFX file which then installs ComboJack.  



LEO:  Holy cow.



STEVE:  I know.  I mean, that's - unfortunately, I mean, this is  how circuitous this route is to get this thing installed in your machine, but it's happening.  It's been found on people's machines.  They're thinking, what lost passport?  And then before they know it they just can't help themselves.  They need to take a look at it to see whose lost passport it is.  And then, bang.  If they attempt to send cryptocurrency to anybody, it ends up going to the bad guys.  So seems like a hard way to make money.  But cryptocurrency is the thing now.



Okay.  Two more things, then we'll do our last break, and then talk about MemCrashed, as it's being called.  So Leo, I was 10 years old in 1965.



LEO:  Ten years old.



STEVE:  Having been born in 1955.



LEO:  Just a kid.



STEVE:  And I guess you were, what, eight, probably, I think.



LEO:  '56, so.



STEVE:  So what happened is that the phrase "Danger Will Robinson" entered...



[Clip] Danger, danger.



STEVE:  I mean, it's incredible when you think about it.  I mean, "Danger Will Robinson," that's just a thing now.  I mean, it's a meme.  And the other thing I realized was "Does not compute."  I don't think that "Lost in Space" was the first time we saw the robot...



[Clip] Does not compute.



STEVE:  ...waving its rubber, its accordion arms around.  But certainly "Does not compute" became another meme.



LEO:  Is that where that started?  That's amazing.  I bet you're right.



STEVE:  I think so, yeah.  And so...



[Clip] I was programmed for more important work.



STEVE:  Probably asked him for the recipe for a souffl or something.



LEO:  Yes, probably.



[Clip] Warning, warning, warning.



STEVE:  Oh, lord, yes.



LEO:  My childhood's coming right back to me.



STEVE:  So I'm mentioning this because on Friday April 13th, Friday the 13th, April, next month, Netflix releases a 10-episode, what looks like a fantastic reboot of "Lost in Space."  There was, what, maybe - how long ago was it, the feature-length movie?



LEO:  Yeah, that's right.  Was it...



STEVE:  It wasn't that good.  I mean, I watched it, of course.  But it was, eh, it was okay, but not great.  This looks wonderful.  There's a three-minute trailer for Netflix's forthcoming "Lost in Space" series.  I will urge any sci-fi lovers in the audience to go find it.  I've got the link here in the show notes.  It's just shy of three minutes long.  You will know that you're going to want to be, I mean, I will be metering those 10 episodes out over the weekend.  Luckily...



[Clip] Silence, you mental midget.



STEVE:  Luckily, Lorrie loves this kind of stuff as much as I do, so we will be sitting side by side, glued to the screen, probably in two five-hour bursts, I expect.



LEO:  Chunks, big chunks.



STEVE:  Friday night and Saturday night.



[Clip] Power on.



[Clip] It's working great.



LEO:  I have a bunch of those.



STEVE:  Oh, and Leo, I remember at 10 years old being terrified.  My sister, who is also two years younger, we sort of had our heads buried in our pillows, peering out over them, as like the salad monster...



LEO:  Monsters, yes.



STEVE:  Exactly.  The salad monster comes out from behind a rock and like, oh, my god.  Anyway, there are no salad monsters...



[Clip] My micromechanism thanks you.  My computer tapes thank you.  And I thank you.



LEO:  It is not - is it a comedy?  Because that was kind of a comedy.  This looks pretty serious.



STEVE:  No, no, no.  No, no, no.



LEO:  Good-looking robot.  Man, that's a nice robot.



STEVE:  Oh, it's a beautiful robot.  Maybe you should - it's only three minutes.  Let's go ahead.  Even though our listeners, those who are only listening will only hear the audio, you'll get a sense for it.



LEO:  It's dramatic.



STEVE:  Let's run it into the podcast.



[Begin "Lost in Space" trailer]



LEO:  So this is the ship.  The Robinson family is on the ship, and they are now stranded on a planet, and there's the little Will Robinson, exploring the planet for the first time.



STEVE:  But, I mean, beautiful special effects.



LEO:  Yeah, really, it looks [crosstalk].



STEVE:  This is world-class sci-fi.



LEO:  There's Penny.



[Trailer plays]



LEO:  Oh, so the robot isn't their robot.  It's an alien robot.



STEVE:  I don't know.



LEO:  We don't know.  We'll find out, won't we.



[Clip] Danger Will Robinson.



LEO:  Oh, yeah, baby.  Oh, my goodness.  Oh, there goes their ship.



[Trailer plays]



LEO:  So the premise of this originally was it was kind of the Swiss Family Robinsons.



STEVE:  Yes.



LEO:  But it was sci-fi.  They were on a planet.  So where's Dr. Smith?



STEVE:  So Dr. Smith is...



LEO:  So we got Mom, we got Dad, we got Penny, we got Will.  Oh, is that Dr. Smith?  There's another guy.



STEVE:  There was like a younger male.



LEO:  Oh, there's Penny.  There's what's-her-name.  She's the new Dr. Smith?



STEVE:  Yes.  We have a female Dr. Smith.



LEO:  Oh, and I like her.



STEVE:  Yeah, whose agenda we're not quite sure about.



LEO:  Oh.



STEVE:  Just as the old Dr. Smith.  We really didn't know what his deal was.



LEO:  Yeah, we're not sure.  He might have been the first gay guy on TV, actually.



STEVE:  And I think he was - he wanted to sabotage the mission, but he got trapped onboard.



LEO:  He was a bad guy, yeah.



STEVE:  Yeah.  He was a bad guy, but I think he got trapped onboard and wasn't at all happy.  And he was also really annoying.



LEO:  He was kind of prissy and [vocalizing].  But he was kind of the comedy relief; right?



STEVE:  Well, he was always screwing things up.  He was a fly in the ointment.



LEO:  Yeah.



[End trailer]



STEVE:  And it was like, just kill him already.  But no, they never did.



[Clip] Never fear, Smith is here.



LEO:  Oh ho.



STEVE:  Oh, god.  Anyway, so a few weeks from now, oh, boy, it looks like we have 10 hours of wonderful fun.



LEO:  That's going to be a lot of fun, yeah.  The movie was not good, but this might - but they know there's a burden because there's a lot of people our age who have fond memories.



STEVE:  Oh, flush those.  I don't care about those.  This looks wonderful.



LEO:  "Lost in Space," the reboot.



STEVE:  I just want it to be good so we get more, in the same way that we get more "Stranger Things" and we get more...



LEO:  We were disappointed by "Altered Carbon," so this is a chance for Netflix to make good on that.



STEVE:  Yeah, this looks like a healthy family sci-fi drama.  So Rick James in Toronto, Ontario, Canada send me a note mostly, he said - he said:  "Security Now! ASA vulnerability follow-up."  Remember that the ASA/VPN, that was the Cisco vulnerability that was initially not - it didn't look like it affected as many people as it looked like it was going to, but later it turns out that many more of the subsystems had this problem.



So anyway, Rick wrote:  "As I'm sure you and your listeners are aware, Cisco requires its customers to have a support contract to get ASA/VPN software updates.  I wonder if this is a circumstance where Cisco NEEDS [all caps] to release a patch like Microsoft did last year to Windows XP," meaning that it's so bad that you should be forced to have a maintenance upgrade.  And I think Rick is exactly right.  This makes sense.  This is clearly a bad vulnerability.  He says:  "Are software/hardware vendors only responsible to fix their security issues if you're a current customer?  I thought this might be an interesting discussion since we can't talk to Cisco about it."



And then the point of this, well, the thing that brought it to my attention, or the reason I'm bringing it to everyone's attention since we've sort of discussed this already, is:  "PS: I had a 2TB drive 'die' in my Drobo Mini last week, and guess what.  I ran a Level 4 [meaning SpinRite] on it; and, voila, all is good.  SpinRite saved the drive."  He says:  "Love the show and your efforts," and then he says, "cough, bootable Mac SpinRite, cough."  And so, yes, that's, as we know, that's on the way as soon as I get SQRL behind me.  And that's close to happening, too.  So thanks, Rick, for reminding our listeners that, even though you have a Drobo, you can still have a drive go bad.  SpinRite can bring it back for you.



LEO:  All right, Steve.  Let's [crosstalk].



STEVE:  Okay.  So a little bit of closing the loop with our listeners.  Regarding the bill that was threatening to pass in Georgia to outlaw anybody who accessed a computer without authorization, Joel Odom said:  "The bill in Georgia that you mentioned on Security Now! is effectively dead, thanks to the hard work of EFF and the security community such as those of us at Georgia Tech.  Thanks for bringing it to my attention via the podcast."  And it turns out Joel is a Georgia Tech computer security researcher.  So good news that that didn't get off the ground because that was just - we can't have that precedent.  That would really be bad.



And so Ned Griffin said:  "Hey @SGgrc, enabling Quad9 DNS on my router, but the few discussions I've looked are pointing people to changing in their computer setting.  I have control over all PCs on my network, so am I right to assume the change at the router is okay?"  I know we've sort of confused things in the last couple weeks because we've talked about all the variations on that.  So I wanted to just clarify for Ned and anybody else, yes, the normal default for Windows machines, and actually computers in general, iOS devices and Macs and Linux boxes, everybody uses DHCP, meaning that they ask the router, which is this DHCP server, for the DNS address.



If you then set the router so that it provides the Quad9 DNS IPs, all the systems in your network will just synchronously get it next time they boot.  And at least in Windows, if you inspect the IP addresses, you can if you want to bring up a console window and do ipconfig, I-P-C-O-N-F-I-G, and you might need to say /all.  I don't remember, if you don't say /all, if it shows you DNS addresses or DNS IPs.  But definitely if you say "ipconfig /all," that will give you a dump of the current IP configuration, including what IP addresses for DNS your machine is currently using.  That's just a quick way of verifying that your system is getting them from the local network's router, and that is the default, so it almost certainly is unless you've done something to manually override it.



Jared Komoroski said:  "@SGgrc Your description of how Bluehost could securely use the last four [meaning four characters] of a password to verify account ownership is exactly correct.  They store two hashes at password creation.  I confirmed with some Bluehost quality assurance engineers that I know.  Keep up the good work."  So I guessed correctly.  It made sense that that's the way you would do that.  It's kind of the only way you can do that is to capture it at the time that it's being hashed.



So, and I forgot to mention that this looked like - I saw the screenshot that I had referred to.  I looked at it again after last week's podcast.  It looked like somebody was online in a text conversation, like on a web page, asking for the last four of the password, just for ad hoc identity verification.  So that's what the application was, was somebody saying, hey, to make sure you are you, just give me the last four characters of your password.



Which, you know, it's not super secure.  As I said, it does weaken the rest of the password because it's easier to guess the last four, very much like you're able to guess the two halves separately.  But still, as a simple way of asking somebody something that they would know, that somebody else wouldn't know, and that you wouldn't have to separately write down, like, the name of your favorite pet or your first pet or your favorite teacher in high school, who knows.  That's a nice way to do it.



And finally, two people on pronunciation of German or Germanic names.  Martin Badke said:  "Speaking long ago with a German interpreter, she explained, 'In German, when two vowels go walking, the second does the talking.'"



LEO:  Oh, that's handy.  Good to know.



STEVE:  As a teaching phrase.  And then he says:  "D-I-E-B-O-L-D is said with the 'E,' thus Diebold."  And he says: "Consider Einstein and Siemens."  And that's interesting because it's E-I-N, ein, so there you're getting the second vowel, the "I."  And with Siemens, S-I-E-M, you're getting the second vowel, the "E."  And then also adding to that, MidwestGuru said:  "Diebold" - and I'm now working to pronounce it correctly - "is a Germanic name.  When 'E' and 'I' are together, pronounce the second one."  He also used the example of Einstein and Dietrich, where again D-I-E-T-R-I-C-H, Dietrich.  So anyway, thank you, everybody, for your thoughts and feedback.



The picture I have in the show notes is of yesterday's re-record-breaking attack, showing basically a timeline of prior attacks.  One, and I can't quite read it, looks like maybe it's 2001, can't see it.  But anyway, it's 24GB, that it was like, whoa, 24GB, oh, what?  And then a few years later, 100GB, Leo, oh, my god, 100GB.  And then a few years later, 309, oh, three times more.  More than three times more than 100, 309.  A few years later, now we're up to about, looks like about 2016, 650GB.  Okay, wow.  Now, yesterday, yesterday, 1.7, okay.  So I could say it as 1700 gigabits per second, or 1.7 terabits per second is where we are, an Internet-melting DDoS attack.



As I mentioned at the top of the show, this is the result of a devastating service being exposed publicly.  What's really curious is that this had been noted a couple years ago in some of the Black Hat/DefCon-style conferences.  It might have been one of the two of those.  Someone just sort of noted in passing, and it may have been that no one thought anyone was crazy enough to have this publicly exposed.



But there's a service on Linux platforms called memcache, or because it's a service, memcacheD for daemon, meaning that it's running in the background.  It is a very simple, very fast cache, the idea being that, because you might have a very complex SQL database query, after performing a complex lengthy SQL database query, you might want to do some short-term caching.  So it could be in RAM, or it could be on the hard drive, depending.  But the idea being that you've performed some extensive query.  You may need it again.  You don't want to go back and reissue the query.  So you use sort of this intermediate cache where you use a key or a tag and the data.



So it's very simple.  You can't do any fancy queries.  You can just say, "Here's a blob.  Store it under this index."  And so this memcache says okay, and it is accessible over both TCP - and, yes, I did say both - both TCP and UDP.  Well, we know what that means, if it's publicly exposed.  The problem with UDP is there is no provision for verifying the source IP of a query.



As we've often discussed with TCP, there is the famous three-way handshake:  a SYN for synchronized goes in one direction, a SYN with an ACK acknowledging the receipt of the SYN goes in the other direction, and then finally an ACK acknowledging the receipt of the second SYN makes the third interchange of the three-way handshake.  An interesting side effect is that both endpoints have to tell the truth about their IP addresses in order to receive each other's acknowledgments.  Otherwise it doesn't happen.  So TCP cannot be spoofed.



UDP is used for lightweight Internet traffic where you simply say "give me something," like give me an IP address for DNS, and DNS is UDP.  And DNS has been used in so-called amplification and reflection attacks.  It's amplification because a tiny query can generate a large response.  So what that means is that an attacker is able to send little DNS queries to a DNS server, which will respond with much larger replies, thus amplification.  But because it's UDP, nothing prevents the instigator of the attack, the person sending the UDP packet, from lying about their IP.  They put the IP of the intended victim, the target, in the UDP packet so that the DNS server that receives it believes that guy was asking for a DNS address.  And so the larger response goes to the victim of the attack.  Thus the attack is reflected off of the DNS server and amplified, a reflected amplification attack using UDP.



Well, there have been a bunch of different types of UDP services exposed and used over time.  They all pale in comparison to this one.  DNS kind of has to be exposed to be publicly used, but a DNS server normally is not used by the public.  It's normally used by, for example, an ISP's own clients.  So it's possible, even with DNS, to lock it down so that it will not respond to random public queries from the Internet because that's where an attacker would be trying to bounce their bogus query off the DNS server in order to attack somebody else.  So you can lock down DNS.



This memcache normally would never be publicly exposed.  I mean, you wouldn't want it to be publicly exposed.  The idea is that it is used as an intermediate cache for things that an active system is doing.  Okay.  So the first attack, the first big attack that got everybody's attention was about four days ago, and it was an attack on GitHub, not because probably anybody was mad at GitHub, but just they wanted to attack somebody big.  And so this was a 1.35Tb attack on GitHub.



In a post in their engineering blog, GitHub said:  "The attack originated from over a thousand different autonomous systems, across tens of thousands of unique endpoints.  It was an amplification attack using the memcached-based approach that peaked at 1.35 tbps via 126.9 million packets per second."  So these are large packets. They're full-size, 1,400-byte packets, because this is this memcache trying to answer somebody, it thinks somebody's valid query for data that it has cached.



LEO:  You know what impressed me was how quickly it was mitigated.



STEVE:  Yes.



LEO:  That blew me away.



STEVE:  Yes, yes.



LEO:  And that's Akamai.  That's Akamai's DDoS protection.



STEVE:  Yes.  They have strong DDoS protection.  And then yesterday we saw an even bigger one that broke that record.  That was the biggest ever seen, 1.35 tbps, four days ago.  Now we're at 1.7.  So, okay.  What is this?  What's going on?



So there is a site where this software lives, MemCached.org, M-E-M-C-A-C-H-E-D dot org.  And they say, introducing themselves, "What is memcached?  Free and open source, high-performance, distributed memory object caching system, generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load.  Memcached is an in-memory key value store for small chunks of arbitrary data - strings, objects, whatever - from results of database calls, API calls, or page rendering."



LEO:  A lot of PHP sites use it.  My old WordPress.org PHP-based site used it.



STEVE:  Well, Leo, get this.  YouTube, Reddit, Facebook, Twitter, Wikipedia, Microsoft Azure, IBM Bluemix, Amazon Web Services.



LEO:  Oh, boy.



STEVE:  I mean, yes, it is a highly used, very popular tool because, as you said, I mean, it's there, and it allows, you know, there's very often a slow process to pull something together which may be re-requested.  And so what happens is you're able to set a certain size of the cache.  It is an MRU, a Most Recently Used cache.  So if you keep asking for something over and over, that stays at the front of the list.  Something that doesn't get re-requested ends up getting pushed off the end, getting pushed out of the cache in favor of more recently requested things, and those things tend to stay there.  So, I mean, if you've got lots of RAM, and you can commit a chunk of RAM to this in-memory cache, then it can speed up your system.



The problem is the server is normally meant to be bound to the localhost IP.  Oh, and it's running on port 11211.  So it's a well-known port.  That's where the service lives.  And it's supposed to be bound to port 11211 on the localhost IP, so only on 127.0.0.1.  But in some instances where you have a cluster of systems, maybe you do want to make it more widely available.  So it is unbound from that particular IP, and it operates on the interface that the system is using.  Unfortunately, some of those interfaces, those network interfaces are public.  And a recent query of our old friend Shodan reports 87,811 open memcached servers.



The final picture in today's show notes shows the result of a search for the product memcached returned by Shodan on February 26th showing 25,000, the majority of them, 25,000 of them in the U.S.; just shy of 20,000, 19,647 in China; 4,000-some in France; 3,500, little more than that in Japan; almost 3,400 in Hong Kong, and so on.  So there are 87,000 of these on the Internet.  And what happens is the bad guys are able, because it's an open server, you're able to fill them remotely also.



So the remote attackers are creating single huge blobs so that a single query is able to return like a megabyte of data.  They're able to return massive queries.  So not only are they open - UDP thus can be spoofed - but they will accept for caching from the public anything that you want to store there subject to cache expiration.  But if it's a large blob, a megabyte blob, and then you are constantly asking for it over and over, you're going to keep it from expiring out of the cache, that is, you the attacker.  The attacker is going to keep it from expiring out of the cache by continuing to ask for it while they're attacking somebody.  So this is not good, Leo.



LEO:  Yeah.



STEVE:  This is not good.



LEO:  So this is not an exploit in the sense that there's a flaw in the code.  This is actually how it's designed to work.



STEVE:  Yes.  Yes.  But it was never designed to be public facing.  That's the problem.  By no means would you want to offer your RAM on your server to the public.



LEO:  So these are misconfigured memcached servers.



STEVE:  Eighty-seven-plus thousand of them, yes.  Misconfigured.  And notice that your own system's performance will drop if the bad guys use your memcache RAM for their own purposes because your valid pages and so forth won't be cached any longer.  They'll just get pushed out by this blob which is being used to attack somebody else.  So it's not in your interest to have them exposed as using your bandwidth, which you're probably paying something for.  And it's using a lot of bandwidth.  But it's not clear how we're going to fix this.  Once again, notice that the only way these attacks can occur is if an ISP is allowing packets not bearing an IP on their network to egress out onto the public Internet.  Once again...



LEO:  There you go.  There you go.



STEVE:  We've discussed this.  It's the lack of egress filtering.  There is absolutely no reason why UDP packets should be lying about their source IP as they leave an ISP's network.  Simply blocking those packets would cause them to be dropped, and then that network could not be used for attacking.  Now, of course, the argument is, well, yeah, but everybody else's could.  It's like, yes, but we have to start somewhere.  And unfortunately no one is preventing IP address spoofing.  It's just anybody pretty much can do it.  I've seen some reports of some ISPs doing it, but obviously not all.



So, boy, we have a massive new capability.  And it's not clear, I mean, the problem is also there's no authentication.  This is a "get a packet, send a blob, get a packet, send a blob."  No authentication on this protocol.  Again, the idea being it was never meant to be public.  It was meant to be an internal use tag and data store for quick storage and, I mean, the whole point of it always is to be fast.  It can run over TCP, but it also answers to UDP.  And it does so with devastating effect.  So, wow.  I have a feeling, I mean...



LEO:  The mitigation is first and foremost people should just fix their freaking memcache servers.



STEVE:  Yes, yes, yes.  Oh, now, here's the good news also.  The IP of the inbound attack, that's the actual IP of the memcached server.  That is, the bad guys can spoof their IPs, but the attack traffic...



LEO:  Right.  It doesn't, of course.



STEVE:  ...doesn't, exactly.



LEO:  It's the server, yeah.



STEVE:  So, yeah, so it's the IP of the server.  So it is possible to identify and potentially to go knocking on someone's door saying, hey, by the way, do you realize...



LEO:  Could you fix your freaking servers, you morons?



STEVE:  Yeah, could you please bolt down your firewall, block port 11211, because you're attacking people all over the Internet.  And it would behoove people to do that, too.  So it's in their best interests.



LEO:  Yeah.  But also...



STEVE:  On the other hand...



LEO:  Yeah, I mean, it sounds like also their Internet service providers should stop allowing outbound traffic.  Actually, no, because the outbound will appear to be from the memcached server.  So they can't do that; right?  The outbound would be from a legit IP address.



STEVE:  Yeah.  Let's see.  I think you could argue that there is no reason for memcache...



LEO:  Outbound UDP packets.



STEVE:  Yeah, exactly.  There's no reason for any memcache traffic to transit outside in the world.



LEO:  Right, right.



STEVE:  Yeah.  Wow.



LEO:  Very interesting stuff.  Now, what do you make of this fast mitigation?  I mean, that's one thing we've gotten better at.



STEVE:  Yes, yes, yes, yes.



LEO:  Is fixing and surviving DDoS attacks.  What, they just throw a lot of bandwidth at it; right?  I mean...



STEVE:  Well, remember that they know that the traffic is going to be inbound from port 11211.



LEO:  They block that port, got it.



STEVE:  Exactly.  So if they're a huge network, and they've got automated border controls, they can see a massive incoming traffic coming from port 11211 and just say, okay, block that.  Shut it down.



LEO:  Nope, nope, nope.



STEVE:  But still, no small company is going to be able to do that.  I mean, and this is very burdensome.  That's a massive attack.



LEO:  No kidding.



STEVE:  And now that this news is out, there's nothing to prevent all the botnets, all the IoT malware, I mean, you can't even - it's not a remote execution attack, so you can't infect something and then close the back door behind you.  These are open servers that anybody who wants to can find and use.



LEO:  Wow, wow, wow.



STEVE:  Until this problem is mitigated.  So that's not going to happen anytime soon.



LEO:  Steve, another fascinating episode.  I love, you know, of course all the things you do are great.  But I always love hearing the mechanics of these kinds of very big news story attacks, and why it's a problem, and the mitigation.  How simple it would be for people to fix it if they just would sit up and pay attention...



STEVE:  Yeah.



LEO:  ...to what they're doing.  As always, very informative.  That's why we want to all make sure and tune in every Tuesday for Security Now!.  



STEVE:  Why they keep me around, Leo.



LEO:  1:30 p.m. Pacific, 4:30 Eastern.  We spring forward this weekend in California.  So as we enter daylight saving time, that means we're going to record what will appear to be an hour - oh, no, math is so hard.



STEVE:  Danger, Will Robinson.



LEO:  Our clocks go forward, but UTC does not.  So it will appear that we are recording an hour earlier.  All I know is we're going to UTC -7, which means 18:30 UTC.  You can do the math.



STEVE:  And then something about losing an hour of sleep.  I'm not sure where that happens.



LEO:  We do that, as well.  I hate that.



STEVE:  But go to sleep earlier and then you won't.



LEO:  I hate that.  Tune in if you want to watch live to TWiT.tv/live.  And you can also, by the way, join the chatroom, always a lot of fun in there, irc.twit.tv.  They're talking about what's happening on the live stream, so the conversation will make more sense if you're watching the live stream.  You can watch on-demand, of course.  Steve has full beautiful transcripts, along with the audio version of the show, at his site, GRC.com.



And while you're there, it might behoove you to pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility, and all the freebies Steve offers.  And learn about SQRL because it's coming soon.  And, oh, there's so much stuff there.  It's actually a treasure.  It's one of those places, the web used to be full of this, where you would find a site and spend hours just reading all the stuff there.  It's like that, GRC.com.



We have audio and video of the show.  I don't know why you'd want to watch it, but you can.  Oh, I know why.  For the trailers, the movie trailers.  There is always a great - no, TV trailers.  There's always a great TV trailer on every show.  Go to TWiT.tv/sn for Security Now!, or subscribe on your favorite podcast application.  We're there.  We're everywhere.  This show's been going on for a long dang time, 653 episodes.



STEVE:  Coming up on the, well, not quite yet, but the end of - we're in year 12 now.



LEO:  Amazing, amazing, amazing.  Thank you, Steve.  Thank you, everybody, for watching and listening.  And we'll see you next week on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#654

DATE:		March 13, 2018

TITLE:		AMD Chipset Disaster

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-654.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss the just-released news of major trouble for AMD's chipset security, ISPs actively spreading state-sponsored malware, Windows 10 S coming soon, a large pile of cryptocurrency mining-driven shenanigans, tomorrow's Pwn2Own competition start, surprising stats about Spam botnet penetration, and a Week 2 update on the new Memcached DrDoS attacks.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  He was all set to talk about the Memcached exploit spreading like wildfire.  That was the culprit in those DDoS attacks, huge DDoS attacks last week.  But then, all of a sudden, some significant flaws were revealed in the AMD chipsets, and Steve decided he was going to do a deep dive on that.  Lots more security news, too.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 654, recorded Tuesday, March 13th, 2018:  AMD Chipset Disaster.



It's time for Security Now!, the show where we protect you and your loved ones online.  This is becoming one of our biggest and best shows, and it's thanks to this guy right here, Steve Gibson of GRC.com.  Hello, Steve. 



STEVE GIBSON:  Hello, Leo.  Great to be with you again, as always, for our 654th episode of Security Now!.  I had intended for this to be a follow-up on last week's disclosure and discussion of the big memcached huge, record-breaking, 1.7TB distributed reflection denial of service attacks.  And then something even worse happened.  So I was going to call this "Memcrashed Update" or something, and I have all of the sort of like what's happened since last week, which is a lot on that front.  And then this morning a security research firm in Tel Aviv who gave AMD 24, count them, 24 hours of notice...



LEO:  Oh, boy.



STEVE:  Yeah, dropped a major bomb that they had found 13 remotely exploitable vulnerabilities in the chipset that AMD uses to sort of offload its security stuff, sort of like the Secure Enclave, and also just a chipset that's like a glue chipset.  Anyway, this, I mean, the podcast was pretty much assembled, and everything was in place.  And I thought, okay, good.  And then, because the press release hit and the Internet went nuts, first I was thinking, okay, well, there's not time to know about all this yet, so let's just do that next week.



But as I kept reading it, like getting into more of this, and the whole research paper is online, so there was nothing I couldn't find out.  And it's like, oh.  We can't wait.  So this suddenly got renamed from "Memcrashed Update" to the "AMD Chipset Disaster," which surprisingly is not hyperbole, yeah.



LEO:  Doesn't sound good.  Oh, man.



STEVE:  So we're going to discuss the just-released news of major trouble for AMD's chipset security.  And the only possible mitigating factor I can imagine is that maybe there aren't many of them out there.  Except I think there are because they're embedded, and they're laptop, and they're server, and they're enterprise.  And it's like, ouch.  But we're also going to talk about ISPs in East Asia have been found actively spreading state-sponsored malware, that is, ISPs themselves.



A note about Windows 10 S that I noted you and Mary Jo and Paul were talking about last week.  I just wanted to let our listeners know about Microsoft's announcement about 10 S.  We've got a large pile of cryptocurrency mining-driven shenanigans because of course there's money in them thar hills, and so you're going to have some shenanigans.  Also tomorrow is our annual Pwn2Own competition start.  But there's some news about that, and a bit of a twist, about a set of contestants that will no longer be appearing.  I have some surprising stats, I mean, stunning stats about where our spam is coming from.



And then we will finally get to what was going to be the topic of this week, but now it's got pushed down the stack, and that's an update on the memcached distributed reflection denial of service attacks and what's going on with them.  And in fact that's our Picture of the Week that we'll talk about in a second.  So I think, yes, another interesting and, for owners of AMD-based systems, maybe a little frightening, too.



LEO:  Is it the newer Ryzen systems, or is it older AMD?



STEVE:  Yes, the newer Ryzen...



LEO:  It's Ryzen systems.  Oy.



STEVE:  The Ryzen and the EPYCs, I think, is that the way you pronounce it?



LEO:  Yeah.  I think a lot of people bought these Ryzen systems.



STEVE:  Yeah.



LEO:  That's not good.



STEVE:  Yeah.



LEO:  Well, stay tuned.  The thrilling, gripping details of the latest bad news exploit.  Steve?



STEVE:  So our Picture of the Week was going to tie into the title of the podcast.



LEO:  Didn't have time for a second illustration, huh?



STEVE:  Which changed at the last minute.  Okay, but get this.  More than 15,000 memcached DDoS attacks were aimed at 7,100 sites in just the last 10 days.



LEO:  Wow.  This can't all be the same person; right?



STEVE:  No.  And this is the problem is that there are, we're not exactly sure how many, but at least 17,000 open reflectable memcached servers, which anyone on the Internet can use.  And as we'll be talking about, if we survive till the end of the podcast, there are now easy-to-use tools where you enter an IP address, and it goes and finds available memcached servers from Shodan and starts filling them up and sending them reflection UDP packets.  I mean, it was no exaggeration a week ago when we made the podcast about this because this is without question the most potent class of DDoS attacks we've yet encountered.  It is due to the fact that a tiny query can produce a massive response which can be spoofed.  The amplification factor, and we've talked about amplification factors of, oh my god, 200.  And so you send in 10 bytes, and you get 2,000.  Whoo.  Okay, this amplification factor on these memcached DDoS attacks is 51,000.



LEO:  So for every byte you get 51K.



STEVE:  Yes.



LEO:  Wow.



STEVE:  Yes.  So anyway, we'll come back to this at the end of the podcast.  But what this is demonstrating is that this is all, I mean, if anything could have pulled hackers' attention away from cryptojacking, it would be DDoS attack capability that is this juicy and potent and capable.  So it's like, great.  So I'm sure we'll be talking about it next week.  The industry is trying to rally itself; but, as we know, there are servers scattered all over the Internet.  Many of them are unattended.  Many of them are in closets and have been forgotten, and no one is paying any attention to them.  And they'll just happily accept huge blobs of data and then send them out on request.  So, yeah.  Anyway, there's lots more news that we will get to at the end of the podcast.  But what preempted that as our main focus...



LEO:  Must be pretty bad if it preempted that.



STEVE:  So a brand new, some of the first, the very first coverage confused this with Spectre and Meltdown, and this has nothing to do with Spectre and Meltdown.  We know that AMD chips have never been susceptible to the Meltdown problem, but that they do need some microcode fixes in order to help them mitigate the Spectre problem.  So there's that.  But a brand new, completely different, and just coincidental timing, set of 13 critical flaws have just been reported to affect AMD's Ryzen and EPYC processors.  You may want to pull up this site, AMDflaws.com.  I mean, these people are not being shy about their reporting.  They've got...



LEO:  They registered a domain name.  They're serious.  That's serious.



STEVE:  They registered a domain.  They've got graphic artists at work.  They've got videos.  They've got...



LEO:  But this is not AMD.  This is the group that discovered it.



STEVE:  Correct.



LEO:  Is that right?



STEVE:  The group is, yes, the group is CTS Labs. Oh, and you can imagine AMD is not happy right now.  They were given, as I mentioned at the top of the show, one day, 24 hours' notice of this all going public.



LEO:  But this has a good name, which means it's going to spread like wildfire.  Because it's all about the name; right?  Ryzenfall?  I like it.



STEVE:  That's actually a brilliant name.  So the group is CTS Labs.  They're an Israeli Tel Aviv-based team of researchers.  Their press release dated March 13th, which is today, 10:00 a.m. Eastern, said:  "CTS Labs, a cybersecurity research firm and consultancy, today released a severe security advisory on Advanced Micro Devices, Inc. processors.  A CTS Labs security audit," they write, "revealed multiple critical security vulnerabilities and manufacturer backdoors in AMD's latest EPYC [E-P-Y-C], Ryzen, Ryzen Pro, and Ryzen Mobile processors."  Now, technically these are in the chipsets, that is, the supporting chips.  But we'll get to that in a second.



"These vulnerabilities have the potential to put organizations at significantly increased risk of cyberattacks.  CTS Labs has produced a whitepaper report further detailing these vulnerabilities available at AMDflaws.com."  And there's, like, several offers on that page to download the whitepaper, which of course I did, and spent some time with it.



"CTS Labs has also shared this information with" - I'll add editorially yesterday - "AMD, Microsoft, HP, Dell, and select security companies, in order that they may work on developing mitigations and patches, and examine and research these and other potential vulnerabilities at the company," that is to say, AMD.  "CTS Labs has also shared this information with relevant U.S. regulators.  CTS Labs," they write, "is a cyber-security research firm and consultancy based in Tel Aviv, Israel specializing in hardware and embedded systems security.  For more information about CTS Labs, please see cts-labs.com."



Okay.  So as I've said, the vulnerabilities don't affect AMD's Zen CPU cores themselves, but rather two different chips that are part of the Ryzen and EPYC system set.  The first is the ARM-based AMD Secure Processor, which as I mentioned before is kind of like their equivalent of the Secure Enclave; and the second is known as the Promontory chipset, which is produced by Taiwanese subcontractor ASMedia.  And these people are not happy about the work that ASMedia, that is, CTS Labs is unhappy about that.



So all there has been so far from AMD because, I mean, this just happened, is that AMD responded:  "At AMD, security is a top priority, and we are continually working to ensure the safety of our users as new risks arise."  Daily.  They continue:  "We are investigating this report, which we just received, to understand the methodology and merit of the findings."  And so I need to say also that, I mean, my first reaction upon seeing this was, okay, wait, maybe we should hold off saying anything.  But that would mean a week delay rather than a day delay.  And so I thought, well, no, the best that could happen is that it turns out that this is wrong.  But the more I looked at it and dug into their research, the more credible and verifiable this appeared.



Okay, so what happened?  Thirteen critical security vulnerabilities and what they describe as "manufacturer backdoors" were discovered throughout AMD Ryzen and EPYC product lines.  In their little Q&A that is at the AMDflaws.com site they said:  "Am I affected?"  And the answer:  "Any consumer or organization purchasing AMD servers, workstations, or laptops are affected by these vulnerabilities."  And of course we have to back that off a little bit and say, if they contain these affected chipsets.  And so they said:  "This site is to inform the public about the vulnerabilities and call upon AMD and the security community to fix the vulnerable products."  I doubt that much calling upon will be necessary.  But they have another site, safefirmware.com, where the whitepaper can be found at /amdflaws_whitepaper.pdf.



So there are a total of four vulnerabilities, three in firmware and one in hardware.  And this Chimera is the fourth hardware vulnerability which is unfixable, they claim, because it is a hardware problem.  It is something, apparently a backdoor built into, presumably by this AMD subcontractor we'll get to in a second.  The other three - there's Ryzenfall, there's Fallout, and MasterKey - are firmware vulnerabilities which presumably can be fixed.  These guys in their research did succeed in rewriting the firmware remotely of this firmware reprogrammable outboard chip that does the security and were able to install persistent threats into the firmware.



So it looks to me like the biggest problem, I mean, just in terms of practical remediation, is with Chimera.  And so they write:  "Backdoors Inside Ryzen Chipset:  The Chimera vulnerabilities are an array of hidden manufacturer backdoors inside AMD's Promontory chipsets.  These chipsets are an integral part of all Ryzen and Ryzen Pro workstations.  There exist two sets of backdoors, differentiated by their implementation.  One is implemented within the firmware running on the chip, while the other is inside the chip's ASIC hardware.  Because the latter has been manufactured into the chip, a direct fix may not be possible, and the solution may involve either a workaround or a recall.



"The backdoors outlined in this section" - I'm reading from their paper - "provide multiple pathways for malicious code execution inside the chipset's internal processor.  Because the chipset is a core system component, running malware inside the chip could have far-reaching security implications.  The diagram below" - and they have a diagram in their PDF that basically shows the Ryzen CPU connected to this big red box, red because bad, where, like, everything, all of the I/O in the system is going there, the LAN and the WiFi and the USB, I mean, it's the switchboard for the CPU.



So it says:  "The diagram below was taken from the instruction manual of ASUS Crosshair VI Hero Ryzen motherboard.  It can be seen that not only is the chipset connected to the computer's USB, SATA, and PCIE ports" - which is to say pretty much everything - "it is also linked to the computer's LAN, WiFi, and Bluetooth.  In our research," they write, "we have been able to execute our own code inside the chipset and then leverage the latter's Direct Memory Access (DMA) engine to manipulate the operating system running on the main processor.  These two capabilities form the foundation for malware and provide a proof of concept.  We believe that, with additional research, a determined attacker" - who of course now knows where to look - "may also be able to reach the following capabilities."



And so they tick off a key logger.  It may be possible to implement a stealthy key logger by listening to USB traffic that flows through the chipset.  Network access, it may be possible to implement network-based malware by leveraging the chipset's position as a middleman for the machine's LAN, WiFi, and Bluetooth.  Bypass memory protection, it may be possible to leverage the chipset's position to access protected memory areas such as the system management RAM.  They say:  "We have verified this works on a small set of desktop motherboards."



So then they have a second section that I'm sharing just because it's inflammatory but worrisome.  "Third-Party Chip Design Plagued with Hidden Backdoors" is the topic for this.  They said:  "In November 2014 it was announced that AMD signed a contract with the Taiwanese chip manufacturer ASMedia, according to which ASMedia would design AMD's chipset for the upcoming Zen processor series.  This chipset, codenamed Promontory, plays a central role within the company's latest generation Ryzen and Ryzen Pro workstations.  It is responsible for linking the processor to external devices such as hard drives, USB devices, PCIE cards, and occasionally also network, WiFi, and Bluetooth.  Although it is branded AMD, the Promontory chipset is not based on AMD technology.  Rather, it is an amalgamation of several integrated circuits that ASMedia has been selling to OEMs for years, all merged together onto a single silicon die."



Okay.  I'm skipping a whole bunch where they really tear ASMedia apart, saying that the chips that were glued together have a long history of problems.  They exist on other motherboards and may themselves be problems.  But essentially the essence is that ASMedia just pulled a bunch of their existing stuff, kind of threw it together, stuck it on a die, and said here you go.



And so in their Q&A they ask:  "Doesn't this publication put users at risk?"  And I would argue their statement.  "No," they say, "all technical details that could be used to reproduce the vulnerabilities have been redacted from this publication."  They say:  "CTS has shared this information with AMD, Microsoft, and a small number of companies that could produce patches and mitigations."  On the other hand, we know from a practical standpoint that telling the world 24 hours after you reveal this to AMD, there's just no way that that's not irresponsible.  At a minimum, 90 days to allow AMD - or a week to allow AMD to evaluate it independently, come to their own conclusions, and produce a response.  Not even that courtesy was given.



So it's very difficult not to hold these people responsible for, to some degree, any damage which occurs as a consequence of the fact that it's not possible to expect anyone to react in 24 hours to this kind of disclosure.  And this is an outboard chipset.  We know very little about it at this point.  I'm sure I'll have a follow-up next week.  The good news is they did not release, I mean, thank goodness, proof of concept or additional details.



The problem is that the world now knows, the hacker world now knows there is a big juicy target here.  And there's now a race to see, well, first of all, whether there's any remediation that can be done.  Then that has to get created and has to get somehow pushed out to the affected motherboards while, in parallel, bad guys are working to figure out what it was that CTS has figured out and leverage it before that can happen.



So anyway, there's no takeaway for our listeners.  Nothing we can do at this point.  I have a bunch of AMD-based machines.  I'm largely an Intel shop, but I have AMD because they're a strong source of 64-bit systems, as we know.  So we'll just be following this news and of course keep our listeners informed.  The good news is it's not clear what role Microsoft could play.  We do know that Microsoft has an active patchable channel, a patching channel.  As we talked last week, they have started to use that to patch the Intel microcode.



This is outboard microcode, so this will be microcode of that chipset.  So it's not clear to me what the path is for getting that fixed.  Maybe, if there's access to it from the main CPU, that is, programmatic access, then presumably AMD could produce and Microsoft could push a patch for Windows users, at least.  And of course Windows is not the only OS to run on AMD chips.  Maybe we'll see something happen in the Linux community even before.  I mean, who knows?



We'll be watching it closely because this is - as you commented, Leo, at the top of the show when you heard it was the Ryzen family that were affected - this has got to be really bad news for AMD.  I'm just - I'm disappointed, frankly, that CTS did this.  It would have been much more reasonable for them to give AMD some time to respond, for example, for a remediation and a patch to be available in the channel, and then for these guys to say, okay, we're the people who found this.  I don't understand why that didn't happen.



LEO:  Unless they felt like it was going to imminently come out in other ways or...



STEVE:  Yeah, good point, yup.



LEO:  And there are reasons you might want to make it go public as soon as possible.



STEVE:  Well, I mean, nothing could possibly put AMD under more pressure.  So if there was any sense that they weren't going to be listened to, that AMD was blowing them off, that it wasn't going to get fixed, like as you said, maybe they do have some information about something else going on that really makes this have to happen as fast as possible.  We just don't know yet.  So I imagine more information will be coming.  And we'll, of course, keep our listeners in the loop.



LEO:  Yeah, wow.



STEVE:  Yeah.  Not what you want to have happen if you're...



LEO:  I guess this means you shouldn't buy an AMD processor for a while.  I mean, there are people in the chatroom saying, "Gosh, I was going to go get one today."



STEVE:  Yeah.



LEO:  I would wait a little while, at least to hear what mitigations there are.  Maybe it's not a real threat.  Maybe AMD will come out and say, oh, we've analyzed this, it's not a real threat.  Or I don't know.



STEVE:  Yeah.



LEO:  It's not like Intel doesn't have its own problems; right?



STEVE:  Right.  And maybe, I mean, most of this is firmware updatable, apparently, except for the one, the hardware variant of the Chimera flaw.  And we don't know that you can't work around that with firmware to maybe shut down something.  Maybe it means you have to turn off one of your USB ports, or who knows.  But it would be difficult, yes, Leo, to go and lay money out right now.  Well, but it's also a function of your exposure.  If it's your own machine...



LEO:  Do we know what the mechanism for an attack would be?



STEVE:  No, we don't yet.  Although, based on what little we know, it looks like because this chipset, like that outboard chip, this Promontory chip, which is where the problems are, it is the glue for all the I/O.  So hard drive, LAN, WiFi, Bluetooth, I mean, you name it, it potentially has access into the chip.  Though, again, we don't know how it is that you leverage this.  And the good news is, I mean, AMD just must be gasping right now.  I'm sure within the next day or two, because I'm sure they're motivated to help create some AMD-side calibration, we should know more.



And so maybe wait just a couple days and see what AMD says.  If they're able to say, okay, definitively, this is only a local attack through USB,  that kind of thing, then you would probably - and if they're able to say "and we've got a fix on the way," then I'd say, yeah, fine, you know, go ahead.  Wow.



LEO:  Poor Ant Pruitt just got his a week ago.  But it doesn't necessarily mean this is panic time for you, either, as a user.



STEVE:  No, well, and here we are, what is it, it's the middle of March, and we've been talking about Spectre and Meltdown all year.  And there still isn't a widespread known exploit of that.  And so, yeah, it's not, you know, we're 10 weeks in.  And the good news, I mean, if there's an upside, it's that this couldn't possibly motivate AMD any more strongly.  And no doubt ASMedia is saying whoops.



LEO:  Yeah, no kidding.  Well, I even wonder, whoops or did they get caught up to no good?  Right?



STEVE:  Yeah.



LEO:  If you put a backdoor into these chips...



STEVE:  If it's really a hardware backdoor, it's very hard to, I mean, that AMD was unaware of, that AMD didn't ask for...



LEO:  Right.  This may not be AMD's fault.  Probably those, at least, are not AMD's fault.  Probably ASM just did it.  Is ASM on mainland China?



STEVE:  They're Taiwanese.



LEO:  Okay.  So not Taiwan.  That's different.



STEVE:  So, yeah, not mainland.  But who knows?



LEO:  I would be suspicion of mainland Chinese manufacturers because they could easily be subverted by the government, whether they wanted to or not.



STEVE:  Well, in fact we've got a couple stories about what's happening with China today.  So we'll be talking about that in a second.  So, yes, you're certainly right.



One thing that is a little disturbing - this doesn't affect people outside of Turkey, Syria, and Egypt.  But from a technology capability standpoint it is worrisome because it could affect us all.  And that is that ISPs in those three countries have been caught using - I'm annoyed with this term "deep packet inspection."  All it really means is hardware which is operating at the packet level.  That is, it's not just a router which is looking at packet headers and sending things around.  It's going into the packet in order to see what's going on.  It turns out that there is a company, Sandvine, which has a line of hardware called PacketLogic devices, which Sandvine argues are not designed for this purpose, and they're not happy  that they've been called out and having been found.  



LEO:  Well, we should point out that there are ISPs who use Sandvine for deep packet inspection.



STEVE:  Yes.



LEO:  And I think it was Comcast, it was one of the U.S. ISPs was using it to block BitTorrent.



STEVE:  Yes, yes, exactly.  And in fact Citizen Lab are the researchers who caught this behavior and reported their findings to Sandvine, which says that Citizen Lab's, who I strongly believe is correct, report is false, misleading, and wrong, and demanded that Citizen Lab return to them the secondhand PacketLogic device which Citizen Lab used to confirm the attribution of the network fingerprint that the Sandvine devices were using.  So it looks to me like Citizen Lab did everything right.



So here's what they found:  Turkey's Telecom network, which had some overlap in Syria, so thus some Syrian ISPs were involved, or Syrian citizens, at least, were using Sandvine's PacketLogic devices to redirect hundreds of targeted users - so not everybody, but if you were a known customer of Turkey Telecom, so for example journalists, lawyers, and human rights defenders, your typical targets in those environments - to malicious versions of legitimate programs, and those programs were bundled with FinFisher and another spyware or malware known as StrongPity, when they tried to download them from official sites.



Now, that's what's interesting is unfortunately there are a number of sites, many of which are HTTPS.  But when you go to download their software, the download link is HTTP.  So what happens is you're at a legitimate site - and we're talking, for example, Avast Antivirus; CCleaner unfortunately got caught up in this again, and remember that they were a victim of some compromise not too many months back; Opera; and 7-Zip.  Those sites, at least at the time of this reporting, were downloading their software over non-HTTPS.  So even if you went to those sites over HTTPS, so you knew you were there, the download link was not.



So what this PacketLogic device allowed was that to be intercepted since, without TLS, without being in a TLS tunnel, you have no authentication of the connection.  And that caused the legitimate, the otherwise legitimate links and their programs to be funneled from another source undetected, and people believed they had done the right thing.  They believed they were, you know - they were at the legitimate site, but the actual software they received had been tampered with.  So that's the case in Turkey.



In Egypt, the same Sandvine PacketLogic devices were being used by an Egyptian ISP to make money in two different ways.  One, they were secretly - the ISP was injecting cryptocurrency mining script into every HTTP web page their own users, their own customers visited, in order to mine Monero cryptocurrency.  So again, non-HTTPS.  The Sandvine PacketLogic devices were unable to deal with that due to its encryption.  But the second you dropped out of HTTPS, as soon as you just started using a plaintext connection, then they were able to manipulate your traffic and, in this case, add some cryptocurrency mining JavaScript into any non-HTTPS websites that their client customers, that their customers visited.  And they were also redirecting some users to web pages with affiliate ads as another way of generating additional revenue.



So anyway, as far as we know, that's not happening to users outside of those countries.  But it is a real heads-up that there's a vulnerability which is exploitable and we know has been exploited - if not against us, at least theoretically, well, it has been exploited against those customers, those targeted customers, leveraging the fact that, without TLS encryption on the connection, you are still subject to manipulation, even if you're at an otherwise HTTPS site.  And, boy, we have to, you know, maybe it's because they're using content delivery networks; they're using a CDN that doesn't offer HTTPS services.  I've not looked any further to see, for example, why legitimate companies like Avast, CCleaner, Opera, and 7-Zip might be offering non-HTTPS content in some cases.  It's worth keeping an eye on.



LEO:  All right, Steve.  Continue, my friend.



STEVE:  So we've not talked about Windows 10 S because we've not talked a lot about...



LEO:  We know what you think of Windows 10.  We know what you think.



STEVE:  Yes, we do.  So I did want to just mention, so that it hadn't been ignored on this podcast, that Microsoft has announced a change in their policy relative to Windows 10 S.  



LEO:  Paul would say this was what it always was, but they just didn't communicate it well.



STEVE:  Okay.  And now they are.  Last Wednesday evening - is it the Corporate Vice President in charge of Windows, is it...



LEO:  Joe Belfiore, yeah.



STEVE:  Belfiore, okay, yeah, Joe Belfiore.



LEO:  We call him Joey B.  He's great, actually.



STEVE:  Joey B.  He wrote a short blog post to describe what's happening that I'll just share, just so that we have it on the record here.  So it was Windows 10 in S mode coming soon to all editions of Windows 10.  And that's why I think it's relevant to our listeners is that it's becoming a "mode" for Windows 10.



LEO:  Right.



STEVE:  So he said:  "Some of you may have seen a discussion around our plans for Windows 10 S on Twitter today," speaking of last Wednesday.  "And given some additional questions I've received," he writes, "I thought it might be helpful to share more about our plans with Windows 10 S.  Last year we introduced Windows 10 S, an effort to provide a Windows experience that delivers predictable performance and quality through Microsoft-verified apps via the Microsoft Store.  This configuration was offered initially as part of the Surface laptop and has been adopted by our customers and partners for its performance and reliability."



LEO:  Yeah.  I had that Windows Surface laptop.  And what they offered at the time was great.  You could just push a button and turn it into Windows 10 Pro.



STEVE:  Right, meaning you could un-S yourself.



LEO:  Un-S yourself, yeah.  Kiss your S goodbye.



STEVE:  Yeah, that's right.  And apparently everybody did.



LEO:  Yeah.



STEVE:  He said:  "Since that time" - well, and I've heard Paul say it was basically unusable.



LEO:  I would disagree.  But if you can't put Chrome on them - the things about S were it had to be, I think it had to be you could only use Store apps.



STEVE:  Correct, for sure.



LEO:  I think they had to be 32-bit.  I can't remember that issue.  But the problem is...



STEVE:  Probably have to be the UWP...



LEO:  UWP, yeah.  So you couldn't use Chrome.  You couldn't use LastPass.  I mean, there was a lot things - I guess actually you could use LastPass, but you couldn't use Chrome.  You couldn't use...



STEVE:  Well, anything else that you wanted to use that...



LEO:  ...anything that wasn't in the store; right.  So the real question, which we asked on the show, is...



STEVE:  So sort of like the Windows Phone of Windows.



LEO:  Well, it wasn't that bad.  You get most of the stuff.  It's just the real thing is, is there really any benefit to it?  Is it more secure?  Is it faster?  Does it have better battery life?  That was never clear.  That's what Microsoft asserted.



STEVE:  And what do you think their intent was?  Because I've never really understood.  I mean, did they want to create a monitored, managed enclave like for...



LEO:  No, it wasn't that sophisticated.  That's the problem.  It was basically Windows 10 where you couldn't get anything but what was in the Store.  Kind of like if you took your Mac and you said only allow apps from the App Store, which is [crosstalk] OS X.



STEVE:  That's probably what "S" stood for was Windows 10 Store Edition; right?



LEO:  Yeah.  And I think it was to compete with the Chromebook.  Even though the Surface laptop was not inexpensive, I think it was assumed that all the other computers that ran Windows 10 S would be for education, where you might want to lock it down, and for less expensive environments.  It was to compete with the Chromebook.  But the truth is, yeah, it was kind of a muddy product to begin with.  And so considering it a service or a mode of Windows 10 makes more sense.  If you had a switch, just like you do in OS X, where you say we'll only allow people to buy stuff from the App Store, don't allow them to download, it would be more secure; right?  If you couldn't download stuff that wasn't in the App Store?



STEVE:  Absolutely.  Absolutely.



LEO:  So that was the idea.



STEVE:  Well, for example, I'm sure that everything in the App Store has been digitally signed.



LEO:  Right.



STEVE:  And when I was talking just a minute ago about ISPs intercepting HTTP and doing packet-level redirection in order to cause malicious code to get downloaded, I meant to say that just two days ago I was looking for, because it's me, an old version of Adobe Reader.  I don't want DC or 11 or any of the new nonsense where you have to sign up and subscribe and join and be a member.  I just want a PDF to open.  And so I ended up finding, I think it was a version 11, or maybe it was 9.



Anyway, I found something that looked right, but it wasn't from Adobe.  Since then I've realized that Adobe's FTP server does have everything that they ever did there.  It's like, oh, thank goodness.  But a couple days ago I found one from what looked like a reputable download site.  And I'm not recommending that anybody do that because I was lucky, I felt I was lucky to find it.  What I downloaded was 35MB, and I was very careful not to run it.  Instead I checked to see if it was signed, and it was signed by Adobe.  I verified the certificate.  I verified the signature.  I mean, I did all this other stuff that I know how to do, and I know that our listeners know how to do.  Most people don't.



But the point is that that's what you have to go through because in our current world there's just so much crap on the Internet that is trying to take advantage of you.  So it's certainly an advantage for a neophyte Windows user to, if you turn on S mode, then Windows is certainly going to not let you download anything from other than the Store, and anything from the Store will be digitally signed by who knows.



LEO:  And at least presumably vetted, although the Microsoft Store is an abomination.  But it's kind of like this switch on macOS.  Look, allow apps downloaded from either App Store and identified developers.  That means you have to have a certificate.  Or App Store you can actually bypass this if you download something without a certificate, but you have to manually approve it.  And if you put this in App Store and then locked it so that your kids or your users couldn't change it, that would be effectively S mode.



STEVE:  Yes.



LEO:  That's Apple's version of S mode.  And that's what I hope Microsoft does.  I think that would be a good thing.  I don't have a problem with that.



STEVE:  So his statement says, he claims, he says:  "Since that time," that is, the time of the S on the Surface - maybe, oh, that's probably what "S" stood for, too, Surface and Store...



LEO:  Right.



STEVE:  ...and Security and Stability and Safety and the American Way.  Oh, no.  Anyway, so since that time...



LEO:  Microsoft always says it stands for nothing.



STEVE:  Okay.  "Since that time," he says, "we've received great feedback from customers and partners on Windows 10 S.  Customers love the security, the faster boot time..."



LEO:  Oh, please.



STEVE:  It makes it faster, too.



LEO:  No, it doesn't, though.  It's Windows 10 Pro.  It's the same.



STEVE:  Right.  "Better battery life."  Oh, it makes your battery life apparently longer.



LEO:  Yes.  See, if any of this were true, it might make it worthwhile.  But as far as I can tell, it's not.



STEVE:  Right, right.  And, he says, "...and consistent performance over time."  What, because you can't load anything on it.



LEO:  Because you can't put crap on it.  Which is a real problem in the Windows world.



STEVE:  So he says:  "Our partners have brought to market more than 20 devices with Windows 10 S enabled.  We have also heard feedback that the naming was a bit confusing for our customers and partners."  And of course we've done nothing to alleviate that confusion.  And he says:  "Based on that feedback we are simplifying" - oh, it stands for Simplifying - "the experience for our customers.  Starting with the next update to Windows 10" - which I did a little bit of research.  Apparently it's expected to be the spring, so maybe it stands for Spring, the Spring Creators Update, Windows 10 Spring Edition.  Anyway, so with the Windows 10 Spring Creators Update, we believe.  And he's saying:  "Starting with the next update to Windows 10, coming soon, customers can choose to buy a new Windows 10 Home or Windows 10 Pro PC with S mode" - it's a mode - "enabled, and commercial customers will be able to deploy Windows 10 Enterprise with S mode enabled."



He said:  "We expect the majority of customers to enjoy the many benefits" - okay, I said "many" - "the benefits of Windows 10 in S mode.  If a customer does want to switch out of S mode, they will be able to do so," Leo, "at no charge" - you won't be charged for getting all the rest of Windows back - "regardless of edition.  We expect to see new Windows 10 devices ship with S mode, available from our partners in the coming months, so check back here for updates."



LEO:  I'm waiting for one, actually.  It should come this week.



STEVE:  Uh-oh.



LEO:  An HP.



STEVE:  Now, does that mean you can't turn it back on after you've been bad and turned it off?



LEO:  You can, but it's not easy.  You have to reinstall Windows 10 S mode.



STEVE:  Okay.



LEO:  Because I did it on my Surface laptop.  And then, by the way, I did it as an experiment because Paul wanted to know, so I bought the Surface laptop with 10 S on it, put up with 10 S for a while, but said, nah, I really want Chrome.  And so I then flipped it off.  It's very easy.  It doesn't make many changes because frankly all it does is it says, go ahead, get stuff from the store, it's fine.



STEVE:  It loosens its grip.



LEO:  It loosens its grip.  It's very quick.  And that was free.  And then as an experiment we were curious, well, now that you've installed Chrome on it, if you go back to S mode, what happens to Chrome?



STEVE:  Ah, yes, I'm curious, too.



LEO:  And that's why it takes so long.  You have to download S  mode, reinstall.  And, yes, Chrome is gone.



STEVE:  Oh, it expunges your...



LEO:  Expunges.



STEVE:  Okay.



LEO:  And I think that's why it's more complicated to switch it off and on.  I would love to see a switch - see, what would be great is switch it off, just like I can on the Mac, get Chrome,  then switch it back on again.



STEVE:  Yup, and then leave me alone.



LEO:  And then leave me alone; right?  That would be ideal.



STEVE:  So protect me, but forgive my past sins.



LEO:  Exactly.



STEVE:  Yes.



LEO:  That would be nice.  We don't know what they're going to do.



STEVE:  Oh,  S for Sin mode.  No.



LEO:  Sin, right.  Subsin.  Sinless.



STEVE:  So for our listeners, something is coming with the Spring Creators Update that will sort of - it'll be interesting.  He talks about being able to turn it off.  And I guess there were some rumors that you'd have to pay an extra $49 to turn it off?



LEO:  You used to have to.



STEVE:  Ah.  You used to pay more?



LEO:  Yeah, you've got to pay 50 bucks to go to Windows 10 Pro.  So that was...



STEVE:  Oh, okay, Pro.



LEO:  Yeah.  Well, it is Pro, by the way.  That may change.



STEVE:  Underneath the S there's Pro hiding.



LEO:  It was always Pro, yeah.  And the deal with the laptop, Surface laptop is you have till December, the end of the year, and you can get it for free.  And after that it'll be 50 bucks.  But I think they never really did that.  I think they never really did that.  It seemed like a good idea at the time.



STEVE:  Okay.



LEO:  Anyway, I don't know.  I've given that laptop to Megan.  My guess is it's not still in S mode.



STEVE:  So many words start with S that it's just too much fun.



LEO:  I know.  But I am getting one of these new Windows on ARM machines because I'm very curious what they'll be like, running on a Qualcomm.  Nothing wrong with Qualcomm chips, is there?



STEVE:  No, no, as far as we know.  Although apparently that merger was blocked by our President.



LEO:  Yeah, the President, which is really interesting.



STEVE:  Yeah, citing cybersecurity or national security.



LEO:  Yeah, because Broadcom's in Singapore.  Even though they were planning on repatriating to the U.S.  We're in an old Broadcom facility, by the way.  This studio was Broadcom.



STEVE:  Got great plumbing.  Great plumbing.



LEO:  It was so easy to get the servers in and everything, it was great.  Hello, China.  But, yeah, I thought that was very interesting.  I don't know, he didn't give us any information about the justification except to say for security reasons.  Singapore is a U.S. ally.  Maybe he thinks Singapore is China.  I don't know.



STEVE:  Okay, well, I'm just going to bite my tongue and move on.



LEO:  Whatever.  Hey, we're safer.  We're all safer.



STEVE:  So we have an interesting collection of cryptocurrency / malicious mining / cryptojacking news.  I got a kick from a note that I saw on the BleepingComputer site.  Our friend Lawrence Abrams, who is the founder of BleepingComputer, which is an often mentioned on this podcast terrific site.  They were early the main source of really good information when the very first cryptomalware, the file encryption stuff happened.  I think they were the people who were on the top of that news.  And then the so-called ransomware; right?



So I got a kick out of this because he said, I think it was late last week, he said:  "It has been a pretty slow ransomware week, as most of the malware developers have started pushing crypto miners."  So there you have it from the site that has been at the forefront of ransomware.  And as we've been saying, as soon as it became possible to monetize attacks in some other way, like mining, that's what happened.  So here's Larry saying it's been a pretty slow ransomware week as most of the malware developers have switched to crypto mining.



So in a different story there that sort of relates to that, they noted that Check Point Security did a study of the ranking of different malware.  And in BleepingComputer's coverage they said:  "Three in-browser cryptocurrency mining scripts ranked first, second, and fourth in Check Point's most active malware top 10."  So across all malware, cryptocurrency mining is three of the top four and now outranks classic high-output malware distribution infrastructures such as spam botnets, malvertising, and exploit kit operations.  That is, what it's hip to do now is get your mining address, your cryptocurrency address into as many other people's machines as possible, steal their computation cycles, and participate in a pool and turn it into cash flow.



So the number one, not surprisingly, the number one mining script is Coinhive.  Number two is Crypto-Loot, that we have not yet encountered on this podcast.  And number four is JSECoin, so no doubt JavaScript Electric coin.  "These three are online services," they write, "that offer JavaScript libraries that website owners can embed on their sites and generate profit by using their visitors' CPU resources to mine Monero cryptocurrency."



And they note that, while all three are legitimate - and Coinhive, I've been often quoted actually from this podcast noting that I thought maybe they should really be held responsible because there's no way they don't know they are being abused.  And we talked about it, in fact, the opt-in, the explicit opt-in face of Coinhive on a different domain and asking people, please don't block us.  We're really going to make sure people opt in.  We're taking responsibility for the opt-in ourselves so that nobody can use our script from this domain without us making sure the user knows that's happening.



That's really the only thing they could do at this point because they're now - they are a known domain that is being blocked by many of the adblockers.  So in Check Point's case, the company said that its security products - and this is surprising, but I guess this is a function of Check Point's demographic, their customer demographic.  So they said that their security products have detected cryptojacking across 42% of the organizations they protect, 42% of Check Point's customers that are contracted to get Check Point security protection.  Forty-two percent have had cryptojacking mining detected, Coinhive, of course, in the lead with detections found on 20% of all of Check Point's customers, followed by Crypto-Loot in a close second place at 15% of all of Check Point's customers.  So, I mean, it really is prevalent.  It's amazing.



And how does the crypto mining deal with the adblockers?  Well, not surprisingly because, again, the bad guys are as clever as the good guys, we have discussed the state-of-the-art means for botnets avoiding having their command-and-control server domains commandeered and taken over.  And that's the so-called Domain Name Generation Algorithms, where even when the malware is reverse-engineered, what happens is that, based on time of day, which the networks are able to obtain from network time or the system they're on and verify it and so forth, I mean, it's easy to know when it is, they generate pseudorandom domain names, like a bunch of them, like all at once, like a hundred.  And the idea is that, as we've discussed before, the white hats have to acquire all of those domains if they want to block them all because only one of them needs to respond to the botnet, which is happy to try them all.



So it's a very clever way of hugely upping the ante.  And the fact is I think it was the Conficker worm that was one of the early botnets that did this.  It is still in operation because it uses this technique, and it is prohibitively expensive for any group to constantly be having to preemptively register a large block of domains which are changing constantly.  I mean, the bad guys, similar to like DDoS attacks, for which there's really no good defense, have similarly found a way of solving the, oh, this is the domain that this botnet uses, we'll just stomp on it and cut off communication.



Well, not surprisingly, this same technique has now come to cryptocurrency mining, where it is no longer the case that adblockers can simply block www.coinhive.com in order to block the scripts.  Now the domains are being generated with algorithms based on time, and they are constantly changing with the mining script being sourced from domains which are unpredictable moving forward.  So we're seeing a technique that was developed earlier for a different application, now being moved - and unfortunately was powerful and successful - now being used to thwart simple domain name-based adblocking.  That doesn't work for keeping those sorts of scripts off of machines now.



The only thing I can see that is going to work is people starting to become much more aware than they are now about their CPU usage and utilization.  I have for years, I'm looking at it right now, I have Task Man running in the tray minimized, showing the little square that's all green.  But when my CPU starts getting used - there are four of them, it's a quad machine - it comes up so I can sort of see what's going on.  And I would imagine at some point someone will create something that alerts someone if one particular process over some length of time is never letting go of the machine.  Of course then the counterattack or the countermeasure for that would be for the miner to periodically stop mining and then go [humming], and then start up again in order to look more like the kind of process that you might expect to see.  So it's a mess.  And it's going to be with us for a long time because it makes money.



Speaking of which, two different groups, the Imperva crew and the SANS Security folks have been keeping an eye out for other mining ware.  And the Imperva crew discovered new cryptojacking malware targeting Redis servers.  Redis is a BSD-based caching server platform that offers services similar to memcache, but runs on a compatible piece of hardware and is unfortunately exploitable.  And then there's also more cryptojacking going on targeting Windows-based servers.  The SANS Security folks spotted a different campaign which targets the Apache Solr, which is a large, scalable, searchable, open source database system.  And as usual, these are publicly accessible, in all cases, publicly accessible server platforms which are essentially - we can sort of think of them as, in several cases, they have memcached sort of services.  But they're not being used for denial of service attacks, they're being used to mine cryptocurrency.



So again, the pressure that this creates to mint dollars or whatever denomination of currency is causing bad guys to come up with new ways of exploiting existing problems.  And in every case they are exploiting previously patched, in some cases by a year or two, vulnerabilities which are well known, have been long fixed.  But there are servers still operating that have not been updated; and the malware, which sort of ignored these services or servers until now, well, now there's a reason to get in there.  So they're running their cryptocurrency mining on them.



Oh, and I had a couple stats also.  In the case of the Redis servers, last month a different campaign than the one that was seen, but also against actually both Redis and OrientDB servers, netted nearly $1 million.  So this is the other reason.  As we know, the stronger the machine is, the more muscle its CPU has, the better the rate of cryptocurrency mining.  But this demonstrates that whoever it was behind that campaign who took the time and trouble to find and infect those two classes of servers, the Redis and the OrientDB servers, during a month netted just shy of a million dollars.



So this is why we're seeing a lot of this happening and expect to in the future.  It pays off.  And although I don't have a dollar figure, the SANS researchers determined that around 1,777 infections of that Apache Solr had succeeded between the end of February and March 8, so February 28 to March 8, so essentially nine days, in order to find 1,777 Apache Solr instances and set up cryptocurrency mining there.



LEO:  Apache Solr was the plugin used by Equifax that was unpatched that got them breached.



STEVE:  Ah ha ha, yup, makes sense.  Cool.  Also last week, exactly a week ago on March 6th, Microsoft detected a rapidly spreading cryptocurrency malware which successfully infected nearly half a million Windows machines within 12 hours.  A week ago, Windows Defender suddenly detected more than 80,000 instances of a malware named "Dofoil," or also known as "Smoke Loader," which was dropping a cryptocurrency miner as its payload to mine "Electroneum" coins.



LEO:  Sounds like a Marvel universe.



STEVE:  It does, doesn't it, yeah.  Electroneum.  That's a cool name, though.  Go get me some Electroneum.



LEO:  Of course, yeah, it's like Vibranium and Unobtainium.



STEVE:  Rub some Electroneum on that.  That'll help it.  Anyway, by the time Windows Defender could respond, nearly 400,000 additional infections were in place, but they weren't local.  They were rapidly spreading across Russia, Turkey, and Ukraine.  The question which has so far gone unanswered by Microsoft is how such a large audience became infected in such a short period.  We don't know that, but we do know that Defender shot off alarms, and then Microsoft was able to then update Defender and get it to help deal with that infection.



Okay.  And one last cryptocurrency mining story, and then we'll take our final break.  And that is, believe it or not, Leo, there's a currency miner in the Mac App Store, and Apple seems okay.



LEO:  I saw this.  Calendar 2, yeah. 



STEVE:  Calendar 2, exactly.



LEO:  Well, it's because it says - it's in the features.  You can turn it on.



STEVE:  Correct, correct.  And in fact the next page of the show notes I have a screenshot of Calendar 2's features.  Now, it was a little controversial.  Well, in fact the author has now said he's going to back out of that.  But he thought, hey, I'm going to declare it upfront.  I'm going to allow people to have all of the Calendar 2 features if they're willing, as he says, to unobtrusively generate cryptocurrency in the background.



LEO:  That's actually a good deal.  It's otherwise $18 to do that.



STEVE:  Yeah.  So you could pay once for $18 to unlock all new and future calendar features, or you could go the pay-as-you-go route at $0.99 per month to subscribe to unlock all new and future calendar features, or just run with the totally free version, and you just get the basic stuff.  So it was a little controversial because it was enabled, the free cryptocurrency mining version was what you got by default, without explicit notice.



So the critiques I saw suggested that, well, he really should have made it very clear, like asked users do you want to unlock all the features if we just do a little mining in the background?  Also, it was supposed to be in the background.  What the author now says is that there were problems with the library that they got from a third-party, whom he's blaming for these problems.  It was only supposed to take 10 to 20% of the CPU.  On the other hand, we know the more of CPU you take, the more you get.  So for whatever reason, whether it was deliberate or not, it was using way more of the system, which of course brought the device, the Mac, apparently really slowed it down.  And he was mining without permission.



And apparently, even if the user did not give permission, then due to some glitch in the library, we believe, it was causing a problem.  I think that's all true because this ended up not going over well, and he's now said he's going to remove that.  But again, as you said, Leo, if it were able to be unobtrusive, I guess maybe only mining while you're using the calendar, I'm assuming that's the case, it's not just running in the background all the time, and as an alternative to paying money, $18, maybe it makes sense.



LEO:  Why not, yeah.



STEVE:  I think we're going to see more of this.  To me, as I've said, as an alternative to web-based advertising, if it's efficient, that's the quest.  And in fact, you know, I've got a really fun story.  So, okay.  Our last cryptocurrency story is just too funny.  Well, fun.  Get this.  A French heater manufacturing company, and I don't know how to pronounce their name, Qarnot, Q-A-R-N-O-T?



LEO:  Qarnot, I'd say Qarnot, yeah.



STEVE:  Qarnot is actually selling, or at least offering to sell, we don't really know how many people are doing this, a cryptocurrency mining-based space heater.



LEO:  It's got an intuitive interface.



STEVE:  It does.  It'll talk to your smartphone.  It's got little LEDs.



LEO:  It's the first crypto heater.



STEVE:  It's the QC-1, Crypto Heater QC-1.



LEO:  It's noiseless.



STEVE:  Oh, yes, noiseless.  No fans.  No moving parts, so it uses...



LEO:  Oh, a little pricey.



STEVE:  It's a little pricey, yes.  It is 2,900 euros, which is at $3,600.  It's able to hash at 60 megahashes per second.



LEO:  Is that good?



STEVE:  Well, it makes Ethereum.  And at current hash rates, difficulty, and Ethereum trading rate, that would be about $120 per month.  So I don't know.



LEO:  And they don't - electricity not included.



STEVE:  Right.  And so it's not going to probably pay for itself.  I mean, if you sit on it, your butt'll get warm.  So on their website they say "Earn cryptocurrency while heating.  The heat of your QC-1 is generated by two graphics cards" - they have a pair of Nitro Radeon RX 580s with 8GB of RAM, which as I said is able to solve Ethereum at 60 megahashes per second.  They say "...two graphics cards embedded in the device and mining cryptocurrencies or blockchain transactions.  While heating, you create money.  You can watch in real-time how crypto markets are trending on your mobile app and on your QC-1 LEDs."  It's got a little strip of LEDs.



And it says:  "Noiseless high-end design.  The QC-1 crypto heater" - it's hard to even say with a straight face.  Yes, it's a crypto heater - "is the only one in the market [imagine that] to be perfectly noiseless."  Maybe there's some noisy ones.  "It doesn't embed any mobile part - no fans, no hard drives.  This system developed by Qarnot is IP protected," meaning intellectual property, I'm sure.  So they came up with the idea, and they're just going to sit on it.  Well, or maybe not, depending upon how hot it gets.



"Intuitive interface, designed as a plug-and-mine device."  You just plug it in and hook it to your WiFi and off it goes, maybe defraying the cost of heating your home a little bit.  The problem is, again, certainly now where you're using a pair of strong GPUs, your efficiency makes sense.  But again, $3,600 U.S. for a space heater?  I don't know.  Don't know if that quite cuts it.  But give them an A for coming up with the idea.



Okay.  I mentioned that tomorrow, March 14th, is the first day of the 2018 Pwn2Own conference up in Canada.  This is its 12th annual competition.  We've been covering them since 2007, so probably for as long as the podcast has been going since we're in our 12th year also.  And we've always had a lot of fun with the Pwn2Own conferences, the results over the years.  This year there's five categories:  virtualization, web browsers, enterprise applications, servers, and Microsoft - that is one of the co-hosts along with VMware this year, a sponsor - the fifth category is a special Microsoft Windows Insider Preview Challenge category, where actually they figure out what the S for Windows 10 S is actually supposed to stand for.  No, I'm kidding, I don't know.



The main backer, or the organizer, is Trend Micro's Zero Day Initiative, ZDI.  And as I said, the partners for the event are Microsoft and VMware.  As a consequence of them and other sponsors, up to $2 million in U.S. cash and prizes will be awarded.



LEO:  Wow, that's huge.



STEVE:  It is huge.  It's getting significant.  Microsoft offers a Windows Insider Preview Challenge that tests their latest prerelease offerings combined with their configuration on their hardware, and the title "Master of Pwn" will be awarded to the team with the most points at the end of the contest.  And so I thought there were a couple interesting things in the Zero Day Initiative announcement.



They said:  "Since its inception in 2007, Pwn2Own has increased the challenge level at each new competition, and this year is no different," they wrote.  "Web browsers return as a target" - yes, it's a fan favorite, folks - "as do virtual machine guest-to-host escapes.  Enterprise applications remain as targets for this year, and for 2018 Outlook [oh, boy] makes its Pwn2Own debut."  In years past, Outlook would have just been shredded.  It'll be interesting to see how it fares.



They said:  "Our virtualization category grows by two as Oracle becomes a target, and the Windows Insider Preview Challenge includes brand new targets for their virtualization-based security stack.  Server targets expand this year, as well.  Apache was included in last year's event and is joined this year by NGINX, OpenSSL" - oh, this is going to be a really interesting competition - "and Windows SMB server [oops].  Over the years we've seen some groundbreaking research demonstrated, so we can't wait to see," they write, "what contestants bring this year."  However, this year there will be no hacking entrants from the previous year's winning country.



LEO:  Oh.



STEVE:  China's government has decided to keep its security researchers home.  And as a new policy, Chinese researchers will not be attending remote events.  Now, my first thought was that this might be to prevent them from being detained as we know Marcus Hutchins was after last summer's Black Hat conference in Las Vegas, where he was nabbed at the McLaren Airport as he was attempting to return to London, I think it was.  So Brian Gorenc, who's the director of Trend Micro's Zero Day Initiative, said:  "There have been regulatory changes in some countries that no longer allow participation in global exploit contests such as Pwn2Own and Capture the Flag competitions."  And although he didn't explicitly in this quote mention it, he was referring specifically to China.  So there will be no Chinese research teams at Pwn2Own this year, which will likely be felt since for the last several years the Chinese teams dominated the competition.



So I think it's sort of sad because they've got excellent hacking skills.  Although I suppose for the people they were competing against, having China held back sort of shifts the balance.  So it'll be interesting to see who the winners are when China is not present.  Prior year Chinese winners were contacted and asked for comment, but none would remark other than to indicate that they would not be attending the competitions.



And in another little bit of Chinese-related news, a U.S. threat intelligence firm known as Recorded Future has spotted that the Chinese equivalent of our CVE database - we often talk about CVE, that's the Common Vulnerabilities and Exposures database which is located at cve.mitre, M-I-T-R-E, dot org.  China has one known as the CNNVD, the Chinese National Vulnerabilities Database, which is similarly meant to be open kimono and available and accessible.  Anyway, this firm has discovered that China has been retroactively manipulating the data in their own Chinese national vulnerabilities database, basically playing fast and loose with the facts and altering the database to conceal the influence of the Chinese Ministry of State Security.



They produced a report with four key judgments.  They said the CNNVD altered the original publication dates in its public database for at least 267 vulnerabilities that they identified as statistical outliers in their research which was published in November of 2017.  They wrote:  "We assessed in November that CNNVD had a formal vulnerability evaluation process in which high-threat CVEs were evaluated for their operational utility by the MSS [Ministry of State Security] before publication, and that the publication lag was one way to identify vulnerabilities that the MSS was likely considering for use in offensive cyber operations."



Now, in all fairness, it's hard to imagine that our own intelligence services in the U.S. aren't - who knows what manipulation is going on?  There has been, we have had some reporting of this notion that some of these vulnerabilities are being disclosed to the U.S. government before being made public.  So basically a similar operation.



LEO:  And vice versa, Intel told Lenovo before they told the U.S. government.



STEVE:  Exactly.  That was quite controversial, yes.  And although they say here that CNNVD's outright manipulation of these dates implicitly confirmed this Recorded Future's assessment.  They said:  "By retroactively changing the original publication dates on these statistical outliers, CNNVD attempted to hide the evidence of this evaluation process, obfuscate which vulnerabilities the MSS may be utilizing, and limit the methods researchers can use to anticipate Chinese APT [Advanced Persistent Threat] behavior."  And then they conclude, saying:  "This large-scale manipulation of vulnerability data undermines trust in the CNNVD process and could compromise security operations relying solely on the CNNVD for that information."  Although, again, our own CVE database is similarly public and probably contains virtually all of the same information.



I did want to note, just in passing, that last Friday U.S. District Judge Lucy Koh denied in part a motion by Verizon, which is the owner of Yahoo, to dismiss a case brought by plaintiffs who are suing Yahoo for failing to adequately protect their users' security and neglecting to respond to known and dangerous vulnerabilities in a timely manner.  We know from our previous reporting, and widely covered in the industry, that December before last, December 2016, Yahoo first disclosed the truth of the 2013 breach which they said compromised the credentials of a billion of their users.  And then just last October that assessment was revised to three billion impacted accounts.



Judge Koh wrote in her ruling on Friday:  "Plaintiffs explain that, had they known about the inadequacy of these security measures, they would have taken measures to protect themselves.  Plaintiffs' allegations are sufficient to show that they would have behaved differently had Defendants disclosed the security weaknesses of the Yahoo Mail system."  Which it now appears was known to Yahoo.  And at the time we talked about how Yahoo! just decided not to follow the urgent requests of their own security staff to fix problems, but rather spend their resources elsewhere.  So it'll be interesting to see how this settles out.  It is significant, I think, that Yahoo! is being held to account because other companies are certainly keeping an eye on this.



And finally, this stunned me, Leo.  I was very surprised by this.  I have a picture of, I guess it's a doughnut chart.  We used to just have pie charts.  Now we have doughnut charts, for some reason, I don't know why.  But the news is, according to a report that McAfee just released yesterday, McAfee analyzed the source of email spam Internet-wide.  And I was amazed by the distribution.  Two botnets, one called Necurs, N-E-C-U-R-S, and Gamut, G-A-M-U-T.  Those two botnets alone account for 97% of the Internet's spam, email spam.  I'm amazed.  And so this doughnut chart, this Necurs is by far the biggest.  It's generating 60% by volume in the fourth quarter of last year of all of the spam email.  And Gamut is at 37%.  So together 97%.  And then the other three are Lethic, Darkmailer, and then random Others, each with only 1% respectively.



So in McAfee's report they said:  "For most of these months, Necurs has spent its time churning out [what they called] 'lonely girl' spam lures for adult websites, pump-and-dump schemes, and delivering ransomware payloads.  Overall, nearly two out of three spam emails sent in the last quarter of 2017 were sent from the infrastructure of this mammoth," writes McAfee, "botnet.  Second on the list was the Gamut botnet, also built on Windows machines infected with malware that hijacks Windows to send out spam.  Gamut, while smaller in size when compared to Necurs," they write, "had previously been more active in the third quarter," sending more spam than Necurs.  However, they reversed positions.  In Q4, Gamut activity went down, but the botnet still accounted for 37% of all email spam, compared to Necurs' 60%.



They wrote:  "Most of Gamut's email subjects were related to job offer-themed phishing and money mule recruitment, [which is] tricking people to buy products with stolen money and sending the products to crooks; relaying money from hijacked bank accounts" to malicious accounts and so forth.  But I'm just amazed.  Two botnets, two massive Windows botnets are generating 97% of the spam on the Internet.  Wow.



LEO:  That is so amazing.



STEVE:  Huh?  Go ahead.



LEO:  It's amazing, yeah.



STEVE:  Yes, isn't that a surprising distribution?  I'm just stunned.  It's like they put everybody else out of business in the spam business.



LEO:  Yeah.



STEVE:  I got a nice note at the end of February, on the 27th, from Eric in Wisconsin.  The subject was "Yet Another SRSTD Story."  And then he says, his first line in the email is "SRSTD = SpinRiteSavesTheDay."



He said:  "Steve, first of all, THANK YOU [all caps] for your great SpinRite product."  He says:  "I've been meaning to pass along a story of yet another of SpinRite's successes for me for quite some time.  I have often thought this may be just another run-of-the-mill testimonial of how SpinRite has saved the day, but I thought I would share it anyway and let you be the judge."  And of course I'm always happy to have those and to be able to share them with our listeners.



He says:  "Besides, I had to say thanks.  A dentist's office was referred to me for support when their <insert well-known brand here> NAS device" - probably a Synology, just based on what he says later - "an NAS device was indicating multiple drive failures" - sounds like they'd neglected it for quite a while - "and they could no longer access any of their data for the office because the NAS would no longer boot."  He says:  "Of course I asked about their current backup process and was shocked [he says in quotes, parens] ('not really') to hear they had been meaning to address that hole in their process for some time."  So no backups.



"This particular NAS was bootable from its USB port," he said, "so I booted it into SpinRite and let SpinRite go to work.  By now, everyone knows that a happy ending usually ensues, and that is an understatement in this case.  SpinRite plowed through the entire array and upon reboot was happily serving data again.  For safekeeping, I was able to get a copy of their never-backed-up data off the NAS.  Needless to say, they were ecstatic about the recovery and also now have a good local and offsite backup process in place."  Well, clearly Eric has been listening to the podcast and knows the importance of doing that and obviously is up to speed on how to recover data and then keep it safe.



He says:  "I did recommend to them to go purchase their own copy of SpinRite to express their satisfaction, and I believe they have done so.  Thank you again for ALL [in all caps] that you do.  Look forward to listening to about 350 more SN episodes."  Yes, up until we finish with three digits.  He says:  "Keep up the good work.  Eric in Wisconsin."  So Eric, thank you for sharing that with me and allowing me to do so with our listeners.



And so finally, what was going to be the topic of today's podcast was to follow up on last week's podcast, where we introduced our listeners to what had just happened in the world, which was the discovery that there were tens of thousands of open, exposed to the public Internet, servers running memcached, a daemon, a server process, which was listening on port 11211 and would both receive data over that port for local caching and then, based on a tag that that data was tagged to, when later asked for it would send it back out.  The problem is since it was bound by default to both UDP and TCP - which, by the way has been changed in the updated version.  Now it's only bound to TCP, which is a minimal change.  But on the other hand, we know how few of these are probably going to be updated over time.  So these big available caches are going to be sitting on the public Internet for quite a while.



Now, for those who do update, then it will no longer be publicly exposing a UDP port.  That's important because it is trivial to change the source IP of a UDP packet in order to cause the memcache server to believe somebody else asked for this big blob of data.  As I mentioned at the top of the show, it is possible to create a bandwidth amplification of 51,000x.  So just crazy.



And in fact I didn't mention last week that there have been ransom demands being made of the sites that are being DDoSed, and the ransom demand is in the data that the site is being DDoSed with.  If you're being flooded with that much traffic, it's unlikely that many of those demands actually make it through your pipe, like to you, because it's going to crash upstream routers and just wreak havoc on your system because these attacks are so massive.  But I think it sounds like the attackers had a little bit of a sense of humor.



What we have seen, as the Picture of the Week at the top of the show notes showed, was predictably a massive increase in these attacks.  In just the last 10 days - and this was a report that ended several days ago.  So essentially in the week that followed the emergence of this, over 15,000 DDoS attacks based on this memcached service were used to attack 7,100 sites over that period of time.  Also since then the concept of course went wild.  There is now publicly available, freely available, downloadable attack code.



So although this was never a difficult attack to launch, and I'm sure a lot of coders who are capable of coding their own attack tools did so immediately, now even that's not necessary.  There is downloadable turnkey DrDOS - Distributed Reflection Denial Of Service - attack code available where you simply run the tool.  You put in the target, the IP address of the target you wish to attack.  That tool goes out to Shodan and looks up, gets the current list of known to Shodan memcached service IPs and begins filling the cache and bouncing UDP packets requesting that that cache be dumped on whatever victim was targeted.  So that no longer requires any coding.  That stuff is freely available on the Internet.



And the last little bit of news is that there has been some attempt at considering mitigation.  There are two other commands, other than "here's a tag, send me back the blob."  There is an "empty your cache" command, and there is a "shutdown" command.  I haven't seen anybody suggesting the use of the shutdown command.  And of course it would bring, if the memcached service was in active use by the site, having it shut down would bring it to the attention of the site owners pretty quickly.



But there is a "flush the cache."  And I've seen some suggestions that, if a site was under attack, that the "flush the cache" command could be sent proactively back to their array of sites that are attacking it.  Unfortunately, that's like 17,000 different services, at least.  And the moment you start flushing, then the other sites are still attacking you, and the flushed cache can begin to be refilled.  So the problem is - not that there's any practical solution to these attacks.  It's that we have publicly exposed caches now available on the Internet that are by default bound to both UDP and TCP, UDP trivial to spoof.  And so as would have been predicted, massive attacks are now underway leveraging this. 



The only thing that I've seen is that some providers, some big providers like Cloudflare are throttling all traffic across their border across port 11211.  That's the well-known port, as I mentioned, where this service is running.  So by throttling that traffic, at least that prevents both outgoing requests for cached data to remote public servers and prevents the data from getting through their border.  On the other hand, the bandwidth flood will still hit their border and cause a massive attack.



So at this point all we can do is keep an eye on this and see what the trend is moving forward.  This isn't crypto mining.  It's not making anyone any money, unless ransoms are being paid.  It does make DDoS, very, very potent DDoS attacks, far more powerful and easy to create than they were before.  So maybe it shifts the balance.  Or maybe this'll die down after a while, and people will go back to trying to install cryptocurrency mining, which is the new trend on the Internet, making some money that you can exchange for real dollars.  We'll see.



But the big story, of course, is AMD's very, very short notice of some serious problems.  I'm sure we will have much more to say about that on the podcast next week.



LEO:  You bet.  Or, as some speculate, it's just some company trying to dump AMD stock since they didn't publish exploits; right?  I mean, is that conceivable?  Awful lot of work to put in, a lot of technical detail.



STEVE:  Yeah.  And as I said, I was tempted to push this off to next week until I read through their research, which is very compelling.



LEO:  Yeah, okay.



STEVE:  So, yeah, I think that's not the case.  And does anyone know what has happened to AMD stock in the meantime?



LEO:  You know, it's funny, not much.



STEVE:  I'm not an owner of any tech stock.



LEO:  No, we don't have tech stocks.



STEVE:  Of any stock at all.



LEO:  You don't have stock at all.



STEVE:  No.



LEO:  I mean, you must have mutual funds or something.  You have investments.



STEVE:  I'm a municipal bond owner.



LEO:  Oh, there you go.  That's a good [crosstalk].



STEVE:  That's my solution.  Long time ago, back when the interest rates were much higher.



LEO:  Let me just check the AMD stock price.  Yeah, it's up 1%.



STEVE:  Yeah, well, it'll be interesting.  Again, I don't know if the news has hit mainstream.  Maybe there's already some vulnerability fatigue from Spectre and Meltdown, where it's like, okay, well, another one of these big problems.  We dealt with the ones at the beginning of the year, we'll deal with this one.  Who knows.  Anyway, it looked absolutely credible to me.  I'm sure I'll have much more mature coverage, because much more will be known, by a week from now.



LEO:  There was a bit of a selloff when the show began, but it's flattened out now in the afterhours trading.  Yeah, I don't - it's hard to say that the stock has been hurt much.



STEVE:  Yeah.



LEO:  It was going down for a while.  All right, Steve.  Somebody said Steve owns all of Fresno.  He owns all their convertible debt, yes.  Thank you, Steve.  Steve Gibson is at GRC.com.  That's his website, and he tweets @SGgrc.  You'll find him on the Twitter there.  You can leave long DMs for him if you want, if you have comments.  I left you a tweet, a funny article I read that might make a good Photo of the Week next week.



STEVE:  Oh, cool.



LEO:  But Steve always likes more input like that.  You go to  GRC.com you'll find SpinRite, that fine tool for dentists and hard drive users everywhere.  All you need to do is go to GRC.com, pick up a copy, and you'll be ready to test and recover hard drives to your little heart's content.  There's plenty of other stuff there, too, lots of freebies, including of course this show, 64Kb audio and fine handwritten transcripts from Elaine Farris, all there.  Searchable, too, all 654 episodes.



You can also find audio and video at our site, TWiT.tv/sn, or subscribe.  You'll get your show as soon as it's ready on your favorite podcast platform.  Security Now! has been around for a little while, so we're on most, I think almost every program has us.  You can even ask your voice-activated device to listen to Security Now!, and most of the time that works pretty well.  I think that's it for the day and for the week.  But we will see you, we will convene against next Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.



STEVE:  Oh, I'm sure we'll have more news, my friend.



LEO:  Thank you, Steve.  See you then.



STEVE:  Thanks, Leo.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.














